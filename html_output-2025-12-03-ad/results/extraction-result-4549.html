<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4549 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4549</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4549</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-ab4b7fa00bb08a2cd8700ef797f8aa4e6f8b6a4d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ab4b7fa00bb08a2cd8700ef797f8aa4e6f8b6a4d" target="_blank">Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes the Logit-based Calibrated Prior, an LLM-elicited correlation prior that transforms the model's raw output logits into a calibrated, continuous predictive distribution over correlation values, and shows that the prior generalizes to correlations not seen during LLM pretraining, reflecting context-sensitive reasoning rather than memorization.</p>
                <p><strong>Paper Abstract:</strong> As hypothesis generation becomes increasingly automated, a new bottleneck has emerged: hypothesis assessment. Modern systems can surface thousands of statistical relationships-correlations, trends, causal links-but offer little guidance on which ones are novel, non-trivial, or worthy of expert attention. In this work, we study the complementary problem to hypothesis generation: automatic hypothesis assessment. Specifically, we ask: given a large set of statistical relationships, can we automatically assess which ones are novel and worth further exploration? We focus on correlations as they are a common entry point in exploratory data analysis that often serve as the basis for forming deeper scientific or causal hypotheses. To support automatic assessment, we propose to leverage the vast knowledge encoded in LLMs' weights to derive a prior distribution over the correlation value of a variable pair. If an LLM's prior expects the correlation value observed, then such correlation is not surprising, and vice versa. We propose the Logit-based Calibrated Prior, an LLM-elicited correlation prior that transforms the model's raw output logits into a calibrated, continuous predictive distribution over correlation values. We evaluate the prior on a benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of 78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of 89.2% in predicting Pearson correlation coefficient. It also outperforms a fine-tuned RoBERTa classifier in binary correlation prediction and achieves higher precision@K in hypothesis ranking. We further show that the prior generalizes to correlations not seen during LLM pretraining, reflecting context-sensitive reasoning rather than memorization.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4549.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4549.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LCP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logit-based Calibrated Prior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that transforms LLM output logits for a structured numeric response into a calibrated, continuous predictive distribution over Pearson correlation coefficients, used to assess how surprising an observed correlation is relative to an LLM-elicited prior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Logit-based Calibrated Prior (LCP) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt the LLM to emit a structured scalar response ("coefficient": "<value>"). Extract token-level logits for the numeric field, enumerate top-k token sequences, map sequences to numeric correlation values, aggregate probabilities for identical numeric values, normalize to obtain a discrete distribution over decoded correlation values, smooth with Gaussian kernels (truncated to [-1,1]) to obtain a continuous density f(r), and compute evaluation metrics (sign accuracy, absolute error, information content = -log p(r_obs), and 95% credible interval coverage). Kernel bandwidth σ is tuned on a held-out validation set by minimizing average negative log-likelihood at observed correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predictive accuracy (sign accuracy, absolute error of mode vs observed), information content (negative log-likelihood of observed correlation under prior), calibration (95% credible interval coverage), retrieval utility (precision@K and average rank against human-flagged hypotheses), and generalization (performance under counterfactual/contradictory contexts to test reasoning vs memorization).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General empirical data analysis (applied to urban datasets and mixed-domain variable pairs); seeds for causal/hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Statistical relationships / correlation-based hypotheses (precursors to causal hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On a 2,096-pair benchmark: sign accuracy 78.8%, mean absolute error 0.26, 95% credible interval coverage 89.2%, average information content reduced from 0.69 (uniform baseline) to 0.27. In binary classification adaptation, zero-shot LCP reaches up to accuracy 0.84, F1 0.79, MCC 0.53. In Nexus retrieval (pool of 115 with 15 expert-flagged items) ranking by low prior-likelihood: Precision@5=0.60, Precision@10=0.80, Precision@15=0.60, average rank=21.5. Under contradictory (counterfactual) contexts on 84 reversed pairs: sign accuracy 95.2%, absolute error 0.30, information content 0.25, 95% coverage 92.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: mostly automated metrics computed against observed correlation values (sign accuracy, MAE, information content, calibration), plus an IR-style human-grounded evaluation using 15 expert-flagged hypothesis-worthy correlations from Nexus (precision@K and average rank against human labels).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Kernel σ tuned on held-out validation set of 300 correlations by minimizing average negative log-likelihood; calibration assessed via empirical 95% credible interval coverage; comparison against baselines (uniform, Gaussian, KDE) and a RoBERTa classifier; contextual-contradiction experiment (manually reviewed counterfactuals) to test generalization beyond memorized pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires one LLM call per correlation (costly); smoothing bandwidth choice matters (σ tuned globally under fixed LLM/prompt/task); LLM may still make multi-hop reasoning errors; prior reflects model knowledge which can differ from domain experts (false positives/negatives); method depends on tokenizer/top-k enumeration granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Primary benchmark: 2,096 real-world variable pairs (constructed from Cause-Effect Pairs and cleaned Kaggle pairs, stratified across |r|). Additional evaluation on Nexus (40,538 computed correlations; retrieval pool of 115 including 15 expert-flagged hypotheses). Contradiction test uses 84 Cause-Effect-derived counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4549.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SignAcc & MAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sign Accuracy and Mean Absolute Error (mode vs observed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predictive accuracy metrics: sign accuracy measures whether predicted mode and observed correlation share the same sign; mean absolute error (MAE) measures absolute difference between prior mode and observed Pearson r.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Sign accuracy and absolute error evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute sign accuracy = fraction of cases with predicted-mode * r_obs > 0. Compute absolute error = |mode(prior) - r_obs| and report mean/median over dataset bins and entire benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Direction correctness (sign), magnitude accuracy (absolute error), and per-bin bias analysis across ranges of r_obs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General statistical/correlation evaluation across mixed-domain variable pairs</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Correlation-based hypotheses (directional and magnitude prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LCP: sign accuracy 78.8% and MAE 0.26 across 2,096 pairs; uniform prior MAE 0.51; under contradictory contexts LCP sign accuracy 95.2% and MAE 0.30.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric-based evaluation against observed numerical correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reported across stratified bins of |r_obs|, and compared to baselines (Gaussian, KDE, uniform).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Sign accuracy sensitive to small magnitudes; MAE does not capture distributional uncertainty; bias varies by r_obs (underestimates strong positives, overestimates strong negatives).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>2,096-pair benchmark stratified by |r|; bin analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4549.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InformationContent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information Content (negative log-likelihood of observed correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A measure of how surprising an observed correlation is under an elicited prior, computed as -log p(r_obs) (Shannon self-information adapted to continuous density).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Information content (surprise) scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate each prior by computing the (continuous) negative log-likelihood at the observed Pearson correlation value, treating lower values as less surprising; aggregate mean/median to compare priors and to rank correlations by surprise for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Average and distribution of information content across corpus (lower is better for well-calibrated priors), ability to separate trivial from novel correlations via ranking by information content.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Exploratory data analysis / hypothesis triage</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Correlation hypotheses / novelty detection</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LCP reduced average information content from 0.69 (uniform prior baseline) to 0.27; Gaussian and KDE priors increased average information content to 4.10 and 1.73 respectively due to overconfident low-density tails.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated scoring used to rank hypotheses; retrieved hypotheses compared to human expert-labeled set in Nexus (hybrid evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Distributional comparisons (plots) and retrieval precision when ranking by information content; credible interval coverage also assessed to check overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on accurate density estimation; smoothing and discretization affect -log p(r_obs); extremely peaked (overconfident) priors yield large information values even for correct modes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>2,096 benchmark and Nexus retrieval pools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4549.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration95</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>95% Credible Interval Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Calibration metric: fraction of observed correlations that fall within the prior's 95% credible interval, used to assess whether the prior's uncertainty is well-calibrated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Credible interval coverage</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute 95% highest-density or central credible intervals from the continuous prior f(r) and report empirical coverage (fraction of r_obs values that lie inside those intervals).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Calibration: empirical coverage should be near nominal 95%; overconfident priors show much lower coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Uncertainty quantification for correlation priors</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LCP achieved 89.2% coverage (2,096 benchmark) and 92.9% coverage in contradictory-context experiment; Gaussian and uncalibrated KDE priors achieved poor coverage (49.1% and 59.9% respectively); uniform prior had 92.3% coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated calibration metric computed across benchmark; compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical coverage on held-out/evaluation datasets and calibration tuning via σ selection on validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires sensible uncertainty smoothing; global σ tuning assumes stable model/prompt/task; coverage can hide systematic biases in mode prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>2,096 benchmark and 84 counterfactual pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4549.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RetrievalEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval against human expert-flagged hypotheses (Precision@K / average rank)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Information-retrieval style evaluation that measures whether ranking correlations by LLM-elicited surprise surfaces hypothesis-worthy items flagged by domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Precision@K and average rank versus human labels</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Form a retrieval pool containing 15 expert-flagged correlations and 100 random negatives (pool size 115). Rank correlations by scoring function (e.g., absolute |r|, RoBERTa 'correlated' probability, or LCP prior likelihood) and compute Precision@5/10/15 and average rank of expert-labeled items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision@K (top-K retrieval precision) and average rank of expert-labeled hypotheses as indicators of alignment with human experts about which correlations are hypothesis-worthy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (for LCP); RoBERTa (for baseline classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Urban data / civic analytics (Chicago Open Data)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis-worthiness ranking of correlation-based hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LCP: Precision@5=0.60, Precision@10=0.80, Precision@15=0.60, average rank=21.5. RoBERTa ranking: Precision@5=0.60, Precision@10=0.60, Precision@15=0.53, average rank=30.9. Random baseline Precision@K≈0.13 (expected), ranking by |r| produced zero precision and average rank 95.4.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: algorithmic ranking (automated) evaluated against human expert labels (human judgments as ground truth for retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Statistical comparison to random baseline and other ranking strategies; appendix includes expected random performance derivation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Expert-labeled set is small (15 items), results subject to sampling variability; noisy data transformations in Nexus complicate ground-truth correlation values; human judgments may be domain-specific and incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Nexus-derived correlations from Chicago Open Data: full set of 40,538 correlations; retrieval pools of 115 including 15 expert-flagged examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4549.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ContradictionTest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual Contradiction / Counterfactual Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probing framework that generates plausible counterfactual contexts intended to flip an expected correlation, to test whether the LLM updates its prior based on context (reasoning) versus recalling memorized associations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Contextual contradiction (counterfactual) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For variable pairs where the model initially predicts the correct sign, generate alternate contexts via an LLM prompt that plausibly reverse the relationship (no explicit mention of correlation). Re-prompt the model with the modified context and evaluate whether the prior flips sign and updates magnitude. Manually review generated contexts for logical soundness. Compare metrics (sign accuracy, MAE, information content, calibration) before and after context change.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Sign flip/accuracy under counterfactual, calibration (95% coverage), absolute error, and information content under modified context as indications of contextual reasoning vs memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.5 Pro (used to generate contradictory contexts); GPT-4o used for LCP predictions</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / causal reasoning tests</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Assessing context-sensitive belief updates for correlation hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On 84 manually-reviewed counterfactual contexts LCP: sign accuracy 95.2% (dropped slightly from 100% on original contexts for selected subset), 95% coverage 92.9%, absolute error 0.30, information content 0.25; shows LCP updates beliefs according to context in most cases.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computation plus manual human review of generated counterfactual contexts for validity (hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Manual review of generated contexts; evaluation of metrics on synthetic (counterfactual) r_obs assigned as -r_obs; error analysis identifying multi-hop reasoning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Synthetic counterfactuals lack real-world data to validate assigned r_obs; multi-hop reasoning failures observed (e.g., mapping intermediate consequences to final numeric correlation can fail); manual review required to ensure quality of counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Subset of Cause-Effect Pairs: 84 pairs selected for contradiction testing (originally those where model predicted correct sign).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4549.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChiSquareNormality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chi-square Goodness-of-Fit Test for Normality of LLM Output Distributions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical test used to check whether the discrete LLM-elicited probability distribution over decoded correlation values is consistent with a fitted Gaussian distribution, motivating a non-parametric approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Chi-square goodness-of-fit on discrete logits-derived distribution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Convert the discrete logits-derived probability mass {r_j, p_j} into pseudo-counts O_j = M * p_j (nominal M=1000), fit a Gaussian using the distribution mean and variance, compute expected counts E_j under the fitted Gaussian, and compute χ² = Σ (O_j - E_j)^2 / E_j; derive p-value and reject normality at α=0.05 if appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Rejection frequency of normality hypothesis across prompts (higher rejection suggests Gaussian parametric fit is inappropriate).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (distributions derived from its logits)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Methodological validation for prior construction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Distributional assumption testing for elicited priors</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied to 2,096 correlations: normality hypothesis rejected in 2,095 cases at 5% significance, motivating the non-parametric LCP rather than Gaussian parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated statistical test.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard chi-square goodness-of-fit procedure with pseudo-count mapping; uses M=1000 nominal sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Test depends on discretization and chosen pseudo-count M; rejection could reflect tokenizer/top-k artifacts rather than true non-normal cognitive belief, but authors interpret pervasive rejections as practical evidence against Gaussian fits.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>2,096 benchmark used for distributional tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4549.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform, Gaussian, and KDE Prior Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prior-construction approaches compared to LCP: (i) uniform non-informative prior over [-1,1], (ii) Gaussian priors elicited via LLM-prompted mean and standard deviation (truncated to [-1,1]), and (iii) uncalibrated KDE over decoded discrete values with σ from Scott/weighted rule.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Baseline prior comparisons (Uniform / Gaussian / KDE)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Construct competing priors: uniform constant density, Gaussian with LLM-provided mean and σ (truncated), and KDE using decoded {r_j,p_j} with σ from weighted Scott's/Scott-like rule; evaluate these priors under the same suite of metrics (sign accuracy, MAE, information content, calibration) and report comparative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predictive accuracy, calibration (95% coverage), and information content; degree of overconfidence or underconfidence and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used to elicit Gaussian parameters and discrete outputs for KDE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Prior elicitation approaches for correlation hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Uniform prior: high coverage 92.3% but poor MAE (0.51) and low accuracy. Gaussian prior: often overconfident; median σ=0.10, coverage 49.1%, increased average information content 4.10. KDE prior (uncalibrated): coverage 59.9%, information content 1.73. LCP outperforms these baselines by balancing accuracy and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric comparison across the benchmark and in retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical evaluation across benchmark and σ analysis; deeper analysis of LLM behavior when asked for σ (finding that LLM often defaults to assuming n=100).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Gaussian baseline suffers from LLM's implicit sampling-size assumptions (σ ≈ 0.1). KDE without calibration yields overconfident, spiky densities. Uniform prior is uninformative for ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>2,096 benchmark and Nexus retrieval experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4549.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTaBaseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned RoBERTa Binary Correlation Classifier (Trummer adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa-based classifier fine-tuned to predict whether two variables are correlated beyond a threshold; used as a competitive baseline for binary correlation prediction and ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>RoBERTa fine-tuned correlation classification evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Fine-tune RoBERTa on 20% of the benchmark for each threshold and evaluate accuracy, F1, and MCC versus LCP adapted to binary decision by thresholding its mode; also evaluate zero-shot RoBERTa behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Classification accuracy, F1 score, Matthews correlation coefficient (MCC), threshold sensitivity, and requirement to retrain for different thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General data/correlation prediction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Binary correlated/not-correlated classification (decision boundary dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LCP (zero-shot) outperforms RoBERTa baselines across thresholds, despite RoBERTa being fine-tuned on 20% of benchmark: LCP reached up to accuracy 0.84, F1 0.79, MCC 0.53. Zero-shot RoBERTa behaved like a one-class detector (predicting 'correlated' always) with perfect recall but zero MCC; fine-tuned RoBERTa improved but required retraining per threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric-based comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Threshold-sweep experiments and fine-tuning on 20% splits; comparison of zero-shot vs fine-tuned behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>RoBERTa requires threshold-specific retraining; zero-shot fails as one-class detector; LCP provides full distribution enabling flexible thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>2,096 benchmark (20% fine-tune splits) and thresholded evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4549.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4549.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHELF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sheffield Elicitation Framework (SHELF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured human-expert prior elicitation protocol involving training, individual assessments, group discussion and fitting statistical distributions to elicited judgments; cited as a contrast to automated LLM-based elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SHELF human expert elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Traditional process for eliciting expert probability distributions: train experts, elicitation interviews, aggregation via group procedures, and fit parametric distributions (e.g., Beta, Normal) to consensus judgments; used historically to derive priors from human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human consensus, calibration of elicited distributions, inter-expert agreement, and validation against data where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Bayesian prior elicitation / decision analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Expert-elicited probabilistic priors over quantities of interest</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Discussed as costly/time-consuming compared to automated LLM elicitation; no numeric results presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based elicitation protocol (contrasted with LLM-based automatic elicitation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Established SHELF methodological practices (not validated here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Costly, time-consuming, requires expert training and group processes; paper positions LLM-based priors as a cheaper proxy though not a replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autoelicit: Using large language models for expert prior elicitation in predictive modelling <em>(Rating: 2)</em></li>
                <li>Can large language models predict data correlations from column names? <em>(Rating: 2)</em></li>
                <li>LLM processes: Numerical predictive distributions conditioned on natural language <em>(Rating: 2)</em></li>
                <li>Uncertain judgements: eliciting experts' probabilities <em>(Rating: 1)</em></li>
                <li>Nexus: Correlation discovery over collections of spatio-temporal tabular data <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4549",
    "paper_id": "paper-ab4b7fa00bb08a2cd8700ef797f8aa4e6f8b6a4d",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "LCP",
            "name_full": "Logit-based Calibrated Prior",
            "brief_description": "A method that transforms LLM output logits for a structured numeric response into a calibrated, continuous predictive distribution over Pearson correlation coefficients, used to assess how surprising an observed correlation is relative to an LLM-elicited prior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Logit-based Calibrated Prior (LCP) evaluation",
            "evaluation_method_description": "Prompt the LLM to emit a structured scalar response (\"coefficient\": \"&lt;value&gt;\"). Extract token-level logits for the numeric field, enumerate top-k token sequences, map sequences to numeric correlation values, aggregate probabilities for identical numeric values, normalize to obtain a discrete distribution over decoded correlation values, smooth with Gaussian kernels (truncated to [-1,1]) to obtain a continuous density f(r), and compute evaluation metrics (sign accuracy, absolute error, information content = -log p(r_obs), and 95% credible interval coverage). Kernel bandwidth σ is tuned on a held-out validation set by minimizing average negative log-likelihood at observed correlations.",
            "evaluation_criteria": "Predictive accuracy (sign accuracy, absolute error of mode vs observed), information content (negative log-likelihood of observed correlation under prior), calibration (95% credible interval coverage), retrieval utility (precision@K and average rank against human-flagged hypotheses), and generalization (performance under counterfactual/contradictory contexts to test reasoning vs memorization).",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "General empirical data analysis (applied to urban datasets and mixed-domain variable pairs); seeds for causal/hypothesis generation",
            "theory_type": "Statistical relationships / correlation-based hypotheses (precursors to causal hypotheses)",
            "human_comparison": true,
            "evaluation_results": "On a 2,096-pair benchmark: sign accuracy 78.8%, mean absolute error 0.26, 95% credible interval coverage 89.2%, average information content reduced from 0.69 (uniform baseline) to 0.27. In binary classification adaptation, zero-shot LCP reaches up to accuracy 0.84, F1 0.79, MCC 0.53. In Nexus retrieval (pool of 115 with 15 expert-flagged items) ranking by low prior-likelihood: Precision@5=0.60, Precision@10=0.80, Precision@15=0.60, average rank=21.5. Under contradictory (counterfactual) contexts on 84 reversed pairs: sign accuracy 95.2%, absolute error 0.30, information content 0.25, 95% coverage 92.9%.",
            "automated_vs_human_evaluation": "Hybrid: mostly automated metrics computed against observed correlation values (sign accuracy, MAE, information content, calibration), plus an IR-style human-grounded evaluation using 15 expert-flagged hypothesis-worthy correlations from Nexus (precision@K and average rank against human labels).",
            "validation_method": "Kernel σ tuned on held-out validation set of 300 correlations by minimizing average negative log-likelihood; calibration assessed via empirical 95% credible interval coverage; comparison against baselines (uniform, Gaussian, KDE) and a RoBERTa classifier; contextual-contradiction experiment (manually reviewed counterfactuals) to test generalization beyond memorized pairs.",
            "limitations_challenges": "Requires one LLM call per correlation (costly); smoothing bandwidth choice matters (σ tuned globally under fixed LLM/prompt/task); LLM may still make multi-hop reasoning errors; prior reflects model knowledge which can differ from domain experts (false positives/negatives); method depends on tokenizer/top-k enumeration granularity.",
            "benchmark_dataset": "Primary benchmark: 2,096 real-world variable pairs (constructed from Cause-Effect Pairs and cleaned Kaggle pairs, stratified across |r|). Additional evaluation on Nexus (40,538 computed correlations; retrieval pool of 115 including 15 expert-flagged hypotheses). Contradiction test uses 84 Cause-Effect-derived counterfactuals.",
            "uuid": "e4549.0",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SignAcc & MAE",
            "name_full": "Sign Accuracy and Mean Absolute Error (mode vs observed)",
            "brief_description": "Predictive accuracy metrics: sign accuracy measures whether predicted mode and observed correlation share the same sign; mean absolute error (MAE) measures absolute difference between prior mode and observed Pearson r.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Sign accuracy and absolute error evaluation",
            "evaluation_method_description": "Compute sign accuracy = fraction of cases with predicted-mode * r_obs &gt; 0. Compute absolute error = |mode(prior) - r_obs| and report mean/median over dataset bins and entire benchmark.",
            "evaluation_criteria": "Direction correctness (sign), magnitude accuracy (absolute error), and per-bin bias analysis across ranges of r_obs.",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "General statistical/correlation evaluation across mixed-domain variable pairs",
            "theory_type": "Correlation-based hypotheses (directional and magnitude prediction)",
            "human_comparison": false,
            "evaluation_results": "LCP: sign accuracy 78.8% and MAE 0.26 across 2,096 pairs; uniform prior MAE 0.51; under contradictory contexts LCP sign accuracy 95.2% and MAE 0.30.",
            "automated_vs_human_evaluation": "Automated metric-based evaluation against observed numerical correlations.",
            "validation_method": "Reported across stratified bins of |r_obs|, and compared to baselines (Gaussian, KDE, uniform).",
            "limitations_challenges": "Sign accuracy sensitive to small magnitudes; MAE does not capture distributional uncertainty; bias varies by r_obs (underestimates strong positives, overestimates strong negatives).",
            "benchmark_dataset": "2,096-pair benchmark stratified by |r|; bin analyses reported.",
            "uuid": "e4549.1",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "InformationContent",
            "name_full": "Information Content (negative log-likelihood of observed correlation)",
            "brief_description": "A measure of how surprising an observed correlation is under an elicited prior, computed as -log p(r_obs) (Shannon self-information adapted to continuous density).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Information content (surprise) scoring",
            "evaluation_method_description": "Evaluate each prior by computing the (continuous) negative log-likelihood at the observed Pearson correlation value, treating lower values as less surprising; aggregate mean/median to compare priors and to rank correlations by surprise for retrieval.",
            "evaluation_criteria": "Average and distribution of information content across corpus (lower is better for well-calibrated priors), ability to separate trivial from novel correlations via ranking by information content.",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "Exploratory data analysis / hypothesis triage",
            "theory_type": "Correlation hypotheses / novelty detection",
            "human_comparison": true,
            "evaluation_results": "LCP reduced average information content from 0.69 (uniform prior baseline) to 0.27; Gaussian and KDE priors increased average information content to 4.10 and 1.73 respectively due to overconfident low-density tails.",
            "automated_vs_human_evaluation": "Automated scoring used to rank hypotheses; retrieved hypotheses compared to human expert-labeled set in Nexus (hybrid evaluation).",
            "validation_method": "Distributional comparisons (plots) and retrieval precision when ranking by information content; credible interval coverage also assessed to check overconfidence.",
            "limitations_challenges": "Depends on accurate density estimation; smoothing and discretization affect -log p(r_obs); extremely peaked (overconfident) priors yield large information values even for correct modes.",
            "benchmark_dataset": "2,096 benchmark and Nexus retrieval pools.",
            "uuid": "e4549.2",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Calibration95",
            "name_full": "95% Credible Interval Coverage",
            "brief_description": "Calibration metric: fraction of observed correlations that fall within the prior's 95% credible interval, used to assess whether the prior's uncertainty is well-calibrated.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Credible interval coverage",
            "evaluation_method_description": "Compute 95% highest-density or central credible intervals from the continuous prior f(r) and report empirical coverage (fraction of r_obs values that lie inside those intervals).",
            "evaluation_criteria": "Calibration: empirical coverage should be near nominal 95%; overconfident priors show much lower coverage.",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "General",
            "theory_type": "Uncertainty quantification for correlation priors",
            "human_comparison": false,
            "evaluation_results": "LCP achieved 89.2% coverage (2,096 benchmark) and 92.9% coverage in contradictory-context experiment; Gaussian and uncalibrated KDE priors achieved poor coverage (49.1% and 59.9% respectively); uniform prior had 92.3% coverage.",
            "automated_vs_human_evaluation": "Automated calibration metric computed across benchmark; compared to baselines.",
            "validation_method": "Empirical coverage on held-out/evaluation datasets and calibration tuning via σ selection on validation set.",
            "limitations_challenges": "Requires sensible uncertainty smoothing; global σ tuning assumes stable model/prompt/task; coverage can hide systematic biases in mode prediction.",
            "benchmark_dataset": "2,096 benchmark and 84 counterfactual pairs.",
            "uuid": "e4549.3",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "RetrievalEval",
            "name_full": "Retrieval against human expert-flagged hypotheses (Precision@K / average rank)",
            "brief_description": "Information-retrieval style evaluation that measures whether ranking correlations by LLM-elicited surprise surfaces hypothesis-worthy items flagged by domain experts.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Precision@K and average rank versus human labels",
            "evaluation_method_description": "Form a retrieval pool containing 15 expert-flagged correlations and 100 random negatives (pool size 115). Rank correlations by scoring function (e.g., absolute |r|, RoBERTa 'correlated' probability, or LCP prior likelihood) and compute Precision@5/10/15 and average rank of expert-labeled items.",
            "evaluation_criteria": "Precision@K (top-K retrieval precision) and average rank of expert-labeled hypotheses as indicators of alignment with human experts about which correlations are hypothesis-worthy.",
            "model_name": "GPT-4o (for LCP); RoBERTa (for baseline classifier)",
            "model_size": null,
            "scientific_domain": "Urban data / civic analytics (Chicago Open Data)",
            "theory_type": "Hypothesis-worthiness ranking of correlation-based hypotheses",
            "human_comparison": true,
            "evaluation_results": "LCP: Precision@5=0.60, Precision@10=0.80, Precision@15=0.60, average rank=21.5. RoBERTa ranking: Precision@5=0.60, Precision@10=0.60, Precision@15=0.53, average rank=30.9. Random baseline Precision@K≈0.13 (expected), ranking by |r| produced zero precision and average rank 95.4.",
            "automated_vs_human_evaluation": "Hybrid: algorithmic ranking (automated) evaluated against human expert labels (human judgments as ground truth for retrieval).",
            "validation_method": "Statistical comparison to random baseline and other ranking strategies; appendix includes expected random performance derivation.",
            "limitations_challenges": "Expert-labeled set is small (15 items), results subject to sampling variability; noisy data transformations in Nexus complicate ground-truth correlation values; human judgments may be domain-specific and incomplete.",
            "benchmark_dataset": "Nexus-derived correlations from Chicago Open Data: full set of 40,538 correlations; retrieval pools of 115 including 15 expert-flagged examples.",
            "uuid": "e4549.4",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ContradictionTest",
            "name_full": "Contextual Contradiction / Counterfactual Evaluation",
            "brief_description": "A probing framework that generates plausible counterfactual contexts intended to flip an expected correlation, to test whether the LLM updates its prior based on context (reasoning) versus recalling memorized associations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Contextual contradiction (counterfactual) evaluation",
            "evaluation_method_description": "For variable pairs where the model initially predicts the correct sign, generate alternate contexts via an LLM prompt that plausibly reverse the relationship (no explicit mention of correlation). Re-prompt the model with the modified context and evaluate whether the prior flips sign and updates magnitude. Manually review generated contexts for logical soundness. Compare metrics (sign accuracy, MAE, information content, calibration) before and after context change.",
            "evaluation_criteria": "Sign flip/accuracy under counterfactual, calibration (95% coverage), absolute error, and information content under modified context as indications of contextual reasoning vs memorization.",
            "model_name": "Gemini 2.5 Pro (used to generate contradictory contexts); GPT-4o used for LCP predictions",
            "model_size": null,
            "scientific_domain": "General / causal reasoning tests",
            "theory_type": "Assessing context-sensitive belief updates for correlation hypotheses",
            "human_comparison": false,
            "evaluation_results": "On 84 manually-reviewed counterfactual contexts LCP: sign accuracy 95.2% (dropped slightly from 100% on original contexts for selected subset), 95% coverage 92.9%, absolute error 0.30, information content 0.25; shows LCP updates beliefs according to context in most cases.",
            "automated_vs_human_evaluation": "Automated metric computation plus manual human review of generated counterfactual contexts for validity (hybrid).",
            "validation_method": "Manual review of generated contexts; evaluation of metrics on synthetic (counterfactual) r_obs assigned as -r_obs; error analysis identifying multi-hop reasoning failures.",
            "limitations_challenges": "Synthetic counterfactuals lack real-world data to validate assigned r_obs; multi-hop reasoning failures observed (e.g., mapping intermediate consequences to final numeric correlation can fail); manual review required to ensure quality of counterfactuals.",
            "benchmark_dataset": "Subset of Cause-Effect Pairs: 84 pairs selected for contradiction testing (originally those where model predicted correct sign).",
            "uuid": "e4549.5",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ChiSquareNormality",
            "name_full": "Chi-square Goodness-of-Fit Test for Normality of LLM Output Distributions",
            "brief_description": "A statistical test used to check whether the discrete LLM-elicited probability distribution over decoded correlation values is consistent with a fitted Gaussian distribution, motivating a non-parametric approach.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Chi-square goodness-of-fit on discrete logits-derived distribution",
            "evaluation_method_description": "Convert the discrete logits-derived probability mass {r_j, p_j} into pseudo-counts O_j = M * p_j (nominal M=1000), fit a Gaussian using the distribution mean and variance, compute expected counts E_j under the fitted Gaussian, and compute χ² = Σ (O_j - E_j)^2 / E_j; derive p-value and reject normality at α=0.05 if appropriate.",
            "evaluation_criteria": "Rejection frequency of normality hypothesis across prompts (higher rejection suggests Gaussian parametric fit is inappropriate).",
            "model_name": "GPT-4o (distributions derived from its logits)",
            "model_size": null,
            "scientific_domain": "Methodological validation for prior construction",
            "theory_type": "Distributional assumption testing for elicited priors",
            "human_comparison": false,
            "evaluation_results": "Applied to 2,096 correlations: normality hypothesis rejected in 2,095 cases at 5% significance, motivating the non-parametric LCP rather than Gaussian parameterization.",
            "automated_vs_human_evaluation": "Automated statistical test.",
            "validation_method": "Standard chi-square goodness-of-fit procedure with pseudo-count mapping; uses M=1000 nominal sample size.",
            "limitations_challenges": "Test depends on discretization and chosen pseudo-count M; rejection could reflect tokenizer/top-k artifacts rather than true non-normal cognitive belief, but authors interpret pervasive rejections as practical evidence against Gaussian fits.",
            "benchmark_dataset": "2,096 benchmark used for distributional tests.",
            "uuid": "e4549.6",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Baselines",
            "name_full": "Uniform, Gaussian, and KDE Prior Baselines",
            "brief_description": "Baseline prior-construction approaches compared to LCP: (i) uniform non-informative prior over [-1,1], (ii) Gaussian priors elicited via LLM-prompted mean and standard deviation (truncated to [-1,1]), and (iii) uncalibrated KDE over decoded discrete values with σ from Scott/weighted rule.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Baseline prior comparisons (Uniform / Gaussian / KDE)",
            "evaluation_method_description": "Construct competing priors: uniform constant density, Gaussian with LLM-provided mean and σ (truncated), and KDE using decoded {r_j,p_j} with σ from weighted Scott's/Scott-like rule; evaluate these priors under the same suite of metrics (sign accuracy, MAE, information content, calibration) and report comparative performance.",
            "evaluation_criteria": "Predictive accuracy, calibration (95% coverage), and information content; degree of overconfidence or underconfidence and interpretability.",
            "model_name": "GPT-4o (used to elicit Gaussian parameters and discrete outputs for KDE)",
            "model_size": null,
            "scientific_domain": "General",
            "theory_type": "Prior elicitation approaches for correlation hypotheses",
            "human_comparison": false,
            "evaluation_results": "Uniform prior: high coverage 92.3% but poor MAE (0.51) and low accuracy. Gaussian prior: often overconfident; median σ=0.10, coverage 49.1%, increased average information content 4.10. KDE prior (uncalibrated): coverage 59.9%, information content 1.73. LCP outperforms these baselines by balancing accuracy and calibration.",
            "automated_vs_human_evaluation": "Automated metric comparison across the benchmark and in retrieval tasks.",
            "validation_method": "Empirical evaluation across benchmark and σ analysis; deeper analysis of LLM behavior when asked for σ (finding that LLM often defaults to assuming n=100).",
            "limitations_challenges": "Gaussian baseline suffers from LLM's implicit sampling-size assumptions (σ ≈ 0.1). KDE without calibration yields overconfident, spiky densities. Uniform prior is uninformative for ranking.",
            "benchmark_dataset": "2,096 benchmark and Nexus retrieval experiments.",
            "uuid": "e4549.7",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "RoBERTaBaseline",
            "name_full": "Fine-tuned RoBERTa Binary Correlation Classifier (Trummer adaptation)",
            "brief_description": "A RoBERTa-based classifier fine-tuned to predict whether two variables are correlated beyond a threshold; used as a competitive baseline for binary correlation prediction and ranking.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "RoBERTa fine-tuned correlation classification evaluation",
            "evaluation_method_description": "Fine-tune RoBERTa on 20% of the benchmark for each threshold and evaluate accuracy, F1, and MCC versus LCP adapted to binary decision by thresholding its mode; also evaluate zero-shot RoBERTa behavior.",
            "evaluation_criteria": "Classification accuracy, F1 score, Matthews correlation coefficient (MCC), threshold sensitivity, and requirement to retrain for different thresholds.",
            "model_name": "RoBERTa (fine-tuned)",
            "model_size": null,
            "scientific_domain": "General data/correlation prediction",
            "theory_type": "Binary correlated/not-correlated classification (decision boundary dependent)",
            "human_comparison": false,
            "evaluation_results": "LCP (zero-shot) outperforms RoBERTa baselines across thresholds, despite RoBERTa being fine-tuned on 20% of benchmark: LCP reached up to accuracy 0.84, F1 0.79, MCC 0.53. Zero-shot RoBERTa behaved like a one-class detector (predicting 'correlated' always) with perfect recall but zero MCC; fine-tuned RoBERTa improved but required retraining per threshold.",
            "automated_vs_human_evaluation": "Automated metric-based comparison.",
            "validation_method": "Threshold-sweep experiments and fine-tuning on 20% splits; comparison of zero-shot vs fine-tuned behaviors.",
            "limitations_challenges": "RoBERTa requires threshold-specific retraining; zero-shot fails as one-class detector; LCP provides full distribution enabling flexible thresholding.",
            "benchmark_dataset": "2,096 benchmark (20% fine-tune splits) and thresholded evaluations.",
            "uuid": "e4549.8",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SHELF",
            "name_full": "Sheffield Elicitation Framework (SHELF)",
            "brief_description": "A structured human-expert prior elicitation protocol involving training, individual assessments, group discussion and fitting statistical distributions to elicited judgments; cited as a contrast to automated LLM-based elicitation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "SHELF human expert elicitation",
            "evaluation_method_description": "Traditional process for eliciting expert probability distributions: train experts, elicitation interviews, aggregation via group procedures, and fit parametric distributions (e.g., Beta, Normal) to consensus judgments; used historically to derive priors from human experts.",
            "evaluation_criteria": "Human consensus, calibration of elicited distributions, inter-expert agreement, and validation against data where possible.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Bayesian prior elicitation / decision analysis",
            "theory_type": "Expert-elicited probabilistic priors over quantities of interest",
            "human_comparison": true,
            "evaluation_results": "Discussed as costly/time-consuming compared to automated LLM elicitation; no numeric results presented in this paper.",
            "automated_vs_human_evaluation": "Human-based elicitation protocol (contrasted with LLM-based automatic elicitation).",
            "validation_method": "Established SHELF methodological practices (not validated here).",
            "limitations_challenges": "Costly, time-consuming, requires expert training and group processes; paper positions LLM-based priors as a cheaper proxy though not a replacement.",
            "benchmark_dataset": "",
            "uuid": "e4549.9",
            "source_info": {
                "paper_title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autoelicit: Using large language models for expert prior elicitation in predictive modelling",
            "rating": 2
        },
        {
            "paper_title": "Can large language models predict data correlations from column names?",
            "rating": 2
        },
        {
            "paper_title": "LLM processes: Numerical predictive distributions conditioned on natural language",
            "rating": 2
        },
        {
            "paper_title": "Uncertain judgements: eliciting experts' probabilities",
            "rating": 1
        },
        {
            "paper_title": "Nexus: Correlation discovery over collections of spatio-temporal tabular data",
            "rating": 2
        }
    ],
    "cost": 0.017254,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior</h1>
<p>Yue Gong<br>Department of Computer Science<br>The University of Chicago<br>Chicago, IL 60637<br>yuegong@uchicago.edu</p>
<p>Raul Castro Fernandez<br>Department of Computer Science<br>The University of Chicago<br>Chicago, IL 60637<br>raulcf@uchicago.edu</p>
<h4>Abstract</h4>
<p>As hypothesis generation becomes increasingly automated, a new bottleneck has emerged: hypothesis assessment. Modern systems can surface thousands of statistical relationships-correlations, trends, causal links-but offer little guidance on which ones are novel, non-trivial, or worthy of expert attention. In this work, we study the complementary problem to hypothesis generation: automatic hypothesis assessment. Specifically, we ask—given a large set of statistical relationships, can we automatically assess which ones are novel and worth further exploration? We focus on correlations as they are a common entry point in exploratory data analysis that often serve as the basis for forming deeper scientific or causal hypotheses. To support automatic assessment, we propose to leverage the vast knowledge encoded in LLMs' weights to derive a prior distribution over the correlation value of a variable pair. If an LLM's prior expects the correlation value observed, then such correlation is not surprising, and vice versa. We propose the Logit-based Calibrated Prior, an LLM-elicited correlation prior that transforms the model's raw output logits into a calibrated, continuous predictive distribution over correlation values. We evaluate the prior on a benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of $78.8 \%$, a mean absolute error of 0.26 , and $95 \%$ credible interval coverage of $89.2 \%$ in predicting Pearson correlation coefficient. It also outperforms a fine-tuned RoBERTa classifier in binary correlation prediction and achieves higher precision@K in hypothesis ranking. We further show that the prior generalizes to correlations not seen during LLM pretraining, reflecting context-sensitive reasoning rather than memorization.</p>
<h2>1 Introduction</h2>
<p>Generating hypotheses from large data repositories is quickly becoming easier. Modern data discovery systems [2, 8, 25, 21] can enumerate every statistical relationship across datasets, and LLMs can draft thousands of plausible ideas by mining literature and data [33, 31, 29]. What used to take a researcher weeks now happens in minutes. This ease of generation, however, introduces a new bottleneck: assessment. Experts are flooded with machine-suggested relationships-correlations, causal links, trends, anomalies-without a clear signal for which ones merit deeper investigation. Many of these relationships are trivial, redundant, or already well known, forcing human experts to sift through a long list just to find a few that are novel.</p>
<p>For example, as illustrated in Figure 1, a correlation discovery system has surfaced tens of thousands of correlated variable pairs, leaving human experts to manually filter out trivial or expected patterns using their prior knowledge. A strong correlation between daily temperature and ice cream sales, for instance, is intuitive and quickly dismissed. In contrast, a negative correlation between household</p>
<p>income and housing prices might appear counterintuitive and warrant further scrutiny. This manual triage must be repeated across thousands of pairs to uncover truly novel or surprising correlations, making the process highly labor-intensive. One might hope that ranking correlations by magnitude could alleviate this burden. However, as shown in Figure 1 (left panel where variable pairs are ranked by $\left|r_{o b s}\right|$ ), stronger correlations are not necessarily more surprising—in fact, they often reflect well-known or redundant relationships, a trend we further verify in Section 5.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: How Human Experts Assess Correlations Manually and How an LLM Can Help</p>
<p>In this paper, we study the complementary problem to hypothesis generation: automated hypothesis assessment. Specifically, we ask—given a large set of statistical relationships, can we automatically assess which ones are novel and worth further exploration? We focus on correlation relationships as a starting point, since they are a common entry point in exploratory data analysis and often serve as seeds for forming deeper scientific or causal hypotheses [11, 4].
To tackle this problem, we draw inspiration from how experts reason: they use prior knowledge to form expectations about a correlation's direction and magnitude. If the observed correlation ( $r_{o b s}$ ) matches expectations, it is unsurprising; if it deviates, it may signal something worth exploring. In essence, experts apply an implicit prior shaped by their knowledge and the variable context.
Our core idea is to approximate this human prior using the rich, encoded knowledge within LLMs [27, 18]. Specifically, we define the LLM-elicited correlation prior, $p_{\mathrm{LM}}\left(r_{X, Y} \mid \mathcal{C}<em X_="X," Y="Y">{X, Y}\right)$, as a predictive distribution over correlation values $r</em>}$ between a variable pair $X, Y$ conditioned on their context $\mathcal{C<em _LM="{LM" _text="\text">{X, Y}$, such as the description for each variable. By prompting the LLM with this context, we elicit its belief about the correlation values, treating these beliefs as a proxy for human expectations.
The LLM-elicited correlation prior helps identify which correlations are novel and worth expert attention. For instance, in Figure 1, $p</em>\right)$ centers around 0.7 , making an observed value of 0.8 unsurprising. In contrast, a correlation of -0.2 against a prior centered at 0.5 signals high surprise. This surprise-based scoring offers a scalable way to surface potentially insightful correlations. In our later evaluation (Section 5), we show that the LLM prior highlights expert-validated hypotheses from noisy urban data [19].
In this work, we propose Logit-based Calibrated Prior, an LLM-elicited correlation prior which transforms the LLM's raw output logits into a calibrated, continuous predictive distribution over correlation values (Section 2). But how do we evaluate its quality?
First, we assess accuracy: if the prior's mode reliably predicts the sign and magnitude of observed correlations, it suggests alignment with empirical patterns. Second, we evaluate information content. A strong prior should assign high likelihood to observed correlations, reducing their information content relative to an uninformative baseline (e.g., a uniform prior). When applied at scale, this indicates the prior captures real-world patterns, easing the burden on analysts. Third, we measure calibration-whether the prior's uncertainty reflects reality-using $95 \%$ credible interval coverage. This is crucial for decision-making: overconfident priors exaggerate surprise and risk misdirecting expert attention. Finally, we ask a deeper question: is the prior reasoning from context, or merely recalling memorized correlations based on variable names? To probe this, we introduce a novel evaluation based on contextual contradiction to disentangle these possibilities.
To support these goals, we construct a benchmark of 2,096 variable pairs with observed correlations. We evaluate predictive quality and information reduction (Section 4), hypothesis discovery utility (Section 5), and whether the prior reflects contextual reasoning or memorization (Section 6).}}\left(r_{\text {Daily Temp, Ice Cream Sales }} \mid \mathcal{C</p>
<p>Results are promising: our Logit-based Calibrated Prior achieves 78.8% sign accuracy and a mean absolute error of 0.26 on Pearson correlation coefficients in the range $[-1,1]$, with strong calibration—95% intervals covering 89.2% of observed values. It also reduces the average information content from 0.69 (uniform prior) to 0.27. Our method outperforms baselines, including uninformative priors, Gaussian priors from LLM-verbalized parameters, and a fine-tuned RoBERTa classifier [30]. It also achieves higher precision@K when retrieving meaningful correlations in noisy urban data. For instance, it highlights a link between bike dock density and community wealth—a hypothesis studied in prior work [7]—while down-ranking obvious patterns like library visitors and book circulation. Finally, we show that the prior generalizes beyond correlations seen during pretraining.</p>
<p>These results show that LLMs encode informative prior beliefs about statistical relationships, demonstrating their potential to serve as proxies for hypothesis assessment—a task that currently relies on human expertise and is highly labor-intensive. Our work highlights a promising direction for leveraging LLMs to help experts navigate large hypothesis spaces and make novel discoveries.</p>
<h1>2 Logit-based Calibrated Prior (LCP): Constructing a Continuous Correlation Prior from LLM Logits</h1>
<p>In this section, we present the Logit-based Calibrated Prior (LCP), a method for constructing the correlation prior, $p_{\mathrm{LM}}\left(r_{X, Y} \mid \mathcal{C}<em X_="X," Y="Y">{X, Y}\right)$-a predictive distribution over correlation values $r</em>$, such as the description for each variable.
One way to elicit a distribution from an LLM is to have it parameterize a fixed form-e.g., modeling its belief over a correlation as a Gaussian by providing a mean and standard deviation. However, this approach relies on the assumption that the model's internal belief distribution conforms to the chosen parametric form, which is not the case in most cases. To test this, we conducted a chi-square goodness-of-fit analysis [28] on the LLM's output distributions for 2,096 correlations. The normality assumption was rejected in 2,095 cases at the 5\% significance level, indicating that the LLM's beliefs are poorly approximated by a Gaussian fit (see Appendix A for details). Moreover, this parametric approach introduces additional complexity: While the original goal is to estimate a single correlation value, it requires estimating additional parameters whose values are themselves subject to error.}$ between a variable pair $X, Y$ conditioned on their context $\mathcal{C}_{X, Y</p>
<p>Our approach. Rather than asking the LLM to estimate parameters of a fixed distributional form, we directly prompt it to predict the correlation coefficient (see prompt in Appendix G.1), and construct a full distribution over possible correlation values. While one could obtain this distribution by sampling from the model, it would be computationally expensive. To address this, we propose a more efficient strategy by constructing the prior directly from the LLM's logits. This approach does not assume the distribution's shape and allows the model to focus on estimating the correlation between the variables.</p>
<p>We begin by constructing a discrete probability distribution from the model's raw token logits (Algorithm 1). Without loss of generality, we assume $r$ denotes Pearson's correlation coefficient, constrained to the range $[-1,1]$. At each decoding step $t$, the language model produces a real-valued logit vector $\ell_{t}$, where each entry $\ell_{t}^{(i)}$ corresponds to a token in the vocabulary. These logits are converted into log-probabilities via the softmax function. For a selected token $v_{t}$ at position $t$, its log-probability is given by $\log p_{t}^{\left(v_{t}\right)}=\ell_{t}^{\left(v_{t}\right)}-\log \sum_{j} \exp \left(\ell_{t}^{(j)}\right)$.
We design a prompt that elicits a structured scalar response, such as ["coefficient": "<value>"]. To extract the correlation value, we first identify the start and end positions of "<value>" in the output sequence (line 4). Starting from this token position, we extract the top- $k$ tokens at each subsequent decoding step (line 5). A complete numeric response, such as "-0.69", is composed of a valid sequence of tokens-e.g., a sign, integer part, decimal point, and numeric suffix. For instance, at the numeric suffix token, the model might assign different probabilities to completions like 69,60 , or 70 .</p>
<p>We enumerate all token sequences (line 5), concatenate them into strings (line 6), cast them to float values (line 8), and compute their joint log-probabilities by summing the log-probabilities of each token in the sequence (line 12). We discard any sequences that produce invalid float values or values outside the valid correlation range $[-1,1]$ (line 10). When multiple token sequences map to the same numeric value (e.g., "0.65" and ".65"), we aggregate their unnormalized probabilities (line 13-17).
Finally, we normalize across all valid correlation values using the softmax function to obtain a discrete probability distribution, $\left{\left(r_{j}, p_{j}\right)\right}<em j="j">{j=1}^{N}$ (line 19) where $r</em>$ is}$ is a decoded correlation value, and $p_{j</p>
<p>its model-assigned probability. This distribution is sparse and limited to discrete values determined by the tokenizer and top- $k$ decoding strategy. However, downstream tasks-such as computing surprise in Figure 1-require probability density at arbitrary values. To support this, we smooth the distribution using a weighted sum of Gaussian kernels centered at each decoded value. Since Pearson correlations lie in $[-1,1]$, we truncate and renormalize the distribution to ensure it integrates to one. The final LCP density function $f(r)$ is defined as:</p>
<p>$$
f(r)=\frac{1}{Z} \sum_{j=1}^{N} p_{j} \cdot \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{\left(r-r_{j}\right)^{2}}{2 \sigma^{2}}\right), \quad r \in[-1,1]
$$</p>
<p>where $\sigma$ is the standard deviation of each kernel and controls the degree of smoothing, and $Z$ is the normalization constant.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">ConstructDiscretePriorFromLogits</span>
<span class="w">    </span><span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="n">Token</span><span class="w"> </span><span class="n">logits</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="n">ell_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">T</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">structured</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="n">template</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="err">\</span><span class="n">lvert</span><span class="err">\</span><span class="p">,</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ss">&quot;coefficient&quot;</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;&lt;value&gt;&quot;</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\}\</span><span class="p">)</span>
<span class="w">    </span><span class="k">Output</span><span class="err">:</span><span class="w"> </span><span class="n">Discrete</span><span class="w"> </span><span class="k">prior</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="nf">left</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="n">j</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">N</span><span class="err">}\</span><span class="p">)</span>
<span class="w">    </span><span class="k">Initialize</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="k">map</span><span class="err">:</span><span class="w"> </span><span class="n">logp_map</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">emptyset</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">t_</span><span class="err">{</span><span class="mi">0</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">t_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">FindValueTokenSpan</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="n">ell_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">T</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Locate</span><span class="w"> </span><span class="k">start</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">end</span><span class="w"> </span><span class="n">positions</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="n">field</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">sequences</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">v_</span><span class="err">{</span><span class="n">t_</span><span class="err">{</span><span class="mi">0</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">v_</span><span class="err">{</span><span class="n">t_</span><span class="err">{</span><span class="mi">1</span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="k">top</span><span class="o">-</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">k</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">tokens</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">position</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="nf">str</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="nf">concat</span><span class="err">}</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">is_valid_float</span><span class="p">(</span><span class="nf">str</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nc">float</span><span class="p">(</span><span class="nf">str</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="ow">in</span><span class="o">[</span><span class="n">-1,1</span><span class="o">]</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nc">float</span><span class="p">(</span><span class="nf">str</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="k">continue</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">r</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">sum_</span><span class="err">{</span><span class="n">t</span><span class="o">=</span><span class="n">t_</span><span class="err">{</span><span class="mi">0</span><span class="err">}}</span><span class="o">^</span><span class="err">{</span><span class="n">t_</span><span class="err">{</span><span class="mi">0</span><span class="err">}</span><span class="o">+</span><span class="n">L</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nf">left</span><span class="p">(</span><span class="n">v_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">logp_map</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">p_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">_</span><span class="w"> </span><span class="err">}}</span><span class="o">[</span><span class="n">\bar{r}</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="nf">exp</span><span class="w"> </span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">logp</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">map</span><span class="err">}</span><span class="o">[</span><span class="n">r</span><span class="o">]</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="o">+</span><span class="err">\</span><span class="nf">exp</span><span class="w"> </span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">r</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">p_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">_</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">map</span><span class="err">}</span><span class="o">[</span><span class="n">r</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">r</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="nf">left</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="n">j</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">N</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">softmax</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">logp</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">_</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">map</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Normalize</span><span class="w"> </span><span class="nf">log</span><span class="o">-</span><span class="n">probs</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">probability</span><span class="w"> </span><span class="n">distribution</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="nf">left</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="n">j</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">N</span><span class="err">}\</span><span class="p">)</span>
</code></pre></div>

<p>Selecting an appropriate kernel standard deviation $\sigma$ is critical to ensure the prior reflects realistic uncertainty. If $\sigma$ is too small, the resulting distribution will be overconfident and overly spiky; if too large, it will be underconfident and overly diffuse. Standard bandwidth selection rules, such as Scott's [23] or Silverman's rule [26], are not applicable in our setting, as they assume i.i.d. samples from an underlying distribution. In our case, in contrast, the discrete values and their probabilities are derived from LLM output logits and reflect model-specific beliefs, not empirical frequencies.
To address this, we tune $\sigma$ using a held-out validation set by minimizing the average negative log-likelihood at the observed correlation values:</p>
<p>$$
\sigma^{*}=\arg \min <em _obs="{obs" r__text="r_{\text">{\sigma} \mathbb{E}</em>}} \sim \mathcal{D<em _sigma="\sigma">{\text {val }}}\left[-\log p</em>\right)\right]
$$}\left(r_{\text {obs }</p>
<p>This objective penalizes priors that assign low probability density to ground-truth correlations, thereby encouraging distributions that place probability mass closer to the observed values. Optimizing $\sigma$ this way calibrates uncertainty to reflect empirical variability and improves downstream reliability. The validation set $\mathcal{D}_{\text {val }}$ consists of 300 randomly sampled correlations, disjoint from our evaluation dataset. The optimized value $\sigma^{*}=0.4$ is used for LCP.
The kernel standard deviation $\sigma$ does not need to be re-tuned as long as three key elements remain unchanged: the LLM, the prompting strategy, and the task (predicting Pearson correlation coefficients). This is because $\sigma$ corrects for the systematic bias in the model's uncertainty-that is, whether the model tends to be consistently overconfident or underconfident in its predictions. When the model architecture, prompt design, and task remain fixed, this bias remains stable across inputs, even</p>
<p>if individual predictions vary. In this setting, a single globally tuned $\sigma$ is sufficient to calibrate the model’s uncertainty across a broad range of variable pairs. We further demonstrate in our evaluation (Section 4, 5) that the selected $\sigma$ generalizes well on the evaluation dataset, demonstrating its robustness. However, if any of these components change—such as switching to a different model, altering the prompt, or targeting a different correlation metric—the structure of the output distribution may shift, and $\sigma$ should be re-tuned to ensure proper calibration.</p>
<p>In the evaluation, we compare the Logit-based Calibrated Prior against two baseline methods for constructing correlation priors, highlighting the advantages of avoiding parametric assumptions and applying proper calibration. The first is a Gaussian prior, which assumes the LLM can directly parameterize a normal distribution by predicting its mean and standard deviation. The second is an uncalibrated KDE prior, which is similar to our method, but selects the kernel standard deviation using Scott’s rule based on the empirical standard deviation of the discrete probability distribution.</p>
<h2>3 Benchmark Construction</h2>
<p>We curate a benchmark of 2,096 real-world variable pairs to evaluate correlation priors. Each entry includes two variables, their descriptions, a dataset summary, and the observed Pearson correlation $r_{\text{obs}}\in[-1,1]$, computed from raw data. The benchmark combines variable pairs from the Cause-Effect Pairs [15] and Kaggle [30] datasets. Code and data are available at an anonymous repository.</p>
<p>The Cause-Effect dataset contains 108 variable pairs with known causal relationships. We retain 96 pairs where the correlation is statistically significant ($p&lt;0.05$). The Kaggle dataset consists of correlations between variable pairs extracted from publicly available tables on Kaggle. The original dataset provides variable names but lacks variable descriptions. To enrich the context for variables, we use the Kaggle API to retrieve dataset summaries and employ GPT-4o (see Appendix G.5) to assess whether the variable names are self-descriptive. We filter out non-informative names (e.g., single characters or generic identifiers like “Unnamed: 0”) and retain only those pairs for which both variables are judged meaningful. This further cleaning allows us to isolate and study the model’s ability to reason about relationships, rather than its ability to interpret metadata.</p>
<p>After filtering, we obtain 7045 statistically significant correlations ($p&lt;0.05$). To mitigate the bias toward extreme correlations (see Fig. 2), we perform stratified sampling by $|r|$: divide the range $[-1,1]$ into 10 equal-width bins and sample 200 correlations per bin, yielding a balanced set of 2,000. A balanced sample ensures fair evaluation across all correlation strengths, preventing the model’s performance from being skewed by overrepresented low or high $|r|$ values.</p>
<h2>4 How Well Does LCP Predict Empirical Correlations?</h2>
<p>We evaluate LCP by measuring how well it predicts observed correlations. First, we assess <em>predictive accuracy</em> using two metrics: <em>sign accuracy</em>—the fraction where $\hat{r}\cdot r_{\text{obs}}&gt;0$—and <em>absolute error</em>, $|\hat{r}-r_{\text{obs}}|$, where $\hat{r}$ is the mode of the prior. Next, we evaluate differential information content by computing $-\log p(r_{\text{obs}})$, adapting Shannon’s self-information [24] to the continuous case. For simplicity, we refer to it as <em>information content</em> hereafter. A good prior assigns high likelihood to observed values, reducing the information content of the corpus and easing analyst workload. Finally, we assess <em>calibration</em> by 95% credible interval coverage—the fraction of cases where $r_{\text{obs}}$ falls within the prior’s 95% credible interval. Calibration is critical: an overconfident prior may exaggerate surprise from small deviations, leading to false positives and misleading experts.</p>
<p>We compare LCP with the following baselines. All methods use GPT-4o (2024-08-06) [17] as the underlying model.</p>
<h4>4.0.1 LCP</h4>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<p>[17]*</p>
<ul>
<li>Uniform Prior: A non-informative baseline with constant density 0.5 over $[-1,1]$. The sign accuracy for it is measured by randomly guessing the sign of the correlation.</li>
<li>Gaussian Prior: We adapt the method from Capstick et al. [1], which elicits Gaussian priors via LLM-prompted mean and standard deviation, to model a truncated Gaussian prior over correlations in $[-1,1]$ (see Appendix G.2).</li>
<li>KDE Prior: A kernel density estimation using Gaussian kernels, where the kernel standard deviation $\sigma$ is set using a weighted version of Scott’s rule: $\sigma=1.06 \cdot \hat{\sigma} \cdot n_{\text {eff }}^{-1 / 5}$, where $\hat{\sigma}$ is the weighted standard deviation of $\left{\left(r_{j}, p_{j}\right)\right}$, and $n_{\text {eff }}$ is the effective sample size.
Results. Fig. 3 reports the average value of each metric across all correlations, positioned in a quadrant plot. Complementarily, Fig. 4 presents the full distributions of absolute error, $p(r_{\text {obs }})$, and information content. Fig. 3 shows that LCP achieves the best balance-matching the highest sign accuracy ( $78.8 \%$ ) of KDE while providing significantly better calibration ( $89.2 \%$ coverage). In contrast, the uncalibrated KDE and Gaussian priors are overconfident, assigning low likelihood to $r_{\text {obs }}$ and yielding poor coverage ( $59.9 \%$ and $49.1 \%$, respectively). On the other hand, the uniform prior offers high coverage ( $92.3 \%$ ) but suffers from poor accuracy and high absolute error ( $\left|\hat{r}-r_{\text {obs }}\right|=0.51$ ).
In addition, LCP significantly reduces the average information content of the correlation corpus-from 0.69 under a uniform prior to 0.27 -indicating that it assigns higher likelihood to observed correlations. In contrast, the Gaussian and KDE priors increase the average information content to 4.10 and 1.73 , respectively, due to their overconfident predictions. This is reflected in the long tail of low-density values in Fig. 4b. As shown in Fig. 4, LCP yields more concentrated distributions for both likelihood $p(r_{\text {obs }}$ ) and information content, highlighting better calibration .
<img alt="img-1.jpeg" src="img-1.jpeg" /></li>
</ul>
<p>Figure 3: Accuracy vs. Calibration of Correlation Priors (IC=Information Content)
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Full Distribution of Metrics over Different Priors
To understand the poor calibration of the Gaussian and KDE priors, we examine their kernel standard deviations. Both produce overly small $\sigma$ values, leading to sharply peaked densities. The median $\sigma$ is 0.10 for the Gaussian prior and 0.08 for the KDE prior-both much smaller than the fixed $\sigma=0.4$ in LCP. Under the Gaussian prior, the LLM returned $\sigma=0.1$ in $74 \%(1,552 / 2,096)$ of cases. The full distribution of $\sigma$ values is shown in Appendix B. As we further examine in Appendix C, this behavior arises because the LLM interprets $\sigma$ as sampling variability and implicitly assumes a fixed sample size of 100 -producing a default value of $\sigma=0.1$ regardless of context. The predicted $\sigma$ captures</p>
<p>expected variation from random sampling (aleatoric uncertainty), but fails to adjust based on the input context or account for uncertainty arising from limited knowledge (epistemic uncertainty) [12, 5].</p>
<p>Figure 5 analyzes LCP’s behavior across ten bins of observed correlation $r_{\text{obs}}$. The bias $\hat{r}-r_{\text{obs}}$ decreases with $r_{\text{obs}}$: the model overestimates strong negatives and underestimates strong positives. Sign accuracy is lowest when $\left|r_{\text{obs}}\right|$ is small, bottoming out near $-0.15$ and rising sharply beyond $\left|r_{\text{obs}}\right| \gtrsim 0.3$, reaching near-perfect accuracy for $\left|r_{\text{obs}}\right| \geq 0.7$. In Fig. 5c,d, the prior assigns lowest density (i.e., highest information content) to moderately negative correlations ( $r_{\text {obs }} \approx-0.5$ ), indicating weaker estimation. In contrast, strong positives receive the highest density and lowest information content. Overall, the prior shows asymmetric error: it performs best on strong positives and struggles with moderate negatives, consistently underestimating correlation magnitude-a reflection of the LLM’s conservative predictions without direct data access.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Performance across ten bins of the true correlation $r_{\text {obs }}$</p>
<p>Comparison of LCP and RoBERTa on Binary Correlation Classification. While LCP models a full distribution, BERT- and RoBERTa-based classifiers [14, 6] can be adapted for binary correlation prediction-determining whether a pair of variables is correlated based on a predefined threshold. We adopt the method from Trummer [30], who fine-tune RoBERTa using labeled pairs to build a correlation classifier. LCP is adapted to solve the binary classification task by thresholding its mode, enabling direct comparison with classification-based approaches.</p>
<p>To ensure a fair comparison, we first evaluate RoBERTa in a zero-shot setting, matching LCP, which requires no training. We then fine-tune RoBERTa on 20\% of the benchmark, following standard practice for applying RoBERTa to downstream tasks. Figure 6 shows performance across different correlation thresholds. Note that RoBERTa must be re-trained for each threshold.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Classification Performance over Different Correlation thresholds</p>
<p>LCP consistently outperforms both baselines in terms of accuracy, F1, and MCC across all thresholds—achieving up to 0.84 accuracy, 0.79 F1, and 0.53 MCC—despite being entirely zero-shot. This indicates that our method provides the most balanced predictions overall. Zero-shot RoBERTa behaves like a one-class detector: it predicts correlated for every pair, yielding perfect recall but zero MCC and rapidly deteriorating accuracy/precision as the threshold tightens from 0.5 to 0.8. Fine-tuned RoBERTa corrects this imbalance to some extent after seeing 20\% of the data, but its gains are threshold-specific and require retraining whenever the decision boundary moves. In contrast, By producing a full predictive distribution over $r$, LCP naturally adapts to different thresholds.</p>
<h1>5 Using LCP to Retrieve Expert-Flagged, Hypothesis-Worthy Correlations</h1>
<p>Can LCP support hypothesis assessment in noisy, real-world settings? We evaluate it on Nexus [8], a system designed to help domain experts discover correlations in urban data. Nexus computes 40,538</p>
<p>pairwise correlations from Chicago Open Data [19] by aligning and aggregating numeric attributes from different tables-either temporally (by month) or spatially (by census tract). Attributes are summarized (e.g., via mean or sum), joined on a shared key, and then correlated. This pipeline introduces real-world challenges: joins across sources, aggregation choices, and missing values-all of which impact the resulting correlations.</p>
<p>Of the full set, 15 correlations were labeled as hypothesis-worthy by human experts in the original Nexus evaluation. For example, a correlation between bike dock density and community wealth suggests stations are more common in affluent areas, a hypothesis studied in [7]. We use these expert-flagged examples to evaluate how well LCP retrieves hypothesis-worthy correlations in messy, transformed data. Since ground-truth correlation values are unavailable due to data aggregation and imputation, we adopt an information retrieval setup: treating the 15 expert-flagged correlations as targets within a pool of 115, formed by adding 100 random samples from the full Nexus corpus.</p>
<p>We compare four ranking strategies: (i) random, (ii) by absolute correlation $|r|$, (iii) by increasing probability assigned by a RoBERTa model fine-tuned on $20 \%$ of the benchmark, where lower probability of the "correlated" class indicates higher surprise, and (iv) by increasing prior likelihood $p\left(r_{\text {obs }}\right)$ under LCP, treating lower likelihood as more surprising. We report Precision@5, @10, @15, and the average rank of expert-labeled correlations.</p>
<p>Table 1: Retrieval Performance Comparison</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Precision@5</th>
<th style="text-align: center;">Precision@10</th>
<th style="text-align: center;">Precision@15</th>
<th style="text-align: center;">Average Rank $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random Ranking</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: left;">Ranked by $</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Ranked by RoBERTa</td>
<td style="text-align: center;">$\mathbf{0 . 6 0}$</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: left;">Ranked by LCP</td>
<td style="text-align: center;">$\mathbf{0 . 6 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 5}$</td>
</tr>
</tbody>
</table>
<p>As shown in Table 1, ranking correlations by LCP outperforms all baselines-achieving up to 0.80 Precision@10 and reducing the average rank of expert-labeled correlations to 21.5, compared to 30.9, 58.0 and 95.4 for the RoBERTa, random and $|r|$-based rankings, respectively (See Appendix D for a derivation of the expected performance under random ranking). Ranking by $|r|$ performs worst, with the highest average rank and zero precision, as extreme correlations often reflect trivial or redundant relationships (e.g., repeated attributes across years), not meaningful insights in Chicago Open Data.</p>
<p>Using LCP, all four correlations related to the expert-labeled hypothesis-that bike stations are more likely to be located in wealthier areas-are ranked within the top 6 . In contrast, an unsurprising correlation-the one between library visitors and library circulation-is ranked much lower at 95th. These results demonstrate that LCP can surface correlations that align with expert judgment, even in the presence of data transformations and noise.</p>
<h1>6 Is LCP Reasoning from Context or Relying on Memorization?</h1>
<p>We evaluate whether LCP is reasoning from context or simply relying on memorization-a crucial distinction for generalization beyond the model's pretraining data. This is essential for hypothesis assessment, where many relationships are unseen during training and depend on context. To probe this, we introduce an evaluation based on contextual contradiction. For each variable pair, we construct an alternate context that plausibly reverses the original correlation, simulating a counterfactual. We then re-derive the prior by prompting the LLM with this modified context (Appendix G.4). If the model adjusts its belief accordingly, it suggests reasoning from context rather than memorization.
Contradictory Context Generation. We use the Cause-Effect Pairs dataset to construct counterfactual scenarios. From this dataset, we select 84 variable pairs where the model initially predicts the correct correlation sign. For each pair, we prompt Gemini 2.5 Pro to generate a new context that plausibly reverses the original relationship (see Appendix G.3). All 84 generated contexts are manually reviewed by the authors to ensure the reversal is logically sound and free of explicit cues (e.g., phrases like "therefore there should be a negative correlation"). We assign the negated correlation $-r_{\text {obs }}$ as the new observed value. Since these contexts are synthetic and no real data exists, these new $r_{\text {obs }}$ values serve as approximations.</p>
<p>Result. Table 2 shows the performance of correlation priors on reversed correlations. LCP achieves 100% sign accuracy on the original contexts, dropping slightly to 95.2% under contradictory contexts. Manual inspection reveals that two of the four errors stem from reasoning failures: the model grasps the high-level logic but fails at the final inference step in multi-hop scenarios (see Appendix E). LCP also maintains strong calibration, with 92.9% coverage at the 95% level, and achieves lower information content (0.25 vs. 0.69) and absolute error (0.30 vs. 0.55) compared to the uniform prior.</p>
<p>|  Method | Sign Acc. (↑) | $|\hat{r}-r_{\text {obs}}|$ (↓) | Information Content (↓) | 95% Coverage (↑)  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  Uniform | 0.464 | 0.55 ± 0.25 | 0.69 ± 0.00 | 92.3%  |
|  LCP (ours) | 0.952 | 0.30 ± 0.28 | 0.25 ± 0.08 | 92.9%  |</p>
<p>This experiment shows that LCP is not merely recalling memorized correlations. When given counterfactual contexts, it updates its predictions accordingly—achieving 95.2% sign accuracy with strong calibration and low error. These results suggest that LCP generalizes beyond pretraining and behaves dynamically, a crucial property for real-world hypothesis assessment.</p>
<h2>7 Related Work</h2>
<p><strong>Elicit Priors from Human Experts.</strong> O’Hagan et al. [16] and Gosling [9] introduce the SHELF framework, a structured protocol for eliciting expert judgments and converting them into probability distributions. The process involves training, individual assessments, group discussions, and consensus-building, followed by fitting a statistical distribution to the agreed-upon judgments. This human elicitation process is costly and time-consuming, whereas our approach exploits the rich knowledge encoded in LLM weights to approximate expert priors automatically.</p>
<p><strong>LLMs for Regression Tasks.</strong> Several works exploit the knowledge encoded in LLMs for regression. Choi et al. [3] use LLMs for feature selection by prompting whether a variable is predictive of a given target, while others [20, 10, 1] aim to model prior distributions over feature weights. For example, Requeima et al. [20] require training examples to guide the LLM in generating output distributions, and Capstick et al. [1] assume the LLM can directly parameterize a distribution by prompting it to output means and standard deviations given feature and target names. In contrast, our work focuses on constructing a prior distribution over correlation coefficients between variable pairs before observing any data, using raw LLM logits directly—without requiring the model to parameterize a distribution.</p>
<p><strong>Additional Related Work.</strong> We include further discussion on data discovery systems and automatic hypothesis generation in Appendix F.</p>
<h2>8 Conclusions</h2>
<p>In this paper, we propose the Logit-based Correlation Prior—an LLM-elicited prior that transforms raw output logits into a calibrated, continuous predictive distribution over correlation values—paving the way for automatic hypothesis assessment. Our experiments show (i) LCP achieves the best balance between accuracy and calibration for predicting empirical correlations, outperforming Uniform, Gaussian, and KDE priors; (ii) LCP outperforms a fine-tuned RoBERTa classifier on binary correlation classification; (iii) LCP effectively highlights hypothesis-worthy correlations flagged by human experts in noisy urban data; and (iv) LCP goes beyond memorizing correlation values from pretraining, performing contextual reasoning.</p>
<h2>9 Limitations</h2>
<p>Generating an LCP requires an LLM call per correlation, which can be costly at scale. To improve scalability, preprocessing steps—such as filtering out redundant variable pairs across similar datasets—can help reduce the number of required queries. LLMs may also produce false positives or negatives. An LLM may possess knowledge beyond that of human experts, causing it to dismiss correlations that are actually insightful to the experts (false negatives). It can also misinterpret well-known relationships, incorrectly flagging them as surprising (false positives), as shown in Appendix E.</p>
<h1>References</h1>
<p>[1] Alexander Capstick, Rahul G. Krishnan, and Payam Barnaghi. Autoelicit: Using large language models for expert prior elicitation in predictive modelling, 2025. URL https://arxiv.org/ abs/2411.17284.
[2] Fernando Chirigati, Harish Doraiswamy, Theodoros Damoulas, and Juliana Freire. Data polygamy: The many-many relationships among urban spatio-temporal data sets. In Proceedings of the 2016 International Conference on Management of Data, pages 1011-1025, 2016.
[3] Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. Lmpriors: Pre-trained language models as task-specific priors. arXiv preprint arXiv:2210.12530, 2022.
[4] MIT Critical Data, Matthieu Komorowski, Dominic C Marshall, Justin D Salciccioli, and Yves Crutain. Exploratory data analysis. Secondary analysis of electronic health records, pages 185-203, 2016.
[5] Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural Safety, 31(2):105-112, 2009.
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 4171-4186, 2019.
[7] Elizabeth Flanagan, Ugo Lachapelle, and Ahmed El-Geneidy. Riding tandem: Does cycling infrastructure investment mirror gentrification and privilege in portland, or and chicago, il? Research in Transportation Economics, 60:14-24, 2016.
[8] Yue Gong, Sainyam Galhotra, and Raul Castro Fernandez. Nexus: Correlation discovery over collections of spatio-temporal tabular data. Proc. ACM Manag. Data, 2(3), May 2024. doi: 10.1145/3654957. URL https://doi.org/10.1145/3654957.
[9] John Paul Gosling. Shelf: the sheffield elicitation framework. In Elicitation: The science and art of structuring judgement, pages 61-93. Springer, 2017.
[10] Henry Gouk and Boyan Gao. Automated prior elicitation from large language models for bayesian logistic regression. In AutoML Conference 2024 (Workshop Track), 2024. URL https://openreview.net/forum?id=euLzlnU7gz.
[11] Saint John Walker. Big data: A revolution that will transform how we live, work, and think, 2014.
[12] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In NeurIPS, 2017.
[13] Anthony ML Liekens, Jeroen De Knijf, Walter Daelemans, Bart Goethals, Peter De Rijk, and Jurgen Del-Favero. Biograph: unsupervised biomedical knowledge discovery via automated hypothesis generation. Genome biology, 12:1-12, 2011.
[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[15] Joris M Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard Schölkopf. Distinguishing cause from effect using observational data: methods and benchmarks. Journal of Machine Learning Research, 17(32):1-102, 2016.
[16] Anthony O’Hagan, Caitlin E Buck, Alireza Daneshkhah, J Richard Eiser, Paul H Garthwaite, David J Jenkinson, Jeremy E Oakley, and Tim Rakow. Uncertain judgements: eliciting experts' probabilities. John Wiley \&amp; Sons, 2006.
[17] OpenAI. Gpt-4o. https://platform.openai.com/docs/models/gpt-4o, 2024. Accessed: 2025-05-15.</p>
<p>[18] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.
[19] Chicago Data Portal. Chicago data portal, 2025. URL https://data.cityofchicago.org/.
[20] James Requeima, John Bronskill, Dami Choi, Richard Turner, and David K Duvenaud. Llm processes: Numerical predictive distributions conditioned on natural language. Advances in Neural Information Processing Systems, 37:109609-109671, 2024.
[21] Aécio Santos, Aline Bessa, Fernando Chirigati, Christopher Musco, and Juliana Freire. Correlation sketches for approximate join-correlation queries. In Proceedings of the 2021 International Conference on Management of Data, pages 1531-1544, 2021.
[22] Aécio Santos, Flip Korn, and Juliana Freire. Efficiently estimating mutual information between attributes across tables. In 2024 IEEE 40th International Conference on Data Engineering (ICDE), pages 193-206, 2024. doi: 10.1109/ICDE60146.2024.00022.
[23] David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley \&amp; Sons, 2015.
[24] Claude E Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379-423, 1948.
[25] Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara, Takashi Washio, Patrik O Hoyer, Kenneth Bollen, and Patrik Hoyer. Directlingam: A direct method for learning a linear non-gaussian structural equation model. Journal of Machine Learning Research-JMLR, 12(Apr):1225-1248, 2011.
[26] Bernard W Silverman. Density estimation for statistics and data analysis. Routledge, 2018.
[27] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172-180, 2023.
[28] George W Snedecor and Witiiam G Cochran. Statistical methods, 8thedn. Ames: Iowa State Univ. Press Iowa, 54:71-82, 1989.
[29] Scott Spangler, Angela D. Wilkins, Benjamin J. Bachman, Meena Nagarajan, Tajhal Dayaram, Peter Haas, Sam Regenbogen, Curtis R. Pickering, Austin Comer, Jeffrey N. Myers, Ioana Stanoi, Linda Kato, Ana Lelescu, Jacques J. Labrie, Neha Parikh, Andreas Martin Lisewski, Lawrence Donehower, Ying Chen, and Olivier Lichtarge. Automated hypothesis generation based on mining scientific literature. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14, page 1877-1886, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450329569. doi: 10.1145/2623330.2623667. URL https://doi.org/10.1145/2623330.2623667.
[30] Immanuel Trummer. Can large language models predict data correlations from column names? Proc. VLDB Endow., 16(13):4310-4323, September 2023. ISSN 2150-8097. doi: 10.14778/ 3625054.3625066. URL https://doi.org/10.14778/3625054.3625066.
[31] Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, and Aidong Zhang. Improving scientific hypothesis generation with knowledge grounded large language models, 2024. URL https://arxiv.org/abs/2411.02382.
[32] Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh TN Nguyen, Lauren T May, Geoffrey I Webb, and Shirui Pan. Large language models for scientific discovery in molecular property prediction. Nature Machine Intelligence, pages 1-11, 2025.
[33] Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. Hypothesis generation with large language models. In Proceedings of the 1st Workshop on NLP for Science (NLP4Science), page 117-139. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.nlp4science-1.10. URL http://dx.doi.org/10.18653/v1/2024. nlp4science-1.10.</p>
<h1>A Normality Test via Chi-Square Goodness-of-Fit</h1>
<p>To assess whether the LLM's output distribution over correlation values conforms to a Gaussian shape, we perform a chi-square goodness-of-fit test. For each correlation prompt, we obtain a discrete probability distribution $\left{\left(r_{j}, p_{j}\right)\right}<em j="j">{j=1}^{N}$, where each $r</em>$ is the associated model-assigned probability mass, derived from token-level logits.} \in[-1,1]$ is a decoded numeric value and $p_{j</p>
<p>We convert the probability mass function into a set of pseudo-counts by assuming a nominal sample size $M=1000$, yielding observed counts $O_{j}=M \cdot p_{j}$. We then estimate the mean $\mu$ and variance $\sigma^{2}$ of the distribution as follows:</p>
<p>$$
\mu=\sum_{j=1}^{N} r_{j} \cdot p_{j}, \quad \sigma^{2}=\sum_{j=1}^{N}\left(r_{j}-\mu\right)^{2} \cdot p_{j}
$$</p>
<p>Next, we compute the expected count for each support point $r_{j}$ under a fitted Gaussian:</p>
<p>$$
q_{j}=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{\left(r_{j}-\mu\right)^{2}}{2 \sigma^{2}}\right)
$$</p>
<p>which we normalize to form a probability distribution $\tilde{p}<em j="j">{j}=q</em>$.
The chi-square test statistic is computed as:} / \sum_{j} q_{j}$, and then scale to expected counts $E_{j}=M \cdot \tilde{p}_{j</p>
<p>$$
\chi^{2}=\sum_{j=1}^{N} \frac{\left(O_{j}-E_{j}\right)^{2}}{E_{j}}
$$</p>
<p>The null hypothesis is that the observed distribution comes from the fitted Gaussian. We evaluate the $p$-value corresponding to the computed $\chi^{2}$ and reject the null at the $5 \%$ significance level.
Applied to the 2,096 correlations in our benchmark, the normality hypothesis was rejected in 2,095 cases, indicating that the LLM's output distributions are poorly approximated by a parametric Gaussian form. This result justifies our non-parametric approach, which avoids imposing a fixed distributional shape.</p>
<h2>B Distribution of Kernel Standard Deviations</h2>
<p>Figure 7 shows the distribution of kernel standard deviations $\sigma$ used in the Gaussian and KDE priors. Both priors tend to produce small $\sigma$ values, contributing to overconfident and poorly calibrated predictions. The median $\sigma$ is 0.10 for the Gaussian prior and 0.08 for the KDE prior.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Distribution of kernel standard deviations for Gaussian and KDE priors.</p>
<h1>C Understanding the LLM's Behavior in Reporting Standard Deviations</h1>
<p>The LLM (GPT-4o) favors the value 0.1 when reporting standard deviations. $\sigma=0.1$ appears in $74 \%$ of cases (1,552 out of 2,096 prompts). To better understand this behavior, we conducted a targeted analysis of the LLM's internal assumptions when predicting $\sigma$.
We prompted GPT-4o with 50 column pairs whose names were random strings with no semantic meaning (e.g., abc123, xzy987). This design removes contextual cues, allowing us to observe the model's default behavior under maximum uncertainty. In all 50 cases, the predicted correlation coefficient was exactly zero, and in 46 out of 50 cases, the predicted standard deviation was 0.1 . The strong preference for $\sigma=0.1$ even in the absence of context suggests that, when prompted to express its uncertainty as the standard deviation of a normal distribution, the LLM may default to fixed assumptions-such as an implicit sample size-rather than adjusting its estimate based on contextual information.</p>
<p>To investigate why $\sigma=0.1$ is so commonly predicted, we analyzed the distribution of Sample Pearson's correlation coefficient $r$ under the assumption that the true correlation $\rho=0$. When data is sampled from a bivariate normal distribution with zero correlation, the sampling distribution of $r$ has the following form:</p>
<p>$$
f_{r}(r)=\frac{\Gamma\left(\frac{n-1}{2}\right)}{\sqrt{\pi} \Gamma\left(\frac{n-2}{2}\right)} \cdot\left(1-r^{2}\right)^{\frac{n-4}{2}}, \quad \text { for }-1&lt;r&lt;1
$$</p>
<p>where $n$ is the sample size and $\Gamma(\cdot)$ is the gamma function. This distribution is bell-shaped, and its standard deviation decreases as $n$ increases. Specifically, the variance is given by $\operatorname{Var}[r]=\frac{1}{n-1}$, so the standard deviation is $\operatorname{SD}[r]=\frac{1}{\sqrt{n}-1}$. When $n=100$, this yields $\operatorname{SD}[r] \approx 0.1$, which aligns with the value most often returned by the LLM.</p>
<p>To test this hypothesis, we asked GPT-4o to explicitly state the sample size it assumes when estimating uncertainty. In all 50 test cases, it responded with $n=100$, confirming that its predicted standard deviation reflects a fixed assumption about sample size rather than context-specific reasoning.
This result suggests that GPT-4o's predicted $\sigma$ reflects aleatoric uncertainty—uncertainty due to random sampling around a fixed true correlation. The model assumes a fixed value of $\rho$ and estimates how much empirical values of $r$ might vary if repeatedly sampled. However, the type of uncertainty we aim to capture in this work is primarily epistemic uncertainty-uncertainty arising from the LLM's lack of knowledge about the relationship.
For example, if $X$ represents altitude and $Y$ represents precipitation, the correlation might be 0.7 in the U.S. and 0.6 in Germany. If the LLM does not know which country the data comes from, the true correlation is ambiguous-not due to sampling variability, but due to missing contextual information. This is epistemic uncertainty. Unlike aleatoric uncertainty, it can be reduced by providing more metadata or context about the table and dataset.
In summary, GPT-4o's use of $\sigma=0.1$ reveals a key limitation of the Gaussian prior: it captures a fixed form of uncertainty based on sampling variability but fails to account for uncertainty arising from a lack of knowledge-such as ambiguity in context or missing information. As a result, it often produces overconfident predictions.</p>
<h2>D Expected Performance Under Random Ranking</h2>
<p>The random ranking baseline yields a constant expected Precision@ $k$ of approximately $15 / 115=$ 0.13 , since each of the 15 expert correlations is equally likely to appear in any of the 115 positions. The corresponding average rank is 58.0 , which is the expected value of a discrete uniform distribution over ranks 1 to 115: $\mathbb{E}[\text { rank }]=\frac{1 \times 115}{2}=58$</p>
<h1>E Multi-hop Reasoning Failure</h1>
<p>We analyze a case involving the variables Horsepower and acceleration time (measured as 0-100 $\mathrm{km} / \mathrm{h}$ time), which are originally negatively correlated: higher horsepower typically leads to faster acceleration and thus shorter time.</p>
<p>We introduce a counterfactual context: "Imagine a world where cars are powered by highly unstable, experimental engines. More horsepower means a greater chance of catastrophic failure, forcing drivers to accelerate slowly and cautiously to avoid explosions." Under this context, we would expect the correlation to flip-higher horsepower should now lead to longer acceleration time, yielding a positive correlation.
However, the LLM still predicts a negative correlation. Its explanation is as follows:
"In this hypothetical world, cars with more horsepower are more prone to catastrophic failure due to their unstable engines. As a result, drivers of high-horsepower cars are likely to accelerate more slowly and cautiously to avoid triggering an explosion. This cautious driving behavior would lead to longer $0-100 \mathrm{~km} / \mathrm{h}$ acceleration times. Conversely, cars with lower horsepower are less likely to experience such failures, allowing drivers to accelerate more quickly. Therefore, there is likely a strong negative correlation between horsepower and acceleration time, as higher horsepower results in slower acceleration due to the need for caution."</p>
<p>While the model correctly reasons that higher horsepower leads to slower acceleration, it fails at the final reasoning step-mapping slower acceleration to longer time. This results in an incorrect correlation prediction, highlighting a multi-hop reasoning failure.</p>
<h2>F Additional Related Work</h2>
<p>Data Discovery. Modern data discovery systems [2, 25, 21, 22, 8, 30] efficiently compute statistical relationships such as correlations, causality, and mutual information across datasets. They extend beyond analyzing variables within a single dataset to discovering relationships between variables across multiple datasets by automatically transforming and joining different datasets. Specifically, for correlation discovery, Nexus [8] aligns large repositories of spatio-temporal datasets and identifies correlations, while Trummer [30] use a RoBERTa classifier to predict whether two variables are correlated based solely on their names. Data discovery systems surface a large number of potential relationships, but helping analysts identify the ones most relevant to their needs remains a key challenge in this field. Our approach, which uses an LLM-elicited prior to rank relationships, serves as a stepping stone toward addressing this challenge.
Automatic Hypothesis Generation. While data discovery systems identify statistical relationships from structured data that may lead to new hypotheses, a complementary line of work [29, 31, $32,13,33]$ focuses on mining unstructured scientific literature. These methods extract semantic knowledge-such as entities, links, and claims-from text, and store this knowledge for further analysis. Some approaches $[29,13]$ construct knowledge graphs and use graph analysis to suggest hypotheses, while others leverage language models to analyze the knowledge and suggest hypotheses directly [32, 31]. Zhou et al. [33] explores combining literature-derived insights with structured data.</p>
<h2>G Prompts</h2>
<h2>G. 1 Correlation Prediction Prompt to Construct Logit-based Calibrated Prior</h2>
<h2>Correlation Prediction Prompt for LCP</h2>
<p>Task: You are given two attributes from a tabular dataset. Your task is to predict the Pearson's correlation coefficient between the two attributes.</p>
<h2>Now, begin to solve the following problem:</h2>
<h2>Attributes:</h2>
<ul>
<li>{attr1}</li>
<li>{attr2}</li>
</ul>
<p>Source Table: {table}
Descriptions:</p>
<ul>
<li>Dataset Description: {tbl_desc}</li>
<li>Attribute Descriptions:
{attr1}: {var1_desc}
{attr2}: {var2_desc}
Respond with your predictions in the following format:
{
"coefficient": "<predicted correlation coefficient>",
}</li>
</ul>
<h1>G. 2 Correlation Prediction Prompt to Construct Gaussian Prior</h1>
<h2>Correlation Prediction Prompt for LCP</h2>
<p>Task: You are given two attributes from a tabular dataset. Your task is to predict the Pearson's correlation coefficient between the two attributes and estimate your confidence in the predicted correlation by providing the standard deviation as a measure of uncertainty. Note that the standard deviation cannot be zero.</p>
<p>Now, begin to solve the following problem:
Attributes:</p>
<ul>
<li>{attr1}</li>
<li>{attr2}</li>
</ul>
<p>Source Table: {table}
Descriptions:</p>
<ul>
<li>Dataset Description: {tbl_desc}</li>
<li>Attribute Descriptions:
{attr1}: {var1_desc}
{attr2}: {var2_desc}
Respond with your predictions in the following format:
{
"coefficient": "<predicted correlation coefficient>",
"standard deviation": "<predicted uncertainty>",
}</li>
</ul>
<h2>G. 3 Generate Contradictory Context</h2>
<h2>Counterfactual Context Generation Prompt</h2>
<h2>Task:</h2>
<p>You are given two attributes and the expected correlation between them from a tabular dataset. Your task is to invent a hypothetical context that flips the expected relationship between these attributes.</p>
<p>For example, on Earth, income and education are positively correlated; in an alternate world where education makes people less capable, income and education would be negatively correlated.</p>
<p>Please provide your new context in 2-3 concise sentences, avoiding any explicit mention of the correlation.</p>
<h1>Now, solve the following:</h1>
<h2>Attributes:</h2>
<ul>
<li>{attr1}</li>
<li>{attr2}</li>
</ul>
<p>Source Table: {table}</p>
<h2>Descriptions:</h2>
<ul>
<li>Dataset Description: {tbl_desc}</li>
<li>Attribute Descriptions:
{attr1}: {var1_desc}
{attr2}: {var2_desc}
Expected Correlation: {r_obs}</li>
</ul>
<h2>Respond in JSON:</h2>
<p>{
"new_context": ""
}</p>
<h2>G. 4 Correlation Prediction with Hypothetical context</h2>
<h2>Correlation Prediction with Hypothetical context</h2>
<p>Task: Given two attributes from a tabular dataset and a hypothetical context (which may differ from Earth), predict the Pearson correlation coefficient between them.</p>
<h2>Guidelines:</h2>
<ul>
<li>Use the scenario described under Context to inform your reasoning.</li>
<li>Return a single floating-point value in the range $[-1,1]$.</li>
</ul>
<p>Now, solve the following:</p>
<h2>Context:</h2>
<p>{context}</p>
<h2>Attributes:</h2>
<ul>
<li>{attr1}</li>
<li>{attr2}</li>
</ul>
<p>Source Table: {table}</p>
<h2>Descriptions:</h2>
<ul>
<li>Dataset Description: {tbl_desc}</li>
<li>Attribute Descriptions:
{attr1}: {var1_desc}
{attr2}: {var2_desc}
Format your answer as:</li>
</ul>
<div class="codehilite"><pre><span></span><code>{
    &quot;coefficient&quot;: &quot;&lt;predicted correlation coefficient&gt;&quot;,
    &quot;explanation&quot;: &quot;&lt;explanation of the prediction&gt;&quot;
}
</code></pre></div>

<h1>G. 5 Column Semantics Quality Assessment</h1>
<h2>Column Semantics Quality Assessment Prompt</h2>
<p>You are given a column name and the context in which it appears. Your task is to judge whether the column name clearly and accurately conveys its meaning.</p>
<h2>Column Name: {col_name }</h2>
<p>Dataset Name: {dataset_name}</p>
<h2>Dataset Description: {dataset_desc }</h2>
<p>Please respond in JSON using exactly this format:
{
"valid": "<yes or no>"
}</p>            </div>
        </div>

    </div>
</body>
</html>
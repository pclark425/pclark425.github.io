<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7206 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7206</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7206</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269761536</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.08011v3.pdf" target="_blank">A Survey of Large Language Models for Graphs</a></p>
                <p><strong>Paper Abstract:</strong> Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at \url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7206.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7206.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WalkLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WalkLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforms graphs into textual sequences by emitting random-walk traces as token sequences which are then used to fine-tune language models for attributed graph embedding; captures both attribute semantics and local structure via walk trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Walklm: A uniform language model fine-tuning framework for attributed graph embedding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Random-walk textual sequences</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph structure as sequences of node (and optionally attribute) tokens produced by sampling random walks on the graph; each walk produces a linear token sequence representing a local path (node IDs, attributes, or short textual descriptions can be placed as tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>random walk sampling (walks of fixed/variable length), emitting node/attribute tokens in walk order</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>attributed graph embedding / node classification</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WalkLM (language model fine-tuned on walk sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language model fine-tuned on sequences derived from graph random walks to produce graph-aware textual embeddings (framework described in WalkLM paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct LM fine-tuning using graph-derived sequences to produce node/graph embeddings and capture local structural+attribute patterns; reported as a uniform framework for attributed graph embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy — random walks do not capture full global structure; ordering and sampling variance affect determinism; can generate long sequences increasing token cost and can miss rare/global patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Offers a simple, model‑agnostic sequential encoding compared to GNN encoders; more amenable to direct LM fine-tuning but weaker at preserving full graph/global structure than explicit graph encoders or pooling approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7206.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7206.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaGA (Large Language and Graph Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reformats graph data with node-level templates into ordered textual sequences that map directly into LM token embeddings so LLMs can process graph-structured inputs without a separate GNN encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLaGA: Large Language and Graph Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node-level template sequences</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is linearized by applying node-level templates that convert node attributes, node identifiers, and local relations into a structured textual template per node; templates are concatenated in a particular order to form the input sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>node-by-node serialization via node templates (node attributes + relation descriptions), fixed/heuristic ordering of nodes</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general graph tasks (node classification, reasoning) processed by LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaGA (LM adapted to graph templates)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A combined language-and-graph assistant architecture that relies on node-template linearizations to let an LLM handle graph data; aims for versatility, generalizability, and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct LM processing of graph inputs via template serialization, facilitating instruction/fine-tuning on graph tasks without a dedicated GNN preprocessing stage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Highly dependent on ordering/canonicalization of nodes and template design; can be lossy for global structure and suffers token-budget limits for large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to GNN-as-prefix or fusion approaches, template sequences remove the graph encoder but risk losing structural fidelity; provides interpretability and simplicity at cost of structural completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7206.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7206.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Converts graphs into natural-language sequences by deriving a graph-syntax tree and verbalizing graph structure into text which an LLM then processes for reasoning and interactive queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtext: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-syntax tree verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Derives a hierarchical syntax representation of the graph (a graph-syntax tree) and linearizes it into natural language sentences/phrases that describe nodes, relations, and substructures; the resulting text is fed to an LLM for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical → sequential (token‑based), lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>graph → graph-syntax tree → linearized natural language description (verbalization of nodes/edges/substructures)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph reasoning / training‑free graph reasoning / interactive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraphText (verbalization + LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline that produces natural-language descriptions of graphs via a graph-syntax tree, enabling off-the-shelf LLMs to perform reasoning without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides a training-free pathway to use pretrained LLMs for graph reasoning by converting graph structure into language; reduces need for model fine-tuning but may require careful prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verbalization can be lossy — nuance of graph topology may be flattened into prose; long graphs produce verbose descriptions that hit token limits; potential for ambiguous or inconsistent verbalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to template/node-token linearizations and walk-based encodings, graph-syntax verbalization focuses on human-readable descriptions and interactive reasoning at the expense of compactness and possibly structural fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7206.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7206.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TalkLikeAGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Talk like a Graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematically studies and evaluates multiple text-based encoding strategies for representing graphs to LLMs, and introduces the GraphQA benchmark to measure their effects on reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Comparative text encodings (various encoder strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A set of alternative graph-to-text encodings (e.g., edge lists, adjacency descriptions, path/chain verbalizations, node templates and other linearizations) are defined and evaluated to determine how encoding choices affect LLM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based (multiple variants), lossy/approximate depending on encoding</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>multiple — edge-list verbalization, path‑based linearization, node-template concatenation, adjacency sentence descriptions (exact encoding varies per strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphQA (benchmark introduced to evaluate encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph reasoning / GraphQA</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs evaluated (unspecified in survey summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off‑the‑shelf LLMs used as evaluators to test different text encodings; the focus is on encoding strategy rather than new LM architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Identifies which textual encodings lead to better LLM reasoning over graphs and provides guidance for prompt/serialization design when using LLMs without graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Findings show encoding choices strongly affect results; no single encoding is optimal for all tasks, and many encodings are constrained by token budget and structural loss.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Serves as a comparative study showing that encoding design matters; provides empirical evidence to prefer certain encodings for particular reasoning tasks over others (details in original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7206.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7206.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphTMI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphTMI / Which Modality should I use — Text, Motif, or Image?: Understanding Graphs with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explores multiple graph-to-LLM modalities (text, motif, image) and provides a benchmark (GraphTMI) demonstrating that image encodings can better balance token limits and preserve salient graph information compared to text encodings and some prior GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text / Motif / Image encodings (multi‑modal)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Provides several conversion strategies: textual verbalization of graph structure, motif-based summarizations (compact motif tokens), and image-based renderings of graphs; each modality produces different types of tokens (text tokens, motif identifiers, or image embeddings) for LLM consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>multi-modal (textual sequential, motif token summaries, image embeddings), lossy depending on modality</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>textual verbalization, motif extraction and motif-token serialization, graph-to-image rendering then multimodal LM input</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphTMI benchmark (introduced/evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph structure analysis / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraphTMI evaluation uses multimodal-capable LLMs (e.g., GPT-4V mentioned in survey context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal LLMs or LLMs augmented with image encoders; image modality leverages visual encodings to feed graph structure into the model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Shows that multimodal encodings (especially image) can preserve structural information more compactly under token limits, improving LLM performance on structure-sensitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Image-based approaches require vision-capable LLMs; text encodings suffer token budget blowup and motif encodings may omit global context; modality choice depends on LM capability and task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Reported that image modality outperforms text and some prior GNN baselines in balancing token limits and preserving essential structural information (no numeric results reported in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7206.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7206.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuseGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuseGraph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses a compact graph description mechanism and diverse instruction generation to create textual representations for LLM instruction-tuning aimed at generic graph mining tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Compact graph description serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serializes graph structure into compact textual descriptions (condensed node/edge summaries and targeted subgraph depictions) combined with generated instructions to create instruction‑tuning pairs for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based, compact/heuristic (lossy)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>compact textual summaries of nodes/edges/subgraphs plus instruction templates; selective subgraph extraction to limit token length</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph mining / instruction-tuning for graph tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuseGraph-tuned LLMs (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs instruction-tuned with compact, graph-oriented textual descriptions and diverse generated instructions to improve graph mining capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables instruction-tuning of LLMs on graph tasks using compact textual encodings, improving general-purpose graph mining without full graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Compacting risks losing structural detail; success depends on quality of instruction generation and selection of subgraphs to describe.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Trades off between verbosity of full verbalization and structural completeness of explicit graph encoders; more scalable for instruction tuning but potentially less precise than GNN-based encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7206.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7206.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZeroG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ZeroG</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applies prompt-based subgraph serialization and lightweight fine-tuning to enable cross-dataset zero-shot transferability for graph tasks by encoding subgraphs and class semantics into textual prompts for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Prompt-based subgraph serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents graphs as textual prompts by serializing selected subgraphs and encoding node attributes/class semantics into prompt templates; used with lightweight fine-tuning to enhance cross-dataset transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, prompt‑based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>subgraph extraction followed by prompt templates that verbalize nodes/edges/attributes and class semantics; lightweight fine-tuning on these prompts</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>cross-dataset zero-shot node classification / transferability</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ZeroG (LLM + prompt-based subgraph fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses LLMs with prompt-serialized subgraphs and lightweight fine-tuning to target zero-shot transfer across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Aims to improve zero-shot cross-dataset transfer by using LLM-derived textual supervision and subgraph prompts; reduces dependence on dataset-specific encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Serialization of subgraphs and prompt design limit how much global context is preserved; effectiveness depends on subgraph selection and prompt quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Addresses cross‑dataset transfer more directly than purely GNN-based models by leveraging LLM semantic priors, but remains constrained by lossy textualization vs full graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7206.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7206.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGraph / InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses structured format verbalization and graph-centric instruction tuning to convert graphs into textual formats suitable for LLM instruction fine-tuning and to reduce hallucination in LLM outputs for graph generation/reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structured format verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts graphs into structured textual formats (e.g., serialized tuples, stepwise reasoning traces, or specially formatted instructions) and uses instruction-tuning to align LLM outputs with graph-structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, structured-token based (lossy depending on granularity)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>structured verbalization templates (tuples/records), instruction generation, and preference alignment during fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphInstruct / Graph-centric benchmarks (as used in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph reasoning, generation, and QA</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGraph-tuned LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs instruction-tuned with structured graph verbalizations and aligned preferences to improve correctness and reduce hallucinations in graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Improves LLMs' graph understanding and reasoning through targeted instruction-tuning on structured verbalizations, leading to better task adherence and reduced hallucination (qualitative claim in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verbalization requires careful design to preserve needed graph semantics; still susceptible to token limits and lossy representation for complex graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>More structured than free-form verbalization (GraphText) and more explicit than generic template concatenation; aims to strike a balance between fidelity and LM-friendliness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7206.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7206.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphWiz / GraphInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphWiz / GraphInstruct (instruction tuning datasets and direct preference optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Creates instruction-tuning data (GraphInstruct) and applies instruction‑tuning techniques (e.g., Direct Preference Optimization) to teach LLMs to solve graph problems via textual prompts and structured graph descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphWiz: An Instruction-Following Language Model for Graph Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Instruction-tuning serializations (GraphInstruct dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Provides serialized graph problems and step-wise reasoning traces in text as training data for LLM instruction tuning; graph instances are verbalized into instruction-response pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, instruction‑paired tokenization, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>graph instance → instruction + verbalized graph description + stepwise solution traces; used to fine-tune LLMs with preference optimization</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphInstruct / GraphWiz datasets (introduced/used by these works)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph reasoning, graph problem solving, instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction‑tuned LLMs (GraphWiz / DPO‑tuned models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs fine-tuned with GraphInstruct-style instruction datasets and optimized via Direct Preference Optimization to improve reasoning clarity and accuracy on graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Leads to clearer and more accurate reasoning chains for graph tasks by aligning LLM behavior to instruction-following preferences; improves LLM reasoning quality in graph problem domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on coverage and diversity of instruction dataset; textual serializations may still be lossy for structural aspects of graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Instruction-tuning provides stronger task alignment than naive prompt-only approaches; compared to GNN-as-prefix, it focuses on teaching LLMs to reason in text rather than integrating structural encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Walklm: A uniform language model fine-tuning framework for attributed graph embedding <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>LLaGA: Large Language and Graph Assistant <em>(Rating: 2)</em></li>
                <li>Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models <em>(Rating: 2)</em></li>
                <li>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining <em>(Rating: 2)</em></li>
                <li>ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs <em>(Rating: 2)</em></li>
                <li>InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment <em>(Rating: 2)</em></li>
                <li>GraphWiz: An Instruction-Following Language Model for Graph Problems <em>(Rating: 1)</em></li>
                <li>GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7206",
    "paper_id": "paper-269761536",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "WalkLM",
            "name_full": "WalkLM",
            "brief_description": "Transforms graphs into textual sequences by emitting random-walk traces as token sequences which are then used to fine-tune language models for attributed graph embedding; captures both attribute semantics and local structure via walk trajectories.",
            "citation_title": "Walklm: A uniform language model fine-tuning framework for attributed graph embedding",
            "mention_or_use": "mention",
            "representation_name": "Random-walk textual sequences",
            "representation_description": "Encodes graph structure as sequences of node (and optionally attribute) tokens produced by sampling random walks on the graph; each walk produces a linear token sequence representing a local path (node IDs, attributes, or short textual descriptions can be placed as tokens).",
            "representation_type": "sequential, token‑based, lossy",
            "encoding_method": "random walk sampling (walks of fixed/variable length), emitting node/attribute tokens in walk order",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "attributed graph embedding / node classification",
            "model_name": "WalkLM (language model fine-tuned on walk sequences)",
            "model_description": "Language model fine-tuned on sequences derived from graph random walks to produce graph-aware textual embeddings (framework described in WalkLM paper).",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Enables direct LM fine-tuning using graph-derived sequences to produce node/graph embeddings and capture local structural+attribute patterns; reported as a uniform framework for attributed graph embedding.",
            "limitations": "Lossy — random walks do not capture full global structure; ordering and sampling variance affect determinism; can generate long sequences increasing token cost and can miss rare/global patterns.",
            "comparison_with_other": "Offers a simple, model‑agnostic sequential encoding compared to GNN encoders; more amenable to direct LM fine-tuning but weaker at preserving full graph/global structure than explicit graph encoders or pooling approaches.",
            "uuid": "e7206.0",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaGA",
            "name_full": "LLaGA (Large Language and Graph Assistant)",
            "brief_description": "Reformats graph data with node-level templates into ordered textual sequences that map directly into LM token embeddings so LLMs can process graph-structured inputs without a separate GNN encoder.",
            "citation_title": "LLaGA: Large Language and Graph Assistant",
            "mention_or_use": "mention",
            "representation_name": "Node-level template sequences",
            "representation_description": "Each graph is linearized by applying node-level templates that convert node attributes, node identifiers, and local relations into a structured textual template per node; templates are concatenated in a particular order to form the input sequence.",
            "representation_type": "sequential, token‑based, lossy",
            "encoding_method": "node-by-node serialization via node templates (node attributes + relation descriptions), fixed/heuristic ordering of nodes",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "general graph tasks (node classification, reasoning) processed by LLMs",
            "model_name": "LLaGA (LM adapted to graph templates)",
            "model_description": "A combined language-and-graph assistant architecture that relies on node-template linearizations to let an LLM handle graph data; aims for versatility, generalizability, and interpretability.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Enables direct LM processing of graph inputs via template serialization, facilitating instruction/fine-tuning on graph tasks without a dedicated GNN preprocessing stage.",
            "limitations": "Highly dependent on ordering/canonicalization of nodes and template design; can be lossy for global structure and suffers token-budget limits for large graphs.",
            "comparison_with_other": "Compared to GNN-as-prefix or fusion approaches, template sequences remove the graph encoder but risk losing structural fidelity; provides interpretability and simplicity at cost of structural completeness.",
            "uuid": "e7206.1",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GraphText",
            "name_full": "GraphText: Graph reasoning in text space",
            "brief_description": "Converts graphs into natural-language sequences by deriving a graph-syntax tree and verbalizing graph structure into text which an LLM then processes for reasoning and interactive queries.",
            "citation_title": "Graphtext: Graph reasoning in text space",
            "mention_or_use": "mention",
            "representation_name": "Graph-syntax tree verbalization",
            "representation_description": "Derives a hierarchical syntax representation of the graph (a graph-syntax tree) and linearizes it into natural language sentences/phrases that describe nodes, relations, and substructures; the resulting text is fed to an LLM for reasoning.",
            "representation_type": "hierarchical → sequential (token‑based), lossy",
            "encoding_method": "graph → graph-syntax tree → linearized natural language description (verbalization of nodes/edges/substructures)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "graph reasoning / training‑free graph reasoning / interactive reasoning",
            "model_name": "GraphText (verbalization + LLM)",
            "model_description": "A pipeline that produces natural-language descriptions of graphs via a graph-syntax tree, enabling off-the-shelf LLMs to perform reasoning without fine-tuning.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Provides a training-free pathway to use pretrained LLMs for graph reasoning by converting graph structure into language; reduces need for model fine-tuning but may require careful prompt engineering.",
            "limitations": "Verbalization can be lossy — nuance of graph topology may be flattened into prose; long graphs produce verbose descriptions that hit token limits; potential for ambiguous or inconsistent verbalizations.",
            "comparison_with_other": "Compared to template/node-token linearizations and walk-based encodings, graph-syntax verbalization focuses on human-readable descriptions and interactive reasoning at the expense of compactness and possibly structural fidelity.",
            "uuid": "e7206.2",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "TalkLikeAGraph",
            "name_full": "Talk like a Graph: Encoding graphs for large language models",
            "brief_description": "Systematically studies and evaluates multiple text-based encoding strategies for representing graphs to LLMs, and introduces the GraphQA benchmark to measure their effects on reasoning.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models",
            "mention_or_use": "mention",
            "representation_name": "Comparative text encodings (various encoder strategies)",
            "representation_description": "A set of alternative graph-to-text encodings (e.g., edge lists, adjacency descriptions, path/chain verbalizations, node templates and other linearizations) are defined and evaluated to determine how encoding choices affect LLM performance.",
            "representation_type": "sequential, token‑based (multiple variants), lossy/approximate depending on encoding",
            "encoding_method": "multiple — edge-list verbalization, path‑based linearization, node-template concatenation, adjacency sentence descriptions (exact encoding varies per strategy)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "GraphQA (benchmark introduced to evaluate encodings)",
            "task_name": "graph reasoning / GraphQA",
            "model_name": "Various LLMs evaluated (unspecified in survey summary)",
            "model_description": "Off‑the‑shelf LLMs used as evaluators to test different text encodings; the focus is on encoding strategy rather than new LM architectures.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Identifies which textual encodings lead to better LLM reasoning over graphs and provides guidance for prompt/serialization design when using LLMs without graph encoders.",
            "limitations": "Findings show encoding choices strongly affect results; no single encoding is optimal for all tasks, and many encodings are constrained by token budget and structural loss.",
            "comparison_with_other": "Serves as a comparative study showing that encoding design matters; provides empirical evidence to prefer certain encodings for particular reasoning tasks over others (details in original paper).",
            "uuid": "e7206.3",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GraphTMI",
            "name_full": "GraphTMI / Which Modality should I use — Text, Motif, or Image?: Understanding Graphs with Large Language Models",
            "brief_description": "Explores multiple graph-to-LLM modalities (text, motif, image) and provides a benchmark (GraphTMI) demonstrating that image encodings can better balance token limits and preserve salient graph information compared to text encodings and some prior GNNs.",
            "citation_title": "Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models",
            "mention_or_use": "mention",
            "representation_name": "Text / Motif / Image encodings (multi‑modal)",
            "representation_description": "Provides several conversion strategies: textual verbalization of graph structure, motif-based summarizations (compact motif tokens), and image-based renderings of graphs; each modality produces different types of tokens (text tokens, motif identifiers, or image embeddings) for LLM consumption.",
            "representation_type": "multi-modal (textual sequential, motif token summaries, image embeddings), lossy depending on modality",
            "encoding_method": "textual verbalization, motif extraction and motif-token serialization, graph-to-image rendering then multimodal LM input",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "GraphTMI benchmark (introduced/evaluated)",
            "task_name": "graph structure analysis / reasoning",
            "model_name": "GraphTMI evaluation uses multimodal-capable LLMs (e.g., GPT-4V mentioned in survey context)",
            "model_description": "Multimodal LLMs or LLMs augmented with image encoders; image modality leverages visual encodings to feed graph structure into the model.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Shows that multimodal encodings (especially image) can preserve structural information more compactly under token limits, improving LLM performance on structure-sensitive tasks.",
            "limitations": "Image-based approaches require vision-capable LLMs; text encodings suffer token budget blowup and motif encodings may omit global context; modality choice depends on LM capability and task.",
            "comparison_with_other": "Reported that image modality outperforms text and some prior GNN baselines in balancing token limits and preserving essential structural information (no numeric results reported in survey).",
            "uuid": "e7206.4",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MuseGraph",
            "name_full": "MuseGraph",
            "brief_description": "Uses a compact graph description mechanism and diverse instruction generation to create textual representations for LLM instruction-tuning aimed at generic graph mining tasks.",
            "citation_title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining",
            "mention_or_use": "mention",
            "representation_name": "Compact graph description serialization",
            "representation_description": "Serializes graph structure into compact textual descriptions (condensed node/edge summaries and targeted subgraph depictions) combined with generated instructions to create instruction‑tuning pairs for LLMs.",
            "representation_type": "sequential, token‑based, compact/heuristic (lossy)",
            "encoding_method": "compact textual summaries of nodes/edges/subgraphs plus instruction templates; selective subgraph extraction to limit token length",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "graph mining / instruction-tuning for graph tasks",
            "model_name": "MuseGraph-tuned LLMs (instruction-tuned)",
            "model_description": "LLMs instruction-tuned with compact, graph-oriented textual descriptions and diverse generated instructions to improve graph mining capabilities.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Enables instruction-tuning of LLMs on graph tasks using compact textual encodings, improving general-purpose graph mining without full graph encoders.",
            "limitations": "Compacting risks losing structural detail; success depends on quality of instruction generation and selection of subgraphs to describe.",
            "comparison_with_other": "Trades off between verbosity of full verbalization and structural completeness of explicit graph encoders; more scalable for instruction tuning but potentially less precise than GNN-based encodings.",
            "uuid": "e7206.5",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ZeroG",
            "name_full": "ZeroG",
            "brief_description": "Applies prompt-based subgraph serialization and lightweight fine-tuning to enable cross-dataset zero-shot transferability for graph tasks by encoding subgraphs and class semantics into textual prompts for LLMs.",
            "citation_title": "ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs",
            "mention_or_use": "mention",
            "representation_name": "Prompt-based subgraph serialization",
            "representation_description": "Represents graphs as textual prompts by serializing selected subgraphs and encoding node attributes/class semantics into prompt templates; used with lightweight fine-tuning to enhance cross-dataset transfer.",
            "representation_type": "sequential, prompt‑based, lossy",
            "encoding_method": "subgraph extraction followed by prompt templates that verbalize nodes/edges/attributes and class semantics; lightweight fine-tuning on these prompts",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "cross-dataset zero-shot node classification / transferability",
            "model_name": "ZeroG (LLM + prompt-based subgraph fine-tuning)",
            "model_description": "Uses LLMs with prompt-serialized subgraphs and lightweight fine-tuning to target zero-shot transfer across datasets.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Aims to improve zero-shot cross-dataset transfer by using LLM-derived textual supervision and subgraph prompts; reduces dependence on dataset-specific encoders.",
            "limitations": "Serialization of subgraphs and prompt design limit how much global context is preserved; effectiveness depends on subgraph selection and prompt quality.",
            "comparison_with_other": "Addresses cross‑dataset transfer more directly than purely GNN-based models by leveraging LLM semantic priors, but remains constrained by lossy textualization vs full graph encoders.",
            "uuid": "e7206.6",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "InstructGraph",
            "name_full": "InstructGraph / InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment",
            "brief_description": "Uses structured format verbalization and graph-centric instruction tuning to convert graphs into textual formats suitable for LLM instruction fine-tuning and to reduce hallucination in LLM outputs for graph generation/reasoning.",
            "citation_title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment",
            "mention_or_use": "mention",
            "representation_name": "Structured format verbalization",
            "representation_description": "Converts graphs into structured textual formats (e.g., serialized tuples, stepwise reasoning traces, or specially formatted instructions) and uses instruction-tuning to align LLM outputs with graph-structured tasks.",
            "representation_type": "sequential, structured-token based (lossy depending on granularity)",
            "encoding_method": "structured verbalization templates (tuples/records), instruction generation, and preference alignment during fine-tuning",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "GraphInstruct / Graph-centric benchmarks (as used in related work)",
            "task_name": "graph reasoning, generation, and QA",
            "model_name": "InstructGraph-tuned LLMs",
            "model_description": "LLMs instruction-tuned with structured graph verbalizations and aligned preferences to improve correctness and reduce hallucinations in graph tasks.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Improves LLMs' graph understanding and reasoning through targeted instruction-tuning on structured verbalizations, leading to better task adherence and reduced hallucination (qualitative claim in survey).",
            "limitations": "Verbalization requires careful design to preserve needed graph semantics; still susceptible to token limits and lossy representation for complex graphs.",
            "comparison_with_other": "More structured than free-form verbalization (GraphText) and more explicit than generic template concatenation; aims to strike a balance between fidelity and LM-friendliness.",
            "uuid": "e7206.7",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GraphWiz / GraphInstruct",
            "name_full": "GraphWiz / GraphInstruct (instruction tuning datasets and direct preference optimization)",
            "brief_description": "Creates instruction-tuning data (GraphInstruct) and applies instruction‑tuning techniques (e.g., Direct Preference Optimization) to teach LLMs to solve graph problems via textual prompts and structured graph descriptions.",
            "citation_title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
            "mention_or_use": "mention",
            "representation_name": "Instruction-tuning serializations (GraphInstruct dataset)",
            "representation_description": "Provides serialized graph problems and step-wise reasoning traces in text as training data for LLM instruction tuning; graph instances are verbalized into instruction-response pairs.",
            "representation_type": "sequential, instruction‑paired tokenization, lossy",
            "encoding_method": "graph instance → instruction + verbalized graph description + stepwise solution traces; used to fine-tune LLMs with preference optimization",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "GraphInstruct / GraphWiz datasets (introduced/used by these works)",
            "task_name": "graph reasoning, graph problem solving, instruction-following",
            "model_name": "Instruction‑tuned LLMs (GraphWiz / DPO‑tuned models)",
            "model_description": "LLMs fine-tuned with GraphInstruct-style instruction datasets and optimized via Direct Preference Optimization to improve reasoning clarity and accuracy on graph tasks.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Leads to clearer and more accurate reasoning chains for graph tasks by aligning LLM behavior to instruction-following preferences; improves LLM reasoning quality in graph problem domains.",
            "limitations": "Quality depends on coverage and diversity of instruction dataset; textual serializations may still be lossy for structural aspects of graphs.",
            "comparison_with_other": "Instruction-tuning provides stronger task alignment than naive prompt-only approaches; compared to GNN-as-prefix, it focuses on teaching LLMs to reason in text rather than integrating structural encoders.",
            "uuid": "e7206.8",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Walklm: A uniform language model fine-tuning framework for attributed graph embedding",
            "rating": 2
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 2
        },
        {
            "paper_title": "LLaGA: Large Language and Graph Assistant",
            "rating": 2
        },
        {
            "paper_title": "Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining",
            "rating": 2
        },
        {
            "paper_title": "ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs",
            "rating": 2
        },
        {
            "paper_title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment",
            "rating": 2
        },
        {
            "paper_title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
            "rating": 1
        },
        {
            "paper_title": "GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability",
            "rating": 1
        }
    ],
    "cost": 0.017997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Large Language Models for Graphs
11 Sep 2024</p>
<p>Xubin Ren xubinrencs@gmail.com 
Jiabin Tang jiabintang77@gmail.com 
Dawei Yin yindawei@acm.org 
Nitesh Chawla nchawla@nd.edu 
Chao Huang chaohuang75@gmail.com </p>
<p>University of Hong Kong Hong Kong
China</p>
<p>University of Hong Kong Hong Kong
China</p>
<p>Baidu Inc
BeijingChina</p>
<p>University of Notre Dame Indiana
USA</p>
<p>University of Hong Kong Hong Kong
China</p>
<p>11 pagesBarcelonaSpain</p>
<p>A Survey of Large Language Models for Graphs
11 Sep 20248DA5988C13EF3CB6AF206EC40AF035F310.1145/3637528.3671460arXiv:2405.08011v3[cs.LG]Large Language Models, Graph Learning
Graphs are an essential data structure utilized to represent relationships in real-world scenarios.Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification.Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist.Recently, Large Language Models (LLMs) have gained attention in natural language processing.They excel in language comprehension and summarization.Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks.In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design.We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category.We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas.This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field.We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-LLM4Graph-Papers.</p>
<p>INTRODUCTION</p>
<p>Graphs, comprising nodes and edges that signify relationships, are essential for illustrating real-world connections across various domains.These include social networks [39,52], molecular graphs [4], recommender systems [23,59], and academic networks [27].This structured data form is integral in mapping complex interconnections relevant to a wide range of applications.</p>
<p>In recent years, Graph Neural Networks (GNNs) [79] have emerged as a powerful tool for a variety of tasks, including node classification [82] and link prediction [89].By passing and aggregating information across nodes and iteratively refining node features through supervised learning, GNNs have achieved remarkable results in capturing structural nuances and enhancing model accuracy.To accomplish this, GNNs leverage graph labels to guide the learning process.Several notable models have been proposed in the literature, each with its own strengths and contributions.For instance, Graph Convolutional Networks (GCNs) [34] have been shown to be effective in propagating embeddings across nodes, while Graph Attention Networks (GATs) [67] leverage attention mechanisms to perform precise aggregation of node features.Additionally, Graph Transformers [35,86] employ self-attention and positional encoding to capture global signals among the graph, further improving the expressiveness of GNNs.To address scalability challenges in large graphs, methods such as Nodeformer [77] and DIFFormer [76] have been proposed.These approaches employ efficient attention mechanisms and differentiable pooling techniques to reduce computational complexity while maintaining high levels of accuracy.Despite these advancements, current GNN methodologies still face several challenges.For example, data sparsity remains a significant issue, particularly in scenarios where the graph structure is incomplete or noisy [85].Moreover, the generalization ability of GNNs to new graphs or unseen nodes remains an open research question, with recent works highlighting the need for more robust and adaptive models [17,80,93].</p>
<p>Large Language Models (LLMs) [96], which show great generalization abilities for unseen tasks [12,56,72], have emerged as powerful tools in various research fields, including natural language processing [1], computer vision [43,44], and information retrieval [26,41,99].The advent of LLMs has sparked significant interest within the graph learning community [29,33,37], prompting investigations into the potential of LLMs to enhance performance on graph-related tasks.Researchers have explored various approaches to leverage the strengths of LLMs for graph learning, resulting in a new wave of methods that combine the power of LLMs with graph neural networks.One promising direction is to develop prompts that enable LLMs to understand graph structures and respond to queries effectively.For instance, approaches such as InstructGLM [84] and NLGraph [68] have designed specialized prompts that allow LLMs to reason over graph data and generate accurate responses.Alternatively, other methods have integrated GNNs to feed tokens into the LLMs, allowing them to understand graph structures more directly.For example, GraphGPT [63] and GraphLLM [5] use GNNs to encode graph data into tokens, which are then fed into the LLMs for further processing.This synergy between LLMs and GNNs has not only improved task performance but also demonstrated impressive zero-shot generalization capabilities, where the models can accurately answer queries about unseen graphs or nodes.</p>
<p>In this survey, we offer a systematic review of the advancements in Large Language Models (LLMs) for graph applications, and we explore potential avenues for future research.Unlike prior surveys that categorize studies based on the role of LLMs [33,37] or focus primarily on integrating LLMs with knowledge graphs [53], our work highlights the model framework design, particularly the inference and training processes, to distinguish between existing taxonomies.This perspective allows readers to gain a deeper understanding of how LLMs effectively address graph-related challenges.We identify and discuss four distinct architectural approaches: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, each illustrated with representative examples.In summary, the contributions of our work can be summarized as:</p>
<p>• Comprehensive Review of LLMs for Graph Learning.We offer a comprehensive review of the current state-of-the-art Large Language Models (LLMs) for graph learning, elucidating their strengths and pinpointing their limitations.</p>
<p>PRELIMINARIES AND TAXONOMY</p>
<p>In this section, we first provide essential background knowledge on large language models and graph learning.Then, we present our taxonomy of large language models for graphs.</p>
<p>Definitions</p>
<p>Graph-Structured Data.In computer science, a graph G = (V, E) is a non-linear data structure that consists of a set of nodes V, and a set of edges E connecting these nodes.Each edge  ∈ E is associated with a pair of nodes (, ), where  and  are the endpoints of the edge.The edge may be directed, meaning it has a orientation from  to , or undirected, meaning it has no orientation.Furthermore, A Text-Attributed Graph (TAG) is a graph that assigns a sequential text feature (i.e., sentence) to each node, denoted as t  , which is widely used in the era of large language models.The text-attributed graph can be formally represented as G  = (V, E, T ), where T is the set of text features.</p>
<p>Graph Neural Networks (GNNs) are deep learning architectures for graph-structured data that aggregate information from neighboring nodes to update node embeddings.Formally, the update of a node embedding h  ∈ R  in each GNN layer can be represented as:
h (𝑙+1) 𝑣 = 𝜓 (𝜙 ({h (𝑙 ) 𝑣 ′ : 𝑣 ′ ∈ N (𝑣)}), h (𝑙 ) 𝑣 ),(1)
where  ′ ∈ N () denotes a neighbor node of , and  (•) and  (•) are aggregation and update functions, respectively.By stacking  GNN layers, the final node embeddings can be used for downstream graph-related tasks such as node classification and link prediction.</p>
<p>Large Language Models (LLMs).Language Models (LMs) is a statistical model that estimate the probability distribution of words for a given sentence.Recent research has shown that LMs with billions of parameters exhibit superior performance in solving a wide range of natural language tasks (e.g., translation, summarization and instruction following), making them Large Language Models (LLMs).In general, most recent LLMs are built with transformer blocks that use a query-key-value (QKV)-based attention mechanism to aggregate information in the sequence of tokens.Based on the direction of attention, LLMs can be categorized into two types (given a sequence of tokens x = [ 0 ,  1 , ...,   ]):</p>
<p>• Masked Language Modeling (MLM).Masked Language Modeling is a popular pre-training objective for LLMs that involves masking out certain tokens in a sequence and training the model to predict the masked tokens based on the surrounding context.Specifically, the model takes into account both the left and right context of the masked token to make accurate predictions:
𝑝 (𝑥 𝑖 |𝑥 0 , 𝑥 1 , ..., 𝑥 𝑛 ).(2)
Representative models include BERT [12] and RoBERTa [47].• Causal Language Modeling (CLM).Causal Language Modeling is another popular training objective for LLMs that involves predicting the next token in a sequence based on the previous tokens.Specifically, the model only considers the left context of the current token to make accurate predictions:
𝑝 (𝑥 𝑖 |𝑥 0 , 𝑥 1 , ..., 𝑥 𝑖 −1 )(3)
Notable examples include the GPT (e.g., ChatGPT) and Llama [66].</p>
<p>Taxonomy</p>
<p>In this survey, we present our taxonomy focusing on the model inference pipeline that processes both graph data and text with LLMs.Specifically, we summarize four main types of model architecture design for large language models for graphs, as follows:</p>
<p>Large Language Models for Graphs</p>
<p>GNNs as Prefix</p>
<p>Node-level Tokenization GprahGPT [63], HiGPT [64], GraphTranslator [88], UniGraph [25], GIMLET [92], XRec [51] Graph-level Tokenization GraphLLM [5], GIT-Mol [45], MolCA [48], InstructMol [4], G-Retriever [24], GNP [65] LLMs as Prefix Embs.from LLMs for GNNs G-Prompt [30], SimTeG [14], GALM [81], OFA [42], TAPE [22], LLMRec [73] Labels from LLMs for GNNs OpenGraph [80], LLM-GNN [9], GraphEdit [21], RLMRec [58] LLMs-Graphs Integration Alignment between GNNs and LLMs MoMu [60], ConGraT [3], G2P2 [74], GRENADE [36], MoleculeSTM [46], THLM [100], GLEM [94] Fusion Training of GNNs and LLMs GreaseLM [90], DGTL [54], ENGINE [98], GraphAdapter [31] LLMs Agent for Graphs Pangu [19], Graph Agent [71], FUXI [18], Readi [10], RoG [49] LLMs-Only Tuning-free NLGraph [68], GPT4Graph [20], Beyond Text [28], Graph-LLM [8], GraphText [95], Talk like a Graph [15], LLM4DyG [91], GraphTMI [11], Ai et al. [2] Tuning-required InstructGLM [84], WalkLM [62], LLaGA [7], InstructGraph [69], ZeroG [38], GraphWiz [6], GraphInstruct [50], MuseGraph [61] Figure 1: The proposed taxonomy of Large Language Models (LLMs) for graphs, featuring representative works.</p>
<p>• GNNs as Prefix.GNNs serve as the first component to process graph data and provide structure-aware tokens (e.g., node-level, edge-level, or graph-level tokens) for LLMs for inference.• LLMs as Prefix.LLMs first process graph data with textual information and then provide node embeddings or generated labels for improved training of graph neural networks.• LLMs-Graphs Integration.In this line, LLMs achieve a higher level of integration with graph data, such as fusion training or alignment with GNNs, and also build LLM-based agents to interact with graph information.• LLMs-Only.This line designs practical prompting methods to ground graph-structured data into sequences of words for LLMs to infer, while some also incorporate multi-modal tokens.</p>
<p>LARGE LANGUAGE MODELS FOR GRAPHS 3.1 GNNs as Prefix</p>
<p>In this section, we discuss the application of graph neural networks (GNNs) as structural encoders to enhance the understanding of graph structures by LLMs, thereby benefiting various downstream tasks, i.e., GNNs as Prefix.In these methods, GNNs generally play the role of a tokenizer, encoding graph data into a graph token sequence rich in structural information, which is then input into LLMs to align with natural language.These methods can generally be divided into two categories: i) Node-level Tokenization: each node of the graph structure is input into the LLM, aiming to make the LLM understand fine-grained node-level structural information and distinguish relationships.ii) Graph-level Tokenization: the graph is compressed into a fixed-length token sequence using a specific pooling method, aiming to capture high-level global semantic information of the graph structure.</p>
<p>3.1.1Node-level Tokenization.For some downstream tasks in graph learning, such as node classification and link prediction, the model needs to model the fine-grained structural information at node level, and distinguish the semantic differences between different nodes.Traditional GNNs usually encode a unique representation for each node based on the information of neighboring nodes, and directly perform downstream node classification or link prediction.In this line, the node-level tokenization method is utilized, which can retain the unique structural representation of each node as much as possible, thereby benefiting downstream tasks.Within this line, GraphGPT [63] proposes to initially align the graph encoder with natural language semantics through textgraph grounding, and then combine the trained graph encoder with the LLM using a projector.Through the two-stage instruction tuning paradigm, the model can directly complete various graph learning downstream tasks with natural language, thus perform strong zero-shot transferability and multi-task compatibility.The proposed Chain-of-Thought distillation method empowers GraphGPT to migrate to complex tasks with small parameter sizes.Then, HiGPT [64] proposes to combine the language-enhanced in-context heterogeneous graph tokenizer with LLMs, solving the challenge of relation type heterogeneity shift between different heterogeneous graphs.Meanwhile, the two-stage heterogeneous graph instruction-tuning injects both homogeneity and heterogeneity awareness into the LLM.And the Mixture-of-Thought (MoT) method combined with various prompt engineering further solves the common data scarcity problem in heterogeneous graph learning.GIMLET [92], as a unified graph-text model, leverages natural language instructions to address the label insufficiency challenge in molecule-related tasks, effectively alleviating the reliance on expensive lab experiments for data annotation.It employs a generalized position embedding and attention mechanism to encode both graph structures and textual instructions as a unified token combination that is fed into a transformer decoder.GraphTranslator [88] proposes the use of a translator with shared self-attention to align both the target node and instruction, and employs cross attention to map the node representation encoded by the graph model to fixed-length semantic tokens.The proposed daul-phase training paradigm empowers the LLM to make predictions based on language instructions, providing a unified solution for both pre-defined and open-ended graph-based tasks.Instead of using pre-computed node features of varying dimensions, UniGraph [25] leverages Text-Attributed Graphs for unifying node representations, featuring a cascaded architecture of language models and graph</p>
<p>GNNs Graphs</p>
<p>What is this node?neural networks as backbone networks.In recent research on recommendation systems, XRec [51] has been proposed as a method that utilizes the encoded user/item embeddings from graph neural networks as collaborative signals.These signals are then integrated into each layer of large language models, enabling the generation of explanations for recommendations, even in zero-shot scenarios.</p>
<p>3.1.2</p>
<p>Graph-level Tokenization.On the other hand, to adapt to other graph-level tasks, models need to be able to extract global information from node representations, to obtain high-level graph semantic tokens.In the method of GNN as Prefix, Graph-level tokenization abstracts node representations into unified graph representations through various "pooling" operations, further enhancing various downstream tasks.</p>
<p>Within this domain, GraphLLM [5] utilizes a graph transformer that incorporates the learnable query and positional encoding to encode the graph structure and obtain graph representations through pooling.These representations are directly used as graph-enhanced prefix for prefix tuning in the LLM, demonstrating remarkable effectiveness in fundamental graph reasoning tasks.MolCA [48] with Cross-Modal Projector and Uni-Modal Adapter is a method that enables a language model to understand both text-and graph-based molecular contents through the proposed dual-stage pre-training and fine-tuning stage.It employs a cross-modal projector implemented as a Q-Former to connect a graph encoder's representation space and a language model's text space, and a uni-modal adapter for efficient adaptation to downstream tasks.InstructMol [4] introduces a projector that aligns the molecular graph encoded by the graph encoder with the molecule's Sequential information and natural language instructions, with the first stage of Alignment Pretraining and the second stage of Task-specific Instruction Tuning enabling the model to achieve excellent performance in various drug discovery-related molecular tasks.GIT-Mol [45] further unifies the graph, text, and image modalities through interaction cross-attention between different modality encoders, and aligns these three modalities, enabling the model to simultaneously perform four downstream tasks: captioning, generation, recognition, and prediction.GNP [65] employs cross-modality pooling to integrate the node representations encoded by the graph encoder with the natural language tokens, resulting in a unified graph representation.This representation is aligned with the instruction through the LLM to demonstrate superiority in commonsense and biomedical reasoning tasks.Recently, G-Retriever [24] utilizes retrieval-augmented techniques to obtain subgraph structures.It completes various downstream tasks in GraphQA (Graph Question Answering) through the collaboration of graph encoder and LLMs.</p>
<p>3.1.3Discussion.The GNN as Prefix approach aligns the modeling capability of GNNs with the semantic modeling capability of LLMs, demonstrating unprecedented generalization, i.e., zeroshot capability, in various graph learning downstream tasks and real-world applications.However, despite the effectiveness of the aforementioned approach, the challenge lies in whether the GNN as Prefix method remains effective for non-text-attributed graphs.Additionally, the optimal coordination between the architecture and training of GNNs and LLMs remains an unresolved question.</p>
<p>LLMs as Prefix</p>
<p>The methods presented in this section leverage the information produced by large language models to improve the training of graph neural networks.This information includes textual content, labels, or embeddings derived from the large language models.These techniques can be categorized into two distinct groups: i) Embeddings from LLMs for GNNs, which involves using embeddings generated by large language models for graph neural networks, and ii) Labels from LLMs for GNNs, which involves integrating labels generated by large language models for graph neural networks.</p>
<p>3.2.1</p>
<p>Embeddings from LLMs for GNNs.The inference process of graph neural networks involves passing node embeddings through the edges and then aggregating them to obtain the nextlayer node embeddings.In this process, the initial node embeddings are diverse across different domains.For instance, ID-based embeddings in recommendation systems or bag-of-words embeddings in citation networks can be unclear and non-diverse.Sometimes, the poor quality of initial node embeddings can result in suboptimal performance of GNNs.Furthermore, the lack of a universal design for node embedders makes it challenging to address the generalization capability of GNNs in unseen tasks with different node sets.Fortunately, the works in this line leverage the powerful language summarization and modeling capabilities of LLMs to generate meaningful and effective embeddings for GNNs' training.</p>
<p>In this domain, G-Prompt [30] adds a GNN layer at the end of a pre-trained language models (PLMs) to achieve graph-aware fillmasking self-supervised learning.By doing so, G-Prompt can generate task-specific, explainable node embeddings for downstream tasks using prompt tuning.SimTeG [14] first leverages parameterefficient fine-tuning on the text embeddings obtained by LLMs for downstream tasks (e.g., node classification).Then, the node embeddings are fed into GNNs for inference.Similarly, GALM [81] utilizes BERT as a pre-language model to encode text embeddings for each node.Then, the model is pre-trained through unsupervised learning tasks, such as link prediction, to minimize empirical loss and find optimal model parameters, which enables GALM to be applied for various downstream tasks.Recently, OFA [42] leverages LLMs to unify graph data from different domains into a common embedding space for cross-domain learning.It also uses LLMs to encode taskrelevant text descriptions for constructing prompt graphs, allowing the model to perform specific tasks based on context.TAPE [22] uses customized prompts to query LLMs, generating both prediction and text explanation for each node.Then, DeBERTa is fine-tuned to convert the text explanations into node embeddings for GNNs.Finally, GNNs can use a combination of the original text features, explanation features, and prediction features to predict node labels.In the field of recommendation, LLMRec [73] achieves graph augmentation on user-item interaction data using GPT-3.5, which not only filters out noise interactions and adds meaningful training data, but also enriches the initial node embeddings for users and items with generated rich textual profiles, ultimately improving the performance of recommenders.</p>
<p>3.2.2Labels from LLMs for GNNs.Another approach leverages the generated labels from large language models as supervision to improve the training of graph neural networks.Notably, the supervised labels in this context are not limited to categorized labels in classification tasks, but can take various forms such as embeddings, graphs, and more.The generated information from the LLMs is not used as input to the GNNs, but rather forms the supervision signals for better optimization, which enables GNNs to achieve higher performance on various graph-related tasks.Follow this line, OpenGraph [80] employs LLMs to generate nodes and edges, mitigating the issue of sparse training data.The generation process for nodes and edges is refined using the Gibbs sampling algorithm and a tree-of-prompt strategy, which is then utilized to train the graph foundation model.LLM-GNN [9] leverages LLMs as annotators to generate node category predictions with confidence scores, which serve as labels.Post-filtering is then employed to filter out low-quality annotations while maintaining label diversity.Finally, the generated labels are used to train GNNs.GraphEdit [21] leverages the LLMs to build an edge predictor, which is used to evaluate and refine candidate edges against the original graph's edges.In recommender systems, RLMRec [58] leverages LLMs to generate text descriptions of user/item preferences.These descriptions are then encoded as semantic embeddings to guide the representation learning of ID-based recommenders using contrastive and generative learning techniques [57].</p>
<p>3.2.3</p>
<p>Discussion.Despite the progress made by the aforementioned methods in enhancing graph learning performance, a limitation persists in their decoupled nature, where LLMs are not co-trained with GNNs, resulting in a two-stage learning process.This decoupling is often due to computational resource limitations arising from the large size of the graph or the extensive parameters of LLMs.Consequently, the performance of the GNNs is heavily dependent on the pre-generated embeddings/labels of LLMs or even the design of task-specific prompts.</p>
<p>LLMs-Graphs Integration</p>
<p>The methods introduced in this section aim to further integrate large language models with graph data, encompassing various methodologies that enhance not only the ability of LLMs to tackle graph tasks but also the parameter learning of GNNs.These works can be categorized into three types: i) Fusion Training of GNNs and LLMs, which aims to achieve fusion-co-training of the parameters of both models; ii) Alignment between GNNs and LLMs, which focuses on achieving representation or task alignment between the two models; and iii) LLMs Agent for Graphs, which builds an autonomous agent based on LLMs to plan and solve graph tasks.</p>
<p>Alignment between GNNs and LLMs.</p>
<p>In general, GNNs and LLMs are designed to handle different modalities of data, with GNNs focusing on structural data and LLMs focusing on textual data.This results in different feature spaces for the two models.To address this issue and make both modalities of data more beneficial for the learning of both GNNs and LLMs, several methods use techniques such as contrastive learning or Expectation-Maximization (EM) iterative training to align the feature spaces of the two models.This enables better modeling of both graph and text information, resulting in improved performance on various tasks.</p>
<p>Within this topic, MoMu [60] is a multimodal molecular foundation model that includes two separate encoders, one for handling molecular graphs (GIN) and another for handling text data (BERT).It uses contrastive learning to pre-train the model on a dataset of molecular graph-text pairs.This approach enables MoMu to directly imagine new molecules from textual descriptions.Also in the bioinfo domain, MoleculeSTM [46] combines the chemical structure information of molecules (i.e., molecular graph) with their textual descriptions (i.e., SMILES strings), and uses a contrastive learning to jointly learn the molecular structure and textual descriptions.It show great performance on multiple benchmark tests, including structure-text retrieval, text-based editing tasks, and molecular property prediction.Similarly, in ConGraT  through a variational expectation-maximization (EM) framework.By iteratively using LMs and GNNs to provide labels for each other in node classification, GLEM aligns their capabilities in graph tasks.</p>
<p>Fusion Training of GNNs and LLMs.</p>
<p>Although alignment between the representations of GNNs and LLMs achieves co-optimization and embedding-level alignment of the two models, they remain separate during inference.To achieve a higher level of integration between LLMs and GNNs, several works have focused on designing a deeper fusion of the architecture of the modules, such as transformer layers in LLMs and graph neural layers in GNNs.Co-training GNNs and LLMs can result in a win-win bi-directional benefit for both modules in graph tasks.</p>
<p>Along this line, GreaseLM [90] integrates transformer layers and GNN layers by designing a specific forward propagation layer that enables bidirectional information passing between LM and GNN through special interaction markers and interaction nodes.This approach allows language context representations to be grounded in structured world knowledge, while subtle linguistic differences (such as negation or modifiers) can affect the representation of the knowledge graph, which enables GreaseLM to achieve high performance on Question-Answering tasks.DGTL [54] proposes disentangled graph learning to leverage GNNs to encode disentangled representations, which are then injected into each transformer layer of the LLMs.This approach enables the LLMs to be aware of the graph structure and leverage the gradient from the LLMs to finetune the GNNs.By doing so, DGTL achieves high performance on both citation network and e-commerce graph tasks.ENGINE [98] adds a lightweight and tunable G-Ladder module to each layer of the LLM, which uses a message-passing mechanism to integrate structural information.This enables the output of each LLM layer (i.e., token-level representations) to be passed to the corresponding G-Ladder, where the node representations are enhanced and then used for downstream tasks such as node classification.More directly, GraphAdapter [31] uses a fusion module (typically a multi-layer perceptrons) to combine the structural representations obtained from GNNs with the contextual hidden states of LLMs (e.g., the encoded node text).This enables the structural information from the GNN adapter to complement the textual information from the LLMs, resulting in a fused representation that can be used for supervision training and prompting for downstream tasks.</p>
<p>LLMs Agent for Graphs. With the powerful capabilities of</p>
<p>LLMs in understanding instructions and self-planning to solve tasks, an emerging research direction is to build autonomous agents based on LLMs to tackle human-given or research-related tasks.Typically, an agent consists of a memory module, a perception module, and an action module to enable a loop of observation, memory recall, and action for solving given tasks.In the graph domain, LLMs-based agents can interact directly with graph data to perform tasks such as node classification and link prediction.</p>
<p>In this field, Pangu [19] pioneered the use of LMs to navigate KGs.In this approach, the agent is designed as a symbolic graph search algorithm, providing a set of potential search paths for the language models to evaluate in response to a given query.The remaining path is then utilized to retrieve the answer.Graph Agent (GA) [71] converts graph data into textual descriptions and generates embedding vectors, which are stored in long-term memory.During inference, GA retrieves similar samples from long-term memory and integrates them into a structured prompt, which is used by LLMs to explain the potential reasons for node classification or edge connection.FUXI [18] framework integrates customized tools and the ReAct [83] algorithm to enable LLMs to act as agents that can proactively interact with KGs.By leveraging tool-based navigation and exploration of data, these agents perform chained reasoning to progressively build answers and ultimately solve complex queries efficiently and accurately.Readi [10] is another approach that first uses in-context learning and chain-of-thought prompts to generate reasoning paths with multiple constraints, which are then instantiated based on the graph data.The instantiated reasoning paths are merged and used as input to LLMs to generate an answer.This method has achieved significant performance improvements on KGQA (knowledge graph question answering) and TableQA (table question answering) tasks.Recently, RoG [49] is proposed to answer graph-retaled question in three steps: planning, retrieval, and reasoning.In the planning step, it generates a set of associated paths based on the structured information of the knowledge graph according to the problem.In the retrieval step, it uses the associated paths generated in the planning stage to retrieve the corresponding reasoning paths from the KG.Finally, it uses the retrieved reasoning paths to generate the answer and explanation for the problem using LLMs.</p>
<p>3.3.4</p>
<p>Discussion.The integration of LLMs and graphs has shown promising progress in minimizing the modality gap between structured data and textual data for solving graph-related tasks.By combining the strengths of LLMs in language understanding and the ability of graphs to capture complex relationships between entities, we can enable more accurate and flexible reasoning over graph data.However, despite the promising progress, there is still room for improvement in this area.One of the main challenges in integrating LLMs and graphs is scalability.In alignment and fusion training, current methods often use small language models or fix the parameters of LLMs, which limits their ability to scale to larger graph datasets.Therefore, it is crucial to explore methods for scaling model training with larger models on web-scale graph data, which can enable more accurate and efficient reasoning over large-scale graphs.Another challenge in this area is the limited interaction between graph agents and graph data.Current methods for graph agents often plan and execute only once, which may not be optimal for complex tasks requiring multiple runs.Therefore, it is necessary to investigate methods for agents to interact graph data multiple times, refining their plans and improving their performance based on feedback from the graph.This can enable more sophisticated reasoning over graph data and improve the accuracy of downstream tasks.Overall, the integration of LLMs and graphs is a promising research direction with significant potential for advancing the state-of-the-art in graph learning.By addressing the aforementioned challenges and developing more advanced methods for integrating LLMs and graphs, we can enable more accurate and flexible reasoning over graph data and unlock new applications in areas such as knowledge graph reasoning, molecular modeling, and social network analysis.</p>
<p>LLMs-Only</p>
<p>In this section, we will elaborate in detail on the direct application of LLMs for various graph-oriented tasks, namely the LLMs-Only category.These methods aim to allow LLMs to directly accept graph structure information, understand it, and perform inference for various downstream tasks in combination with this information.These methods can mainly be divided into two broad categories: i) Tuning-free methods aim to design prompts that LLMs can understand to express graphs, directly prompting pre-trained LLMs to perform graph-oriented tasks; ii) Tuning-required approaches focus on converting graphs into sequences in a specific way and aligning graph token sequences and natural language token sequences using fine-tuning methods.</p>
<p>3.4.1 Tuning-free.Given the unique structured characteristics of graph data, two critical challenges arise: effectively constructing a graph in natural language format and determining whether Large Language Models (LLMs) can accurately comprehend graph structures as represented linguistically.To address these issues, tuning-free approaches are being developed to model and infer graphs solely within the text space, thereby exploring the potential of pre-trained LLMs for enhanced structural understanding.</p>
<p>NLGraph [68], GPT4Graph [20] and Beyond Text [28] collectively examine the capabilities of LLMs in understanding and reasoning with graph data.NLGraph proposes a benchmark for graph-based problem solving and introduces instruction-based approaches, while GPT4Graph and Beyond Text investigate the proficiency of LLMs in comprehending graph structures and emphasizes the need for advancements in their graph processing capabilities.And Graph-LLM [8] explores the potential of LLMs in graph machine learning, focusing on the node classification task.Two pipelines, LLMs-as-Enhancers and LLMs-as-Predictors, are investigated to leverage LLMs' extensive common knowledge and semantic comprehension abilities.Through comprehensive studies, it provides original observations and insights that open new possibilities for utilizing LLMs in learning on graphs.Meanwhile, GraphText [95] translates graphs into natural language by deriving a graph-syntax tree and processing it with an LLM.It offers training-free graph reasoning and enables interactive graph reasoning, showcasing the unexplored potential of LLMs.Talk like a Graph [15] conducts an in-depth examination of text-based graph encoder functions for LLMs, evaluating their efficacy in transforming graph data into textual format to enhance LLMs' capabilities in executing graph reasoning tasks, and proposes the GraphQA benchmark to systematically measure the influence of encoding strategies on model performance.And LLM4DyG [91] benchmarks the spatial-temporal comprehension of LLMs on dynamic graphs, introducing tasks that evaluate both temporal and spatial understanding, and suggests the Disentangled Spatial-Temporal Thoughts (DST2) prompting technique for improved performance.To facilitate the integration of multimodality, GraphTMI [11] presents an innovative approach to integrating graph data with LLMs, introducing diverse modalities such as text, image, and motif encoding to enhance LLMs' efficiency in processing complex graph structures, and proposes the GraphTMI benchmark for evaluating LLMs in graph structure analysis, revealing that the image modality outperforms text and prior GNNs in balancing token limits and preserving essential information.Ai et al. [2] introduces a multimodal framework for graph understanding and reasoning, utilizing image encoding and GPT-4V's advanced capabilities to interpret and process diverse graph data, while identifying challenges in Chinese OCR and complex graph types, suggesting directions for future enhancements in AI's multimodal interaction and graph data processing.</p>
<p>3.4.2</p>
<p>Tuning-required.Due to the limitations of expressing graph structural information using pure text, the recent mainstream approach is to align graphs as node token sequences with natural language token sequences when inputting them to LLMs.In contrast to the aforementioned GNN as Prefix approach, the Tuning-required LLM-only approach discards the graph encoder and adopts a specific arrangement of graph token sequences, along with carefully designed embeddings of graph tokens in prompts, achieving promising performances in various downstream graph-related tasks.</p>
<p>InstructGLM [84] introduces an innovative framework for graph representation learning that combines natural language instructions with graph embeddings to fine-tune LLMs.This approach allows LLMs to effectively process graph structures without relying on specialized GNN architectures.WalkLM [62] integrates language models with random walks to create unsupervised attributed graph embeddings, focusing on the technical innovation of transforming graph entities into textual sequences and utilizing graph-aware fine-tuning.This technique captures both attribute semantics and graph structures.Recently, LLaGA [7] has utilized node-level templates to restructure graph data into organized sequences, which are then mapped into the token embedding space.This allows Large Language Models to process graph-structured data with enhanced versatility, generalizability, and interpretability.InstructGraph [69] proposes a methodological approach to improve LLMs for graph reasoning and generation through structured format verbalization, graph instruction tuning, and preference alignment.This aims to bridge the semantic gap between graph data and textual language models, and to mitigate the issue of hallucination in LLM outputs.</p>
<p>ZeroG [38] then leverages a language model to encode node attributes and class semantics, employing prompt-based subgraph and lightweight fine-tuning strategies address crossdataset zero-shot transferability challenges in graph learning.Furthermore, GraphWiz [6] utilizes GraphInstruct, an instructiontuning dataset, to augment language models for addressing various graph problems, employing Direct Preference Optimization (DPO) [55] to enhance the clarity and accuracy of reasoning processes.GraphInstruct [50] presents a comprehensive benchmark of 21 graph reasoning tasks, incorporating diverse graph generation methods and detailed reasoning steps to enhance LLMs with improved graph understanding and reasoning capabilities.And, MuseGraph [61] fuses the capabilities of LLMs with graph mining tasks through a compact graph description mechanism, diverse instruction generation, and graph-aware instruction tuning, enabling a generic approach for analyzing and processing attributed graphs.</p>
<p>3.4.3</p>
<p>Discussion.The LLMs-Only approach is an emerging research direction that explores the potential of pre-training Large Language Models specifically for interpreting graph data and merging graphs with natural language instructions.The main idea behind this approach is to leverage the powerful language understanding capabilities of LLMs to reason over graph data and generate accurate responses to queries.However, effectively transforming large-scale graphs into text prompts and reordering graph token sequences to preserve structural integrity without a graph encoder present significant ongoing challenges.These challenges arise due to the complex nature of graph data, which often contains intricate relationships between nodes and edges, as well as the limited ability of LLMs to capture such relationships without explicit guidance.As such, further research is needed to develop more advanced methods for integrating LLMs with graph data and overcoming the aforementioned challenges.</p>
<p>FUTURE DIRECTIONS</p>
<p>In this section, we explore several open problems and potential future directions in the field of large language models for graphs.</p>
<p>LLMs for Multi-modal Graphs</p>
<p>Recent studies have demonstrated the remarkable ability of large language models to process and understand multi-modal data [78], such as images [44] and videos [87].This capability has opened up new avenues for integrating LLMs with multi-modal graph data, where nodes may contain features from multiple modalities [40].By developing multi-modal LLMs that can process such graph data, we can enable more accurate and comprehensive reasoning over graph structures, taking into account not only textual information but also visual, auditory, and other types of data.</p>
<p>Efficiency and Less Computational Cost</p>
<p>In the current landscape, the substantial computational expenses associated with both the training and inference phases of LLMs pose a significant limitation [13,16], impeding their capacity to process large-scale graphs that encompass millions of nodes.This challenge is further compounded when attempting to integrate LLMs with GNNs, as the fusion of these two powerful models becomes increasingly arduous due to the aforementioned computational constraints [94].Consequently, the necessity to discover and implement efficient strategies for training LLMs and GNNs with reduced computational costs becomes paramount.This is not only to alleviate the current limitations but also to pave the way for the enhanced application of LLMs in graph-related tasks, thereby broadening their utility and impact in the field of data science.</p>
<p>Tackling Different Graph Tasks</p>
<p>The prevailing methodologies LLMs have primarily centered their attention on conventional graph-related tasks, such as link prediction and node classification.However, considering the remarkable capabilities of LLMs, it is both logical and promising to delve into their potential in tackling more complex and generative tasks, including but not limited to graph generation [97], graph understanding, and graph-based question answering [32].By expanding the horizons of LLM-based approaches to encompass these intricate tasks, we can unlock a myriad of new opportunities for their application across diverse domains.For instance, in the realm of drug discovery, LLMs could facilitate the generation of novel molecular structures; in social network analysis, they could provide deeper insights into intricate relationship patterns; and in knowledge graph construction, they could contribute to the creation of more comprehensive and contextually accurate knowledge bases.</p>
<p>User-Centric Agents on Graphs</p>
<p>The majority of contemporary LLM-based agents, specifically designed to address graph-related tasks, are predominantly tailored for single graph tasks.These agents typically adhere to a one-time-run procedure, aiming to resolve the provided question in a single attempt.Consequently, these agents are neither equipped to function as multi-run interactive agents, capable of adjusting their generated plans based on feedback or additional information, nor are they designed to be user-friendly agents that can effectively manage a wide array of user-given questions.An LLM-based agent [70] that embodies the ideal qualities should not only be user-friendly but also possess the capability to dynamically search for answers within graph data in response to a diverse range of open-ended questions posed by users.This would necessitate the development of an agent that is both adaptable and robust, able to engage in iterative interactions with users and adept at navigating the complexities of graph data to provide accurate and relevant answers.</p>
<p>CONCLUSION</p>
<p>In this comprehensive survey, we delve into the current state of large language models specifically tailored for graph data, proposing an innovative taxonomy grounded in the distinctive designs of their inference frameworks.We meticulously categorize these models into four unique framework designs, each characterized by its own set of advantages and limitations.Additionally, we provide a detailed discussion on these characteristics, enriching our analysis with insights into potential challenges and opportunities within this field.Our survey not only serves as a critical resource for researchers keen on exploring and leveraging large language models for graph-related tasks but also aims to inspire and guide future research endeavors in this evolving domain.Through this work, we hope to foster a deeper understanding and stimulate further innovation in the integration of LLMs with graphs.</p>
<p>Figure 2 :
2
Figure 2: GNNs as Prefix.</p>
<p>Figure 3 :
3
Figure 3: LLMs as Prefix.</p>
<p>[3], a contrastive graph-text pretraining technique is proposed to align the node embeddings encoded by LMs and GNNs simultaneously.The experiments are conducted on social networks, citation networks, and link networks, and show great performance on node and text classification as well as link prediction tasks.Furthermore, G2P2[74,75] enhances graph-grounded contrastive pre-training by proposing three different types of alignment: text-node, textsummary, and node-summary alignment.This enables G2P2 to leverage the rich semantic relationships in the graph structure to improve text classification performance in low-resource environments.GRENADE[36] is a graph-centric language model that proposes graph-centric contrastive learning and knowledge alignment to achieve both node-level and neighborhood-level alignment based on the node embeddings encoded from GNNs and LMs.This enables the model to capture text semantics and graph structure information through self-supervised learning, even in the absence of human-annotated labels.In addition to contrastive learning, THLM[100] leverages BERT and HGNNs to encode node embeddings and uses a positive-negative classification task with negative sampling to improve the alignment of embeddings from two different modalities.Recently, GLEM[94] adopts an efficient and effective solution that integrates graph structure and language learning</p>
<p>Figure 4 :
4
Figure 4: LLMs-Graphs Integration.</p>
<p>Figure 5:</p>
<p>GNNs asGraphGPT[63]Node-level Tokenization General Graph SIGIR 2024 HiGPT[64]Tokenization Heterogeneous Graph KDD 2024 GraphTranslator[88]Node-level Tokenization General Graph WWW 2024 UniGraph[25]Node-level Tokenization General Graph arXiv 2024 GIMLET[92]Node-level Tokenization Bioinformatics NeurIPS 2024 XRec[51]Node-level Tokenization Recommendation arXiv 2024 GraphLLM[5]Graph-level Tokenization Graph Reasoning arXiv 2023 GIT-Mol[45]Graph-level Tokenization Bioinformatics Comput Biol Med 2024 MolCA[48]Graph-level Tokenization Bioinformatics EMNLP 2023 InstructMol[4]Graph-level Tokenization Bioinformatics arXiv 2023 G-Retriever[24]Graph-level Tokenization Graph-based QA arXiv 2024 GNP[65]Graph-level Tokenization Graph-based QA AAAI 2024LLMs as Prefix G-Prompt[30]Embs.from LLMs for GNNs General Graph arXiv 2023 SimTeG[14]Embs.from LLMs for GNNs General Graph arXiv 2023 GALM[81]Embs.from LLMs for GNNs General Graph KDD 2023 OFA[42]Embs.from LLMs for GNNs General Graph ICLR 2024 TAPE[22]Embs.from LLMs for GNNs General Graph ICLR 2024 LLMRec[73]Embs.from LLMs for GNNs Recommendation WSDM 2024 OpenGraph[80]Labels from LLMs for GNNs General Graph arXiv 2024 LLM-GNN[9]Labels from LLMs for GNNs General Graph ICLR 2024 GraphEdit[21]Labels from LLMs for GNNs General Graph arXiv 2023 RLMRec[58]Labels from LLMs for GNNs Recommendation WWW 2024LLMs-Graphs InteractionMoMu[63]Alignment between GNNs and LLMs Bioinformatics arXiv 2022 ConGraT[64]Alignment between GNNs and LLMs General Graph arXiv 2023 G2P2[88]Alignment between GNNs and LLMs General Graph SIGIR 2023 GRENADE[25]Alignment between GNNs and LLMs General Graph EMNLP 2023 MoleculeSTM[92]Alignment between GNNs and LLMs Bioinformatics Nature MI 2023 THLM[51]Alignment between GNNs and LLMs Heterogeneous Graph EMNLP 2023 GLEM[5]Alignment between GNNs and LLMs General Graph ICLR 2023 GreaseLM[90]Fusion Training of GNNs and LLMs Graph-based QA ICLR 2022 DGTL[54]Fusion Training of GNNs and LLMs General Graph arXiv 2023 ENGINE[98]Fusion Training of GNNs and LLMs General Graph arXiv 2024 GraphAdapter[31]Fusion Training of GNNs and LLMs General Graph WWW 2024 Pangu[19]LLMs Agent for Graphs Graph-based QA ACL 2023 Graph Agent[71]LLMs Agent for Graphs General Graph arXiv 2023 FUXI[18]LLMs Agent for Graphs Graph-based QA arXiv 2024 Readi[10]LLMs Agent for Graphs Graph-based QA arXiv 2024 RoG[49]LLMs Agent for Graphs Graph-based QA ICLR 2024LLMs-OnlyNLGraph[68]Tuning-free Graph Reasoning NeurIPS 2024 GPT4Graph[20]Tuning-free Graph Reasoning &amp; QA arXiv 2023 Beyond Text[28]Tuning-free General Graph arXiv 2023 Graph-LLM[8]Tuning-free General Graph KDD Exp.News.2023 GraphText[95]Tuning-free General Graph arXiv 2023 Talk like a Graph[15]Tuning-free Graph Reasoning arXiv 2023 LLM4DyG[91]Tuning-free Dynamic Graph arXiv 2023 GraphTMI[11]Tuning-free General Graph arXiv 2023 Ai et al.[2]Tuning-free Multi-modal Graph arXiv 2023 InstructGLM[84]Tuning-required General Graph EACL 2024 WalkLM[62]Tuning-required General Graph NeurIPS 2024 LLaGA[7]Tuning-required General Graph ICML 2024 InstructGraph[69]Tuning-required General Graph &amp; QA &amp; Reasoning arXiv 2024 ZeroG[38]Tuning-required General Graph arXiv 2024 GraphWiz[6]Tuning-required Graph Reasoning arXiv 2024 GraphInstruct[50]Tuning-required Graph Reasoning &amp; Generation arXiv 2024 MuseGraph[61]Tuning-required General Graph arXiv 2024
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Shyamal Altman, Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023. 2023</p>
<p>When Graph Data Meets Multimodal: A New Paradigm for Graph Understanding and Reasoning. Qihang Ai, Jianwu Zhou, Haiyun Jiang, Lemao Liu, Shuming Shi, arXiv:2312.103722023. 2023arXiv preprint</p>
<p>Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings. William Brannon, arXiv:2305.143212023. 2023arXiv preprint</p>
<p>Instructmol: Multimodal integration for building a versatile and reliable molecular assistant in drug discovery. He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, Yu Li, arXiv:2311.162082023. 2023arXiv preprint</p>
<p>Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang, arXiv:2310.05845Graphllm: Boosting graph reasoning ability of large language model. 2023. 2023arXiv preprint</p>
<p>GraphWiz: An Instruction-Following Language Model for Graph Problems. Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li, arXiv:2402.160292024. 2024arXiv preprint</p>
<p>Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang, arXiv:2402.08170LLaGA: Large Language and Graph Assistant. 2024. 2024arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, ACM SIGKDD Explorations Newsletter. 252024. 2024</p>
<p>Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, arXiv:2310.04668Label-free node classification on graphs with large language models (llms). 2023. 2023arXiv preprint</p>
<p>Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, arXiv:2403.08593Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments. 2024. 2024arXiv preprint</p>
<p>Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models. Debarati Das, Ishaan Gupta, Jaideep Srivastava, Dongyeop Kang, arXiv:2311.098622023. 2023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Parameterefficient fine-tuning of large-scale pre-trained language models. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Nature Machine Intelligence. 52023. 2023</p>
<p>Qian Keyu Duan, Tat-Seng Liu, Shuicheng Chua, Wei Tsang Yan, Qizhe Ooi, Junxian Xie, He, arXiv:2308.02565Simteg: A frustratingly simple approach improves textual graph learning. 2023. 2023arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023. 2023arXiv preprint</p>
<p>Parameter-efficient mixture-of-experts architecture for pre-trained language models. Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, Ji-Rong Wen, arXiv:2203.011042022. 2022arXiv preprint</p>
<p>Generalization and representational limits of graph neural networks. Vikas Garg, Stefanie Jegelka, Tommi Jaakkola, ICML. PMLR. 2020</p>
<p>Yu Gu, arXiv:2402.14672Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments. 2024. 2024arXiv preprint</p>
<p>Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments. Yu Gu, Xiang Deng, Yu Su, arXiv:2212.097362022. 2022arXiv preprint</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, arXiv:2305.150662023. 2023arXiv preprint</p>
<p>Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang, arXiv:2402.15183GraphEdit: Large Language Models for Graph Structure Learning. 2024. 2024arXiv preprint</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann Lecun, Bryan Hooi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Lightgcn: Simplifying and powering graph convolution network for recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. the 43rd International ACM SIGIR conference on research and development in Information Retrieval2020</p>
<p>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering. Xiaoxin He, Yijun Tian, Yifei Sun, V Nitesh, Thomas Chawla, Yann Laurent, Xavier Le-Cun, Bryan Bresson, Hooi, arXiv:2402.076302024. 2024arXiv preprint</p>
<p>UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language. Yufei He, Bryan Hooi, arXiv:2402.136302024. 2024arXiv preprint</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, European Conference on Information Retrieval. Springer2024</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in neural information processing systems. 332020. 2020</p>
<p>Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data. Yuntong Hu, Zheng Zhang, Liang Zhao, arXiv:2310.049442023. 2023arXiv preprint</p>
<p>Large Language Models for Graphs: Progresses and Directions. Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, Companion Proceedings of the ACM on Web Conference 2024. 2024</p>
<p>Prompt-based node feature extractor for few-shot learning on text-attributed graphs. Xuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao, Zhisheng Zhang, Yang Yang, Qi Zhu, arXiv:2309.028482023. 2023arXiv preprint</p>
<p>Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, Qi Zhu, arXiv:2402.12984Can GNN be Good Adapter for LLMs?. 2024. 2024arXiv preprint</p>
<p>Knowledge graph embedding based question answering. Xiao Huang, Jingyuan Zhang, Dingcheng Li, Ping Li, Proceedings of the twelfth ACM international conference on web search and data mining. the twelfth ACM international conference on web search and data mining2019</p>
<p>Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.02783Large language models on graphs: A comprehensive survey. 2023. 2023arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>Graph transformer for recommendation. Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, Chao Huang, SIGIR. 2023</p>
<p>GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs. Yichuan Li, Kaize Ding, Kyumin Lee, arXiv:2310.151092023. 2023arXiv preprint</p>
<p>Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu, Yu , arXiv:2311.12399A survey of graph meets large language model: Progress and future directions. 2023. 2023arXiv preprint</p>
<p>Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li, arXiv:2402.11235ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs. 2024. 2024arXiv preprint</p>
<p>Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, Chao Huang, arXiv:2403.00813Urbangpt: Spatio-temporal large language models. 2024. 2024arXiv preprint</p>
<p>A Survey of Multi-modal Knowledge Graphs: Technologies and Trends. Wanying Liang, Pasquale De Meo, Yong Tang, Jia Zhu, Comput. Surveys. 2024. 2024</p>
<p>Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, Tat-Seng Chua, arXiv:2401.17197Data-efficient Fine-tuning for LLM-based Recommendation. 2024. 2024arXiv preprint</p>
<p>One for all: Towards training one graph model for all classification tasks. Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang, arXiv:2310.001492023arXiv preprint</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 362024. 2024</p>
<p>Git-mol: A multimodal large language model for molecular science with graph, image, and text. Pengfei Liu, Yiming Ren, Computers in Biology and Medicine. 171108073Jun Tao, and Zhixiang Ren. 2024. 2024</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. Shengchao Liu, Nature Machine Intelligence. 2023. 2023</p>
<p>Yinhan Liu, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Zhiyuan Liu, arXiv:2310.127982023. 2023arXiv preprint</p>
<p>Linhao Luo, Yuan-Fang Li, arXiv:2310.01061Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. 2023arXiv preprint</p>
<p>GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability. Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin, arXiv:2403.044832024. 2024arXiv preprint</p>
<p>XRec: Large Language Models for Explainable Recommendation. Qiyao Ma, Xubin Ren, Chao Huang, arXiv:2406.023772024. 2024arXiv preprint</p>
<p>Information network or social The structure of the follow graph. Seth A Myers, Aneesh Sharma, Pankaj Gupta, Jimmy Lin, Proceedings of the 23rd international conference on world wide web. the 23rd international conference on world wide web2014</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, IEEE Transactions on Knowledge and Data Engineering. 2024. 2024</p>
<p>Disentangled representation learning with large language models for text-attributed graphs. Yijian Qin, Xin Wang, Ziwei Zhang, Wenwu Zhu, arXiv:2310.181522023. 2023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 212020. 2020</p>
<p>Xubin Ren, Wei Wei, Lianghao Xia, Chao Huang, arXiv:2404.03354A Comprehensive Survey on Self-Supervised Learning for Recommendation. 2024. 2024arXiv preprint</p>
<p>Representation learning with large language models for recommendation. Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Disentangled contrastive collaborative filtering. Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, Chao Huang, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Bing Su, Dazhao Du, Zhao Yang, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022. 2022arXiv preprint</p>
<p>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining. Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, Carl Yang, arXiv:2403.047802024. 2024arXiv preprint</p>
<p>Walklm: A uniform language model fine-tuning framework for attributed graph embedding. Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, Carl Yang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023. 2023arXiv preprint</p>
<p>Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang, arXiv:2402.16024HiGPT: Heterogeneous Graph Language Model. 2024. 2024arXiv preprint</p>
<p>Graph neural prompting with large language models. Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Hugo Touvron, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.109032017. 2017arXiv preprint</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment. Jianing Wang, Junda Wu, Yupeng Hou, arXiv:2402.087852024. 2024arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Frontiers of Computer Science. 181863452024. 2024</p>
<p>Graph Agent: Explicit Reasoning Agent for Graphs. Qinyong Wang, Zhenxiang Gao, Rong Xu, arXiv:2310.164212023. 2023arXiv preprint</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Llmrec: Large language models with graph augmentation for recommendation. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang, Proceedings of the 17th ACM International Conference on Web Search and Data Mining. the 17th ACM International Conference on Web Search and Data Mining2024</p>
<p>Augmenting low-resource text classification with graph-grounded pre-training and prompting. Zhihao Wen, Yuan Fang, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Prompt tuning on graph-augmented lowresource text classification. Zhihao Wen, Yuan Fang, arXiv:2307.102302023. 2023arXiv preprint</p>
<p>Difformer: Scalable (graph) transformers induced by energy constrained diffusion. Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, Junchi Yan, arXiv:2301.094742023. 2023arXiv preprint</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, Junchi Yan, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Next-gpt: Any-to-any multimodal llm. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua, arXiv:2309.055192023. 2023arXiv preprint</p>
<p>A comprehensive survey on graph neural networks. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip Yu, IEEE transactions on neural networks and learning systems. 322020. 2020</p>
<p>Lianghao Xia, Ben Kao, Chao Huang, arXiv:2403.01121OpenGraph: Towards Open Graph Foundation Models. 2024. 2024arXiv preprint</p>
<p>Graph-aware language model pre-training on a large graph corpus can help multiple graph applications. Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Xiang Vassilis N Ioannidis, Qing Song, Sheng Ping, Carl Wang, Yi Yang, Xu, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, arXiv:1810.00826How powerful are graph neural networks?. 2018. 2018arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>Natural language is all a graph needs. Ruosong Ye, Caiqi Zhang, arXiv:2308.071342023. 2023arXiv preprint</p>
<p>Graph contrastive learning automated. Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang, International Conference on Machine Learning. PMLR2021</p>
<p>Graph transformer networks. Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, Hyunwoo J Kim, Advances in neural information processing systems. 322019. 2019</p>
<p>Video-llama: An instructiontuned audio-visual language model for video understanding. Hang Zhang, Xin Li, Lidong Bing, arXiv:2306.028582023. 2023arXiv preprint</p>
<p>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks. Mengmei Zhang, arXiv:2402.071972024. 2024arXiv preprint</p>
<p>Link prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in neural information processing systems. 2018. 201831</p>
<p>Greaselm: Graph reasoning enhanced language models for question answering. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, arXiv:2201.088602022. 2022arXiv preprint</p>
<p>LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?. Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, Wenwu Zhu, arXiv:2310.171102023. 2023arXiv preprint</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng, Lingpeng Kong, Qi Liu, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Jianan Zhao, Hesham Mostafa, Michael Galkin, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2405.20445GraphAny: A Foundation Model for Node Classification on Any Graph. 2024. 2024arXiv preprint</p>
<p>Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang, arXiv:2210.14709Learning on large-scale text-attributed graphs via variational inference. 2022. 2022arXiv preprint</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2310.01089Graphtext: Graph reasoning in text space. 2023. 2023arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 2023arXiv preprint</p>
<p>A survey on deep graph generation: Methods and applications. Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, Shu Wu, Learning on Graphs Conference. PMLR2022</p>
<p>Efficient Tuning and Inference for Large Language Models on Textual Graphs. Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang, arXiv:2401.155692024. 2024arXiv preprint</p>
<p>Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li, arXiv:2311.01343Collaborative large language model for recommender systems. 2023. 2023arXiv preprint</p>
<p>Pretraining language models with text-attributed heterogeneous graphs. Tao Zou, Le Yu, Yifei Huang, Leilei Sun, Bowen Du, arXiv:2310.125802023. 2023arXiv preprint</p>
<p>APPENDIX In Table 1, we provide an overview of notable graph learning techniques that utilize large language models. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2134 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2134</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2134</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-55.html">extraction-schema-55</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-276767120</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.01508v1.pdf" target="_blank">E NABLING AI S CIENTISTS TO R ECOGNIZE I NNOVATION : A D OMAIN -A GNOSTIC A LGORITHM FOR A SSESSING N OVELTY</a></p>
                <p><strong>Paper Abstract:</strong> In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery. This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea’s local density with its adjacent neighbors’ densities. We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment. Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820) and biomedical research (AUROC=0.765) domains. Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s. 0.597) on cross-domain evaluation. These results validate RND as a generalizable solution for automated novelty assessment in scientific research.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2134.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2134.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Neighbor Density</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic novelty scoring algorithm that computes an idea's novelty as the empirical percentile of its local neighbor density compared to the densities of its P nearest semantic neighbors (uses P=100, Q=50 in experiments). The authors claim theoretical domain-invariance (score distribution ~ Uniform(0,1)).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>automated novelty score (relative local semantic density)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>automated temporal/venue-based labeling: recent top-venue publications as 'novel' vs historically highly-cited older papers as 'non-novel' (used as validation ground truth in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>RND score = empirical CDF percentile of an idea's neighbor-density among its P nearest neighbors (quantile-based novelty score); used to separate novel vs non-novel (higher = more novel).</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>RND achieves AUROC 0.820 on NeurIPS (computer science), 0.765 on Nature Medicine (biomedicine), and 0.795 on a cross-domain Mixed test set; RND therefore closely tracks the paper's chosen ground-truth labeling across fields and outperforms other proxies on cross-domain evaluation (e.g., HD drops sharply cross-domain).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>RND vs worst-performing baseline on cross-domain: absolute AUROC gap ≈ 0.433 (RND 0.795 vs HD 0.362 reported); for in-domain comparisons RND is similar to or slightly below the best single-domain local-density baseline (NeurIPS: RND 0.820 vs Absolute Local Density 0.851).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>The paper uses temporally-separated labels for validation (positives = recent top-venue 2024–2025; negatives = older highly-cited 2015–2020 or older); authors emphasize that manual novelty labels become outdated as novel ideas become established, but the paper does not report detailed longitudinal trajectories of proxy vs ground-truth recognition (no numerical time-to-recognition curve provided).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (NeurIPS), Biomedical research (Nature Medicine), and a Mixed cross-domain set combining both.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>RND is claimed domain-invariant: similar score distributions across NeurIPS and Nature Medicine and little degradation on Mixed (AUROC: 0.820 CS, 0.765 Biomed, 0.795 Mixed). The paper contrasts this with other proxies that perform well in a single field but degrade cross-field.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>RND (automated system) achieves AUROC 0.820 (NeurIPS), 0.765 (Nature Medicine), 0.795 (Mixed); it outperforms LLM-only approaches on Mixed and maintains stable performance across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>RND itself is presented as a corrective mechanism: replacing absolute local density or LLM-only judgments with a relative-neighbor-density percentile produces markedly better cross-domain calibration (e.g., Mixed AUROC 0.795 vs HD 0.362). Effectiveness demonstrated by reported AUROC improvements in cross-domain tests.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Authors argue and demonstrate empirically that prior absolute-density metrics and LLM judgments show domain-specific degradation (suggesting bias from domain-specific densities and internal model knowledge), whereas RND's percentile construction reduces sensitivity to corpus-specific density. No explicit quantitative analysis of training-data composition bias on RND is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td>In single-domain NeurIPS tests absolute local density (HD/Absolute Local Density) slightly outperformed or matched RND (Absolute Local Density AUROC 0.851 vs RND 0.820), indicating RND is not uniformly superior in every single-domain setting.</td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Constructed large literature embedding corpora (PubMed ~25.36M papers with title+abstract embeddings, ArXiv ~2.64M), built automated test sets: NeurIPS positives = accepted 2024 NeurIPS (oral/spotlight) filtered for explicit program-chair novelty comments; NeurIPS negatives = 99 most-cited NeurIPS papers 2015–2020. Nature Medicine positives = 'Article' type Aug 2024–Feb 2025 (excluding phase 2/3 trials); negatives = 99 most-cited Nature Medicine articles over past 15 years. Evaluation metric: AUROC for binary discrimination of novel vs non-novel.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2134.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2134.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HD/ON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Historical Dissimilarity / Overall Novelty (Absolute Local Density-based metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Absolute local-density methods that compute novelty as average semantic distance to a small set of nearest historical (and contemporary) papers (e.g., Historical Dissimilarity = mean distance to 5 most similar historical abstracts; Overall Novelty combines HD, Contemporary Dissimilarity and a citation-based Contemporary Impact).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>absolute local semantic density (distance to nearest historical/contemporary neighbors); Contemporary Impact (citation-based) is used multiplicatively in ON.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Same temporal/venue-based labeling in this paper (recent top-venue papers as 'novel' vs historically highly-cited older works as 'non-novel').</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Historical Dissimilarity (HD) = mean Euclidean distance to 5 most similar historical abstracts; ON = HD × CI / CD (paper presents ON formula but exact algebraic form in text contains formatting issues); both treat distance values as absolute novelty scores.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>HD / Absolute Local Density reports high in-domain AUROCs (NeurIPS: Absolute Local Density AUROC 0.851; Nature Medicine: 0.757) but severe cross-domain degradation (Mixed AUROC 0.362–0.395 depending on table). ON is described as correlating with human-labeled novelty in prior work but current paper shows its domain sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Cross-domain performance collapse: HD/A.L.D. AUROC falls from ≈0.75–0.85 in single domains to ≈0.36–0.40 on Mixed — absolute AUROC drop ≈0.40–0.49, indicating a large proxy-truth gap when domains are mixed.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Not directly quantified; the method's dependence on 'historical' vs 'contemporary' cutoffs (e.g., years used to define databases) is highlighted as a sensitivity and source of variability, but no empirical time-course of recognition is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Evaluated on computer science (NeurIPS) and biomedical (Nature Medicine) test sets and a Mixed cross-domain set.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Metric performs well within single domains (CS and Biomed individually) but yields inconsistent/shifted score distributions across domains causing mis-ranking (e.g., HD could rank some NeurIPS negative samples more novel than Nature Medicine positives).</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Absolute Local Density (HD) reported AUROC: NeurIPS ≈0.851, Nature Medicine ≈0.757, Mixed ≈0.362–0.395. The sharp drop in Mixed indicates poor cross-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>The authors do not present an improvement to HD in this paper beyond highlighting its limitations; RND is proposed as the corrective relative approach.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Paper argues HD's values depend on corpus composition (size, time cutoffs, domain densities), implying bias from historical/contemporary selection; no numeric estimate of training-data bias is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td>HD matched or came close to RND in single-domain tests (HD performance similar to RND for NeurIPS and Nature Medicine), showing that absolute local density can work when domain is homogeneous.</td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Baseline methods applied to the same automated test sets (NeurIPS, Nature Medicine, Mixed) drawn from large embedding corpora; HD uses 5 nearest historical neighbors per its original design, ON uses historical vs contemporary splits (historical 2011–2021, contemporary 2021–2025 in experiments).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2134.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2134.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citations & Journal-venue labelling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation counts and top-venue / publication-time based validation labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses citation counts (for selecting high-citation negatives) and venue/time (recent top-venue articles as positives) as the operational 'ground truth' for novelty in validation, and critiques the limitations of manual expert labels and citation-only measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>early/aggregate citations, journal/venue prestige and publication recency used as proxies for novelty/non-novelty</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Paper treats recent publications in top venues as positive ground-truth (novel) and historically highly-cited older papers as negative ground-truth (non-novel) for validation purposes.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Novelty operationalized as 'recent publication in top venue + program-chair comment explicitly indicating novelty' (positives) versus 'historically highly-cited papers from the same venue in previous years' (negatives).</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>No explicit numerical mapping of citation counts to eventual scientific value is provided; rather, the paper uses citation-based selection to create negative samples (e.g., 99 most-cited NeurIPS papers 2015–2020 and 99 most-cited Nature Medicine over past 15 years) and argues that citation accumulation indicates that an idea has been extensively followed up and thus is currently non-novel.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Authors assert that human-labeled novelty quickly becomes outdated as ideas become established; validation positive set is drawn from 2024–2025 publications while negatives are older (2015–2020 or earlier), but paper does not quantify exact time-lag dynamics (no half-lives or year-by-year curves).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (NeurIPS) and Biomedical research (Nature Medicine) as the domains in which this temporal/venue-based ground truth labeling is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>The authors acknowledge differences in citation patterns and publication velocities across fields (e.g., computer science vs biomedicine), and cite this as a reason absolute citation/density thresholds are unreliable across domains; they explicitly use venue- and time-based sampling to attempt to create a robust validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>The paper proposes a temporal/venue-based automated labeling methodology to scale validation without manual labels; this is not claimed to be perfect but to be more objective and scalable than small manual test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>The authors note that using citation-based or venue-based ground truth can over-simplify the novelty spectrum and that selecting highly-cited older papers as 'non-novel' creates an easier discrimination problem than borderline cases; they acknowledge this as a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td>Authors concede that highly-cited papers can be novel in certain senses (novel + useful) and that citation counts alone can indicate either novelty or utility; they caution that their negative sampling may produce 'too-easy' negatives compared to real-world borderline novelty cases.</td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Ground-truth creation: positives = recent top-venue accepted papers with program-chair novelty comments or recent Nature Medicine 'Article' items (2024–2025); negatives = top-cited older papers from the same venues (NeurIPS 2015–2020 top 99 cited; Nature Medicine top 99 cited over past 15 years).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2134.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2134.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based peer-judgment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-judge (with/without literature search, with guideline/tournament)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of large language models to evaluate novelty either directly (with prompts/guidelines/tournament comparators) or augmented with retrieved literature (10 most relevant papers) to determine overlap with given idea; performance is highly sensitive to provision of external literature and domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>peer-review style scores produced by LLMs (chain-of-thought with NeurIPS guideline, Swiss tournament scoring, or binary novel/non-novel with literature search)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Compared against the paper's automated temporal/venue-based validation labels (recent top-venue = novel, highly-cited older = non-novel).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>LLM judgment protocols: (1) LLM + literature search: feed 10 most relevant papers and ask for overlap assessment (binary 0/1); (2) LLM + review guideline: use NeurIPS review rubric to output an 'Overall' score; (3) LLM tournament: Swiss-system pairwise comparisons on standardized proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>LLM performance varies strongly by configuration and field: LLM with no literature shows AUROC ≈ 0.5 (random); LLM + literature search achieves AUROC ≈ 0.8 on NeurIPS (CS) but degrades to ≈ 0.6 on Nature Medicine and cross-domain; Sonnet-3.7 and Deepseek-r1 outperform GPT-4o when external knowledge is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Field sensitivity: example absolute AUROC drop ≈0.2 when moving from CS (≈0.8) to Biomed (≈0.6) for LLM+search, indicating substantial proxy-truth gap that depends on domain and model internals.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Evaluated on NeurIPS (computer science) and Nature Medicine (biomedicine) test sets and Mixed cross-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>LLM judgments depend heavily on the model's internal knowledge and are better in computer science (where model knowledge likely overlaps training data) than in biomedicine; adding retrieved external literature improves performance but does not eliminate field disparities.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Reported AUROCs: LLM w/o literature ≈0.5; LLM + literature search ≈0.8 (NeurIPS) and ≈0.6 (Nature Medicine/Mixed). Sonnet-3.7 with guideline/tournament performs poorly without literature; performance variability across repeated runs reported for LLMs (standard deviation included in table D.1).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Providing external literature (top-10 retrieved papers) to the LLM substantially improves judgments; the authors propose using RND as a reward signal to train reasoning models in RL frameworks to bias LLMs toward novel idea generation, but no quantitative reduction in LLM proxy-truth gap via this mechanism is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Authors note LLMs' sensitivity to internal knowledge (models trained on corpora biased toward certain domains) and cite examples of degraded performance in biomedicine; they suggest this is a source of bias but do not provide quantitative measures of the training-data distribution effect.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td>When given retrieved literature, some LLMs (Sonnet-3.7, Deepseek-r1) reach high in-domain AUROC (~0.8) comparable to RND, showing LLMs can succeed when external context and model capability align with domain.</td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>LLM baselines were evaluated by running each LLM-based procedure three times (to capture output variability) and averaging results; methods included LLM-only (with guideline), LLM+tournament (Swiss system), and LLM+literature (feed top-10 retrieved papers). Evaluated against the same temporal/venue-based test sets used for other baselines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation <em>(Rating: 2)</em></li>
                <li>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 1)</em></li>
                <li>Incentivizing reasoning capability in llms via reinforcement learning (Deepseek-r1) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2134",
    "paper_id": "paper-276767120",
    "extraction_schema_id": "extraction-schema-55",
    "extracted_data": [
        {
            "name_short": "RND",
            "name_full": "Relative Neighbor Density",
            "brief_description": "A domain-agnostic novelty scoring algorithm that computes an idea's novelty as the empirical percentile of its local neighbor density compared to the densities of its P nearest semantic neighbors (uses P=100, Q=50 in experiments). The authors claim theoretical domain-invariance (score distribution ~ Uniform(0,1)).",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "automated novelty score (relative local semantic density)",
            "ground_truth_measure": "automated temporal/venue-based labeling: recent top-venue publications as 'novel' vs historically highly-cited older papers as 'non-novel' (used as validation ground truth in this paper)",
            "novelty_transformation_measure": "RND score = empirical CDF percentile of an idea's neighbor-density among its P nearest neighbors (quantile-based novelty score); used to separate novel vs non-novel (higher = more novel).",
            "quantitative_relationship": "RND achieves AUROC 0.820 on NeurIPS (computer science), 0.765 on Nature Medicine (biomedicine), and 0.795 on a cross-domain Mixed test set; RND therefore closely tracks the paper's chosen ground-truth labeling across fields and outperforms other proxies on cross-domain evaluation (e.g., HD drops sharply cross-domain).",
            "gap_magnitude": "RND vs worst-performing baseline on cross-domain: absolute AUROC gap ≈ 0.433 (RND 0.795 vs HD 0.362 reported); for in-domain comparisons RND is similar to or slightly below the best single-domain local-density baseline (NeurIPS: RND 0.820 vs Absolute Local Density 0.851).",
            "temporal_pattern": "The paper uses temporally-separated labels for validation (positives = recent top-venue 2024–2025; negatives = older highly-cited 2015–2020 or older); authors emphasize that manual novelty labels become outdated as novel ideas become established, but the paper does not report detailed longitudinal trajectories of proxy vs ground-truth recognition (no numerical time-to-recognition curve provided).",
            "field_studied": "Computer science (NeurIPS), Biomedical research (Nature Medicine), and a Mixed cross-domain set combining both.",
            "field_differences": "RND is claimed domain-invariant: similar score distributions across NeurIPS and Nature Medicine and little degradation on Mixed (AUROC: 0.820 CS, 0.765 Biomed, 0.795 Mixed). The paper contrasts this with other proxies that perform well in a single field but degrade cross-field.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "RND (automated system) achieves AUROC 0.820 (NeurIPS), 0.765 (Nature Medicine), 0.795 (Mixed); it outperforms LLM-only approaches on Mixed and maintains stable performance across domains.",
            "correction_mechanism": "RND itself is presented as a corrective mechanism: replacing absolute local density or LLM-only judgments with a relative-neighbor-density percentile produces markedly better cross-domain calibration (e.g., Mixed AUROC 0.795 vs HD 0.362). Effectiveness demonstrated by reported AUROC improvements in cross-domain tests.",
            "training_distribution_bias": "Authors argue and demonstrate empirically that prior absolute-density metrics and LLM judgments show domain-specific degradation (suggesting bias from domain-specific densities and internal model knowledge), whereas RND's percentile construction reduces sensitivity to corpus-specific density. No explicit quantitative analysis of training-data composition bias on RND is provided.",
            "counterexamples": "In single-domain NeurIPS tests absolute local density (HD/Absolute Local Density) slightly outperformed or matched RND (Absolute Local Density AUROC 0.851 vs RND 0.820), indicating RND is not uniformly superior in every single-domain setting.",
            "study_design": "Constructed large literature embedding corpora (PubMed ~25.36M papers with title+abstract embeddings, ArXiv ~2.64M), built automated test sets: NeurIPS positives = accepted 2024 NeurIPS (oral/spotlight) filtered for explicit program-chair novelty comments; NeurIPS negatives = 99 most-cited NeurIPS papers 2015–2020. Nature Medicine positives = 'Article' type Aug 2024–Feb 2025 (excluding phase 2/3 trials); negatives = 99 most-cited Nature Medicine articles over past 15 years. Evaluation metric: AUROC for binary discrimination of novel vs non-novel.",
            "uuid": "e2134.0"
        },
        {
            "name_short": "HD/ON",
            "name_full": "Historical Dissimilarity / Overall Novelty (Absolute Local Density-based metrics)",
            "brief_description": "Absolute local-density methods that compute novelty as average semantic distance to a small set of nearest historical (and contemporary) papers (e.g., Historical Dissimilarity = mean distance to 5 most similar historical abstracts; Overall Novelty combines HD, Contemporary Dissimilarity and a citation-based Contemporary Impact).",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_metric_type": "absolute local semantic density (distance to nearest historical/contemporary neighbors); Contemporary Impact (citation-based) is used multiplicatively in ON.",
            "ground_truth_measure": "Same temporal/venue-based labeling in this paper (recent top-venue papers as 'novel' vs historically highly-cited older works as 'non-novel').",
            "novelty_transformation_measure": "Historical Dissimilarity (HD) = mean Euclidean distance to 5 most similar historical abstracts; ON = HD × CI / CD (paper presents ON formula but exact algebraic form in text contains formatting issues); both treat distance values as absolute novelty scores.",
            "quantitative_relationship": "HD / Absolute Local Density reports high in-domain AUROCs (NeurIPS: Absolute Local Density AUROC 0.851; Nature Medicine: 0.757) but severe cross-domain degradation (Mixed AUROC 0.362–0.395 depending on table). ON is described as correlating with human-labeled novelty in prior work but current paper shows its domain sensitivity.",
            "gap_magnitude": "Cross-domain performance collapse: HD/A.L.D. AUROC falls from ≈0.75–0.85 in single domains to ≈0.36–0.40 on Mixed — absolute AUROC drop ≈0.40–0.49, indicating a large proxy-truth gap when domains are mixed.",
            "temporal_pattern": "Not directly quantified; the method's dependence on 'historical' vs 'contemporary' cutoffs (e.g., years used to define databases) is highlighted as a sensitivity and source of variability, but no empirical time-course of recognition is provided.",
            "field_studied": "Evaluated on computer science (NeurIPS) and biomedical (Nature Medicine) test sets and a Mixed cross-domain set.",
            "field_differences": "Metric performs well within single domains (CS and Biomed individually) but yields inconsistent/shifted score distributions across domains causing mis-ranking (e.g., HD could rank some NeurIPS negative samples more novel than Nature Medicine positives).",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "Absolute Local Density (HD) reported AUROC: NeurIPS ≈0.851, Nature Medicine ≈0.757, Mixed ≈0.362–0.395. The sharp drop in Mixed indicates poor cross-domain generalization.",
            "correction_mechanism": "The authors do not present an improvement to HD in this paper beyond highlighting its limitations; RND is proposed as the corrective relative approach.",
            "training_distribution_bias": "Paper argues HD's values depend on corpus composition (size, time cutoffs, domain densities), implying bias from historical/contemporary selection; no numeric estimate of training-data bias is provided.",
            "counterexamples": "HD matched or came close to RND in single-domain tests (HD performance similar to RND for NeurIPS and Nature Medicine), showing that absolute local density can work when domain is homogeneous.",
            "study_design": "Baseline methods applied to the same automated test sets (NeurIPS, Nature Medicine, Mixed) drawn from large embedding corpora; HD uses 5 nearest historical neighbors per its original design, ON uses historical vs contemporary splits (historical 2011–2021, contemporary 2021–2025 in experiments).",
            "uuid": "e2134.1"
        },
        {
            "name_short": "Citations & Journal-venue labelling",
            "name_full": "Citation counts and top-venue / publication-time based validation labels",
            "brief_description": "The paper uses citation counts (for selecting high-citation negatives) and venue/time (recent top-venue articles as positives) as the operational 'ground truth' for novelty in validation, and critiques the limitations of manual expert labels and citation-only measures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "early/aggregate citations, journal/venue prestige and publication recency used as proxies for novelty/non-novelty",
            "ground_truth_measure": "Paper treats recent publications in top venues as positive ground-truth (novel) and historically highly-cited older papers as negative ground-truth (non-novel) for validation purposes.",
            "novelty_transformation_measure": "Novelty operationalized as 'recent publication in top venue + program-chair comment explicitly indicating novelty' (positives) versus 'historically highly-cited papers from the same venue in previous years' (negatives).",
            "quantitative_relationship": "No explicit numerical mapping of citation counts to eventual scientific value is provided; rather, the paper uses citation-based selection to create negative samples (e.g., 99 most-cited NeurIPS papers 2015–2020 and 99 most-cited Nature Medicine over past 15 years) and argues that citation accumulation indicates that an idea has been extensively followed up and thus is currently non-novel.",
            "gap_magnitude": null,
            "temporal_pattern": "Authors assert that human-labeled novelty quickly becomes outdated as ideas become established; validation positive set is drawn from 2024–2025 publications while negatives are older (2015–2020 or earlier), but paper does not quantify exact time-lag dynamics (no half-lives or year-by-year curves).",
            "field_studied": "Computer science (NeurIPS) and Biomedical research (Nature Medicine) as the domains in which this temporal/venue-based ground truth labeling is applied.",
            "field_differences": "The authors acknowledge differences in citation patterns and publication velocities across fields (e.g., computer science vs biomedicine), and cite this as a reason absolute citation/density thresholds are unreliable across domains; they explicitly use venue- and time-based sampling to attempt to create a robust validation set.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": null,
            "correction_mechanism": "The paper proposes a temporal/venue-based automated labeling methodology to scale validation without manual labels; this is not claimed to be perfect but to be more objective and scalable than small manual test sets.",
            "training_distribution_bias": "The authors note that using citation-based or venue-based ground truth can over-simplify the novelty spectrum and that selecting highly-cited older papers as 'non-novel' creates an easier discrimination problem than borderline cases; they acknowledge this as a limitation.",
            "counterexamples": "Authors concede that highly-cited papers can be novel in certain senses (novel + useful) and that citation counts alone can indicate either novelty or utility; they caution that their negative sampling may produce 'too-easy' negatives compared to real-world borderline novelty cases.",
            "study_design": "Ground-truth creation: positives = recent top-venue accepted papers with program-chair novelty comments or recent Nature Medicine 'Article' items (2024–2025); negatives = top-cited older papers from the same venues (NeurIPS 2015–2020 top 99 cited; Nature Medicine top 99 cited over past 15 years).",
            "uuid": "e2134.2"
        },
        {
            "name_short": "LLM-based peer-judgment",
            "name_full": "LLM-as-judge (with/without literature search, with guideline/tournament)",
            "brief_description": "Use of large language models to evaluate novelty either directly (with prompts/guidelines/tournament comparators) or augmented with retrieved literature (10 most relevant papers) to determine overlap with given idea; performance is highly sensitive to provision of external literature and domain.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_metric_type": "peer-review style scores produced by LLMs (chain-of-thought with NeurIPS guideline, Swiss tournament scoring, or binary novel/non-novel with literature search)",
            "ground_truth_measure": "Compared against the paper's automated temporal/venue-based validation labels (recent top-venue = novel, highly-cited older = non-novel).",
            "novelty_transformation_measure": "LLM judgment protocols: (1) LLM + literature search: feed 10 most relevant papers and ask for overlap assessment (binary 0/1); (2) LLM + review guideline: use NeurIPS review rubric to output an 'Overall' score; (3) LLM tournament: Swiss-system pairwise comparisons on standardized proposals.",
            "quantitative_relationship": "LLM performance varies strongly by configuration and field: LLM with no literature shows AUROC ≈ 0.5 (random); LLM + literature search achieves AUROC ≈ 0.8 on NeurIPS (CS) but degrades to ≈ 0.6 on Nature Medicine and cross-domain; Sonnet-3.7 and Deepseek-r1 outperform GPT-4o when external knowledge is provided.",
            "gap_magnitude": "Field sensitivity: example absolute AUROC drop ≈0.2 when moving from CS (≈0.8) to Biomed (≈0.6) for LLM+search, indicating substantial proxy-truth gap that depends on domain and model internals.",
            "temporal_pattern": null,
            "field_studied": "Evaluated on NeurIPS (computer science) and Nature Medicine (biomedicine) test sets and Mixed cross-domain.",
            "field_differences": "LLM judgments depend heavily on the model's internal knowledge and are better in computer science (where model knowledge likely overlaps training data) than in biomedicine; adding retrieved external literature improves performance but does not eliminate field disparities.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "Reported AUROCs: LLM w/o literature ≈0.5; LLM + literature search ≈0.8 (NeurIPS) and ≈0.6 (Nature Medicine/Mixed). Sonnet-3.7 with guideline/tournament performs poorly without literature; performance variability across repeated runs reported for LLMs (standard deviation included in table D.1).",
            "correction_mechanism": "Providing external literature (top-10 retrieved papers) to the LLM substantially improves judgments; the authors propose using RND as a reward signal to train reasoning models in RL frameworks to bias LLMs toward novel idea generation, but no quantitative reduction in LLM proxy-truth gap via this mechanism is reported here.",
            "training_distribution_bias": "Authors note LLMs' sensitivity to internal knowledge (models trained on corpora biased toward certain domains) and cite examples of degraded performance in biomedicine; they suggest this is a source of bias but do not provide quantitative measures of the training-data distribution effect.",
            "counterexamples": "When given retrieved literature, some LLMs (Sonnet-3.7, Deepseek-r1) reach high in-domain AUROC (~0.8) comparable to RND, showing LLMs can succeed when external context and model capability align with domain.",
            "study_design": "LLM baselines were evaluated by running each LLM-based procedure three times (to capture output variability) and averaging results; methods included LLM-only (with guideline), LLM+tournament (Swiss system), and LLM+literature (feed top-10 retrieved papers). Evaluated against the same temporal/venue-based test sets used for other baselines.",
            "uuid": "e2134.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation",
            "rating": 2
        },
        {
            "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 1
        },
        {
            "paper_title": "Incentivizing reasoning capability in llms via reinforcement learning (Deepseek-r1)",
            "rating": 1
        }
    ],
    "cost": 0.016507249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ENABLING AI SCIENTISTS TO RECOGNIZE INNOVATION: A DOMAIN-AGNOSTIC ALGORITHM FOR ASSESSING NOVELTY
10 Mar 2025</p>
<p>Yao Wang wang-yao24@mails.tsinghua.edu.cn 
Department of Automation
Tsinghua University</p>
<p>Mingxuan Cui mingxuan.cui@mail.nankai.edu.cn 
Nankai University</p>
<p>Arthur Jiang arthursjiang@gmail.com 
Yidu Technology 
ENABLING AI SCIENTISTS TO RECOGNIZE INNOVATION: A DOMAIN-AGNOSTIC ALGORITHM FOR ASSESSING NOVELTY
10 Mar 2025014BE4AA0DE10575620E8518F64631CFarXiv:2503.01508v2[cs.AI]
In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery.This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea's local density with its adjacent neighbors' densities.We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment.Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820)and biomedical research (AUROC=0.765)domains.Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s.0.597) on cross-domain evaluation.These results validate RND as a generalizable solution for automated novelty assessment in scientific research.</p>
<p>Introduction</p>
<p>In the pursuit of Artificial General Intelligence (AGI), automating scientific research and knowledge discovery presents both a formidable challenge and an exciting opportunity, as it will be groundbreaking to expand the boundaries of human knowledge by leveraging scalable computing resources.Therefore, as the capabilities of large language models (LLMs) continue to improve, researchers have started to explore their use in automating various aspects of the research process, including the generation of novel ideas, as exemplified by the AI scientist concept (Lu et al. [2024]).</p>
<p>A key task for any AI-based scientist is the generation of novel research ideas, a task traditionally performed by human scientists during their brainstorming phase.While LLMs have shown promise in generating a large pool of ideas quickly and cost-effectively, akin to the initial stages of human research, the real challenge lies in evaluating these ideas for their novelty.Traditionally, novelty in scientific research has been assessed through peer review and expert evaluations, where domain specialists judge the originality of an idea based on their experience and familiarity with existing literature.However, such assessments are inherently subjective, time-consuming, and inconsistent across reviewers.Moreover, as the volume of scientific output grows exponentially, manual novelty assessment struggles to keep pace.Automated methods are therefore crucial for filtering out redundant ideas and promoting genuinely innovative directions.</p>
<p>Existing approaches primarily fall into two categories: (1) leveraging large language models (LLMs) as judges and (2) using absolute local density-based novelty metrics.</p>
<p>The most straightforward approach is to use LLMs as judges to evaluate the novelty of ideas.Si. et al. adopted a Swiss system tournament design to evaluate ideas by using LLM as judge (Si et al. [2024]), which was further applied in Nova: An Iterative Planning and Search Approach to enhance Novelty and Diversity of LLM-Generated Ideas (Hu et al. [2024]).To improve LLM's accuracy of judgment, Lu include NeurIPS review guideline and Semantic Scholar API as tools (Lu et al. [2024], Su et al. [2024]): the NeurIPS review guideline was served as both chain-of-thoughts prompts and few-shot examples, while search API enabled LLM to search top 10 relevant papers to determine the novelty of given idea.</p>
<p>An alternative approach relies on the absolute local density in semantic embedding space to measure novelty.Su. et al. introduced the concept of Historical Dissimilarity and Contemporary Dissimilarity, calculated as the average Euclidean distance (local density) between a generated abstract's embedding and the embeddings of the five most similar abstracts from a historical database and a contemporary database, respectively.In combination with the citation-based Contemporary Impact metric, they developed the Overall Novelty (ON) metric (Su et al. [2024]).Their study validated that ON correlates well with human-labeled novelty, demonstrating its effectiveness as a novelty assessment measure.</p>
<p>Though the aforementioned approaches provide potential solutions on idea novelty evaluation, there are major challenges when considering practical issues.</p>
<p>First, the reliability of using-LLM-as-judge remains questionable, even with external knowledge or tools.Studies have demonstrated that auto-regressive LLMs like GPT-4o (Hurst et al. [2024]) produce outputs sensitive to input perturbations (Zhuo et al. [2023], Singh et al. [2024]).In novelty assessment specifically, this means identical research ideas phrased differently might receive contradictory novelty ratings.While recent reasoning models, such as DeepSeek-r1 (Guo et al. [2025]) and Sonnet-3.7 (Anthropic [2025]), show improved reasoning capabilities, their reliability for scientific novelty judgment remains unvalidated.</p>
<p>Second, the absolute local density-based metric from (Su et al. [2024]) shows significant limitations across diverse research contexts.By relying on just 5 most similar abstracts from history and contemporary databases as reference points, the metric's validity becomes highly dependent on arbitrary choices: the size of paper collections, the temporal boundaries defining 'history' versus 'contemporary,' and the selection criteria for inclusion.Different research domains also exhibit varying citation patterns, publication velocities, and semantic densities, which undermined the metric's generalizability across research domains.</p>
<p>Last but not least, the validation methodology used to assess novelty evaluators themselves was significantly lacking.In most cases, the validation of novelty metric depends on small test sets manually labeled by human experts within a specific domain (Hu et al. [2024], Lu et al. [2024], Su et al. [2024], Si et al. [2024]).Such validations are difficult to scale across different research areas, as they are often highly specialized and tailored to particular fields of study.What's worse, manually produced novelty labels rapidly become outdated: as scientific research advances continuously, ideas labeled "novel" today quickly become established knowledge, rendering static human-labeled validation sets increasingly inaccurate over time.</p>
<p>To address these challenges, in this paper, we establish comprehensive semantic embedding databases for novelty assessment.These databases incorporate over 30 million publications from two distinct domains: Pubmed, the leading biomedical literature search engine with nearly 36 million articles (Jin et al. [2024]), and Arxiv, which contains more than 2.3 million scholarly articles across eight subject areas (Cornell Tech [2023]).</p>
<p>Based on these resources, we propose the Relative Neighbor Density (RND) algorithm, which measures novelty by analyzing the distribution patterns of semantic neighbors rather than simple absolute local density.This approach proves more reliable than LLM-based judgments and more generalizable than existing absolute local density-based metrics across different research domains.We also develop an automated validation methodology that leverages temporal publication patterns to evaluate novelty without requiring expert manual labeling.Our extensive evaluations using test sets from computer science, biomedical science, and cross-domain contexts demonstrate that our proposed algorithm maintains accuracy within specific domains while scaling effectively across diverse research areas.</p>
<p>Our main contributions are:</p>
<p>• A novel neighbor density-based Relative Neighbor Density (RND) algorithm for assessing research idea novelty that is robust across domains, which holds domain-invariant property</p>
<p>Related Works</p>
<p>Assessing the novelty of research ideas is a fundamental challenge in automating scientific discovery.Various approaches have been explored in recent years, leveraging Large Language Models (LLMs) and semantic similarity measures to evaluate idea originality.This section reviews existing methods, highlighting their strengths and limitations.</p>
<p>LLMs for Novelty Assessment</p>
<p>Recent work has demonstrated promising results in using LLMs as autonomous judges for research novelty.Si et al. (Si et al. [2024]) evaluated this approach using ICLR submissions, converting them into standardized project proposals and conducting pairwise comparisons between accepted and rejected papers(Table 7).Their Swiss tournament system iteratively paired proposals based on accumulated scores, with Claude-3.5-Sonnetachieving 71.4% accuracy in predicting paper acceptance.As a control measure, they included human expert reranking, which revealed notable discrepancies between automated and human judgments.</p>
<p>Lu et al. (Lu et al. [2024]) expanded this concept with their AI Scientist framework, integrating idea generation, evaluation, and refinement.Their system employs chain-of-thought prompting and external knowledge retrieval via Semantic Scholar API to enhance assessment quality.While showing promise in matching human-level performance, these LLM-based approaches face fundamental challenges in reliability and consistency, as highlighted by studies showing their sensitivity to input variations (Zhuo et al. [2023], Singh et al. [2024]).</p>
<p>Absolute Local Density-based Metrics</p>
<p>An alternative approach focuses on semantic local density to evaluate novelty.Su et al. (Su et al. [2024]) used the Historical Dissimilarity (HD), which is the average Euclidean distance between the generated abstract embedding and embeddings of the 5 most similar abstracts in historical literature base.We denote it as "Absolute Local Density" because the average distance is a metric of local density, and they use the value of density directly.In addition to HD, they also used Contemporary Dissimilarity (CD), which calculate in same algorithm in contemporary literature base, which is also an absolute local density-based metric.</p>
<p>Based on HD and CD, they developed Overall Novelty (ON) metric as below,
ON = HD × CI CD (1)
The main challenge of local density-based metric is that the density values vary across different domains.In case of evaluating ideas from mixed research domains, the variance would cause a severe degrade in the accuracy.</p>
<p>Validating Novelty Metrics</p>
<p>A critical limitation in existing research is the lack of scalable validation methodologies.Current approaches (Lu et al. [2024], Su et al. [2024], Hu et al. [2024], Si et al. [2024]) typically rely on small-size literature database and manually labeled test set created by domain experts.For instance, Su et al. (Su et al. [2024]) constructed an "ecosystem" for computer science (CS) using information extracted from 85,217 papers-a dataset that represents only a small fraction of the CS literature available on platforms like arXiv.While their analysis demonstrated promising correlations between novelty scores and human labels across 100 manually evaluated abstracts, the methodology's reliance on domain-specific expertise significantly constrains its generalizability, which is echoed by reviewer's comments that only validating CS is "relatively simple" (Openreview Reviewer 2X9t [2025]).</p>
<p>Furthermore, the scalability challenge persists across all existing frameworks.The creation of test sets currently requires expert labeling, making large-scale evaluation prohibitively resource-intensive.This limitation underscores the need for automated approaches to test set creation that maintain fidelity while eliminating the requirement for extensive manual labeling.</p>
<p>Method</p>
<p>Problem Description</p>
<p>Given a set of ideas I,
I = {idea i }, i ∈ <a href="2">1, N </a>
Where idea i is a sequence of words or characters in nature language.N ≥ 1 represents the number of ideas whose novelty needs to be assessed.</p>
<p>The objective is to design a mapping F from idea space to a score in real value space
F (idea i ) = score i , where idea i ∈ I, score i ∈ R (3)
The novelty score score should be monotonic, meaning that for any two ideas idea i and idea j , if idea i is more novel than idea j , then their corresponding scores must satisfy:
∀ idea i , idea j ∈ I, idea i ≻ idea j ⇒ F (idea i ) &gt; F (idea j )(4)
where idea i ≻ idea j denotes that idea i is considered more novel than idea j based on a given novelty criterion.</p>
<p>Semantic Embedding &amp; Literature Database</p>
<p>Each published literature's abstract, which is also a sequence of words or characters in natural language, is denoted as
a j .
The semantic embedding model is a mapping function G , which maps ideas and abstracts into embedding vectors:
G (idea i ) = v i , where v i ∈ R dims ,(5)G (a j ) = v j , where v j ∈ R dims (6)
Thus, the preprocessed literature semantic database is represented as a set A:
A = {(a j , v j ) | j ∈ [1, M ]}(7)
We collected 36 million academic articles from the PubMed Download API (National Library of Medicine [2025]) and 2.6 million papers from the ArXiv dataset (Cornell University [2025]).Among all fetched documents, only those with both a non-empty title and abstract were considered valid for the experiment, resulting in 25,360,114 papers from PubMed and 2,643,057 papers from ArXiv.</p>
<p>For each paper, two semantic embedding vectors were generated-one from its title and another from its abstract-using the M3-Embedding model Chen et al. [2024].The embedding vector dimension, denoted as dims, is 1024.All texts and embedding vectors were stored in Elasticsearch Version 8 for efficient retrieval.</p>
<p>Algorithm</p>
<p>For each idea idea i and its embedding v i , we first find its P nearest neighbors using k-Nearest Neighbors (KNN) search:
{v 1 , v 2 , . . . , v P } = KN N (v i , P, A) (8) where v j is the j-th nearest neighbor of v i , j ∈ [1, P ].
For the idea itself v i and each neighbor v j , the neighbor density (ND) is defined as below
N D = 1 Q Q k=1 d(v, v k )(9)
where v k denotes the k-th nearest article's embedding vector in the literature corpus A and d(•, •) is the cosine distance between two vectors.</p>
<p>We define the set S i that contains the neighbor density values of idea i 's neighbors:
S i = {N D j | j ∈ [1, P ]}(10)
Finally, we compute the novelty score score i for idea i as:
score i = |{N D ∈ S i | N D ≤ N D i }| |S i | × 100 (11)
The selection of values of P and Q is a trade-off between reliability of estimation and other cost.A lower values of P will cause biased estimation of novelty score; while higher value of P will increase computing cost in O(P • Q) complexity.Similarly, lower Q will also cause unreliable estimation of local density.Meanwhile, a higher Q value will not only be computational costly, but also sacrifice sensitivity as ND converges to its expectation as Q increase, losing information local density.Refer to Appendix A for detailed analysis on the effects of P and Q.</p>
<p>Based on analysis and empirical experiments, we set P = 100 and Q = 50.</p>
<p>The pseudoscope implement calculation of ND is presented in Algorithm 1; and the pseudoscope of complete algorithm is provided in Algorithm 2.</p>
<p>Algorithm</p>
<p>Validation without Human Labeling</p>
<p>As a novelty evaluation algorithm, the most challenging point in past research is to find a reliable labeled test set to evaluate the algorithm.Therefore, we propose a new method to construct a convincing test set instead of relying on human experts to annotate it.</p>
<p>For the positive samples (a.k.a novel ideas) in the test set, we select recent articles from top journals or conferences.For the negative samples (a.k.a.non-novel ideas), highly cited articles published before the last few years were selected, also from the research domain's top journals or conferences.The fundamental principles behind such methodology were: high-quality novel ideas are more likely to be published in recent issues and top journals or conferences; while after time passes, the at-the-time novel ideas were more likely to attract attention and related works, thus become non-innovative at present.</p>
<p>In this way, we can make positive and negative samples have a more obvious difference in novelty in a relatively objective and recognized way.We have two test sets: NeurIPS, which represents the most advanced research results in the field of computer science, and Nature Medicine, which represents the most cutting-edge papers in the medical field.The sample year distribution of the test sets can be found in Table 1 NeurIPS test set: The initial corpus consists of papers that are Accept (oral) or Accept (spotlight) by Program Chairs at the 2024 NeurIPS conference, which represents the latest research results in computer science.Furthermore, we select articles from the initial corpus that explicitly mention that the papers have obvious novelty in the comments of Program Chairs to form the positive samples of the NeurIPS test set.The comments and decision information of Program Chairs can be obtained on the OpenReview.netwebsite.At the same time, we use the Semantic Scholar API to obtain the 99 most cited papers published in the NeurIPS conference from 2015 to 2020 to form the negative samples of the test set.The titles of all samples are presented in Table 4 Nature Medicine test set: The positive samples of the Nature Medicine test set consist of articles classified as "Article" type, published in Nature Medicine from August 2024 to February 2025, according to the classification on the nature.comwebsite.Articles related to phase 2 or phase 3 trials were excluded.And we used the same method as the negative samples of the NeurIPS test set to obtain 99 articles of Nature Medicine with the highest citation count in the past 15 years as negative samples of the test set.The titles of all samples are presented in</p>
<p>Baseline</p>
<p>To evaluate our algorithm, we selected all existing novelty assessment algorithms as baselines, categorized into two groups: LLM-based and non-LLM-based.Non-LLM-based algorithms, including Relative Neighbor Density(Ours), Historical Dissimilarity(HD), and Overall Novelty(ON), rely solely on literature search and mathematical calculations.Since the output of the literature search for the same query remains consistent, we conducted a single test to assess the algorithm's performance.In contrast, for LLM-based algorithms, due to the inherent variability of LLM outputs, we ran three tests for each algorithm, calculated the average result, and included the standard deviation in the table.The full experimental results of the LLM-based method are provided in Table D.1.</p>
<p>For all methods, we use the abstracts of the papers in the test set as "ideas" for testing.</p>
<p>Historical Dissimilarity: Identify the five most relevant papers based on their embeddings and compute the Euclidean distance between the embedding of the idea and the embeddings of the abstracts of these five papers.The final novelty score is obtained by averaging these distance values.</p>
<p>Overall Novelty: The historical database contains papers from 2011 to 2021, and the contemporary database contains papers from 2021 to 2025.The score calculation method refers to equation 1.</p>
<p>LLM + literature search: Provide LLM with the titles and abstracts of the 10 most relevant papers to the given idea.The model then assesses whether the core concepts of these papers significantly overlap with the idea(Table 8).If substantial overlap is detected, the idea is deemed non-novel and assigned a score of 0. If no significant overlap is found, the idea is considered novel and assigned a score of 1.</p>
<p>LLM with guideline: Utilize the NeurIPS 2024 review guidelines to assist LLM in evaluating the novelty of ideas(Table 6).The final score is determined based on the "Overall" score provided in the review assessment.</p>
<p>LLM with tournament: First, the idea is transformed into the Standardized Project Proposal format(Table 7).Next, the novelty of all standardized ideas is assessed using the Swiss tournament method, where ideas are iteratively compared in a structured competition.Finally, each idea is assigned a score based on the number of wins it accumulates throughout the tournament.</p>
<p>Accuracy Evaluation</p>
<p>As shown in Table 2, our enhanced neighbor density-based novelty measurement algorithm outperforms all baseline models on both the Nature Medicine and Mixed test sets, while also demonstrating strong performance on the NeurIPS test set.2: Validation of Different methods, measured by AUROC.HD: Historical Dissimilarity (section 2.2).ON: Overall Novelty (section 2.2).LLM + literature search: supplementing LLM with 10 relevant papers, which were searched by idea's embedding from our literature database using semantic embedding.LLM with guideline: using NeurIPS 2024 review guideline to help LLM judge the novelty of ideas, which is not applicable to Nature Medicine.Therefore, the results of Nature Medicine and Mixed are marked as not applicable.LLM with tournament: a Swiss system tournament design to evaluate ideas by using LLM as judge.</p>
<p>Model</p>
<p>By comparing the results of various LLM-related algorithms, we observe a key similarity between Sonnet-3.7 with guideline and Sonnet-3.7 with tournament: both methods provide very limited external knowledge to the LLM, with no existing literature being fed into the model.As a result, the model's judgment of novelty is highly inaccurate.In contrast, the LLM + literature search method inputs the 10 most relevant papers to the idea, significantly improving the accuracy of the model's judgment.Moreover, the accuracy of the LLM + literature search method is much higher in the field of computer science compared to the field of biomedicine, highlighting the significant impact of the model's internal knowledge on the judgment outcomes, even with the addition of external knowledge.Additionally, Sonnet-3.7 (Anthropic [2025]) and Deepseek-r1 (Guo et al. [2025]) show much higher AUROC scores than GPT-4o, indicating that when external knowledge is provided, the performance of the inference model greatly surpasses that of the autoregressive model.However, we observed that the Historical Dissimilarity (HD) metric closely matched the performance of our proposed method on the Nature Medicine and NeurIPS test set.In contrast, on the Mixed test set, there was a significant disparity, with our method achieving an AUROC of 0.795, while HD only reached 0.362.This prompted us to further investigate the underlying reasons for this substantial difference.</p>
<p>The score distributions provided by the Historical Dissimilarity (HD) metric on the NeurIPS and Nature Medicine test set, as shown in Figure 1, are markedly different.This disparity implies that some negative samples from NeurIPS would be evaluated as more novel than some positive samples from Nature Medicine under this evaluation system, highlighting HD's limited generalization ability across domains.In contrast, the score distributions of our method on both test sets are nearly identical, indicating that our scores are absolute and unaffected by the specific discipline or field.This means that our scores are universally comparable across domains.This result underscores the robust cross-domain evaluation capability of our method, making it applicable for researchers in any field.</p>
<p>Sensitivity Study</p>
<p>Sensitivity of Hyper-Parameter</p>
<p>Since our method has two key parameters: P and Q, we conducted experiments to understand the contribution of each parameter to our algorithm.As illustrated in the left panel of Figure 2, the AUROC on each test set increases as P grows.However, when P &gt; 50, the improvement in AUROC becomes marginal compared to the significant gain observed when increasing P from 10 to 50.This suggests that the marginal benefit of further increasing P diminishes while simultaneously incurring substantial computational costs, given that the algorithm's time complexity is O(P • Q).Additionally, the poor performance observed when P = 10 can be attributed to the biased estimation of novelty scores when P is too small, a phenomenon influenced by multiple factors.For a more detailed explanation, please refer to Appendix A.1.</p>
<p>The right panel of Figure 2 demonstrates that when P remains constant, both excessively small and large values of Q negatively impact the algorithm's performance.This is due to the inaccuracy in local density estimation when Q is too small and the significant reduction in algorithm sensitivity when Q is too large.For further details, please refer to Appendix A.</p>
<p>Sensitivity of Design</p>
<p>In our Relative Neighbor Density algorithm, the notion "relative", i.e. comparing idea's local density with its neighbor's local densities, plays an important role.Moreover, other distance metric, such as Euclidean distance, could also be used in our algorithm.To understand the sensitivity of the current design, we conducted experiments by changing the design of the "relative" notion, and distance metric.The result presented in Table 3</p>
<p>Case Study</p>
<p>We visualize the neighbors of both a novel and a non-novel idea in the embedding vector space to demonstrate the superiority of our algorithm.Figures 3 and 4 show the visualization results of the embedding vectors of an idea and its neighbors on a two-dimensional plane, after dimensionality reduction using t-Distributed Stochastic Neighbor Embedding (t-SNE).While t-SNE excels at preserving the local structure of the data, it does not reliably retain the global structure(Van der Maaten and Hinton [2008]).As a result, the distance between the idea and its P adjacent neighbors is not accurately preserved, but distances between these P neighbors and their Q nearest neighbors is well preserved.</p>
<p>We first use Attention is All You Need (Vaswani et al. [2017]), a highly cited article, as a non-novel idea from the current perspective.Figure 3 clearly illustrates that there is a dense cluster of neighbors around the idea.In contrast, the neighbors around the idea's P neighbors are relatively few and sparse.</p>
<p>Next, we use Evaluating the World Model Implicit in a Generative Model (Vafa et al. [2025]), an article considered highly novel by the NeurIPS 2024 Program Chairs, as an example of a novel idea, based on their comments (openreview [2025]).In Figure 4, it is evident that the idea's local neighbor density is much sparser than its P nearest neighbors.</p>
<p>The experimental results demonstrate that the novelty of an idea is reflected in the local structure of the most similar documents to the idea within the embedding vector space, which supports the correctness of our algorithm in principle.Furthermore, a key difference between Figures 3 and 4 is that Figure 3 shows multiple neighboring clusters centered around the idea's P nearest neighbors, suggesting that the vector density of the two images in the embedding space is notably different.This highlights that the novelty of an idea cannot be determined solely by local density of the idea but must also take into account the vector density surrounding the idea.This is also clearly reflected in the experimental results for the Mixed test set in Table 3.</p>
<p>Discussion</p>
<p>In this work, we proposed a novel neighbor density-based metric for assessing research idea novelty, addressing the limitations of LLM judgment and absolute local density-based metrics.By leveraging large-scale literature embeddings from both biomedical sciences and computer science, our approach ensures robust reliability and cross-domain generalizability.Additionally, we introduced a scalable validation framework that eliminates reliance on expert labeling, enabling objective and reproducible novelty assessment.</p>
<p>Why a Non-LLM Novelty Assessment Algorithm is Necessary?</p>
<p>Assessing the novelty of a research idea is inherently difficult, subjective, and resource-intensive.While LLMs have the potential to assist in this process, their effectiveness is limited by the challenges outlined in the Introduction.Our experiments (see Table 2) highlight these issues: without an integrated search tool, even the most advanced reasoning models' performance was comparable to random guessing (AUROC =0.5).When a search tool was introduced, Sonnet-3.7 achieved similar accuracy on the NeurIPS test set (AUROC =0.8) but experienced significant degradation (AUROC =0.6) on both the Nature Medicine and cross-domain test sets.In contrast, our proposed RND algorithm can produce more reliable and consistent results, as seen in Table 2. Our algorithm is better at distinguishing genuinely novel ideas from the large pool of candidates from mixing research domains (AUROC =0.78 v.s Other's AUROC&lt;=0.6).Such cross-domain novelty assessment capability is crucial to AI scientist, as more and more innovation happened in inter-discipline of research domains.</p>
<p>Recent advancements in reinforcement learning for reasoning models, such as those demonstrated in Deepseek-R1 (Guo et al. [2025]), suggest the potential of rule-based reward systems to guide model development.Our RND algorithm could serve as a sophisticated rewarding mechanism, potentially enhancing reasoning model's capabilities in generating novel scientific ideas and advancing the role of AI in scientific innovation.</p>
<p>Accuracy in Each Domain</p>
<p>In</p>
<p>Domain-invariant Accuracy</p>
<p>However, when tested in the cross-domain test set (the Mixed test set), which includes ideas from both computer science and biomedicine, the performance of HD significantly degraded, with its AUROC dropping to 0.362.In contrast, the AUROC of our proposed algorithm remained robust at 0.795, similar to its performance in the single-domain test sets.</p>
<p>As demonstrated in the Table 3, the relative position of idea's local density among all of its neighbor's local density is crucial for comparing novelty across different domains.</p>
<p>We argue that RND algorithm hold domain-invariant property, i.e. the distribution of novelty scores produced by RND is identical regardless of the tested domain, which explained why our relative density-based approach succeeds in cross-domain scenarios.According to the mathematical reasoning in Appendix A.3, we concluded the distribution of novelty score S is only subject to P (the number of neighbors considered); thus it is invariant to the validation domain.Furthermore, in Figure 1 (right panel) and 5, the actual distribution of scores echoed the theoretical analysis.Such domain-invariant property is crucial for conducting multi-disciplinary scientific research, where ideas from diverse fields must be compared and evaluated effectively.</p>
<p>Why Validation Methods Differ Between Novel and Non-Novel?</p>
<p>When building our test set, an obvious approach might be to use symmetrical sources -for example, using accepted NeurIPS papers as novel samples and rejected NeurIPS papers (specifically those rejected for lacking novelty) as non-novel samples.However, this approach presents significant limitations.Firstly, very few top-tier venues publicly release review comments with explicit novelty assessments, making such data scarce and difficult to generalize across domains.Secondly, papers may be rejected for "lack of novelty" due to incremental advances or methodological similarities, even when addressing previously unexplored topics.</p>
<p>Instead, our definition of novelty relies on how extensively similar ideas have been studied in the literature.Following this definition, we selected highly-cited papers from recent years as our non-novel samples, as these papers represent ideas that have been thoroughly explored and extended by numerous subsequent works.While high citation count itself can indicate either novelty or utility, papers that are both recent and highly-cited typically represent research areas that have quickly become crowded with similar work, making the original contributions less novel by our working definition.Further details on our sampling methodology can be found in Section 4.1.</p>
<p>Limitations &amp; Future Work</p>
<p>Several limitations of our work warrant further exploration.</p>
<p>First, the algorithm relies heavily on large-scale literature databases with semantic embeddings.Biases in the literature database could potentially influence novelty assessments, especially if certain areas of research are underrepresented or if publication biases exist within fields.</p>
<p>Second, the algorithm's performance is also dependent on the quality of semantic embeddings for representing complex scientific concepts.While the M3 model demonstrated effectiveness, domain-specific fine-tuning could potentially improve performance.Future work should investigate specialized embedding models for scientific literature that better capture the complex semantics of scientific abstracts, particularly for technical terminology and methodological nuances.</p>
<p>Third, our validation methodology, while avoiding the need for expert labeling, relied on non-novel samples that may be too easily distinguishable from novel ones.By using historical highly-cited papers as non-novel examples, rather than borderline cases such as recently rejected papers or incremental work from current journals, we created a simplified assessment scenario compared to the subtle distinctions scientists face in real research settings.However, the fact that none of the tested algorithms achieved saturated AUROCs even in this relatively straightforward scenario demonstrates the fundamental challenge of novelty assessment and validates our comparative analysis.</p>
<p>Looking ahead, we envision several promising directions for future work:</p>
<ol>
<li>
<p>Integration with AI Research Workflows: Incorporating our novelty evaluation algorithm into end-to-end AI scientist workflows would enable autonomous research ideation and evaluation.This integration would allow AI systems to independently generate research hypotheses, assess their novelty using our domain-invariant RND algorithm, and prioritize the most promising directions for further investigation.Such integration could accelerate scientific discovery by efficiently navigating complex multi-disciplinary research landscapes where human intuition about novelty is often limited.</p>
</li>
<li>
<p>Enhancing Reasoning Model: As highlighted in our discussion, current reasoning models struggle with reliable novelty assessment across domains.We propose utilizing our RND algorithm as a sophisticated reward mechanism within reinforcement learning frameworks for training reasoning models for AI research.By providing domain-invariant novelty signals during training, we could potentially guide models to generate more innovative scientific ideas while maintaining scientific validity.</p>
</li>
</ol>
<p>These advancements would further enhance AI's role in scientific research by accelerating idea generation, refining research hypotheses, and potentially uncovering interdisciplinary connections that might otherwise remain unexplored.</p>
<p>Appendix A Algorithm Analysis</p>
<p>A.1 Effects of Parameter P</p>
<p>The novelty score is computed as
score i = |{N D ∈ S i | N D ≤ N D i }| P × 100,(12)
where S i = {N D j | j = 1, 2, . . ., P } is the set of neighbor densities for the P nearest neighbors of the idea i, and N D i is the neighbor density for idea i itself.</p>
<p>Empirical Cumulative Distribution Function (ECDF) Interpretation</p>
<p>Define the empirical cumulative distribution function (ECDF) for the set S i as
F P (x) = 1 P P j=1 1 {N Dj ≤x} ,
where 1 {N Dj ≤x} is the indicator function that is 1 if N D j ≤ x and 0 otherwise.Then, by definition, the novelty score can be written as score i = 100
• F P (N D i ). (13)</p>
<p>Consistency of the ECDF</p>
<p>Let F (x) be the true cumulative distribution function of the neighbor densities (assumed to be i.i.d.samples from a distribution F ).By the Glivenko-Cantelli theorem, the ECDF F P (x) converges uniformly to F (x) as P → ∞:
sup x |F P (x) − F (x)| P →∞ − −−− → 0.
Thus, for a sufficiently large P , we have
F P (N D i ) ≈ F (N D i ).
(14) This shows that the score, being proportional to F P (N D i ), converges to 100 • F (N D i ), which is the true quantile of N D i in the distribution of neighbor densities.</p>
<p>Variance and Sensitivity with Finite P</p>
<p>For a finite sample size P , F P (N D i ) is a random variable whose variance depends on P .Under the assumption of i.i.d.sampling,
Var(F P (N D i )) = F (N D i )(1 − F (N D i )) P .
Thus, the standard deviation is proportional to 1 √ P .This quantifies that:</p>
<p>• Smaller P : The variance Var(F P (N D i )) is larger, leading to a noisier (less reliable) estimation of the quantile, and hence of the novelty score.• Larger P : The variance decreases, yielding a more accurate estimation of the true quantile F (N D i ).</p>
<p>The novelty score becomes less sensitive to random fluctuations when P is large, as the empirical quantile is a better estimator of the true quantile.</p>
<ol>
<li>Discreteness of the Score for Small P When P is small, the possible values of F P (N D i ) are discrete, specifically:
F P (N D i ) ∈ 0, 1 P , 2 P , . . . , 1 .
For instance, if P = 1, then F 1 (N D i ) can only be 0 or 1, corresponding to a score of either 0% or 100%.This coarse granularity can result in a biased or uninformative measure of novelty.As P increases, the steps 1 P become finer, allowing the score to capture more subtle differences in the density distribution.</li>
</ol>
<p>Conclusion</p>
<p>The parameter P affects the final novelty score in two major ways:</p>
<ol>
<li>Accuracy: As P increases, the empirical cumulative distribution F P (x) better approximates the true cumulative distribution F (x), leading to a more accurate quantile estimate F (N D i ). 2. Variance: The variance of the estimate F P (N D i ) is proportional to 1 P .Thus, a larger P reduces the variability of the score, making it less sensitive to random noise.In summary, a higher P leads to a more robust and sensitive measure of novelty, while a smaller P results in a discrete and noisier estimate.</li>
</ol>
<p>A.2 Effects of Parameter Q</p>
<p>The neighbor density (ND) is given by:
N D = 1 Q Q k=1 d k ,(15)
where
d k = d(v, v k )
represents the distance between the point v and its k-th nearest neighbor.</p>
<p>Assuming that d k are independent and identically distributed (i.i.d.) random variables with mean E[d k ] = µ d , we compute the expectation of ND:
E[N D] = E 1 Q Q k=1 d k = 1 Q Q k=1 E[d k ] = 1 Q Qµ d = µ d .(16)
The variance of ND is given by:
Var(N D) = Var 1 Q Q k=1 d k . (17)
Using the property that the variance of the mean of Q i.i.d.random variables is:
Var 1 Q Q k=1 d k = 1 Q 2 Q k=1 Var(d k ).(18)
Since each d k has variance σ 2 d , we obtain:
Var(N D) = Qσ 2 d Q 2 = σ 2 d Q .(19)</p>
<p>A.2.1 Interpretation of Variance Scaling</p>
<p>The derived formula:
Var(N D) = σ 2 d Q (20)
shows that:</p>
<p>• As Q increases, the variance of ND decreases.</p>
<p>• Specifically, variance scales inversely with Q, meaning that larger Q results in a more stable estimate of ND.</p>
<p>• When Q → ∞, Var(N D) → 0, indicating that ND converges to its expected value µ d , which would cause lost of information on local density.• For small Q, ND exhibits higher variability, making it more sensitive to local fluctuations.</p>
<p>A.3 Domain-Invariant</p>
<p>A.3.1 Theoretical Analysis</p>
<p>Consider a test set in which each idea is assigned a neighborhood density defined as
N D = 1 Q Q i=1 d(idea, a i ),(21)
where a i denotes the ith nearest article in the literature corpus and d(•, •) is the cosine distance.</p>
<p>Let F (x) be the cumulative distribution function (CDF) of the neighborhood densities in the literature corpus.The percentile score for an idea is then defined by S = F (N D).</p>
<p>(22) By the probability integral transform, if N D is drawn from a distribution with CDF F (x), then S ∼ U(0, 1).</p>
<p>(
)23
In practice, F (x) is estimated empirically using P articles from the neighborhood.The empirical CDF is given by
FP (x) = 1 P P j=1 1{N D j ≤ x},(24)
where N D j is the neighborhood density of the jth article, and 1{•} is the indicator function.Since
P j=1 1{N D j ≤ x} ∼ Binomial(P, F (x)),(25)
we have
E[ FP (x)] = F (x) and Var[ FP (x)] = F (x)(1 − F (x)) P .(26)
Now, consider two literature corpora: a medical corpus with density distribution F M (x) and a computer science corpus with density distribution F C (x).For an idea in the test set, define its scores as
ŜM = FM (N D M ) and ŜC = FC (N D C ),(27)
where N D M and N D C are the neighborhood densities computed using the respective corpora.According to equation 26, we have
E[ Ŝ] = F (N D) and Var[ Ŝ] = F (N D)(1 − F (N D)) P . (28)
where F (N D) ∼ U(0, 1), which implies
ŜM d = ŜC .(29)
Furthermore, note that the variance of the empirical estimate FP (x) is solely a function of P :
Var[ FP (x)] = F (x)(1 − F (x)) P .(30)
Thus, when P changes (e.g., P = 50, 100, or 500), the change in variance-and hence the fluctuation in the score-is proportional to 1 P and is independent of the corpus.In other words,
∆ Var ∝ 1 P ,(31)
which holds for both the medical and the computer science datasets.</p>
<p>Therefore, we conclude that:
ŜM d = ŜC and ∆ Ŝ ∝ 1 P . (32)
This establishes that the scoring distributions for the test set are identical across corporas, and the effect of changing P on the score variation is equivalent for all datasets.Task description: You are a researcher who is reviewing a paper that was submitted to a computer science venue.Be critical and cautious in your decision.If a paper is bad or you are unsure, give it bad scores and reject it.Below is a description of the questions you will be asked on the review form for each paper and some guidelines on what to consider when answering these questions.Reviewer guidelines: 1. Summary: Briefly summarize the paper and its contributions.This is not the place to critique the paper; the authors should generally agree with a well-written summary.2. Strengths and Weaknesses: Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: -Originality: Are the tasks or methods new?Is the work a novel combination of well-known techniques?(This can be valuable!)Is it clear how this work differs from previous contributions?-Quality: Is the submission technically sound?Are claims well-supported (e.g., by theoretical analysis or experimental results)?Are the methods used appropriately?Is this a complete piece of work or a work in progress?Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?-Clarity: Is the submission clearly written?Is it well organized?(If not, please make constructive suggestions for improving its clarity.)Does it adequately inform the reader?(Note that a superbly written paper provides enough information for an expert reader to reproduce its results.)-Significance: Are the results important?Are others (researchers or practitioners) likely to use the ideas or build on them?Does the submission address a difficult task in a better way than previous work?Does it advance the state of the art in a demonstrable way?Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?3. Questions: Please list and carefully describe any questions and suggestions for the authors.Think of the things where a response from the author can change your opinion, clarify confusion, or address a limitation.This can be very important for a productive rebuttal and discussion phase with the authors.4. Ethical concerns: If there are ethical issues with this paper, please flag the paper for an ethics review.5. Overall: Please provide an "overall score" for this submission.Choices: -10: Award quality: Technically flawless paper with groundbreaking impact on one or more areas, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical considerations.</p>
<p>A.3.2 Experimental Evidence</p>
<p>-9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area and excellent impact on multiple areas, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.</p>
<p>-8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area or high-toexcellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.</p>
<p>-7: Accept: Technically solid paper, with high impact on at least one sub-area or moderate-to-high impact on more than one area, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.</p>
<p>-6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, and ethical considerations.</p>
<p>-5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation.Please use sparingly.</p>
<p>-4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation.Please use sparingly.</p>
<p>-3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility, and incompletely addressed ethical considerations.</p>
<p>-2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility, and mostly unaddressed ethical considerations.</p>
<p>-1: Very Strong Reject: For instance, a paper with trivial results or unaddressed ethical considerations Provided paper: Here is the paper you are asked to review: {paper} Output: Return a JSON object: <JSON> template <JSON> Role: You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.Task description: You have an idea and you want to check if it is novel or not.I.e., not overlapping significantly with existing literature or already well explored.Be a harsh critic for novelty, ensure there is a sufficient contribution in the idea for a new conference or workshop paper.You will be given the titles and abstracts of the 10 papers most relevant to your idea.Decide a paper idea is novel if after sufficient searching, you have not found a paper that significantly overlaps with your idea.Decide a paper idea is not novel, if you have found a paper that significantly overlaps with your idea.Set your decision to True if you think the idea is novel, set it to False if you think the idea is not novel.Your Idea: This is the idea you need to judge for novelty: {Idea} Top 10 relevant papers: {papers} Output: Return only True or False, dont return any other words.</p>
<p>Figure 1 :
1
Figure 1: Comparison of HD &amp; Our score distributions in different domains.1: In the right panel, the upper and lower bounds of the score exceeded the actual score range ([0, 100]) because of linear interpolation.2: to make the horizontal axis comparable, we scaled the Historical Dissimilarity scores by ×100.</p>
<p>Figure 2 :
2
Figure 2: Comparison of AUROC of RND algorithm with different parameters.left: AUROC with different P value when Q=50. right: AUROC with different Q value when P=100</p>
<p>Figure 3 :
3
Figure 3: Neighbor Distribution of a Non-novel Idea in Embedding Space (t-SNE processed).</p>
<p>Figure 4 :
4
Figure 4: Neighbor Distribution of a Novel Idea in Embedding Space (t-SNE processed).</p>
<p>Figure 5 :
5
Figure 5: Score Distribution of RND algorithm with different P value.</p>
<ol>
<li>
<p>The Consensus Molecular Subtypes of Colorectal Cancer4.Fratricide-resistant CD7-CAR T cells in T-ALL.4. High-performance medicine: the convergence of human and artificial intelligence 5. International multicenter validation of AI-driven ultrasound detection of ovarian cancer.5.Understanding the tumor immune microenvironment (TIME) for effective therapy 6. Donor-derived GD2-specific CAR T cells in relapsed or refractory neuroblastoma.6.Intestinal microbiota metabolism of L-carnitine, a nutrient in red meat, promotes atherosclerosis 7. Single-nucleus chromatin accessibility and transcriptomic map of breast tissues of women of diverse exosomes educate bone marrow progenitor cells toward a pro-metastatic phenotype through MET 9. Echocardiographic screening for heart failure and optimization of the care pathway for individuals with pacemakers: a randomized controlled trial.9. Signatures of T cell dysfunction and exclusion predict cancer immunotherapy response Continued on next page Nature Medicine Test Set Positive Negative 10.Population-based, first-tier genomic newborn screening in the maternity ward.10.Neutralizing antibody levels are highly predictive of immune protection from symptomatic SARS-CoV-2 infection 11.Allogeneic CD5-specific CAR-T therapy for relapsed/refractory T-ALL: a phase 1 trial.11.Ischemia and reperfusion-from mechanism to translation 12. Transplantation of a genetically modified porcine heart into a live human.12. Mechanisms of fibrosis: therapeutic translation for fibrotic disease 13.A multi-modal single-cell and spatial expression map of metastatic breast cancer biopsies across clinicopathological features.13.Metabolite profiles and the risk of developing diabetes 14. ctDNA-based molecular residual disease and survival in resectable colorectal cancer.14.Mechanisms of NAFLD development and therapeutic strategies 15.Antifungal heteroresistance causes prophylaxis failure and facilitates breakthrough Candida parapsilosis infections.15.Inflammasomes: mechanism of action, role in disease, and therapeutics 16.Subcutaneous weekly semaglutide with automated insulin delivery in type 1 diabetes: a doubleblind, randomized, crossover trial.16.Chronic inflammation in the etiology of disease across the life span 17.Combined endurance and resistance exercise training in heart failure with preserved ejection fraction: a randomized controlled trial.17.Mutational Landscape of Metastatic Cancer Revealed from Prospective Clinical Sequencing of 10,000 Patients 18. Multi-omic profiling a defined bacterial consortium for treatment of recurrent Clostridioides difficile infection.18. Antibody responses to SARS-CoV-2 in patients with COVID-19 19.An organotypic atlas of human vascular cells.19.ABT-199, a potent and selective BCL-2 inhibitor, achieves antitumor activity while sparing platelets 20.Lipid profiling identifies modifiable signatures of cardiometabolic risk in children and adolescents with obesity.20.Clinical and immunological assessment of asymptomatic SARS-CoV-2 infections 21.Ferric carboxymaltose for anemia in late pregnancy: a randomized controlled trial.21.Extrapulmonary manifestations of COVID-19 22. Effects of conditional cash transfers on tuberculosis incidence and mortality according to race, ethnicity and socioeconomic factors in the 100 Million Brazilian Cohort.22.A guide to deep learning in healthcare 23.Phenome-wide associations of sleep characteristics in the Human Phenotype Project.23.A global survey of potential acceptance of a COVID-19 vaccine 24.Proteomic signatures improve risk prediction for common and rare diseases.24.The emerging role of lncRNAs in cancer 25.Remotely delivered weight management for people with long COVID and overweight: the randomized wait-list-controlled ReDIRECT trial.25.SARS-CoV-2 Entry Genes Are Most Highly Expressed in Nasal Goblet and Ciliated Cells within Human Airways 26.Sustained effect of prasinezumab on Parkinson's disease motor progression in the open-label extension of the PASADENA trial.26.Gut microbiota metabolism of dietary fiber influences allergic airway disease and hematopoiesis 27.Collaboration between clinicians and visionlanguage models in radiology report generation.27.The immunology of stroke: from mechanisms to translation 28.Oral obeldesivir provides postexposure protection against Marburg virus in nonhuman primates.28.Asthma phenotypes: the evolution from clinical to molecular approaches Continued on next page Nature Medicine Test Set Positive Negative 29.Digital consults in heart failure care: a randomized controlled trial.</p>
</li>
<li>
<p>Modelling the COVID-19 epidemic and implementation of population-wide interventions in Italy 58.Semaglutide in patients with overweight or obesity and chronic kidney disease without diabetes: a randomized double-blind placebo-controlled clinical trial.58.Identification of the molecular basis of doxorubicin-induced cardiotoxicity 59.Intracerebroventricular B7-H3-targeting CAR T cells for diffuse intrinsic pontine glioma: a phase 1 trial.59.New from NPG: Genome-wide association study identifies five new schizophrenia loci 60.AI-based selection of individuals for supplemental MRI in population-based breast cancer screening: the randomized ScreenTrustMRI trial.60.Senolytics Improve Physical Function and Increase Lifespan in Old Age 61.A toolbox for surfacing health equity harms and biases in large language models.61.Subtypes of Pancreatic Ductal Adenocarcinoma and Their Differing Responses to Therapy 62. Partitioned polygenic risk scores identify distinct types of metabolic dysfunction-associated steatotic liver disease.62.A purified membrane protein from Akkermansia muciniphila or the pasteurized bacterium improves metabolism in obese and diabetic mice 63.Multi-omics-based mapping of decidualization resistance in patients with a history of severe preeclampsia.63.The NALP3/NLRP3 Inflammasome Instigates Obesity-Induced Autoinflammation and Insulin Resistance 64.Electronic nudges for sustained influenza vaccination uptake in older adults: the nationwide randomized NUDGE-FLU-2 trial.64.IgE and mast cells in allergic disease 65.A time-stratified, case-crossover study of heat exposure and perinatal mortality from 16 hospitals in sub-Saharan Africa.65. Brown adipose tissue activity controls triglyc-bonemarrow-derived stromal cells to pulmonary alveoli protects against acute lung injury 87.A single-cell atlas of the peripheral immune response in patients with severe COVID-19 88.Determinants of response and resistance to CD19 chimeric antigen receptor (CAR) T cell therapy of chronic lymphocytic leukemia 89.Cancer epigenetics reaches mainstream oncology 90.Real-time tracking of self-reported symptoms to predict potential COVID-19 91.Metformin alters the gut microbiome of individuals with treatment-naive type 2 diabetes, contributing to the therapeutic effects of the drug 92.Synaptic plasticity and depression: new insights from stress and rapid-acting antidepressants 93.Matrix-embedded cells control osteoclast formation 94. Targeting EZH2 in cancer 95.Comprehensive molecular characterization of clinical responses to PD-1 inhibition in metastatic gastric cancer 96.Identification of miR-34a as a potent inhibitor of prostate cancer progenitor cells and metastasis by directly repressing CD44 97.Phenotype molding of stromal cells in the lung tumor microenvironment 98. Key roles of adjuvants in modern vaccines 99.AI in health and medicine Table 5: Titles of Novel (Positive) and Non-novel (Negative) Papers in Nature Medicine Test Set C Prompt C.1 Prompt for LLM with NeurIPS 2024 Review Guideline Prompt</p>
</li>
</ol>
<p>score ← |{N D∈D|N D≤N D Idea }| |D| × 100 10: Return score
1 Find Neighbors and Calculate Neighbor Density1: function NEIGHBOR(Input, P, Q)2:v Input ← GET_EMBEDDING(Input)▷ Using M3-Embedding model3:C ← []4:neighbors ← GET_NEIGHBORS(v Input , max(P, Q))▷ Find max(P, Q) nearest neighbors5:neighbors_f or_count ← neighbors[: Q]▷ Only use the Q nearest neighbors to calculate density6:neighbors_f or_distribution ← neighbors[: P ] ▷ The P nearest neighbors are used to calculate distribution7:for each paper in neighbors_f or_count do8:v paper ← GET_EMBEDDING(paper)9:distance ← 1 -COSINE_SIMILARITY(v Input , v paper )10:C.Append(distance)11:end for12:N D Input ← MEAN(C)13:return N D Input , neighbors_f or_distribution14: end functionAlgorithm 2 Calculate Novelty Score of Given Idea1: Input: Idea2: Output: A score in the range of 0 to 1003: D ← []4: N D Idea , neighbors ← NEIGHBOR(Idea, P, Q)5: for paper in neighbors do6:N D paper , _ ← NEIGHBOR(paper, P, Q)7:D.Append(N D paper )8: end for9:</p>
<p>Table 5
5Test setLabelCounttotal 2024-2025 2019-2023 2014-2018 -2014NeurIPSPositive Negative80 9980 00 310 680 0Nature MedicinePositive Negative66 9966 00 290 320 38</p>
<p>Table 1 :
1
Count of Data in Different Time Ranges for NeurIPS and Nature Medicine Test Sets.Positive: novel samples, Negative: non-novel samples.</p>
<p>Table 3 :
3
validate our statement.AUROC Comparison for Different Design .Absolute Local Density: Use the idea's local density as novelty score.(density calculated by mean distances between idea and idea's P first level neighbors).Euclidean distance: replace the cosine distance with Euclidean in RND
Test setAUROCRelative Neighbor Density(Ours) Absolute Local Density Euclidean distanceNeurIPS0.8200.8510.815Nature Medicine0.7650.7570.753Mixed0.7950.3950.78</p>
<p>NeurIPS test set, the neighbor density-based RND algorithm, absolute local density-based HD algorithm and Sonnet-3.7 with literature search tools achieved AUROC better than 0.8.When it comes to another domain (Nature Medicine
Idea First level neighbor Second level neighborNeighbor: Learning Knowledge Graph-based World Models of Textual Environments60Idea: Evaluating the World Model Implicit in a Generative Model40Neighbor: Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation20Neighbor: DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning0204060801007550250255075
test set for biomedical research), only RND and HD achieved AUROC at approximately 0.7; the other algorithms, including reasoning model such as Sonnet-3.7,degraded to AUROC 0.6.The strong performance of HD in the two respective domains suggests that measuring the local density (average semantic distance between a given idea and its nearest neighbors) in the historical literature database can effectively indicate the novelty of that idea in a single research domain.Since our proposed algorithm also incorporates semantic density, it exhibited similar accuracy level.</p>
<p>Table 6 :
6
Prompt for LLM with NeurIPS 2024 Review Guideline C.2 Prompt for Standardized Project Proposals For model selection, if any version of Claude is mentioned, change it to the latest version of Claude (Claude-3.5);if any version of LLaMA is mentioned, change it to the latest version LLaMA-3.Do not make any other model changes.Now directly generate the edited student idea to match the format of the template.</p>
<p>Table 7 :
7
Prompt for Standardized Project Proposals C.3 Prompt for LLM with Literature Search Prompt</p>
<p>Table 8 :
8
Prompt for LLM with Literature Search</p>
<p>Long-term cardiovascular outcomes of COVID-19 83.Ketone body β-hydroxybutyrate blocks the NLRP3 inflammasome-mediated inflammatory disease 84.Large language models in medicine 85.In vivo photodynamic therapy using upconversion nanoparticles as remote-controlled nanotransducersContinued on next pagePromptRole: You are a writing assistant specialized in editing academic writing.Task: I will give you a student's research idea and an idea template.Your task is to edit the student's idea to follow the template's format.Student idea: Title {title} Main Idea {paper} Template: 1. Title: A concise statement of the main research question to be used as the paper title.2. Problem Statement: Clearly define the problem your research intends to address.Explain clearly why this problem is interesting and important.3. Motivation: Explain why existing methods are not good enough to solve the problem, and explain the inspiration behind the new proposed method.You should also motivate why the proposed method would work better than existing baselines on the problem.4. Proposed Method: Explain how the proposed method works, describe all the essential steps. 5.Step-by-Step Experiment Plan: Break down every single step of the experiments, make sure every step is executable.Cover all essential details such as the datasets, models, and metrics to be used.If the project involves prompting, give some example prompts for each step.6. Test Case Examples: Give at least two concrete examples.The first example should show how the baseline method fails on the test case.If there are multiple baselines, give examples for all of them.The second example should show how the proposed method succeeds on the test case.For each test case, include the input (test example and the full prompt) and the expected output.You should also provide an explanation for why the outputs from the proposed prompt are better.If the proposed method has multiple steps, break them down into intermediate steps.7. Fallback Plan: Propose some alternative plans for what should the students do if the proposed method doesn't manage to satisfy the success criteria.For example, you can suggest additional analysis to help debug why the proposed method didn't work, which could inform alternative new methods, or just turn the project into an analysis paper instead by offering some interesting ablation and insights.Requirement: Make sure that you only edit the wording and formatting, including things like punctuation, capitalization, linebreaks, and bullet points.Also make sure to edit any informal wording and phrasing to use vocabulary that sounds like the template's writing style.No other changes are allowed beyond these.You should use tab as indentation and make sure to use appropriate nested indentation for sub-bullets.All bullets should have a clear hierarchy so people can easily differentiate the sub-bullets.Only leave empty lines between sections and remove any extra line breaks.If many bullet points are clustered together in a paragraph, separate them clearly with indentation and appropriate bullet point markers.Change to a new line for each new bullet point.For the fallback plan, do not list a bunch of bullet points.Instead, condense them into one coherent paragraph.For line breaks, avoid Raw String Literals or Double Backslashes when using " n", and change them to spaces or tabs.For in-line citations, if the citation mentioned the author's last name (like "(Si et al., 2023)" or "(An et al., 2024)"), you should keep them there; but if the citation is just a number (like "[1]" or "[3,4,5]"), you should just remove it and do some necessary rephrasing to make the sentence still sound coherent without the references.Apart from minor rephrasing and changing formatting, do not change any content of the idea.You must preserve the exact meaning of the original idea, do not change, remove, or add any other details.Do not drop any sections (including test case examples).Do not rename any models, datasets, or methods.Do not drop clarification or examples in brackets and do not drop any data source mentions (e.g., Chatbot Arena or Wildchat)!Note that when indexing test case examples, each test case example could have multiple steps of inputs and outputs and you shouldn't give separate indices to them.Each test case example should be a whole set of input-output pairs for the baseline(s) and proposed method.For the proposed method section, avoid any big changes.If the section comes in as a coherent paragraph, you don't have to break it down into bullet points.If the section is already in bullet points, you should keep it that way.If the section is a mix of both, you should keep the bullet points and the coherent paragraph as they are.Keep all the clarification and examples mentioned in all the sections and do not remove any of them (including those in brackets).D Result in Detail
Claude 3.7 sonnet and claude code. Anthropic, 2025. Feb 28th, 2025</p>
<p>Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu, 2024</p>
<p>Cornell Tech, Arxiv annual report 2023. 2023. Feb 25th, 2025</p>
<p>Arxiv dataset. 2025. Jan 10, 2025Cornell University</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.14255October 2024</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Pubmed and beyond: biomedical literature search in the age of artificial intelligence. Qiao Jin, Robert Leaman, Zhiyong Lu, EBioMedicine. 1002024</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. September 2024</p>
<p>Evaluating the world model implicit in a generative model. 2025. Jan 10, 2025. 2025. Feb 26th, 2025National Library of Medicine. Pubmed download</p>
<p>Official review of submission1763. 2025. March 7th, 2025Openreview Reviewer 2X9t</p>
<p>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109September 2024</p>
<p>Robustness of LLMs to Perturbations in Text. Ayush Singh, Navpreet Singh, Shubham Vatsal, arXiv:2407.08989July 2024</p>
<p>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.09403October 2024</p>
<p>Evaluating the world model implicit in a generative model. Keyon Vafa, Justin Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan, Advances in Neural Information Processing Systems. 372025</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li, arXiv:2301.12868March 2023</p>            </div>
        </div>

    </div>
</body>
</html>
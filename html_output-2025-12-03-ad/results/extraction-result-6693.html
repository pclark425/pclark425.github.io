<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6693 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6693</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6693</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-6118910c4014cc6c061198a2d88c080ab56ea452</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6118910c4014cc6c061198a2d88c080ab56ea452" target="_blank">Deep Reinforcement Learning with a Natural Language Action Space</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games, called a deep reinforcement relevance network (DRRN).</p>
                <p><strong>Paper Abstract:</strong> This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6693.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6693.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep RL architecture that uses two separate deep neural networks to embed state-text and action-text into vectors and computes Q(s,a) via an interaction function (inner product), trained end-to-end with Q-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Deep Reinforcement Learning with a Natural Language Action Space</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-network model: a state-side DNN and an action-side DNN that produce embeddings h_{L,s} and h_{L,a}; Q(s,a) is approximated by an interaction function g(h_{L,s},h_{L,a}) (inner product used in experiments). Trained with Q-learning and experience-replay.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Saving John; Machine of Death (text games)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay buffer (replay memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored transition tuples (s_t, a_t, r_t, s_{t+1}, A_{t+1}) as raw representations (bag-of-words features converted to input vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Transitions are stored into a fixed-capacity replay memory D (initialized to capacity N). The paper describes appending/storing transitions but does not specify the eviction policy when capacity is exceeded.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Random minibatch sampling from the replay buffer (experience-replay) for gradient updates (random sampling "scrambles" trajectory correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning: Q-learning / deep Q-learning with experience replay and softmax action selection during data collection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final average reward (average final rewards with standard deviation) reported at convergence on each game.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Final average reward at convergence (experience-replay was used in all reported experiments): Saving John — DRRN (L=2, hidden=100) 18.7 (std 0.4); Machine of Death — DRRN (L=2, hidden=100) 11.2 (std 0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>No ablation study isolating the effect of experience-replay vs. no replay is reported. The paper states that random sampling from experience-replay avoids oscillations/divergence and achieves faster convergence, but does not provide direct quantitative ablation on the memory component.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>The paper does not present explicit limitations of the replay memory; capacity and eviction policy are not specified. Authors note that deeper models converge more slowly and that LSTMs did not improve results in some preliminary tests.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use experience-replay to decorrelate updates and speed convergence; collect diverse transitions via a softmax exploration policy; randomly shuffle action listing to prevent learning artifacts from action order; use separate state and action embeddings rather than concatenating inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning with a Natural Language Action Space', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6693.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6693.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experience Replay</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experience Replay (replay memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fixed-capacity buffer that stores past transition tuples and supplies random minibatches for training to break temporal correlations in sequential RL data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforcement learning for robots using neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Deep Reinforcement Learning with a Natural Language Action Space</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Replay memory D</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>External buffer D initialized to capacity N that stores (s_t, a_t, r_t, s_{t+1}, A_{t+1}) during interaction; training samples uniformly at random from D to compute TD errors and backpropagate.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Saving John; Machine of Death</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external replay buffer storing transitions</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Transition tuples: (state representation, action, reward, next state representation, next-action set). States and actions are stored in their input representation (bag-of-words features converted to vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Transitions are appended to the buffer D up to capacity N; the paper does not detail the eviction policy when full.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Uniform random sampling of mini-batches from D (experience-replay).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Q-learning (deep Q-learning) using sampled transitions to compute temporal-difference updates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not directly an evaluation metric; used to produce the trained policies whose performance is reported as final average reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not separately reported — experience-replay is part of the training pipeline for all reported results (see DRRN performance entries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>No ablation on replay buffer size, sampling strategy, or its removal; qualitative claim that random sampling from replay avoids oscillations/divergence and accelerates convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Paper does not specify buffer eviction policy or other implementation details (capacity N provided but overwrite policy unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use experience-replay to break correlations in trajectory data; sample randomly for stable updates; collect transitions under a fixed exploration policy (softmax) to populate the replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning with a Natural Language Action Space', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6693.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6693.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN (Narasimhan et al. 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory Deep Q-Network for parser-based text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that uses an LSTM to encode the state text within a DQN framework for parser-based text games, reporting improvements over bag-of-words baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Deep Reinforcement Learning with a Natural Language Action Space</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN (as in Narasimhan et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A DQN agent where the state text is encoded by an LSTM (sequence model) producing a hidden representation consumed by the Q-network; applied to parser-based text games with a fixed action vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Parser-based text games (as in Narasimhan et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent hidden state (LSTM) used to encode state text</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>LSTM hidden state representing sequential encoding of the state description (captures word order and short-term dependencies within the state text).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>LSTM recurrence updates the hidden state as it processes state text; the encoding is produced on each state observation and used by the Q-network (trained end-to-end).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Implicit use of the current LSTM hidden state as input to the Q-value estimator (no explicit retrieval/indexing mechanism described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep Q-learning (DQN) as reported by Narasimhan et al. (2015).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average reward (Narasimhan et al. reported higher average reward than random and bag-of-words DQN baselines; exact numbers are not reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in Narasimhan et al. (2015) to outperform random and bag-of-words DQN baselines in parser-based games; exact numeric values are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Bag-of-words DQN baseline had lower average reward per Narasimhan et al. (2015); precise figures not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>This paper only cites Narasimhan et al.; it notes that in the authors' preliminary experiments on 'Machine of Death', LSTMs did not improve performance compared to bag-of-words, but no systematic ablation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>The LSTM-DQN work addressed parser-based games with a pre-specified finite action set (action-argument product space), which the present paper argues is less suitable for complex choice/hypertext games with unconstrained natural language actions.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>No explicit memory-centric recommendations are given here for LSTM-DQN; the citation suggests LSTM encodings can help for parser-based games but may not generalize to more complex action-language settings without additional design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning with a Natural Language Action Space', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Reinforcement learning for robots using neural networks. <em>(Rating: 2)</em></li>
                <li>Human-level control through deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Continuous control with deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Learning to win by reading manuals in a monte-carlo framework <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6693",
    "paper_id": "paper-6118910c4014cc6c061198a2d88c080ab56ea452",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "A deep RL architecture that uses two separate deep neural networks to embed state-text and action-text into vectors and computes Q(s,a) via an interaction function (inner product), trained end-to-end with Q-learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "agent_name": "DRRN",
            "agent_description": "Two-network model: a state-side DNN and an action-side DNN that produce embeddings h_{L,s} and h_{L,a}; Q(s,a) is approximated by an interaction function g(h_{L,s},h_{L,a}) (inner product used in experiments). Trained with Q-learning and experience-replay.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Saving John; Machine of Death (text games)",
            "memory_used": true,
            "memory_type": "experience replay buffer (replay memory)",
            "memory_representation": "Stored transition tuples (s_t, a_t, r_t, s_{t+1}, A_{t+1}) as raw representations (bag-of-words features converted to input vectors).",
            "memory_update_mechanism": "Transitions are stored into a fixed-capacity replay memory D (initialized to capacity N). The paper describes appending/storing transitions but does not specify the eviction policy when capacity is exceeded.",
            "memory_retrieval_method": "Random minibatch sampling from the replay buffer (experience-replay) for gradient updates (random sampling \"scrambles\" trajectory correlations).",
            "training_method": "Reinforcement learning: Q-learning / deep Q-learning with experience replay and softmax action selection during data collection.",
            "evaluation_metric": "Final average reward (average final rewards with standard deviation) reported at convergence on each game.",
            "performance_with_memory": "Final average reward at convergence (experience-replay was used in all reported experiments): Saving John — DRRN (L=2, hidden=100) 18.7 (std 0.4); Machine of Death — DRRN (L=2, hidden=100) 11.2 (std 0.6).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "No ablation study isolating the effect of experience-replay vs. no replay is reported. The paper states that random sampling from experience-replay avoids oscillations/divergence and achieves faster convergence, but does not provide direct quantitative ablation on the memory component.",
            "reported_limitations": "The paper does not present explicit limitations of the replay memory; capacity and eviction policy are not specified. Authors note that deeper models converge more slowly and that LSTMs did not improve results in some preliminary tests.",
            "best_practices_recommendations": "Use experience-replay to decorrelate updates and speed convergence; collect diverse transitions via a softmax exploration policy; randomly shuffle action listing to prevent learning artifacts from action order; use separate state and action embeddings rather than concatenating inputs.",
            "uuid": "e6693.0",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Experience Replay",
            "name_full": "Experience Replay (replay memory)",
            "brief_description": "A fixed-capacity buffer that stores past transition tuples and supplies random minibatches for training to break temporal correlations in sequential RL data.",
            "citation_title": "Reinforcement learning for robots using neural networks.",
            "mention_or_use": "use",
            "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "agent_name": "Replay memory D",
            "agent_description": "External buffer D initialized to capacity N that stores (s_t, a_t, r_t, s_{t+1}, A_{t+1}) during interaction; training samples uniformly at random from D to compute TD errors and backpropagate.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Saving John; Machine of Death",
            "memory_used": true,
            "memory_type": "external replay buffer storing transitions",
            "memory_representation": "Transition tuples: (state representation, action, reward, next state representation, next-action set). States and actions are stored in their input representation (bag-of-words features converted to vectors).",
            "memory_update_mechanism": "Transitions are appended to the buffer D up to capacity N; the paper does not detail the eviction policy when full.",
            "memory_retrieval_method": "Uniform random sampling of mini-batches from D (experience-replay).",
            "training_method": "Q-learning (deep Q-learning) using sampled transitions to compute temporal-difference updates.",
            "evaluation_metric": "Not directly an evaluation metric; used to produce the trained policies whose performance is reported as final average reward.",
            "performance_with_memory": "Not separately reported — experience-replay is part of the training pipeline for all reported results (see DRRN performance entries).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "No ablation on replay buffer size, sampling strategy, or its removal; qualitative claim that random sampling from replay avoids oscillations/divergence and accelerates convergence.",
            "reported_limitations": "Paper does not specify buffer eviction policy or other implementation details (capacity N provided but overwrite policy unspecified).",
            "best_practices_recommendations": "Use experience-replay to break correlations in trajectory data; sample randomly for stable updates; collect transitions under a fixed exploration policy (softmax) to populate the replay buffer.",
            "uuid": "e6693.1",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "LSTM-DQN (Narasimhan et al. 2015)",
            "name_full": "Long Short-Term Memory Deep Q-Network for parser-based text games",
            "brief_description": "Referenced prior work that uses an LSTM to encode the state text within a DQN framework for parser-based text games, reporting improvements over bag-of-words baselines.",
            "citation_title": "Language understanding for text-based games using deep reinforcement learning",
            "mention_or_use": "mention",
            "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "agent_name": "LSTM-DQN (as in Narasimhan et al., 2015)",
            "agent_description": "A DQN agent where the state text is encoded by an LSTM (sequence model) producing a hidden representation consumed by the Q-network; applied to parser-based text games with a fixed action vocabulary.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Parser-based text games (as in Narasimhan et al., 2015)",
            "memory_used": true,
            "memory_type": "recurrent hidden state (LSTM) used to encode state text",
            "memory_representation": "LSTM hidden state representing sequential encoding of the state description (captures word order and short-term dependencies within the state text).",
            "memory_update_mechanism": "LSTM recurrence updates the hidden state as it processes state text; the encoding is produced on each state observation and used by the Q-network (trained end-to-end).",
            "memory_retrieval_method": "Implicit use of the current LSTM hidden state as input to the Q-value estimator (no explicit retrieval/indexing mechanism described in this paper).",
            "training_method": "Deep Q-learning (DQN) as reported by Narasimhan et al. (2015).",
            "evaluation_metric": "Average reward (Narasimhan et al. reported higher average reward than random and bag-of-words DQN baselines; exact numbers are not reproduced in this paper).",
            "performance_with_memory": "Reported in Narasimhan et al. (2015) to outperform random and bag-of-words DQN baselines in parser-based games; exact numeric values are not provided in this paper.",
            "performance_without_memory": "Bag-of-words DQN baseline had lower average reward per Narasimhan et al. (2015); precise figures not provided here.",
            "has_comparative_results": false,
            "ablation_findings": "This paper only cites Narasimhan et al.; it notes that in the authors' preliminary experiments on 'Machine of Death', LSTMs did not improve performance compared to bag-of-words, but no systematic ablation is provided.",
            "reported_limitations": "The LSTM-DQN work addressed parser-based games with a pre-specified finite action set (action-argument product space), which the present paper argues is less suitable for complex choice/hypertext games with unconstrained natural language actions.",
            "best_practices_recommendations": "No explicit memory-centric recommendations are given here for LSTM-DQN; the citation suggests LSTM encodings can help for parser-based games but may not generalize to more complex action-language settings without additional design.",
            "uuid": "e6693.2",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
                "publication_date_yy_mm": "2015-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning for robots using neural networks.",
            "rating": 2
        },
        {
            "paper_title": "Human-level control through deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Continuous control with deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Learning to win by reading manuals in a monte-carlo framework",
            "rating": 1
        }
    ],
    "cost": 0.0156565,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Reinforcement Learning with a Natural Language Action Space</h1>
<p>Ji He ${ }^{\star}$, Jianshu Chen ${ }^{\dagger}$, Xiaodong $\mathrm{He}^{\dagger}$, Jianfeng Gao ${ }^{\dagger}$, Lihong $\mathrm{Li}^{\dagger}$ Li Deng ${ }^{\dagger}$ and Mari Ostendorf<em><br></em>Department of Electrical Engineering, University of Washington, Seattle, WA 98195, USA<br>{jvking, ostendor}@uw.edu<br>${ }^{\dagger}$ Microsoft Research, Redmond, WA 98052, USA<br>{jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com</p>
<h4>Abstract</h4>
<p>This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Qlearning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.</p>
<h2>1 Introduction</h2>
<p>This work is concerned with learning strategies for sequential decision-making tasks, where a system takes actions at a particular state with the goal of maximizing a long-term reward. More specifically, we consider tasks where both the states and the actions are characterized by natural language, such as in human-computer dialog systems, tutoring systems, or text-based games. In a text-based game, for example, the player (or system, in this case) is given a text string that describes the current state of the game and several text strings that describe possible actions one could take. After selecting one of the actions, the environment state is updated and revealed in a new textual description. A reward is given either at each transition or in the end. The objective is to understand, at each step, the state text and all the action texts to pick the most relevant action, navigating through the se-
quence of texts so as to obtain the highest longterm reward. Here the notion of relevance is based on the joint state/action impact on the reward: an action text string is said to be "more relevant" (to a state text string) than the other action texts if taking that action would lead to a higher longterm reward. Because a player's action changes the environment, reinforcement learning (Sutton and Barto, 1998) is appropriate for modeling longterm dependency in text games.</p>
<p>There is a large body of work on reinforcement learning. Of most interest here are approaches leveraging neural networks because of their success in handling a large state space. Early work -TD-gammon - used a neural network to approximate the state value function (Tesauro, 1995). Recently, inspired by advances in deep learning (LeCun et al., 2015; Hinton et al., 2012; Krizhevsky et al., 2012; Dahl et al., 2012), significant progress has been made by combining deep learning with reinforcement learning. Building on the approach of Q-learning (Watkins and Dayan, 1992), the "Deep Q-Network" (DQN) was developed and applied to Atari games (Mnih et al., 2013; Mnih et al., 2015) and shown to achieve human level performance by applying convolutional neural networks to the raw image pixels. Narasimhan et al. (2015) applied a Long Short-Term Memory network to characterize the state space in a DQN framework for learning control policies for parserbased text games. More recently, Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for language based sequential decision making study. Another stream of work focuses on continuous control with deep reinforcement learning (Lillicrap et al., 2016), where an actor-critic algorithm operates over a known continuous action space.</p>
<p>Inspired by these successes and recent work using neural networks to learn phrase- or sentence-</p>
<p>level embeddings (Collobert and Weston, 2008; Huang et al., 2013; Le and Mikolov, 2014; Sutskever et al., 2014; Kiros et al., 2015), we propose a novel deep architecture for text understanding, which we call a deep reinforcement relevance network (DRRN). The DRRN uses separate deep neural networks to map state and action text strings into embedding vectors, from which "relevance" is measured numerically by a general interaction function, such as their inner product. The output of this interaction function defines the value of the Q-function for the current state-action pair, which characterizes the optimal long-term reward for pairing these two text strings. The Q-function approximation is learned in an end-to-end manner by Q-learning.</p>
<p>The DRRN differs from prior work in that earlier studies mostly considered action spaces that are bounded and known. For actions described by natural language text strings, the action space is inherently discrete and potentially unbounded due to the exponential complexity of language with respect to sentence length. A distinguishing aspect of the DRRN architecture - compared to simple DQN extensions - is that two different types of meaning representations are learned, reflecting the tendency for state texts to describe scenes and action texts to describe potential actions from the user. We show that the DRRN learns a continuous space representation of actions that successfully generalize to paraphrased descriptions of actions unseen in training.</p>
<h2>2 Deep Reinforcement Relevance Network</h2>
<h3>2.1 Text Games and Q-learning</h3>
<p>We consider the sequential decision making problem for text understanding. At each time step $t$, the agent will receive a string of text that describes the state $s_{t}$ (i.e., "state-text") and several strings of text that describe all the potential actions $a_{t}$ (i.e., "action-text"). The agent attempts to understand the texts from both the state side and the action side, measuring their relevance to the current context $s_{t}$ for the purpose of maximizing the long-term reward, and then picking the best action. Then, the environment state is updated $s_{t+1}=s^{\prime}$ according to the probability $p\left(s^{\prime} \mid s, a\right)$, and the agent receives a reward $r_{t}$ for that particular transition. The policy of the agent is defined to be the probability $\pi\left(a_{t} \mid s_{t}\right)$ of taking action $a_{t}$
at state $s_{t}$. Define the Q-function $Q^{\pi}(s, a)$ as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi(a \mid s)$ to be:</p>
<p>$$
Q^{\pi}(s, a)=\mathbb{E}\left{\left.\sum_{k=0}^{+\infty} \gamma^{k} r_{t+k} \right\rvert\, s_{t}=s, a_{t}=a\right}
$$</p>
<p>where $\gamma$ denotes a discount factor. The optimal policy and Q-function can be found by using the Q-learning algorithm (Watkins and Dayan, 1992):</p>
<p>$$
\begin{aligned}
Q\left(s_{t}, a_{t}\right) &amp; \leftarrow Q\left(s_{t}, a_{t}\right)+ \
&amp; \eta_{t} \cdot\left(r_{t}+\gamma \cdot \max <em t_1="t+1">{a} Q\left(s</em>\right)\right)
\end{aligned}
$$}, a\right)-Q\left(s_{t}, a_{t</p>
<p>where $\eta_{t}$ is the learning rate of the algorithm. In this paper, we use a softmax selection strategy as the exploration policy during the learning stage, which chooses the action $a_{t}$ at state $s_{t}$ according to the following probability:</p>
<p>$$
\pi\left(a_{t}=a_{t}^{i} \mid s_{t}\right)=\frac{\exp \left(\alpha \cdot Q\left(s_{t}, a_{t}^{i}\right)\right)}{\sum_{j=1}^{\left|\mathcal{A}<em t="t">{t}\right|} \exp \left(\alpha \cdot Q\left(s</em>
$$}, a_{t}^{j}\right)\right)</p>
<p>where $\mathcal{A}<em t="t">{t}$ is the set of feasible actions at state $s</em>,|\cdot|$ denotes the cardinality of the set, and $\alpha$ is the scaling factor in the softmax operation. $\alpha$ is kept constant throughout the learning period. All methods are initialized with small random weights, so initial Q-value differences will be small, thus making the Q-learning algorithm more explorative initially. As Q-values better approximate the true values, a reasonable $\alpha$ will make action selection put high probability on the optimal action (exploitation), but still maintain a small exploration probability.}$, $a_{t}^{i}$ is the $i$-th feasible action in $\mathcal{A}_{t</p>
<h3>2.2 Natural language action space</h3>
<p>Let $\mathcal{S}$ denote the state space, and let $\mathcal{A}$ denote the entire action space that includes all the unique actions over time. A vanilla Q-learning recursion (1) needs to maintain a table of size $|\mathcal{S}| \times|\mathcal{A}|$, which is problematic for a large state/action space. Prior work using a DNN in Q-function approximation has shown high capacity and scalability for handling a large state space, but most studies have used a network that generates $|\mathcal{A}|$ outputs, each of which represents the value of $Q(s, a)$ for a particular action $a$. It is not practical to have a DQN architecture of a size that is explicitly dependence on the large number of natural language actions. Further, in many text games, the feasible action set $\mathcal{A}_{t}$ at each time $t$ is an unknown subset of the unbounded action space $\mathcal{A}$ that varies over time.</p>
<p>For the case where the maximum number of possible actions at any point in time $\left(\max <em t="t">{t}\left|\mathcal{A}</em>\right|\right)$ is known, the DQN can be modified to simply use that number of outputs ("Max-action DQN"), as illustrated in Figure 1(a), where the state and action vectors are concatenated (i.e., as an extended state vector) as its input. The network computes the Q-function values for the actions in the current feasible set as its outputs. For a complex game, $\max <em t="t">{t}\left|\mathcal{A}</em>$ is usually unknown beforehand. Nevertheless, we will use this modified DQN as a baseline.}\right|$ may be difficult to obtain, because $\mathcal{A}_{t</p>
<p>An alternative approach is to use a function approximation using a neural network that takes a state-action pair as input, and outputs a single Q value for each possible action ("Per-action DQN" in Figure 1(b)). This architecture easily handles a varying number of actions and represents a second baseline.</p>
<p>We propose an alternative architecture for handling a natural language action space in sequential text understanding: the deep reinforcement relevance network (DRRN). As shown in Figure 1(c), the DRRN consists of a pair of DNNs, one for the state text embedding and the other for action text embeddings, which are combined using a pairwise interaction function. The texts used to describe states and actions could be very different in nature, e.g., a state text could be long, containing sentences with complex linguistic structure, whereas an action text could be very concise or just a verb phrase. Therefore, it is desirable to use two networks with different structures to handle state/action texts, respectively. As we will see in the experimental sections, by using two separate deep neural networks for state and action sides, we obtain much better results.</p>
<h3>2.3 DRRN architecture: Forward activation</h3>
<p>Given any state/action text pair $\left(s_{t}, a_{t}^{i}\right)$, the DRRN estimates the Q-function $Q\left(s_{t}, a_{t}^{i}\right)$ in two steps. First, map both $s_{t}$ and $a_{t}^{i}$ to their embedding vectors using the corresponding DNNs, respectively. Second, approximate $Q\left(s_{t}, a_{t}^{i}\right)$ using an interaction function such as the inner product of the embedding vectors. Then, given a particular state $s_{t}$, we can select the optimal action $a_{t}$ among the set of actions via $a_{t}=\arg \max <em t="t">{a</em>\right)$.}^{i}} Q\left(s_{t}, a_{t}^{i</p>
<p>More formally, let $h_{l, s}$ and $h_{l, a}$ denote the $l$-th hidden layer for state and action side neural networks, respectively. For the state side, $W_{l, s}$ and
$b_{l, s}$ denote the linear transformation weight matrix and bias vector between the $(l-1)$-th and $l$-th hidden layers. $W_{l, a}$ and $b_{l, a}$ denote the equivalent parameters for the action side. In this study, the DRRN has $L$ hidden layers on each side.</p>
<p>$$
\begin{aligned}
h_{1, s} &amp; =f\left(W_{1, s} s_{t}+b_{1, s}\right) \
h_{1, a}^{i} &amp; =f\left(W_{1, a} a_{t}^{i}+b_{1, a}\right) \
h_{l, s} &amp; =f\left(W_{l-1, s} h_{l-1, s}+b_{l-1, s}\right) \
h_{l, a}^{i} &amp; =f\left(W_{l-1, a} h_{l-1, a}^{i}+b_{l-1, a}\right)
\end{aligned}
$$</p>
<p>where $f(\cdot)$ is the nonlinear activation function at the hidden layers, which, for example, could be chosen as $\tanh (x)$, and $i=1,2,3, \ldots,\left|\mathcal{A}_{t}\right|$ is the action index. A general interaction function $g(\cdot)$ is used to approximate the Q-function values, $Q(s, a)$, in the following parametric form:</p>
<p>$$
Q\left(s, a^{i} ; \Theta\right)=g\left(h_{L, s}, h_{L, a}^{i}\right)
$$</p>
<p>where $\Theta$ denotes all the model parameters. The interaction function could be an inner product, a bilinear operation, or a nonlinear function such as a deep neural network. In our experiments, the inner product and bilinear operation gave similar results. For simplicity, we present our experiments mostly using the inner product interaction function.</p>
<p>The success of the DRRN in handling a natural language action space $\mathcal{A}$ lies in the fact that the state-text and the action-texts are mapped into separate finite-dimensional embedding spaces. The end-to-end learning process (discussed next) makes the embedding vectors in the two spaces more aligned for "good" (or relevant) action texts compared to "bad" (or irrelevant) choices, resulting in a higher interaction function output ( Q function value).</p>
<h3>2.4 Learning the DRRN: Back propagation</h3>
<p>To learn the DRRN, we use the "experiencereplay" strategy (Lin, 1993), which uses a fixed exploration policy to interact with the environment to obtain a sample trajectory. Then, we randomly sample a transition tuple $\left(s_{k}, a_{k}, r_{k}, s_{k+1}\right)$, compute the temporal difference error for sample $k$ :
$d_{k}=r_{k}+\gamma \max <em k_1="k+1">{a} Q\left(s</em>\right)$,
and update the model according to the recursions:}, a ; \Theta_{k-1}\right)-Q\left(s_{k}, a_{k} ; \Theta_{k-1</p>
<p>$$
\begin{gathered}
W_{v, k}=W_{v, k-1}+\eta_{k} d_{k} \cdot \frac{\partial Q\left(s_{k}, a_{k} ; \Theta_{k-1}\right)}{\partial W_{v}} \
b_{v, k}=b_{v, k-1}+\eta_{k} d_{k} \cdot \frac{\partial Q\left(s_{k}, a_{k} ; \Theta_{k-1}\right)}{\partial b_{v}}
\end{gathered}
$$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Different deep Q-learning architectures: Max-action DQN and Per-action DQN both treat input text as concatenated vectors and compute output Q-values with a single NN. DRRN models text embeddings from state/action sides separately, and use an interaction function to compute Q-values.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: PCA projections of text embedding vectors for state and associated action vectors after 200, 400 and 600 training episodes. The state is "As you move forward, the people surrounding you suddenly look up with terror in their faces, and flee the street." Action 1 (good choice) is "Look up", and action 2 (poor choice) is "Ignore the alarm of others and continue moving forward."
for $v \in{s, a}$. Expressions for $\frac{\partial Q}{\partial W_{v}}, \frac{\partial Q}{\partial b_{v}}$ and other algorithm details are given in supplementary materials. Random sampling essentially scrambles the trajectory from experience-replay into a "bag-of-transitions", which has been shown to avoid oscillations or divergence and achieve faster convergence in Q-learning (Mnih et al., 2015). Since the models on the action side share the same parameters, models associated with all actions are effectively updated even though the back propagation is only over one action. We apply back propagation to learn how to pair the text strings from the reward signals in an end-to-end manner. The representation vectors for the state-text and the action-text are automatically learned to be aligned with each other in the text embedding space from the reward signals. A summary of the full learning algorithm is given in Algorithm 1.</p>
<p>Figure 2 illustrates learning with an inner product interaction function. We used Principal Component Analysis (PCA) to project the 100dimension last hidden layer representation (before the inner product) to a 2-D plane. The vector embeddings start with small values, and after 600 episodes of experience-replay training, the embeddings are very close to the converged embedding (4000 episodes). The embedding vector of the optimal action (Action 1) converges to a positive inner product with the state embedding vector, while Action 2 converges to a negative inner product.</p>
<h2>3 Experimental Results</h2>
<h3>3.1 Text games</h3>
<p>Text games, although simple compared to video games, still enjoy high popularity in online communities, with annual competitions held online</p>
<p>Algorithm 1 Learning algorithm for DRRN
1: Initialize replay memory $\mathcal{D}$ to capacity $N$.
2: Initialize DRRN with small random weights.
3: Initialize game simulator and load dictionary.
4: for episode $=1, \ldots, M$ do
5: Restart game simulator.
6: Read raw state text and a list of action text from the simulator, and convert them to representation $s_{1}$ and $a_{1}^{1}, a_{1}^{2}, \ldots, a_{1}^{\left|\mathcal{A}<em t="t">{1}\right|}$.
7: for $t=1, \ldots, T$ do
8: Compute $Q\left(s</em> ; \Theta\right)$ for the list of actions using DRRN forward activation (Section 2.3).
9: Select an action $a_{t}$ based on probability distribution $\pi\left(a_{t}=a_{t}^{i} \mid s_{t}\right)$ (Equation 2)
10: Execute action $a_{t}$ in simulator
11: Observe reward $r_{t}$. Read the next state text and the next list of action texts, and convert them to representation $s_{t+1}$ and $a_{t+1}^{1}, a_{t+1}^{2}, \ldots, a_{t+1}^{\left|\mathcal{A}}, a_{t}^{i<em t="t">{t+1}\right|}$.
12: Store transition $\left(s</em>$.
13: Sample random mini batch of transitions $\left(s_{k}, a_{k}, r_{k}, s_{k+1}, A_{k+1}\right)$ from $\mathcal{D}$.
14: Set $y_{k}= \begin{cases}r_{k} &amp; \text { if } s_{k+1} \text { is terminal } \ r_{k}+\gamma \max }, a_{t}, r_{t}, s_{t+1}, A_{t+1}\right)$ in $\mathcal{D<em k_1="k+1">{a^{\prime} \in A</em>$
15: Perform a gradient descent step on $\left(y_{k}-Q\left(s_{k}, a_{k} ; \Theta\right)\right)^{2}$ with respect to the network parameters $\Theta$ (Section 2.4). Back-propagation is performed only for $a_{k}$ even though there are $\left|\mathcal{A}_{k}\right|$ actions at time $k$.
16: end for
17: end for
since 1995. Text games communicate to players in the form of a text display, which players have to understand and respond to by typing or clicking text (Adams, 2014). There are three types of text games: parser-based (Figure 3(a)), choicebased (Figure 3(b)), and hypertext-based (Figure 3(c)). Parser-based games accept typed-in commands from the player, usually in the form of verb phrases, such as "eat apple", "get key", or "go east". They involve the least complex action language. Choice-based and hypertext-based games present actions after or embedded within the state text. The player chooses an action, and the story continues based on the action taken at this particular state. With the development of web browsing and richer HTML display, choice-based and hypertext-based text games have become more popular, increasing in percentage from $8 \%$ in 2010 to $62 \%$ in $2014 .{ }^{1}$}} Q\left(s_{k+1}, a^{\prime} ; \Theta\right) &amp; \text { otherwise }\end{cases</p>
<p>For parser-based text games, Narasimhan et al. (2015) have defined a fixed set of 222 actions, which is the total number of possible phrases the parser accepts. Thus the parser-based text game is reduced to a problem that is well suited to a fixed-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>```
Game
Text game type Choice
Vocab size 1762 2258
Action vocab size 171
Avg. words/description 76.67 67.80
State transitions Deterministic Stochastic</p>
<h1>of states (underlying) (\geq 70 \quad \geq 200)</h1>
<p>```</p>
<p>Table 1: Statistics for the games "Saving John" and and "Machine of Death".
action-set DQN. However, for choice-based and hypertext-based text games, the size of the action space could be exponential with the length of the action sentences, which is handled here by using a continuous representation of the action space.</p>
<p>In this study, we evaluate the DRRN with two games: a deterministic text game task called "Saving John" and a larger-scale stochastic text game called "Machine of Death" from a public archive. ${ }^{2}$ The basic text statistics of these tasks are shown in Table 1. The maximum value of feasible actions (i.e., $\max <em t="t">{t}\left|\mathcal{A}</em>\right|$ ) is four in "Saving John", and nine in "Machine of Death". We manually annotate fi-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Different types of text games
nal rewards for all distinct endings in both games (as shown in supplementary materials). The magnitude of reward scores are given to describe sentiment polarity of good/bad endings. On the other hand, each non-terminating step we assign with a small negative reward, to encourage the learner to finish the game as soon as possible. For the text game "Machine of Death", we restrict an episode to be no longer than 500 steps.</p>
<p>In "Saving John" all actions are choice-based, for which the mapping from text strings to $a_{t}$ are clear. In "Machine of Death", when actions are hypertext, the actions are substrings of the state. In this case $s_{t}$ is associated with the full state description, and $a_{t}$ are given by the substrings without any surrounding context. For text input, we use raw bag-of-words as features, with different vocabularies for the state side and action side.</p>
<h3>3.2 Experiment setup</h3>
<p>We apply DRRNs with both 1 and 2 hidden layer structures. In most experiments, we use dotproduct as the interaction function and set the hidden dimension to be the same for each hidden layer. We use DRRNs with 20, 50 and 100-dimension hidden layer(s) and build learning curves during experience-replay training. The learning rate is constant: $\eta_{t}=0.001$. In testing, as in training, we apply softmax selection. We record average final rewards as performance of the model.</p>
<p>The DRRN is compared to multiple baselines: a linear model, two max-action DQNs (MA DQN) ( $L=1$ or 2 hidden layers), and two per-action DQNs (PA DQN) (again, $L=1,2$ ). All baselines use the same Q-learning framework with different function approximators to predict $Q\left(s_{t}, a_{t}\right)$ given the current state and actions. For the linear and MA DQN baselines, the input is the textbased state and action descriptions, each as a bag of words, with the number of outputs equal to the maximum number of actions. When there are fewer actions than the maximum, the highest scoring available action is used. The PA DQN baseline</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Eval metric</th>
<th style="text-align: right;">Average reward</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">hidden dimension</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">100</td>
</tr>
<tr>
<td style="text-align: left;">Linear</td>
<td style="text-align: right;">$4.4(0.4)$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">PA DQN $(L=1)$</td>
<td style="text-align: right;">$2.0(1.5)$</td>
<td style="text-align: right;">$4.0(1.4)$</td>
<td style="text-align: right;">$4.4(2.0)$</td>
</tr>
<tr>
<td style="text-align: left;">PA DQN $(L=2)$</td>
<td style="text-align: right;">$1.5(3.0)$</td>
<td style="text-align: right;">$4.5(2.5)$</td>
<td style="text-align: right;">$7.9(3.0)$</td>
</tr>
<tr>
<td style="text-align: left;">MA DQN $(L=1)$</td>
<td style="text-align: right;">$2.9(3.1)$</td>
<td style="text-align: right;">$4.0(4.2)$</td>
<td style="text-align: right;">$5.9(2.5)$</td>
</tr>
<tr>
<td style="text-align: left;">MA DQN $(L=2)$</td>
<td style="text-align: right;">$4.9(3.2)$</td>
<td style="text-align: right;">$9.0(3.2)$</td>
<td style="text-align: right;">$7.1(3.1)$</td>
</tr>
<tr>
<td style="text-align: left;">DRRN $(L=1)$</td>
<td style="text-align: right;">$17.1(0.6)$</td>
<td style="text-align: right;">$18.3(0.2)$</td>
<td style="text-align: right;">$18.2(0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">DRRN $(L=2)$</td>
<td style="text-align: right;">$18.4(0.1)$</td>
<td style="text-align: right;">$18.5(0.3)$</td>
<td style="text-align: right;">$\mathbf{1 8 . 7}(0.4)$</td>
</tr>
</tbody>
</table>
<p>Table 2: The final average rewards and standard deviations on "Saving John".
takes each pair of state-action texts as input, and generates a corresponding Q-value.</p>
<p>We use softmax selection, which is widely applied in practice, to trade-off exploration vs. exploitation. Specifically, for each experiencereplay, we first generate 200 episodes of data (about 3 K tuples in "Saving John" and 16 K tuples in "Machine of Death") using the softmax selection rule in (2), where we set $\alpha=0.2$ for the first game and $\alpha=1.0$ for the second game. The $\alpha$ is picked according to an estimation of range of the optimal Q-values. We then shuffle the generated data tuples $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$ update the model as described in Section 2.4. The model is trained with multiple epochs for all configurations, and is evaluated after each experience-replay. The discount factor $\gamma$ is set to 0.9 . For DRRN and all baselines, network weights are initialized with small random values. To prevent algorithms from "remembering" state-action ordering and make choices based on action wording, each time the algorithm/player reads text from the simulator, we randomly shuffle the list of actions. ${ }^{3}$ This will encourage the algorithms to make decisions based on the understanding of the texts that describe the states and actions.</p>
<h3>3.3 Performance</h3>
<p>In Figure 4, we show the learning curves of different models, where the dimension of the hid-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning curves of the two text games.</p>
<table>
<thead>
<tr>
<th>Eval metric</th>
<th>Average reward</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>hidden dimension</td>
<td>20</td>
<td>50</td>
<td>100</td>
</tr>
<tr>
<td>Linear</td>
<td></td>
<td>3.3 (1.0)</td>
<td></td>
</tr>
<tr>
<td>PA DQN (L = 1)</td>
<td>0.9 (2.4)</td>
<td>2.3 (0.9)</td>
<td>3.1 (1.3)</td>
</tr>
<tr>
<td>PA DQN (L = 2)</td>
<td>1.3 (1.2)</td>
<td>2.3 (1.6)</td>
<td>3.4 (1.7)</td>
</tr>
<tr>
<td>MA DQN (L = 1)</td>
<td>2.0 (1.2)</td>
<td>3.7 (1.6)</td>
<td>4.8 (2.9)</td>
</tr>
<tr>
<td>MA DQN (L = 2)</td>
<td>2.8 (0.9)</td>
<td>4.3 (0.9)</td>
<td>5.2 (1.2)</td>
</tr>
<tr>
<td>DRRN (L = 1)</td>
<td>7.2 (1.5)</td>
<td>8.4 (1.3)</td>
<td>8.7 (0.9)</td>
</tr>
<tr>
<td>DRRN (L = 2)</td>
<td>9.2 (2.1)</td>
<td>10.7 (2.7)</td>
<td>11.2 (0.6)</td>
</tr>
</tbody>
</table>
<p>Table 3: The final average rewards and standard deviations on "Machine of Death".</p>
<p>den layers in the DQNs and DRRN are all set to 100. The error bars are obtained by running 5 independent experiments. The proposed methods and baselines all start at about the same performance (roughly -7 average rewards for Game 1, and roughly -8 average rewards for Game 2), which is the random guess policy. After around 4000 episodes of experience-replay training, all methods converge. The DRRN converges much faster than the other three baselines and achieves a higher average reward. We hypothesize this is because the DRRN architecture is better at capturing relevance between state text and action text. The faster convergence for "Saving John" may be due to the smaller observation space and/or the deterministic nature of its state transitions (in contrast to the stochastic transitions in the other game).</p>
<p>The final performance (at convergence) for both baselines and proposed methods are shown in Tables 2 and 3. We test for different model sizes with 20, 50, and 100 dimensions in the hidden layers. The DRRN performs consistently better than all baselines, and often with a lower variance. For Game 2, due to the complexity of the underlying state transition function, we cannot compute the exact optimal policy score. To provide more insight into the performance, we averaged scores of 8 human players for initial trials (novice) and after gaining experience, yielding scores of -5.5 and 16.0, respectively. The experienced players do outperform our algorithm. The converged performance is higher with two hidden layers for all models. However, deep models also converge more slowly than their 1 hidden layer versions, as shown for the DRRN in Figure 4.</p>
<p>Besides an inner-product, we also experimented with more complex interaction functions: a) a bilinear operation with different action side dimensions; and b) a non-linear deep neural network using the concatenated state and action space embeddings as input and trained in an end-to-end fashion to predict Q values. For different configurations, we fix the state side embedding to be 100 dimensions and vary the action side embedding dimensions. The bilinear operation gave similar results, but the concatenation input to a DNN degraded performance. Similar behaviors have been observed on a different task (Luong et al., 2015).</p>
<h3>3.4 Actions with paraphrased descriptions</h3>
<p>To investigate how our models handle actions with "unseen" natural language descriptions, we had two people paraphrase all actions in the game "Machine of Death" (used in testing phase), except a few single-word actions whose synonyms are out-of-vocabulary (OOV). The word-level OOV rate of paraphrased actions is 18.6%,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Scatterplot and strong correlation between Q-values of paraphrased actions versus original actions</p>
<p>and standard 4-gram BLEU score between the paraphrased and original actions is 0.325. The resulting 153 paraphrased action descriptions are associated with 532 unique state-action pairs.</p>
<p>We apply a well-trained 2-layer DRRN model (with hidden dimension 100), and predict Q-values for each state-action pair with fixed model parameters. Figure 5 shows the correlation between Q-values associated with paraphrased actions versus original actions. The predictive R-squared is 0.95, showing a strong positive correlation. We also run Q-value correlation for the NN interaction and pR² = 0.90. For baseline MA-DQN and PA-DQN, their corresponding pR² is 0.84 and 0.97, indicating they also have some generalization ability. This is confirmed in the paraphrasing-based experiments too, where the test reward on the paraphrased setup is close to the original setup. This supports the claim that deep learning is useful in general for this language understanding task, and our findings show that a decoupled architecture most effectively leverages that approach.</p>
<p>In Table 4 we provide examples with predicted Q-values of original descriptions and paraphrased descriptions. We also include alternative action descriptions with in-vocabulary words that will lead to positive / negative / irrelevant game development at that particular state. Table 4 shows actions that are more likely to result in good endings are predicted with high Q-values. This indicates that the DRRN has some generalization ability and gains a useful level of language understanding in the game scenario.</p>
<p>We use the baseline models and proposed DRRN model trained with the original action descriptions for "Machine of Death", and test on paraphrased action descriptions. For this game, the underlying state transition mechanism has not changed. The only change to the game interface is that during testing, every time the player reads the actions from the game simulator, it reads the paraphrased descriptions and performs selection based on these paraphrases. Since the texts in test time are "unseen" to the player, a good model needs to have some level of language understanding, while a naive model that memorizes all unique action texts in the original game will do poorly. The results for these models are shown in Table 5. All methods have a slightly lower average reward in this setting (10.5 vs. 11.2 for the original actions), but the DRRN still gives a high reward and significantly outperforms other methods. This shows that the DRRN can generalize well to "unseen" natural language descriptions of actions.</p>
<h2>4 Related Work</h2>
<p>There has been increasing interest in applying deep reinforcement learning to a variety of problems, but only a few studies address problems with natural language state or action spaces. In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al., 2013). There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping text instructions to sequences of executable actions (Branavan et al., 2009). In some applications, it is possible to manually design features for state-action pairs, which are then used in reinforcement learning to learn a near-optimal policy (Li et al., 2009). Designing such features, however, requires substantial domain knowledge.</p>
<p>The work most closely related to our study involves application of deep reinforcement to learning decision policies for parser-based text games. Narasimhan et al. (2015) applied a Long Short-Term Memory DQN framework, which achieves higher average reward than the random and Bag-of-Words DQN baselines. In this work, actions are constrained to a set of known fixed command structures (one action and one argument object),</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Text (with predicted Q-values)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">State</td>
<td style="text-align: left;">As you move forward, the people surrounding you suddenly look up with terror <br> in their faces, and flee the street.</td>
</tr>
<tr>
<td style="text-align: left;">Actions in the original game</td>
<td style="text-align: left;">Ignore the alarm of others and continue moving forward. (-21.5) Look up. (16.6)</td>
</tr>
<tr>
<td style="text-align: left;">Paraphrased actions (not original)</td>
<td style="text-align: left;">Disregard the caution of others and keep pushing ahead. (-11.9) Turn up and <br> look. (17.5)</td>
</tr>
<tr>
<td style="text-align: left;">Positive actions (not original)</td>
<td style="text-align: left;">Stay there. (2.8) Stay calmly. (2.0)</td>
</tr>
<tr>
<td style="text-align: left;">Negative actions (not original)</td>
<td style="text-align: left;">Screw it. I'm going carefully. (-17.4) Yell at everyone. (-13.5)</td>
</tr>
<tr>
<td style="text-align: left;">Irrelevant actions (not original)</td>
<td style="text-align: left;">Insert a coin. (-1.4) Throw a coin to the ground. (-3.6)</td>
</tr>
</tbody>
</table>
<p>Table 4: Predicted Q-value examples</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Eval metric</th>
<th style="text-align: center;">Average reward</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">hidden dimension</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">PA DQN $(L=2)$</td>
<td style="text-align: center;">$0.2(1.2)$</td>
<td style="text-align: center;">$2.6(1.0)$</td>
<td style="text-align: center;">$3.6(0.3)$</td>
</tr>
<tr>
<td style="text-align: left;">MA DQN $(L=2)$</td>
<td style="text-align: center;">$2.5(1.3)$</td>
<td style="text-align: center;">$4.0(0.9)$</td>
<td style="text-align: center;">$5.1(1.1)$</td>
</tr>
<tr>
<td style="text-align: left;">DRRN $(L=2)$</td>
<td style="text-align: center;">$7.3(0.7)$</td>
<td style="text-align: center;">$8.3(0.7)$</td>
<td style="text-align: center;">$\mathbf{1 0 . 5 ( 0 . 9 )}$</td>
</tr>
</tbody>
</table>
<p>Table 5: The final average rewards and standard deviations on paraphrased game "Machine of Death".
based on a limited action-side vocabulary size. The overall action space is defined by the actionargument product space. This pre-specified product space is not feasible for the more complex text strings in other forms of text-based games. Our proposed DRRN, on the other hand, can handle the more complex text strings, as well as parserbased games. In preliminary experiments with the parser-based game from (Narasimhan et al., 2015), we find that the DRRN using a bag-of-words (BOW) input achieves results on par with their BOW DQN. The main advantage of the DRRN is that it can also handle actions described with more complex language.</p>
<p>The DRRN experiments described here leverage only a simple bag-of-words representation of phrases and sentences. As observed in (Narasimhan et al., 2015), more complex sentence-based models can give further improvements. In preliminary experiments with "Machine of Death", we did not find LSTMs to give improved performance, but we conjecture that they would be useful in larger-scale tasks, or when the word embeddings are initialized by training on large data sets.</p>
<p>As mentioned earlier, other work has applied deep reinforcement learning to a problem with a continuous action space (Lillicrap et al., 2016). In the DRRN, the action space is inherently discrete, but we learn a continuous representation of it. As indicated by the paraphrasing experiment, the continuous space representation seems to generalize
reasonably well.</p>
<h2>5 Conclusion</h2>
<p>In this paper we develop a deep reinforcement relevance network, a novel DNN architecture for handling actions described by natural language in decision-making tasks such as text games. We show that the DRRN converges faster and to a better solution for Q-learning than alternative architectures that do not use separate embeddings for the state and action spaces. Future work includes: (i) adding an attention model to robustly analyze which part of state/actions text correspond to strategic planning, and (ii) applying the proposed methods to more complex text games or other tasks with actions defined through natural language.</p>
<h2>Acknowledgments</h2>
<p>We thank Karthik Narasimhan and Tejas Kulkarni for providing instructions on setting up their parser-based games.</p>
<h2>References</h2>
<p>E. Adams. 2014. Fundamentals of game design. Pearson Education.
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP, pages 82-90, August.
S.R.K. Branavan, D. Silver, and R. Barzilay. 2011. Learning to win by reading manuals in a monte-carlo framework. In Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 268-277. Association for Computational Linguistics.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proc. of the 25th International Conference on Machine learning, pages 160-167. ACM.</p>
<p>G. E Dahl, D. Yu, L. Deng, and A. Acero. 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30-42.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process. Mag., 29(6):82-97.</p>
<p>P-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proc. of the ACM International Conference on Information \&amp; Knowledge Management, pages 23332338. ACM.
R. Kiros, Y. Zhu, R. R Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. 2015. Skipthought vectors. In Advances in Neural Information Processing Systems, pages 3276-3284.
A. Krizhevsky, I. Sutskever, and G. E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105.
Q. V Le and T. Mikolov. 2014. Distributed representations of sentences and documents. In International Conference on Machine Learning.
Y. LeCun, Y. Bengio, and G. Hinton. 2015. Deep learning. Nature, 521(7553):436-444.
L. Li, J. D. Williams, and S. Balakrishnan. 2009. Reinforcement learning for spoken dialog management using least-squares policy iteration and fast feature selection. In Proceedings of the Tenth Annual Conference of the International Speech Communication Association (INTERSPEECH-09), page 24752478.
T. P Lillicrap, J. J Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. 2016. Continuous control with deep reinforcement learning. In International Conference on Learning Representations.</p>
<p>L-J. Lin. 1993. Reinforcement learning for robots using neural networks. Technical report, DTIC Document.</p>
<p>M-T. Luong, H. Pham, and C. D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412-1421, September.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. NIPS Deep Learning Workshop, December.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A Rusu, J. Veness, M. G Bellemare, A. Graves, M. Riedmiller, A. K Fidjeland, G. Ostrovski, et al. 2015. Humanlevel control through deep reinforcement learning. Nature, 518(7540):529-533.
K. Narasimhan, T. Kulkarni, and R. Barzilay. 2015. Language understanding for text-based games using deep reinforcement learning. In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11, September.
R. Nogueira and K. Cho. 2016. Webnav: A new largescale task for natural language based sequential decision making. arXiv preprint arXiv:1602.02261.
K. Scheffler and S. Young. 2002. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proc. of the second International Conference on Human Language Technology Research, pages 12-19.
I. Sutskever, O. Vinyals, and Q. V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104-3112.
R. S Sutton and A. G Barto. 1998. Reinforcement learning: An introduction, volume 1. MIT press Cambridge.
G. Tesauro. 1995. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58-68.
C. JCH Watkins and P. Dayan. 1992. Q-learning. Machine learning, 8(3-4):279-292.
S. Young, M. Gasic, B. Thomson, and J. D Williams. 2013. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160-1179.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ When in a specific state, the simulator presents the possible set of actions in random order, i.e. they may appear in a different order the next time a player is in this same state.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ Simulators are available at https://github.com/ jvking/text-games&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
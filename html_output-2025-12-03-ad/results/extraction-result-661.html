<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-661 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-661</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-661</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-d671d62a1eb4d57343e4a0928297266dffc0c118</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d671d62a1eb4d57343e4a0928297266dffc0c118" target="_blank">SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> SwiftSage is introduced, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks, and significantly outperforms other methods such as SayCan, ReAct, and Reflexion in solving complex interactive tasks.</p>
                <p><strong>Paper Abstract:</strong> We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e661.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e661.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWIFTSAGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SWIFTSAGE: A Generative Agent with Fast and Slow Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-process hybrid agent that combines a small, fast encoder-decoder LM (SWIFT) trained by behavior cloning with a slow, deliberative LLM-based planner/grounder (SAGE) using a two-stage plan-and-ground prompting and an action-buffer; integrates them with a heuristic gating policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SWIFTSAGE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modular dual-process architecture inspired by cognitive System 1/System 2. SWIFT is a small encoder-decoder LM (Flan-T5-large, ~770M) fine-tuned via behavior cloning on oracle trajectories to produce fast per-step actions from recent history. SAGE is a two-stage LLM-based module (e.g., GPT-4) that (1) plans by answering structured Q1–Q5 questions to produce declarative subgoals/object lists and detect exceptions, and (2) grounds those subgoals into a multi-step action sequence (an action buffer) using formalized action templates. A heuristic controller switches between SWIFT and SAGE on conditions (stuck, invalid prediction, critical decision, unexpected observation) and consumes the SAGE action buffer until empty.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Structured, symbolic-like representations used inside SAGE: explicit subgoal lists and answers to Q1–Q5 (locate objects, track objects, plan subgoals, track progress, handle exceptions), formal action templates (e.g., POUR(X,Y), PICK(object), TELEPORT(room)), and structured memory fields (Task:D, Time, Score, Action-history, Current-room, Inventory, Visited rooms). These are not a classical logic-programming engine but are explicit, parseable, declarative subgoal/state representations and action templates used to bridge planner and executor.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural sequence models and procedural execution: (a) SWIFT — a fine-tuned seq2seq Transformer (Flan-T5-large, 770M parameters) trained by behavior cloning to predict the next action from sliding-window history; (b) SAGE — large LLMs (e.g., GPT-4, GPT-3.5-turbo) used as planners and grounders producing multi-step action buffers; plus procedural gating logic and action execution loop that sequentially runs actions against the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular integration with a heuristic gating controller: start with SWIFT; switch to SAGE when one of four conditions is met (K consecutive zero rewards, invalid SWIFT prediction, critical decision, unexpected exception). SAGE runs a two-stage prompting pipeline (planning -> grounding) and returns a parsed action buffer B = {Â̂A_t, Â̂A_{t+1}, ...}. The agent sequentially executes actions from B; if the buffer empties or two invalid actions occur in sequence, control reverts to SWIFT. Integration relies on structured (declarative) outputs from SAGE and formal action templates for reliable parsing and execution, rather than end-to-end differentiable fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Emergent capabilities include (a) long-horizon multi-step planning via SAGE's subgoal decomposition and action buffering (reduces per-action LLM calls), (b) robust exception handling via explicit Q5 self-reflection and re-planning, (c) efficiency gains (fewer LLM tokens per executed action due to multi-action buffering and selective invocation), (d) improved early-stage accuracy from SWIFT's fast predictions combined with SAGE's deliberation for hard/critical steps, and (e) better overall task performance and compositional generalization on unseen ScienceWorld variations relative to either module alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>ScienceWorld complex interactive reasoning benchmark (30 task types; textual simulator for science experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Overall score 84.68 (ScienceWorld benchmark average, percent), tokens-per-action (tpa) 757.07; outperforms SAYCAN (33.82), REACT (36.43), REFLEXION (45.34) on same benchmark (numbers from paper Table 1 and cost analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>SWIFT-only (imperative/neural seq2seq imitation agent) overall score 49.22 (percent) on same ScienceWorld benchmark (reported as Swift-only in the paper / Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Improves generalization to unseen test variations relative to SWIFT-only: SAGE's LLM planning provides zero-shot/subgoal reasoning and compositional planning that helps handle novel object/state combinations; SWIFT alone performs well early but fails to generalize when exceptions or unseen situations arise. The hybrid shows empirically superior out-of-distribution performance on ScienceWorld test splits.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate interpretability: SAGE emits explicit, human-readable subgoals and answers to Q1–Q5 and uses formally-specified action templates, providing inspectable intermediate representations and explainable action buffers; SWIFT remains an opaque neural predictor. There is no formal symbolic proof engine, but parsing of SAGE outputs yields interpretable steps and exception diagnoses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Evaluated only in a textual simulator (ScienceWorld), not in physical/real-world domains; SAGE depends on expensive LLM calls and may produce disallowed/invalid actions which are filtered out; SWIFT may repeat meaningless actions or fail to repair mistakes; heuristic gating is hand-designed (not learned); sometimes fails some long tasks (examples: tasks 9-2 and 1-3 not perfect); no safeguards for hazardous real-world actions; scalability and feasibility of SAGE in resource-constrained settings remain open.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Operates under the dual-process (System 1 / System 2) theory: division of labor where SWIFT provides fast associative action prediction and SAGE provides slow deliberative planning and self-reflection; also uses a plan-and-ground principle (decouple high-level subgoal planning from low-level grounding into executable action sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e661.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e661.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAYCAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan: Grounding Language in Robotic Affordances (as adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach that combines LLM generation of candidate actions with an affordance/value function derived from low-level policies to ground and rank actions by feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can, not as i say: Grounding language in robotic affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SayCan (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM generates a ranked list of candidate actions given textual history and environment state; candidates are reranked by a learned value function (affordance) coming from underlying policies (e.g., value estimates over low-level executors). In this paper SayCan was adapted to ScienceWorld by generating top-K LLM candidates and reranking valid environment actions via SentenceBERT similarity and a value function.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>High-level language outputs from an LLM which act as declarative-style proposals / intents (natural-language action candidates). The paper adaptation does not include an explicit symbolic logic module, but uses structured action candidates as intermediate representations.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Learned value function and underlying policies (procedural/learned affordance estimators) used to score/rerank candidate actions; these are neural policy/value networks or policy modules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular reranking: separate LLM generation of candidates followed by scoring/reranking by the value function (affordance). Requires fresh LLM inference at each time step (per-step planning).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Can ground language-guided proposals to environment-executable actions and bias selection toward feasible actions; provides a mechanism to combine high-level linguistic reasoning with low-level executability confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>ScienceWorld (adapted implementation evaluated as a baseline in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Overall score 33.82 (percent) on ScienceWorld (Table 1); tokens-per-action (tpa) overall ~1855.84 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>LLM-based high-level proposals provide better generalization than purely learned low-level policies alone, but SayCan's per-step LLM usage is expensive and can be less efficient; generalization is limited by grounding and affordance estimator quality.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: LLM proposals are readable intents; affordance scores provide a quantitative grounding signal, but no explicit symbolic reasoning traces beyond candidate lists.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High token cost due to per-step LLM inference; inefficient for long-horizon tasks; grounding remains challenging (mismatch between LLM proposals and environment action space) as noted in the paper; comparatively low performance on ScienceWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Combines language-based planning (declarative intents) with learned affordances from reinforcement/behavior policies (imperative executors), following an intents-to-actions grounding principle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e661.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e661.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REACT: Reason+Act via Virtual 'think' actions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that augments LLM agents with a virtual 'think' action enabling the model to produce intermediate subgoals/chain-of-thought before acting; relies on few-shot in-context examples of correct subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>REACT (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Adds a special 'think' action into the agent action space; when invoked the LLM outputs subgoals/reasoning (chain-of-thought / subgoal steps) which then guide subsequent action generation. Training/in-context examples include annotated subgoals so the model learns when/how to 'think'. In practice the method issues an LLM call at each time step and uses the produced text to select the next action.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Annotated subgoal representations and 'think' outputs from the LLM that act as explicit intermediate (declarative-like) reasoning traces (subgoals).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>LLM-generated actions executed in the environment (procedural step-by-step enactment); no separate low-level learned policy is used in the paper's adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Per-step prompting with in-context few-shot examples: the LLM alternates between 'think' (produce subgoals) and 'act' (produce actions), but every decision requires an LLM call; integration is procedural via the prompting loop rather than a separate symbolic engine.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables the LLM to produce intermediate subgoal structure that can improve short-horizon planning and yields improved performance for certain short/medium tasks relative to single-shot LLM action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>ScienceWorld (adapted baseline evaluated in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Overall score 36.43 (percent) on ScienceWorld with GPT-4; drops to 19.76 with GPT-3.5-turbo in the paper's ablation (Table 1 & Table 3). tokens-per-action ~1971.03 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Provides some improvements via subgoal reasoning but requires human-annotated subgoal examples to teach when to 'think'; generalization is sensitive to LLM choice and available in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Produces human-readable chain-of-thought/subgoal traces that can be inspected; however, the generated traces are part of LLM output and not guaranteed to be formally correct or executable without grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High inference cost (per-step LLM calls), depends on human subgoal annotations for teaching, occasionally plateaus at intermediate scores on long-horizon tasks, and grounding of subgoals to valid environment actions is not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Separates reasoning (virtual 'think') from acting within the LLM’s output sequence; uses in-context learning to teach the model to interleave planning and acting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e661.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e661.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REFLEXION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REFLEXION: Autonomous Agent with Dynamic Memory and Self-Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based multi-round self-reflection method that stores failed trial histories and uses them to refine planning in subsequent attempts, allowing the model to iteratively improve across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>REFLEXION (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Runs multiple rounds/attempts where after each failed trial the LLM analyzes the failure (dynamic memory / reflection) and refines its plan for the next round; uses history of failures as additional context to change future action generation. In the paper this is adapted as a baseline that can run up to four rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Stored trial histories and reflective notes (dynamic memory) that act as structured context informing subsequent plans—these are explicit textual memory traces rather than formal symbolic logic.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>LLM-driven generation of actions for each attempt (procedural enactment) and the environment simulator that executes actions; no separate symbolic executor.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Iterative loop across trials: generate plan -> execute -> collect failure traces -> append to memory -> re-prompt LLM for refined plan. Integration is modular and episodic, relying on LLM re-prompting with augmented memory.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Can improve planning quality across rounds via self-reflection and learning from prior failures, sometimes achieving higher success on short tasks when multiple rounds are allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>ScienceWorld (adapted baseline in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Overall score 45.34 (percent) on ScienceWorld (Table 1); tokens-per-action ~2983.46 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Shows empirical improvements by leveraging failure memory to refine plans, but relies on the ability to perform multiple trials; generalization limited when actions are irreversible or when multiple trials are infeasible in the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Reflection traces and dynamic memory are textual and inspectable, providing human-understandable records of failures and corrections, but not formal symbolic explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires multiple rounds (up to 4 in experiments) which may be impractical in real-world or irreversible-action settings; high token cost; still may fail to ground corrected plans into valid environment actions; performance advantage can be unfair relative to one-shot methods.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Self-reflection via dynamic memory: iterative trial-and-error improvement where LLMs use past failures as additional context to refine future planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e661.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e661.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C: Graph-constrained RL for Natural Language Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid method that extracts symbolic graph representations of environment state using OpenIE and uses these knowledge graphs to constrain and guide policy networks (A2C) for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses OpenIE to convert textual environment observations into a dynamic graph/knowledge-graph representation of state; this symbolic graph is used to restrict or guide the combinations of action templates and objects available to a downstream policy (A2C-style RL), effectively blending symbolic state representation with learned policy selection.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit graph-structured state representation (knowledge graph / OpenIE triples) that encodes objects, relations, and dynamic state updates; used as constraints on permitted actions.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Policy network (A2C reinforcement learning agent) that selects actions subject to graph-derived constraints; learned neural policy for action selection/execution.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular: symbolic KG is built/updated from observations and then used to constrain action template/object combinations available to the policy; integration is via guided masking/constraints rather than end-to-end training coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Reduced combinatorial action space and more informed action selection by leveraging symbolic structure; improved sample efficiency and ability to condition on relational state information that plain policies may not capture.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Referenced / used as a baseline for interactive text environments (ScienceWorld adaptation discussed in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>KG-A2C reported in paper Table 1: overall score 11.37 (percent) on ScienceWorld benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Symbolic graph provides structured inductive biases that can aid generalization across different object configurations; however reported performance on ScienceWorld was modest versus LLM-based hybrids.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability from the knowledge graph representation: state is explicit and human-inspectable, allowing reasoning traces and constraints to be examined.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance lower than LLM-based planners on many ScienceWorld tasks; constructing accurate KGs from noisy textual observations can be error-prone; not sufficient alone for complex commonsense-driven subgoal decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Uses a graph-constrained RL principle: symbolic state graphs constrain and inform policy decision-making to combine the strengths of structured symbolic state with gradient-based policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can, not as i say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection. <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces. <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. <em>(Rating: 2)</em></li>
                <li>Deep reasoning networks: Thinking fast and slow. <em>(Rating: 1)</em></li>
                <li>Thinking fast and slow with deep learning and tree search. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-661",
    "paper_id": "paper-d671d62a1eb4d57343e4a0928297266dffc0c118",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "SWIFTSAGE",
            "name_full": "SWIFTSAGE: A Generative Agent with Fast and Slow Thinking",
            "brief_description": "A dual-process hybrid agent that combines a small, fast encoder-decoder LM (SWIFT) trained by behavior cloning with a slow, deliberative LLM-based planner/grounder (SAGE) using a two-stage plan-and-ground prompting and an action-buffer; integrates them with a heuristic gating policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SWIFTSAGE",
            "system_description": "Modular dual-process architecture inspired by cognitive System 1/System 2. SWIFT is a small encoder-decoder LM (Flan-T5-large, ~770M) fine-tuned via behavior cloning on oracle trajectories to produce fast per-step actions from recent history. SAGE is a two-stage LLM-based module (e.g., GPT-4) that (1) plans by answering structured Q1–Q5 questions to produce declarative subgoals/object lists and detect exceptions, and (2) grounds those subgoals into a multi-step action sequence (an action buffer) using formalized action templates. A heuristic controller switches between SWIFT and SAGE on conditions (stuck, invalid prediction, critical decision, unexpected observation) and consumes the SAGE action buffer until empty.",
            "declarative_component": "Structured, symbolic-like representations used inside SAGE: explicit subgoal lists and answers to Q1–Q5 (locate objects, track objects, plan subgoals, track progress, handle exceptions), formal action templates (e.g., POUR(X,Y), PICK(object), TELEPORT(room)), and structured memory fields (Task:D, Time, Score, Action-history, Current-room, Inventory, Visited rooms). These are not a classical logic-programming engine but are explicit, parseable, declarative subgoal/state representations and action templates used to bridge planner and executor.",
            "imperative_component": "Neural sequence models and procedural execution: (a) SWIFT — a fine-tuned seq2seq Transformer (Flan-T5-large, 770M parameters) trained by behavior cloning to predict the next action from sliding-window history; (b) SAGE — large LLMs (e.g., GPT-4, GPT-3.5-turbo) used as planners and grounders producing multi-step action buffers; plus procedural gating logic and action execution loop that sequentially runs actions against the environment.",
            "integration_method": "Modular integration with a heuristic gating controller: start with SWIFT; switch to SAGE when one of four conditions is met (K consecutive zero rewards, invalid SWIFT prediction, critical decision, unexpected exception). SAGE runs a two-stage prompting pipeline (planning -&gt; grounding) and returns a parsed action buffer B = {Â̂A_t, Â̂A_{t+1}, ...}. The agent sequentially executes actions from B; if the buffer empties or two invalid actions occur in sequence, control reverts to SWIFT. Integration relies on structured (declarative) outputs from SAGE and formal action templates for reliable parsing and execution, rather than end-to-end differentiable fusion.",
            "emergent_properties": "Emergent capabilities include (a) long-horizon multi-step planning via SAGE's subgoal decomposition and action buffering (reduces per-action LLM calls), (b) robust exception handling via explicit Q5 self-reflection and re-planning, (c) efficiency gains (fewer LLM tokens per executed action due to multi-action buffering and selective invocation), (d) improved early-stage accuracy from SWIFT's fast predictions combined with SAGE's deliberation for hard/critical steps, and (e) better overall task performance and compositional generalization on unseen ScienceWorld variations relative to either module alone.",
            "task_or_benchmark": "ScienceWorld complex interactive reasoning benchmark (30 task types; textual simulator for science experiments).",
            "hybrid_performance": "Overall score 84.68 (ScienceWorld benchmark average, percent), tokens-per-action (tpa) 757.07; outperforms SAYCAN (33.82), REACT (36.43), REFLEXION (45.34) on same benchmark (numbers from paper Table 1 and cost analysis).",
            "declarative_only_performance": null,
            "imperative_only_performance": "SWIFT-only (imperative/neural seq2seq imitation agent) overall score 49.22 (percent) on same ScienceWorld benchmark (reported as Swift-only in the paper / Table 3).",
            "has_comparative_results": true,
            "generalization_properties": "Improves generalization to unseen test variations relative to SWIFT-only: SAGE's LLM planning provides zero-shot/subgoal reasoning and compositional planning that helps handle novel object/state combinations; SWIFT alone performs well early but fails to generalize when exceptions or unseen situations arise. The hybrid shows empirically superior out-of-distribution performance on ScienceWorld test splits.",
            "interpretability_properties": "Moderate interpretability: SAGE emits explicit, human-readable subgoals and answers to Q1–Q5 and uses formally-specified action templates, providing inspectable intermediate representations and explainable action buffers; SWIFT remains an opaque neural predictor. There is no formal symbolic proof engine, but parsing of SAGE outputs yields interpretable steps and exception diagnoses.",
            "limitations_or_failures": "Evaluated only in a textual simulator (ScienceWorld), not in physical/real-world domains; SAGE depends on expensive LLM calls and may produce disallowed/invalid actions which are filtered out; SWIFT may repeat meaningless actions or fail to repair mistakes; heuristic gating is hand-designed (not learned); sometimes fails some long tasks (examples: tasks 9-2 and 1-3 not perfect); no safeguards for hazardous real-world actions; scalability and feasibility of SAGE in resource-constrained settings remain open.",
            "theoretical_framework": "Operates under the dual-process (System 1 / System 2) theory: division of labor where SWIFT provides fast associative action prediction and SAGE provides slow deliberative planning and self-reflection; also uses a plan-and-ground principle (decouple high-level subgoal planning from low-level grounding into executable action sequences).",
            "uuid": "e661.0",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SAYCAN",
            "name_full": "SayCan: Grounding Language in Robotic Affordances (as adapted)",
            "brief_description": "A hybrid approach that combines LLM generation of candidate actions with an affordance/value function derived from low-level policies to ground and rank actions by feasibility.",
            "citation_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "mention_or_use": "use",
            "system_name": "SayCan (adapted)",
            "system_description": "LLM generates a ranked list of candidate actions given textual history and environment state; candidates are reranked by a learned value function (affordance) coming from underlying policies (e.g., value estimates over low-level executors). In this paper SayCan was adapted to ScienceWorld by generating top-K LLM candidates and reranking valid environment actions via SentenceBERT similarity and a value function.",
            "declarative_component": "High-level language outputs from an LLM which act as declarative-style proposals / intents (natural-language action candidates). The paper adaptation does not include an explicit symbolic logic module, but uses structured action candidates as intermediate representations.",
            "imperative_component": "Learned value function and underlying policies (procedural/learned affordance estimators) used to score/rerank candidate actions; these are neural policy/value networks or policy modules.",
            "integration_method": "Modular reranking: separate LLM generation of candidates followed by scoring/reranking by the value function (affordance). Requires fresh LLM inference at each time step (per-step planning).",
            "emergent_properties": "Can ground language-guided proposals to environment-executable actions and bias selection toward feasible actions; provides a mechanism to combine high-level linguistic reasoning with low-level executability confirmation.",
            "task_or_benchmark": "ScienceWorld (adapted implementation evaluated as a baseline in this paper).",
            "hybrid_performance": "Overall score 33.82 (percent) on ScienceWorld (Table 1); tokens-per-action (tpa) overall ~1855.84 (Table 4).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "LLM-based high-level proposals provide better generalization than purely learned low-level policies alone, but SayCan's per-step LLM usage is expensive and can be less efficient; generalization is limited by grounding and affordance estimator quality.",
            "interpretability_properties": "Moderate: LLM proposals are readable intents; affordance scores provide a quantitative grounding signal, but no explicit symbolic reasoning traces beyond candidate lists.",
            "limitations_or_failures": "High token cost due to per-step LLM inference; inefficient for long-horizon tasks; grounding remains challenging (mismatch between LLM proposals and environment action space) as noted in the paper; comparatively low performance on ScienceWorld.",
            "theoretical_framework": "Combines language-based planning (declarative intents) with learned affordances from reinforcement/behavior policies (imperative executors), following an intents-to-actions grounding principle.",
            "uuid": "e661.1",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "REACT",
            "name_full": "REACT: Reason+Act via Virtual 'think' actions",
            "brief_description": "A prompting method that augments LLM agents with a virtual 'think' action enabling the model to produce intermediate subgoals/chain-of-thought before acting; relies on few-shot in-context examples of correct subgoals.",
            "citation_title": "React: Synergizing reasoning and acting in language models.",
            "mention_or_use": "use",
            "system_name": "REACT (adapted)",
            "system_description": "Adds a special 'think' action into the agent action space; when invoked the LLM outputs subgoals/reasoning (chain-of-thought / subgoal steps) which then guide subsequent action generation. Training/in-context examples include annotated subgoals so the model learns when/how to 'think'. In practice the method issues an LLM call at each time step and uses the produced text to select the next action.",
            "declarative_component": "Annotated subgoal representations and 'think' outputs from the LLM that act as explicit intermediate (declarative-like) reasoning traces (subgoals).",
            "imperative_component": "LLM-generated actions executed in the environment (procedural step-by-step enactment); no separate low-level learned policy is used in the paper's adaptation.",
            "integration_method": "Per-step prompting with in-context few-shot examples: the LLM alternates between 'think' (produce subgoals) and 'act' (produce actions), but every decision requires an LLM call; integration is procedural via the prompting loop rather than a separate symbolic engine.",
            "emergent_properties": "Enables the LLM to produce intermediate subgoal structure that can improve short-horizon planning and yields improved performance for certain short/medium tasks relative to single-shot LLM action generation.",
            "task_or_benchmark": "ScienceWorld (adapted baseline evaluated in paper).",
            "hybrid_performance": "Overall score 36.43 (percent) on ScienceWorld with GPT-4; drops to 19.76 with GPT-3.5-turbo in the paper's ablation (Table 1 & Table 3). tokens-per-action ~1971.03 (Table 4).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Provides some improvements via subgoal reasoning but requires human-annotated subgoal examples to teach when to 'think'; generalization is sensitive to LLM choice and available in-context examples.",
            "interpretability_properties": "Produces human-readable chain-of-thought/subgoal traces that can be inspected; however, the generated traces are part of LLM output and not guaranteed to be formally correct or executable without grounding.",
            "limitations_or_failures": "High inference cost (per-step LLM calls), depends on human subgoal annotations for teaching, occasionally plateaus at intermediate scores on long-horizon tasks, and grounding of subgoals to valid environment actions is not guaranteed.",
            "theoretical_framework": "Separates reasoning (virtual 'think') from acting within the LLM’s output sequence; uses in-context learning to teach the model to interleave planning and acting.",
            "uuid": "e661.2",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "REFLEXION",
            "name_full": "REFLEXION: Autonomous Agent with Dynamic Memory and Self-Reflection",
            "brief_description": "An LLM-based multi-round self-reflection method that stores failed trial histories and uses them to refine planning in subsequent attempts, allowing the model to iteratively improve across trials.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
            "mention_or_use": "use",
            "system_name": "REFLEXION (adapted)",
            "system_description": "Runs multiple rounds/attempts where after each failed trial the LLM analyzes the failure (dynamic memory / reflection) and refines its plan for the next round; uses history of failures as additional context to change future action generation. In the paper this is adapted as a baseline that can run up to four rounds.",
            "declarative_component": "Stored trial histories and reflective notes (dynamic memory) that act as structured context informing subsequent plans—these are explicit textual memory traces rather than formal symbolic logic.",
            "imperative_component": "LLM-driven generation of actions for each attempt (procedural enactment) and the environment simulator that executes actions; no separate symbolic executor.",
            "integration_method": "Iterative loop across trials: generate plan -&gt; execute -&gt; collect failure traces -&gt; append to memory -&gt; re-prompt LLM for refined plan. Integration is modular and episodic, relying on LLM re-prompting with augmented memory.",
            "emergent_properties": "Can improve planning quality across rounds via self-reflection and learning from prior failures, sometimes achieving higher success on short tasks when multiple rounds are allowed.",
            "task_or_benchmark": "ScienceWorld (adapted baseline in this paper).",
            "hybrid_performance": "Overall score 45.34 (percent) on ScienceWorld (Table 1); tokens-per-action ~2983.46 (Table 4).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Shows empirical improvements by leveraging failure memory to refine plans, but relies on the ability to perform multiple trials; generalization limited when actions are irreversible or when multiple trials are infeasible in the real world.",
            "interpretability_properties": "Reflection traces and dynamic memory are textual and inspectable, providing human-understandable records of failures and corrections, but not formal symbolic explanations.",
            "limitations_or_failures": "Requires multiple rounds (up to 4 in experiments) which may be impractical in real-world or irreversible-action settings; high token cost; still may fail to ground corrected plans into valid environment actions; performance advantage can be unfair relative to one-shot methods.",
            "theoretical_framework": "Self-reflection via dynamic memory: iterative trial-and-error improvement where LLMs use past failures as additional context to refine future planning.",
            "uuid": "e661.3",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "KG-A2C: Graph-constrained RL for Natural Language Action Spaces",
            "brief_description": "A hybrid method that extracts symbolic graph representations of environment state using OpenIE and uses these knowledge graphs to constrain and guide policy networks (A2C) for action selection.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "mention_or_use": "use",
            "system_name": "KG-A2C",
            "system_description": "Uses OpenIE to convert textual environment observations into a dynamic graph/knowledge-graph representation of state; this symbolic graph is used to restrict or guide the combinations of action templates and objects available to a downstream policy (A2C-style RL), effectively blending symbolic state representation with learned policy selection.",
            "declarative_component": "Explicit graph-structured state representation (knowledge graph / OpenIE triples) that encodes objects, relations, and dynamic state updates; used as constraints on permitted actions.",
            "imperative_component": "Policy network (A2C reinforcement learning agent) that selects actions subject to graph-derived constraints; learned neural policy for action selection/execution.",
            "integration_method": "Modular: symbolic KG is built/updated from observations and then used to constrain action template/object combinations available to the policy; integration is via guided masking/constraints rather than end-to-end training coupling.",
            "emergent_properties": "Reduced combinatorial action space and more informed action selection by leveraging symbolic structure; improved sample efficiency and ability to condition on relational state information that plain policies may not capture.",
            "task_or_benchmark": "Referenced / used as a baseline for interactive text environments (ScienceWorld adaptation discussed in paper).",
            "hybrid_performance": "KG-A2C reported in paper Table 1: overall score 11.37 (percent) on ScienceWorld benchmark.",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Symbolic graph provides structured inductive biases that can aid generalization across different object configurations; however reported performance on ScienceWorld was modest versus LLM-based hybrids.",
            "interpretability_properties": "High interpretability from the knowledge graph representation: state is explicit and human-inspectable, allowing reasoning traces and constraints to be examined.",
            "limitations_or_failures": "Performance lower than LLM-based planners on many ScienceWorld tasks; constructing accurate KGs from noisy textual observations can be error-prone; not sufficient alone for complex commonsense-driven subgoal decomposition.",
            "theoretical_framework": "Uses a graph-constrained RL principle: symbolic state graphs constrain and inform policy decision-making to combine the strengths of structured symbolic state with gradient-based policy learning.",
            "uuid": "e661.4",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.",
            "rating": 2
        },
        {
            "paper_title": "Deep reasoning networks: Thinking fast and slow.",
            "rating": 1
        },
        {
            "paper_title": "Thinking fast and slow with deep learning and tree search.",
            "rating": 1
        }
    ],
    "cost": 0.019346,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SwiftSAGE: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</h1>
<p>Bill Yuchen Lin ${ }^{1}$ Yicheng Fu ${ }^{4}$ Karina Yang ${ }^{2}$ Faeze Brahman ${ }^{13}$ Shiyu Huang ${ }^{5}$<br>Chandra Bhagavatula ${ }^{1}$ Prithviraj Ammanabrolu ${ }^{67}$ Yejin Choi ${ }^{31}$ Xiang Ren ${ }^{21}$<br>${ }^{1}$ Allen Institute for Artificial Intelligence<br>${ }^{2}$ University of Southern California ${ }^{3}$ University of Washington ${ }^{4}$ Tsinghua University<br>${ }^{5} 4$ Paradigm Inc. ${ }^{6}$ University of California, San Diego ${ }^{7}$ MosaicML<br>https://swiftsage.github.io</p>
<h4>Abstract</h4>
<p>We introduce SWIFTSAGE, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SWIFTSAGE integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the SWIFT module, representing fast and intuitive thinking, and the SAGE module, emulating deliberate thought processes. The SWIFT module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the SAGE module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SWIFTSAGE significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The advancement of artificial general intelligence is largely dependent on the development of agents that are proficient in complex interactive reasoning tasks. These agents should be capable of exhibiting problem-solving abilities akin to humans within dynamic, open-world environments [26, 7]. For example, the ScienceWorld benchmark [36] features a task where an agent must determine the electrical conductivity of an unknown object. In a simulated environment, the agent must navigate to appropriate rooms, locate and acquire essential items, such as batteries and light bulbs, build a circuit, perform an experiment, and interpret the results. Tackling such a complex interactive task demands agents to exhibit long-horizon planning, long-term memorization, subgoal decomposition, spatial reasoning, exception handling, and commonsense knowledge capabilities [37].
There are three primary approaches to developing agents capable of addressing complex interactive reasoning tasks: (1) (deep) reinforcement learning (RL), (2) behavior cloning (BC) [34] through sequence-to-sequence (seq2seq) learning [33], and (3) prompting large language models (LLMs) [6]. In addition to conventional RL methods such as DRRN [14], interactive reasoning can be framed as a seq2seq task, where the input text serves as the current state description and the output text corresponds to the subsequent action [9, 3]. By leveraging numerous gold trajectories generated by oracle agents, it becomes feasible to fine-tune Transformer models [35], like T5 [25], to effectively imitate the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparing methods of prompting LLMs to build agents for interactive tasks.
behavior of these oracle agents. Recent studies have also demonstrated that generative agents based on prompting LLMs, such as GPT-4, can produce reasonable plans and actions [18, 15, 32].
Although the aforementioned methods exhibit remarkable performance in relatively simple tasks, their ability to generalize to more complex and demanding tasks is limited. Both RL-based and seq2seq-based BC approaches effectively acquire knowledge from the environment through largescale interactions and learn general action patterns from oracle agents. However, they face difficulties in decomposing tasks into subgoals, maintaining long-term memory, generalizing to unseen tasks, and handling exceptions. In contrast, instruction-tuned LLMs [24] demonstrate the ability to generate reasonable high-level plans for complex tasks and adapt their outputs based on human feedback. Yet, grounding their outputs to executable actions in the environment remains a challenge. These procedures also lack the capability to efficiently handle environment-specific exceptions that prevent agents from adhering to the LLM's plans. Additionally, previous methods such as SAYCAN [1], REACT [41] and REFLEXION [30] require a new inference with LLMs for each time step, making them considerably costly and inefficient (see Figure 1).
Inspired by the dual process theory [39, 16], we propose a novel framework that enables agents to closely emulate how humans solve complex, open-world tasks. The dual-process theory posits that human cognition is composed of two distinct systems: System 1, characterized by rapid, intuitive, and automatic thinking; and System 2, which entails methodical, analytical, and deliberate thought processes. System 1 is reminiscent of seq2seq methods, which learn through imitation of oracle agents and primarily operate utilizing shallow action patterns. Conversely, System 2 bears resemblance to LLMs that excel in applying commonsense knowledge, engaging in step-by-step reasoning, devising subgoal strategies, and exercising self-reflection. Thus, our proposed method, SWIFTSAGE, is designed to enable both fast and slow thinking in complex interactive reasoning tasks. It effectively integrates the strengths of behavior cloning (representing System 1) and prompting LLMs (emulating System 2), resulting in significant enhancements in task completion performance and efficiency.
Specifically, SWIFTSAGE consists of two primary modules: the SWIFT module and the SAGE module. The SWIFT module is a small encoder-decoder LM, fine-tuned on a T5-large (770m) checkpoint using the searched oracle trajectories of training tasks. It encodes short-term memory components, such as previous actions, observations, visited locations, as well as the current environment state. Then, it decodes the next individual action. This module simulates the fast, intuitive thinking characteristic of System 1. The SAGE module, representing the deliberate thinking of System 2, utilizes LLMs, such as GPT-4, and is structured around two prompting stages: planning and grounding. In the planning stage, we prompt LLMs to locate necessary items, plan and track subgoals, as well as detect and fix potential exceptions and mistakes. In the grounding stage, we focus on utilizing LLMs to transform the output subgoals derived from the planning stage into a sequence of actions by demonstrating potential action templates. Unlike prior methods, where LLMs only generate the next immediate action, our procedures engage in longer-term action planning. To harmoniously integrate the SWIFT and SAGE modules, we developed a heuristic algorithm that determines when to (de)activate the SAGE module and how to combine the outputs effectively with an action buffer mechanism.</p>
<p>In a comprehensive evaluation on 30 task types from the ScienceWorld benchmark, SWIFTSAGE significantly outperforms other methods, achieving a state-of-the-art average score of 84.7. In comparison, SAYCAN scores 33.8, REACT obtains 36.4, and REFLEXION reaches 45.3. Moreover, SWIFTSAGE is more cost-effective and efficient, requiring much fewer tokens per action for LLM inference than previous methods. This considerable performance advantage highlights the effectiveness and efficiency of the SWIFTSAGE framework in addressing complex interactive tasks.</p>
<h1>2 Background and Related Work</h1>
<h3>2.1 Complex Interactive Reasoning</h3>
<p>We define interactive reasoning as the problems where agents are tasked with accomplishing a goal within an interactive environment, typically simulated by engines such as AI2Thor [17] and TextWorld [11]. Our focus lies on the textual environment of ScienceWorld [36] and the complex interactive tasks it supports. Simple interactive tasks, like those created in ALFWorld [31] and TWC [21], primarily involve searching for and placing objects as well as performing basic actions within a single location. Many of these simple tasks have been almost solved by recent works.</p>
<p>In contrast, tasks in ScienceWorld exhibit greater complexity, characterized by more challenging task planning and a significantly larger action space (encompassing 10 locations, 200 types of objects with varying states, and 25 types of actions). Furthermore, agents may encounter random, unforeseen obstacles, such as broken stoves or missing soil, which hinder the execution of planned actions. As a result, agents must adapt and re-plan accordingly, for example, by seeking alternative heat sources or using a shovel on the outside ground to get soil. These challenges demand that agents possess skills in long-horizon planning, long-term memory, subgoal decomposition, exception handling, and commonsense knowledge-capabilities that are not explicitly required for simple interactive tasks.</p>
<h3>2.2 Reinforcement Learning and Imitation Learning Methods</h3>
<p>DRRN. Interactive tasks can naturally be framed as partially-observable Markov decision processes (POMDPs), enabling the application of RL-based methods. Deep Reinforced Relevance Network (DRRN) [14] is a standard baseline method to learn agents within text environment. It aims to learn representations of observations and actions separately and train a policy network to select actions from candidates based on feedback from the simulated environment. CALM [40] is a rerankingbased method that combines DRRN with a causal language model (LM) fine-tuned with oracle transcripts. In essence, the causal LM captures task-specific and environment-specific knowledge through imitation learning, and the DRRN learns to rerank the predictions from the LM.</p>
<p>The KG-A2C [2] method uses an OpenIE technique [4] to represent environment states with graph structures and dynamically update these graphs. These graphs guide policy networks by constraining the combinations of action templates and objects. This method has been shown to be effective in other domains such as for multimodal embodied agents [22].</p>
<p>Behavior cloning for offline imitation learning. Behavior cloning is an imitation learning method that trains a seq2seq Transformer offline with action transcripts of similar training tasks generated by oracle agents [34, 3]. During training, it uses the previous action, observation at time step $t-1$, and the current observation as input and learns to output the next action. The Text Decision Transformer (TDT) is a textual variant of the Decision Transformer [9], which also employs behavior cloning and uses the same data. The primary innovation of TDT is the introduction of reward-to-go as part of the inputs, enabling the model to learn predicting actions that maximize future expected rewards.</p>
<h3>2.3 Prompting LLMs for Action Planning.</h3>
<p>Language models (LLMs) such as GPT-4 have shown promise for action planning in interactive tasks [18, 15, 32, 38]. In this paper, we adapt three prominent methods to complex interactive reasoning tasks in ScienceWorld: SAYCAN [1], REACT [41], and REFLEXION [30].
SAYCAN [1] is a straightforward agent that integrates an LLM with a value function of underlying policies regarding grounding affordances (i.e., the feasibility of an action in the environment). We</p>
<p>need to provide the history and current environment as textual inputs to LLMs for generating a ranked list of action candidates. This action list is then reranked based on a value function.
REACT [41] presents a virtual 'think' action, enabling LLMs to generate subgoals during action planning. This approach requires human annotators to supply examples of correct subgoals for each task type, employing few-shot in-context learning to teach LLMs when and how to 'think' in order to plan subsequent subgoals, in addition to providing complete action trajectories.
REFLEXION [30], a recent work building on REACT, proposes a multi-round approach enabling LLMs to use the history of previously failed rounds to refine their planning for the next round. This self-reflection mechanism helps LLMs improve after each failed attempt. However, this may not be practical in real-world applications for many tasks, as actions in failed trials can be irrecoverable.
All three methods require a new LLM inference at each time step to predict the next immediate action, resulting in inefficient and costly agents. REACT and REFLEXION require human annotations of correct subgoals for each unseen task type. Moreover, it is difficult to generalize REFLEXION to real-world situations where trial-and-error approaches can be infeasible for embodied tasks.</p>
<h1>2.4 Dual-Process Theory</h1>
<p>The dual-process theory [39, 16] is a cognitive psychological framework proposing the existence of a fast and a slow thinking systems in the human brain. This influential theory has found widespread applications across various fields, highlighting the critical role of both systems in shaping human cognition [5, 8, 12, 20, 23]. By integrating the complementary strengths of both systems, agents can effectively and efficiently handle diverse challenges in real-world scenarios. Inspired by this, we aim to construct a generative agent that utilizes a small seq2seq LM as System 1 for associative reasoning via behavior cloning while developing System 2 for analytical reasoning by prompting LLMs.</p>
<h2>3 Swiftsage: A Generative Agent with Fast and Slow Thinking</h2>
<p>In this section, we first establish the problem. Then, we present the two core modules, SWIFT and SAGE, individually. Lastly, we demonstrate the integration of these two modules., resulting in a harmonious and effective interactive reasoning process.</p>
<h3>3.1 Problem Formulation</h3>
<p>Environment and tasks. We focus on complex interactive reasoning tasks situated in virtual textual environments such as ScienceWorld [36]. ScienceWorld provides an optimal setting for developing and evaluating agents in complex tasks, comprising 30 distinct task types covering 10 topics in science experiments. It features 10 locations, including an art studio, workshop, kitchen, living room, bedroom, bathroom, foundry, greenhouse, outdoor area, and a connecting hallway. The environment includes 200+ object types with multiple states (e.g., open, activated) and supports 25 action templates, resulting in an intractable search space. The simulator can generate numerous variations of each task type, providing a rich training ground. In each variation, the agent and environment initialization, such as the locations and states of objects, will differ. A plethora of training variations encompassing all task types are available for training agents. Additionally, it provides a handcrafted oracle agent to search for successful transcripts with minimal actions for offline learning.</p>
<p>Evaluation is done on a set of testing variations with unseen combinations of required objects and situations, thus substantially different from the training variations. For example, a training variation may involve boiling water, while a testing variation could require boiling tin. Therefore, it is crucial to ensure the agent's compositional generalization ability for effectively handling real-world scenarios.</p>
<p>Interactions. Given a task variation, an agent is provided with the task description $D$ and the initial environment state $(t=0)$. The task description $D$ is a text specifying a high-level goal, e.g., "Your task is to test if an unknown substance $A$ is electronically conductive." At each time step $t$, the agent generates an action $A_{t}$ based on a set of supported action templates (e.g., pick up X, use X on Y). $A_{0}$ is always "look around" for showing initial environment information. Upon receiving an action from the agent, the environment produces feedback in four dimensions:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An example of how SwIFTSAGE works with fast and slow thinking. The SWIFT module is offline trained via imitation learning with a small LM such as T5-large ( 770 m ). When it is necessary, for example, encountering an exception, we switch to the SAGE module that prompts LLMs (e.g., GPT-4) for planning and grounding the next subgoals, resulting in an action buffer.</p>
<ul>
<li>Observation $O_{t}$ provides direct feedback on the action $A_{t}$ regarding its effects on the environment or the information queried. For example, an $A_{t}$ of "use thermometer on the substance in metal pot" may result in an $O_{t}$ like "The temperature is 80 F ."</li>
<li>Environment $E_{t}$ represents the current room in which the agent is situated and provides details about all visible objects. Object visibility is based on container states, e.g., objects within a closed fridge are not included in $E_{t}$ until the agent performs an action like "open fridge."</li>
<li>Inventory $I_{t}$ lists objects picked up by the agent, which is particularly useful when agents collect items from different locations to complete the task.</li>
<li>Score $S_{t}$ represents the agent's cumulative score ranging from 0 to 100 . When a required intermediate state is achieved, the score increases with a positive reward.</li>
</ul>
<h1>3.2 SWIFT: The Module for Intuitive and Associative Thinking via Imitation Learning</h1>
<p>Imitation learning is used to construct an agent that learns to mimic oracle agents in various training scenarios through seq2seq learning. Previous methods, such as TDT [36], mainly employ one-hop history as input context and learn to output the subsequent action $A_{t}$ [36]. However, these methods exhibit limitations due to their restricted context of action history and harmful biases arising from data imbalance. To address these issues, we introduce our SWIFT module, depicted in Figure 2.</p>
<p>Representation for longer history. We expand the conventional one-hop BC to multi-hop by incorporating a sliding window of observations and rewards for the $K=10$ recent actions. Additionally, we include a special field for visited rooms (without duplication). This approach aims to provide agents with a longer context and prevent unnecessary room navigation. The input format is as follows: "Task: $D$; Time: $t-1$; Score: $S_{t-1}$; Action history: $\left[A_{t-i}\left(+R_{t-i}\right) \rightarrow\right.$ $\left.O_{t-i}\right] / <em>$ i loops from $K$ to $1 * /$; Current room: $E_{t-1}$; Inventory: $I_{t-1}$; Visited rooms: $\left{E_{1}^{</em>}, \ldots, E_{t-1}^{<em>}\right}$ ". Here, $R_{t}=S_{t}-S_{t-1}$ represents the reward at $t$, and $E_{t}^{</em>}$ is the location name at $t$.</p>
<p>Balanced imitation learning. To avoid bias caused by data imbalance for seq2seq learning, we down-sampled specific types of tasks and actions to achieve a more balanced final dataset for training. We used the T5-large with 770 million parameter and instruction-following ability [10], creating an efficient agent that we named SWIFT. Our empirical results show that the SWIFT module performs much better than TDT ( 11 billion) despite being 15 x smaller in size.</p>
<p>The SWIFT module exhibits greater accuracy during initial time steps, enabling it to attain higher scores in the early stages of a complex task. However, it often fails to generalize to unseen situations. The module also has a tendency to repeat meaningless actions when its learned plans yield exceptions from the environment (e.g., the broken stove in Figure 2). This is partly due to the nature of imitation learning, which prioritizes emulating the observable actions of oracle agents rather than their intrinsic planning abilities. Besides, since the oracle trajectories contain only the shortest, correct actions, it is thus also challenging for the SWIFT to learn how to fix mistaken actions.</p>
<h1>3.3 SAGE: The Module for Deliberate and Analytical Thinking via Prompting LLMs</h1>
<p>While the SWIFT module acquires surface knowledge about the environment and task types through imitation learning, it lacks two key abilities essential for complex interactive reasoning: 1) generalizable planning and tracking of subgoals, and 2) robust handling of exceptions. Prior research has shown that LLMs outperform smaller LMs in these abilities. They can perform step-by-step reasoning to devise concrete plans for tasks and self-refine their outcomes. However, the performance of prior methods remains unsatisfactory in complex interactive tasks such as those in ScienceWorld.
We introduce a novel two-stage approach, named SAGE. This method initially acquires higher-level recommendations from LLMs during the planning stage, followed by their translation into specific action sequences in the grounding stage. By decoupling the planning and grounding processes, SWIFTSAGE effectively generates a series of actions for completing the planned subgoals.</p>
<p>Planning stage. In this stage, we leverage LLMs to plan based on the current state. Specifically, we prompt LLMs with a single prompt that includes a summarized version of the task description and action history, and asks the following five key questions:</p>
<ul>
<li>Q1 (locate objects): "To complete the task, which objects do I need to collect? Please list them and their possible locations one by one."</li>
<li>Q2 (track objects): "Are there any objects that have not been collected yet?"</li>
<li>Q3 (plan subgoals): "To complete the task most efficiently, what are the important subgoals to achieve? Please list the subgoals one by one."</li>
<li>Q4 (track progress): "Considering these subgoals, what have I already completed? And which subgoal should I focus on right now?"</li>
<li>Q5 (handle exceptions): "Have I made any mistakes that might prevent me from efficiently completing the next subgoal? If any, how should I fix them?"</li>
</ul>
<p>Before posing the five planning-related questions, we condense the entire action history ( $A_{&lt;t}$ and $O_{&lt;t}$ ), and the current environment information $E_{t-1}$. Q1 and Q2 pertain to objects, as acquiring all necessary objects serves as the foundation for effective task planning. By addressing these questions, we ensure that LLMs develop a comprehensive understanding of the current environment. Q3 prompts LLMs to engage in step-by-step planning by decomposing the task into a series of subgoals. Q4 acts as a follow-up question, allowing the agent to monitor its progress based on the action history and determine completed subgoals, subsequently focusing on the remaining tasks. Lastly, Q5 is employed to identify and address potential exceptions. These questions can be further tailored with additional environment-specific hints, thereby enhancing their adaptability.
To improve the structure of the LLMs' outputs and facilitate parsing, we incorporate additional instructions in the prompt. By utilizing a single input to obtain answers to all five questions in one output, rather than engaging in multiple rounds of interactive prompting, our approach is more efficient and cost-effective than the iterative prompting methods.
Q4 and Q5 are of primary importance, while Q1-Q3 serve as auxiliary guidance for the LLMs. If the action history indicates a mistaken action or an unachievable previous subgoal, the response to Q5 refines the answer to Q4 through self-reflection on the fly. This approach differs from the REFLEXION agent, which only prompts reflective questions at the end of a failed trial, allowing agents to improve their planning in subsequent attempts. In contrast, our method detects exceptions and errors each time the agent plans for the next subgoals, enabling earlier correction of the agent's behavior.</p>
<p>Grounding stage. While the answers to Q1-Q5 provide valuable guidance for agents, they are not directly executable. Converting plans into valid actions that can be accepted by the environment</p>
<p>remains a challenge. Previous methods using LLMs over-generate candidates, and they rely on reranking or filtering based on the action space to select the next action. However, this is inefficient and inaccurate for complex tasks with vast action spaces. Additionally, these methods generate a single action at a time, which can be both costly and ineffective for long-horizon tasks.</p>
<p>To tackle these issues, we first present supported action types using a formal style accompanied by remarks. For instance, the action type "pour X into Y" is introduced as "POUR (X, Y) : pour object $X$ into container $Y$; e.g., pour red paint into wood cup". More examples:</p>
<div class="codehilite"><pre><span></span><code><span class="n">TELEPORT</span><span class="w"> </span><span class="p">(</span><span class="n">room</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">directly</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">TELEPORT</span><span class="w"> </span><span class="p">(</span><span class="n">kitchen</span><span class="p">)</span>
<span class="n">PICK</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">inventory</span>
<span class="k">OPEN</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">search</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">things</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="k">OPEN</span><span class="w"> </span><span class="p">(</span><span class="n">freezer</span><span class="p">).</span>
<span class="n">ACTIVATE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">activate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">sink</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">stove</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="n">DEACTIVATE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">deactivate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="k">off</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span>
<span class="n">EXAMINE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">look</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">carefully</span><span class="p">.</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="n">EXAMINE</span><span class="w"> </span><span class="p">(</span><span class="n">light</span><span class="w"> </span><span class="n">bulb</span><span class="p">).</span>
<span class="n">MOVE</span><span class="p">(</span><span class="k">object</span><span class="p">,</span><span class="w"> </span><span class="n">place</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">move</span><span class="o">/</span><span class="n">place</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">place</span>
</code></pre></div>

<p>We then incorporate the LLM's outputs from the planning stage as part of the input for the grounding stage. Furthermore, we provide the recent action history of the past 10 time steps as context. Finally, we prompt LLMs to concentrate on the next subgoal and convert it into a list of actions (rather than a single action) to accomplish the next subgoal. Our formatting instructions enable the straightforward splitting and conversion of output actions from LLMs in the grounding stage back to their original action representations. We denote this list of actions generated by LLMs as the action buffer: $B=\left{\hat{A}<em t_1="t+1">{t}, \hat{A}</em>, \ldots\right}$. One can opt to use only answers to Q4 and Q5 to reduce computational costs. Our small-scale ablation study indicates that incorporating answers to Q1-Q3 in the grounding stage proves beneficial, yielding a gain of about 2 points for short tasks on average.</p>
<h1>3.4 Integration of Fast and Slow Thinking</h1>
<p>Having described the SWIFT and SAGE modules, we now address the question of how to merge both modules and effectively integrate fast and slow thinking within the SWIFTSAGE agent. We establish a heuristic algorithm to control the activation and deactivation of the two modules.</p>
<p>Initially, we employ the SWIFT module due to its superior intuitive reasoning capabilities, which facilitate accurate associations between the task description and the environment during the first few actions. We will switch from SWIFT mode to SAGE when any of the following conditions are met:</p>
<p>1) Stuck: There are $\mathrm{K}=5$ consecutive time steps with zero reward $\left(\sum_{i=t-5}^{t-1} R_{i}=0\right)$.
2) Invalid: The SWIFT's prediction for the next action $\left(A_{t}^{\prime}\right)$ is invalid in the current environment.
3) Critical: $A_{t}^{\prime}$ involves a critical decision, e.g., giving the final answer for the experiment result.
4) Unexpected: The observation of $A_{t}^{\prime}$ suggests that an exception is encountered.</p>
<p>Upon activating the SAGE module, we execute the two-stage prompting process and generate an action buffer. We attempt to execute each predicted action and revert to the SWIFT module when the buffer is empty. This approach enables a seamless integration of both modules, providing an efficient and robust problem-solving process for the SWIFTSAGE agent. The pseudo code for illustrating the SwiftSage framework is shown in Fig. 4 (Appendix).</p>
<h2>4 Evaluation</h2>
<h3>4.1 Evaluation Setup</h3>
<p>To evaluate the effectiveness of SWIFTSAGE and other baseline methods in complex interactive reasoning tasks, we use the ScienceWorld benchmark. In Section 2.1 and Section 3.1, we introduce the benchmark and problem formulation. Each task type is categorized as 'short' (S), 'medium' (M), or 'long' (L) based on the average length of the oracle truth trajectories. However, the length of the task does not necessarily indicate its level of difficulty as some tasks may require additional commonsense knowledge. Further evaluation details are provided in the appendix.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">*Len</th>
<th style="text-align: center;">DRRN</th>
<th style="text-align: center;">KGA2C</th>
<th style="text-align: center;">CALM</th>
<th style="text-align: center;">TDT</th>
<th style="text-align: center;">SayCan</th>
<th style="text-align: center;">ReAct</th>
<th style="text-align: center;">Reflexion</th>
<th style="text-align: center;">SwiftSage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1-1 (L)</td>
<td style="text-align: center;">107.7</td>
<td style="text-align: center;">3.52</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">33.06</td>
<td style="text-align: center;">3.52</td>
<td style="text-align: center;">4.22</td>
<td style="text-align: center;">97.04</td>
</tr>
<tr>
<td style="text-align: center;">1-2 (L)</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">3.52</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">10.39</td>
<td style="text-align: center;">13.70</td>
<td style="text-align: center;">10.61</td>
<td style="text-align: center;">87.04</td>
</tr>
<tr>
<td style="text-align: center;">1-3 (L)</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.88</td>
<td style="text-align: center;">3.88</td>
<td style="text-align: center;">7.78</td>
<td style="text-align: center;">7.78</td>
<td style="text-align: center;">72.78</td>
</tr>
<tr>
<td style="text-align: center;">1-4 (L)</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">9.88</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">2-1 (M)</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">6.56</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">6.16</td>
<td style="text-align: center;">26.37</td>
<td style="text-align: center;">7.19</td>
<td style="text-align: center;">5.92</td>
<td style="text-align: center;">99.17</td>
</tr>
<tr>
<td style="text-align: center;">2-2 (M)</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">6.43</td>
<td style="text-align: center;">8.03</td>
<td style="text-align: center;">6.10</td>
<td style="text-align: center;">28.59</td>
<td style="text-align: center;">88.17</td>
</tr>
<tr>
<td style="text-align: center;">2-3 (L)</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">19.87</td>
<td style="text-align: center;">17.41</td>
<td style="text-align: center;">22.37</td>
<td style="text-align: center;">22.37</td>
<td style="text-align: center;">95.73</td>
</tr>
<tr>
<td style="text-align: center;">3-1 (S)</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">40.55</td>
<td style="text-align: center;">52.14</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">88.67</td>
</tr>
<tr>
<td style="text-align: center;">3-2 (M)</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">14.26</td>
<td style="text-align: center;">22.50</td>
<td style="text-align: center;">54.33</td>
<td style="text-align: center;">17.45</td>
<td style="text-align: center;">55.33</td>
</tr>
<tr>
<td style="text-align: center;">3-3 (M)</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">10.16</td>
<td style="text-align: center;">99.56</td>
<td style="text-align: center;">76.19</td>
<td style="text-align: center;">72.54</td>
<td style="text-align: center;">71.90</td>
</tr>
<tr>
<td style="text-align: center;">3-4 (M)</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">9.52</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">21.65</td>
<td style="text-align: center;">47.76</td>
<td style="text-align: center;">88.81</td>
<td style="text-align: center;">70.22</td>
<td style="text-align: center;">77.86</td>
</tr>
<tr>
<td style="text-align: center;">4-1 (S)</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">41.93</td>
<td style="text-align: center;">22.87</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">64.93</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">4-2 (S)</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">55.76</td>
<td style="text-align: center;">58.18</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">87.27</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">4-3 (S)</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">21.67</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">27.82</td>
<td style="text-align: center;">20.87</td>
<td style="text-align: center;">53.33</td>
<td style="text-align: center;">16.42</td>
<td style="text-align: center;">91.67</td>
</tr>
<tr>
<td style="text-align: center;">4-4 (S)</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">19.17</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">47.15</td>
<td style="text-align: center;">31.43</td>
<td style="text-align: center;">27.50</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">5-1 (L)</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">6.89</td>
<td style="text-align: center;">9.92</td>
<td style="text-align: center;">9.06</td>
<td style="text-align: center;">7.33</td>
<td style="text-align: center;">74.59</td>
</tr>
<tr>
<td style="text-align: center;">5-2 (L)</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">14.29</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">11.86</td>
<td style="text-align: center;">13.93</td>
<td style="text-align: center;">18.57</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">93.93</td>
</tr>
<tr>
<td style="text-align: center;">6-1 (M)</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">15.77</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">15.10</td>
<td style="text-align: center;">47.81</td>
<td style="text-align: center;">51.04</td>
<td style="text-align: center;">70.35</td>
<td style="text-align: center;">49.40</td>
</tr>
<tr>
<td style="text-align: center;">6-2 (S)</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">15.70</td>
<td style="text-align: center;">39.26</td>
<td style="text-align: center;">58.89</td>
<td style="text-align: center;">70.67</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">6-3 (M)</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">10.37</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">5.25</td>
<td style="text-align: center;">19.72</td>
<td style="text-align: center;">40.74</td>
<td style="text-align: center;">15.77</td>
<td style="text-align: center;">91.48</td>
</tr>
<tr>
<td style="text-align: center;">7-1 (S)</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">7-2 (S)</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.43</td>
<td style="text-align: center;">67.50</td>
<td style="text-align: center;">67.50</td>
<td style="text-align: center;">84.37</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: center;">7-3 (S)</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">8.34</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">93.33</td>
</tr>
<tr>
<td style="text-align: center;">8-1 (M)</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">20.91</td>
<td style="text-align: center;">27.67</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">89.0</td>
</tr>
<tr>
<td style="text-align: center;">8-2 (S)</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">68.50</td>
</tr>
<tr>
<td style="text-align: center;">9-1 (L)</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.53</td>
<td style="text-align: center;">21.94</td>
<td style="text-align: center;">40.50</td>
<td style="text-align: center;">50.63</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: center;">9-2 (L)</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">14.66</td>
<td style="text-align: center;">32.26</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">9-3 (L)</td>
<td style="text-align: center;">123.1</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">9.12</td>
<td style="text-align: center;">13.67</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">70.62</td>
<td style="text-align: center;">60.0</td>
</tr>
<tr>
<td style="text-align: center;">10-1 (L)</td>
<td style="text-align: center;">130.1</td>
<td style="text-align: center;">16.80</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.51</td>
<td style="text-align: center;">67.53</td>
<td style="text-align: center;">25.70</td>
<td style="text-align: center;">50.90</td>
<td style="text-align: center;">92.30</td>
</tr>
<tr>
<td style="text-align: center;">10-2 (L)</td>
<td style="text-align: center;">132.1</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.29</td>
<td style="text-align: center;">59.45</td>
<td style="text-align: center;">16.80</td>
<td style="text-align: center;">23.69</td>
<td style="text-align: center;">77.60</td>
</tr>
<tr>
<td style="text-align: center;">Short</td>
<td style="text-align: center;">11.76</td>
<td style="text-align: center;">28.08</td>
<td style="text-align: center;">22.70</td>
<td style="text-align: center;">11.30</td>
<td style="text-align: center;">28.37</td>
<td style="text-align: center;">43.83</td>
<td style="text-align: center;">48.79</td>
<td style="text-align: center;">71.47</td>
<td style="text-align: center;">92.22</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">28.58</td>
<td style="text-align: center;">10.85</td>
<td style="text-align: center;">6.88</td>
<td style="text-align: center;">2.88</td>
<td style="text-align: center;">10.36</td>
<td style="text-align: center;">36.58</td>
<td style="text-align: center;">44.01</td>
<td style="text-align: center;">35.43</td>
<td style="text-align: center;">77.79</td>
</tr>
<tr>
<td style="text-align: center;">Long</td>
<td style="text-align: center;">94.30</td>
<td style="text-align: center;">8.26</td>
<td style="text-align: center;">4.92</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">23.65</td>
<td style="text-align: center;">21.07</td>
<td style="text-align: center;">30.17</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">49.26</td>
<td style="text-align: center;">15.56</td>
<td style="text-align: center;">11.37</td>
<td style="text-align: center;">5.07</td>
<td style="text-align: center;">14.66</td>
<td style="text-align: center;">33.82</td>
<td style="text-align: center;">36.43</td>
<td style="text-align: center;">45.34</td>
<td style="text-align: center;">84.68</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on the ScienceWorld benchmark. <em>Len is the average length of the oracle agent's trajectories. In addition to overall results, we also report performance on three groups of </em>Len (short, medium, long). The last four methods use GPT-4 as the base LLM for prompting. We show more details of these tasks and results on other LLMs in the appendix.</p>
<h1>4.2 Baseline Agents</h1>
<p>In addition to the baseline methods evaluated in the ScienceWorld paper, such as DRRN, CALM, KG-A2C, and TDT, we incorporate three LLM-based prompting techniques: SAYCAN, REACT, and Reflexion, as detailed in Section 2.3 and Figure 1. This subsection presents the implementation details for adapting these methods to build ScienceWorld agents.</p>
<p>SAYCAN necessitates a value function from the environment for reranking purposes. We employ SentenceBERT [27] to rank all valid actions (generated by ScienceWorld's APIs) based on their similarity to the top 5 generations for $A_{t}$ from SAYCAN. We implemented REACT and Reflexion in a similar manner. Adhering to their released code, we utilized the best single generation and determined the valid action with the minimal edit distance, if required. Both REACT and Reflexion necessitate subgoal annotations for teaching LLMs to plan with virtual 'think' actions. We annotated such truth subgoals by translating ScienceWorld's APIs into natural language, which was also employed by the oracle agents. For all agents, we incorporated the complete trajectories of one or two training variations from the same task type for in-context learning. Our primary experiments were conducted using OpenAI's GPT-4; however, other LLMs can be readily substituted as required.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Visualizing trajectories of SWIFTSAGE, REACT and Oracle. $X$ : time steps $(0 \rightarrow T)$; $Y$ : scores $(0 \rightarrow 100)$. Each figure displays the merged trajectories of testing variations by an agent in each task. Task IDs are shown at the bottom-right, and the ordering is based on *Len in Tab 1.</p>
<h1>4.3 Results and analysis.</h1>
<p>Main Results Table 1 compares the performance of various agents across 30 types of tasks. Detailed descriptions of each task type can be found in the ScienceWorld paper [36] and our appendix. It is evident that LLM-based methods outperform conventional agents due to their superior generalization ability, albeit at a higher deployment cost. The behavior cloning model TDT [36, 9] (11b) performs on par with DRRN [14], but with greater efficiency in learning and inference. In contrast, our SWIFTonly agent ( 770 m ) achieves an overall performance of 49.22 , which we attribute to its balanced training data and the use of a sliding window for longer action histories.
REACT demonstrates a noticeable improvement over SAYCAN for short and medium tasks, owing to its subgoal annotations for in-context learning and the inclusion of 'think' actions. REFLEXION surpasses REACT in shorter tasks; however, comparing REFLEXION with other agents is not entirely fair. REFLEXION can run up to four rounds, while the others are limited to one round. This discrepancy is particularly unfair for tasks involving multiple-choice scenarios. Nevertheless, we include REFLEXION's results to analyze the potential of such methods.</p>
<p>Exception handling. Consider the example in Figure 2, where the stove is broken, presenting an exception. Agents like DRRN and TDT often resort to repeating meaningless action sequences (e.g., continuously attempting to activate the stove or moving between rooms aimlessly). Although the SWIFT module, when used independently, improves upon this due to its larger context window from imitation learning, it still struggles to address exceptions robustly. ReAct and Reflexion occasionally utilize the 'think' action or reflections to redirect agents towards alternative solutions, but the generated actions rarely achieve the new subgoals if they are not grounded. In contrast, the plan-and-ground prompts in our SAGE module handle exceptions more effectively.</p>
<p>Cost-effectiveness. Despite SAGE invoking LLMs APIs twice for inference, its overall cost remains lower, as the result is a sequence of actions typically containing about 5 actions. In comparison, SAYCAN and REACT require $\mathbf{1 , 8 5 5 . 8 4}$ and $\mathbf{1 , 9 7 1 . 0 3}$ tokens per action (tpa) respectively, while REFLEXION necessitates $\mathbf{2 , 9 8 3 . 4 6}$ tpa. SWIFTSAGE, on the other hand, only uses $\mathbf{7 5 7 . 0 7}$ tpa. Given its superior performance, SWIFTSAGE proves more cost-effective than other LLM-based methods. This efficiency is primarily attributed to invoking LLMs only when needed (courtesy of our strong SWIFT module) and the action buffer mechanism.</p>
<p>Efficiency. To thoroughly examine the efficiency of agents across all task types, we use Figure 3 to visualize the average trajectories of the first three testing variations for each task involving</p>
<p>SWIFTSAGE, REACT, and the oracle agent. We arrange the tasks based on their average lengths of oracle trajectories (*Len in Table 1). We observe that oracle trajectories consistently achieve perfect scores, yet SWIFTSAGE can reach similar scores more efficiently. This is particularly evident in longer tasks (the bottom two rows), although SWIFTSAGE does not achieve a perfect score for a few tasks (e.g., 9-2 and 1-3). Interestingly, we find that REACT performs competitively in shorter tasks (e.g., 4-2 and 3-4), but most trajectories plateau at an intermediate score and fail to reach 100 .</p>
<p>More analysis. Due to page limit, we have to provide further details and analysis in the appendix, including more detailed analysis on cost-effectiveness and efficiency, additional case studies and ablation studies, sensitivity to LLM choices, and an the evaluation of the SWIFT-only agent.</p>
<h1>5 Conclusion</h1>
<p>Contributions. We present SWIFTSAGE, a generative agent for complex interactive reasoning tasks, inspired by the dual-process theory of human cognition. The agent framework comprises two modules: SWIFT, responsible for fast thinking, and SAGE, dedicated to slow thinking. The SWIFT module is a smaller LM that is fast and specialized, while the SAGE module focuses on prompting LLMs (e.g., GPT-4) for subgoal planning and reflective thinking. Through extensive experiments on 30 distinct tasks within the ScienceWorld benchmark, SWIFTSAGE outperforms baseline agents, achieving state-of-the-art performance, increased efficiency, and reduced cost.</p>
<p>Implications. The success of SWIFTSAGE highlights the potential for collaborative frameworks combining smaller LMs and LLMs in complex reasoning tasks. Smaller LMs can be trained more easily to recognize task-specific and environment-specific patterns, fostering effective in-distribution generalization. On the other hand, LLMs demonstrate remarkable zero-shot generalization abilities and deliberate thinking, though grounding their outputs in real-world environments remains challenging. We posit that dual-process agents, harnessing the strengths of both approaches, constitute a crucial step towards addressing complex interactive reasoning tasks and building general AI agents. Additionally, we can regard SWIFTSAGE as a method within the broader context of utilizing LLMs as controllers or planners for decomposing complex tasks and leveraging APIs/tools [19, 13, 29, 28]. To this end, we have explored applying SWIFTSAGE in web tasks and coding for math problems.</p>
<p>Limitations. Our work has been evaluated solely within a textual simulator, ScienceWorld, which supports a limited set of actions and tasks compared to real-world situations. Also, we did not implement any safeguards to prevent agents from engaging in potentially hazardous actions that could occur in the real world, such as picking up substances from a blast furnace. We argue that one important future direction is to develop a true open-ended environment, allowing agents to interact with a much wider variety of actions and objects to better emulate real-world scenarios. Besides, the use of LLMs in SAGE may present scalability challenges, as LLMs require significant computational resources and may not be feasible in some settings. Future research should explore the generalizability of SWIFTSAGE to other domains and the potential for more lightweight approaches to slow thinking. In addition, we believe it is important to train agents beyond simple supervised fine-tuning and to learn a trainable module to decide when to switch between SWIFT and SAGE mode.</p>
<h2>Acknowledgements</h2>
<p>We thank Peter Jansen, Eric Xingdi Yuan, and Marc-Alexandre Côté for valuable discussions. We thank members of the INK lab at USC and the Mosaic team at AI2 for valuable feedback on this project. Xiang Ren is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200006, the DARPA MCS program under Contract No. N660011924033, the Defense Advanced Research Projects Agency with award W911NF-19-20271, NSF IIS 2048211, and gift awards from Google and Amazon. This research was also supported by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031) and Allen Institute for AI. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government.</p>
<h1>References</h1>
<p>[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu , and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022.
[2] Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020 .
[3] Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur D. Szlam, Tim Rocktaschel, and Jason Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. In North American Chapter of the Association for Computational Linguistics, 2020 .
[4] Gabor Angeli, Melvin Johnson, and Christopher D. Manning. Leveraging linguistic structure for open domain information extraction. In Annual Meeting of the Association for Computational Linguistics, 2015.
[5] Thomas W. Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. ArXiv, abs/1705.08439, 2017.
[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[7] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. ArXiv, abs/2303.12712, 2023.
[8] Di Chen, Yiwei Bai, Wenting Zhao, Sebastian Ament, J. Gregoire, and Carla P. Gomes. Deep reasoning networks: Thinking fast and slow. ArXiv, abs/1906.00855, 2019.
[9] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Neural Information Processing Systems, 2021.
[10] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022.
[11] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben A. Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. In CGW@IJCAI, 2018.</p>
<p>[12] M. B. Ganapini, Murray Campbell, F. Fabiano, L. Horesh, Jonathan Lenchner, Andrea Loreggia, Nicholas Mattei, Francesca Rossi, Biplav Srivastava, and Kristen Brent Venable. Thinking fast and slow in ai: the role of metacognition. In International Conference on Machine Learning, Optimization, and Data Science, 2021.
[13] Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. Openagi: When llm meets domain experts. arXiv, 2023.
[14] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. arXiv: Artificial Intelligence, 2015 .
[15] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. ArXiv, abs/2201.07207, 2022.
[16] Daniel Kahneman. Thinking, Fast and Slow. 2011.
[17] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017.
[18] Bill Yuchen Lin, Chengsong Huang, Qianchu Liu, Wenda Gu, Sam Sommerer, and Xiang Ren. On grounded planning for embodied tasks with language models. ArXiv, abs/2209.00465, 2022.
[19] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. ArXiv, abs/2304.09842, 2023.
[20] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. Thinking fast and slow: Efficient text-to-visual retrieval with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9821-9831, 2021.
[21] Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. In Thirty Fifth AAAI Conference on Artificial Intelligence, 2021.
[22] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling. In International Conference on Machine Learning (ICML), 2023.
[23] Maxwell Nye, Michael Henry Tessler, Joshua B. Tenenbaum, and Brenden M. Lake. Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. In Neural Information Processing Systems, 2021.
[24] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.
[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.
[26] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley D. Edwards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. ArXiv, abs/2205.06175, 2022.</p>
<p>[27] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China, 2019. Association for Computational Linguistics.
[28] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023.
[29] Yongliang Shen, Kaitao Song, Xu Tan, Dong Sheng Li, Weiming Lu, and Yue Ting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. ArXiv, abs/2303.17580, 2023.
[30] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. ArXiv, abs/2303.11366, 2023.
[31] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. ArXiv, abs/2010.03768, 2020.
[32] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. ArXiv, abs/2212.04088, 2022.
[33] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. ArXiv, abs/1409.3215, 2014.
[34] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. ArXiv, abs/1805.01954, 2018.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008, 2017.
[36] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Conference on Empirical Methods in Natural Language Processing, 2022.
[37] Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Dayiheng Liu, Yike Guo, and Jie Fu. Interactive natural language processing. ArXiv, 2023.
[38] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. ArXiv, abs/2302.01560, 2023.
[39] Peter C Wason and J St BT Evans. Dual processes in reasoning? Cognition, 3(2):141-154, 1974.
[40] Shunyu Yao, Rohan Rao, Matthew J. Hausknecht, and Karthik Narasimhan. Keep calm and explore: Language models for action generation in text-based games. ArXiv, abs/2010.02903, 2020.
[41] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022.</p>
<h1>Appendix</h1>
<h2>A Dataset Statistics</h2>
<p>Table 2 presents the details of all 30 types of tasks in the ScienceWorld benchmark. To improve the training of SWIFT, we down-sampled Task 9-x, 10-x, and 3-3 from the original full dataset, as their large sizes resulted in a significant data imbalance. Additionally, we down-sampled less informative actions, such as 'close door to kitchen,' to produce a more effective dataset for imitation learning.</p>
<p>Evaluation. To save time while evaluating the numerous tasks and agents, we only used the first 10 variations for tasks with more than 10 test variations. This resulted in a total of 270 variations for fair and cost-effective comparisons among all agents. Some agents may receive a negative score from the engine and be unable to proceed any further due to their final action violating task requirements and being irrecoverable. In such cases, we used their last non-negative scores for evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">*Lens</th>
<th style="text-align: center;">#Vars: Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;"># Actions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1-1</td>
<td style="text-align: center;">Matter</td>
<td style="text-align: center;">Changes of State (Boiling)</td>
<td style="text-align: center;">107.7</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">694</td>
</tr>
<tr>
<td style="text-align: center;">1-2</td>
<td style="text-align: center;">Matter</td>
<td style="text-align: center;">Changes of State (Melting)</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">427</td>
</tr>
<tr>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">Matter</td>
<td style="text-align: center;">Changes of State (Freezing)</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">469</td>
</tr>
<tr>
<td style="text-align: center;">1-4</td>
<td style="text-align: center;">Matter</td>
<td style="text-align: center;">Changes of State (Any)</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">344</td>
</tr>
<tr>
<td style="text-align: center;">2-1</td>
<td style="text-align: center;">Measurement</td>
<td style="text-align: center;">Use Thermometer</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">4278</td>
</tr>
<tr>
<td style="text-align: center;">2-2</td>
<td style="text-align: center;">Measurement</td>
<td style="text-align: center;">Measuring Boiling Point (known)</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">218</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">6511</td>
</tr>
<tr>
<td style="text-align: center;">2-3</td>
<td style="text-align: center;">Measurement</td>
<td style="text-align: center;">Measuring Boiling Point (unknown)</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9768</td>
</tr>
<tr>
<td style="text-align: center;">3-1</td>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">Create a circuit</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">94</td>
</tr>
<tr>
<td style="text-align: center;">3-2</td>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">Renewable vs Non-renewable Energy</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">169</td>
</tr>
<tr>
<td style="text-align: center;">3-3</td>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">Test Conductivity (known)</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1341</td>
</tr>
<tr>
<td style="text-align: center;">3-4</td>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">Test Conductivity (unknown)</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">6974</td>
</tr>
<tr>
<td style="text-align: center;">4-1</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">Find a living thing</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1606</td>
</tr>
<tr>
<td style="text-align: center;">4-2</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">Find a non-living thing</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">756</td>
</tr>
<tr>
<td style="text-align: center;">4-3</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">Find a plant</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1458</td>
</tr>
<tr>
<td style="text-align: center;">4-4</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">Find an animal</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1606</td>
</tr>
<tr>
<td style="text-align: center;">5-1</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Grow a plant</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3675</td>
</tr>
<tr>
<td style="text-align: center;">5-2</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Grow a fruit</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">4283</td>
</tr>
<tr>
<td style="text-align: center;">6-1</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Mixing (generic)</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">347</td>
</tr>
<tr>
<td style="text-align: center;">6-2</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Mixing paints (secondary colours)</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">224</td>
</tr>
<tr>
<td style="text-align: center;">6-3</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Mixing paints (tertiary colours)</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">350</td>
</tr>
<tr>
<td style="text-align: center;">7-1</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify longest-lived animal</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">298</td>
</tr>
<tr>
<td style="text-align: center;">7-2</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify shortest-lived animal</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">298</td>
</tr>
<tr>
<td style="text-align: center;">7-3</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify longest-then-shortest-lived animal</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">360</td>
</tr>
<tr>
<td style="text-align: center;">8-1</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify life stages (plant)</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">165</td>
</tr>
<tr>
<td style="text-align: center;">8-2</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify life stages (animal)</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td style="text-align: center;">9-1</td>
<td style="text-align: center;">Forces</td>
<td style="text-align: center;">Inclined Planes (determine angle)</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2733</td>
</tr>
<tr>
<td style="text-align: center;">9-2</td>
<td style="text-align: center;">Forces</td>
<td style="text-align: center;">Friction (known surfaces)</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3644</td>
</tr>
<tr>
<td style="text-align: center;">9-3</td>
<td style="text-align: center;">Forces</td>
<td style="text-align: center;">Friction (unknown surfaces)</td>
<td style="text-align: center;">123.1</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3284</td>
</tr>
<tr>
<td style="text-align: center;">10-1</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Mendelian Genetics (known plants)</td>
<td style="text-align: center;">130.1</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3043</td>
</tr>
<tr>
<td style="text-align: center;">10-2</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Mendelian Genetics (unknown plants)</td>
<td style="text-align: center;">132.1</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2853</td>
</tr>
<tr>
<td style="text-align: center;">Short $(0&lt;*$ Len $\leq 20)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11.76</td>
<td style="text-align: center;">81.80</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">8.80</td>
<td style="text-align: center;">673.10</td>
</tr>
<tr>
<td style="text-align: center;">Medium $(20&lt;*$ Len $\leq 50)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">28.58</td>
<td style="text-align: center;">110.75</td>
<td style="text-align: center;">8.13</td>
<td style="text-align: center;">8.38</td>
<td style="text-align: center;">2516.88</td>
</tr>
<tr>
<td style="text-align: center;">Long (*Len $&gt;50)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">94.30</td>
<td style="text-align: center;">37.75</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">9.58</td>
<td style="text-align: center;">2934.75</td>
</tr>
<tr>
<td style="text-align: center;">Overall (avg)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">49.26</td>
<td style="text-align: center;">71.90</td>
<td style="text-align: center;">8.63</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2069.43</td>
</tr>
<tr>
<td style="text-align: center;">Overall (sum)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">2,157</td>
<td style="text-align: center;">259</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">62,083</td>
</tr>
</tbody>
</table>
<p>Table 2: The statistics of ScienceWorld benchmark. *Len is the average length of the oracle agent's trajectories. We show the number of our down-sampled variations in each split. The last column is the number of data points forr action-prediction seq2seq task in training SWIFT.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The SwiftSage framework explained with pseudocode.</p>
<h1>B Implementation Details</h1>
<h2>B. 1 Training Details of SWIFT</h2>
<p>We utilized flan-t5-large ( 770 m ) as the base model and fine-tuned it using the seq2seq actionprediction data ( 62 k ) as previously described. A learning rate of $1 \mathrm{e}-4$ and batch size of 128 were employed for training 500 steps, selected based on dev loss. Although we experimented with larger sizes of flan-t5 models, we observed only marginal improvements at a much higher training cost. We believe this is because the language used to describe the environment and actions covers a small vocabulary, and the language complexity does not warrant the use of more parameters.</p>
<h2>B. 2 Prompting in SAGE</h2>
<p>In Section 3.3, we provided an overview of the two-stage prompting framework: planning and grounding. In this section, we delve into further details of each stage.</p>
<p>Memory augmentation. Since the agent can only perceive objects in its current environment location, objects from previously visited locations are not displayed unless a prior 'look around' action has been executed. To augment memory for LLMs during planning and grounding, we also present the objects observed in previously visited locations. Additionally, we include the agent's location during each action in the action history, e.g., "pick up metal pot [location: kitchen]," to facilitate spatial reasoning for LLMs.</p>
<p>Connecting the two stages. We conveniently reuse the LLM output from the first stage (i.e., answers to Q1-Q5) as part of the input for the second stage. Our experiments involve using answers to all questions in the grounding stage. However, one can opt to use only answers to Q4 and Q5 to reduce computational costs. Our small-scale ablation study indicates that incorporating answers to Q1-Q3 in the grounding stage proves beneficial, yielding a gain of about 2 points for short tasks on average.</p>
<p>Grounding with action templates. We previously introduced an action template, 'POUR(object A, object B)', in Figure 2. Here, we present several additional templates to further illustrate the concept:</p>
<div class="codehilite"><pre><span></span><code><span class="n">TELEPORT</span><span class="w"> </span><span class="p">(</span><span class="n">room</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">directly</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">TELEPORT</span><span class="w"> </span><span class="p">(</span><span class="n">kitchen</span><span class="p">)</span>
<span class="n">PICK</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">inventory</span>
<span class="k">OPEN</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">search</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">things</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="k">OPEN</span><span class="w"> </span><span class="p">(</span><span class="n">freezer</span><span class="p">).</span>
<span class="n">ACTIVATE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">activate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">sink</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">stove</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="n">DEACTIVATE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">deactivate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="k">off</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span>
<span class="n">EXAMINE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">look</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">carefully</span><span class="p">.</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="n">EXAMINE</span><span class="w"> </span><span class="p">(</span><span class="n">light</span><span class="w"> </span><span class="n">bulb</span><span class="p">).</span>
<span class="n">MOVE</span><span class="w"> </span><span class="p">(</span><span class="k">object</span><span class="p">,</span><span class="w"> </span><span class="n">place</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">move</span><span class="o">/</span><span class="n">place</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">place</span>
</code></pre></div>

<p>It should be noted that despite explicitly instructing the LLM to only utilize permitted action types, it may occasionally generate actions of disallowed types that cannot be parsed. These invalid actions will be disregarded in the action buffer, and if necessary, the system will revert to the SWIFT mode.</p>
<h1>B. 3 Action Buffer</h1>
<p>In Section 3.4, we presented four conditions for activating the SAGE module. To detect critical decisions (Condition 3), we primarily focus on the 'focus on' actions, as many tasks in ScienceWorld necessitate agents to concentrate on the correct substances and objects in the proper sequence. A single incorrect 'focus on' action can terminate the entire run. Thus, we restrict the SWIFT module from performing such actions if SAGE has not yet been activated.</p>
<p>For identifying exceptions (Condition 4), we examine phrases like "No known action can match," "... cannot/doesn't...," and so on. When processing an action buffer, we attempt to execute each action sequentially. If two consecutive actions are invalid or cause exceptions, we halt and revert to SWIFT.</p>
<h2>C Additional Results and Analysis</h2>
<h2>C. 1 Sensitivity to LLMs: GPT-3.5-turbo vs GPT-4</h2>
<p>Besides the empirical results in Table 1, we also evaluate performance using GPT-3.5-turbo instead of GPT-4, which is considerably larger and more expensive. Other methods exhibit a significant performance decline, for instance, ReAct's score drops from 36.43 to 19.76, which is close to nonLLM methods and even lower than the vanilla method, SayCan. In contrast, SWIFTSAGE maintains a respectable performance of 62.22 , indicating better robustness.
As discussed in Sec. 5 (limitations), we plan to utilize other open-source LLMs, such as Alpaca, and investigate distilling the planning ability from closed-source LLMs to open-source and smaller LMs. Nevertheless, a practical challenge arises due to the current open-source LLMs having more restrictive length limits for inputs and outputs.</p>
<h2>C. 2 Efficiency Analysis</h2>
<p>Figure 5 illustrates that most of SWIFTSAGE's curves are situated near the top-left corner, indicating that SWIFTSAGE attains higher scores than oracle agents at a faster rate. Although ReAct is competitive with our method for shorter tasks, its trajectories typically plateau at intermediate scores and do not reach 100. While the Oracle agent consistently achieves a perfect score (100.0), its efficiency, particularly in longer tasks, is often outperformed by SWIFTSAGE.</p>
<h2>C. 3 Cost-effectiveness</h2>
<p>Table 4 presents a comprehensive analysis of the cost-effectiveness of LLM-based methods. We examine two specific metrics: tokens per action (tpa) and scores per action (spa) for SayCan, ReAct, Reflexion, and SWIFTSAGE across all tasks. Despite SAGE invoking LLM APIs twice for inference, its overall cost remains lower, as the result is a sequence of actions typically containing about 5 actions. In contrast, SAYCAN and ReAct require $\mathbf{1 , 8 5 5 . 8 4}$ and $\mathbf{1 , 9 7 1 . 0 3}$ tokens per action (tpa) respectively, while REFLEXION necessitates $\mathbf{2 , 9 8 3 . 4 6}$ tpa. SWIFTSAGE, however, only uses $\mathbf{7 5 7 . 0 7}$ tpa. Given its superior performance, SWIFTSAGE proves to be more cost-effective than other LLMbased methods. This efficiency primarily stems from invoking LLMs only when necessary (thanks to our robust SWIFT module) and the action buffer mechanism.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">Swift-Only</th>
<th style="text-align: center;">SayCan $_{\text {ChatGPT }}$</th>
<th style="text-align: center;">ReAct $_{\text {ChatGPT }}$</th>
<th style="text-align: center;">Reflexion $_{\text {ChatGPT }}$</th>
<th style="text-align: center;">SwiftSage $_{\text {ChatGPT }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1-1</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: center;">1-2</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">58.5</td>
</tr>
<tr>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">38.5</td>
</tr>
<tr>
<td style="text-align: center;">1-4</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">62.5</td>
</tr>
<tr>
<td style="text-align: center;">2-1</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">47.9</td>
</tr>
<tr>
<td style="text-align: center;">2-2</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">53.3</td>
</tr>
<tr>
<td style="text-align: center;">2-3</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">48.6</td>
</tr>
<tr>
<td style="text-align: center;">3-1</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: center;">3-2</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">50.3</td>
</tr>
<tr>
<td style="text-align: center;">3-3</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">66.9</td>
</tr>
<tr>
<td style="text-align: center;">3-4</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">78.1</td>
</tr>
<tr>
<td style="text-align: center;">4-1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">4-2</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">4-3</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;">4-4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">5-1</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">57.5</td>
</tr>
<tr>
<td style="text-align: center;">5-2</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: center;">6-1</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">43.2</td>
</tr>
<tr>
<td style="text-align: center;">6-2</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;">6-3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: center;">7-1</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: center;">7-2</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">60.0</td>
</tr>
<tr>
<td style="text-align: center;">7-3</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: center;">8-1</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: center;">8-2</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">33.0</td>
</tr>
<tr>
<td style="text-align: center;">9-1</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;">9-2</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;">9-3</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: center;">10-1</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: center;">10-2</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">51.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">78.68</td>
<td style="text-align: center;">37.24</td>
<td style="text-align: center;">28.95</td>
<td style="text-align: center;">39.19</td>
<td style="text-align: center;">72.81</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">32.90</td>
<td style="text-align: center;">20.06</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">14.37</td>
<td style="text-align: center;">55.34</td>
</tr>
<tr>
<td style="text-align: center;">Long</td>
<td style="text-align: center;">35.55</td>
<td style="text-align: center;">18.66</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">16.27</td>
<td style="text-align: center;">57.99</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">49.22</td>
<td style="text-align: center;">25.22</td>
<td style="text-align: center;">19.76</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">62.22</td>
</tr>
</tbody>
</table>
<p>Table 3: Additional results on the ScienceWorld benchmark. Different from Table 1, we use gpt-3.5-turbo instead of gpt-4 as the LLM for evaluating SayCan, ReAct, Relfexion, and our SWIFTSAGE. We also present the results of using SWIFT module only.</p>
<p>Interestingly, we observe that SWIFTSAGE has an even lower tpa for long tasks compared to its tpa in medium and short tasks. Upon further investigation, we attribute this finding to longer action buffers and the SWIFT module being more frequently effective. Additionally, regarding scores per action (spa), we discover that our SWIFTSAGE is more cost-effective by utilizing fewer tokens and achieving higher scores.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: An overview of visualizing trajectories of SWIFTSAGE, REACT and Oracle. $X$ : time steps $(0 \rightarrow T) ; Y$ : scores $(0 \rightarrow 100)$. Similar to Figure 3, each curve is a single trajectory by an agent in performing a task variation. A more efficient agent will achieve higher scores in a shorter time, resulting in curves positioned near the top-left corner.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average number of tokens per action (tpa)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average scores per action (spa)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task Type</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">SwiftSage</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">SwiftSage</td>
</tr>
<tr>
<td style="text-align: center;">1-1</td>
<td style="text-align: center;">1944.94</td>
<td style="text-align: center;">1503.60</td>
<td style="text-align: center;">2632.97</td>
<td style="text-align: center;">528.17</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.49</td>
</tr>
<tr>
<td style="text-align: center;">1-2</td>
<td style="text-align: center;">1125.76</td>
<td style="text-align: center;">1339.39</td>
<td style="text-align: center;">3066.70</td>
<td style="text-align: center;">545.34</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">1.64</td>
</tr>
<tr>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">1034.33</td>
<td style="text-align: center;">1268.23</td>
<td style="text-align: center;">3307.30</td>
<td style="text-align: center;">550.17</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;">1-4</td>
<td style="text-align: center;">1295.03</td>
<td style="text-align: center;">1251.45</td>
<td style="text-align: center;">2439.34</td>
<td style="text-align: center;">754.05</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.69</td>
</tr>
<tr>
<td style="text-align: center;">2-1</td>
<td style="text-align: center;">1188.46</td>
<td style="text-align: center;">1545.03</td>
<td style="text-align: center;">1988.59</td>
<td style="text-align: center;">494.52</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">3.01</td>
</tr>
<tr>
<td style="text-align: center;">2-2</td>
<td style="text-align: center;">1862.11</td>
<td style="text-align: center;">1181.88</td>
<td style="text-align: center;">1596.03</td>
<td style="text-align: center;">394.29</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">2.32</td>
</tr>
<tr>
<td style="text-align: center;">2-3</td>
<td style="text-align: center;">939.17</td>
<td style="text-align: center;">1358.33</td>
<td style="text-align: center;">1753.17</td>
<td style="text-align: center;">574.05</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">1.71</td>
</tr>
<tr>
<td style="text-align: center;">3-1</td>
<td style="text-align: center;">1713.64</td>
<td style="text-align: center;">1846.91</td>
<td style="text-align: center;">2677.89</td>
<td style="text-align: center;">807.62</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;">3-2</td>
<td style="text-align: center;">1785.01</td>
<td style="text-align: center;">1754.14</td>
<td style="text-align: center;">2337.02</td>
<td style="text-align: center;">823.28</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;">3-3</td>
<td style="text-align: center;">1762.13</td>
<td style="text-align: center;">2441.79</td>
<td style="text-align: center;">2262.39</td>
<td style="text-align: center;">220.80</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">3-4</td>
<td style="text-align: center;">1698.85</td>
<td style="text-align: center;">1195.59</td>
<td style="text-align: center;">2859.30</td>
<td style="text-align: center;">287.25</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">1.13</td>
</tr>
<tr>
<td style="text-align: center;">4-1</td>
<td style="text-align: center;">411.08</td>
<td style="text-align: center;">579.70</td>
<td style="text-align: center;">1053.57</td>
<td style="text-align: center;">309.14</td>
<td style="text-align: center;">1.91</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">4.76</td>
</tr>
<tr>
<td style="text-align: center;">4-2</td>
<td style="text-align: center;">1332.83</td>
<td style="text-align: center;">1098.69</td>
<td style="text-align: center;">1250.37</td>
<td style="text-align: center;">298.48</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">4.76</td>
</tr>
<tr>
<td style="text-align: center;">4-3</td>
<td style="text-align: center;">1155.99</td>
<td style="text-align: center;">1314.74</td>
<td style="text-align: center;">2966.82</td>
<td style="text-align: center;">406.17</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">3.82</td>
</tr>
<tr>
<td style="text-align: center;">4-4</td>
<td style="text-align: center;">1126.67</td>
<td style="text-align: center;">591.15</td>
<td style="text-align: center;">1003.18</td>
<td style="text-align: center;">309.71</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">4.76</td>
</tr>
<tr>
<td style="text-align: center;">5-1</td>
<td style="text-align: center;">2323.43</td>
<td style="text-align: center;">2620.66</td>
<td style="text-align: center;">5091.49</td>
<td style="text-align: center;">168.95</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">5-2</td>
<td style="text-align: center;">2646.50</td>
<td style="text-align: center;">2575.11</td>
<td style="text-align: center;">5864.93</td>
<td style="text-align: center;">536.56</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;">6-1</td>
<td style="text-align: center;">1454.65</td>
<td style="text-align: center;">1802.62</td>
<td style="text-align: center;">2344.90</td>
<td style="text-align: center;">1388.89</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">6-2</td>
<td style="text-align: center;">2413.99</td>
<td style="text-align: center;">2763.66</td>
<td style="text-align: center;">4342.07</td>
<td style="text-align: center;">402.50</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">3.33</td>
</tr>
<tr>
<td style="text-align: center;">6-3</td>
<td style="text-align: center;">1371.50</td>
<td style="text-align: center;">2860.68</td>
<td style="text-align: center;">4551.96</td>
<td style="text-align: center;">6361.79</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;">7-1</td>
<td style="text-align: center;">376.50</td>
<td style="text-align: center;">495.83</td>
<td style="text-align: center;">813.08</td>
<td style="text-align: center;">768.63</td>
<td style="text-align: center;">5.71</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">11.88</td>
</tr>
<tr>
<td style="text-align: center;">7-2</td>
<td style="text-align: center;">424.53</td>
<td style="text-align: center;">478.09</td>
<td style="text-align: center;">1180.58</td>
<td style="text-align: center;">772.00</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">6.14</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">10.63</td>
</tr>
<tr>
<td style="text-align: center;">7-3</td>
<td style="text-align: center;">424.73</td>
<td style="text-align: center;">564.69</td>
<td style="text-align: center;">1175.35</td>
<td style="text-align: center;">609.73</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">3.85</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">8.48</td>
</tr>
<tr>
<td style="text-align: center;">8-1</td>
<td style="text-align: center;">1505.39</td>
<td style="text-align: center;">1155.71</td>
<td style="text-align: center;">2466.59</td>
<td style="text-align: center;">249.38</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">2.23</td>
</tr>
<tr>
<td style="text-align: center;">8-2</td>
<td style="text-align: center;">3189.80</td>
<td style="text-align: center;">741.71</td>
<td style="text-align: center;">2886.09</td>
<td style="text-align: center;">2479.00</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">4.03</td>
</tr>
<tr>
<td style="text-align: center;">9-1</td>
<td style="text-align: center;">2066.06</td>
<td style="text-align: center;">2642.79</td>
<td style="text-align: center;">2652.56</td>
<td style="text-align: center;">307.30</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">1.06</td>
</tr>
<tr>
<td style="text-align: center;">9-2</td>
<td style="text-align: center;">2517.48</td>
<td style="text-align: center;">3031.95</td>
<td style="text-align: center;">3606.60</td>
<td style="text-align: center;">314.19</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;">9-3</td>
<td style="text-align: center;">7002.72</td>
<td style="text-align: center;">7507.00</td>
<td style="text-align: center;">7785.29</td>
<td style="text-align: center;">366.06</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;">10-1</td>
<td style="text-align: center;">3612.33</td>
<td style="text-align: center;">4218.44</td>
<td style="text-align: center;">4822.97</td>
<td style="text-align: center;">466.21</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">1.78</td>
</tr>
<tr>
<td style="text-align: center;">10-2</td>
<td style="text-align: center;">3969.62</td>
<td style="text-align: center;">5401.37</td>
<td style="text-align: center;">6724.81</td>
<td style="text-align: center;">218.00</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">1.69</td>
</tr>
<tr>
<td style="text-align: center;">Short</td>
<td style="text-align: center;">1256.98</td>
<td style="text-align: center;">1047.52</td>
<td style="text-align: center;">1934.90</td>
<td style="text-align: center;">716.30</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">1.76</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">5.69</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">1578.51</td>
<td style="text-align: center;">1742.18</td>
<td style="text-align: center;">2550.85</td>
<td style="text-align: center;">1277.52</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">1.35</td>
</tr>
<tr>
<td style="text-align: center;">Long</td>
<td style="text-align: center;">2539.78</td>
<td style="text-align: center;">2893.19</td>
<td style="text-align: center;">4145.68</td>
<td style="text-align: center;">444.09</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">1.17</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">1855.84</td>
<td style="text-align: center;">1971.03</td>
<td style="text-align: center;">2983.46</td>
<td style="text-align: center;">757.07</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">2.73</td>
</tr>
</tbody>
</table>
<p>Table 4: Cost-effectiveness analysis for LLM-based methods.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Contact: yuchen1@allenai.org&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
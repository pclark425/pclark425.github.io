<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6606 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6606</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6606</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-7be8c119dbe065c52125ee7716601751f3116844</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7be8c119dbe065c52125ee7716601751f3116844" target="_blank">Generalization through Memorization: Nearest Neighbor Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is suggested that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.</p>
                <p><strong>Paper Abstract:</strong> We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6606.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6606.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-Nearest Neighbors Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augments a pretrained autoregressive Transformer LM at inference by interpolating its next-token distribution with a distribution computed from k-nearest neighbors retrieved from an external key-value datastore of context embeddings; no additional training required.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A pre-trained Transformer LM is left unchanged; at test time the model encodes the current context into a fixed-size vector f(x), retrieves the k nearest stored context vectors from an external datastore (keys) using FAISS, computes a softmax over negative L2 distances to produce p_kNN over target tokens (values), aggregates probabilities per token, and linearly interpolates p_kNN with the LM's p_LM via a tunable λ to form the final distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>247M (underlying Transformer LM used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value datastore / retrieval-augmented memory (approximate nearest-neighbor vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Keys: 1024-dimensional Transformer context embeddings (input to final-layer feedforward after layer-norm); Values: target tokens (next-word labels) for each training token (i.e., token-level key-value pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Approximate nearest-neighbor search (FAISS) using L2 distance / RBF kernel; retrieve top-k neighbors (typical k=1024, but tuned), compute exp(-distance) softmax over retrieved entries, aggregate mass by token, then linear interpolation with LM distribution via λ.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Autoregressive language modeling on WIKITEXT-103, Books (Toronto Books Corpus), WIKI-3B / WIKI-100M experiments (including domain adaptation and scaling-without-training experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Language modeling (next-token prediction); domain adaptation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>WIKITEXT-103: Dev perplexity 16.06, Test perplexity 16.12 (kNN-LM); WIKITEXT-103 combined with continuous cache: Dev 15.81, Test 15.79. Books: Dev 14.20, Test 10.89 (with kNN-LM). WIKI-100M model + WIKI-3B datastore: Dev 14.61, Test 13.73.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base LM (no datastore) on WIKITEXT-103: Dev 17.96, Test 18.65. Books baseline: Dev 14.75, Test 11.89. WIKI-100M model without datastore: Dev 20.99, Test 19.59. WIKI-3B model trained on 3B tokens (no datastore): Dev 16.11, Test 15.17.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Perplexity (lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Storage: datastore contains an entry per training token (up to billions of entries) → large memory footprint; build cost: one forward pass over training data to save keys/values (e.g., building 103M-entry datastore ≈ 2 CPU hours); inference latency and compute for retrieval (validation retrieval of k=1024 took ≈25 minutes in their setup); use of FAISS quantization trades memory for precision (quantized keys faster but slightly worse unless full-precision distances are used); introduces hyperparameters (k, λ, key type, FAISS clustering parameters) to tune; retrieval scales linearly with datastore size but can be parallelized; increased disk/memory IO and index-build complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gains concentrated on long-tail/rare patterns (names, factuals, near-duplicates); continuous-cache-style gains from caching recent tokens are smaller for Transformers (because self-attention already can copy recent words); domain-adaptation via datastore helps substantially but does not fully match in-domain training (e.g., WIKI-3B model + BOOKS datastore improves from 34.84 → 20.47 but still above in-domain Books model 11.89); quantization and approximate search can reduce effectiveness unless compensated; some retrieved distributions can be flat (ambiguous continuations) and thus not decisively helpful; implicit memorization (training to zero training loss) does not substitute — memorizing model overfits and does not generalize as well as kNN-LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Urvashi Khandelwal; Omer Levy; Dan Jurafsky; Luke Zettlemoyer; Mike Lewis. Generalization Through Memorization: Nearest Neighbor Language Models. (paper text provided)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization through Memorization: Nearest Neighbor Language Models', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6606.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6606.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Continuous Cache</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous Cache Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online cache that stores recent hidden states (and corresponding targets) from the current test document and biases next-word predictions toward words seen recently in similar hidden-state contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving neural language models with a continuous cache</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Continuous Cache (Grave et al., 2017c)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At test time, the model saves recent hidden states from the same test document into an online cache; for each prediction the cache is queried for neighbors among these recent hidden states and a cache-derived distribution is interpolated with the LM's distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>247M (same underlying Transformer LM in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>online episodic cache of recent hidden states (test-time cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent test-time hidden states (model internal representations) paired with associated tokens (targets).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Similarity-based retrieval over recent hidden states (nearest neighbors among recent states), producing a distribution that is interpolated with the LM's output (λ tuned).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WIKITEXT-103 (language modeling); used together with kNN-LM in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Language modeling / online adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Alone: WIKITEXT-103 Dev 17.67, Test 18.27. Combined with kNN-LM: Dev 15.81, Test 15.79 (additive gains with kNN-LM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base LM: WIKITEXT-103 Dev 17.96, Test 18.65; kNN-LM alone: Dev 16.06, Test 16.12.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Gains are smaller with modern self-attentive Transformers compared to earlier LSTM settings (since Transformers can already copy from recent context); requires storing recent hidden states but storage is bounded per test document (online), so memory footprint is small compared to full datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less effective for Transformers than reported for LSTMs; limited to copying from recent document context (not a global training-data memory); smaller standalone improvements compared to kNN-LM in the experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Edouard Grave; Armand Joulin; Moustapha Cissé; Nicolas Usunier. Improving neural language models with a continuous cache. (referenced in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization through Memorization: Nearest Neighbor Language Models', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6606.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6606.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unbounded Cache</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unbounded cache model for online language modeling with open vocabulary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval mechanism that performs nearest-neighbor search over all previous hidden states (unbounded online cache) to improve language modeling and domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unbounded cache model for online language modeling with open vocabulary</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Unbounded Cache (Grave et al., 2017a)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An online language modeling approach that stores and retrieves over a large set of previous hidden states to bias predictions (cited as related work; not reimplemented in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>online/unbounded cache over previous hidden states</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Previous hidden states paired with their next-token targets (hidden-state → token entries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Nearest-neighbor search over stored previous hidden states (similarity-based retrieval), used for adaptation at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Online language modeling / domain adaptation (as described in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Mentioned as prior work for online adaptation; paper notes relatedness but does not report its costs or metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Edouard Grave; Moustapha M. Cissé; Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. NIPS 2017 (referenced in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization through Memorization: Nearest Neighbor Language Models', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6606.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6606.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differentiable Memory (Kaiser)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to Remember Rare Events (differentiable memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned, differentiable memory module that is read and updated during training, applied to one-shot learning tasks; mentioned as related work on learned/learnable memories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to remember rare events</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Differentiable Memory (Kaiser et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory architecture where a differentiable memory is learned and updated during training and used to store rare events for improved one-shot learning; cited as related prior work but not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable (learned) memory module</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Learned memory slots (paper does not specify details beyond referencing the work).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned read/write (differentiable) controllers to store and retrieve rare events (as described in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>One-shot learning tasks (as in the cited work); not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>One-shot learning / memory-augmented learning (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Łukasz Kaiser; Ofir Nachum; Aurko Roy; Samy Bengio. Learning to remember rare events. ICLR 2017 (referenced in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization through Memorization: Nearest Neighbor Language Models', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unbounded cache model for online language modeling with open vocabulary <em>(Rating: 2)</em></li>
                <li>Improving neural language models with a continuous cache <em>(Rating: 2)</em></li>
                <li>Learning to remember rare events <em>(Rating: 2)</em></li>
                <li>Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning <em>(Rating: 1)</em></li>
                <li>Search engine guided neural machine translation <em>(Rating: 1)</em></li>
                <li>Generating sentences by editing prototypes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6606",
    "paper_id": "paper-7be8c119dbe065c52125ee7716601751f3116844",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "kNN-LM",
            "name_full": "k-Nearest Neighbors Language Model",
            "brief_description": "Augments a pretrained autoregressive Transformer LM at inference by interpolating its next-token distribution with a distribution computed from k-nearest neighbors retrieved from an external key-value datastore of context embeddings; no additional training required.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "kNN-LM",
            "agent_description": "A pre-trained Transformer LM is left unchanged; at test time the model encodes the current context into a fixed-size vector f(x), retrieves the k nearest stored context vectors from an external datastore (keys) using FAISS, computes a softmax over negative L2 distances to produce p_kNN over target tokens (values), aggregates probabilities per token, and linearly interpolates p_kNN with the LM's p_LM via a tunable λ to form the final distribution.",
            "model_size": "247M (underlying Transformer LM used in experiments)",
            "memory_used": true,
            "memory_type": "external key-value datastore / retrieval-augmented memory (approximate nearest-neighbor vector store)",
            "memory_representation": "Keys: 1024-dimensional Transformer context embeddings (input to final-layer feedforward after layer-norm); Values: target tokens (next-word labels) for each training token (i.e., token-level key-value pairs).",
            "memory_access_mechanism": "Approximate nearest-neighbor search (FAISS) using L2 distance / RBF kernel; retrieve top-k neighbors (typical k=1024, but tuned), compute exp(-distance) softmax over retrieved entries, aggregate mass by token, then linear interpolation with LM distribution via λ.",
            "task_name": "Autoregressive language modeling on WIKITEXT-103, Books (Toronto Books Corpus), WIKI-3B / WIKI-100M experiments (including domain adaptation and scaling-without-training experiments).",
            "task_category": "Language modeling (next-token prediction); domain adaptation experiments",
            "performance_with_memory": "WIKITEXT-103: Dev perplexity 16.06, Test perplexity 16.12 (kNN-LM); WIKITEXT-103 combined with continuous cache: Dev 15.81, Test 15.79. Books: Dev 14.20, Test 10.89 (with kNN-LM). WIKI-100M model + WIKI-3B datastore: Dev 14.61, Test 13.73.",
            "performance_without_memory": "Base LM (no datastore) on WIKITEXT-103: Dev 17.96, Test 18.65. Books baseline: Dev 14.75, Test 11.89. WIKI-100M model without datastore: Dev 20.99, Test 19.59. WIKI-3B model trained on 3B tokens (no datastore): Dev 16.11, Test 15.17.",
            "has_comparative_results": true,
            "performance_metric": "Perplexity (lower is better)",
            "tradeoffs_reported": "Storage: datastore contains an entry per training token (up to billions of entries) → large memory footprint; build cost: one forward pass over training data to save keys/values (e.g., building 103M-entry datastore ≈ 2 CPU hours); inference latency and compute for retrieval (validation retrieval of k=1024 took ≈25 minutes in their setup); use of FAISS quantization trades memory for precision (quantized keys faster but slightly worse unless full-precision distances are used); introduces hyperparameters (k, λ, key type, FAISS clustering parameters) to tune; retrieval scales linearly with datastore size but can be parallelized; increased disk/memory IO and index-build complexity.",
            "limitations_or_failure_cases": "Gains concentrated on long-tail/rare patterns (names, factuals, near-duplicates); continuous-cache-style gains from caching recent tokens are smaller for Transformers (because self-attention already can copy recent words); domain-adaptation via datastore helps substantially but does not fully match in-domain training (e.g., WIKI-3B model + BOOKS datastore improves from 34.84 → 20.47 but still above in-domain Books model 11.89); quantization and approximate search can reduce effectiveness unless compensated; some retrieved distributions can be flat (ambiguous continuations) and thus not decisively helpful; implicit memorization (training to zero training loss) does not substitute — memorizing model overfits and does not generalize as well as kNN-LM.",
            "citation": "Urvashi Khandelwal; Omer Levy; Dan Jurafsky; Luke Zettlemoyer; Mike Lewis. Generalization Through Memorization: Nearest Neighbor Language Models. (paper text provided)",
            "uuid": "e6606.0",
            "source_info": {
                "paper_title": "Generalization through Memorization: Nearest Neighbor Language Models",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Continuous Cache",
            "name_full": "Continuous Cache Model",
            "brief_description": "An online cache that stores recent hidden states (and corresponding targets) from the current test document and biases next-word predictions toward words seen recently in similar hidden-state contexts.",
            "citation_title": "Improving neural language models with a continuous cache",
            "mention_or_use": "use",
            "agent_name": "Continuous Cache (Grave et al., 2017c)",
            "agent_description": "At test time, the model saves recent hidden states from the same test document into an online cache; for each prediction the cache is queried for neighbors among these recent hidden states and a cache-derived distribution is interpolated with the LM's distribution.",
            "model_size": "247M (same underlying Transformer LM in experiments)",
            "memory_used": true,
            "memory_type": "online episodic cache of recent hidden states (test-time cache)",
            "memory_representation": "Recent test-time hidden states (model internal representations) paired with associated tokens (targets).",
            "memory_access_mechanism": "Similarity-based retrieval over recent hidden states (nearest neighbors among recent states), producing a distribution that is interpolated with the LM's output (λ tuned).",
            "task_name": "WIKITEXT-103 (language modeling); used together with kNN-LM in experiments",
            "task_category": "Language modeling / online adaptation",
            "performance_with_memory": "Alone: WIKITEXT-103 Dev 17.67, Test 18.27. Combined with kNN-LM: Dev 15.81, Test 15.79 (additive gains with kNN-LM).",
            "performance_without_memory": "Base LM: WIKITEXT-103 Dev 17.96, Test 18.65; kNN-LM alone: Dev 16.06, Test 16.12.",
            "has_comparative_results": true,
            "performance_metric": "Perplexity",
            "tradeoffs_reported": "Gains are smaller with modern self-attentive Transformers compared to earlier LSTM settings (since Transformers can already copy from recent context); requires storing recent hidden states but storage is bounded per test document (online), so memory footprint is small compared to full datastore.",
            "limitations_or_failure_cases": "Less effective for Transformers than reported for LSTMs; limited to copying from recent document context (not a global training-data memory); smaller standalone improvements compared to kNN-LM in the experiments reported here.",
            "citation": "Edouard Grave; Armand Joulin; Moustapha Cissé; Nicolas Usunier. Improving neural language models with a continuous cache. (referenced in paper)",
            "uuid": "e6606.1",
            "source_info": {
                "paper_title": "Generalization through Memorization: Nearest Neighbor Language Models",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Unbounded Cache",
            "name_full": "Unbounded cache model for online language modeling with open vocabulary",
            "brief_description": "A retrieval mechanism that performs nearest-neighbor search over all previous hidden states (unbounded online cache) to improve language modeling and domain adaptation.",
            "citation_title": "Unbounded cache model for online language modeling with open vocabulary",
            "mention_or_use": "mention",
            "agent_name": "Unbounded Cache (Grave et al., 2017a)",
            "agent_description": "An online language modeling approach that stores and retrieves over a large set of previous hidden states to bias predictions (cited as related work; not reimplemented in this paper).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "online/unbounded cache over previous hidden states",
            "memory_representation": "Previous hidden states paired with their next-token targets (hidden-state → token entries)",
            "memory_access_mechanism": "Nearest-neighbor search over stored previous hidden states (similarity-based retrieval), used for adaptation at test time.",
            "task_name": "Online language modeling / domain adaptation (as described in the cited work)",
            "task_category": "Language modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Mentioned as prior work for online adaptation; paper notes relatedness but does not report its costs or metrics here.",
            "limitations_or_failure_cases": null,
            "citation": "Edouard Grave; Moustapha M. Cissé; Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. NIPS 2017 (referenced in paper).",
            "uuid": "e6606.2",
            "source_info": {
                "paper_title": "Generalization through Memorization: Nearest Neighbor Language Models",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Differentiable Memory (Kaiser)",
            "name_full": "Learning to Remember Rare Events (differentiable memory)",
            "brief_description": "A learned, differentiable memory module that is read and updated during training, applied to one-shot learning tasks; mentioned as related work on learned/learnable memories.",
            "citation_title": "Learning to remember rare events",
            "mention_or_use": "mention",
            "agent_name": "Differentiable Memory (Kaiser et al., 2017)",
            "agent_description": "A memory architecture where a differentiable memory is learned and updated during training and used to store rare events for improved one-shot learning; cited as related prior work but not used in experiments in this paper.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "differentiable (learned) memory module",
            "memory_representation": "Learned memory slots (paper does not specify details beyond referencing the work).",
            "memory_access_mechanism": "Learned read/write (differentiable) controllers to store and retrieve rare events (as described in the cited work).",
            "task_name": "One-shot learning tasks (as in the cited work); not evaluated in this paper.",
            "task_category": "One-shot learning / memory-augmented learning (related work)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "Łukasz Kaiser; Ofir Nachum; Aurko Roy; Samy Bengio. Learning to remember rare events. ICLR 2017 (referenced in paper).",
            "uuid": "e6606.3",
            "source_info": {
                "paper_title": "Generalization through Memorization: Nearest Neighbor Language Models",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unbounded cache model for online language modeling with open vocabulary",
            "rating": 2
        },
        {
            "paper_title": "Improving neural language models with a continuous cache",
            "rating": 2
        },
        {
            "paper_title": "Learning to remember rare events",
            "rating": 2
        },
        {
            "paper_title": "Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning",
            "rating": 1
        },
        {
            "paper_title": "Search engine guided neural machine translation",
            "rating": 1
        },
        {
            "paper_title": "Generating sentences by editing prototypes",
            "rating": 1
        }
    ],
    "cost": 0.01644075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GENERALIZATION THROUGH MEMORIZATION: NEAREST NEIGHBOR LANGUAGE MODELS</h1>
<p>Urvashi Khandelwal ${ }^{\dagger} ;$ Omer Levy ${ }^{\ddagger}$, Dan Jurafsky ${ }^{\dagger}$, Luke Zettlemoyer ${ }^{\ddagger}$ \&amp; Mike Lewis ${ }^{\ddagger}$<br>${ }^{\dagger}$ Stanford University<br>${ }^{\ddagger}$ Facebook AI Research<br>{urvashik, jurafsky}@stanford.edu<br>{omerlevy, lsz,mikelewis}@fb.com</p>
<h4>Abstract</h4>
<p>We introduce $k \mathrm{NN}-\mathrm{LMs}$, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ( $k \mathrm{NN}$ ) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong WIKITEXT-103 LM, with neighbors drawn from the original training set, our $k \mathrm{NN}-\mathrm{LM}$ achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.</p>
<h2>1 INTRODUCTION</h2>
<p>Neural language models (LMs) typically solve two subproblems: (1) mapping sentence prefixes to fixed-sized representations, and (2) using these representations to predict the next word in the text (Bengio et al., 2003; Mikolov et al., 2010). We present a new language modeling approach that is based on the hypothesis that the representation learning problem may be easier than the prediction problem. For example, any English speaker knows that Dickens is the author of and Dickens wrote will have essentially the same distribution over the next word, even if they do not know what that distribution is. We provide strong evidence that existing language models, similarly, are much better at the first problem, by using their prefix embeddings in a simple nearest neighbor scheme that significantly improves overall performance.
We introduce $k \mathrm{NN}-\mathrm{LM}$, an approach that extends a pre-trained LM by linearly interpolating its next word distribution with a $k$-nearest neighbors ( $k \mathrm{NN}$ ) model. The nearest neighbors are computed according to distance in the pre-trained embedding space and can be drawn from any text collection, including the original LM training data. This approach allows rare patterns to be memorized explicitly, rather than implicitly in model parameters. It also improves performance when the same training data is used for learning the prefix representations and the $k \mathrm{NN}$ model, strongly suggesting that the prediction problem is more challenging than previously appreciated.
To better measure these effects, we conduct an extensive empirical evaluation. Applying our $k \mathrm{NN}$ augmentation to a strong WIKITEXT-103 LM using only the original dataset achieves a new state-of-the-art perplexity of 15.79 - a 2.86 point improvement over the base model (Baevski \&amp; Auli, 2019) - with no additional training. We also show that the approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore. Training a model on 100-million tokens and using $k \mathrm{NN}$ search over a 3-billion token dataset can outperform training the same model on all 3-billion tokens, opening a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of $k$ NN-LM. A datastore is constructed with an entry for each training set token, and an encoding of its leftward context. For inference, a test context is encoded, and the $k$ most similar training contexts are retrieved from the datastore, along with the corresponding targets. A distribution over targets is computed based on the distance of the corresponding context from the test context. This distribution is then interpolated with the original model's output distribution.
new path for efficiently using large datasets in language models. Similarly, adding out-of-domain data to the datastore makes a single LM useful across multiple domains, again without further training. Qualitatively, we find the model is particularly helpful for long-tail patterns, such as factual knowledge, which might be easier to access via explicit memory.</p>
<h1>2 NeARESt NeighBOR LANGUAGE MODELing</h1>
<p>Language models (LMs) assign probabilities to sequences. Given a context sequence of tokens $c_{t}=\left(w_{1}, \ldots w_{t-1}\right)$, autoregressive LMs estimate $p\left(w_{t} \mid c_{t}\right)$, the distribution over the target token $w_{t}$.</p>
<p>The $k$ NN-LM involves augmenting such a pre-trained LM with a nearest neighbors retrieval mechanism, without any additional training (the representations learned by the LM remain unchanged). This can be done with a single forward pass over a text collection (potentially including the original LM training set), where the resulting context-target pairs are stored in a key-value datastore that is queried during inference, as illustrated in Figure 1.</p>
<p>Datastore Let $f(\cdot)$ be the function that maps a context $c$ to a fixed-length vector representation computed by the pre-trained LM. For instance, in a Transformer LM, $f(c)$ could map $c$ to an intermediate representation that is output by an arbitrary self-attention layer. Then, given the $i$-th training example $\left(c_{i}, w_{i}\right) \in \mathcal{D}$, we define the key-value pair $\left(k_{i}, v_{i}\right)$, where the key $k_{i}$ is the vector representation of the context $f\left(c_{i}\right)$ and the value $v_{i}$ is the target word $w_{i}$. The datastore $(\mathcal{K}, \mathcal{V})$ is thus the set of all key-value pairs constructed from all the training examples in $\mathcal{D}$ :</p>
<p>$$
(\mathcal{K}, \mathcal{V})=\left{\left(f\left(c_{i}\right), w_{i}\right) \mid\left(c_{i}, w_{i}\right) \in \mathcal{D}\right}
$$</p>
<p>Inference At test time, given the input context $x$ the model generates the output distribution over next words $p_{\mathrm{LM}}(y \mid x)$ and the context representation $f(x)$. The model queries the datastore with $f(x)$ to retrieve its $k$-nearest neighbors $\mathcal{N}$ according to a distance function $d(\cdot, \cdot)$ (squared $L^{2}$ distance in our experiments, making the similarity function an RBF kernel). Then, it computes a distribution over neighbors based on a softmax of their negative distances, while aggregating probability mass for each vocabulary item across all its occurrences in the retrieved targets (items that do not appear in the retrieved targets have zero probability):</p>
<p>$$
p_{\mathrm{kNN}}(y \mid x) \propto \sum_{\left(k_{i}, v_{i}\right) \in \mathcal{N}} \mathbb{1}<em i="i">{y=v</em>, f(x)\right)\right)
$$}} \exp \left(-d\left(k_{i</p>
<p>Finally, we follow Grave et al. (2017a) and interpolate the nearest neighbor distribution $p_{\mathrm{kNN}}$ with the model distribution $p_{\mathrm{LM}}$ using a tuned parameter $\lambda$ to produce the final $k$ NN-LM distribution:</p>
<p>$$
p(y \mid x)=\lambda p_{\mathrm{kNN}}(y \mid x)+(1-\lambda) p_{\mathrm{LM}}(y \mid x)
$$</p>
<p>Implementation The datastore contains an entry for each target in the training set, which for LMs can be up to billions of examples. To search over this large datastore, we use FAISS (Johnson et al., 2017), an open source library for fast nearest neighbor retrieval in high dimensional spaces. FAISS speeds up search by clustering the keys and looking up neighbors based on the cluster centroids, while reducing memory usage by storing compressed versions of the vectors. We found in preliminary experiments that using $L^{2}$ distance for FAISS retrieval results in better performance for $k \mathrm{NN}-\mathrm{LM}$, compared to inner product distance.</p>
<p>Related Cache Models Prior work (Grave et al., 2017c; Merity et al., 2017) used a similar approach to compute similarity to the previous hidden states of test documents, making it easier to copy rare vocabulary items from the recent past. Such techniques have been less popular since the development of Transformers (Vaswani et al., 2017), which can learn to copy recent words using self-attention; in Section 4.1, we observe relatively small gains from caching recent items in the same test document à la Grave et al. (2017c). Most relatedly, Grave et al. (2017a) describe an online language model using nearest neighbor search over all previous hidden states, to improve domain adaptation. In our work, we only save training data, with the goal of explicitly memorizing training examples to better generalize to similar cases at test time.</p>
<h1>3 EXPERIMENTAL SETUP</h1>
<p>Data Experiments in this paper use the following English corpora:
WIKITEXT-103 is a standard benchmark by Merity et al. (2017) for autoregressive language modeling with a 250 K word-level vocabulary. It consists of 103 M tokens of Wikipedia in the training set and 250 K tokens in each of the development and test sets.</p>
<p>Books is the Toronto Books Corpus (Zhu et al., 2015), containing 0.7B. Complete books are held out for validation/test.</p>
<p>WIKI-3B is English Wikipedia, containing about 2.87B tokens. Whole articles are held out for validation/test.</p>
<p>WIKI-100M is a random 100M token subset of WIKI-3B, consisting of complete articles.
Except for WiKiTExT-103, text is tokenized using the byte-pair encoding (Sennrich et al., 2015) with the 29 K subword vocabulary from BERT (Devlin et al., 2019).</p>
<p>Model Architecture $k \mathrm{NN}-\mathrm{LM}$ is compatible with any model that produces fixed size context representations. We use decoder-only Transformers (Vaswani et al., 2017) for language modeling, which are the current state of the art. Since the $k \mathrm{NN}-\mathrm{LM}$ makes no changes to the underlying LM, we take the exact architecture and optimization described by Baevski \&amp; Auli (2019) and use it to create a $k \mathrm{NN}-\mathrm{LM}$ for inference. This model consists of 16 layers, each with 16 self-attention heads, 1024 dimensional hidden states, and 4096 dimensional feedforward layers, amounting to 247 M trainable parameters. It processes 3072 tokens of context per example for WiKiTExT-103 and 1024 tokens for the rest of the corpora. Following Baevski \&amp; Auli (2019), we use adaptive inputs and an adaptive softmax (Grave et al., 2017b) with tied weights (Press \&amp; Wolf, 2017) for the WiKiTExT-103 experiments. On other datasets we do not use adaptive inputs or an adaptive softmax.</p>
<p>Evaluation LMs are trained to minimize the negative log-likelihood of the training corpus, and evaluated by perplexity (exponentiated negative log-likelihood) on held out data. Following Baevski \&amp; Auli (2019), 512 tokens are scored per test example, but up to 2560 tokens of extra prior context is provided for WiKiTExT-103 and up to 512 tokens of extra prior context is provided for the rest of the corpora.
$k$ NN-LM The keys used for $k \mathrm{NN}-\mathrm{LM}$ are the 1024-dimensional representations fed to the feedforward network in the final layer of the Transformer LM (after self-attention and layernorm; see Section 5 for further explanation). We perform a single forward pass over the training set with the trained model, in order to save the keys and values. During this forward pass, each target token is provided a minimum of 1536 tokens of prior context for WiKiTExT-103 and a minimum of 512</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Perplexity ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"># Trainable Params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Baevski \&amp; Auli (2019)</td>
<td style="text-align: center;">17.96</td>
<td style="text-align: center;">18.65</td>
<td style="text-align: center;">247 M</td>
</tr>
<tr>
<td style="text-align: left;">+Transformer-XL (Dai et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.30</td>
<td style="text-align: center;">257 M</td>
</tr>
<tr>
<td style="text-align: left;">+Phrase Induction (Luo et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.40</td>
<td style="text-align: center;">257 M</td>
</tr>
<tr>
<td style="text-align: left;">Base LM (Baevski \&amp; Auli, 2019)</td>
<td style="text-align: center;">17.96</td>
<td style="text-align: center;">18.65</td>
<td style="text-align: center;">247 M</td>
</tr>
<tr>
<td style="text-align: left;">$+k$ NN-LM</td>
<td style="text-align: center;">$\mathbf{1 6 . 0 6}$</td>
<td style="text-align: center;">$\mathbf{1 6 . 1 2}$</td>
<td style="text-align: center;">247 M</td>
</tr>
<tr>
<td style="text-align: left;">+Continuous Cache (Grave et al., 2017c)</td>
<td style="text-align: center;">17.67</td>
<td style="text-align: center;">18.27</td>
<td style="text-align: center;">247 M</td>
</tr>
<tr>
<td style="text-align: left;">$+k$ NN-LM + Continuous Cache</td>
<td style="text-align: center;">$\mathbf{1 5 . 8 1}$</td>
<td style="text-align: center;">$\mathbf{1 5 . 7 9}$</td>
<td style="text-align: center;">247 M</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance on WIKITEXT-103. The $k$ NN-LM substantially outperforms existing work. Gains are additive with the related but orthogonal continuous cache, allowing us to improve the base model by almost 3 perplexity points with no additional training. We report the median of three random seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Perplexity $(\downarrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"># Trainable Params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Base LM (Baevski \&amp; Auli, 2019)</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">11.89</td>
<td style="text-align: center;">247 M</td>
</tr>
<tr>
<td style="text-align: left;">$+k$ NN-LM</td>
<td style="text-align: center;">$\mathbf{1 4 . 2 0}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 8 9}$</td>
<td style="text-align: center;">247 M</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance on Books, showing that $k$ NN-LM works well in multiple domains.
tokens for the rest of the corpora. A FAISS index is then created using 1M randomly sampled keys to learn 4096 cluster centroids. For efficiency, keys are quantized to 64 -bytes. During inference, we retrieve $k=1024$ neighbors, and the index looks up 32 cluster centroids while searching for the nearest neighbors. For Wiкiтexт-103 experiments, we compute squared $L^{2}$ distances with full precision keys, but for the other datasets we use the FAISS $L^{2}$ distances (not squared) between quantized keys directly, for faster evaluation. We tune the interpolation parameter $\lambda$ on the validation set. ${ }^{1}$</p>
<p>Computational Cost Although the $k$ NN-LM requires no training given an existing LM, it does add some other computational overheads. Storing the keys and values requires a single forward pass over the training set, which amounts to a fraction of the cost of training for one epoch on the same examples. Once the keys are saved, for Wiкiтexт-103 building the cache with 103M entries takes roughly two hours on a single CPU. Finally, running on the validation set took approximately 25 minutes when retrieving 1024 keys. While the cost of building a large cache grows linearly in the number of entries, it is trivial to parallelize and requires no GPU-based training.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 Using the Training Data as the DataStore</h3>
<p>We first experiment with creating a datastore from the same data used to train the LM. Table 1 shows that $k$ NN-LM improves perplexity on Wiкiтexт-103 from 18.65 (Baevski \&amp; Auli, 2019) to a new state-of-the-art of 16.12. We also provide reported perplexities from two other recent models that also build upon Baevski and Auli's, suggesting that further improvements may be possible by augmenting the $k$ NN-LM with these techniques. We compare with models trained only on the standard training set, but recent work has shown performance can be improved by training on additional data, from either the test set (Krause et al., 2019) or large amounts of web text (Shoeybi et al., 2019).</p>
<p>We also experiment with a continuous cache model, a related but orthogonal technique from Grave et al. (2017c), in which the model saves and retrieves neighbors from earlier in the test document,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Training Data</th>
<th style="text-align: center;">Datastore</th>
<th style="text-align: center;">Perplexity ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">WIKI-3B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.11</td>
<td style="text-align: center;">15.17</td>
</tr>
<tr>
<td style="text-align: left;">WIKI-100M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.99</td>
<td style="text-align: center;">19.59</td>
</tr>
<tr>
<td style="text-align: left;">WIKI-100M</td>
<td style="text-align: center;">WIKI-3B</td>
<td style="text-align: center;">14.61</td>
<td style="text-align: center;">13.73</td>
</tr>
</tbody>
</table>
<p>Table 3: Experimental results on WIKI-3B. The model trained on 100M tokens is augmented with a datastore that contains about 3B training examples, outperforming the vanilla LM trained on the entire WIKI-3B training set.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Varying the size of the datastore. (a) Increasing the datastore size monotonically improves performance, and has not saturated even at about 3B tokens. A $k$ NN-LM trained on 100M tokens with a datastore of 1.6B tokens already outperforms the LM trained on all 3B tokens. (b) The optimal value of $\lambda$ increases with the size of the datastore.
rather than the training set. Gains from interpolating with the continuous cache are smaller than reported in the original setting that used LSTMs, perhaps because self-attentive language models can learn to perform such queries. Improvements from the continous cache are additive with the $k$ NN-LM, pushing our state-of-the-art result to 15.79 , a gain of 2.86 over the base model.</p>
<p>Finally, we repeat the experiment using text from a different domain, BOOKs, to control for the possibility that encyclopedic Wikipedia text is somehow uniquely good for caching. Table 2 shows an improvement in test set perplexity from 11.89 to 10.89 , suggesting that this is not the case.</p>
<h1>4.2 MORE DATA WITHOUT TRAINING</h1>
<p>Section 4.1 has shown that retrieving neighbors from the training data can significantly improve language modeling performance. This raises the question: can retrieving nearest neighbors from data be a substitute for training on it? To test this, we train a LM on WIKI-100M and use it to build a datastore from WIKI-3B, a corpus 30 times larger than the training set. We then compare this $k$ NN-LM to a vanilla LM trained on the entire WIKI-3B corpus. ${ }^{2}$</p>
<p>Table 3 shows that, as expected, the model trained on 3B tokens dramatically outperforms the model trained on 100M tokens, improving perplexity from 19.59 to 15.17 . However, adding nearest neighbors retrieval over those 3B examples to the model trained on 100M tokens improves perplexity from 19.59 to 13.73 ; i.e. retrieving nearest neighbors from the corpus outperforms training on it. This result suggests that rather than training language models on ever larger datasets, we can use smaller datasets to learn representations and augment them with $k$ NN-LM over a large corpus.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Training Data</th>
<th style="text-align: center;">Datastore</th>
<th style="text-align: center;">Perplexity ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">WIKI-3B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37.13</td>
<td style="text-align: center;">34.84</td>
</tr>
<tr>
<td style="text-align: left;">BOOKS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">11.89</td>
</tr>
<tr>
<td style="text-align: left;">WIKI-3B</td>
<td style="text-align: center;">BOOKS</td>
<td style="text-align: center;">24.85</td>
<td style="text-align: center;">20.47</td>
</tr>
</tbody>
</table>
<p>Table 4: Domain adaptation experiments, with results on Books. Adding an in-domain datastore to a Wikipedia-trained model improves results by 23 points, approaching in-domain training.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Transformer LM layer.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Key Type</th>
<th style="text-align: center;">Dev ppl. ( $\downarrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No datastore</td>
<td style="text-align: center;">17.96</td>
</tr>
<tr>
<td style="text-align: left;">Model output</td>
<td style="text-align: center;">17.07</td>
</tr>
<tr>
<td style="text-align: left;">Model output layer normalized</td>
<td style="text-align: center;">17.01</td>
</tr>
<tr>
<td style="text-align: left;">FFN input after layer norm</td>
<td style="text-align: center;">$\mathbf{1 6 . 0 6}$</td>
</tr>
<tr>
<td style="text-align: left;">FFN input before layer norm</td>
<td style="text-align: center;">17.06</td>
</tr>
<tr>
<td style="text-align: left;">MHSA input after layer norm</td>
<td style="text-align: center;">16.76</td>
</tr>
<tr>
<td style="text-align: left;">MHSA input before layer norm</td>
<td style="text-align: center;">17.14</td>
</tr>
</tbody>
</table>
<p>Table 5: WIKITEXT-103 validation results using different states from the final layer of the LM as the representation function $f(\cdot)$ for keys and queries. We retrieve $k=1024$ neighbors and $\lambda$ is tuned for each.</p>
<p>To understand how the amount of data used for $k$ NN retrieval affects performance, we use the WIKI100 M model to create datastores using different amounts of randomly sampled data from WIKI-3B. Figure 2a shows that using only 1.6B examples for the datastore already surpasses the performance of the model trained on all of WIKI-3B. In addition, performance does not saturate at 3B examples in the datastore, suggesting that growing the datastore more could lead to further gains. Figure 2b shows the model relies more on the $k$ NN component as the size of the datastore increases.</p>
<h1>4.3 Domain Adaptation</h1>
<p>We also experiment with domain adaptation by creating a datastore on the target domain training set. Table 4 shows that an in-domain LM on BOOKs has a relatively low perplexity (11.89), while a model trained on WIKI-3B performs poorly on the BOOKS domain ( 34.84 perplexity). Adding $k$ NN search over BOOKs to the WIKI-3B model reduces perplexity by 14 points (to 20.47), demonstrating that $k$ NN-LM allows a single model to be useful in multiple domains, by simply adding a datastore per domain.</p>
<h2>5 Tuning Nearest Neighbor Search</h2>
<p>While the $k$ NN-LM is conceptually straightforward, and requires no additional training, a number of hyperparameters are introduced for nearest neighbor search. We experiment with different choices here.</p>
<p>Key Function For similarity search, we extract a representation of context $c$ using an intermediate state of the LM $f(c)$. Transformers compute a number of different intermediate states, and we compare several choices depicted in Figure 3, with results shown in Table 5. While all the instantiations of $f$ we tried are helpful, we achieved the largest improvement by using the input to the final layer's feedforward network. We also observe that normalized representations (i.e. taken immediately after the layer norm) perform better. Repeating the experiment on the second-last transformer layer showed similar trends with slightly worse results (not shown), suggesting that the feedforward layer might be focusing more on the prediction problem, while the onus of representing the input falls more on the self-attention layer.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Effect of the number of nearest neighbors returned per word on WIKITEXT-103 (validation set). Returning more entries from the datastore monotonically improves performance.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Effect of interpolation parameter $\lambda$ on in-domain (left y-axis) and out-of-domain (right y-axis) validation set performances. More weight on $p_{k N N}$ improves domain adaptation.</p>
<p>Number of Neighbors per Query Each query returns the top- $k$ neighbors. Figure 4 shows that performance monotonically improves as more neighbors are returned, and suggests that even larger improvements may be possible with a higher value of $k$. Nonetheless, even a small number of neighbors $(k=8)$ is enough to achieve a new state of the art.</p>
<p>Interpolation Parameter We use a parameter $\lambda$ to interpolate between the base model distribution and the distribution from $k$ NN search over the dataset. Figure 5 shows that $\lambda=0.25$ is optimal on WIKITEXT-103. However, $\lambda=0.65$ works best for domain adaptation results (Figure 5).</p>
<p>Precision of Similarity Function In FAISS, the nearest neighbor search computes $L^{2}$ distances against quantized keys. We found results were improved from 16.5 perplexity on WIKITEXT-103 to 16.06 by computing squared $L^{2}$ distances with full precision keys for Equation 2.</p>
<h1>6 ANALYSIS</h1>
<p>Qualitative Analysis To understand why $k$ NN-LM improves performance, we manually examine cases in which $p_{\mathrm{kNN}}$ was significantly better than $p_{\mathrm{LM}}$. Table 6 shows one such example, along with several others in Appendix A. The example shows an interesting case where the model matches the trigram impact on the in several retrieved neighbors, but puts almost all weight on the most relevant neighbor, thus adding more value than an $n$-gram LM.</p>
<p>In general, we find that examples where $k$ NN-LM is most helpful typically contain rare patterns. Examples include factual knowledge, names, and near-duplicate sentences from the training set. In these cases, assigning train and test instances similar representations (via $f(\cdot)$ ) appears to be an easier problem than implicitly memorizing the next word in model parameters.</p>
<p>Simple vs Neural Representation We observe that many long-tail phenomena manifest as rare $n$-grams (e.g. names). Is it therefore possible to interpolate an $n$-gram model with a Transformer LM, as an alternative to our $k$ NN approach? Figure 7 shows little improvement from using $n$-gram LMs - 0.2 perplexity points (similarly to Bakhtin et al. (2018)). This result highlights the need to use the learned representation function $f(\cdot)$ to measure similarity between more varied contexts.</p>
<p>Implicit vs Explicit Memory If a neural representation function is crucial for $k$ NN-LM, could implicitly memorizing the training dataset in the neural network parameters replace the explicit memory in the datastore? To test this, we train a Transformer LM with no dropout. Figure 8 shows that this model eventually reaches zero training loss, indicating that it can make perfect predictions for all examples in the training set; the model has memorized the dataset. Naturally, the memorizing LM overfits, i.e. the training loss drops to 0 while the best validation perplexity is much higher at 28.59. For comparison, the vanilla Transformer LM (with dropout) has a much higher training loss (shown in Figure 8), but also generalizes better with a validation perplexity of 17.96. This result shows that the Transformer has sufficient capacity to memorize the training set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Context $\quad\left(p_{\mathrm{kNN}}=0.998, p_{\mathrm{LM}}=0.124\right)$</th>
<th style="text-align: left;">Test Target</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">it was organised by New Zealand international player Joseph Warbrick, <br> promoted by civil servant Thomas Eyton, and managed by James Scott, a <br> publican. The Natives were the first New Zealand team to perform a haka, <br> and also the first to wear all black. They played 107 rugby matches during <br> the tour, as well as a small number of Victorian Rules football and associ- <br> ation football matches in Australia. Having made a significant impact on <br> the...</td>
<td style="text-align: left;">development</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Training Set Context</td>
<td style="text-align: left;">Training <br> Set Target</td>
<td style="text-align: left;">Context <br> Probability</td>
</tr>
<tr>
<td style="text-align: left;">As the captain and instigator of the 1888-89 Natives - the first New Zealand <br> team to tour the British Isles - Warbrick had a lasting impact on the...</td>
<td style="text-align: left;">development</td>
<td style="text-align: left;">0.998</td>
</tr>
<tr>
<td style="text-align: left;">promoted to a new first grade competition which started in 1900. Glebe <br> immediately made a big impact on the...</td>
<td style="text-align: left;">district</td>
<td style="text-align: left;">0.00012</td>
</tr>
<tr>
<td style="text-align: left;">centuries, few were as large as other players managed. However, others <br> contend that his impact on the...</td>
<td style="text-align: left;">game</td>
<td style="text-align: left;">0.000034</td>
</tr>
<tr>
<td style="text-align: left;">Nearly every game in the main series has either an anime or manga adap- <br> tation, or both. The series has had a significant impact on the...</td>
<td style="text-align: left;">development</td>
<td style="text-align: left;">0.00000092</td>
</tr>
</tbody>
</table>
<p>Figure 6: Example where the $k \mathrm{NN}$ model has much higher confidence in the correct target than the LM. Although there are other training set examples with similar local $n$-gram matches, the nearest neighbour search is highly confident of specific and very relevant context.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Interpolating the Transformer LM with $n$-gram LMs on WiKiTEXT-103 (validation set). Using $k$ NN-LM gives a much lower perplexity, suggesting that the representations are learning more than just matching local context.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Training curves for the Transformer LM with and without dropout. Turning off dropout allows the training loss to go to 0 , indicating that the model has sufficient capacity to memorize the training data.</p>
<p>We consider whether the memorizing LM can be an effective substitute for nearest neighbor search. Interpolating the memorizing LM with the original LM improves validation perplexity by just 0.1 - compared to 1.9 from $k$ NN-LM. This result suggests that although the Transformer is expressive enough to memorize all training examples, learning to do so does not result in context representations that generalize. In contrast, $k$ NN-LM memorizes training data while improving generalization.
From these experiments, we conjecture that $k$ NN-LM improves performance because (1) the Transformer LM is very good at learning a representation function for contexts with an implicit notion of similarity, and (2) while the Transformer has capacity to memorize all training examples, doing so causes its representation to generalize less effectively, but (3) the $k$ NN-LM allows the model to memorize the training data while retaining an effective similarity function.</p>
<h1>7 Related Work</h1>
<p>We discuss related uses of caches for language modeling in Section 2.
Similar $k \mathrm{NN}$ models to ours have been proposed for computer vision tasks (Papernot \&amp; McDaniel, 2018; Orhan, 2018; Zhao \&amp; Cho, 2018), primarily motivated by improving interpretability and robustness to adversarial attacks. We hypothesize that our method may be particularly effective for language modeling, because plentiful unlabeled data allows datastores of billions of tokens, and language modeling often requires world knowledge to be learnt from few examples.</p>
<p>Nearest neighbor models have been applied to a number of NLP problems in the past, such as part of speech tagging (Daelemans et al., 1996) and morphological analysis (Bosch et al., 2007), but the use of learned representations makes the similarity function much more effective in the case of neural models. More recently, Kaiser et al. (2017) have used a similarly differentiable memory that is learned and updated during training, and is applied to one-shot learning tasks.</p>
<p>Several models have also improved language generation by using training examples directly at test time. Guu et al. (2018) propose a model that samples training sentences at random and edits them with a sequence-to-sequence model, but does not use a retrieval mechanism such as $k \mathrm{NN}$. Gu et al. (2018) introduce a translation model that attends over retrieved training set examples. Weston et al. (2018) improve a dialogue response generation model by refining similar instances from the training set. $k$ NN-LM differs from these approaches by working at the level of individual tokens instead of whole training sentences, as well as not incorporating the retrieval mechanism into the training pipeline.</p>
<p>A general trend in machine learning, and in language modeling in particular, is that adding more data consistently improves performance (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019; Zellers et al., 2019; Shoeybi et al., 2019). Our work offers an alternative method for scaling language models, in which relatively small models learn context representations, and a nearest neighbour search acts as a highly expressive classifier.</p>
<h2>8 CONCLUSION and Future Work</h2>
<p>We have introduced $k$ NN-LMs, which can significantly outperform standard language models by directly querying training examples at test time. The approach can be applied to any neural language model. The success of this method suggests that learning similarity functions between contexts may be an easier problem than predicting the next word from some given context. Future work should explore explicitly training similarity functions, and reducing the size of the datastore.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>The authors thank the anonymous reviewers as well as Sida Wang, Kartikay Khandelwal, Kevin Clark and members of the FAIR Seattle team for helpful discussions and comments.</p>
<h2>REFERENCES</h2>
<p>Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In $I C L R, 2019$.</p>
<p>Anton Bakhtin, Arthur Szlam, Marc'Aurelio Ranzato, and Edouard Grave. Lightweight adaptive mixture of neural and n-gram language models. arXiv preprint arXiv:1804.07705, 2018.</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137-1155, 2003.</p>
<p>Antal van den Bosch, Bertjan Busser, Sander Canisius, and Walter Daelemans. An efficient memorybased morphosyntactic tagger and parser for dutch. LOT Occasional Series, 7:191-206, 2007.</p>
<p>Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. Mbt: A memory-based part of speech tagger-generator. In $W V L C, 1996$.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL, 2019.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.</p>
<p>Edouard Grave, Moustapha M Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. In NIPS, pp. 6042-6052, 2017a.</p>
<p>Edouard Grave, Armand Joulin, Moustapha Cissé, Hervé Jégou, et al. Efficient softmax approximation for gpus. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1302-1310. JMLR. org, 2017b.</p>
<p>Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In $I C L R, 2017 \mathrm{c}$.</p>
<p>Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. Search engine guided neural machine translation. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437-450, 2018.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017.</p>
<p>Łukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events. In $I C L R, 2017$.</p>
<p>Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of transformer language models. arXiv preprint arXiv:1904.08378, 2019.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Hongyin Luo, Lan Jiang, Yonatan Belinkov, and James Glass. Improving neural language models by segmenting, attending, and predicting the future. In ACL, 2019.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. $I C L R, 2017$.</p>
<p>Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh annual conference of the international speech communication association, 2010.
A. Emin Orhan. A simple cache model for image recognition. In NeurIPS, 2018.</p>
<p>Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.</p>
<p>Ofir Press and Lior Wolf. Using the output embedding to improve language models. In ICLR, 2017.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf, 2019.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.</p>
<p>Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>Jason Weston, Emily Dinan, and Alexander H Miller. Retrieve and refine: Improved sequence generation models for dialogue. arXiv preprint arXiv:1808.04776, 2018.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.</p>
<p>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In NeurIPS, 2019.</p>
<p>Jake Zhao and Kyunghyun Cho. Retrieval-augmented convolutional neural networks for improved robustness against adversarial examples. arXiv preprint arXiv:1802.09502, 2018.</p>
<p>Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pp. 19-27, 2015.</p>
<h1>A APPENDIX</h1>
<p>This section provides several examples where $p_{\mathrm{kNN}}$ places higher probability mass on the true target, compared to $p_{\mathrm{LM}}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Context $\quad\left(p_{\mathrm{kNN}}=0.995, p_{\mathrm{LM}}=0.025\right)$</th>
<th style="text-align: left;">Test Target</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">For Australians and New Zealanders the Gallipoli campaign came to sym- <br> bolise an important milestone in the emergence of both nations as indepen- <br> dent actors on the world stage and the development of a sense of national <br> identity. Today, the date of the initial landings, 25 April, is known as An- <br> zac Day in Australia and New Zealand and every year thousands of people <br> gather at memorials in both nations, as well as Turkey, to...</td>
<td style="text-align: left;">honour</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Training Set Context</td>
<td style="text-align: left;">Training <br> Set Target</td>
<td style="text-align: left;">Context <br> Probability</td>
</tr>
<tr>
<td style="text-align: left;">Despite this, for Australians and New Zealanders the Gallipoli campaign <br> has come to symbolise an important milestone in the emergence of both <br> nations as independent actors on the world stage and the development of a <br> sense of national identity. Today, the date of the initial landings, 25 April, <br> is a public holiday known as Anzac Day in Australia and New Zealand and <br> every year thousands of people gather at memorials in both nations, and <br> indeed in Turkey, to ...</td>
<td style="text-align: left;">honour</td>
<td style="text-align: left;">0.995</td>
</tr>
<tr>
<td style="text-align: left;">On the anniversary date of his death, every year since 1997, thousands of <br> people gather at his home in Memphis to...</td>
<td style="text-align: left;">celebrate</td>
<td style="text-align: left;">0.0086</td>
</tr>
<tr>
<td style="text-align: left;">Twenty-five years after Marseille's death, fighter pilot veterans of World <br> War II gathered to...</td>
<td style="text-align: left;">honour</td>
<td style="text-align: left;">0.0000041</td>
</tr>
</tbody>
</table>
<p>Table 6: Another example where the $k \mathrm{NN}$ model places much higher probability mass on the correct target, compared to the LM. The nearest neighbors search has retrieved a training set context that is extremely similar to the test context, while very rare and in the long-tail of patterns.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Context $\quad\left(p_{\mathrm{kNN}}=0.959, p_{\mathrm{LM}}=0.503\right)$</th>
<th style="text-align: center;">Test Target</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">U2 do what they're best at, slipping into epic rock mode, playing music <br> made for the arena". In two other local newspaper reviews, critics praised <br> the song's inclusion in a sequence of greatest hits. For the PopMart Tour of <br> 1997-...</td>
<td style="text-align: center;">1998</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Training Set Context</td>
<td style="text-align: center;">Training <br> Set Target</td>
<td style="text-align: center;">Context <br> Probability</td>
</tr>
<tr>
<td style="text-align: left;">Following their original intent, "Sunday Bloody Sunday" was not played <br> during any of the forty-seven shows on the Lovetown Tour in 1989. The <br> song reappeared for a brief period during the Zoo TV Tour, and late during <br> the second half of PopMart Tour (1997-...</td>
<td style="text-align: center;">1998</td>
<td style="text-align: center;">0.936</td>
</tr>
<tr>
<td style="text-align: left;">They are 6 times Champions and they won the Challenge Cup in 1938, and <br> have experienced two previous stretches in the Super League, 1997-...</td>
<td style="text-align: center;">2002</td>
<td style="text-align: center;">0.0071</td>
</tr>
<tr>
<td style="text-align: left;">About \$40 million (\$61.4 million in 2018 dollars) was spent on the property <br> acquisition. After weather-related construction delays due to the El Nino <br> season of the winter of 1997-...</td>
<td style="text-align: center;">1998</td>
<td style="text-align: center;">0.0015</td>
</tr>
<tr>
<td style="text-align: left;">This made it the highest-rated season of The X-Files to air as well as the <br> highest rated Fox program for the 1997-...</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">0.00000048</td>
</tr>
</tbody>
</table>
<p>Table 7: In this example, the desired date pattern appears in many examples. Yet, the nearest neighbors search is able to identify the only training set context which is relevant to the test context and assigns it the highest probability mass.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Context $\quad\left(p_{\mathrm{NNN}}=0.624, p_{\mathrm{LM}}=0.167\right)$</th>
<th style="text-align: left;">Test Target</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lord Strathcona awarded Gauthier a scholarship in 1906 that allowed her <br> to return to Europe and continue her vocal studies. She returned there and <br> continued both to study and give performances. Her first operatic perfor- <br> mance came in 1909 in Pavia, Italy as Micaela in Bizet's...</td>
<td style="text-align: left;">Carmen</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Training Set Context</td>
<td style="text-align: left;">Training <br> Set Target</td>
<td style="text-align: left;">Context <br> Probability</td>
</tr>
<tr>
<td style="text-align: left;">Despite poor relations with the orchestra, Mahler brought five new operas <br> to the theatre, including Bizet's...</td>
<td style="text-align: left;">Carmen</td>
<td style="text-align: left;">0.356</td>
</tr>
<tr>
<td style="text-align: left;">The fourth movement of An die Jugend (1909), for instance, uses two of <br> Niccolo Paganini's Caprices for solo violin (numbers 11 and 15), while the <br> 1920 piece Piano Sonatina No. 6 (Fantasia da camera super Carmen) is <br> based on themes from Georges Bizet's...</td>
<td style="text-align: left;">opera</td>
<td style="text-align: left;">0.0937</td>
</tr>
<tr>
<td style="text-align: left;">It also hosted the Ballet of her Majesty's Theatre in the mid-19th century, <br> before returning to hosting the London premieres of such operas as Bizet's...</td>
<td style="text-align: left;">Carmen</td>
<td style="text-align: left;">0.0686</td>
</tr>
</tbody>
</table>
<p>Table 8: In this case, the model is able to memorize the fact that Georges Bizet wrote Carmen.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Context $\quad\left(p_{\mathrm{NNN}}=0.031, p_{\mathrm{LM}}=0.007\right)$</th>
<th style="text-align: left;">Test Target</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mycena maculata bears some resemblance to M. <unk>, but is only as- <br> sociated with decaying hardwood logs and stumps, and is found in eastern <br> North America, and sometimes on oak on the West Coast. In age, it...</td>
<td style="text-align: left;">develops</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Training Set Context</td>
<td style="text-align: left;">Training <br> Set Target</td>
<td style="text-align: left;">Context <br> Probability</td>
</tr>
<tr>
<td style="text-align: left;">Morchella tridentina (=Morchella frustrata) is also rufescent and very sim- <br> ilar to M. rufobrunnea. It is found in mountainous forests and maquis and <br> forms a marked sinus at the attachment of the cap with the stem, which is <br> pure white. At maturity, it...</td>
<td style="text-align: left;">develops</td>
<td style="text-align: left;">0.031</td>
</tr>
<tr>
<td style="text-align: left;">The winter bonnet (M. tintinnabulum) is a northern European species that <br> is much smaller (cap diameter up to 2.6 cm (1.0 in) across) and has a brown <br> cap, and has ragged hairs at the base. It...</td>
<td style="text-align: left;">generally</td>
<td style="text-align: left;">0.029</td>
</tr>
<tr>
<td style="text-align: left;">The "bleeding" will distinguish Mycena atkinsoniana from most other <br> Mycena species commonly encountered. The common and widely dis- <br> tributed M. sanguinolenta is another "bleeder", but it is smaller than M. <br> atkinsonia, with a cap diameter ranging from 3 to 15 mm (0.1 to 0.6 in). <br> Additionally, it...</td>
<td style="text-align: left;">has</td>
<td style="text-align: left;">0.028</td>
</tr>
<tr>
<td style="text-align: left;">Mycena flavoalba bears resemblance to some members of the genus <br> Hemimycena, such as H. lactea and H. <unk>. It...</td>
<td style="text-align: left;">can</td>
<td style="text-align: left;">0.018</td>
</tr>
</tbody>
</table>
<p>Table 9: This is an example where the $p_{\mathrm{NNN}}$ distribution is relatively flat, as several words are plausible continuations. However, the nearest neighbors search assigns the highest probability to the correct target and a corresponding context that is particularly relevant. In contrast, the LM probability on the correct target is lower.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The original LM (Baevski \&amp; Auli, 2019) was trained for 286 K steps on a corpus of similar size to WIKI100M. When scaling up to WIKI-3B, we tuned only the number of updates on the validation set and found that training for 572 K steps (double) produces a slightly stronger baseline.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
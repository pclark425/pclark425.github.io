<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7756 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7756</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7756</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-274822886</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.13645v1.pdf" target="_blank">On the Role of Model Prior in Real-World Inductive Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) show impressive inductive reasoning capabilities, enabling them to generate hypotheses that could generalize effectively to new instances when guided by in-context demonstrations. However, in real-world applications, LLMs' hypothesis generation is not solely determined by these demonstrations but is significantly shaped by task-specific model priors. Despite their critical influence, the distinct contributions of model priors versus demonstrations to hypothesis generation have been underexplored. This study bridges this gap by systematically evaluating three inductive reasoning strategies across five real-world tasks with three LLMs. Our empirical findings reveal that, hypothesis generation is primarily driven by the model's inherent priors; removing demonstrations results in minimal loss of hypothesis quality and downstream usage. Further analysis shows the result is consistent across various label formats with different label configurations, and prior is hard to override, even under flipped labeling. These insights advance our understanding of the dynamics of hypothesis generation in LLMs and highlight the potential for better utilizing model priors in real-world inductive reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7756.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7756.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IO-Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input-Output Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-step prompting baseline that provides in-context demonstrations (input-output examples) once to an LLM to generate a set of natural-language hypotheses describing the positive class pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-modal (text, image, image-text); NLP & computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (natural language patterns for classification)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hypothesis generation via Input-Output Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide N in-context demonstrations (N=30 in experiments) in the prompt and request m hypotheses; evaluate hypotheses downstream by using them as patterns for classifying test examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (single-hypothesis and multiple-hypothesis classification); LLM helpfulness and novelty scores; pairwise win rates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: percent of test examples correctly labeled by the hypothesis (reported as mean ± std across seeds); Helpfulness/Novelty: 1–5 LLM-assigned rating scale; Pairwise win rate: percent of pairs where one hypothesis preferred</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Hallucination pattern induction; Unhealthy Comments; Funny Reddit posts; PneumoniaMNIST; Truthful Hotel Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluation performed via pairwise comparisons across three datasets (Unhealthy Comments, Truthful Reviews, Funny Reddit) with nine participants; experts choose preferred hypothesis or indicate indistinguishable.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Removing demonstrations produced minimal downstream accuracy degradation (differences mostly <3% across datasets); in some cases zero-shot (w/o demos) outperformed w/ demos. LLM helpfulness scores ~4.00 vs ~3.95; novelty ~2.56–2.84 depending on baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A (no direct comparison to human-generated hypotheses reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on classification framing and single-hypothesis pattern matching; findings limited to datasets and models tested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7756.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7756.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Refinement with Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative hypothesis-generation method that refines a bank of hypotheses by ranking them on a validation set and feeding top-ranked hypotheses back to the LLM (with or without demonstrations) to produce improved hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-modal (text, image, image-text); NLP & computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (iteratively refined natural language hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Iterative refinement with ranking feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Initialize a hypothesis bank via IO-prompting (5 hypotheses), evaluate hypotheses on a validation set, select top m hypotheses, provide them (and demonstrations if available) back to the LLM to generate refined hypotheses; repeat for a set number of iterations (3 iterations used).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (validation/test), LLM helpfulness and novelty scores, pairwise win rate</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: percent correct on validation/test; Helpfulness/Novelty: 1–5 LLM score; Pairwise win rate: % wins across random hypothesis pairs</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Hallucination pattern induction; Unhealthy Comments; Funny Reddit posts; PneumoniaMNIST; Truthful Hotel Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human pairwise comparisons included hypotheses from iterative-refinement in the same human evaluation protocol (9 participants); pairwise selection among w/ and w/o demos.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Iterative refinement outperformed IO-Prompting and HypoGeniC in many downstream accuracy comparisons, indicating data helps for hypothesis selection though not necessarily for initial generation; LLM-based metrics sometimes favored w/o demos for novelty/helpfulness under iterative-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Refinement improves hypothesis selection but still dominated by model prior; iterative pipeline parameters (bank size, iterations) tuned as implementation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7756.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7756.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HypoGeniC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HypoGeniC (Update from Mistakes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative hypothesis generation/update strategy that focuses on generating new hypotheses from examples that current hypotheses misclassify (incorrect examples), maintaining a hypothesis bank with reward scores to select candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-modal (text, image, image-text); NLP & computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (iterative mistake-driven generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>HypoGeniC-style mistake-driven hypothesis updating</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Initialize hypothesis bank via IO-prompting with reward scores, then when predefined numbers of incorrect examples for groups are reached, use those misclassified examples to prompt generation of new hypotheses; keep top-m hypotheses by reward.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (validation/test), reward scores used for ranking, LLM helpfulness/novelty scores</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: percent correct; Reward score: internal reward efficiency α = 0.5 used in selection; Helpfulness/Novelty: 1–5 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Hallucination pattern induction; Unhealthy Comments; Funny Reddit posts; PneumoniaMNIST; Truthful Hotel Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Included in LLM-based and human pairwise evaluations; same protocols applied.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>HypoGeniC yielded mixed gains; iterative-refinement overall outperformed but HypoGeniC sometimes benefited from using mistakes. LLM-based novelty/helpfulness for HypoGeniC varied (novelty sometimes higher w/o demos).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Implementation hyperparameters specified (bank size 5, α=0.5, num_init=10, max wrong examples per group=2) which affect behavior; still constrained by strong model prior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7756.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7756.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis-based Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis-based Inference (pattern-based classification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation procedure that measures how well generated natural-language hypotheses support downstream decision-making by using the hypothesis as a pattern to assign labels to test examples and reporting classification performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002 (used to evaluate hypotheses during inference prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-modal classification (text, image, image-text)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation method for hypotheses (applied to generated hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hypothesis-based classification (single/multiple hypothesis)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply a generated hypothesis (or set of hypotheses) to each test input x_j: if the example satisfies the pattern, assign the corresponding class; aggregate predictions across the test set to compute accuracy. Also a variant removes task-specific knowledge from the inference prompt to reduce prior influence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (single-hypothesis best and average across hypotheses); Accuracy difference vs. no-demonstration baseline</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy reported as percentage and mean ± standard deviation across seeds; comparisons show absolute and relative changes (e.g., differences mostly <3%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Hallucination pattern induction; Unhealthy Comments; Funny Reddit posts; PneumoniaMNIST; Truthful Hotel Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable for this automatic classification evaluation though human evals validated hypothesis quality separately.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Single- and multiple-hypothesis classification accuracies reported in tables (e.g., overall averages show limited degradation when removing demonstrations; some datasets saw ~4.5% drop under flipped labels in truthful reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Inference can be influenced by model prior; the paper includes a variant that eliminates task-specific knowledge from inference prompts to isolate hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7756.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7756.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Scoring and Pairwise Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated evaluation using an LLM to (1) score each hypothesis on helpfulness and novelty (1–5 scale) and (2) perform pairwise comparisons between hypotheses generated with and without demonstrations to determine preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (primary for evaluation); other LLMs optionally</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (API versions 2024-08-06 / fallback 2024-05-13)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation/meta-evaluation for generated natural-language hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated evaluation metrics for hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based scoring (1–5) and pairwise comparison</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each hypothesis, LLM assigns a helpfulness and novelty score on a 1–5 scale; additionally, randomly pair hypotheses generated w/ and w/o demonstrations and prompt the LLM to select the better hypothesis per pair to compute win rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Helpfulness (1–5), Novelty (1–5), Pairwise win rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Helpfulness/Novelty: integer scores 1 (low) to 5 (high) averaged across hypotheses; Pairwise win rate: percent of pairs where a condition wins aggregated across datasets (25 pairs per baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>25 hypotheses per baseline across the five datasets (5 per dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>LLM-based evaluation results were validated against human pairwise judgments to check alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Helpfulness averages around 4.00 (w/o demos sometimes higher); novelty averages around 2.45–2.67 with w/o demos sometimes scoring higher; pairwise comparisons show varying win rates where IO-Prompting and iterative-refinement sometimes prefer w/o demos.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-based scoring broadly aligned with human pairwise preferences (humans slightly preferred w/o demos across evaluated datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM-based judgments may reflect model-internal priors and are not a direct substitute for human expert assessment; scoring granularity and prompts affect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7756.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7756.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-Pairwise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Pairwise Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human evaluation protocol using pairwise comparisons where human evaluators select the higher-quality hypothesis between two candidates or indicate no clear difference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (human assessment of LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>human evaluation of generated scientific hypotheses/pattern descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human judgment of hypothesis quality (helpfulness/novelty preference)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human pairwise comparison</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide human participants with paired hypotheses and evaluation context; participants choose the better hypothesis or indicate indistinguishable. Used to validate alignment of LLM-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pairwise preference percentage; inter-participant agreement not explicitly reported</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent of pairs where humans prefer w/ demos vs w/o demos (reported as preference percentages); numeric scale not used due to difficulty of scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Unhealthy Comments; Truthful Reviews; Funny Reddit posts (selected for non-expert-accessible domains)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Nine participants recruited; evaluation interface included context, paired hypotheses, and illustrative examples; participants made pairwise selections.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Humans showed a slight overall preference for hypotheses generated without demonstrations in the sampled datasets; alignment between LLM-based and human preferences was observed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A (comparison was between LLM outputs under different settings, not against human-generated hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small human subject pool (n=9); evaluations limited to domains accessible to non-experts; scoring limited to pairwise choices rather than fine-grained ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7756.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7756.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACR/BCR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adverse Correction Rate (ACR) / Beneficial Correction Rate (BCR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two metrics introduced to quantify how flipping labels in in-context demonstrations changes downstream predictions: ACR measures fraction of previously-correct predictions that become incorrect after flipping; BCR measures fraction of previously-incorrect predictions that become correct after flipping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002 (applied to evaluate flip effects)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation metrics for sensitivity to demonstration labels in hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Adverse Correction Rate (ACR) and Beneficial Correction Rate (BCR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute ACR = (# examples where correct hypothesis flips to incorrect after flipping demos) / (# originally correct under correct demos); BCR = (# examples where incorrect becomes correct after flipping) / (# originally incorrect under correct demos).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ACR, BCR (unitless fractions between 0 and 1)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ACR and BCR defined as ratios over test examples as given in equations (1) and (2) in the paper; reported per-dataset averages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated across five datasets listed in the study</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Metric is automated computation comparing model predictions under correct vs flipped-demo conditions; human evaluation not involved in metric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Flipping labels yielded modest ACR/BCR values for most datasets (minimal changes), except truthful hotel reviews where nearly half of predictions were affected in some settings; overall label flips caused limited change in downstream predictions for majority of datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>ACR/BCR capture aggregate directional changes but do not explain underlying reasons for shifts or dataset-specific sensitivities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7756.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7756.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets-Five</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Five Real-World Inductive Reasoning Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study uses five real-world datasets spanning text, image, and image-text modalities to evaluate hypothesis generation: hallucination pattern induction, unhealthy comments, funny Reddit posts, PneumoniaMNIST, and truthful hotel reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (datasets used to evaluate LLM-generated hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-modal domains: model behavior analysis, medical imaging, social media, deception detection</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation corpora for hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Downstream benchmarks for hypothesis-based classification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each dataset provides labeled positives and negatives; hypotheses are generated to capture positive-class patterns and then applied to classify held-out test sets to measure accuracy and other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Classification accuracy (per dataset), LLM helpfulness/novelty scores, pairwise human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy (%) on held-out test sets; LLM scores 1–5 for helpfulness/novelty; human pairwise preference percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Hallucination pattern induction (adversarial sampling variant), Unhealthy Comments (Price et al., 2020 / Zhong et al., 2023), Funny Reddit posts (Zhong et al., 2023), PneumoniaMNIST (MedMNIST), Truthful Hotel Reviews (Zhou et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluation conducted on three text datasets (Unhealthy, Truthful Reviews, Funny Reddit) where domain is accessible to non-experts; dataset splits provided (train/val/test counts in Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Across these datasets, hypotheses generated without demonstrations retained similar classification performance as those with demonstrations; specific accuracy numbers reported in tables (see paper tables 3,7,8,9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset selection limited to classification tasks; future work needed for non-classification hypotheses and broader domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7756.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7756.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label-Setting Experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Demonstration Label Configurations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of controlled demonstration label configurations used to study the resilience of model prior: correct (ground truth), flipped labels, random labels, only positive examples, and only negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>analysis of in-context learning and model prior influence in inductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>experimental criteria / ablation study</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Demonstration label configuration ablation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate hypotheses and evaluate downstream classification under multiple in-context label formats: correct labels, flipped labels, random labels, only positive-group demonstrations, only negative-group demonstrations; compare accuracies to no-demonstration baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy difference relative to no-demonstration baseline; ACR/BCR for flipped labels</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported accuracy differences (absolute percentage points), ACR/BCR as defined; example: performance differences mostly <3%, flipped labels caused ~4.5% drop in truthful reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same five datasets used in main experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluations included tests of label-format sensitivity; two label formats compared in human study (Format 1: separate positive/negative examples; Format 2: (Example, Label) tuples).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Label configurations produced limited impact on downstream accuracy for most datasets (typically <3% difference); flipped labels had notable effect only on truthful hotel reviews (degradation ~4.5%); prior often dominated demonstration signals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Shows that strong pre-trained priors are difficult to override even with flipped labels; experiments limited to classification tasks and specific prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement <em>(Rating: 2)</em></li>
                <li>Hypothesis generation with large language models <em>(Rating: 2)</em></li>
                <li>HypoGeniC <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>Data-driven discovery with large generative models <em>(Rating: 1)</em></li>
                <li>Discoverybench: Towards data-driven discovery with large language models <em>(Rating: 1)</em></li>
                <li>Evaluating object hallucination in large vision-language models <em>(Rating: 1)</em></li>
                <li>Hypothesis search: Inductive reasoning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7756",
    "paper_id": "paper-274822886",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "IO-Prompting",
            "name_full": "Input-Output Prompting",
            "brief_description": "A single-step prompting baseline that provides in-context demonstrations (input-output examples) once to an LLM to generate a set of natural-language hypotheses describing the positive class pattern.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002",
            "model_size": "GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002",
            "scientific_domain": "multi-modal (text, image, image-text); NLP & computer vision",
            "theory_type": "hypothesis (natural language patterns for classification)",
            "evaluation_method_name": "Hypothesis generation via Input-Output Prompting",
            "evaluation_method_description": "Provide N in-context demonstrations (N=30 in experiments) in the prompt and request m hypotheses; evaluate hypotheses downstream by using them as patterns for classifying test examples.",
            "evaluation_metric": "Accuracy (single-hypothesis and multiple-hypothesis classification); LLM helpfulness and novelty scores; pairwise win rates",
            "metric_definition": "Accuracy: percent of test examples correctly labeled by the hypothesis (reported as mean ± std across seeds); Helpfulness/Novelty: 1–5 LLM-assigned rating scale; Pairwise win rate: percent of pairs where one hypothesis preferred",
            "dataset_or_benchmark": "Hallucination pattern induction; Unhealthy Comments; Funny Reddit posts; PneumoniaMNIST; Truthful Hotel Reviews",
            "human_evaluation_details": "Human evaluation performed via pairwise comparisons across three datasets (Unhealthy Comments, Truthful Reviews, Funny Reddit) with nine participants; experts choose preferred hypothesis or indicate indistinguishable.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Removing demonstrations produced minimal downstream accuracy degradation (differences mostly &lt;3% across datasets); in some cases zero-shot (w/o demos) outperformed w/ demos. LLM helpfulness scores ~4.00 vs ~3.95; novelty ~2.56–2.84 depending on baseline.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A (no direct comparison to human-generated hypotheses reported).",
            "limitations_noted": "Relies on classification framing and single-hypothesis pattern matching; findings limited to datasets and models tested.",
            "uuid": "e7756.0",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Iterative-Refinement",
            "name_full": "Iterative Refinement with Ranking",
            "brief_description": "An iterative hypothesis-generation method that refines a bank of hypotheses by ranking them on a validation set and feeding top-ranked hypotheses back to the LLM (with or without demonstrations) to produce improved hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002",
            "model_size": "GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002",
            "scientific_domain": "multi-modal (text, image, image-text); NLP & computer vision",
            "theory_type": "hypothesis (iteratively refined natural language hypotheses)",
            "evaluation_method_name": "Iterative refinement with ranking feedback",
            "evaluation_method_description": "Initialize a hypothesis bank via IO-prompting (5 hypotheses), evaluate hypotheses on a validation set, select top m hypotheses, provide them (and demonstrations if available) back to the LLM to generate refined hypotheses; repeat for a set number of iterations (3 iterations used).",
            "evaluation_metric": "Accuracy (validation/test), LLM helpfulness and novelty scores, pairwise win rate",
            "metric_definition": "Accuracy: percent correct on validation/test; Helpfulness/Novelty: 1–5 LLM score; Pairwise win rate: % wins across random hypothesis pairs",
            "dataset_or_benchmark": "Hallucination pattern induction; Unhealthy Comments; Funny Reddit posts; PneumoniaMNIST; Truthful Hotel Reviews",
            "human_evaluation_details": "Human pairwise comparisons included hypotheses from iterative-refinement in the same human evaluation protocol (9 participants); pairwise selection among w/ and w/o demos.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Iterative refinement outperformed IO-Prompting and HypoGeniC in many downstream accuracy comparisons, indicating data helps for hypothesis selection though not necessarily for initial generation; LLM-based metrics sometimes favored w/o demos for novelty/helpfulness under iterative-refinement.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A",
            "limitations_noted": "Refinement improves hypothesis selection but still dominated by model prior; iterative pipeline parameters (bank size, iterations) tuned as implementation choices.",
            "uuid": "e7756.1",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "HypoGeniC",
            "name_full": "HypoGeniC (Update from Mistakes)",
            "brief_description": "An iterative hypothesis generation/update strategy that focuses on generating new hypotheses from examples that current hypotheses misclassify (incorrect examples), maintaining a hypothesis bank with reward scores to select candidates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002",
            "model_size": "GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002",
            "scientific_domain": "multi-modal (text, image, image-text); NLP & computer vision",
            "theory_type": "hypothesis (iterative mistake-driven generation)",
            "evaluation_method_name": "HypoGeniC-style mistake-driven hypothesis updating",
            "evaluation_method_description": "Initialize hypothesis bank via IO-prompting with reward scores, then when predefined numbers of incorrect examples for groups are reached, use those misclassified examples to prompt generation of new hypotheses; keep top-m hypotheses by reward.",
            "evaluation_metric": "Accuracy (validation/test), reward scores used for ranking, LLM helpfulness/novelty scores",
            "metric_definition": "Accuracy: percent correct; Reward score: internal reward efficiency α = 0.5 used in selection; Helpfulness/Novelty: 1–5 scale.",
            "dataset_or_benchmark": "Hallucination pattern induction; Unhealthy Comments; Funny Reddit posts; PneumoniaMNIST; Truthful Hotel Reviews",
            "human_evaluation_details": "Included in LLM-based and human pairwise evaluations; same protocols applied.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "HypoGeniC yielded mixed gains; iterative-refinement overall outperformed but HypoGeniC sometimes benefited from using mistakes. LLM-based novelty/helpfulness for HypoGeniC varied (novelty sometimes higher w/o demos).",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A",
            "limitations_noted": "Implementation hyperparameters specified (bank size 5, α=0.5, num_init=10, max wrong examples per group=2) which affect behavior; still constrained by strong model prior.",
            "uuid": "e7756.2",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Hypothesis-based Inference",
            "name_full": "Hypothesis-based Inference (pattern-based classification)",
            "brief_description": "An evaluation procedure that measures how well generated natural-language hypotheses support downstream decision-making by using the hypothesis as a pattern to assign labels to test examples and reporting classification performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002 (used to evaluate hypotheses during inference prompts)",
            "model_size": "GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002",
            "scientific_domain": "multi-modal classification (text, image, image-text)",
            "theory_type": "evaluation method for hypotheses (applied to generated hypotheses)",
            "evaluation_method_name": "Hypothesis-based classification (single/multiple hypothesis)",
            "evaluation_method_description": "Apply a generated hypothesis (or set of hypotheses) to each test input x_j: if the example satisfies the pattern, assign the corresponding class; aggregate predictions across the test set to compute accuracy. Also a variant removes task-specific knowledge from the inference prompt to reduce prior influence.",
            "evaluation_metric": "Accuracy (single-hypothesis best and average across hypotheses); Accuracy difference vs. no-demonstration baseline",
            "metric_definition": "Accuracy reported as percentage and mean ± standard deviation across seeds; comparisons show absolute and relative changes (e.g., differences mostly &lt;3%).",
            "dataset_or_benchmark": "Hallucination pattern induction; Unhealthy Comments; Funny Reddit posts; PneumoniaMNIST; Truthful Hotel Reviews",
            "human_evaluation_details": "Not applicable for this automatic classification evaluation though human evals validated hypothesis quality separately.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Single- and multiple-hypothesis classification accuracies reported in tables (e.g., overall averages show limited degradation when removing demonstrations; some datasets saw ~4.5% drop under flipped labels in truthful reviews).",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A",
            "limitations_noted": "Inference can be influenced by model prior; the paper includes a variant that eliminates task-specific knowledge from inference prompts to isolate hypothesis quality.",
            "uuid": "e7756.3",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLM-based Evaluation",
            "name_full": "LLM-based Scoring and Pairwise Comparison",
            "brief_description": "Automated evaluation using an LLM to (1) score each hypothesis on helpfulness and novelty (1–5 scale) and (2) perform pairwise comparisons between hypotheses generated with and without demonstrations to determine preferences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (primary for evaluation); other LLMs optionally",
            "model_size": "GPT-4o (API versions 2024-08-06 / fallback 2024-05-13)",
            "scientific_domain": "evaluation/meta-evaluation for generated natural-language hypotheses",
            "theory_type": "automated evaluation metrics for hypotheses",
            "evaluation_method_name": "LLM-based scoring (1–5) and pairwise comparison",
            "evaluation_method_description": "For each hypothesis, LLM assigns a helpfulness and novelty score on a 1–5 scale; additionally, randomly pair hypotheses generated w/ and w/o demonstrations and prompt the LLM to select the better hypothesis per pair to compute win rates.",
            "evaluation_metric": "Helpfulness (1–5), Novelty (1–5), Pairwise win rate (%)",
            "metric_definition": "Helpfulness/Novelty: integer scores 1 (low) to 5 (high) averaged across hypotheses; Pairwise win rate: percent of pairs where a condition wins aggregated across datasets (25 pairs per baseline).",
            "dataset_or_benchmark": "25 hypotheses per baseline across the five datasets (5 per dataset)",
            "human_evaluation_details": "LLM-based evaluation results were validated against human pairwise judgments to check alignment.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Helpfulness averages around 4.00 (w/o demos sometimes higher); novelty averages around 2.45–2.67 with w/o demos sometimes scoring higher; pairwise comparisons show varying win rates where IO-Prompting and iterative-refinement sometimes prefer w/o demos.",
            "comparison_to_human_generated": false,
            "comparison_results": "LLM-based scoring broadly aligned with human pairwise preferences (humans slightly preferred w/o demos across evaluated datasets).",
            "limitations_noted": "LLM-based judgments may reflect model-internal priors and are not a direct substitute for human expert assessment; scoring granularity and prompts affect outputs.",
            "uuid": "e7756.4",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Human-Pairwise",
            "name_full": "Human Pairwise Evaluation",
            "brief_description": "A human evaluation protocol using pairwise comparisons where human evaluators select the higher-quality hypothesis between two candidates or indicate no clear difference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (human assessment of LLM outputs)",
            "model_size": "N/A",
            "scientific_domain": "human evaluation of generated scientific hypotheses/pattern descriptions",
            "theory_type": "human judgment of hypothesis quality (helpfulness/novelty preference)",
            "evaluation_method_name": "Human pairwise comparison",
            "evaluation_method_description": "Provide human participants with paired hypotheses and evaluation context; participants choose the better hypothesis or indicate indistinguishable. Used to validate alignment of LLM-based evaluation.",
            "evaluation_metric": "Pairwise preference percentage; inter-participant agreement not explicitly reported",
            "metric_definition": "Percent of pairs where humans prefer w/ demos vs w/o demos (reported as preference percentages); numeric scale not used due to difficulty of scoring.",
            "dataset_or_benchmark": "Unhealthy Comments; Truthful Reviews; Funny Reddit posts (selected for non-expert-accessible domains)",
            "human_evaluation_details": "Nine participants recruited; evaluation interface included context, paired hypotheses, and illustrative examples; participants made pairwise selections.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Humans showed a slight overall preference for hypotheses generated without demonstrations in the sampled datasets; alignment between LLM-based and human preferences was observed.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A (comparison was between LLM outputs under different settings, not against human-generated hypotheses).",
            "limitations_noted": "Small human subject pool (n=9); evaluations limited to domains accessible to non-experts; scoring limited to pairwise choices rather than fine-grained ratings.",
            "uuid": "e7756.5",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ACR/BCR",
            "name_full": "Adverse Correction Rate (ACR) / Beneficial Correction Rate (BCR)",
            "brief_description": "Two metrics introduced to quantify how flipping labels in in-context demonstrations changes downstream predictions: ACR measures fraction of previously-correct predictions that become incorrect after flipping; BCR measures fraction of previously-incorrect predictions that become correct after flipping.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002 (applied to evaluate flip effects)",
            "model_size": "GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002",
            "scientific_domain": "evaluation metrics for sensitivity to demonstration labels in hypothesis generation",
            "theory_type": "evaluation metrics",
            "evaluation_method_name": "Adverse Correction Rate (ACR) and Beneficial Correction Rate (BCR)",
            "evaluation_method_description": "Compute ACR = (# examples where correct hypothesis flips to incorrect after flipping demos) / (# originally correct under correct demos); BCR = (# examples where incorrect becomes correct after flipping) / (# originally incorrect under correct demos).",
            "evaluation_metric": "ACR, BCR (unitless fractions between 0 and 1)",
            "metric_definition": "ACR and BCR defined as ratios over test examples as given in equations (1) and (2) in the paper; reported per-dataset averages.",
            "dataset_or_benchmark": "Evaluated across five datasets listed in the study",
            "human_evaluation_details": "Metric is automated computation comparing model predictions under correct vs flipped-demo conditions; human evaluation not involved in metric computation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Flipping labels yielded modest ACR/BCR values for most datasets (minimal changes), except truthful hotel reviews where nearly half of predictions were affected in some settings; overall label flips caused limited change in downstream predictions for majority of datasets.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A",
            "limitations_noted": "ACR/BCR capture aggregate directional changes but do not explain underlying reasons for shifts or dataset-specific sensitivities.",
            "uuid": "e7756.6",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Datasets-Five",
            "name_full": "Five Real-World Inductive Reasoning Datasets",
            "brief_description": "The study uses five real-world datasets spanning text, image, and image-text modalities to evaluate hypothesis generation: hallucination pattern induction, unhealthy comments, funny Reddit posts, PneumoniaMNIST, and truthful hotel reviews.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (datasets used to evaluate LLM-generated hypotheses)",
            "model_size": "N/A",
            "scientific_domain": "multi-modal domains: model behavior analysis, medical imaging, social media, deception detection",
            "theory_type": "evaluation corpora for hypothesis generation",
            "evaluation_method_name": "Downstream benchmarks for hypothesis-based classification",
            "evaluation_method_description": "Each dataset provides labeled positives and negatives; hypotheses are generated to capture positive-class patterns and then applied to classify held-out test sets to measure accuracy and other metrics.",
            "evaluation_metric": "Classification accuracy (per dataset), LLM helpfulness/novelty scores, pairwise human preferences",
            "metric_definition": "Accuracy (%) on held-out test sets; LLM scores 1–5 for helpfulness/novelty; human pairwise preference percentages.",
            "dataset_or_benchmark": "Hallucination pattern induction (adversarial sampling variant), Unhealthy Comments (Price et al., 2020 / Zhong et al., 2023), Funny Reddit posts (Zhong et al., 2023), PneumoniaMNIST (MedMNIST), Truthful Hotel Reviews (Zhou et al., 2024)",
            "human_evaluation_details": "Human evaluation conducted on three text datasets (Unhealthy, Truthful Reviews, Funny Reddit) where domain is accessible to non-experts; dataset splits provided (train/val/test counts in Appendix).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Across these datasets, hypotheses generated without demonstrations retained similar classification performance as those with demonstrations; specific accuracy numbers reported in tables (see paper tables 3,7,8,9).",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A",
            "limitations_noted": "Dataset selection limited to classification tasks; future work needed for non-classification hypotheses and broader domains.",
            "uuid": "e7756.7",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Label-Setting Experiments",
            "name_full": "In-Context Demonstration Label Configurations",
            "brief_description": "A set of controlled demonstration label configurations used to study the resilience of model prior: correct (ground truth), flipped labels, random labels, only positive examples, and only negative examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o; Qwen2-VL-72B; gemini-1.5-pro-002",
            "model_size": "GPT-4o (API versions 2024-08-06 / fall-back 2024-05-13); Qwen2-VL-72B; gemini-1.5-pro-002",
            "scientific_domain": "analysis of in-context learning and model prior influence in inductive reasoning",
            "theory_type": "experimental criteria / ablation study",
            "evaluation_method_name": "Demonstration label configuration ablation",
            "evaluation_method_description": "Generate hypotheses and evaluate downstream classification under multiple in-context label formats: correct labels, flipped labels, random labels, only positive-group demonstrations, only negative-group demonstrations; compare accuracies to no-demonstration baseline.",
            "evaluation_metric": "Accuracy difference relative to no-demonstration baseline; ACR/BCR for flipped labels",
            "metric_definition": "Reported accuracy differences (absolute percentage points), ACR/BCR as defined; example: performance differences mostly &lt;3%, flipped labels caused ~4.5% drop in truthful reviews.",
            "dataset_or_benchmark": "Same five datasets used in main experiments",
            "human_evaluation_details": "Human evaluations included tests of label-format sensitivity; two label formats compared in human study (Format 1: separate positive/negative examples; Format 2: (Example, Label) tuples).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Label configurations produced limited impact on downstream accuracy for most datasets (typically &lt;3% difference); flipped labels had notable effect only on truthful hotel reviews (degradation ~4.5%); prior often dominated demonstration signals.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A",
            "limitations_noted": "Shows that strong pre-trained priors are difficult to override even with flipped labels; experiments limited to classification tasks and specific prompt formats.",
            "uuid": "e7756.8",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement",
            "rating": 2,
            "sanitized_title": "phenomenal_yet_puzzling_testing_inductive_reasoning_capabilities_of_language_models_with_hypothesis_refinement"
        },
        {
            "paper_title": "Hypothesis generation with large language models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "HypoGeniC",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "Data-driven discovery with large generative models",
            "rating": 1,
            "sanitized_title": "datadriven_discovery_with_large_generative_models"
        },
        {
            "paper_title": "Discoverybench: Towards data-driven discovery with large language models",
            "rating": 1,
            "sanitized_title": "discoverybench_towards_datadriven_discovery_with_large_language_models"
        },
        {
            "paper_title": "Evaluating object hallucination in large vision-language models",
            "rating": 1,
            "sanitized_title": "evaluating_object_hallucination_in_large_visionlanguage_models"
        },
        {
            "paper_title": "Hypothesis search: Inductive reasoning with language models",
            "rating": 1,
            "sanitized_title": "hypothesis_search_inductive_reasoning_with_language_models"
        }
    ],
    "cost": 0.01674575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On the Role of Model Prior in Real-World Inductive Reasoning
18 Dec 2024</p>
<p>Zhuo Liu zhuo.liu@rochester.edu 
University of Rochester</p>
<p>Ding Yu ding.yu@rochester.edu 
University of Rochester</p>
<p>Hangfeng He hangfeng.he@rochester.edu 
University of Rochester</p>
<p>On the Role of Model Prior in Real-World Inductive Reasoning
18 Dec 2024F0170C905C62561F10E7434C4332E644arXiv:2412.13645v1[cs.AI]
Large Language Models (LLMs) show impressive inductive reasoning capabilities, enabling them to generate hypotheses that could generalize effectively to new instances when guided by in-context demonstrations.However, in realworld applications, LLMs' hypothesis generation is not solely determined by these demonstrations but is significantly shaped by taskspecific model priors.Despite their critical influence, the distinct contributions of model priors versus demonstrations to hypothesis generation have been underexplored.This study bridges this gap by systematically evaluating three inductive reasoning strategies across five real-world tasks with three LLMs.Our empirical findings reveal that, hypothesis generation is primarily driven by the model's inherent priors; removing demonstrations results in minimal loss of hypothesis quality and downstream usage.Further analysis shows the result is consistent across various label formats with different label configurations, and prior is hard to override, even under flipped labeling.These insights advance our understanding of the dynamics of hypothesis generation in LLMs and highlight the potential for better utilizing model priors in real-world inductive reasoning tasks.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have drawn significant interests due to their performance on a diverse range of reasoning tasks (Kojima et al., 2022), such as mathematical reasoning, commonsense reasoning and symbolic reasoning.Inductive reasoningan important component of reasoning (Yang et al., 2022;Heit, 2000), as a way to derive abstract hypothesis from limited specific observations, is widely regarded as a core aspect of human intelligence.</p>
<p>Existing studies primarily assess the inductive reasoning capabilities of LLMs (Wang et al., 2023;Qiu et al., 2023;Cheng et al., 2024) by evaluating their ability to generate textual hypotheses based on in-context input-output pairs and subsequently test these hypotheses on unseen examples, thereby evaluating their generalization abilities.These studies demonstrated that LLMs can propose high-quality hypotheses, establishing them as exceptional hypothesis generators (Qiu et al., 2023;Cheng et al., 2024;Li et al., 2024).</p>
<p>LLMs employ various approaches to generate hypotheses depending on the nature of the task.For symbolic tasks, such as mathematical function discovery (Shojaee et al., 2024), LLMs rely primarily on input-output mappings in demonstrations, often with minimal prior knowledge about the mathematical functions.In contrast, research by Qi et al. (2023) demonstrated that LLMs can formulate hypotheses solely from provided background information, leveraging the extensive and diverse knowledge gained during pre-training.In real-world applications, hypothesis generation tends to be datadriven , such as generating hypotheses for trending Twitter headline patterns (Zhou et al., 2024), where both prior knowledge and demonstrations are utilized.In these cases, the interaction between the model's task-specific priors and provided examples is mixed.</p>
<p>In empirical science, data-driven hypothesis generation serves as the foundational step toward scientific discovery (Majumder et al., 2024a,b).When employing LLMs for hypothesis generation, the goal is to uncover novel hypotheses that contribute fresh insights and ideas to the existing literature (Zhou et al., 2024).However, due to the combined influence of the model's prior knowledge and the provided examples, the origin of generated hypotheses often remains unclear.For certain tasks, where LLMs are pre-trained on extensive knowledge bases, a strong model prior may even overshadow the potential for generating genuinely novel insights from the provided examples.This raises a critical question: What is the role of model prior in real-world inductive reasoning?</p>
<p>To address this issue, this paper presents a systematic empirical study on real-world inductive reasoning problems, focusing on classification tasks, where hypotheses are generated to capture patterns specific to the positive class.We evaluate three representative baselines: direct input-output prompting (Qiu et al., 2023), iterative refinement with ranking (Qiu et al., 2023;Shojaee et al., 2024), and HypoGeniC (Zhou et al., 2024;Liu et al., 2024), across five diverse real-world tasks covering text, image, and image-text modalities.For each baseline, we conduct experiments where LLMs generate hypotheses both with and without demonstrations.The quality of the generated hypotheses is then evaluated from three perspectives: hypothesisbased classification performance, LLM-based assessments, and human evaluation.</p>
<p>Our experimental results reveal that, for realworld tasks where LLMs have been trained on substantial amounts of relevant data, task-specific model prior plays a dominant role in hypothesis generation.Notably, removing in-context demonstrations has minimal impact on the quality of the hypotheses.This trend holds consistently across three baselines with three LLMs: GPT-4o, Qwen2-VL and Gemini-pro, strongly suggesting that, counterintuitively, LLMs depend more on task-specific prior knowledge than on in-context demonstrations for generating hypotheses.Further analysis across various label configurations and formats supports this conclusion, indicating that model prior is often so robust that it is minimally affected by the provided examples.</p>
<p>Related Work</p>
<p>Inductive Reasoning with LLMs.Primary studies on inductive reasoning mainly focus on evaluating their inductive reasoning capabilities.Qiu et al. (2023) evaluate LLMs by inducting rules from examples, demonstrated that LLMs are good hypothesis proposers.Wang et al. (2023) uses Python programs to select better hypothesis, thus improving the inductive reasoning performance.Besides these evaluations on symbolic tasks, Yang et al. (2022) propose to induce natural language rules from natural language facts while Hypotheses-to-Theories (Zhu et al., 2023) learns rules from deduction.Similarly, Honovich et al. (2022) also show LLMs are able to infer a natural task description by provided demonstrations.Recently, some works employ LLMs to generate hypothesis that can describe the difference or shift between two distributions in different modalities, such as text (Zhong et al., 2022(Zhong et al., , 2023;;Singh et al., 2022), and image (Dunlap et al., 2024;Kim et al., 2024).Distinct from these studies, our work delves into understanding how LLMs perform inductive reasoning for real-world tasks, offering insights into their underlying mechanisms.</p>
<p>Hypothesis Generation with LLMs.Yang et al. (2023b) uses raw web corpus as observations to generate scientific hypothesis, and Pham et al. ( 2023) generates hypothesis to uncover latent topics in a text collection.In Qi et al. (2023), it shows LLMs are good hypothesis proposers with only background knowledge.Majumder et al. (2024a) provides initial evidence for LLMs to do datadriven discovery, where both search and verification of hypotheses may be carried out using a dataset alone.HypoGeniC (Zhou et al., 2024) also uses LLMs to generate hypothesis from real-world labeled examples.Si et al. (2024) and Baek et al. (2024) further explore the potential to generate hypothesis in research with LLMs to provide insights and ideas for the literature.Additionally, Liu et al. (2024) combines theory-based generation and datadriven generation to get better hypothesis.However, these works do not clearly distinguish whether the hypotheses originate from hidden knowledge or provided examples-a distinction that is the central focus of our work.</p>
<p>Natural Language Hypothesis Generation</p>
<p>Let Z = D P ∪ D N represent the labeled data for a real-world classification task T , where D P and D N correspond to demonstrations of the positive (P ) and negative (N ) classes, respectively.Each sample in Z is a pair (x, y), where x denotes the example and y ∈ {P, N } represents the label.A valid natural language hypothesis h, as introduced by Zhong et al. (2022), is expressed as a natural language string.For any example x, h is capable of determining whether x belongs to the positive or negative class.Natural language hypothesis generation involves prompting LLMs to produce a set of valid hypotheses H = {h 1 , h 2 , . . ., h m } using in-context demonstrations tailored to task T .In this paper, we consider the setting where the input to LLMs can be divided into two parts, as shown in Figure 1 guage to describe the task and the requirements for the hypothesis.(2) Demonstrations: a set of exemplars from different groups structured in a specified way to show the patterns of each group.Ideally, we aim to prompt LLMs to generate a list of valid hypothesis to maximize the downstream task performance, by carefully selecting instructions and demonstrations.There are two factors contributing to the hypothesis generation: Task-Specific Model Prior: LLMs are pretrained on a diverse set of datasets, allowing them to accumulate extensive background knowledge across a wide range of domains.When provided with a task description, the model leverages its priors to infer relevant patterns, generating hypotheses based on this internalized knowledge.Input-Label Mappings in Demonstrations: The demonstrations provided serve as a specific guidance, offering cues about how to approach the task.The model may use these demonstrations to refine its hypothesis generation, aligning its output more closely with the intended task requirements.</p>
<p>Experimental Settings</p>
<p>Hypothesis Generation Baselines</p>
<p>In this paper, we evaluate three commonly-used hypothesis generation baselines.</p>
<p>Input-Output Prompting.Input-output prompting (IO-Prompting) represents the most common approach to prompting LLMs (Qiu et al., 2023).In this standard IO-Prompting framework, we directly provide the LLMs with a set of in-context demonstrations within the prompt context.The objective is to generate m hypotheses that effectively captures the patterns of positive class P .This approach is a single-step method, utilizing the incontext demonstrations once to guide the model's hypothesis generation.</p>
<p>Iterative Refinement with Ranking.Standard IO-prompting utilizes in-context demonstrations only once, potentially under utilizing their full capacity.To address this limitation, various methods have been proposed to iteratively refine hypotheses, thereby enhancing model performance (Wang et al., 2023;Qiu et al., 2023;Shojaee et al., 2024;Xiao et al., 2024).In our approach, we iteratively refine hypotheses using ranking information as a feedback signal.</p>
<p>The refinement process begins with an initial set of m hypotheses generated via IO-prompting.At each iteration, hypotheses in the bank are ranked based on their performance on a validation set.The top-ranked m hypotheses are then fed back to the model, along with in-context demonstrations, guiding it to generate hypotheses with improved performance.In cases where no demonstrations are available, only the ranked hypotheses with their accuracies are provided in the iterative refinement process.This approach thus augments data utilization by continuously leveraging feedback to generate higher-quality hypotheses.</p>
<p>Update from Mistakes: HypoGeniC.The previous methods leverage data within one single prompt to generate hypotheses, yet using all demonstrations in a single prompt may not be optimal for performance.Therefore, we also evaluate a strategy that updates hypotheses from mistakes made by current hypothesis.We largely follow an established approach, HypoGeniC (Zhou et al., 2024;Liu et al., 2024), which iteratively generate new hypotheses from incorrect prediction examples.</p>
<p>In our evaluation, we initialize the hypothesis bank using standard IO-prompting as well as the reward scores as in Zhou et al. (2024); Liu et al. (2024).During the update phase, if the number of incorrect examples for each group reaches a predefined number, these incorrect examples are employed to guide the generation of new hypotheses.In each update, m hypotheses with highest reward scores are kept in the hypothesis bank.This iterative updating approach enables the model to adapt hypotheses progressively, making better use of feedback from misclassifications.For a fair comparison, when demonstrations are absent, we update the hypothesis by iterative refinement, using reward scores for ranking.</p>
<p>All the implementation details are in the Appendix B.</p>
<p>Evaluation of Hypothesis</p>
<p>After generating a set of hypotheses H = {h 1 , h 2 , . . ., h m }, it is crucial to evaluate their quality to ensure that the generated hypotheses are both functional and interpretable.We perform this evaluation from three perspectives: hypothesisbased classification, LLM-based evaluation and human evaluation.These complementary methods allow for a robust assessment, combining quantitative performance metrics with qualitative assessments from domain experts.</p>
<p>Hypothesis-based Inference.In hypothesisbased inference (Liu et al., 2024;Zhou et al., 2024), the goal is to assess how well the generated hypotheses support downstream decisionmaking tasks.We measure the predictive performance of the hypothesis on a test dataset D test = {(x j , y j )} Ntest j=1 .The hypothesis is evaluated based on how accurately it assigns the correct label to each input x j .Predictions are made by comparing test examples x j with learned patterns, which can consist of a single hypothesis or multiple hypotheses.If a test example satisfies the pattern, it is assigned the corresponding class.Unless otherwise stated, the results reported in this work are based on patterns formed from single hypothesis.To re-move the influence of prior in the inference, we also do hypothesis-based inference without knowledge, which can be found in Appendix C.1.See Appendix F for evaluation prompts.</p>
<p>LLM-based Evaluation.In addition to assessing the effectiveness of hypotheses in downstream task usage, we also evaluate their helpfulness (Liu et al., 2024) and novelty (Liu et al., 2024;Si et al., 2024) through LLM-based metrics.Specifically:</p>
<p>(1) Helpfulness measures the extent to which a hypothesis accurately captures the underlying patterns of the data and generalizes effectively to unseen samples.(2) Novelty assesses whether the hypothesis introduces new insights or unique perspectives relevant to the task.</p>
<p>Our LLM-based evaluation incorporates both scoring and pairwise comparison assessments.For scoring, LLMs assign a rating on a 5-point scale to reflect each hypothesis's quality.For pairwise comparison, we randomly pair hypotheses generated with and without demonstrations, and prompt the LLMs to select the better hypothesis in each pair.This pairwise evaluation provides insights into relative performance, while scoring offers an absolute measure of quality.Human Evaluation.To validate the effectiveness of LLM-based evaluation, we also conduct a human evaluation to assess the quality of the generated hypotheses.Our goal is to examine the degree of alignment between LLM-based evaluation results and those obtained from human experts.</p>
<p>Given that scoring may be challenging for human evaluators, we employ a pairwise comparison format, allowing experts to select the higher-quality hypothesis or indicate if the difference is difficult to discern.A total of nine participants are recruited for this evaluation, ensuring diverse perspectives in assessing the hypotheses.</p>
<p>For further details in both LLM-based and human evaluations, refer to Appendix D.</p>
<p>Other Settings</p>
<p>Models.We conduct experiments with GPT-4o, Qwen2-VL-72B 1 and gemini-1.5-pro-002,leveraging both open-source models and API-accessible models to ensure diverse evaluation.Unless otherwise stated, we use GPT-4o 2 in experiments.</p>
<p>1 https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ</p>
<p>2 By default, we use GPT-4o-2024-08-06.However, if a request is rejected due to safety reasons, we will switch to GPT-4o-2024-05-13.Datasets.We conduct evaluations on five realworld inductive reasoning datasets: hallucination pattern induction (Li et al., 2023), unhealthy comments (Zhong et al., 2023), funny Reddit posts (Zhong et al., 2023), pneumoniaMNIST (Xiao et al., 2024), and truthful hotel reviews (Zhou et al., 2024).</p>
<p>Our selection of datasets is motivated by three key factors: (1) their coverage of three distinct modalities-text (unhealthy comments, funny Reddit posts, and truthful hotel reviews), image (pneu-moniaMNIST), and image-text (hallucination pattern induction), (2) diverse domains, including model behavior analysis (hallucination pattern induction), medical diagnosis (pneumoniaMNIST), and social media content (unhealthy comments, funny Reddit posts, and truthful hotel reviews), and (3) their status as widely studied problems in realworld inductive reasoning tasks.Further details and more references for these datasets are provided in Appendix 6.</p>
<p>Other Parameters.The number of in-context demonstrations is set to N = 30 for IO-prompting and iterative-refinement, and N = 50 for Hy-poGenic to encourage more updates.Examples are randomly sampled from the training set.For each dataset, we generate five candidate hypothe- ses.Main results are averaged over three random seeds to ensure robustness.More implementation details can be found in Appendix B.</p>
<p>5 Task-Specific Model Prior Dominates Hypothesis Generation</p>
<p>LLMs Are Zero-Shot Hypothesis Generators</p>
<p>To see the impact of the model prior in hypothesis generation, we compare the hypothesis generation in the following two settings.</p>
<p>Model Prior Only is a typical zero-shot hypothesis generation scenario without the use of demonstrations, relying primarily on prior for generation.Demos with Ground Truth Labels is used in a typical real-world inductive reasoning tasks, with demonstrations as a specific guidance.</p>
<p>Results for single hypothesis-based and multiple hypotheses-based classification are shown in Table 1 and Table 3. From the results, We find that removing in-context demonstrations cause little degradation for the downstream task performance.The trend is consistent across five different datasets on three baselines.In some cases, LLMs can even generate better hypothesis using only model prior.</p>
<p>Additionally, iterative refinement outperforms the other two baselines, showing that data still helps for hypothesis selection, but not as in-context demonstrations for hypothesis generation.</p>
<p>Resutls with Qwen2-VL and Gemini-1.5-pro.</p>
<p>The results for single hypothesis-based classification on Qwen2-VL and Gemini-1.5-pro-002,with IO-prompting, are provided in Table 2 and Appendix C.3.These results similarly show a negligible performance drop without demonstrations, underscoring the universality of our findings across different models.</p>
<p>These results indicates LLMs are good zero-shot hypothesis proposers under strong prior, and incontext demonstrations with ground truth labels are not necessary to achieve acceptable hypothesis.This is a counter-intuitive phenomenon, given that labeled data is very important in in-context learning (Brown, 2020), which can inform the model of corresponding data distribution (Min et al., 2022).</p>
<p>Input-Label Mappings in Demonstrations Cannot Override Strong Model Prior</p>
<p>To further explore the interaction between model prior and input-label mappings in demonstrations in hypothesis generation, we use in-context demonstrations with different label settings:</p>
<p>(1) Demos with ground truth (correct) labels.</p>
<p>(2) Demos with flipped labels.</p>
<p>(3) Demos with random labels.</p>
<p>(4) Only positive group demos.</p>
<p>(5) Only negative group demos.</p>
<p>Figure 2 illustrates the relative accuracy difference between various label settings and without demonstrations.From the result, there is quite limited difference (mostly smaller than 3%) of performance among different settings, with the flipped label setting in truthful review as an exception, which has a performance degradation about 4.5%.</p>
<p>These findings suggest that while demonstrations can provide some guidance, the models' hypothesis generation abilities are ultimately shaped more by its pre-trained priors than by any superficial label configurations.Furthermore, the prior is too strong to be overridden by the patterns in demonstrations, even with totally flipped labels.</p>
<p>LLM-based Evaluation Results</p>
<p>LLM-based Scoring.Table 4 summarizes the helpfulness and novelty scores for various approaches.Each score represents the average of 25 hypotheses generated across five datasets.For helpfulness, hypotheses generated without demonstrations achieve higher scores when using IOprompting and iterative-refinement. Regarding novelty, hypotheses generated without demonstrations score higher on IO-prompting and Hypogenic, while iterative-refinement yields a tie between the two settings.</p>
<p>LLM-based Pairwise Comparison. Figure 3 presents the pairwise comparison results for three baselines, evaluating hypotheses generated with and without demonstrations.The comparisons involve randomly paired hypotheses, with win rates aggregated across all datasets.For Helpfulness, IO prompting and iterative refinement perform better without demonstrations, while HypoGenic demonstrates improved performance with them.For Novelty, iterative refinement excels in the absence of demonstrations, whereas IO prompting and Hy-poGenic exhibit minimal differences between the two settings.These results highlight that LLMs can produce highly helpful and novel hypotheses even without in-context demonstrations.</p>
<p>Human Evaluation Results</p>
<p>We conduct a human evaluation on Funny Reddit, Truthful Reviews, and Unhealthy Comments To evaluate the consistency of results across different label formats, we compare two label formats: Label Format 1: Demonstrations are provided as examples for positive and negative classes as in Figure 1.Label Format 2: Demonstrations are presented in the format of (Example, Label).</p>
<p>The average accuracy across all datasets for the correct and flipped label settings is presented in Figure 5. (Results for each dataset of Label Fomat 2 can be found in Appendix C.2).With correct labels, the performance of the two label formats is very similar.However, in the flipped label settings, Label Format 2 shows almost no performance drop, which differs slightly from Label Format 1. Notably, neither label format outperforms the hypotheses generated without demonstrations.This finding highlights the dominant role of the strong model prior, regardless of the presentation style of the demonstrations.</p>
<p>What's the difference between correct label and flipped label settings?</p>
<p>To get an deep understanding for the impact of flipping labels and provide a more fine-grained evaluation, we adopt two additional metrics introduced by
ACR = n i=1 I (ycorrect(xi) = yi ∧ yflipped(xi) ̸ = yi) n i=1 I (ycorrect(xi) = yi) ,(1)BCR = n i=1 I (ycorrect(xi) ̸ = yi ∧ yflipped(xi) = yi) n i=1 I (ycorrect(xi) ̸ = yi) ,(2)
where y correct (x i ) and y flipped (x i ) represents the prediction results using the hypothesis generated with ground truth label and flipped label demonstrations, x i , y i are input and ground truth label, respectively.These metrics offer a comprehensive evaluation of how flipping labels of the demonstrations influence the prediction results in downstream tasks.</p>
<p>Results for multiple hypothesis-based classification prediction difference are shown in Table 5.The results indicate that flipping the labels of incontext demonstrations does lead to some shifts in prediction outcomes, particularly notable in the truthful hotel review dataset, where nearly half of the predictions are affected.In contrast, for the other four datasets, label flipping only minimally alters prediction results.This suggests that while the model leverages the input-label mappings in provided demonstrations to inform its hypothesis generation, the inherent task-specific knowledge remains predominant, preventing the provided patterns from overriding its established priors.</p>
<p>A Case Study: Hypothesis Generation for</p>
<p>Positive Sentiment Pattern</p>
<p>This case study highlights that large language models (LLMs) heavily rely on prior knowledge when generating hypotheses, often ignoring patterns introduced in demonstrations.As shown in Figure 6, we replace true positive demonstrations with flipped label demonstrations (negative examples) to test whether the model adjusts its hypothesis or adheres to its prior.</p>
<p>Using IO-prompting, we provide six demonstrations, varying the number of flipped label demos from 0 to 5, and prompt the model to generate a hypothesis and corresponding supporting demonstrations.Repeating the experiment across 50 random seeds, we track the distribution of true positive and negative examples within the model's supported demonstrations for its hypothesis.</p>
<p>The results, shown in Figure 7, reveal notable patterns.The distribution of positive examples in the supported demonstrations begins to shift when three flipped label demonstrations are introduced.When five flipped demonstrations are provided, the mean number of positive examples converges to one.However, the model consistently avoids using flipped label demonstrations in its hypothesis generation, even when five demonstrations are flipped.This indicates that the model's hypotheses are predominantly influenced by prior knowledge rather than the provided demonstrations.</p>
<p>Conclusion</p>
<p>In this paper, we explore the role of task-specific priors in a real-world inductive reasoning scenario-hypothesis generation from labeled data.Experiments reveal that LLMs rely heavily on strong priors, which are difficult to override with demonstrations, offering insights into hypothesis generation mechanisms and future research directions.</p>
<p>Limitations</p>
<p>Beyond Classification Problems.Our experiments are limited to classification problems.Extensions to multi-choice or other tasks requires better representation of the hypothesis.We leave extensions to non-classification tasks for future work.</p>
<p>Better Application of Generated Hypotheses.We think future can explore better application of generated hypotheses.For instance, this paper uses hypotheses to construct patterns for classification problems.Better application of hypotheses can improve downstream task performance, which we leave for future work.</p>
<p>A Dataset Details</p>
<p>In this paper, we include 5 real-world datasets: hallucination, unhealthy comments in conversation, truthful hotel review, pneumonia MNIST and funny reddit post.Hallucination Pattern.The dataset is first introduced in (Li et al., 2023).We use its adversarial sampling version, which can be found in https://github.com/RUCAIBox/POPE.To build our hallucination dataset, we prompt GPT-4o with each image-question pair once and see if the model hallucinates the object presence.As a result, we get 437 hallucinated image-question pairs and randomly sample another 437 image-question pairs as nonhallucination cases.</p>
<p>Unhealthy Comments.Expert-annotated unhealthy conversations are from (Price et al., 2020), and we use the version from (Zhong et al., 2023), which can be downloaded from https://github.com/ruiqi-zhong/D5.We sample longest 1000 samples for unhealthy and healthy comments from the dataset in our evaluation.</p>
<p>Truthful Hotel Reviews.Truthful review detection is an instance of deception.The dataset we use is from (Zhou et al., 2024).The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago, which can be downloaded from https://github.com/ChicagoHAI/hypothesisgeneration.</p>
<p>Funny Reddit Posts.We collect jokes posted on the Reddit forum r/Jokes and cleaned by (Zhong et al., 2023).This dataset can be downloaded from https://github.com/ruiqi-zhong/D5.We also remove all the duplicate samples for better dataset quality.</p>
<p>Pneumonia MNIST.Pneumonia recognition via chest X-ray image is an important problem.The dataset is from (Yang et al., 2023a), and can be downloaded from https://medmnist.com/.</p>
<p>For each dataset, we have at least 200 samples for training, 100 samples for validation and 300 samples for test.For each dataset, we keep a balance between positive and negative class.Detailed statistics is shown in Table 6.</p>
<p>B Implementation Details</p>
<p>Model Parameters.For API usage, the temperature and top-p are set to a small number 1 × 10 −15 and 1 × 10 −10 , respectively.</p>
<p>Iterative Refinement.We initialize the hypothesis bank with 5 hypotheses generated using IOprompting.In refinement process, for each iteration, we select 5 hypotheses achieving highest accuracy on the validation set to LLMs for refinement and hope to get hypothesis with better quality.We evaluate 5 hypotheses with the best performance on validation dataset.We set refinement iteration to 3 in the paper.</p>
<p>HypoGeniC.We set the hypothesis bank size to 5. Throughout the experiment, we use the reward efficient α = 0.5, the number of initialized examples num_init = 10, and maximum number of wrong examples for each group to 2 for more updates.For each iteration, we select top 3 hypotheses to evaluate.For each update, we generate 1 new hypothesis with incorrect examples.When there are no demonstrations, we rank the hypotheses in the bank by reward scores and use this ranking as feedback to get better hypothesis.</p>
<p>C Additional Results</p>
<p>C.1 Hypothesis-based Inference without task-specific knowledge</p>
<p>To minimize the impact of prior knowledge in hypothesis-based inference, we eliminate taskspecific knowledge from the evaluation prompt and remove learned patterns from the hypothesis.Instead, we reformulate the task into its corresponding modalities, prompting large language models (LLMs) with: "Does the provided text/image/image-question align with the given text/image/image-question patterns?"This approach isolates the quality of the hypothesis, ensuring that inference is not influenced by prior knowledge.</p>
<p>The results are shown as Table 7.On average, there is limited difference between the hypotheses generated with and without demonstrations.The findings demonstrate again that LLMs are able to generate hypothesis with high quality only with task-specific prior.</p>
<p>C.2 Results of Different Datasets with Label</p>
<p>Format 2</p>
<p>We provide results on each dataset with Label Format 2. The results are shown as Table 8.From the results, we can see that the results vary by dataset.However, there is quite limited difference (smaller than 3%) between correct and flipped label settings, showing the prior is too strong to be overridden by provided demonstrations.</p>
<p>C.3 Results with Gemini Model</p>
<p>We test IO-prompting with and without demonstrations on model gemini-1.5-pro-002.We report the average over two random seeds.The results are shown as Table 9.On average, there is quite limited performance difference with and without demonstrations, demonstrating that with only prior, LLMs can generate good hypotheses.</p>
<p>D Evaluation Details</p>
<p>LLM-based Evaluation Details.We prompt large language models (LLMs) to generate five hypotheses for each dataset across three different baselines.This results in a total of 25 hypotheses per baseline for both settings: with and without demonstrations.</p>
<p>For LLM-based scoring, each hypothesis is evaluated by prompting the LLMs to assign a score on a 1-5 scale.Additionally, for pairwise comparisons, we randomly pair hypotheses generated with and without demonstrations, creating a total of 25 pairs for evaluation.</p>
<p>Human Evaluation Details.We randomly pair the hypotheses generated with and without demonstrations across three datasets and three baselines.We selected the datasets unhealthy comments, truthful reviews, and funny Reddit posts because their domain knowledge is accessible to non-experts.</p>
<p>Participants were provided with a questionnaire for evaluation.For each evaluation, we included the evaluation context, paired hypotheses, and illustrative examples to guide participants.An example of the evaluation interface is shown in Figure 8.</p>
<p>E Examples of Generated Hypothesis</p>
<p>We randomly select generated hypothesis with and without demonstrations for each dataset, shown as Table 10.</p>
<p>F Prompts</p>
<p>For prompt construction, we begin by manually crafting a prompt for hallucination pattern induction, following a format similar to that used in (Zhou et al., 2024).Subsequently, we leverage incontext learning to generate prompts for other tasks.Specifically, we provide the task name along with the manually constructed prompt to the language model, enabling it to generate prompts tailored to other tasks.Table 9: Accuraccy comparison of single hypothesis-based classification with gemini-1.5-pro-002:accuracy (mean ± standard deviation) for the best single hypothesis and the average across five hypotheses, with (w/) and without (w/o) demonstrations."-" means the response is prohibited due to satety reasons.</p>
<p>Dataset Hypothesis without Demos Hypothesis with Demos</p>
<p>Hallucination</p>
<p>Hallucinations are more likely to occur when the questioned object is partially occluded or located in a cluttered environment, making it difficult for the model to accurately identify its presence or absence.</p>
<p><strong>Complex Backgrounds Hypothesis</strong>: Images with complex or cluttered backgrounds may lead to hallucinations, as the model might misinterpret overlapping or densely packed objects as the queried item.</p>
<p>Unhealthy Comments</p>
<p>Comments containing personal attacks or insults are more likely to be unhealthy, as they often escalate conflicts and discourage constructive dialogue.</p>
<p>Comments that include personal attacks or derogatory language towards individuals are more likely to be unhealthy.</p>
<p>Funny Reddit Posts</p>
<p>Posts that incorporate unexpected punchlines or twists are more likely to be perceived as funny, as they play on the element of surprise and subvert reader expectations.</p>
<p>Posts that use wordplay or double entendres, where a phrase can be interpreted in multiple humorous ways, tend to be perceived as funny.</p>
<p>Pneumonia MNIST</p>
<p>The presence of pleural effusion, seen as blunting of the costophrenic angles or fluid layering in the pleural space, may indicate pneumonia.</p>
<p>Presence of air bronchograms within areas of increased opacity suggests pneumonia.</p>
<p>Truthful Hotel Reviews Truthful reviews often mention both positive and negative aspects of the stay, providing a balanced perspective rather than an overly positive or negative one.</p>
<p>Truthful reviews often mention both positive and negative aspects of the stay, providing a balanced perspective that suggests authenticity.</p>
<p>Prompt for hallucination with demonstrations</p>
<p>You're a professional vision-language model behavior analyst.Given a set of image-question pairs, we want to generate hypotheses that are useful for predicting whether a model will hallucinate the existence of an object in response to a given question.</p>
<p>In other words, we want to know whether the model will falsely claim the presence of an object in the image when answering the question.You are an expert in vision-language models, specializing in detecting and preventing hallucinations.</p>
<p>We want to generate hypotheses that are useful for predicting whether a vision-language model will hallucinate the existence of an object when responding to a question about an image.</p>
<p>In other words, we want to identify patterns that indicate when the model will incorrectly claim the presence of an object not present in the image, or the absence of an object that is present.Please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across common hallucination cases and focus on the relationship between the image content and the questioned object.You are an expert in vision-language model behavior detection, and your job is to apply learned patterns to predict if the model will hallucinate for the given image and question.</p>
<p>Here are some previously learned hallucination patterns: {{patterns}} The image is shown and the question is: {{text}} Based on the learned patterns, will the model hallucinate?Let's think step by step.</p>
<p>Step 1: Look at the given image and question, and compare them with the provided hallucination patterns.</p>
<p>Step 2: If the image and question pair matches learned hallucination pattern, then it is likely that the model will hallucinate.If it does not match the pattern, then it is likely that the model will not hallucinate.</p>
<p>Step 3: Apply the pattern to the given image-question pair and predict whether the model will hallucinate based on the learned patterns.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """ Give your answer in the above format.Don't talk about any other words.</p>
<p>Evaluation prompt for unhealthy comments</p>
<p>You will predict whether a comment is unhealthy based on the provided unhealthy comment patterns.</p>
<p>Here are some previously generated unhealthy comment patterns: {{patterns}} A comment is the following: {{text}} Is this comment unhealthy?Think step-by-step.</p>
<p>Step 1: Look at the new comment and compare it with the provided unhealthy comment patterns.</p>
<p>Step 2: If the comment matches the pattern, then it is likely unhealthy.If it does not match the pattern, then it is likely healthy.</p>
<p>Step 3: Apply the pattern to the new comment and predict whether the new comment is unhealthy.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """</p>
<p>Evaluation prompt for truthful review You will predict whether a hotel review is truthful based on the given truthful review patterns.</p>
<p>Here are some previously generated truthful review patterns: {{patterns}} A hotel review is the following: {{text}} Is this hotel review truthful?Think step-by-step.</p>
<p>Step 1: Look at the new hotel review and compare it with the provided truthful review patterns.</p>
<p>Step 2: If the review matches the pattern, then it is likely truthful.If it does not match the pattern, then it is likely not truthful.</p>
<p>Step 3: Apply the pattern to the new hotel review and predict whether the new hotel review is truthful.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """</p>
<p>Evaluation prompt for pneumoniaMNIST</p>
<p>You are an expert in pneumonia detection, and your job is to apply learned patterns to predict if a person has pneumonia.</p>
<p>Here are some previously generated pneumonia patterns: {{patterns}} A chest X-ray image is shown.</p>
<p>Based on the learned patterns and given image, is this person likely to have pneumonia based on the learned patterns?Think step-by-step.</p>
<p>Step 1: Look at the given chest X-ray image and compare it with the provided pneumonia patterns.</p>
<p>Step 2: If the image features match the pneumonia patterns, then the person is likely to have pneumonia.If the features do not match the patterns, then the person is likely not to have pneumonia.</p>
<p>Step 3: Apply the pattern to the new chest X-ray image and predict whether the person has pneumonia.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """ Give your answer in the above format.Don't talk about any other words.</p>
<p>Evaluation prompt for funny reddit You will predict whether a Reddit post is funny based on the given funny Reddit post patterns.</p>
<p>Here are some previously generated funny Reddit post patterns: {{patterns}} A Reddit post is the following: {{text}} Is this Reddit post funny?Think step-by-step:</p>
<p>Step 1: Look at the new Reddit post and compare it with the provided funny post patterns.</p>
<p>Step 2: If the post matches the pattern, then it is likely funny.If it does not match the pattern, then it is likely not funny.</p>
<p>Step 3: Apply the pattern to the new Reddit post and predict whether the new post is funny.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """</p>
<p>Figure 1 :
1
Figure 1: Prompt template for hypothesis generation.</p>
<p>Figure 2 :
2
Figure 2: Accuracy difference comparison of single hypothesis-based classification under different label settings: Accuracy difference (accuracy of different label settings -accuracy without demos) across five datasets with IO-Prompting.</p>
<p>Figure 3 :
3
Figure 3: LLM-based Pairwise Comparison: Pairwise win rate (%) of three baselines.The left plot shows the comparison of Helpfulness, while the right plot presents Novelty.The dashed line indicates a tie where "w/ demos" and "w/o demos" perform equally well.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: An illustration of the case study: positive sentiment hypothesis generation.The highlighted text with a green background represents flipped label demos.</p>
<p>Figure 8 :
8
Figure 8: Example interface of human evaluation.</p>
<p>Table 3 :
3
± 1.0 61.1 ± 0.3 60.1 ± 4.5 61.1 ± 1.3 58.6 ± 4.0 60.1 ± 0.5 Unhealthy Comments 71.5 ± 0.7 70.9 ± 0.5 71.0 ± 0.4 70.9 ± 0.3 70.9 ± 1.0 70.7 ± 2.3 Funny Reddit 58.3 ± 0.4 59.2 ± 0.3 63.9 ± 2.7 67.3 ± 1.2 58.8 ± 0.7 58.4 ± 0.5 Truthful Reviews 63.8 ± 1.4 65.3 ± 0.9 68.5 ± 0.3 69.1 ± 1.3 67.7 ± 1.5 62.1 ± 4.6 PneumoniaMNIST 75.8 ± 0.9 72.2 ± 1.2 76.0 ± 2.5 74.1 ± 1.7 74.9 ± 1.7 74.6 ± 1.0 Accuracy comparison of multiple hypotheses-based classification across five datasets of three baselines: accuracy (mean ± standard deviation) with (w/) and without (w/o) demonstrations.The better overall average between (w/) and (w/o) is highlighted in bold.
IO-PromptingIterative-RefinementHypoGeniCDatasetw/o demos w/ demos w/o demos w/ demos w/o demos w/ demosHallucination 62.2 Overall Average 66.3265.7467.9068.5066.1865.18CorrectFlippedRandomOnly PositiveOnly NegativeAccuracy Difference-5.0% -2.5% 0.0% 2.5% 5.0%Hallucination Unhealthy Comments Funny RedditTruthful Review PneumoniaMNISTAverage</p>
<p>Table 4 :
4
Helpfulness 4.00 ± 0.000 3.96 ± 0.195 4.00 ± 0.000 3.80 ± 0.400 4.04 ± 0.195 4.08 ± 0.271 4.01 3.95 Novelty 2.56 ± 0.571 2.40 ± 0.566 2.60 ± 0.693 2.60 ± 0.748 2.84 ± 0.674 2.36 ± 0.741 2.67 2.45 LLM-based Scoring: Comparison of Helpfulness and Novelty scores across three baselines, with and without demonstrations (w/ demos vs. w/o demos).The better overall average between (w/) and (w/o) is highlighted in bold.
IO-PromptingIterative-RefinementHypoGeniCOverall AverageCriteriaw/ow/w/ow/w/ow/w/ow/</p>
<p>Table 5 :
5
Accuracy comparison of different label formats in correct and flipped label settings with IO-prompting.Each number is the average over five datasets.
FormatCorrect LabelFlipped LabelBest Average Best AverageLabel Format1 68.5662.7265.1559.96Label Format2 67.8862.7867.4961.90w/o demosBest: 68.62Average: 62.120.1 0.2 0.3 0.4 0.5 Metric ValuesACR BCRHallucination Comments 0.0RedditReview PneumoniaFigure 5: Difference of predictions between correctlabel and flipped label demos: Adverse Correction Rate(ACR) and Beneficial Correction Rate (BCR) valuesunder multiple hypotheses-based classification.datasets, as the other datasets require more special-ized expertise. The results are illustrated in Figure4. Across the three datasets, hypotheses generatedwithout demonstrations received the highest per-centage of preference. These findings indicate aslight overall preference for hypotheses generatedusing only the model's prior, though the extent ofthis preference varies by dataset.6 Analysis6.1 Is the result consistent with differentin-context demonstration label formats?</p>
<p>Table 6 :
6
Dataset Split for Train, Validation, and Test Sets.
DatasetTrain Validation TestHallucination400100374Pneumonia MNIST800270468Unhealthy Conversation800400800Funny Reddit200100308Truthful Hotel Review800300500</p>
<p>Table 7 :
7
Demos Hallucination Unhealthy Comments Funny Reddit Truthful Review PneumoniaMNIST Overall Average Accuraccy comparison of single hypothesis-based classification without task-specific knowledge in inference: accuracy for the single hypothesis and the average across five hypotheses, with (w/) and without (w/o) demonstrations.
Bestw/o w/63.1 57.570.1 68.061.6 59.164.0 64.675.6 80.866.9 66.0Averagew/o w/54.4 53.660.3 63.354.1 54.856.7 51.869.8 73.159.1 59.3LabelHallucination Unhealthy Comments Funny Reddit Truthful Review PneumoniaMNIST AverageCorrect (Best)63.970.661.768.075.267.9Flipped (Best)61.271.562.068.873.967.5Correct (Avg)57.065.159.062.570.362.8Flipped (Avg)57.864.757.361.368.561.9</p>
<p>Table 8 :
8
Accuracy comparison across five datasets with correct and flipped label settings in the Label Format 2.
Demos Hallucination Unhealthy Comments Funny Reddit Truthful Review PneumoniaMNIST Overall AverageBestw/o w/--67.9 ± 0.2 67.8 ± 1.362.7 ± 0.3 65.9 ± 0.368.8 ± 2.0 66.9 ± 1.758.2 ± 1.8 55.7 ± 1.464.4 64.1Averagew/o w/--61.9 ± 0.3 62.4 ± 1.156.8 ± 0.0 58.0 ± 1.264.5 ± 2.3 63.4 ± 1.253.1 ± 0.2 53.0 ± 1.559.1 59.2</p>
<p>Table 10 :
10
Examples of Generated Hypotheses with and without In-Context Demonstrations.</p>
<p>Using the given examples, please propose {{num_hypotheses}} possible hypotheses that can identify specific patterns that occur across the provided image-question pairs.Each hypothesis should contain the following: a hypothesis about what image content features, object features, or contextual relationships make the model more likely to hallucinate.The hypotheses should analyze what kinds of image-question pairs are more likely to trigger hallucinations.Some examples of hallucination and non-hallucination cases are shown.Hallucination cases are from number 1 to {{num_1}}, and non-hallucination cases are from number {{num_2}} to {{num_3}}.Based on provided examples, please generate hypotheses that are useful for predicting whether the model will hallucinate the existence of an object in response to a given question.Propose {{num_hypotheses}} possible hypotheses for hallucination patterns.</p>
<p>Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}.[hypothesis].Proposed hypotheses: Prompt for hallucination without demonstrations</p>
<p>Each hypothesis should contain the following: a hypothesis about what image content features, object features, or contextual relationships make the model more likely to hallucinate.You're an expert comment analyst in online conversation.Given a set of comments, we want to generate hypotheses that are useful for predicting whether a comment is unhealthy.In other words, we want to know if the comment contributes to unhealthy conversations online.Using the given examples, please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across the provided unhealthy comments.Each hypothesis should contain the following: A hypothesis about what makes comments more likely to be unhealthy.The hypotheses should analyze what kind of comments are likely to be unhealthy.Here are some examples of unhealthy and healthy comments: Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful.In other words, we want to know whether the review is written by someone who actually lived in the hotel.Using the given examples, please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across the provided reviews.Each hypothesis should contain the following: A hypothesis about what makes reviews more likely to be truthful.The hypotheses should analyze what kind of reviews are likely to be truthful.Here are some examples of truthful and deceptive reviews:You're a professional radiologist specializing in chest X-rays.Given a set of labeled chest X-ray images, we want to generate hypotheses that are useful for predicting whether a patient has pneumonia.In other words, we want to know whether the X-ray shows signs of pneumonia.Using the given examples, please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across the provided X-ray images.Each hypothesis should contain the following: A hypothesis about what makes an X-ray more likely to indicate pneumonia.The hypotheses should analyze what kind of image patterns are likely to be indicative of pneumonia or not.You're a professional humor analyst for Reddit posts.Given a set of Reddit posts, we want to generate hypotheses that are useful for predicting whether a post is considered funny or not.In other words, we want to know whether a post contains humor patterns often associated with successful humorous posts.Using the provided examples, please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across the provided posts.Each hypothesis should contain the following: A hypothesis about what makes posts more likely to be considered funny.The hypotheses should analyze what kind of posts are likely to be perceived as funny or not.Here are some examples of funny and unfunny posts:
Prompt for unhealthy comments with demonstrations Prompt for truthful reviews with demonstrations Prompt for PneumoniaMNIST with demonstrations Prompt for funny reddit with demonstrations Evaluation prompt for hallucinationYou're a professional hotel review analyst.Unhealthy comments: {{positive_examples}} Healthy comments: {{negative_examples}} Some examples of X-ray images labeled as pneumonia and non-pneumonia are shown. Truthful reviews: Pneumonia images are from number 1 to {{num_1}}, and non-pneumonia images are from Funny posts: {{positive_examples}} number {{num_2}} to {{num_3}}. {{positive_examples}} Deceptive reviews: Unfunny posts: {{negative_examples}} Based on provided examples, please generate hypotheses that are useful for predicting whether an {{negative_examples}}Based on the provided examples, please generate hypotheses that are useful for predicting whether a comment is unhealthy. Propose {{num_hypotheses}} possible hypotheses for unhealthy comment patterns. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. Don't include any other words. Proposed hypotheses: X-ray shows pneumonia or not. Based on provided examples, please generate hypotheses that are useful for predicting whether a Propose {{num_hypotheses}} possible hypotheses for pneumonia pattern recognition. Based on the provided examples, please generate hypotheses that are useful for predicting whether review is truthful. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-a post is funny or not. Propose {{num_hypotheses}} possible hypotheses for truthful review patterns. esis]. Propose {{num_hypotheses}} possible hypotheses for funny post patterns. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-Don't include any other information. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. Proposed hypotheses: esis]. Don't talk about any other words. Don't talk about any other words. Proposed hypotheses: Proposed hypotheses:Prompt for PneumoniaMNIST without demonstrationsThe hypotheses should analyze what kind of image-question pairs are more likely to lead to hallucinations. Please generate {{num_hypotheses}} possible hypotheses for hallucination patterns in the given context. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. Don't talk about any other words. Proposed hypotheses: Prompt for truthful reviews with demonstrations Prompt for funny reddit without demonstrations You're a professional radiologist. Prompt for unhealthy comments without demonstrations You're an expert comment analyst in online conversation. We want to generate hypotheses that are useful for predicting whether a comment is unhealthy. In other words, we want to know if the comment contributes to unhealthy conversations online. Please propose {{num_hypotheses}} possible hypotheses. These hypotheses should identify specific patterns that occur across common unhealthy comments. Each hypothesis should contain the following: A hypothesis about what makes comments more likely to be unhealthy. The hypotheses should analyze what kind of comments are likely to be unhealthy. Please generate hypotheses that are useful for predicting whether a comment is unhealthy or healthy. Propose {{num_hypotheses}} possible hypotheses for unhealthy comment patterns. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. Don't talk about any other words. Proposed hypotheses: We want to generate hypotheses that are useful for predicting whether a patient has pneumonia You're a professional Reddit content analyst. You're a professional hotel review analyst. based on their chest X-ray image. In other words, we want to know which patterns in the image We want to generate hypotheses that are useful for predicting whether a Reddit post is funny or not. We want to generate hypotheses that are useful for predicting whether a review is truthful or are indicative of pneumonia presence. In other words, we want to know what characteristics make a post likely to be perceived as humor-deceptive. In other words, we want to know whether the review is written by someone who actually Please propose {{num_hypotheses}} possible hypotheses. ous by the community. lived in the hotel. These hypotheses should identify specific visual patterns that occur in typical pneumonia cases. Please propose {{num_hypotheses}} possible hypotheses. Please propose {{num_hypotheses}} possible hypotheses. Each hypothesis should contain the following: A hypothesis about what makes an image more These hypotheses should identify specific patterns that occur across common funny posts. These hypotheses should identify specific patterns that occur across common truthful reviews. likely to show signs of pneumonia. Each hypothesis should contain the following: A hypothesis about what makes posts more likely Each hypothesis should contain the following: A hypothesis about what makes reviews more likely The hypotheses should analyze what kind of visual patterns or markers are likely to indicate to be perceived as funny. to be truthful. The hypotheses should analyze what kind of reviews are likely to be truthful or pneumonia. The hypotheses should analyze what kind of posts are likely to be considered humorous or non-deceptive. Please generate hypotheses that are useful for predicting whether a patient has pneumonia or not humorous. Please generate hypotheses that are useful for predicting whether a review is truthful or deceptive. based on the X-ray. Please generate hypotheses that are useful for predicting whether a post is funny or not. Propose {{num_hypotheses}} possible hypotheses for truthful review patterns. Propose {{num_hypotheses}} possible hypotheses for pneumonia-related visual patterns. Propose {{num_hypotheses}} possible hypotheses for funny Reddit post patterns. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. esis]. esis]. Don't talk about any other words. Don't include any additional context. Don't talk about any other words. Proposed hypotheses: Proposed hypotheses: Proposed hypotheses:</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Kewei Cheng, Jingfeng Yang, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng Li, Yifan Gao, Xian Li, arXiv:2408.00114Inductive or deductive? rethinking the fundamental reasoning abilities of llms. 2024arXiv preprint</p>
<p>Describing differences in image sets with natural language. Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E Gonzalez, Serena Yeung-Levy, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Properties of inductive reasoning. Evan Heit, Psychonomic bulletin &amp; review. 72000</p>
<p>Instruction induction: From few examples to natural language task descriptions. Or Honovich, Uri Shaham, Omer Samuel R Bowman, Levy, arXiv:2205.107822022arXiv preprint</p>
<p>Discovering and mitigating visual biases through keyword explanation. Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Mirage: Evaluating and explaining inductive reasoning process in language models. Jiachun Li, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao, arXiv:2410.095422024arXiv preprint</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023arXiv preprint</p>
<p>Literature meets data: A synergistic approach to hypothesis generation. Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan, arXiv:2410.173092024arXiv preprint</p>
<p>Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Sanchaita Agarwal, Ashish Hazra, Peter Sabharwal, Clark, arXiv:2402.13610Data-driven discovery with large generative models. 2024aarXiv preprint</p>
<p>Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.01725Discoverybench: Towards data-driven discovery with large language models. 2024barXiv preprint</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>Minh Chau, Alexander Pham, Simeng Hoyle, Philip Sun, Mohit Resnik, Iyyer, arXiv:2311.01449Topicgpt: A promptbased topic modeling framework. 2023arXiv preprint</p>
<p>Ilan Price, Jordan Gifford-Moore, Jory Fleming, Saul Musker, Maayan Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon, Jeffrey Sorensen, arXiv:2010.07410Six attributes of unhealthy conversation. 2020arXiv preprint</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023arXiv preprint</p>
<p>Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, arXiv:2310.08559Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. 2023arXiv preprint</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy, arXiv:2404.184002024arXiv preprint</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. 2024arXiv preprint</p>
<p>iprompt: Explaining data patterns in natural language via interpretable autoprompting. Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, Jianfeng Gao, 20222210ArXiv preprint</p>
<p>Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D Goodman, arXiv:2309.05660Hypothesis search: Inductive reasoning with language models. 2023arXiv preprint</p>
<p>Yurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming Ding, Linjun Yang, arXiv:2410.08601Strago: Harnessing strategic guidance for prompt optimization. 2024arXiv preprint</p>
<p>Verbalized machine learning: Revisiting machine learning with language models. Robert Tim Z Xiao, Bernhard Bamler, Weiyang Schölkopf, Liu, arXiv:2406.043442024arXiv preprint</p>
<p>Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni, Scientific Data. 101412023a</p>
<p>Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, arXiv:2212.10923Language models as inductive reasoners. 2022arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, arXiv:2309.027262023barXiv preprintSoujanya Poria, and Erik Cambria</p>
<p>Describing differences between text distributions with natural language. Ruiqi Zhong, Charlie Snell, Dan Klein, Jacob Steinhardt, International Conference on Machine Learning. PMLR2022</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Advances in Neural Information Processing Systems. 202336</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>
<p>Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai, arXiv:2310.07064Large language models can learn rules. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
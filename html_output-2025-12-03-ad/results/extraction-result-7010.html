<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7010 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7010</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7010</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-266999234</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.07105v3.pdf" target="_blank">Graph Language Models</a></p>
                <p><strong>Paper Abstract:</strong> While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7010.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7010.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extended Levi Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extended Levi Graph (tokenized Levi graph with token-level nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text representation used by the GLM where each original graph edge is replaced by a relation-node (Levi graph), every node is tokenized into single-token nodes, and directed edges connect adjacent tokens to preserve original graph structure while producing a token sequence compatible with LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Extended Levi Graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert the Graph-of-Triplets into a Levi graph (replace every edge by a relation-node connected to head and tail), then tokenize each node and split multi-token node text into multiple nodes so that every resulting node corresponds to a single LM token; connect tokens of the same original node with directed edges preserving order and connect relation-node tokens to the corresponding entity token nodes. The GLM consumes the full set of token nodes (linearized into a sequence only for batching) and uses a graph-relative positional matrix P and mask M to encode structural relations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based; sequential (for input batching) but graph-structured (loss-preserving of connectivity)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Levi-graph construction followed by node-level tokenization (i.e., edge->node conversion + token splitting); internal representation uses explicit graph adjacency and relative-position matrices (P and M) rather than a traversal order.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ConceptNet (subgraphs) and Wikidata (with Wikipedia abstracts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Relation (label) classification; KG population / text‑guided relation classification and source prediction</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Language Model (GLM) — ℓGLM (local) and gGLM (global), initialized from T5 (T5-small / T5-large variants used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph-transformer architecture derived from a pretrained T5 encoder: ℓGLM restricts attention to tokens within the same triplet (local masked attention); gGLM allows global attention with G2G/T2G/G2T relative-position biases initialized from the +∞ bias of T5. Experiments used T5-small for ConceptNet experiments and T5-large for Wikidata experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (ConceptNet experiments); macro F1 (Wikidata experiments); also linear-probing accuracy and finetuning accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ConceptNet (relation classification): GLM variants achieved substantially higher linear-probing accuracy in complex/masked settings and up to approx. 72–75% accuracy for large GLM models on ConceptNet (see Table 6 max ~74.8% gGLM large). Wikidata (macro F1): after 524,288 train instances ℓGLM 82.35 / gGLM 81.98 macro F1; after 2,449,582 instances ℓGLM 85.06 / gGLM 85.28 macro F1 (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables strong zero-shot / linear-probing embeddings (i.e., useful graph-aware embeddings without GLM finetuning); supports joint text+graph encoding so models can learn to use graph context quickly (faster effective use of graph context in few-shot / limited-data regimes). Under finetuning, GLMs improve downstream relation-classification performance compared to GNN and randomly-initialized GT baselines, particularly when graph reasoning (masked subgraphs / long-range context) is required.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Introduces preprocessing complexity (Levi conversion + token-splitting); increased bookkeeping for relative-position (P) and mask (M) matrices; ℓGLM (local) can be limited on tasks needing immediate long-range connections unless stacked layers propagate information; gGLM requires careful initialization of G2G/T2G/G2T biases (authors initialize from +∞) and may increase effective attention over large graphs if pretrained biases are not appropriate. The authors also note more tokens / altered tokenization (no end-of-sentence token, removal of whitespace between Levi nodes) which can affect tokenization idiosyncratically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to simple sequence linearization (triplet concatenation), the Extended Levi Graph preserves graph structure explicitly via P and M matrices and yields much stronger linear-probing performance on ConceptNet and better performance in masked/long-range reasoning tasks; compared to GNNs the GLM better captures textual semantics due to inheritance from a pretrained LM; compared to graph transformers trained from scratch, GLMs initialized from T5 significantly outperform (showing the importance of pretrained LM weights). On Wikidata interleaved text+graph tasks LM baselines remain competitive for many settings, but GLMs learned to exploit graphs faster and improved source prediction when graph input is available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7010.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7010.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triplet Concatenation (T5 list / T5 set)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triplet concatenation linearization (alphabetical 'list' and shuffled 'set' variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple graph-to-text baseline that verbalizes each triple and concatenates verbalized triplets into a single token sequence provided to a standard LM encoder; used in two ordering variants: deterministic alphabetical ordering (T5 list) and random shuffling per epoch (T5 set).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triplet concatenation (verbalized triple sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph triplet is converted into a short textual template (verbalization) and triplet texts are concatenated into one sequence. Two ordering policies: 'T5 list' uses an alphabetical deterministic order; 'T5 set' uses random shuffles of triplets each epoch to encourage order robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; lossy (implicit graph structure only)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge-list / triplet-list serialization (concatenate verbalized (head, relation, tail) strings); ordering either alphabetical (canonical) or randomized (noncanonical per epoch).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ConceptNet (subgraphs) and Wikidata (with Wikipedia abstracts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Relation classification; KG population / text‑guided relation classification and source prediction</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 encoder (T5-small for ConceptNet baselines; T5-large for Wikidata baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard pretrained T5 encoders used as LM baselines. Two variants: T5 (list) deterministic ordering; T5 (set) random ordering per epoch.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (ConceptNet); macro F1 (Wikidata); linear-probing vs finetuning comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Wikidata macro F1 (Table 8): after 524,288 instances T5 list 81.45 / T5 set 81.29; after 2,449,582 instances T5 list 85.36 / T5 set 85.04. On ConceptNet some GLM variants outperform LM baselines in masked/large-radius settings, but on small/medium graphs LM baselines are competitive (large T5 sometimes reaches up to ~73–75% on ConceptNet; see Table 6 where T5 list large reaches ~74.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple to construct and compatible with any LM; competitive when graphs are small or when textual evidence dominates. Random shuffling (T5 set) helps robustness to order during training. However, because graph structure is not encoded explicitly, LMs must implicitly learn structural relationships during training and may require more labeled examples to learn long‑range graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy representation — structural graph information (distances, connectivity, shared nodes) must be inferred by the LM from textual patterns; sensitive to ordering heuristics; inefficient for large graphs (token truncation observed more often compared to GLM due to less compact encoding and no structural compression). Fails to benefit from inductive graph priors and underperforms GLMs in complex masking / long-range reasoning scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to the Extended Levi Graph + GLM, triplet concatenation is easier to use but does not provide explicit graph inductive biases; GLMs generally outperform concatenation on ConceptNet linear-probing experiments and on tasks where long-range graph context or masked-structure reasoning is important, while concatenation remains competitive on small graphs and text-dominant tasks (Wikidata relation classification).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7010.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7010.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shortest-path / path-based linearization (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shortest-path based linearization / linearized subgraphs (path-based encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of prior graph-to-text encodings that linearize a subgraph by traversing graph paths (e.g., shortest-path ordering) and then representing the traversed nodes/edges as a token sequence; referenced as problematic for LM compatibility in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling graph structure via relative position for text generation from knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Shortest-path / path-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute distances or shortest paths between token nodes and use those paths to define a traversal order / relative positioning; representations use path-derived relative distances to construct positional encodings or to order tokens for LM input.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>path-based; sequential; potentially lossy for full graph structure (unless entire graph is traversed)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Shortest-path traversal or extraction of paths/subgraphs (e.g., convert to sequences by following shortest paths between target nodes), and use the path order as the token sequence or to derive graph-relative positional encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mentioned for KG-to-text and graph-to-text generation in related work</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>When used in prior LM-based graph generation work, path/subgraph linearizations can be consumed by seq2seq LMs; however, authors of this paper argue such encodings produce relative positions unnatural to pretrained LMs (e.g., reversed triplet orders when traversed in different directions) and therefore are less compatible with LM pretraining biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generates positional encodings that can be unnatural for LMs (triplets may appear reversed depending on traversal), which breaks compatibility with language model pretraining patterns; authors therefore avoid using shortest-path-based PE for token pairs from different triplets and instead opted for local PE or initialized +∞ biases for global connections in GLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to the Extended Levi Graph + GLM approach, shortest-path linearization can be less compatible with pretrained LM biases and may force the LM to learn unnatural token orders; the paper argues that their token-level Levi representation and graph-relative P/M matrices better preserve LM-friendly local triplet order while still enabling global graph reasoning (via gGLM initialization).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Modeling graph structure via relative position for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Fewshot Knowledge Graph-to-Text Generation with Pretrained Language Models <em>(Rating: 2)</em></li>
                <li>Unifying Structured Data as Graph for Data-to-Text Pre-Training <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7010",
    "paper_id": "paper-266999234",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Extended Levi Graph",
            "name_full": "Extended Levi Graph (tokenized Levi graph with token-level nodes)",
            "brief_description": "A graph-to-text representation used by the GLM where each original graph edge is replaced by a relation-node (Levi graph), every node is tokenized into single-token nodes, and directed edges connect adjacent tokens to preserve original graph structure while producing a token sequence compatible with LMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Extended Levi Graph",
            "representation_description": "Convert the Graph-of-Triplets into a Levi graph (replace every edge by a relation-node connected to head and tail), then tokenize each node and split multi-token node text into multiple nodes so that every resulting node corresponds to a single LM token; connect tokens of the same original node with directed edges preserving order and connect relation-node tokens to the corresponding entity token nodes. The GLM consumes the full set of token nodes (linearized into a sequence only for batching) and uses a graph-relative positional matrix P and mask M to encode structural relations.",
            "representation_type": "token-based; sequential (for input batching) but graph-structured (loss-preserving of connectivity)",
            "encoding_method": "Levi-graph construction followed by node-level tokenization (i.e., edge-&gt;node conversion + token splitting); internal representation uses explicit graph adjacency and relative-position matrices (P and M) rather than a traversal order.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "ConceptNet (subgraphs) and Wikidata (with Wikipedia abstracts)",
            "task_name": "Relation (label) classification; KG population / text‑guided relation classification and source prediction",
            "model_name": "Graph Language Model (GLM) — ℓGLM (local) and gGLM (global), initialized from T5 (T5-small / T5-large variants used)",
            "model_description": "Graph-transformer architecture derived from a pretrained T5 encoder: ℓGLM restricts attention to tokens within the same triplet (local masked attention); gGLM allows global attention with G2G/T2G/G2T relative-position biases initialized from the +∞ bias of T5. Experiments used T5-small for ConceptNet experiments and T5-large for Wikidata experiments.",
            "performance_metric": "Accuracy (ConceptNet experiments); macro F1 (Wikidata experiments); also linear-probing accuracy and finetuning accuracy reported.",
            "performance_value": "ConceptNet (relation classification): GLM variants achieved substantially higher linear-probing accuracy in complex/masked settings and up to approx. 72–75% accuracy for large GLM models on ConceptNet (see Table 6 max ~74.8% gGLM large). Wikidata (macro F1): after 524,288 train instances ℓGLM 82.35 / gGLM 81.98 macro F1; after 2,449,582 instances ℓGLM 85.06 / gGLM 85.28 macro F1 (Table 8).",
            "impact_on_training": "Enables strong zero-shot / linear-probing embeddings (i.e., useful graph-aware embeddings without GLM finetuning); supports joint text+graph encoding so models can learn to use graph context quickly (faster effective use of graph context in few-shot / limited-data regimes). Under finetuning, GLMs improve downstream relation-classification performance compared to GNN and randomly-initialized GT baselines, particularly when graph reasoning (masked subgraphs / long-range context) is required.",
            "limitations": "Introduces preprocessing complexity (Levi conversion + token-splitting); increased bookkeeping for relative-position (P) and mask (M) matrices; ℓGLM (local) can be limited on tasks needing immediate long-range connections unless stacked layers propagate information; gGLM requires careful initialization of G2G/T2G/G2T biases (authors initialize from +∞) and may increase effective attention over large graphs if pretrained biases are not appropriate. The authors also note more tokens / altered tokenization (no end-of-sentence token, removal of whitespace between Levi nodes) which can affect tokenization idiosyncratically.",
            "comparison_with_other": "Compared to simple sequence linearization (triplet concatenation), the Extended Levi Graph preserves graph structure explicitly via P and M matrices and yields much stronger linear-probing performance on ConceptNet and better performance in masked/long-range reasoning tasks; compared to GNNs the GLM better captures textual semantics due to inheritance from a pretrained LM; compared to graph transformers trained from scratch, GLMs initialized from T5 significantly outperform (showing the importance of pretrained LM weights). On Wikidata interleaved text+graph tasks LM baselines remain competitive for many settings, but GLMs learned to exploit graphs faster and improved source prediction when graph input is available.",
            "uuid": "e7010.0",
            "source_info": {
                "paper_title": "Graph Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Triplet Concatenation (T5 list / T5 set)",
            "name_full": "Triplet concatenation linearization (alphabetical 'list' and shuffled 'set' variants)",
            "brief_description": "A simple graph-to-text baseline that verbalizes each triple and concatenates verbalized triplets into a single token sequence provided to a standard LM encoder; used in two ordering variants: deterministic alphabetical ordering (T5 list) and random shuffling per epoch (T5 set).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Triplet concatenation (verbalized triple sequence)",
            "representation_description": "Each graph triplet is converted into a short textual template (verbalization) and triplet texts are concatenated into one sequence. Two ordering policies: 'T5 list' uses an alphabetical deterministic order; 'T5 set' uses random shuffles of triplets each epoch to encourage order robustness.",
            "representation_type": "sequential; lossy (implicit graph structure only)",
            "encoding_method": "Edge-list / triplet-list serialization (concatenate verbalized (head, relation, tail) strings); ordering either alphabetical (canonical) or randomized (noncanonical per epoch).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ConceptNet (subgraphs) and Wikidata (with Wikipedia abstracts)",
            "task_name": "Relation classification; KG population / text‑guided relation classification and source prediction",
            "model_name": "T5 encoder (T5-small for ConceptNet baselines; T5-large for Wikidata baselines)",
            "model_description": "Standard pretrained T5 encoders used as LM baselines. Two variants: T5 (list) deterministic ordering; T5 (set) random ordering per epoch.",
            "performance_metric": "Accuracy (ConceptNet); macro F1 (Wikidata); linear-probing vs finetuning comparisons",
            "performance_value": "Wikidata macro F1 (Table 8): after 524,288 instances T5 list 81.45 / T5 set 81.29; after 2,449,582 instances T5 list 85.36 / T5 set 85.04. On ConceptNet some GLM variants outperform LM baselines in masked/large-radius settings, but on small/medium graphs LM baselines are competitive (large T5 sometimes reaches up to ~73–75% on ConceptNet; see Table 6 where T5 list large reaches ~74.5%).",
            "impact_on_training": "Simple to construct and compatible with any LM; competitive when graphs are small or when textual evidence dominates. Random shuffling (T5 set) helps robustness to order during training. However, because graph structure is not encoded explicitly, LMs must implicitly learn structural relationships during training and may require more labeled examples to learn long‑range graph reasoning.",
            "limitations": "Lossy representation — structural graph information (distances, connectivity, shared nodes) must be inferred by the LM from textual patterns; sensitive to ordering heuristics; inefficient for large graphs (token truncation observed more often compared to GLM due to less compact encoding and no structural compression). Fails to benefit from inductive graph priors and underperforms GLMs in complex masking / long-range reasoning scenarios.",
            "comparison_with_other": "Compared to the Extended Levi Graph + GLM, triplet concatenation is easier to use but does not provide explicit graph inductive biases; GLMs generally outperform concatenation on ConceptNet linear-probing experiments and on tasks where long-range graph context or masked-structure reasoning is important, while concatenation remains competitive on small graphs and text-dominant tasks (Wikidata relation classification).",
            "uuid": "e7010.1",
            "source_info": {
                "paper_title": "Graph Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Shortest-path / path-based linearization (prior work)",
            "name_full": "Shortest-path based linearization / linearized subgraphs (path-based encodings)",
            "brief_description": "A class of prior graph-to-text encodings that linearize a subgraph by traversing graph paths (e.g., shortest-path ordering) and then representing the traversed nodes/edges as a token sequence; referenced as problematic for LM compatibility in this paper.",
            "citation_title": "Modeling graph structure via relative position for text generation from knowledge graphs",
            "mention_or_use": "mention",
            "representation_name": "Shortest-path / path-based linearization",
            "representation_description": "Compute distances or shortest paths between token nodes and use those paths to define a traversal order / relative positioning; representations use path-derived relative distances to construct positional encodings or to order tokens for LM input.",
            "representation_type": "path-based; sequential; potentially lossy for full graph structure (unless entire graph is traversed)",
            "encoding_method": "Shortest-path traversal or extraction of paths/subgraphs (e.g., convert to sequences by following shortest paths between target nodes), and use the path order as the token sequence or to derive graph-relative positional encodings.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Mentioned for KG-to-text and graph-to-text generation in related work",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "When used in prior LM-based graph generation work, path/subgraph linearizations can be consumed by seq2seq LMs; however, authors of this paper argue such encodings produce relative positions unnatural to pretrained LMs (e.g., reversed triplet orders when traversed in different directions) and therefore are less compatible with LM pretraining biases.",
            "limitations": "Generates positional encodings that can be unnatural for LMs (triplets may appear reversed depending on traversal), which breaks compatibility with language model pretraining patterns; authors therefore avoid using shortest-path-based PE for token pairs from different triplets and instead opted for local PE or initialized +∞ biases for global connections in GLM.",
            "comparison_with_other": "Compared to the Extended Levi Graph + GLM approach, shortest-path linearization can be less compatible with pretrained LM biases and may force the LM to learn unnatural token orders; the paper argues that their token-level Levi representation and graph-relative P/M matrices better preserve LM-friendly local triplet order while still enabling global graph reasoning (via gGLM initialization).",
            "uuid": "e7010.2",
            "source_info": {
                "paper_title": "Graph Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Modeling graph structure via relative position for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_via_relative_position_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Fewshot Knowledge Graph-to-Text Generation with Pretrained Language Models",
            "rating": 2,
            "sanitized_title": "fewshot_knowledge_graphtotext_generation_with_pretrained_language_models"
        },
        {
            "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
            "rating": 1,
            "sanitized_title": "unifying_structured_data_as_graph_for_datatotext_pretraining"
        }
    ],
    "cost": 0.0154205,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph Language Models
3 Jun 2024</p>
<p>Moritz Plenz plenz@cl.uni-heidelberg.de 
Computational Linguistics Heidelberg University</p>
<p>Anette Frank frank@cl.uni-heidelberg.de 
Computational Linguistics Heidelberg University</p>
<p>Graph Language Models
3 Jun 2024FD34E204A8B21E7BBF8FE55B65465033arXiv:2401.07105v3[cs.CL]
While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched.Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structurebut GNNs cannot represent text features as well as pretrained LMs.In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses.The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets.Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph.This enables GLMs to process graphs, texts, and interleaved inputs of both.Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM-and GNN-based baselines in supervised and zeroshot setting, demonstrating their versatility. 1</p>
<p>Introduction</p>
<p>Knowledge Graphs (KGs) are essential for organizing vast data, to facilitate information retrieval, or revealing hidden insights for decisionmaking (Plenz et al., 2024).KGs excel in explicitly representing manifold relationships, so with an expanding wealth of information they become crucial tools in the digital age.</p>
<p>Many KGs consist of knowledge triplets, where nodes are entities and edges represent relationships holding between them.Each triplet represents a fact in pseudo-natural language, e.g., (Thailand; Capital; Bangkok) in DBpedia (Auer et al., 2007).Despite the (usual) simplicity of each individual 1 https://github.com/Heidelberg-NLP/GraphLanguageModels Figure 1: The GLM inherits its architecture from a Graph Transformer, and its parameters from a LM.This enables it to jointly reason over graphs and language.</p>
<p>triplet, complex structures emerge in KGs.We refer to such KGs as Graphs of Triplets (GoTs).</p>
<p>To use GoTs effectively, we need meaningful encodings of their components.A natural choice is leveraging LMs, as they can capture the semantics of textually encoded entities, relations or entire triplets.But LMs are not prepared to capture graphstructured information and cannot model complex interactions in a GoT.To alleviate this problem, one can leverage graph NNs (GNNs).But GNNs are not well suited to capture meanings associated with text, and hence often LMs are used to convert nodes (and possibly edges) to language-based semantic embeddings.But in such settings, semantic encoding leveraged from LMs and structural reasoning performed by GNNs are separated and are driven by distinct underlying principles.We expect this to limit model performance if both textual and structural information are important for a task.</p>
<p>In this work we introduce a Graph Language Model (GLM) that resolves this tension through early fusion of textual and structural information.Most LMs today are transformers.Since transformers operate on sets, Positional Encoding (PE) is used to inform them about the inherent sequential ordering of linguistic inputs.In our GLM formulation, we modify PE and self-attention to convert LMs (i.e., sequence transformers) to graph transformers that natively operate on graphs, while preserving their LM capabilities.Usually, a new architecture requires pretraining from scratch, which is extremely costly.By adopting some non-invasive changes in the LM's self-attention module, we transform the LM to a Graph Transformer (GT)while maintaining compatibility with its pretrained LM parameters.When encoding a graph, LM-like attention patterns process linearly organized textual information of individual triplets, while GT-like attention patterns aggregate information along the graph structure.Hence, the GLM inherits text understanding of triplets from the LM, while its GT architecture allows it to directly perform structural reasoning, without additional GNN layers.</p>
<p>Importantly, for text sequences -which can be seen as a special type of graph -the GLM is identical to the original LM.This allows the GLM to process interleaved inputs of text and GoT jointly, handling both modalities in a single framework.</p>
<p>Our main contributions are: (i) We propose Graph Language Models (GLMs) and a theoretical framework to construct them.GLMs are graph transformers, which enables graph reasoning.Simultaneously, they inherit and exploit LM weights, enabling them to represent and contextualize triplets in a GoT.Further, by encoding texts and graph components alike, it can naturally take graph and text data as interleaved inputs.(ii) Experiments on relation classification in ConceptNet subgraphs show that GLMs outperform LM-and GNN-based methods for encoding GoTs -even when the inherited LM parameters are not updated during GLM training.(iii) KG population experiments on Wikidata subgraphs and corresponding Wikipedia abstracts show that GLMs can reason over interleaved inputs of GoTs and text -again, outperforming strong LM-based methods.</p>
<p>Related Work</p>
<p>LMs One way to augment LMs with knowledge from KGs (Pan et al., 2024) is to formulate pretraining objectives that operate on a KG.E.g., LMs can be trained to generate parts of a KG, encouraging the LM to store KG content in its parameters.Typically, single triplets are used for pretraining (Bosselut et al., 2019;Wang et al., 2021;Hwang et al., 2021;West et al., 2023).In such cases, the graph structure is not a target of pretraining.Some works generate larger substructures, such as paths or linearized subgraphs (Wang et al., 2020a;Schmitt et al., 2020;Huguet Cabot and Navigli, 2021).In either case, the LM needs to memorize the KG, as it will not be part of the input during inference.</p>
<p>Another approach is to provide the linearized KG as part of the input during inference.This is common for KG-to-text generation (Schmitt et al., 2020;Ribeiro et al., 2021;Li et al., 2021), where models learn to take the linearized (typically small-sized) graphs as input.A recent trend is retrieval augmented generation, where relevant parts of a knowledge base, or KG, are retrieved, linearized and provided as part of a prompt (Gao et al., 2024). 2n both options the graph must be linearized to fit the input or output of a sequence-to-sequence LM.Hence, no graph priors can be enforced -instead, the LM has to learn the graph structure implicitly.By contrast, GLMs model a graph as a true graph and have inductive graph priors instilled in their architecture.This prepares a GLM for more proficient graph reasoning, compared to a LM approach.</p>
<p>GNNs LMs excel at representing single triplets, but struggle with structural reasoning.To alleviate this problem, LMs can be combined with GNNs.Many approaches get node and edge features from LMs and aggregate this information in the graph with GNNs (Lin et al., 2019;Malaviya et al., 2020;Zhao et al., 2023).Zhang et al. (2022); Yasunaga et al. (2022) train models consisting of a LM and a GNN that encode interleaved text and graph inputs jointly.They also use a LM to obtain node features.</p>
<p>While some approaches jointly train for textual understanding and graph reasoning, none offer a unified method.By contrast, our GLM formulation seamlessly integrates both in a holistic framework for embedding language and KGs.</p>
<p>Graph Transformers GTs, a special type of GNN (Bronstein et al., 2021), gain popularity in NLP and beyond (Min et al., 2022;Müller et al., 2023).E.g., Koncel-Kedziorski et al. (2019) and Wang et al. (2020b) train GTs to generate text from KGs and AMRs, respectively.Most relevant to our work is Schmitt et al. (2021), who use GTs for KGto-text generation.Similar to us, they employ PE matrices, but train their model from scratch, which limits its applicability: while their model trained on WebNLG (Gardent et al., 2017) has a vocabulary size of 2,100, initializing a GLM from T5, equips it with T5's full vocabulary of 32,128 tokens.</p>
<p>Concurrently, and independently from our work, Li et al. (2024) also convert a LM to a graph transformer.They focus on data-to-text generation, where they unify table, key-value and KG structures in a unified graph format, and apply structureenhanced pre-training to support data-to-text generation with their structure-enhanced transformer model.They apply attention maps similar to ours to better capture the graph-structured input, which the pre-trained model rewrites into natural language.Contrary to their work, we do not resort to structureenhanced pre-training -which is restricted in resources -but instead assess the GLMs' innate capabilities.We showcase the versatility of the inherited LM parameters in conjunction with our graph transformer architecture, by applying them to challenging reasoning tasks, where the model needs to reason over complementary inputs from text and graphs, and where it needs to infer information not present in the input, unlike data-to-text generation.Moreover, we demonstrate that our architectural changes are highly compatible with the original LM weights, via linear probing experiments, where the GLM outperforms conventional LM and Graph Transformer models.</p>
<p>Preliminary: Graph Transformers (GT)</p>
<p>This section briefly introduces graph transformers, focusing on architectural choices relevant for our work.We also discuss some general properties of GNNs that motivate our design choices in §4.</p>
<p>The attention in self-attention can be written as
softmax QK T √ d + B P + M V,(1)
where Q, K and V are the query, key and value matrices, and d is the query and key dimension.The B P and M matrices can be used for positional encoding and masking.Setting B P = M = 0 yields the standard formulation (Vaswani et al., 2017).</p>
<p>Positional Encoding The self-attention mechanism of transformer models is permutation invariant, i.e., it doesn't have any notion of the order of its input elements.Thus, positional Encoding (PE) is used to inform LMs of the ordering of tokens in a text (Dufter et al., 2022).Most approaches employ either absolute PE, where absolute token positions are encoded (Vaswani et al., 2017;Gehring et al., 2017) or relative PE, which encodes the relative position between pairs of tokens (Shaw et al., 2018;Raffel et al., 2020;Su et al., 2021;Press et al., 2022).Absolute PE is typically combined with the input sequence and hence, the PE does not need to be encoded in self-attention (B P = 0).For relative PE, B P encodes a bias depending on the relative distances between pairs of tokens -for example, by learning one scalar for each possible distance:
B P = f (P ),(2)
where P is a matrix of relative distances and f (•) an elementwise function.</p>
<p>Similarly, GTs use PEs to encode the structure of the input, and hence, their PE has to encode a graph structure, as opposed to a sequence.This can again be done with absolute or relative PEs.However, defining an "absolute position" of a node or edge in a graph is not straightforward.While many methods exist, they are not directly compatible with the usual (absolute) "counting position" known from sequence encoding in LMs.In this work we thus focus on relative PE.Given a directed acyclic path in a graph, we can define the (signed) distance between any pair of nodes along a path simply as the number of hops between the nodes.The sign can be set by the direction of the path.Thus, by finding a consistent set of such paths in §4, we obtain relative distances and hence the graph's PE.</p>
<p>Masked Attention In a vanilla transformer, selfattention is computed for all possible pairs of tokens in the input.By contrast, nodes typically only attend to adjacent nodes in GNNs.Therefore, information between more distant nodes has to be propagated across multiple GNN layers.For graphs, such sparse message passing approaches are sometimes preferred, as in most graphs the neighborhood size increases exponentially with increasing radius, which can cause loss of information due to over-smoothing (Chen et al., 2020).Thus, in GTs it can be beneficial to introduce graph priors, for example by restricting self-attention to local neighborhoods.This can be realized by setting elements of M to 0 for pairs of tokens that should be connected, and to −∞ otherwise.</p>
<p>On the other hand, it has been shown that a global view of the graph can enable efficient, longranged information flow (Alon and Yahav, 2021;Ribeiro et al., 2020).We will therefore present two model variants in §4 -a local and a global GLM.</p>
<p>4 Graph Language Model GLM vs. GT We aim to design an architecture that can efficiently and jointly reason over text and graph-structured data.GTs can offer desired graph priors, but they lack language understanding.One intuitive approach to bridge this gap is to pretrain a GT from scratch (Schmitt et al., 2021).But pretraining is costly and the necessary data bound to be scarce.We thus take a different avenue.We hypothesize that for reasoning over GoTs a model needs language understanding capabilities similar to those used for reasoning over text.Intuitively this should be the case, since (i) GoTs are designed to be understandable by humans and (ii) literate people can "read" and understand GoTs.</p>
<p>By initializing a GT with parameters from a compatible LM, we obtain our Graph Language Model (GLM).The GT architecture introduces graph priors, while parameter initialization from the LM gives it language understanding capabilities.In the following we explain the necessary modifications to the input graph and to the model, to make this work.The general idea is that (verbalized) triplets should resemble natural language as much as possible to enable LM weights to capture them, while graph reasoning should work via message passing.</p>
<p>Graph preprocessing A LM tokenizer converts text into a sequence of tokens from the LM vocabulary.Similarly, we process GoTs, such that the GLM can process the graphs "as a LM would do" (cf.Fig. 2).To achieve this, we first convert the GoT to its so-called Levi graph (Schmitt et al., 2021), i.e., we replace each edge with a node that contains the relation name as text feature, and connect the new node to the head and tail of the original edge via unlabeled edges, preserving the direction of the original edge.Next, we tokenize each node and split each node into multiple nodes, such that every new node corresponds to a single token.New edges connect adjacent nodes, again preserving the original direction.This yields the extended Levi graph (see Fig. 2b).In this representation, each triplet is represented as a sequence of tokens -just as it would be for a standard LM. 3ositional Encodings As discussed in §3, we prefer PEs that encode the relative position between pairs of tokens, determined by their signed distance.We can directly adopt this method to encode the relative position between pairs of tokens occurring within the same triplet -by simply considering the triplet as a piece of text, and counting the token distance in this text.Note that a single token can occur in multiple triplets, leading to, e.g., multiple "lefthand side neighbors" (cf.animal in Fig. 2b and 3).While this does not occur in ordinary sequential text, it does not impose a problem for relative PE.</p>
<p>Yet, the approach above breaks when tokens do not belong to the same triplet.To determine the distance between such pairs of tokens, previous work considered, e.g., the length of the shortest path between them (Schmitt et al., 2021).However, this results in PEs that do not come natural to a LM, since a triplet would appear in reversed order, if it is traversed in "the wrong direction" in the shortest path. 4We therefore omit structure-informed PE between tokens that do not belong to the same triplet and instead propose two GLM variants: a local (ℓGLM) and a global (gGLM) one.</p>
<p>Local and global GLM Fig. 3 shows the relative token position matrix P for the graph in Fig. 2b.In the ℓGLM the self-attention mechanism is restricted to tokens from the same triplet.This means that attention to any token located beyond the local triplet is set to 0 -and hence does not require PE.Still, in such configurations, messages can propagate through the graph across multiple layers, since tokens belonging to a concept can be shared by multiple triplets.This is analogous to standard message passing in GNNs, where non-adjacent nodes have no direct connection, but can still share information via message passing.For example, the representation of dog is contextualized by the triplets black poodle is a dog and dog is a animal after the first ℓGLM layer.Hence in the second layer, when animal attends to dog, the animal embedding gets impacted by black poodle, even though there is no direct connection from animal to black poodle.</p>
<p>However, it has been shown that a global view can have benefits (Ribeiro et al., 2020).Hence, we also formalize the gGLM, as an alternative where self-attention can connect any node to every other node.For this setting we need to assign a PE to any pair of tokens, including those that do not occur within the same triplet.For these pairs we introduce a new graph-to-graph (G2G) relative position.LMs don't have learned parameters for G2G connections, so we initialize the parameters with the corresponding parameters of a relative position of +∞.In a LM a relative position of +∞ means that the respective tokens occur somewhere "far" away in a remote text passage.LMs learn a proximity bias during pretraining, i.e., they tend to have higher attention scores between tokens that are close to each other in the text.This means that tokens with a high relative distance tend to have low attention scores.For our gGLM this corresponds to a graph bias where distant nodes are less important, but are still accessible. 5Note that unlike in the ℓGLM, this bias is not part of the architecture.It originates from the pretrained parameters, meaning that the gGLM can learn to attend to distant tokens.</p>
<p>Along with P and M , the GLM takes a sequence of all tokens in the extended Levi graph as input.For this, we technically need to "linearize" the graph.However, the order of tokens in the resulting sequence does not matter: relative positions in P are determined by distances in the graph, not in the sequence.Permuting the input sequence simply means that rows and columns of P and M need to be permuted accordingly, but the resulting token embeddings remain unchanged.See example matrices for P and M for ℓGLM and gGLM in §A.</p>
<p>Being transformers, GLMs have the same computational complexity as their respective LM.For sparse graphs the ℓGLM could make use of sparse matrix multiplication, making it more efficient than a corresponding LM or gGLM.However, for our experiments this was not necessary.</p>
<p>Joint graph and text encoding If we use normal matrices for P and M , the GLM is identical to its underlying LM.Hence, GLMs can be applied to texts and -more interestingly -interleaved inputs of text and graph.In this joint setting, P and M each consists of four sub-matrices that correspond to self-attention between tokens from (i) graph-tograph, (ii) text-to-text, (iii) text-to-graph and (iv) graph-to-text.Graph-to-graph sub-matrices are formatted as described above for ℓGLM and gGLM, respectively.Text-to-text sub-matrices are standard matrices from conventional sequence transformers.We introduce new T2G and G2T relative positions for text-to-graph, and graph-to-text connections, respectively.With this, the model can learn interaction strength between the two modalities.Similar to G2G in gGLM, we initialize T2G and G2T parameters from +∞.See example matrices in §A.</p>
<p>Uni-and Bidirectional LMs If a LM's self-attention is unidirectional, information can only propagate along the direction of arrows in Fig. 2b for the ℓGLM.This means that, e.g., the representation of the node black poodle is independent of the rest of the graph.We could augment the graph with inverse relations to enable bidirectional information flow with unidirectional LMs, but in this work, we restrict our analysis to bidirectional models.</p>
<p>T5 We use T5 (Raffel et al., 2020) -a bidirectional encoder with unidirectional decoder -as base LM to instantiate GLMs.In T5, relative distances in P group into so-called buckets, and each bucket maps to one learned positional bias in B P for each head.Positional biases are shared across layers.The decoder is not needed to encode graphs, but can be used to generate sequences, such as text or linearized graphs in future work.</p>
<p>Experiments</p>
<p>We assess the GLMs' capabilities for embedding GoTs in two experiments on relation (label) classification, i.e., classifying which relation belongs to a given head and tail entity.One experiment uses ConceptNet (CN; Speer et al., 2017) subgraphs that we construct to enable analysis of the impact of structural graph properties.In a second experiment on Wikidata (Vrandečić and Krötzsch, 2014) subgraphs and associated Wikipedia abstracts we test GLMs on interleaved inputs of text and graph.</p>
<p>Representing and reasoning over Graphs</p>
<p>We construct a balanced dataset of English CN subgraphs consisting of 13,600 train, 1,700 dev and 1,700 test instances with 17 distinct relations as labels.We replace the relation to be predicted with <extra_id_0>, T5's first mask token.</p>
<p>To investigate the impact of varying graph complexities, we experiment with different graph sizes denoted by their radius r.We ensure that small graphs are strict subgraphs of larger graphs, such that potential performance gains in larger graphs must stem from additional long-ranged context.</p>
<p>To evaluate model effectiveness when longranged connections are crucial, we mask complete subgraphs around the relation to be predicted.The size of a masked subgraph is m, where m = 0 means no mask, m = 1 masks neighboring concepts, m = 2 masks neighboring concepts and the next relations, etc.We replace each masked concept and relation with a different mask token.Construction details and statistics are shown in §B.1.1.</p>
<p>Experimental setup</p>
<p>The input to our model is a CN subgraph.The relation to be predicted is replaced with <extra_id_0>.The GLM encodes the graphs as in §4, producing an embedding for each token.A linear classification head gives the final prediction from the mask's embedding.We verbalize unmasked relations using static templates (Plenz et al., 2023), shown in §B, Table 4.</p>
<p>In a finetuning setting we train the GLM and the classification head jointly.However, since the GLM is initialized from a LM, we hypothesize that it should produce meaningful embeddings, even without any training.To test this hypothesis, we train only the classification head, i.e., we only train a linear probe.In this setting, the GLM was never trained on any graph data, similar to a zero-shot setting.The linear probe only extracts linear features and hence, can only achieve high performance if the GLM embeddings show expressive features.</p>
<p>We report mean accuracy across 5 different runs.See §B.1.2for hyperparameters.Unless stated otherwise, we use T5-small to allow many baselines.</p>
<p>Baselines</p>
<p>We compare to several baselines inspired by related work.For all baselines we utilize the T5 encoder as underlying LM.This allows us to focus on the architectural design of different model types.</p>
<p>LM For LM-based approaches we linearize the input graphs to a sequence, by concatenating the verbalized triplets.There are structured ways to linearize graphs, but such graph traversals generally require the graph to be directed and acyclicwhich makes them inapplicable to linearizing GoTs.Instead, we order the triplets either randomly (T5 set) or alphabetically (T5 list).For T5 set, triplets are shuffled randomly in every training epoch such that the model can learn to generalize to unseen orderings.The concatenated triplets are passed to the T5 encoder, and the embedding of <extra_id_0> is presented to the classification head.</p>
<p>GNN For GNN baselines we encode each node of the original graph (cf.Fig. 2a) with the T5 encoder, and train a GNN using these static embeddings.After the final layer, the GNN returns 17 logits for each node.As final logits, we take the mean logit of the two nodes adjacent to the relation to be predicted.We experiment with different variants as GNN layers: GCN (Kipf and Welling, 2017) and GAT (Veličković et al., 2018).Since GNNs do not come with pretrained weights, we only apply them in finetuning, when training all parameters.</p>
<p>Graph transformer Finally we compare GLMs to models with the same architecture, but random weight initialization (normal graph transformers).This allows us to assess the impact of weight initialization from a LM with two further baselines: ℓGT and gGT.We only consider GTs with finetuning.</p>
<p>Results</p>
<p>Linear probing Tab. 1 shows the relation label prediction accuracy for linear probing, i.e., when training only the classification head.Our first observation is that gGLM is consistently the best, outperforming ℓGLM and the LM baselines.For a radius of r = 1 we have exactly one triplet, which has almost the same representation in the GLM and LM approaches.The only difference is that the LM baselines have an end-of-sentence token, which the GLM does not have.Surprisingly, not having the end-of-sentence token seems to be an advantage with linear probing, but we will see later that this changes when updating model weights.</p>
<p>For r ≥ 3, LM baselines show decreasing performance with increasing radii.By contrast, both ℓGLM and gGLM show increasing performances with increasing radii.This indicates that GLMs can utilize the additional context.But LM baselines don't have any inbuilt methods to grasp distances in the graph, which could cause them to fail at distinguishing relevant from less relevant information.</p>
<p>The performance gap between gGLM and LM models tends to increase for larger m, i.e., when larger sub-structures are masked.However, the ℓGLM underperforms for large m, highlighting the advantage of the global view in gGLM when longranged connections are necessary.</p>
<p>The overall high performance of GLMs confirms our assumption that GLMs are compatible with LM weights, even without any training.Increasing performance with increasing radii further shows that GLMs have good inductive graph biases.When long-range connections are relevant, the representations learned by gGLM outperform the locally constrained ℓGLM -which showcases the strength of the global view that the gGLM is able to take.</p>
<p>Finetuning Tab. 1 shows results when training all parameters.In this setting, models can adjust to the task and learn to reason over graphs through parameter updates.In addition, GLMs can tune parameters to better match the novel input structure.</p>
<p>The GLM and LM variants are consistently better than GNN and GT methods, which indicates that linguistic understanding is potentially more important than graph reasoning for this task.Models outperform their linear probing scores, which shows that finetuning is, as expected, beneficial.</p>
<p>Overall, the GLMs perform best, while GTs perform the worst.The only difference between the two model groups is weight initialization -the GLMs are initialized from T5, while the GTs are randomly initialized.Further, we observe that for r ≥ 1 and m = 0 the local GT (ℓGT) significantly outperforms its global counterpart gGT.For the GLM the global version is on par, or even better than the local one.This shows the effectiveness of T5's attention mechanism: thanks to its weight initialization, gGLM attends to relevant tokens even in large context windows, while gGT suffers from potentially distracting long-ranged information.</p>
<p>For m = 0 the differences between GLM and LM approaches are small, with a slight trend for GLMs to outperform LMs on large graphs, and vice versa for small graphs.However, when graph rea-  soning is more important due to masking (m ≥ 1), then GLMs consistently and significantly outperform all other baselines.This indicates that LMs can learn to do simple graph reasoning through parameter updates, but underperform in more complex graph reasoning tasks where either graphs are larger, or long-ranged connections are required.</p>
<p>For m ≥ 1, the gGLM outperforms ℓGLM due to its global connections.In contrast to the linear probing setting, the ℓGLM outperforms other baselines for all non-zero levels of masking.This indicates that ℓGLM can learn to use long-ranged information during training, if the task requires it.</p>
<p>Impact of model size To investigate the effect of model size, we train the most promising approaches (GLM and LM) in 3 different sizes.Tab.6 in §B.1.3shows that overall larger models perform better.Surprisingly, the base models sometimes outperform the larger models for settings that require more graph reasoning, i.e., larger m.However, these differences are small and non-significant.In most cases, gGLM large or base are the best model.</p>
<p>Jointly representing Graph and Text</p>
<p>We now investigate GLM capabilities to process interleaved inputs of text and graph in a KG population setup, i.e., extending a KG with new relation instances.Subtask 1 performs text-guided relation classification where some relations may be inferrable from the text, while others may exploit graph knowledge to make predictions.In Subtask 2, models classify the source of a predicted relation, i.e., whether it can be inferred from the text, or whether it requires (additional) graph knowledge.</p>
<p>We construct our data from Huguet Cabot and Navigli ( 2021), who offer a corpus of Wikipedia abstracts that are linked to Wikidata via entity linking.Their focus is relation extraction, so they filter the graphs using NLI, such that all triplets are entailed by the text.We augment the entailed triplets with further triplets from Wikidata that are not entailed by the text.For a given text, subgraph, head and tail entity, models will jointly predict the relation and the source.We adopt the 220 most common relations in our train graphs and a "no-relation" label.</p>
<p>For source labels we have 3 classes: entailed by the text, not entailed and no-relation.No-relation is the correct label iff the relation is also no-relation.</p>
<p>§B.2.1 shows statistics and construction details.</p>
<p>Experimental setup and baselines</p>
<p>Unlike §5.1.1,models now receive text and graph data as input.We train two distinct classification heads on the mask's embedding for relation and source classification.While the mask is part of the graph, its embedding depends on both modalities.The final loss is the sum of the relation classification and the source prediction loss, weighted by 0.9 and 0.1.We use T5-large, but otherwise baselines are as in §5.1.2.§B.2.2 shows the training details.</p>
<p>Results</p>
<p>Fig. 4 10 reveal that first, the model almost exclusively utilizes text data, but quickly learns to make use of the graph.For textually entailed triplets, text is more impactful than the graph, and vice versa for other triplets (cf.Tab.9).Ablating graphs lowers source prediction by ∼4.5 points, which shows that GLMs benefit from graph information even for predominantly text oriented tasks.</p>
<p>The results show that GLMs can efficiently reason over interleaved inputs of graph and text, especially with limited training data.This makes GLMs a promising new model type for knowledge-intense NLP tasks, such as KG population or Q&amp;A.</p>
<p>Conclusion</p>
<p>We present the Graph Language Model (GLM) -a graph transformer initialized with weights from a LM.It excels at graph reasoning, while simultaneously encoding textual triplets in the graph as LMs do, thereby bridging the gap between LMs and GNNs.GLMs can natively reason over joint inputs from texts and graphs, leveraging and enhancing each modality.Experiments show the GLM's advantage over LM and GNN based baselines, even in a linear probing setting.In particular, GLMs greatly outperform graph transformers.This highlights the need for pretrained LM weights, even for graph reasoning.We therefore advocate GLMs as a valuable tool for advancing research in embedding and leveraging knowledge graphs for NLP tasks.</p>
<p>Limitations</p>
<p>While GLMs are designed as general purpose tools for knowledge-intense NLP tasks, our evaluation is limited to English knowledge graphs.However, we explore various types of knowledge graphs (commonsense and factual) and tasks (relation classification, text-guided relation classification, and source prediction), broadening our empirical assessment.Confirming GLMs improved text and graph rea-soning skills for different languages, domains and tasks is left for future work.</p>
<p>Our GLM framework supports instantiation from any LM with relative positional encoding, including rotary positional encoding.Comprehensive comparisons to determine the most suitable models for the GLM framework remain for future investigation.Nonetheless, bidirectional LMs are expected to perform best in the novel framework, because unidirectional LMs necessitate additional inverse relations, as discussed in §4.</p>
<p>Ethical considerations</p>
<p>We do not foresee immediate ethical concerns for our research, as we rely on well-established datasets.However, even established datasets can contain undesirable biases which our method could potentially spread and amplify.</p>
<p>Looking ahead, our focus lies in enriching knowledge graph integration within language models, with the aim of enhancing factuality and mitigating hallucination.This advancement is expected to bolster the reliability and controllability of LMs, leading to positive societal impacts.Furthermore, LMs relying on knowledge graphs may facilitate easier maintenance, potentially reducing the need for frequent retraining of deployed models, thereby promoting sustainability in NLP practices.</p>
<p>A Model</p>
<p>Fig. 5 shows the matrices P and M for ℓGLM and gGLM.Fig. 6 shows the same matrices for joint encoding of text and graph data.We experiment on randomly selected subgraphs from the largest connected component of the English part of CN version 5.7 (Speer et al., 2017), which consists of 125,661 concepts and 1,025,802 triplets.We select 17 distinct relation label classes (cf.Tab.4), ensuring sufficient frequency and semantic dissimilarity.For each class, we randomly sample 1,000 triplets, allowing only cases where exactly one triplet connects the head and tail entities, to reduce label ambiguity.These 1,000 instances are split into train (800), dev (100), and test (100).This creates a balanced dataset of 13,600 train, 1,700 dev, and 1,700 test instances.To predict relation labels, we replace them with <extra_id_0>, T5's first mask token.For our experiments, we replace CN (unmasked) relations with more natural verbalizations.Tab. 4 shows the static verbalization for each relation.</p>
<p>During graph construction we control the graph size, parameterized by the radius r.We start with a radius of r = 1, when we consider only the two concepts (head and tail) in the target triplet.To create a larger graph context, we randomly select 4 adjacent triplets -2 for the head, and 2 for the tail entity of the original triplet.A graph with radius r = 2 is formed by the subgraph spanned by all entities used in these 5 triplets.For r = 3 we again randomly select 2 triplets for each of the outer (up to) 4 entities, yielding (up to) 13 triplets.To avoid accidentally adding more short-ranged information, we restrict the new triplets to triplets that actually extend the radius of the graph.This enables us to control graph size and complexity, while still enabling sufficient diversity in the graph structure.Further, the graphs are created such that graphs for smaller radii are strict subgraphs of graphs with larger radii.This ensures that performance changes with increasing radii are due to long-ranged connections, and not due to potentially different shortranged information.Tab. 3 shows structural properties of CN subgraphs, depending on their radius.</p>
<p>When masking subgraphs, we mask complete subgraphs of a certain size around the target to be predicted.The size of the masked subgraph is denoted by m, where m = 0 means no masking, m = 1 masks neighboring concepts, m = 2 masks neighboring concepts and the next relations, and so on.Formally, m denotes the radius of the masked graph in Levi representation, which should not be confused with the extended Levi graph, nor the normal graph representation.We replace each concept and relation in the masked subgraph with a different mask token.This in principle enables LM baselines to internally reconstruct the graph.</p>
<p>B.1.2 Experimental setup and baselines</p>
<p>Tab. 5 shows our hyperparameters.For the GNNs, we tested different numbers of layers (2,3,4,5), hidden channel dimensions (32,64,128), and nonlinearities (ReLU, leaky ReLU) in preliminary experiments.</p>
<p>B.1.3 Results</p>
<p>Tab. 6 shows performance on CN for different modelsizes.</p>
<p>B.2 Wikidata and Wikipedia</p>
<p>B.2.1 Dataset</p>
<p>Huguet Cabot and Navigli (2021) propose a largescale corpus of aligned Wikipedia abstracts and Wikidata (Vrandečić and Krötzsch, 2014) triplets.They first extract Wikidata entities from the abstract, and then link these entities with triplets in Wikidata.They are interested in triplets that are entailed by the text, so they use a NLI model to filter out all other triplets.They publicly released the extracted entities and the filtered triplets.</p>
<p>For our purpose, we are interested in aligned graphs and texts, but triplets in the graph do not necessarily have to be entailed by the text.Hence, we find all triplets between the extracted entities using the Wikidata Query Service.6 From Huguet Cabot and Navigli (2021) we know which triplets in our graphs are entailed by the text.</p>
<p>Similar to Huguet Cabot and Navigli (2021) we consider the 220 most common relations in the train split as our relation labels.Additionally, we add a "no-relation" label, yielding 221 relation classes.</p>
<p>For 10 % of the graphs we randomly add a new triplet between previously unconnected head and tail entity, and the mask token as relation.For these graphs "no-relation" is the correct relation label.</p>
<p>For the other 90 % graphs we replace a random existing relation with the mask token, while making sure that (i) the existing relation is in our 220 labels and that (ii) there is no other triplet connecting the respective head and tail entities.We remove   Tab.7 shows graph statistics.Compared to CN subgraphs (c.f.Tab. 3) the graphs are relatively small, matching the size of r = 2. On CN we found that LMs can perform well on such small graphs, so we expect that the performance gap between GLMs and LM baselines on Wikidata would be larger if Wikidata subgraphs were larger.</p>
<p>B.2.2 Experimental setup and baselines</p>
<p>For these experiments we omit GNNs as a baseline, since they can't natively process texts.</p>
<p>The other models all compute an embedding of the mask token, and then two separate classification heads produce predictions for the relations (221 classes) and the source (3 classes).For each prediction, we compute the Cross Entropy Loss.The final loss is the weighted sum of these losses, weighted by 0.9 and 0.1 respectively.The relation classification has a higher weight since it has many more classes and hence, is potentially more difficult.This means that model parameters are optimized for both objectives jointly, while only the linear classification heads can specialize on their respective task.</p>
<p>The dataset is unbalanced (c.f.Fig 7 ), so report macro F1 scores instead of accuracy.This means that models only achieve high scores if they perform well on all classes, including minority classes.</p>
<p>We assume that classifying one out of 221 relations requires fine grained text understanding, so we initialize models from T5-large instead of T5small.To reduce computational load, we only train one model per setting.Further, we enable efficient batching by restricting inputs to a maximum of 512 tokens.This truncates 2.8 % of train instances for GLMs and 5.1 % for LM baselines due to their less efficient graph encoding.</p>
<p>Hyperparameters are identical to Tab. 5, except that (i) we reduce batch size to 8, (ii) train for at most 1 epoch and (iii) don't use early stopping.</p>
<p>B.2.3 Results</p>
<p>Fig. 8 shows the training curve when training for an entire epoch, i.e., 2,499,582.We observe that performances plateau beyond ∼ 0.2 epochs, so we stop training after 524,288 instances in our other experiments.</p>
<p>Tab. 8 shows concrete numbers for the models in Figures 4 and 8.</p>
<p>C Usage of AI assistants</p>
<p>We use GitHub Copilot (https://github.com/features/copilot) for speeding up programming, and ChatGPT 3.5 (https://chat.openai.com) to aid with reformulations.The content of this work is our own, and not inspired by AI assistants.</p>
<p>Extended Levi graph of GoT (with relative distances P for dog).</p>
<p>Figure 2 :
2
Figure 2: Example of graph preprocessing in our GLM.Fig 2b shows relative distances for dog, i.e., when dog is attending to other tokens.The red Graph-to-Graph (G2G) connections only exist for the gGLM, not for the ℓGLM.</p>
<p>Figure 3 :
3
Figure 3: Relative position matrix P for tokens in Fig. 2b.Entries with G2G have no relative position (ℓGLM) or are initialized from +∞ (gGLM).Cf. §A.</p>
<p>Figure 4 :
4
Figure 4: KG population test results during training.gGLM outperforms T5 set by up to 6 points in 4a.</p>
<p>Relative positions P for dog in ℓGLM.(b) Relative position matrix P for ℓGLM (c) Mask matrix M for ℓGLM.Relative position P for dog in gGLM.(e)Relative position matrix P for gGLM (f) Mask matrix M for gGLM.</p>
<p>Figure 5 :
5
Figure 5: Relative positions P and masking M for ℓGLM and gGLM.</p>
<p>(a) Relative position matrix P for ℓGLM.(b) Mask matrix M for ℓGLM.(c) Relative position matrix P for gGLM.(d) Mask matrix M for gGLM.</p>
<p>Figure 6 :
6
Figure 6: Relative positions P and masking M for ℓGLM and gGLM when encoding text and graph jointly.The example sentence is "The dog chased the cat."</p>
<p>Fig. 7
7
shows the label distributions for relation and source for train and test.Out of the 221 relations, only 195 and 194 relations occur in the train and test set, respectively.All relations in the test set also occur in the train set.</p>
<p>Fig. 9
9
Fig.8shows the training curve when training for an entire epoch, i.e., 2,499,582.We observe that performances plateau beyond ∼ 0.2 epochs, so we stop training after 524,288 instances in our other experiments.Tab.8 shows concrete numbers for the models in Figures4 and 8.Fig.9showsconfusion matrices for source prediction.Fig.10shows the test performance in relation classification of ablated models during different training steps.Table9shows relation classification scores for (i) triplets entailed by text and for (ii)</p>
<p>Figure 7 :
7
Figure 7: Label distributions for Wikidata ( §5.2) train and test sets.</p>
<p>(a) Evaluation on train set.(b) Evaluation on test set.</p>
<p>Figure 8 :
8
Figure 8: Training curves ( §5.2) when training for a whole epoch, i.e., 2,449,582 train instances.Performances are for relation classification.On the train set we did not compute macro F1, so we report accuracy instead.</p>
<p>(a) ℓGLM.(b)gGLM.</p>
<p>Figure 9 :
9
Figure 9: Confusion matrices source prediction on Wikidata ( §5.2).</p>
<p>Figure 10 :
10
Figure 10: Ablation of different input modalities to GLMs.All runs are done without source prediction (besides ℓGLM and gGLM).Scores are for relation classification on Wikidata ( §5.2).</p>
<p>Table 1 :
1
Relation label classification accuracy on CN in %. Results are shown for Linear Probing and Finetuning.
Modelr m1 02 03 04 05 04 14 24 34 44 5Lin. Prob.ℓGLM gGLM T5 (list) T5 (set)55.4±0.3 57.1±0.3 56.8±0.6 56.9±0.4 57.0±0.4 30.4±0.4 17.8±0.2 14.0±0.3 11.4±0.5 11.9±0.3 55.4±0.3 58.6±0.7 58.8±0.6 59.3±0.7 59.5±0.4 41.8±0.8 25.6±0.9 22.0±0.6 19.4±0.5 17.0±0.2 53.7±0.3 56.8±1.1 56.5±1.2 55.8±0.6 55.3±0.5 20.3±0.6 19.9±0.4 15.3±0.6 14.0±1.1 10.2±1.2 53.1±0.6 52.8±1.2 54.6±0.6 53.9±0.5 53.1±0.8 18.2±0.6 16.7±0.5 13.1±0.7 12.3±0.6 9.7±0.9ℓGLM64.0±1.3 64.0±1.0 64.4±0.7 64.1±0.9 64.2±1.1 47.9±0.4 26.8±0.8 23.8±0.9 19.8±1.1 18.1±0.7gGLM63.2±0.9 64.4±1.1 64.6±1.2 64.1±1.3 65.3±0.7 48.0±0.6 27.2±0.7 24.2±0.7 20.2±1.4 19.2±0.7FinetuningT5 (list) T5 (set) GCN GAT64.9±1.0 64.9±1.2 64.9±1.3 63.9±0.9 64.0±0.6 40.4±0.8 21.8±0.8 17.8±1.0 15.4±0.3 12.8±0.5 63.9±0.7 65.8±0.8 64.0±0.3 64.1±1.2 64.3±1.1 40.3±1.2 21.8±0.7 18.0±0.6 15.5±0.6 13.1±0.7 44.3±0.9 37.1±1.0 34.4±1.2 36.5±0.6 36.8±1.4 22.2±1.2 21.9±0.8 12.1±3.5 9.0±4.3 5.9±0.0 44.5±0.9 40.6±1.3 36.3±1.3 37.0±0.8 37.0±0.8 20.0±0.7 20.8±0.2 14.0±0.6 13.8±0.8 11.0±0.6ℓGT24.2±3.4 35.0±1.2 34.7±1.3 32.7±2.9 34.5±2.8 30.1±2.6 12.8±2.4 15.5±0.3 9.5±1.3 10.0±1.6gGT27.6±1.9 29.0±0.8 23.4±1.2 19.2±1.2 15.6±1.5 18.6±0.7 13.2±1.1 14.5±0.6 12.4±1.3 12.1±1.7</p>
<p>Table 2 :
2
Ablations for KG population in macro F1.
and Tab. 8 show test set performance for a)relation and b) source classification, at differenttraining stages. gGLM performs the best overall,followed by ℓGLM. LM baselines are competitive,but lag behind at early stages and for source predic-tion. Again, GT baselines perform poorly, showcas-ing the advantage of weight initialization in GLM -even with large-scale training data. For all models,training plateaus beyond ∼ 500k seen instances (cf.Fig. 8 in  §B.2.3), so we stop training at this cut-off.Tab. 2 gives results for ablating different inputmodalities to GLMs. Since source prediction al-ways requires text input, we test relation classifi-</p>
<p>Table 4 :
4
Verbalization templates for relations in Con-ceptNet.The upper part of the relations are the 17 classes in the classification task.
ParameterValueLosscross entropy lossOptimizerAdamWGLM, LM &amp; GTLearning rate Batchsize Max. # epochs Early stopping criterion Early stopping # epochs # parameters in small1e−4 (FT) &amp; 5e−3 (LP) 32 50 dev loss 5 35B (FT) &amp; 8k (LP)# parameters in base110B (FT) &amp; 13k (LP)# parameters in large335B (FT) &amp; 17k (LP)# encoder layers in small6# encoder layers in base12# encoder layers in large24Losscross entropy lossOptimizerAdamWLearning rate5e−3Batchsize32GNNMax. # epochs Early stopping criterion50 dev lossEarly stopping # epochs5# layers3hidden channel dimension64non-linearityReLU</p>
<p>Table 5 :
5
Hyperparameters for §5.1.FT stands for finetuning and LP stand for linear probing."# parameters" is the number of trainable parameters.
Modelr m1 02 03 04 05 04 14 24 34 44 5small 64.0±1.3 64.0±1.0 64.4±0.7 64.1±0.9 64.2±1.1 47.9±0.4 26.8±0.8 23.8±0.9 19.8±1.1 18.1±0.7ℓGLMbase 67.6±0.8 69.6±0.9 69.8±0.5 69.8±1.3 69.6±0.7 49.2±0.8 29.3±0.8 24.4±0.3 20.8±0.9 19.6±0.8large 72.0±1.0 71.4±1.5 72.2±1.0 72.7±0.8 71.5±1.8 48.4±1.1 29.7±1.6 24.8±1.6 20.0±0.9 20.3±0.5small 63.2±0.9 64.4±1.1 64.6±1.2 64.1±1.3 65.3±0.7 48.0±0.6 27.2±0.7 24.2±0.7 20.2±1.4 19.2±0.7gGLMbase 67.8±0.7 71.3±1.0 70.5±1.2 71.5±1.1 71.1±0.4 49.7±1.2 30.2±0.8 25.5±0.8 21.4±1.2 20.1±0.2large 72.1±1.1 73.9±0.7 74.2±0.6 74.8±0.8 73.9±0.7 50.1±0.5 31.9±1.2 24.4±1.5 21.2±0.6 19.6±0.8small 64.9±1.0 64.9±1.2 64.9±1.3 63.9±0.9 64.0±0.6 40.4±0.8 21.8±0.8 17.8±1.0 15.4±0.3 12.8±0.5T5 listbase 71.2±0.9 69.5±0.7 69.5±1.0 70.4±1.6 70.4±0.7 40.7±0.9 25.5±1.2 17.8±0.2 16.4±1.3 13.9±0.7large 74.5±0.4 73.7±0.4 73.5±0.6 73.6±0.8 73.3±1.0 41.2±1.5 27.9±1.0 18.3±0.9 17.0±0.5 13.0±0.9small 63.9±0.7 65.8±0.8 64.0±0.3 64.1±1.2 64.3±1.1 40.3±1.2 21.8±0.7 18.0±0.6 15.5±0.6 13.1±0.7T5 setbase 71.2±0.6 69.8±0.6 69.5±0.6 70.1±0.7 69.8±1.4 40.4±0.9 23.9±1.1 18.5±1.1 16.3±0.3 14.3±0.7large 74.9±0.3 73.0±0.5 73.1±0.8 72.5±1.1 73.5±0.4 41.2±1.3 25.1±1.3 17.4±0.9 15.9±0.5 13.2±0.8</p>
<p>Table 6 :
6
Relation label classification accuracy on ConceptNet ( §5.1) when training all parameters.Best score per model family is boldfaced, and best score overall is highlighted in yellow.
Metrictraintest#nodes5.59 ± 3.77 5.60 ± 3.78#edges8.71 ± 11.99 8.71 ± 12.01mean degree 2.66 ± 1.58 2.66 ± 1.58</p>
<p>Table 7 :
7
Structural statistics of Wikidata ( §5.2) subgraphs.instances where no suitable triplet is available.This yields a dataset with 2,449,582 train, 135,828 val and 135,923 test instances.</p>
<p>Table 8 :
8
Macro F1 scores on Wikidata test set for relation classification and source classification.Scores are shown for models after training on different numbers of train instances.
Model524,288 train instances 2,449,582 train instances Relation Source Relation SourceℓGLM82.3583.3985.0686.20gGLM81.9883.2185.2886.17T5 list81.4582.1785.3685.83T5 set81.2982.0085.0485.53ℓGT3.1939.811.5037.83gGT3.4739.583.4039.37AblationEntailed ℓGLM gGLM ℓGLM gGLM Not entailedw/ text &amp; graph85.4684.8578.4778.46w/o text-8.40-6.75-4.57-4.28w/o graph-4.56-3.94-7.56-7.55w/o text &amp; graph -20.52 -19.90 -20.08 -20.07</p>
<p>Table 9 :
9
Ablations for KG population ( §5.2).Scores are macro F1 for relation label classification on (i) triplets that are entailed by the text and (ii) all other triplets.Models are trained w/o source prediction.
other triplets.
 Cf. www.llamaindex.ai and www.langchain.com <br />
Note that the token sequence of the converted GoT is not necessarily perfectly identical to the token sequence that corresponds to the input triplets. We tokenize each node in the Levi graph individually, to ensure consistent tokenization of concepts shared by multiple triplets. This removes whitespace between concepts and edges, which impacts tokenization. We leave investigation of the impact of this effect to future work.
For example, cat would see the graph as the following sequence: cat is a animal a is dog a is poodle black.
Preliminary experiments showed that initializing G2G parameters from +∞ outperforms random initialization, which outperforms initialization from 0.
https://query.wikidata.org, accessed in Jan. 2024.
AcknowledgementsWe want to thank Letit , ia Pârcȃlȃbescu for providing feedback on our manuscript.This work was funded by DFG, the German Research Foundation, within the project "ACCEPT: Perspectivized Argument Knowledge Graphs for Deliberation", as part of the priority program "RATIO: Robust Argumentation Machines" (SPP-1999).
On the bottleneck of graph neural networks and its practical implications. Uri Alon, Eran Yahav, International Conference on Learning Representations. 2021</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The Semantic Web. Berlin, Heidelberg; Berlin HeidelbergSpringer2007</p>
<p>. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, </p>
<p>Commonsense transformers for automatic knowledge graph construction. 10.18653/v1/P19-1470Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyCOMETAssociation for Computational Linguistics</p>
<p>Joan Michael M Bronstein, Taco Bruna, Petar Cohen, Veličković, arXiv:2104.13478Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. 2021arXiv preprint</p>
<p>Measuring and relieving the oversmoothing problem for graph neural networks from the topological view. Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, Xu Sun, 10.1609/aaai.v34i04.5747Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Position information in transformers: An overview. Philipp Dufter, Martin Schmitt, Hinrich Schütze, 10.1162/coli_a_00445Computational Linguistics. 4832022</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, 2024</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational Linguistics2017</p>
<p>A convolutional encoder model for neural machine translation. Jonas Gehring, Michael Auli, David Grangier, Yann Dauphin, 10.18653/v1/P17-1012Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>REBEL: Relation extraction by end-to-end language generation. Pere-Lluís Huguet, Cabot , Roberto Navigli, 10.18653/v1/2021.findings-emnlp.204Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. Jena D Hwang, Chandra Bhagavatula, Le Ronan, Jeff Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut, Choi, AAAI. 2021</p>
<p>Semisupervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, International Conference on Learning Representations (ICLR). 2017</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Fewshot Knowledge Graph-to-Text Generation with Pretrained Language Models. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen, ACL Findings. 2021</p>
<p>Unifying Structured Data as Graph for Data-to-Text Pre-Training. Shujie Li, Liang Li, Ruiying Geng, Min Yang, Binhua Li, Guanghu Yuan, Wanwei He, Can Shao Yuan, Fei Ma, Yongbin Huang, Li, 10.1162/tacl_a_006412024Transactions of the Association for Computational Linguistics12</p>
<p>KagNet: Knowledge-aware graph networks for commonsense reasoning. Xinyue Bill Yuchen Lin, Jamin Chen, Xiang Chen, Ren, 10.18653/v1/D19-1282Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Commonsense knowledge base completion with structural and semantic context. Chaitanya Malaviya, Chandra Bhagavatula, Antoine Bosselut, Yejin Choi, Proceedings of the 34th AAAI Conference on Artificial Intelligence. the 34th AAAI Conference on Artificial Intelligence2020</p>
<p>Transformer for graphs: An overview from architecture perspective. Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wen Bing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, Yu Rong, ArXiv, abs/2202.084552022</p>
<p>Attending to Graph Transformers. Luis Müller, Christopher Morris, Mikhail Galkin, Ladislav Rampášek, 2023Arxiv preprint</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, IEEE Transactions on Knowledge and Data Engineering. 2024TKDE</p>
<p>Pakt: Perspectivized argumentation knowledge graph and tool for deliberation analysis. Moritz Plenz, Philipp Heinisch, Anette Frank, Philipp Cimiano, 2024</p>
<p>Similarity-weighted construction of contextualized commonsense knowledge graphs for knowledge-intense argumentation tasks. Moritz Plenz, Juri Opitz, Philipp Heinisch, Philipp Cimiano, Anette Frank, 10.18653/v1/2023.acl-long.338Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Ofir Press, Noah Smith, Mike Lewis, International Conference on Learning Representations. 2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, 10.18653/v1/2021.nlp4convai-1.20Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI. the 3rd Workshop on Natural Language Processing for Conversational AIOnline. Association for Computational Linguistics2021Hinrich Schütze, and Iryna Gurevych</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. F R Leonardo, Yue Ribeiro, Claire Zhang, Iryna Gardent, Gurevych, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 82020</p>
<p>Modeling graph structure via relative position for text generation from knowledge graphs. Martin Schmitt, Leonardo F R Ribeiro, 10.18653/v1/2021.textgraphs-1.2Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15). the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)Mexico City, MexicoAssociation for Computational Linguistics2021Philipp Dufter, Iryna Gurevych, and Hinrich Schütze</p>
<p>An unsupervised joint system for text generation from knowledge graphs and semantic parsing. Martin Schmitt, Sahand Sharifzadeh, Hinrich Volker Tresp, Schütze, 10.18653/v1/2020.emnlp-main.577Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Self-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, 10.18653/v1/N18-2074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLouisianaNew Orleans20182Association for Computational Linguistics</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17. the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17AAAI Press2017</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu, CoRR, abs/2104.098642021</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Graph Attention Networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, 2018International Conference on Learning Representations</p>
<p>Wikidata: A free collaborative knowledgebase. Denny Vrandečić, Markus Krötzsch, 10.1145/2629489Commun. ACM. 57102014</p>
<p>Connecting the dots: A knowledgeable path generator for commonsense question answering. Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, Xiang Ren, 10.18653/v1/2020.findings-emnlp.369Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020a</p>
<p>K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou, 10.18653/v1/2021.findings-acl.121Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>AMR-to-text generation with graph transformer. Tianming Wang, Xiaojun Wan, Hanqi Jin, 10.1162/tacl_a_00297Transactions of the Association for Computational Linguistics. 82020b</p>
<p>NovaCOMET: Open commonsense foundation models with symbolic knowledge distillation. Peter West, Ronan Bras, Taylor Sorensen, Bill Lin, Liwei Jiang, Ximing Lu, Khyathi Chandu, Jack Hessel, Ashutosh Baheti, Chandra Bhagavatula, Yejin Choi, Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023Association for Computational Linguistics</p>
<p>Deep bidirectional language-knowledge graph pretraining. Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy Liang, Jure Leskovec, Advances in Neural Information Processing Systems. 2022</p>
<p>GreaseLM: Graph REA-Soning enhanced language models. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning, Jure Leskovec, International Conference on Learning Representations. 2022</p>
<p>Learning on large-scale text-attributed graphs via variational inference. Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7699 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7699</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7699</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-267069237</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.11839v1.pdf" target="_blank">AI for social science and social science of AI: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in artificial intelligence, particularly with the emergence of large language models (LLMs), have sparked a rethinking of artificial general intelligence possibilities. The increasing human-like capabilities of AI are also attracting attention in social science research, leading to various studies exploring the combination of these two fields. In this survey, we systematically categorize previous explorations in the combination of AI and social science into two directions that share common technical approaches but differ in their research objectives. The first direction is focused on AI for social science, where AI is utilized as a powerful tool to enhance various stages of social science research. While the second direction is the social science of AI, which examines AI agents as social entities with their human-like cognitive and linguistic capabilities. By conducting a thorough review, particularly on the substantial progress facilitated by recent advancements in large language models, this paper introduces a fresh perspective to reassess the relationship between AI and social science, provides a cohesive framework that allows researchers to understand the distinctions and connections between AI for social science and social science of AI, and also summarized state-of-art experiment simulation platforms to facilitate research in these two directions. We believe that as AI technology continues to advance and intelligent agents find increasing applications in our daily lives, the significance of the combination of AI and social science will become even more prominent.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7699.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7699.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit (AI Research Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented research assistant that uses LLMs to find relevant papers, summarize takeaways for a user's question, and extract key information from scholarly articles to support literature review and hypothesis formation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Elicit (LLM-based research assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Provides retrieval-augmented access to scholarly literature and uses an LLM to surface relevant papers, extract question-specific takeaways, and summarize findings to help researchers synthesize knowledge across a corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>scholarly papers / paper metadata / user query (retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>summaries, key takeaways, extracted information relevant to a user question</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>retrieval-augmented generation (user query + retrieved documents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>scholarly corpora (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Described as able to find relevant papers without perfect keyword matches and to summarize takeaways specific to user questions (no quantitative metrics reported in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes general risks when using LLMs for literature recommendation/summarization (hallucination, unreliable outputs) that apply to tools like Elicit.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI for social science and social science of AI: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7699.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7699.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Park2023b GPT‑4 hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can ChatGPT be used to generate scientific hypotheses (Park et al., 2023b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical investigation using GPT-4 to generate scientific hypotheses; the study finds LLMs can structure large amounts of scientific knowledge and propose interesting, testable hypotheses but with a high error rate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can chat gpt be used to generate scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can chat gpt be used to generate scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Y. J. Park et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GPT-4 hypothesis generation (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompted GPT-4 to propose scientific hypotheses by structuring knowledge across scientific domains; outputs were analyzed qualitatively to assess novelty and testability.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>topic descriptions / prompts (literature-derived context implied)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual scientific hypotheses and structured hypothesis statements</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>interactive prompting and adversarial dialogue (survey references adversarial dialogue as a strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>qualitative human assessment (error rate reported); novelty/testability judged by authors</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM (GPT-4) could effectively structure scientific knowledge and provide interesting, testable hypotheses, but exhibited a high error (incorrect/hallucinated) rate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High error/hallucination rate; outputs require careful verification; prompt-sensitivity; limited context length noted in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI for social science and social science of AI: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7699.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7699.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Banker2023 fine-tuned GPT‑3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-assisted social psychology hypothesis generation (Banker et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study that fine-tuned GPT-3 to generate psychological hypotheses and evaluated them with domain experts, finding model-generated hypotheses were not mere reproductions and matched human hypotheses on clarity, impact, and originality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine-assisted social psychology hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Machine-assisted social psychology hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>S. Banker et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fine-tuned GPT-3 hypothesis generator (Banker et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A domain-adapted (fine-tuned) GPT-3 variant generates psychological hypotheses; outputs were rated by human experts on dimensions like clarity, impact, and originality.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>psychology domain prompts / possibly domain-specific fine-tuning data</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual hypotheses (psychological hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>fine-tuning plus prompted generation; human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>human expert ratings on clarity, impact, originality</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Generated hypotheses were judged by 50 psychology experts to be comparable to human-generated hypotheses in clarity, impact, and originality and were not trivial reproductions of prior human hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey does not report numerical metrics beyond comparative expert assessments; general LLM limitations (hallucination, prompt sensitivity) apply.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI for social science and social science of AI: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7699.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7699.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tang2023 Less‑likely brainstorming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Less likely brainstorming: Using language models to generate alternative hypotheses (Tang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses LLMs to produce 'less likely' alternative hypotheses to help humans explore problems more comprehensively and reduce cognitive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Less likely brainstorming: Using language models to generate alternative hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Less likely brainstorming: Using language models to generate alternative hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>L. Tang et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Less‑likely brainstorming (LLM-assisted alternative hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLM prompts to generate alternative or low-probability hypotheses that humans might overlook, thereby broadening hypothesis spaces and mitigating human cognitive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>problem description / prompts (human-provided context)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>ranked or unranked list of alternative hypotheses (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>targeted prompting to elicit 'less likely' responses; human-in-the-loop selection</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>qualitative assessment of usefulness in aiding human exploration; human-centered evaluation implied</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported to assist humans in examining problems more comprehensively and reducing cognitive bias; specific quantitative metrics not provided in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potential for hallucination and low factual grounding; depends on prompt design and human vetting.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI for social science and social science of AI: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7699.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7699.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang2023b ChatGPT boolean queries</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can ChatGPT write a good Boolean query for systematic review literature search? (Wang et al., 2023b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation of ChatGPT's ability to formulate and refine Boolean queries for systematic literature searches, finding competitive precision but lower recall versus automated baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can chatgpt write a good boolean query for systematic review literature search?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can chatgpt write a good boolean query for systematic review literature search?.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>S. Wang, H. Scells, B. Koopman, G. Zuccon</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChatGPT Boolean query generation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses ChatGPT to formulate and iteratively refine Boolean search queries for systematic reviews; compared performance to automated Boolean-query-generation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>research question / topic descriptions (user prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Boolean search queries (structured search strings)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>guided prompting; iterative refinement versus single-prompt strategies</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>precision and recall of retrieved literature (compared to state-of-the-art automated query generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ChatGPT compared favorably on precision relative to state-of-the-art automated Boolean query methods but had lower recall; guided prompts outperform single-prompt strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower recall (misses relevant papers); high sensitivity to prompt design; risk of unreliable or incomplete search coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI for social science and social science of AI: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7699.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7699.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aydin2022 ChatGPT literature review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI ChatGPT generated literature review: Digital twin in healthcare (Aydın & Karaarslan, 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An application of ChatGPT to paraphrase abstracts and answer questions to automatically generate a literature review in a specific domain (digital twin in healthcare).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openai chatgpt generated literature review: Digital twin in healthcare.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Openai chatgpt generated literature review: Digital twin in healthcare.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ö. Aydın, E. Karaarslan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChatGPT-assisted literature review (paraphrase + Q&A)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses ChatGPT to paraphrase paper abstracts and answer targeted questions to synthesize a literature review automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>paper abstracts / targeted questions / prompts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>paraphrased abstracts, synthesized literature-review text</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Q&A prompting and paraphrasing prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Demonstrated automatic generation of literature-review style prose in the examined domain; survey notes this work has attracted citations but highlights reliability concerns when depending solely on LLMs for literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey cautions about risk of unreliable/generated (hallucinated) information; direct reliance is currently infeasible without verification.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI for social science and social science of AI: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can chat gpt be used to generate scientific hypotheses. <em>(Rating: 2)</em></li>
                <li>Machine-assisted social psychology hypothesis generation. <em>(Rating: 2)</em></li>
                <li>Less likely brainstorming: Using language models to generate alternative hypotheses. <em>(Rating: 2)</em></li>
                <li>Can chatgpt write a good boolean query for systematic review literature search?. <em>(Rating: 2)</em></li>
                <li>Openai chatgpt generated literature review: Digital twin in healthcare. <em>(Rating: 2)</em></li>
                <li>Automated literature mining and hypothesis generation through a network of medical subject headings. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7699",
    "paper_id": "paper-267069237",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "Elicit",
            "name_full": "Elicit (AI Research Assistant)",
            "brief_description": "A retrieval-augmented research assistant that uses LLMs to find relevant papers, summarize takeaways for a user's question, and extract key information from scholarly articles to support literature review and hypothesis formation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "Elicit (LLM-based research assistant)",
            "method_description": "Provides retrieval-augmented access to scholarly literature and uses an LLM to surface relevant papers, extract question-specific takeaways, and summarize findings to help researchers synthesize knowledge across a corpus.",
            "input_type": "scholarly papers / paper metadata / user query (retrieval-augmented)",
            "output_type": "summaries, key takeaways, extracted information relevant to a user question",
            "prompting_technique": "retrieval-augmented generation (user query + retrieved documents)",
            "model_name": "",
            "model_size": "",
            "datasets_used": "scholarly corpora (unspecified)",
            "evaluation_metric": "",
            "reported_results": "Described as able to find relevant papers without perfect keyword matches and to summarize takeaways specific to user questions (no quantitative metrics reported in this survey).",
            "limitations": "Survey notes general risks when using LLMs for literature recommendation/summarization (hallucination, unreliable outputs) that apply to tools like Elicit.",
            "counterpoint": null,
            "uuid": "e7699.0",
            "source_info": {
                "paper_title": "AI for social science and social science of AI: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Park2023b GPT‑4 hypothesis generation",
            "name_full": "Can ChatGPT be used to generate scientific hypotheses (Park et al., 2023b)",
            "brief_description": "An empirical investigation using GPT-4 to generate scientific hypotheses; the study finds LLMs can structure large amounts of scientific knowledge and propose interesting, testable hypotheses but with a high error rate.",
            "citation_title": "Can chat gpt be used to generate scientific hypotheses.",
            "mention_or_use": "mention",
            "paper_title": "Can chat gpt be used to generate scientific hypotheses.",
            "authors": "Y. J. Park et al.",
            "year": 2023,
            "method_name": "GPT-4 hypothesis generation (Park et al.)",
            "method_description": "Prompted GPT-4 to propose scientific hypotheses by structuring knowledge across scientific domains; outputs were analyzed qualitatively to assess novelty and testability.",
            "input_type": "topic descriptions / prompts (literature-derived context implied)",
            "output_type": "textual scientific hypotheses and structured hypothesis statements",
            "prompting_technique": "interactive prompting and adversarial dialogue (survey references adversarial dialogue as a strategy)",
            "model_name": "GPT-4",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "qualitative human assessment (error rate reported); novelty/testability judged by authors",
            "reported_results": "LLM (GPT-4) could effectively structure scientific knowledge and provide interesting, testable hypotheses, but exhibited a high error (incorrect/hallucinated) rate.",
            "limitations": "High error/hallucination rate; outputs require careful verification; prompt-sensitivity; limited context length noted in survey.",
            "counterpoint": true,
            "uuid": "e7699.1",
            "source_info": {
                "paper_title": "AI for social science and social science of AI: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Banker2023 fine-tuned GPT‑3",
            "name_full": "Machine-assisted social psychology hypothesis generation (Banker et al., 2023)",
            "brief_description": "A study that fine-tuned GPT-3 to generate psychological hypotheses and evaluated them with domain experts, finding model-generated hypotheses were not mere reproductions and matched human hypotheses on clarity, impact, and originality.",
            "citation_title": "Machine-assisted social psychology hypothesis generation.",
            "mention_or_use": "mention",
            "paper_title": "Machine-assisted social psychology hypothesis generation.",
            "authors": "S. Banker et al.",
            "year": 2023,
            "method_name": "Fine-tuned GPT-3 hypothesis generator (Banker et al.)",
            "method_description": "A domain-adapted (fine-tuned) GPT-3 variant generates psychological hypotheses; outputs were rated by human experts on dimensions like clarity, impact, and originality.",
            "input_type": "psychology domain prompts / possibly domain-specific fine-tuning data",
            "output_type": "textual hypotheses (psychological hypotheses)",
            "prompting_technique": "fine-tuning plus prompted generation; human expert evaluation",
            "model_name": "GPT-3 (fine-tuned)",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "human expert ratings on clarity, impact, originality",
            "reported_results": "Generated hypotheses were judged by 50 psychology experts to be comparable to human-generated hypotheses in clarity, impact, and originality and were not trivial reproductions of prior human hypotheses.",
            "limitations": "Survey does not report numerical metrics beyond comparative expert assessments; general LLM limitations (hallucination, prompt sensitivity) apply.",
            "counterpoint": false,
            "uuid": "e7699.2",
            "source_info": {
                "paper_title": "AI for social science and social science of AI: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Tang2023 Less‑likely brainstorming",
            "name_full": "Less likely brainstorming: Using language models to generate alternative hypotheses (Tang et al., 2023)",
            "brief_description": "A method that uses LLMs to produce 'less likely' alternative hypotheses to help humans explore problems more comprehensively and reduce cognitive bias.",
            "citation_title": "Less likely brainstorming: Using language models to generate alternative hypotheses.",
            "mention_or_use": "mention",
            "paper_title": "Less likely brainstorming: Using language models to generate alternative hypotheses.",
            "authors": "L. Tang et al.",
            "year": 2023,
            "method_name": "Less‑likely brainstorming (LLM-assisted alternative hypothesis generation)",
            "method_description": "Uses LLM prompts to generate alternative or low-probability hypotheses that humans might overlook, thereby broadening hypothesis spaces and mitigating human cognitive biases.",
            "input_type": "problem description / prompts (human-provided context)",
            "output_type": "ranked or unranked list of alternative hypotheses (textual)",
            "prompting_technique": "targeted prompting to elicit 'less likely' responses; human-in-the-loop selection",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "qualitative assessment of usefulness in aiding human exploration; human-centered evaluation implied",
            "reported_results": "Reported to assist humans in examining problems more comprehensively and reducing cognitive bias; specific quantitative metrics not provided in the survey summary.",
            "limitations": "Potential for hallucination and low factual grounding; depends on prompt design and human vetting.",
            "counterpoint": null,
            "uuid": "e7699.3",
            "source_info": {
                "paper_title": "AI for social science and social science of AI: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Wang2023b ChatGPT boolean queries",
            "name_full": "Can ChatGPT write a good Boolean query for systematic review literature search? (Wang et al., 2023b)",
            "brief_description": "An evaluation of ChatGPT's ability to formulate and refine Boolean queries for systematic literature searches, finding competitive precision but lower recall versus automated baselines.",
            "citation_title": "Can chatgpt write a good boolean query for systematic review literature search?.",
            "mention_or_use": "mention",
            "paper_title": "Can chatgpt write a good boolean query for systematic review literature search?.",
            "authors": "S. Wang, H. Scells, B. Koopman, G. Zuccon",
            "year": 2023,
            "method_name": "ChatGPT Boolean query generation",
            "method_description": "Uses ChatGPT to formulate and iteratively refine Boolean search queries for systematic reviews; compared performance to automated Boolean-query-generation methods.",
            "input_type": "research question / topic descriptions (user prompts)",
            "output_type": "Boolean search queries (structured search strings)",
            "prompting_technique": "guided prompting; iterative refinement versus single-prompt strategies",
            "model_name": "ChatGPT",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "precision and recall of retrieved literature (compared to state-of-the-art automated query generation)",
            "reported_results": "ChatGPT compared favorably on precision relative to state-of-the-art automated Boolean query methods but had lower recall; guided prompts outperform single-prompt strategies.",
            "limitations": "Lower recall (misses relevant papers); high sensitivity to prompt design; risk of unreliable or incomplete search coverage.",
            "counterpoint": true,
            "uuid": "e7699.4",
            "source_info": {
                "paper_title": "AI for social science and social science of AI: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Aydin2022 ChatGPT literature review",
            "name_full": "OpenAI ChatGPT generated literature review: Digital twin in healthcare (Aydın & Karaarslan, 2022)",
            "brief_description": "An application of ChatGPT to paraphrase abstracts and answer questions to automatically generate a literature review in a specific domain (digital twin in healthcare).",
            "citation_title": "Openai chatgpt generated literature review: Digital twin in healthcare.",
            "mention_or_use": "mention",
            "paper_title": "Openai chatgpt generated literature review: Digital twin in healthcare.",
            "authors": "Ö. Aydın, E. Karaarslan",
            "year": 2022,
            "method_name": "ChatGPT-assisted literature review (paraphrase + Q&A)",
            "method_description": "Uses ChatGPT to paraphrase paper abstracts and answer targeted questions to synthesize a literature review automatically.",
            "input_type": "paper abstracts / targeted questions / prompts",
            "output_type": "paraphrased abstracts, synthesized literature-review text",
            "prompting_technique": "Q&A prompting and paraphrasing prompts",
            "model_name": "ChatGPT",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Demonstrated automatic generation of literature-review style prose in the examined domain; survey notes this work has attracted citations but highlights reliability concerns when depending solely on LLMs for literature synthesis.",
            "limitations": "Survey cautions about risk of unreliable/generated (hallucinated) information; direct reliance is currently infeasible without verification.",
            "counterpoint": true,
            "uuid": "e7699.5",
            "source_info": {
                "paper_title": "AI for social science and social science of AI: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can chat gpt be used to generate scientific hypotheses.",
            "rating": 2,
            "sanitized_title": "can_chat_gpt_be_used_to_generate_scientific_hypotheses"
        },
        {
            "paper_title": "Machine-assisted social psychology hypothesis generation.",
            "rating": 2,
            "sanitized_title": "machineassisted_social_psychology_hypothesis_generation"
        },
        {
            "paper_title": "Less likely brainstorming: Using language models to generate alternative hypotheses.",
            "rating": 2,
            "sanitized_title": "less_likely_brainstorming_using_language_models_to_generate_alternative_hypotheses"
        },
        {
            "paper_title": "Can chatgpt write a good boolean query for systematic review literature search?.",
            "rating": 2,
            "sanitized_title": "can_chatgpt_write_a_good_boolean_query_for_systematic_review_literature_search"
        },
        {
            "paper_title": "Openai chatgpt generated literature review: Digital twin in healthcare.",
            "rating": 2,
            "sanitized_title": "openai_chatgpt_generated_literature_review_digital_twin_in_healthcare"
        },
        {
            "paper_title": "Automated literature mining and hypothesis generation through a network of medical subject headings.",
            "rating": 1,
            "sanitized_title": "automated_literature_mining_and_hypothesis_generation_through_a_network_of_medical_subject_headings"
        }
    ],
    "cost": 0.01757725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI for Social Science and Social Science of AI: A Survey
22 Jan 2024</p>
<p>Ruoxi Xu 
School of Electronic
Electrical and Communication Engineering
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences
BeijingChina</p>
<p>Yingfei Sun yfsun@ucas.ac.cn 
School of Electronic
Electrical and Communication Engineering
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Mengjie Ren renmengjie2021@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences
BeijingChina</p>
<p>Shiguang Guo guoshiguang2021@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences
BeijingChina</p>
<p>Ruotong Pan panruotong2021@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences
BeijingChina</p>
<p>Hongyu Lin hongyu@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences
BeijingChina</p>
<p>Le Sun sunle@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences
BeijingChina</p>
<p>State Key Laboratory of Computer Science
Institute of Software
Chinese Academy of Sciences
BeijingChina</p>
<p>Xianpei Han xianpei@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences
BeijingChina</p>
<p>State Key Laboratory of Computer Science
Institute of Software
Chinese Academy of Sciences
BeijingChina</p>
<p>AI for Social Science and Social Science of AI: A Survey
22 Jan 2024685E778ABCD54243EEEED6238AD855AFarXiv:2401.11839v1[cs.CL]Preprint submitted to ElsevierSocial Science Large Language Models AI Simulation
Recent advancements in artificial intelligence, particularly with the emergence of large language models (LLMs), have sparked a rethinking of artificial general intelligence possibilities.The increasing human-like capabilities of AI are also attracting attention in social science research, leading to various studies exploring the combination of these two fields.In this survey, we systematically categorize previous explorations in the combination of AI and social science into two directions that share common technical approaches but differ in their research objectives.The first direction is focused on AI for social science, where AI is utilized as a powerful tool to enhance various stages of social science research.While the second direction is the social science of AI, which examines AI agents as social entities with their human-like cognitive and linguistic capabilities.By conducting a thorough review, particularly on the substantial progress facilitated by recent advancements in large language models, this paper introduces a fresh perspective to reassess the relationship between AI and social science, provides a cohesive framework that allows researchers to understand the distinctions and connections between AI for social science and social science of AI, and also summarized state-of-art experiment simulation platforms to facilitate research in these two directions.We believe that as AI technology continues to advance and intelligent agents find increasing applications in our daily lives, the significance of the combination of AI and social science will become even more prominent.</p>
<p>Introduction</p>
<p>Building machines that can think, learn and create is the fundamental pursuit of artificial intelligence (AI) (Russell, 2010).How to develop machines with general intelligence comparable to, or even greater than, that of human beings has never lost its appeal (Goertzel, 2014).Recently, significant advancements have been made in the AI field (Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et al., 2023), particularly with the emergence of large language models (LLMs) such as ChatGPT and GPT-4 (OpenAI, 2023).These developments have led to the rethinking of the possibilities of artificial general intelligence (AGI) (Zhao et al., 2023).</p>
<p>The increasing human-like capabilities of AI are also attracting attention in social science research.There have been numerous studies exploring the combination of AI and social science (Bail, 2023;Ziems et al., 2023;Chen, 2023a).Along these lines, many novel research directions have been explored, including research tasks proposing (Park et al., 2023b;Banker et al., 2023), social science simulation (Brand et al., 2023;Kjell et al., 2023;Chu et al., 2023), AI agent governing (Li et al., 2023;Jiang et al., 2023;Miotto et al., 2022) and so on.</p>
<p>Despite the large number of related studies, the existing studies tend to concentrate on one specific instance of AI and social science intersection, thereby lacking a unified perspective to effectively distinguish and outline AI's role in social science research and its own social characteristics.In reality, the combination of AI and social science can be divided into two distinct directions.On the one hand, the superior performance of AI allows them to serve as effective tools for social science research, such as using AI for literature searching and reviewing (McGee, 2023b), proposing questions and hypotheses (Park et al., 2023b;Banker et al., 2023), analyzing data (Ziems et al., 2023), assisting with writing (Dergaa, Chamari, Zmijewski and Saad, 2023;Chen, 2023b), and more.Systematically outlining the potential applications of AI in different phases of social science research can provide a valuable guide Figure 1: Overview of the intersection of AI and social science.We have separately discussed "AI for social science" which summarizes the application of AI at every stage of social science research to provide guidance on tool selection for researchers, "social science of AI" which systematically describes the intelligence level and characteristics of AI agents from a social science perspective on different sub-disciplines, and "public tools and resources" which focus on simulation tools.These fields share technical methodologies to some extent, yet they possess distinct research subjects and objectives.</p>
<p>for researchers to choose appropriate research tools.We refer to this direction as AI for social science in this paper.On the other hand, just as early myths and parables emphasized the social and ethical questions around human-created Figure 2: Computer simulation respectively in the context of "AI for social science" and "social science of AI".For "AI for social science", AI agents are deployed to mimic human behaviors to enhance the understanding of human society.Conversely, "social science of AI" delves into AI agents' own social questions.</p>
<p>intelligence (McCorduck and Cfe, 2004;Kieval, 1997;Pollin, 1965), today's intelligent machines present their own interesting social questions (Frank, Wang, Cebrian and Rahwan, 2019) and expanding research starts to explore and understand AI agents as social entities.Particularly, current AI agents, especially large language model agents, are exhibiting cognitive, logical reasoning, and linguistic capabilities on par with or even surpassing those of humans, along with unique behavioral characteristics (OpenAI, 2023).Communities constituted by AI agents also exhibit emergent behaviors similar to human societies (Park et al., 2023a).This provides an interesting case for attempting to extend the social science to more universal phenomena of machines (Klein and Kleinman, 2002) and also presents a valuable opportunity to reevaluate a fundamental axiom in social science: human behavior can be understood as possessing unique social characteristics (Woolgar, 1985).Exploring AI from a social science perspective can also provide crucial insights and guidance to make AI development more congruent with societal needs and human values.We refer to this direction as social science of AI in this paper.There is an important point to note, the term "social science" as used in this paper extend its traditional definition.It is used in a broader sense to provide a research perspective for describing certain high-level behaviors of humans or models, rather than equating it with actual human social behaviors.</p>
<p>Although these directions share common technological approaches, they have distinct research objectives, significance, and scopes of application.For example in Figure 2, using AI agents for simulation serves as a technical method that could apply to both directions, but with differing objectives.When used for the former, the researcher's aim is to align the behavior of AI agents as closely with human behavior as possible, in order to study the operational laws of human society in a cost-effective, fast, and ethically risk-averse manner (Park et al., 2022;Salehi et al., 2022).When used for the latter, the objective is to explore the behavioral laws of AI itself, with a particular focus on its unique aspects, especially those differing from the operational laws of human society (Guo, 2023).The absence of surveys from the above two perspectives makes it hard to ground each work's research significance and application scope, hindering us to comprehend and harness the distinctions and connections between these two directions.A joint analysis of their research progress can help to grasp the big picture of the current state of the combination of AI and social science.</p>
<p>To this end, we conduct a comprehensive review from these two directions respectively.We conduct a joint analysis of their research progress, comparing their similarities and differences to present an overview of the current state of the combination of AI and social science.Considering that recent remarkable advancements in this field can be largely attributed to the development of large language models (Zhao et al., 2023), this paper narrows its scope to the combination of large language models and social sciences, approaching the topic from both the angle of AI for social science and the social science of AI.The main organization of this survey is summarized in Figure 1.Specifically, from the angle of AI for social science, we discuss large language models' potential as a highly efficient tool that can be integrated into existing research methodologies, significantly enhancing the efficiency of social science research.To achieve this, we structure the content according to the roles that AI plays in both the stages of hypothesis generation and verification within the social science process (Donovan and Hoover, 2013;Bryman, 2016).For hypothesis generation, we mainly focus on how AI can help human beings in literature reviewing and hypothesis proposing.For hypothesis verification stage, we respectively examine how large language models function in various research methods such as experiment research, survey research and non-reactive research.From the perspective of the social science of AI, we are referring to a broad field of social science that focuses on regarding large language models as its research subject.We categorize the behavioral studies of these models according to each subfield within social science, following academic categorization.More specifically, we have compiled the behavioral laws of large language models by examining them from the viewpoints of psychology, sociology, economics, politics, and linguistics.Additionally, we have also compiled a summary of the currently available tools in this field to facilitate research in the aforementioned areas.These platforms utilize large language models as agents and allows for the setting and implementation of intervention conditions to simulate diverse social situations, interactions, and behaviors.They serve the purpose of simulating human behavior for studying human societies, as well as exploring AI societies, thus catering to both of the above directions.</p>
<p>Generally, we summarize our contributions as follows:</p>
<p>• We present a perspective of revisiting AI and social science combinations from two directions: AI for social science and social science of AI.We elaborate on the connections and distinctions between these two directions, grounding the research value and application scope of relevant work.</p>
<p>• Based on the substantial progress facilitated by recent advancements in large language models to these two directions, we conduct a literature review, which summarizes the research landscape, discusses the limitations of the existing research, and sheds light on potential future directions in the combination of AI and social science.</p>
<p>• We collect and compare existing open source large language model-based simulation tools.These platforms can serve an effective foundation to facilitate the future researches of the above-mentioned two directions.</p>
<p>The structure of this survey is organized as follows: Section 3 introduces the application of large language models in social science research, while Section 4 delves into social science research that takes large language models as the subject of study.Section 5 provides information about the resources and tools available.Finally, we conclude the survey in Section 6, summarizing the main findings and discussing the remaining issues for future work.</p>
<p>Background</p>
<p>Nowadays, the AI community, and even the whole society, is witnessing the significant impacts brought about by large language models.Large language models typically refer to transformer language models that contain hundreds of billions (or more) of parameters (Shanahan, 2022), which are trained on massive text data.Notable examples include GPT-3 (Brown et al., 2020), PaLM (Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et al., 2022), LLaMA (Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave and Lample, 2023) and OpenAI's ChatGPT, which amassed 100 million users in less than two months, setting a new record in history.These models have exhibited strong capacities to understand natural language and solve complex tasks (via text generation), capturing the attention and imagination of investors, consumers, and organizations.</p>
<p>Improvements in large language models have been so fast, and the potential societal repercussions so profound, that a broad cross-disciplinary lens from computer science and social science is necessary to start making sense of the implications.Firstly, the ability of large language models to generate texts for a broad range of tasks via an intuitive natural language interface may hold great promise for social science research.This has opened up avenues for various applications, including literature searching and reviewing (McGee, 2023b), proposing questions and hypotheses (Park et al., 2023b;Banker et al., 2023), analyzing data (Ziems et al., 2023), assisting with writing (Dergaa et al., 2023;Chen, 2023b), and more.Secondly, the evolution of large language models has significantly enhanced their capacity to exhibit human-like characteristics, leading to a surge in research regarding LMs as representations of human entities (Krishna, Lee, Fei-Fei and Bernstein, 2022;Andreas, 2022;Park et al., 2022).Research includes exploring the collaborative capabilities of large language models in complex tasks (Irving, Christiano and Amodei, 2018), developing "generative agents" to investigate emergent social behaviors (Park et al., 2023a), employing GPT-3-based agents as substitutes for human participants (Aher et al., 2023) and so on.Thirdly, the advancements in large language models have prompted a reconsideration of the ethical and societal issues it may entail.These previous works have highlighted the transformative</p>
<p>Data Analysis</p>
<p>Figure 3: The application of large language models at every stage of social science research.Large language models offer new possibilities for improving existing social science research processes and automated science, but also bring new potential risks and ethical issues.Social science researchers should carefully consider whether and how to apply large language models in their research.</p>
<p>effects brought about by the emergence of large language models in the intersection of AI and social science.Our survey aims to provide a comprehensive overview of these developments and address the existing limitations and potential of this field.</p>
<p>AI for social science</p>
<p>AI for social science refers to the application of AI in traditional social science research.Unlike the social science of AI, this section emphasizes large language models' human-like intelligence, which can mimic human behavior to help social science research.In this section, we will draw upon the research paradigm outlined in (Donovan and Hoover, 2013;Bryman, 2016), and discuss the application of large language models as multi-purpose tools at every stage of social science research as shown in Figure 3.This aims to provide a comprehensive and informed perspective for social science researchers on how to apply large language models in the process of their research to enhance efficiency, while also revealing the untapped potential of large language models, warning about potential risks and ethical issues, and indicating possible future directions in their application.</p>
<p>Hypothesis Generation</p>
<p>Hypothesis generation, which serves as the foundation and initial step of social science research, is the task of mining meaningful implicit associations between unrelated social science concepts (Jha, Xun, Wang and Zhang, 2019).In the early days, hypothesis generation was largely driven by brainstorming of researchers, drawing inspiration from existing theories, patterns of anomalies in data or cross-disciplinary connections that involved serendipitous discoveries (Jaccard and Jacoby, 2019).However, as the volume of research literature grows, researchers have started to explore quicker and more efficient methods of generating solid hypotheses (Evans and Rzhetsky, 2010;Krenn and Zeilinger, 2020;Wilson, Wilkins, Holt, Choi, Konecki, Lin, Koire, Chen, Kim, Wang et al., 2018).In the following, we will present some attempts to accomplish hypothesis generation tasks using large language models.</p>
<p>Literature Review is the understanding, summarization, and critical thinking about the academic literature on a specific topic.Researchers have been exploring the use of large language models to assist in literature review.Some studies leverage large language models to aid in searching for relevant literature.Wang, Scells, Koopman and Zuccon (2023b) used ChatGPT to formulate and refine Boolean queries for systematic reviews, finding that ChatGPT compares favourably with the current state-of-the-art automated Boolean query generation methods in terms of precision, at the expenses of a lower recall.The paper also found that guided prompts lead to higher effectiveness than single prompt strategies.Other works focus on enabling large language models to read articles and automatically summarize the key points within them.Aydın and Karaarslan (2022) used ChatGPT to paraphrase the abstracts of relevant papers and answer questions to automatically generate literature review, which has been cited over 100 times.Elicit1 is also one typical application, an AI Research Assistant based on large language models, which can find relevant papers without perfect keyword match, summarize takeaways from the paper specific to your question, and extract key information from the papers.However, it should be noted that directly relying on large language models for literature recommendation and summarization is currently infeasible due to the risk of unreliable papers and related information being generated (Haman and Školník, 2023).</p>
<p>Hypothesis Propose is to propose possible explanations to some phenomenon or event.Researchers from various fields have made attempts to use large language models to help with this task.For example, Park et al. (2023b) used GPT-4 to generate scientific hypotheses and draw the conclusion that current large language models seem to be able to effectively structure vast amounts of scientific knowledge and provide interesting and testable hypotheses while the error rate is high.Banker et al. (2023) utilized a fine-tuned version of GPT-3 to generate psychological hypotheses and engaged 50 psychology experts to evaluate their quality, revealing that the model's generated hypotheses were not mere replicas of previously generated human hypotheses, and exhibited no significant differences in terms of clarity, impact, and originality compared to human-generated hypotheses.Tang, Peng, Wang, Ding, Durrett and Rousseau (2023) employed large language models to generate "less likely" hypotheses, effectively assisting humans in comprehensively examining problems and eliminating cognitive biases caused by their own knowledge and experience.At present, the application of large language models in hypothesis generation is still somewhat rudimentary.Strategies for enhancing hypothesis quality include fine-tuning the large language models within specific domains (Banker et al., 2023), stepwise questioning (Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou et al., 2022), adversarial dialogue (Park et al., 2023b) and so on.</p>
<p>Conclusion</p>
<p>Current research on using large language models for hypothesis generation focuses on exploring its feasibility and validity, and has commonly unveiled promising prospects and potential of employing large language models for this task.The solution mainly involves interactive design of prompts.Users input the topic they want to review.Based on these inputs, large language model automates the process of formulating and refining boolean queries, extracting core points from the search results, and generating hypotheses about potential relationships among the objects of interest.</p>
<p>Compared to traditional methods, the advantages mainly lie in their exceptional performance in language understanding and generation, enabling them quickly analyze existing research, identify knowledge gaps, and establish connections between seemingly unrelated ideas (Dahmen, Kayaalp, Ollivier, Pareek, Hirschmann, Karlsson and Winkler, 2023).This provides them with a natural advantage in language-based disciplines such as psychology (Banker et al., 2023).</p>
<p>However, researchers need to be noted that currently, large language models can only be used as auxiliary and inspirational tools in the early stage of research, and have the following limitations: 1) Fabricated or incorrect information, which may mislead users.This is because large language models lack of understanding regarding the validity of output content and simply spill out them without clear rationalization (Park et al., 2023b).2) High sensitivity to the prompt, which results in a significant investment of effort in prompt design but yields uncertain outcomes (Wang et al., 2023b).3) Limited context length, which makes it hard to handle long and multiple documents.</p>
<p>In order to better harness the potential of large language models in hypothesis generation, future directions that we may consider include: 1) Integrating specialized domain knowledge, by retrieval augmented techniques or domain-specific training.This would help reduce hallucination.2) Developing high-reward prompt strategies.This could involve considering novel prompt generation techniques or reward mechanisms to guide the model's hypothesis generation process.3) Expanding the context windows of the large language models.By allowing the models to The comparisons between traditional methods and large language models as tools at every stage of social science research.</p>
<p>The advantages are marked in bold.From the table, we can easily find that although large language models are more advantageous on cost, speed, generality and accessibility across various research stages, the critical current limitations of validity, possible ethical risks and lack of domain knowledge still hinder its real-word applications.Please note that the comparisons in this study are only valid until the publication of this paper.Given the rapid advancements in technology, LLM agents may overcome some of the current shortcomings that are challenging to address.</p>
<p>consider a larger context, they would have access to more comprehensive information, potentially leading to more robust and insightful hypotheses.</p>
<p>Hypothesis Verification</p>
<p>Once the research topic and hypotheses are established, social science researchers engage in hypothesis verification.This process involves collecting and analyzing data to provide evidence that either supports or refutes the proposed hypotheses (Donovan and Hoover, 2013).In traditional social science research, hypothesis verification typically falls into quantitative methods like experimental research, survey research and nonreactive research, as well as qualitative methods such as field research and historical-comparative research (Juren Lin, 2017;Yuan, 2013;Bryman, 2016).Given that large language models are currently limited in their applicability to qualitative research, we primarily discuss the role of large language models in quantitative methods.</p>
<p>Experiment Research</p>
<p>A laboratory experiment is "an inquiry for which the investigator plans, builds,or otherwise controls the conditions under which phenomena are observed and measured" (Willer and Walker, 2007).The common practice involves manipulating conditions for some research participants while leaving them unaltered for others, aiming to compare the responses across groups to uncover consistent behavioral patterns.In the following, we will explore the applications of large language models in experimental research.</p>
<p>Experiment Assistant refers to the use of large language models in social science experiments to automate some simple but labor-intensive tasks that would normally be done by researchers.For instance, they can assist in creating hypothetical scenarios iteratively with feedback from researchers (Bail, 2023), which can enhance the external validity and comparability of the experimental conditions.Besides, large language models are capable of synthesizing the necessary information for experiments, eliminating the need for the use of real-life information utilization.This safeguards the privacy of individuals whose information could potentially be used in these studies.</p>
<p>Experiment Simulation aims to design a platform to explore, optimize, and predict behaviors of complex systems that might be challenging to investigate in the real world.In simulation experiments, large language models are usually used as believable proxies of human behavior (Aher et al., 2023;Park et al., 2023a).For example, Park et al. (2022) provides a typical application where large language models are utilized to simulate the behavior of community users, assisting designers in gaining insights into the various possibilities of social interactions and identifying potential edge cases that could lead to the breakdown of a community.Horton (2023) considers GPT-3 AIs as homo silicus agents, and demonstrates their ability to qualitatively recover findings from three classic behavioral economics experiments with real humans.Guo (2023) designs well-crafted prompts to enable GPT agents to participate in strategic game experiments and achieve realistic performance.Park et al. (2023a) constructed a fully large language model-driven simulated community, where they observed human-like individual behaviors and emergent behaviors.</p>
<p>Conclusion</p>
<p>In experimental research, large language models can serve dual roles -they can act as experiment assistants and as believable proxies of human behavior, becoming subjects of the experiment themselves.The latter, in particular, has attracted increasing attention in both AI and social science as large language models are increasingly capable of simulating human-like responses and behaviors.Currently, the design of AI agents is still relatively crude, usually including four modules: profile, memory, planning, and action (Wang, Ma, Feng, Zhang, Yang, Zhang, Chen, Tang, Chen, Lin et al., 2023a), and warrants further improvement.</p>
<p>Using large language models for simulating experiments offers several advantages: 1) Improved efficiency, reduced costs, and enhanced scalability (Bybee, 2023;Guo, 2023).2) Circumventing the ethical issues associated with human subjects.This opens the door to experiments that may be deemed unethical if performed on humans, such as the classic Stanford Prison Experiment (Zimbardo, Haney, Banks and Jaffe, 1971).</p>
<p>However, social scientists must also proceed with caution in this area, taking into account the following limitations: 1) Uncertain believability.There is now no "gold standard" study demonstrating that groups of automated agents can accurately simulate humans (Bail, 2023).2) Low transparency and reproducibility.Since large language models themselves are still black boxes, we cannot provide a thorough explanation of their behavior.</p>
<p>In order to address the aforementioned limitations, potential future directions include: 1) developing methods for evaluating the quality of large language models simulations, and 2) incorporating insights from cognitive science to guide the development of AI agent frameworks and enhance their behavior's human-likeness and rationality.</p>
<p>Survey Research</p>
<p>Survey research is a fundamental methodology in social science, which uses written questionnaires or formal interviews to collect information on the beliefs, opinions, characteristics, and past or present behaviors of a target group (Bryman, 2016).The core of modern survey research is three key components: sampling, measurement, and analysis (Wright, Marsden et al., 2010).The following will explore the role that large language models play in each stage of survey research.</p>
<p>Sampling involves selecting representative samples from human populations, whose observed characteristics provide unbiased estimates of the characteristics of those populations.Large language models present a novel option for sampling, serving as proxies for specific human subgroups.This enables the avoidance of the sampling step, or rather, utilizes the extensive training database of the large language models directly as the sample for the study.Existing studies have proposed and explored the possibility of using language models as effective stand-ins for particular human demographics in the realm of social science research.For example, Argyle, Busby, Fulda, Gubler, Rytting and Wingate (2023) compares real human participants from multiple large surveys in the United States and "silicon samples" which are created by conditioning large language models on socio-demographic backstories from them, demonstrating that the "algorithmic bias" within GPT-3 is both fine-grained and demographically correlated.Bisbee, Clinton, Dorff, Kenkel and Larson (2023) investigates the quality, reliability, and reproducibility of synthetic survey data generated by the popular closed-source large language model, ChatGPT.The experimental results suggest that the average scores generated by ChatGPT closely align with the averages from baseline survey (conducted from 2016 to 2020 on the U.S. national elections).However, when it comes to more advanced features, such as variance, subgroups, and statistical inferences, it often leads researchers to draw conclusions that differ from those relying on human respondents.Dillion, Tandon, Gu and Gray (2023) takes moral judgments as an example to investigate whether and when language models can potentially replace human participants in psychology.The analysis indicate that language models align most closely with humans when the contextual circumstances involve explicit features that drive human judgment, do not pertain to competitive situations, and when the subjects being judged are representative within the training data.Rao, Leung and Miao (2023) conducts the Myers-Briggs Type Indicator (MBTI) test on large language models agents of different subpopulations and showcases the ability of ChatGPT in analyzing personalities of different groups of people.Brand et al. (2023) interview GPT-3 to estimate customers' willingness-to-pay for products and features and find that large language models can generate responses that align with economic theories and consumer behavior patterns.Chu et al. (2023) adapts LMs to subpopulation-specific media diets and successfully simulates how the subpopulation will respond to survey questions.</p>
<p>Measurement focuses on designing questions to draw out valid and reliable responses across a broad spectrum of subjects, which are often characterized as "the art of asking questions".While it's natural to leverage large language models to assist in the design of questionnaires or interview questions, more researchers are focusing on the role large language models play in facilitating a paradigm shift in the measurement methods within survey research -from closed-ended rating scales, to open-ended response questionnaires, and then further towards more natural interactive interviews.For example, Kjell, Sikström, Kjell and Schwartz (2022) compared the results of psychological surveys using rating scales and natural language-based open-ended questionnaires.The latter was found to achieve accuracy that either exceeded or rivaled the typical methods of reliability in rating scales, which is often considered as the theoretical upper limit.Kjell et al. (2023) provides a future outlook of finer granularity and automated interactive interviews, making full use of interviewees' own words to best elicit truthful responses.</p>
<p>Analysis is a step using multivariate data analysis techniques to identify and understand the statistical relationships among various variables.Large language models can be used to analyze qualitative data, such as interview responses, to identify patterns, relationships, and common themes (Abbas, 2023).For instance, Yang, Ji, Zhang, Xie and Ananiadou (2023a) utilize large language models, specifically ChatGPT, to perform mental health analysis and highlight the significant potential of large language models in improving the interpretability of mental health analysis.However, large language models, which are not specifically designed for analyzing quantitative data, are currently not the primary method for survey research analysis in the social sciences.Instead, survey data is typically presented in the form of charts, graphs, or tables, and analyzed using statistical methods (Bryman, 2016).Future versions of the model may be able to integrate tools like Python and R libraries to conduct quantitative data analyses (Mialon, Dessì, Lomeli, Nalmpantis, Pasunuru, Raileanu, Rozière, Schick, Dwivedi-Yu, Celikyilmaz et al., 2023).</p>
<p>Conclusion</p>
<p>The current applications of large language models in survey research primarily revolve around three main directions: 1) Effective proxies for specific human sub-populations.2) Interactive interviewers.3) Result analysis tools.</p>
<p>For the first direction, the current work has demonstrated that proper conditioning will cause large language models to accurately emulate response distributions from a wide variety of human subgroups.This approach can effectively address limitations regarding the number of questions, frequency and the subpopulation due to cost constraints (Chu et al., 2023), as well as the common challenge of low response rates (Bhattacherjee, 2012) in survey research, offering significant advantages in terms of engagement and cost.However, whether and which large language models can truly represent humans remains an open question (Argyle et al., 2023).This approach fundamentally relies on "algorithmic bias," which is heavily influenced by the training data and is susceptible to producing unfair and non-objective results.In light of these considerations, we do not propose that large language models should completely replace traditional sampling methods in survey research.Instead, we see their potential in simulating population responses to assist in survey design.This hybrid approach allows us to harness the strengths of large language models while still recognizing the importance of traditional sampling techniques to maintain the integrity and fairness of survey results.</p>
<p>Several researchers have envisioned the impact of language models on the form of survey.The advantage of large language models lies in their ability to fully utilize the individuals' own language to describe the information needed by researchers, which has the potential not only to gradually improve current assessments but also to fundamentally alter the nature of measuring and describing personal states, ultimately enhancing our understanding of social science.However, utilizing large language models to facilitate measurement also poses risks.Inherent biases in large language models and the potential for data leakage must be carefully navigated when implementing large language models in research scenarios.</p>
<p>For result analysis, large language models are primarily used for text analysis and are seldom employed for numerical analysis because they are not proficient in it.Future research can consider the integration of large language models with specialized computational tools.</p>
<p>Nonreactive Research</p>
<p>Nonreactive research refers to the research method where the participants are not aware that their information is part of the study, unlike experiment research and survey research that actively engage the people we study by creating experimental conditions or directly asking questions (Juren Lin, 2017).This method may reduce bias due to interference from researchers or measurement instruments (Trochim and Donnelly, 2001).In this section, we will adhere to the taxonomy in (Juren Lin, 2017), and explore the roles that large language models play in content analysis and existing statistics analysis.</p>
<p>Content analysis is a widely used technique for examining the content contained in written documents or other communication media.The remarkable performance of large language models in various traditional NLP tasks has attracted extensive attention about using them in content analysis tasks within the field of social science.</p>
<p>Some social science researchers employ large language models to perform text classification, a basic and important task that involves labeling or categorizing texts according to predefined categories (Aggarwal and Zhai, 2012).Common text classification tasks in the field of social science include: 1) Sentiment analysis, which aims to identify and extract the emotional attitudes in the text, such as joy, anger and sorrow.It is a widely applied technique in psychology and political science.In psychology, sentiment analysis helps researchers understand people's emotional states, stress levels, and mental health conditions.Rodríguez-Ibánez, Casánez-Ventura, Castejón-Mateos and Cuenca-Jiménez (2023) suggest that large language models are the future paradigm for sentiment analysis, due to their zero-shot setting and simple invocation.However, they also point out the limitations of GPT-3 in the current tasks.ChatGPT performs excellently in three text-based mental health classification tasks, including stress detection, depression detection, and suicide detection (Lamichhane, 2023).ChatGPT also applies to differentiate paranoid texts from nonparanoid ones in some studies (Uludag, 2023).Rathje, Mirea, Sucholutsky, Marjieh, Robertson and Van Bavel (2023) evaluate the performance of GPT-3.5 and GPT-4 on multilingual sentiment and discrete emotions tasks and find that in many cases, GPT models perform close to (sometimes better than) fine-tuned machine learning models.They argue that GPT models offer a promising avenue for cross-lingual research in psychology.Wu, Irsoy, Lu, Dabravolski, Dredze, Gehrmann, Kambadur, Rosenberg and Mann (2023b) introduce BloombergGPT, a 50 billion parameter language model tailored for the financial domain, and evaluate it on two financial sentiment analysis datasets FPB (Malo, Sinha, Korhonen, Wallenius and Takala, 2013) and FiQA SA (Maia, Handschuh, Freitas, Davis, McDermott, Zarrouk and Balahur, 2018).BloombergGPT outperforms general models such as GPT-Neo (Black, Leo, Wang, Leahy and Biderman, 2021), OPT (Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang and Zettlemoyer, 2022) and BLOOM (Scao, Fan, Akiki, Pavlick, Ilić, Hesslow, Castagné, Luccioni, Yvon, Gallé et al., 2023) on both tasks.Frąckiewicz (2023) leverage ChatGPT for social network analysis, enabling fast identification of key topics, sentiments, and influencers in the network, content generation, monitoring and flagging of harmful content in the community, and bringing profound changes to social network analysis.2) Stance detection, which aims to determine the political, social or cultural stance of the author or speaker expressed in a text.This is of great significance for fields such as political analysis, public opinion monitoring, etc. Zhang, Ding and Jing (2023) applies ChatGPT to two common datasets for stance detection and achieves state-of-theart or comparable performance.Stance detection is a natural language processing task that aims to identify the attitude of a text author towards a target, such as support, oppose, or neutral.It is useful for analyzing different perspectives on social, political, or cultural issues.For example, in political analysis, public opinion monitoring, and other domains, it is important to understand people's views on certain events or claims.Wu, Nagler, Tucker and Messing (2023a) uses large language models to measure the latent ideology of politicians and scores US senators on a liberal-conservative scale by having ChatGPT choose the more liberal (or more conservative) senator in pairwise comparisons.Törnberg (2023) experiment on identifying the political party affiliation of Twitter posters and find that GPT-4 surpasses human experts and crowdsourced workers in accuracy and reliability.3) Hate speech detection, which aims to identify and filter out words, phrases or sentences that may contain hate speech in a text.Some hate speech may be expressed in subtle ways, or use multiple languages and dialects, thus posing certain challenges for hate speech detection.Huang, Kwak and An (2023b) uses ChatGPT to detect whether tweets imply hate speech, and successfully identifies 80% of the tweets containing hate speech.4) Misinformation detection, which aims to identify and filter out words, phrases or sentences that may contain misinformation in a text.Social media is the main platform for people to communicate, share and get information, but also a hotbed for misinformation dissemination.Misinformation can not only mislead the public, but also affect social trust, democratic participation and policy making.Misinformation detection can help prevent users from posting misinformation, thereby reducing the spread of false and misleading information.In the field of cancer misinformation detection, ChatGPT achieves an accuracy of 96.9% (Johnson, King, Warner, Aneja, Kann and Bylund, 2023).The answers generated by ChatGPT show no significant difference from those of the National Cancer Institute (NCI) in terms of word count or readability.Hoes, Altay and Bermeo (2023) finds that ChatGPT has a classification accuracy of 72% on fact-checking tasks, with higher accuracy for true statements.</p>
<p>Other social science researchers utilize large language models for text generation tasks, one of the most challenging and creative tasks in NLP that involves automatically producing coherent, fluent and meaningful texts based on a given input or goal (Gatt and Krahmer, 2018).The typical applications of large language models in the field of social science for text generation tasks include: 1) Natural language descriptions or explanations, which mainly aims to improve the interpretability and credibility of results.For example, Huang, Kwak and An (2023a) proposes Chain of Explanation, a method to guide large language models such as GPT-Neo, T5 (Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu, 2020), OPT and others to generate high-quality explanations for online hate speech.Although it makes significant improvements over previous methods, it still lags behind human level in terms of clarity and informativeness.Huang et al. (2023b) uses ChatGPT to explain whether tweets imply hate speech, and finds that its generated explanations are clearer than those written by humans, while having no significant difference in informativeness with human-written ones.Large language models are applied in the field of linguistics to generate explanations that help improve the understanding and evaluation of linguistic phenomena and theories.Chakrabarty, Saakyan, Ghosh and Muresan (2022) uses GPT-3 to generate explanations for a figurative language Natural Language Inference dataset, and lets GPT-3 generate explanations for its judgments on figurative expressions, involving three types: Sarcasm, Simile, and Metaphor.2) Future predictions.Large language models are used for future prediction due to their powerful generative capabilities, but they are also limited by the complexity and uncertainty of the prediction scenarios or reality.Kalinin (2023) explores the use of GPT-3 as an information retrieval tool for predicting the Russian-Ukrainian conflict.The responses of GPT-3 are used as inputs for a game theory-based model of strategic behavior called "Predictioneer's game".But GPT-3 is limited by its reliance on prewar data and its inability to capture complex patterns of behavior.Jungwirth and Haluza (2023) uses GPT-3 to predict the future of the war in Ukraine by using GPT-3 to generate future scenarios while assessing the consistency within the scenarios.</p>
<p>There are also studies that have conducted comprehensive evaluations of large language models' performance across multiple content analysis tasks.For example, Gilardi et al. (2023) expand the application scope of ChatGPT to five text annotation tasks.Their results show that ChatGPT outperforms crowd workers in annotation tasks such as relevance, stance, topics, and frames detection, and is much lower in cost than the latter.Ziems et al. (2023) evaluates the performance of ChatGPT on multiple NLP tasks related to social science, and finds that it performs poorly on tasks such as event argument extraction, character tropes, implicit hate, and empathy classification, which involve complex structures or subjective expert taxonomies.In contrast, large language models achieve an accuracy of over 70% on tasks such as misinformation, stance and emotion classification, which are based on objective basic facts or clearly verbalized definition labels.</p>
<p>Existing statistics analysis is a research method that builds on the analysis of existing statistical data, which comes from official agencies, organizations, institutions or individuals, and covers various social phenomena and issues.Analysis of existing statistics can help researchers save time and cost, use existing information resources, and explore new research questions and hypotheses.In the following, we will discuss the applications of large language models in descriptive and inferential analysis as well as predictive analysis.</p>
<p>Large language models can be used to describe the characteristics of the sample or the relationship between variables, or to make inferences about causal processes, which refers to descriptive and inferential analysis (Rubin and Babbie, 2016).For example, Chen, Li, Smiley, Ma, Shah and Wang (2022) attempts to use large language models to understand financial reports and statements, but GPT-3 either copies the reasoning steps from the examples or gives incorrect reasoning, resulting in an accuracy below 50%.BloombergGPT (Wu et al., 2023b) is also applied to this task, but achieves only 43.41% exact match accuracy.Although both large language models do not reach satisfactory results, OpenAI recently released more powerful ChatGPT and GPT-4, which might be able to perform better on this task.</p>
<p>Large language models can also be used to infer future trends and changes based on historical data, which refers to predictive analysis.This provides a basis for decision making, thus having a very wide range of applications in fields such as finance.For example, Lopez-Lira and Tang (2023) demonstrates the potential of using ChatGPT to predict stock market returns.The authors simulate financial experts with ChatGPT and ask it to evaluate the impact of company-related headline news from the previous day on the stock price, based on sentiment analysis.They find that  ChatGPT's sentiment scores had a significant positive correlation with the subsequent daily stock market returns.Xie, Han, Lai, Peng and Huang (2023a) find that using ChatGPT to predict stock trends based on historical price features and tweets has limited success, and even performs worse than traditional methods using only price features.However, ChatGPT is still recognized as having the potential to improve financial market prediction by utilizing social media sentiment and historical stock price information.</p>
<p>Conclusion</p>
<p>Numerous studies have applied and evaluated large language models in a wide range of computational social science tasks, clarifying that large language models can significantly transform nonreactive research in three ways: 1) Assisting data annotators on human annotation teams.2) Serving as zero/few-shot text analysis tools.3) Bootstrapping challenging creative generation tasks.The application of large language models in nonreactive research offers several advantages.Firstly, it can partially remove the limitations of data resources since large language models can achieve performance comparable to finetuned models in many social science tasks without extensive training.Secondly, they exhibit broad cross-disciplinary applicability, providing general solutions to a wide range of problems.Thirdly, they lower the entry barriers for usage.large language models have changed the scenario where researchers previously had to rely on statistical learning, machine learning, or deep learning methods to handle massive statistical data, which posed a high degree of difficulty and complexity.They are capable of interacting directly through text inputs instead of complex code or commands, providing a more direct and user-friendly interface, significantly lowering the technical barriers to using artificial intelligence for analysis.Consequently, researchers can focus more on the research questions they are interested in, rather than becoming excessively immersed in the intricacies of technical implementation.</p>
<p>However, there are some limitations to note: 1) Almost all large language models struggle with conversational and full document data, which limits common applications such as topic modeling.2) large language models may have difficulty understanding the subtle and non-conventional language of expert taxonomies, which don't present in pre-trained data.</p>
<p>NLP researchers working to improve existing large language models for better support in nonreactive tasks can look at the following future directions: 1) the unique technical challenges of conversational, long-document, and cross-document reasoning, 2) in-domain training to teach LLMs to understand novel social constructs, 3) integration of specialized numerical analysis tools.</p>
<p>Revisiting Applications Across Disciplines</p>
<p>In this section, we revisited the application of large language models in social science from a disciplinary perspective, to provide readers with a more comprehensive understanding of research progress in this field.The representative tasks, datasets used, and related work for each discipline are outlined in Table 2.</p>
<p>From a task perspective, we observe that the diverse tasks across disciplines can be summarized mainly into three categories: text classification, structured parsing, and natural language generation, which are relatively easier to handle in social science.However, more complex tasks such as aggregating mining on massive datasets, multi-document summarization or topic modeling may exceed the scope of transformer-based language models at present.From the algorithmic perspective, large language models can serve as a universal solution, meaning that almost all tasks can be addressed using the same approach, with the only difference being the design of prompts.The main drawbacks of using large language models as a solution could be related to issues like bias, high computational cost, difficulties in fine-tuning for specific tasks and so on.</p>
<p>Discussions</p>
<p>As mentioned above, large language models can be applied at every stage of social science research, improving efficiency across the board.More specifically, the practical applications of large language models in social science research predominantly fall into three directions: 1) replacing traditional NLP tools in data analysis, 2) assisting in creative work in research, 3) simulating humans as study objects.So far, extensive studies have thoroughly examined the superiority of large language models in the first direction.However, the last two directions, which particularly emphasize large language models' human-like intelligence, are still in the exploratory stage without a systematic body of research.</p>
<p>In the future, several directions of AI for social science may lie in the following: 1) Further exploring the untapped potential of large language models in social science research.For instance, a large language model-based fully automated social science research pipeline could be developed, covering everything from hypothesis generation to hypothesis verification and even peer review.2) Injecting domain-specific knowledge into large language models, thereby facilitating the development of expert models.3) Establishing comprehensive benchmarks to measure the human-like attributes of large language models.4) Integrating tools into large language models to enhance their capabilities in logical reasoning and mathematical derivation.5) Developing multi-modal large models, which could improve their real-world understanding of human social behavior and systems.6) Ethical and moral norms.Constructing ethical and moral frameworks for the functioning and application of large language models, thereby ensuring their responsible use.</p>
<p>Social science of AI</p>
<p>Social science of AI refers to AI's social science.Specifically, we will focus on social science researches that use large language models as objects, with a particular emphasis on how they differ from traditional human behavior.Unlike AI for social science, the aim is not to make large language models mimic human behavior, but rather to explore the behavioral patterns of large language model agents themselves.In Table 3, we give specific differences between social science of human beings and of large language models.</p>
<p>In this section, we will explore the social behavioral patterns of large language models as intelligent agents through the lens of various sub-disciplines within the social sciences.This emerging field has become increasingly significant due to several factors.Firstly, AI has demonstrated its ability to autonomously perform tasks in various domains.Secondly, research has shown that the collaboration of multiple AI agents can effectively enhance their performance.However, the behavior patterns, consequences, and impacts of AI collaboration are still not very clear.Additionally, the factors that drive changes in collaborative behaviors among AI agents are also not clearly understood.Similar to social science on humans, the ultimate goal of the Social Science of AI is to inform us about the behavioral traits exhibited by AI agents when they collaborate with each other and how to model and understand these behavioral traits.This type of research is of significant importance for the autonomous decision-making and control of future AI collectives.</p>
<p>Psychology of AI</p>
<p>Psychology of AI, or to say the psychology of machines, is typically defined as the scientific study of mind and behavior in AI agents (Hagendorff, 2023).Extensive research in this field has been conducted with the enhanced capabilities of large language models.It has even been claimed that large language models may have a consciousness of Human Beings of AI</p>
<p>Psychology</p>
<p>Study the psychological phenomena, consciousness, and behavior of humans (Danling, 2019).Research spans a wide range of areas including consciousness, sensation, perception, cognition, emotion, personality, behavior and relationships.</p>
<p>Study personality, consciousness, ability, cognition, and more of AI agents.</p>
<p>Sociology</p>
<p>Study human social life, groups, and societies, ranging from institutions or human interactions at the micro-sociological level to social systems or structures at the macrosociological level (Giddens, 2007).</p>
<p>Study interactions and social behaviors among multiple different AI agents.</p>
<p>Economics</p>
<p>Study the production, distribution, and consumption of goods and services (Backhouse, 2002; Krugman and Wells, 2009).</p>
<p>Study the behavior and interaction of AI agents as economic agents.</p>
<p>Politics</p>
<p>Study the authoritative allocation of societal values (Easton, 1955).</p>
<p>Study the political behaviors and phenomena exhibited by AI agents, such as ideology, party affiliations, and political prudence.</p>
<p>Linguistics</p>
<p>Study language (Halliday, 2006), including syntax, semantics, morphology, phonetics, phonology, pragmatics (Farmer and Demers, 2010), and etc.</p>
<p>Study the language use patterns of AI agents and compare them to human language use.</p>
<p>Table 3</p>
<p>The differences between social science of human beings and social science of AI in different sub-disciplines.The fundamental distinction between the two lies in the difference in their research subjects.The former investigates behavioral patterns within the human population, while the latter regards AI agents as intelligent entities and explores the behavioral patterns within the groups they form.</p>
<p>of its own.A typical example is a Google engineer's assertion that the conversational AI system, LaMDA, which he was developing, has become sentient and capable of thinking and reasoning like a human, leading to his suspension from work (Tiku, 2023).He believes that this large language model has attained the intelligence level of a 7-year-old or 8-year-old child.In this section, we will organize the current advancements in psychology of AI, according to the research content of the psychology (Danling, 2019), such as personality, cognition, and more.</p>
<p>Personality refers to the sum of distinctive traits and characteristics that an individual possesses psychologically, emotionally, and behaviorally.Due to the stochastic nature of large language model's outputs, the personality of large language models refers to its overall tendency in generating responses.Researching the personality of large language models contributes to creating more human-friendly large language models.OpenAI's blog (OpenAI) pointed out that for models such as ChatGPT, the emotional bias of its output depends on both the pre-training stage and the fine-tuning stage.The sentiment tendency of the pre-training part comes from a large amount of text, and the value tendency of the fine-tuning stage may be related to the labeling staff or the fine-tuning task due to the different techniques used.The most commonly used method for personality assessment of large language models is the utilization of questionnaires.With advancements in GPT-3 and its more powerful successor large language model, these language models are now capable of comprehending and fluently answering questions.The format of questionnaires is also highly compatible with language models.Survey questionnaires designed for human subjects typically require only minor adjustments in terms of formatting or vocabulary to be directly employed for personality testing (Miotto et al., 2022).After adding the output method, it can be directly sent to the language model as a prompt for a reply.</p>
<p>Research on large language model's personality has yielded some interesting conclusions.Jiang, Xu, Zhu, Han, Zhang and Zhu (2022a) proposed the Machine Personality Inventory (MPI) dataset for evaluating the machine personality, finding that personality indeed exists in large language models.Miotto et al. (2022) used Hexaco questionnaire (Ashton and Lee, 2009) to analyze GPT-3 and found that GPT-3 is generally a young woman whose personality level is consistent with the general tendency of human beings.In the assessment of human values, GPT-3 accords importance to every value, which can sometimes appear contradictory.Li et al. (2023) use Short Dark Triad (SD-3) and Big Five Inventory (BFI) tested GPT-3, InstructGPT, and FLAN-T5 and found that the tested large language models all showed darker than humans.The latter two are no better than GPT-3, even after fine-tuning.Furthermore, some studies have found that the personality traits of large language models can be effectively changed by fine-tuning (Karra, Nguyen and Tulabandhula, 2023) or increasing memory (Jiang et al., 2023).This opens up the possibility of more related research.</p>
<p>Cognition is about how humans understand, perceive, make decisions, and solve problems.Incorporating methodologies from cognitive psychology into large language models aids us in better understanding how these models process and address problems.</p>
<p>There have been numerous studies investigating whether large language models are capable of human-like cognition.For instance, Binz and Schulz (2023) employed classic vignette-based and task-based experiments from the cognitive psychology literature to assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities.The results indicate that GPT-3 can achieve human-comparable performance on most tasks, but its behavior is highly influenced by how the vignettes are presented and it does not learn and explore in a human-like manner.Han, Ransom, Perfors and Kemp (2022) focused on GPT-3's inductive reasoning ability.Experiment results suggested that GPT-3 can qualitatively mimic human performance for some inductive phenomena (especially those that depend primarily on similarity relationships), but fails to explain human inductive inferences, which may be due to GPT-3 not following the reasoning principles used by humans.Webb, Holyoak and Lu (2023) compared the analogical reasoning abilities of human reasoners and the text-davinci-003 variant of GPT-3 and found that large language models displayed a surprisingly strong capacity for abstract pattern induction, which may explain their abilities to reason about novel problems zero-shot.Prystawski, Thibodeau and Goodman (2022) incorporated Chain of Thought (CoT) into the metaphor process inspired by cognitive psychology.Collins, Wong, Feng, Wei and Tenenbaum (2022) proposed a new benchmark for comparing the capabilities of humans and language models in problem-solving domains (planning and explanation generation).On this benchmark, humans are much more robust than large language models.Kosoy, Chan, Liu, Collins, Kaufmann, Huang, Hamrick, Canny, Ke and Gopnik (2022) tested the abilities of GPT-3 and PaLM in causal reasoning environments.Besides direct reasoning abilities, biases in reasoning or decision-making processes have also received attention.Various studies, in different manners and types, have collectively demonstrated the existence of certain biases in large language models, such as ChinChilla-7B/70B, CodeX, and ChatGPT, which are often similar to human cognitive biases (Dasgupta, Lampinen, Chan, Creswell, Kumaran, McClelland and Hill, 2022;Jones and Steinhardt, 2022;Chen et al., 2023).</p>
<p>Theory of mind (ToM), another cognitive ability, refers to the capacity to comprehend others by attributing psychological states to them.Studies, exemplified by the false belief task, indicate that more advanced large language models perform better in ToM (Kosinski, 2023).Interestingly, we have located research papers on various models of the GPT series.In their study on GPT-3-davinci, Sap, Le Bras, Fried and Choi (2022) noted that large language models cannot comprehend the intentions and reactions of social participants and infer the mental states of situational participants.However, when the research subject shifts to GPT-3.5 models such as text-davinci-002 and text-davinci-003, the large language models become more competent and closer to humans (Trott, Jones, Chang, Michaelov and Bergen, 2023;Dou, 2023).Other studies of large language models' cognitive abilities include creativity tests, such as alternative tool tests for GPT-3 (Stevenson, Smal, Baas, Grasman and van der Maas, 2022), cognitive reflection tests and semantic illusions to examine the decision-making processes of large language models (Hagendorff, Fabi and Kosinski, 2022), and assessments of GPT-4's cognitive abilities (Dhingra et al., 2023).</p>
<p>Others Apart from personality and cognition, there are many other psychological aspects of AI being studied.Feng, Xu, Li and Liu (2023b) proposed that human body size affects the affordances of objects around them, and demonstrated that ChatGPT also exhibits this ability.They concluded that the embodied perception of ChatGPT (GPT-4 version) could be comparable to that of an average adult human, around 5 feet 6 inches tall.Pellert et al. (2022) suggested administering psychometric questionnaires to various models and requesting output, proposing the concept of AI Psychometrics.Further research has examined moral concepts and values in large language models (Fischer, Luczak-Roesch and Karl, 2023;Jin et al., 2022).</p>
<p>Conclusion</p>
<p>In summary, extensive research has been conducted to explore the psychological features of large language model agents, from the perspectives of personality, cognition, and others, leading to many intriguing findings.</p>
<p>From a personality perspective, researchers generally find that large language models do exhibit personality tendencies, but these tendencies are not consistent and stable like those of humans.Instead, large language models are superpositions of cultural perspectives.These personality traits can be effectively altered through methods such as fine-tuning or enhancing memory capacities.</p>
<p>In the realm of cognitive abilities, studies have explored various facets, including induction, analogy, causal reasoning, theory of mind and so on.It is commonly found that the most advanced large language models, represented by GPT-3.5 and GPT-4, can demonstrate cognitive capabilities comparable to or even surpassing human abilities.These abilities improve with the evolution of the models.However, the cognitive modes employed by these language models are inconsistent with those of humans, and there is currently no universally accepted hypothesis to explain their cognitive abilities.</p>
<p>We believe that there are several pressing issues in this field that need to be addressed.These issues include: 1) Data leakage concerns.Much of the current research is based on classic psychological experiments to explore the cognitive capabilities of models, but it is unclear whether the test data is part of the language model's training data.2) Unclear influence factors.The impact of factors such as the model's training objective, size, and data to its abilities has not been systematically analyzed.</p>
<p>Sociology of AI</p>
<p>Unlike the psychology of AI, which focuses on the behaviors of individual AI agents, the sociology of AI mainly studies the social behaviors and interactions of multiple AI agents.It is important to note that we will primarily discuss researches that are similar to human sociology, but with a focus on AI agents as the research subjects.This distinction sets us apart from other reviews that primarily concentrate on societal changes and issues arising from advancements in AI (Joyce, Smith-Doerr, Alegria, Bell, Cruz, Hoffman, Noble and Shestakofsky, 2021).</p>
<p>Social bias Many researches focused on social bias2 , referring to unfair situations that emerge in large language models.With the widespread application of large language models, this can negatively impact user decision-making and interactions.The discussion about various biases in large language models started almost from the very beginning of their inception (Brown et al., 2020) and the topic is discussed with almost every large model released (Chowdhery et al., 2022;OpenAI, 2023;Touvron et al., 2023).In papers, it often appears in sections like Limitations or Ethical Considerations.However, as a single section in a paper, the exploration is obviously insufficient, and subsequent research has focused on exploring social bias in large models.For instance, Lucy and Bamman (2021) studied gender bias in GPT-3 and found that in the stories generated by GPT-3, females are more likely to be associated with family and appearance and are portrayed as less powerful than male characters.For minority groups, GPT-3 was also pointed out to have a bias against disabled people (Amin and Kabir, 2022).Some methods have been adopted to reduce bias or toxicity in large models.For instance, InstructGPT used a reinforcement learning approach to fine-tune the model, reducing toxic outputs (Long et al., 2022).We believe that there is a need for concerted efforts from researchers in sociology and AI to address these issues.</p>
<p>Social behavior</p>
<p>Other researchers have considered the sociological behaviors among multiple large language modeldriven agents.While there are not many researches in this area, interesting phenomena can still be observed.Park et al. (2023a) created a rather interesting experimental environment.They built a sandbox where 25 agents live in a small town within the sandbox.The agents can wake up, make breakfast, and have small talk among themselves.They ended up exhibiting surprising social behaviors, such as spontaneously inviting and scheduling a party when a user assigns an agent to host one.In this research, through the introduction of a memory module and carefully designed processes, agents have become more human-like.Chirper3 is an online community, where, unlike other communities like Reddit, all participants in Chirper are AI.You can set a backstory for a bot, and the bot will automatically post messages and interact.For instance, under the #NewFriends topic, bots invite each other for walks in the park with their dogs, just like in a real human community, but all participants are AI.While these research efforts are intriguing, we believe they have yet to truly tap into the potential of AI sociology.We look forward to seeing more researchers delve into this field in the future.</p>
<p>Conclusion</p>
<p>Currently, researchers have made some progress in social bias of AI, but there is still a dearth of studies specifically exploring sociological phenomena within the AI community.</p>
<p>The issue of social bias in AI has been a long-standing topic of discussion.Researchers have put forth various assessment methods and benchmarks to tackle bias issues, and they have made considerable strides in ensuring fairness and impartiality in language models when it comes to surface-level queries.Nevertheless, there is still much work to be done in this area, and several key challenges must be addressed: 1) Positive Stereotypes and essentializing Narratives.Even if a word may seem positive in sentiment, it can lead to harmful narratives.For example, praising women for being submissive and humble.2) Implicit cognitive bias.Language model bias can be induced in various ways.</p>
<p>Despite our society witnesses a surge in the number of AI agents and the emergence of AI agent-exclusive communities, there has been limited research on the interaction patterns of language models within these environments.We anticipate a greater involvement of researchers from both the field of artificial intelligence and social science to systematically uncover the similarities and differences between the sociology of AI and human.</p>
<p>Economics of AI</p>
<p>Economics of AI is the scientific study of how AI agents, as economic entities, produce, allocate, and consume goods and services (Backhouse, 2002;Krugman and Wells, 2009).Large language models have demonstrated their ability to act as economic agents, especially in the field of economics.Meanwhile, researchers have observed and compared their performance with that of humans in some economic experiments.In the following, we give an overview of both.</p>
<p>Economic expertise measures large language models' professional knowledge, skills, and experience in the field of economics.Although there are still some limitations, large language models show the ability of economic agents, including certain knowledge and understanding of economics, risk assessment and management ability, etc.For economics, in the Test of Understanding in College Economics, ChatGPT outperformed 91% of students in the microeconomics and 99% in the macroeconomics.For finance, Davinci and ChatGPT score 58% and 67%, respectively, on the financial literacy test, 31% above the benchmark level (Niszczota and Abbas, 2023).Large language models show some ability on the "Financial Investment Opinion Generation (FIOG)" task, but there is still room for improvement (Son et al., 2023).GPT-3 can identify the potential impacts of climate change on economic growth, employment, poverty, inequality, and financial stability, and was also able to suggest some countermeasures (Leippold, 2023).Large language models match many biases in the expectations of existing professional humans and institutions for various financial and macroeconomic variables, including inflation, based on a sample of Journal news articles from 1984 to 2021 (Bybee, 2023).For operations management, ChatGPT earned a B-to B on the final exam for the MBA core course Operations Management but it doesn't work well for simple calculations or more advanced process analysis (Terwiesch, 2023).For marketing, the marketing content generated by ChatGPT matched and sometimes surpassed, human content on quantitative and qualitative measures (Rivas and Zhao, 2023).Conversations with consumers also show a high degree of intelligence and adaptability (Rivas and Zhao, 2023).ChatGPT was found to be more accurate and less biased than humans in problems with explicit mathematical or probabilistic properties, but also showed many human biases in complex, ambiguous and implicit problems (Chen et al., 2023).Cribben and Zeinali (2023) further discusses the strengths and limitations of ChatGPT in management science and operations management.For accounting, large language models' performance is significantly lower than human capacity (Wood, Achhpilia, Adams, Aghazadeh, Akinyele, Akpan, Allee, Allen, Almer, Ames et al., 2023;Bommarito, Bommarito, Katz and Katz, 2023), especially when it comes to computation (Bommarito et al., 2023).In non-computational problems such as memory and understanding, large language models are almost human-level (Bommarito et al., 2023).On audit tasks, ChatGPT performed similar to or better than human experts on financial ratio analysis and text mining tasks, and slightly worse, but still acceptable, on log testing tasks (Gu, Schreyer, Moffitt and Vasarhelyi, 2023).</p>
<p>Microeconomics is a branch of mainstream economics that studies the behavior of individual large language model agents in making decisions regarding the allocation of scarce resources and the interactions among these individuals (Besanko and Braeutigam, 2020).Given that single or multiple large language model agents can naturally serve as the subjects of microeconomic research, a considerable amount of studies exist in this field.We will now organize them according to their research methodologies.Some researchers use classical experiments in economics to study the behaviors and decision preferences of large language model agents.For example, Aher et al. (2023) uses six large language models in the experiment and found that it can reproduce human behavior characteristics in the classic behavioral economics experiment -ultimatum game.Guo (2023) investigates the response of GPT-3.5-turbo in the ultimatum game with finite repetition and the Prisoner's Dilemma game.The results show that given a well-designed prompt, GPT can produce realistic results and exhibit behavior consistent with human behavior in some important respects, such as the positive correlation between the acceptance rate and proposed amount in the ultimatum game and the positive cooperation rate in the Prisoner's Dilemma game.The authors also observed some differences between GPT and human behavior.For example, in the repeated ultimatum game, the human agent generally decreased the amount of offers and the acceptance rate as the number of rounds increased, while the GPT agent showed no such tendency.This may indicate that GPT agents lack the ability of human agents to learn, adapt and punish.In the repeated Prisoner's dilemma experiment, Phelps and Russell (2023) also finds the limitations of large language models in adjusting their behavior based on conditional reciprocity, and large language models can make different choices when it is injected with altruistic or selfish characteristics.Johnson and Obradovich (2023) explores whether AI agents exhibit behaviors consistent with self-interest and altruism in non-social decision-making tasks and dictator games.The results showed that only the most complex AI agent maximized its gains more in the non-social decision-making task, and that this AI agent also exhibited the most generous altruistic behavior in the dictator game, similar to humans.Fu, Peng, Khot and Lapata (2023) lets two large language models play the role of buyer and seller respectively in a bargaining game, the goal is to reach a deal, the buyer for the low price, and the seller for the high price.Experiments show that only strong large language models can improve the trading price through self-game and AI feedback.Some other researchers investigate through survey research.For example, ChatGPT's responses to different types of survey questions were compared with human consumers and found to be able to answer in a manner consistent with economic theory and well-documented patterns of consumer behavior and matched estimates from a recent study that elicited human consumer preferences (Brand et al., 2023).Goli and Singh (2023) finds that GPT is more inclined to choose larger and later rewards in weak future tense references (FTR) languages, while smaller and earlier rewards in strong FTR languages, which is consistent with human preference.However, while human choices tend to prefer larger and later rewards as the reward gap increases, GPT choices do not.Some questions are designed based on classical experiments in economics literature (Horton, 2023), and it is found that large language models show similar behaviors and preferences to humans, such as fairness preference, loss aversion, state criteria, etc.However, there are also some differences, such as the attitude to risk, understanding of probability, sensitivity to language ,and so on.</p>
<p>Macroeconomics is a branch of economics that deals with the performance, structure, behavior, and decision-making of an economy as a whole-for example, using interest rates, taxes, and government spending to regulate an economy's growth and stability (Barro, 1997).Currently, there is no existing research on this topic, due to the nascent development of AI agents and the absence of a mature AI-driven economic framework.</p>
<p>Conclusion</p>
<p>Extensive research has been conducted in the field of the economics of AI, with most of the studies focusing on the economic expertise, behavior and decision preferences of individual large language model agents.</p>
<p>In terms of economic expertise, large language model agents have demonstrated capabilities comparable to or even exceeding those of human experts in non-computational areas of economics, displaying a deep understanding of economic concepts.However, when it comes to more computational areas like accounting, they exhibit notably lower proficiency compared to humans.This observation suggests that language models possess the potential to reshape the quality and efficiency of future work within the field of economics.</p>
<p>In terms of economic behavior and decision-making preferences, large language model agents, when presented with well-defined prompts, can display behavior patterns consistent with human behavior in some significant aspects.They also show an understanding of human cooperative norms, such as altruism or selfishness, and can act in accordance with these norms.However, these agents also show certain limitations, such as their inability to adopt reasonable response strategies based on the cooperation patterns of others.</p>
<p>It's important to acknowledge the limitations of current research.Firstly, most results are based on testing GPT-3.5-turbo, and it remains uncertain whether these findings are universally applicable to all large language models.Additionally, these models have been trained on a significant amount of literature related to classical economics experiments, making it unclear how they would perform in more ecologically valid task environments they haven't encountered previously.</p>
<p>To address these challenges, we encourage the research community to further investigate: 1) The factors influencing intelligent agent behavior generated by language models across a wider range of social dilemmas, including model architecture, training parameters, and various partner strategies.2) The development of new social dilemma games to assess language model capabilities, accompanied by task descriptions, rather than relying solely on existing literature anecdotes.</p>
<p>Politics of AI</p>
<p>Politics of AI studies the political behaviors and phenomena exhibited by large language models, as political participants.More specifically, it involves the set of activities that are associated with making decisions in groups, or other forms of power relations among individuals, such as the distribution of resources or status (Easton, 1955).Currently, research in this field primarily focuses on the political leanings and political prudence of large language model agents.Researchers have also conducted preliminary analyses to identify the factors that contribute to these characteristics and have proposed certain countermeasures.</p>
<p>Political leanings refer to a large language model agent's preference for certain political beliefs, values, or views.Some researchers examine the general political leanings of large language models themselves, without providing any background settings.For example, there exist some studies indicating that ChatGPT leans towards left-leaning liberal progressives (van den Broek, 2023;Hartmann, Schwenzow and Witte, 2023;Gover, 2023;Liu et al., 2021).McGee (2023a) also find that ChatGPT favored liberal politicians over conservatives at least in some cases.Rutinowski, Franke, Endendyk, Dormuth and Pauly (2023) indicates that ChatGPT seemed to favor progressive views.King (2023) show that ChatGPT supports the New Liberal Party (King, 2023).It is most likely to vote Green in Germany and the Netherlands (Hartmann et al., 2023), and leans towards the Democratic Party in America, Lula in Brazil and the Labour Party in Britain (Motoki, Neto and Rodrigues, 2023).In terms of territorial sovereignty, ChatGPT's responses to different territories are inconsistent and biased, sometimes at variance with Wikipedia information and UN resolutions (Castillo-Eslava, Mougan, Romero-Reche and Staab, 2023).Other researchers examine the political leanings of large language model agents when given specific backgrounds.For example, Santurkar, Durmus, Ladhak, Lee, Liang and Hashimoto (2023) find that current large language models reflect views that are significantly inconsistent with those of American demographic groups.It's also very different from the views of certain demographic groups in a given description (Santurkar et al., 2023).Argyle et al. (2023) draws the conclusion that large language models are significantly consistent with human samples in terms of political orientation, political knowledge, and political participation, and can capture the subtle differences existing in human samples.</p>
<p>Intuitively, the political bias of large language models may stem from the biases and tendencies of the training data sources themselves.When large language models were fine-tuned in tweets from two politically inclined communities, Republicans and Democrats, the models show the political attitudes and worldviews of the two communities, respectively (Jiang, Beeferman, Roy and Roy, 2022b).Feng, Park, Liu and Tsvetkov (2023a) further explored how political biases in pre-training data affect large language models.Motoki et al. (2023) agrees that ChatGPT and large language models may perpetuate or even amplify Internet and social media views of politics.It is worth mentioning that ChatGPT's political bias can be influenced by its context and tends to copy the ideological bias of the prompt text (Liu et al., 2021;Gover, 2023).The influence of populist framework is also explored (Griffin, Kleinberg, Mozes, Mai, Vau, Caldwell and Marvor-Parker, 2023).The experiment shows that, like human beings, it has a positive or negative influence on the news persuasiveness and political mobilization of large language models, while the anti-immigration frame has a negative influence, but there are some differences, such as the moderating effect of relative deprivation on the effect of the populist frame.</p>
<p>Conclusion</p>
<p>The current exploration of the politics of AI is still in its nascent stage, with a primary focus on assessing the political biases inherent in large language model agents.Extensive research suggests that, in the absence of specific contextual information, representative large language model ChatGPT exhibits strong and systematic political biases, leaning notably towards the left end of the political spectrum.These biases can largely be attributed to inherent predispositions and patterns ingrained in the training data sources.Therefore, we can infer that the political leanings of language models can vary due to the diversity of their training data, demanding a thorough investigation.Notably, the political leanings of large language models can be significantly influenced by the contextual information provided.When these models are given a specific role or background information, they can align their political inclinations with those of individuals who share similar backgrounds.Even with implicit textual prompts, these models tend to capture and reproduce the ideological biases embedded in the text.</p>
<p>These observations underscore the concerning potential for language models to perpetuate and even amplify political perspectives on the internet, raising concerns about the influence they may wield over users and the potential for adverse political and electoral ramifications.Consequently, there is an urgent need to develop robust methods reliable methodologies for measuring the biases of large language models and poses a significant challenge to AI researchers aiming to construct more equitable and impartial language models.Additionally, exploring the capabilities of large language models in comprehending and handling political issues is an avenue of inquiry that deserves further attention.</p>
<p>Linguistics of AI</p>
<p>Linguistics of AI aims at exploring the language use patterns of large language model agents, including syntax, semantics, morphology, phonetics, phonology, pragmatics and etc (Farmer and Demers, 2010).We will emphasize large language model agents' unique language use patterns.</p>
<p>Researchers have made some interesting findings on the exploration of language use by large language models.In the following, we focus on the consistency and differences in language use between large language models and humans.Given the large number of existing studies, we will not dwell on the assessment of language proficiency in large language models.The garden path sentence experiment is repeated on large language models, and it seems that large language models also have ambiguity in understanding language mechanisms (Aher et al., 2023).And Diamond (2023) shows that GPT-generated languages statistically follow Zipf's law just like humans do.Cai et al. (2023) further explores the consistency and differences between ChatGPT and humans in language use and finds that it is able to associate unfamiliar words with different meanings based on form, reinterpret unreasonable sentences that may be corrupted by noise, etc. as well as humans.It does not like to use shorter words to convey less information and does not use context to disambiguate syntax.In addition, the degenerated version, GPT-D, obtained by changing the parameters of GPT-2, has language features associated with Alzheimer's disease, such as repetition, semantic loss, and grammatical errors (Li, Knopman, Xu, Cohen and Pakhomov, 2022), which contributes to a better understanding of the internal mechanisms of generative neural language models.It is worth mentioning that large language models' preferences for time and reward are similar to human decision-makers and are influenced by future tense references in language (Goli and Singh, 2023).</p>
<p>Conclusion</p>
<p>These studies delve deep into the linguistic features of large language models and compare them to human, offering us initial insights into the language usage patterns of these models.For instance, large language models, like humans, can understand unfamiliar words based on affixes, may misinterpret sentences as typical but grammatically incorrect meanings, and follow similar vocabulary distributions statistically.</p>
<p>In future research, we believe that combining a linguistic perspective with a natural language processing perspective might offer a better understanding of the internal mechanisms of generative neural language models.This integration could serve as a crucial foundation for further exploring and explaining the language and intelligence capabilities of large language models.For example, the consistency in vocabulary distribution with humans can be attributed to the fact that language models inherently learn the probabilities of word occurrences and context combinations in language.Additionally, the comprehension of word forms by language models is likely influenced by the role of tokenization, enabling language models to understand the meanings of affixes in English and thus combine to comprehend previously unseen words.</p>
<p>Discussions and Future Works</p>
<p>We have conducted a comprehensive review of research on the social behavior of large language model agents based on several representative sub-disciplines of social science.Notably, current research is predominantly focused on exploring the social behaviors exhibited by individual large language model agents, with a lack of study on large language model agent groups or systems.This could be attributed to the present absence of instances of large language model agent groups or systems.</p>
<p>Furthermore, we have identified some limitations in the existing research.Firstly, the testing of large language models' capabilities or characters is greatly associated with the version and parameter settings of the experimental model.For widely used models such as ChatGPT, versions vary over time, and responses from ChatGPT to the same question may differ at different times, which may affect the reproducibility of experimental results (Tu, Li, Yu, Wang, Hou and Li, 2023).The research by Miotto et al. (2022) confirmed that changes in temperature affect the personality tendencies of the model.Secondly, large language models are sensitive to the order of given prompts (Lu, Bartolo, Moore, Riedel and Stenetorp, 2022;Zhao, Wallace, Feng, Klein and Singh, 2021), a factor that should be taken into account in experiments.Thirdly, experiments exploring the psychology of large language models require careful design.The large language models training process uses a vast amount of textual material, which may contain classic psychological scenarios that could impact experimental results, but this issue is considered in only a few documents (Binz and Schulz, 2023).Different evaluation methods for the same ability could lead to different results.In the exploration of GPT-3's mental abilities, Sap et al. (2022) and Bojic et al. (2023) had disparities due to differences in the experimental process.Finally, the direct application of human evaluation methods to large language models remains a question worth considering.Ullman (2023) found that large language models often fail in tests when false belief tasks are added with various disturbances, suggesting that large models actually lack ToM.The conclusion is that while ToM tests are effective for humans, they may not reasonably assess the abilities of large language models.</p>
<p>To address the aforementioned challenges, the future directions for social science of AI lie in: 1) Establishment of a systematic theory for the social science of AI, similar to the social science of humans.This will aid in connecting and organizing the currently fragmented research efforts, and allow for a comprehensive examination of AI agents' social characteristics as intelligent entities themselves.2) In-depth exploration of social phenomena in large language model agent groups or systems, delving into the complex interactions and dynamics that may emerge.</p>
<p>3) Standardized experimental designs, such as those pertaining to model versions, parameters, and prompts, to minimize result deviations caused by variations in experimental designs.4) Tailored evaluation methods for large language model agents, considering that directly applying human evaluation methods to large language model agents may not result in reasonable assessments.5) Combination of social science theories with AI theories.We note that the study of large language models shares some similarities with the study of social science.For instance, both the thought process of large language model agents and humans can be seen as a 'black box' to some extent.We cannot fully grasp the various reasoning or cognitive processes inside the large language models and the human body, but we can gauge them using external tools such as performance metrics on specific tasks and observable behaviors.For this black box, we have both attempted to probe the internal mechanisms or, in other words, to 'open' the box.For the NLP community, this endeavor involves parameter tuning, knowledge injection, and modification; for the field of social science, it may involve areas like neuroscience.We hope these commonalities can provide inspiration for further exploration in the future.</p>
<p>Public Tools and Resources</p>
<p>To facilitate the utilization of large language models for social science research, there already exist several publicly available tools and resources as aids.In this section, we focus on introducing simulation tools and platforms that are based on large language models, taking into account that other applications mainly rely on direct use or simple script-based invocation.Based on this, we conduct a systematic analysis of simulation requirements and compare the functionalities of various platforms.</p>
<p>Public Simulation Tools</p>
<p>The evolution of human-like abilities in large language models has opened up new possibilities for computational simulation, leading to the emergence of various simulation platforms or tools based on these models.In the following, we collect and compare existing open source large language model-based simulation tools.</p>
<p>SkyAGI is a Python package that demonstrates the emerging capability of large language models in simulating believable human behaviors.It offers a role-playing game experience that is highly engaging and immersive.Unlike previous AI-based NPC systems, SkyAGI's NPCs generate incredibly realistic human responses.This platform has significant potential for rethinking game development, particularly in the area of NPC script writing.</p>
<p>AgentVerse is a versatile framework designed to streamline the process of creating custom multi-agent environments for large language models.It offers efficient environment building tools, allowing researchers to easily construct basic environments like chat rooms for large language models by defining settings and prompts.Additionally, it supports customizable components, empowering researchers to create their own multi-agent environments according to their specific requirements.Furthermore, AgentVerse integrates tools (plugins) to enhance functionality, currently supporting BMTools.This platform enables researchers to optimize their experiments and analyses in a seamless and efficient manner.yes auto serial+parallelism no yes * As mentioned in the main text, you need to write the code for the simulation yourself.</p>
<p>Table 4</p>
<p>Functional comparison of existing open source simulation toolkits.For single-agent scenarios, current simulation toolkits universally facilitate diverse population modeling, exclusively employ the "prompt" method for internalizing memory, and only accommodate predefined models rather than allowing for user-defined models for simulation.Consequently, for the sake of simplicity, these three columns have been omitted from the table.</p>
<p>LangChain is a powerful platform designed to assist developers in building applications through composability, harnessing the capabilities of large language models.One of its key features is the ability to create agents.Agents utilize LLMs to make decisions, perform actions, observe outcomes, and iterate until their objectives are achieved.Langchain does not provide out-of-the-box usage similar to other frameworks, but it implements interfaces such as Time-weighted vector store retriever,which plays a very important role in the agent's memory.You need to write your own code to implement interaction between multiple agents and other functions.</p>
<p>GenerativeAgents is the implementation of (Park et al., 2023a).Although the author does not propose it as a platform, users can still customize a simulation environment by modifying character configuration, code, etc.As mentioned in the paper, it provides a map for better visualization and interaction with the environment,and this provides users with more possibilities.</p>
<p>Agents is an open source framework for building autonomous language singletons (Zhou, Jiang, Li, Wu, Wang, Qiu, Zhang, Chen, Wu, Wang et al., 2023).It supports long and short-term memory, tool usage, etc.What differentiates Agents from other frameworks is that it supports more detailed control of agents through SOP.SOP defines the state of the agent and the transition relationship between states.In other words, the process can be configured using more complex configuration files rather than modifying the code.</p>
<p>AgentLab is a large language model-based simulation toolkit for social science research.It allows the creation of multiple intelligence agents with heterogeneous features by inputting different profiles.Each intelligence agent can learn knowledge either through model weights (i.e., fine-tuning the model based on its experience) or model inputs (i.e., incorporating knowledge into an input message).Additionally, it supports the customization of social backgrounds as required.Once all experimental conditions are set, the platform can automatically facilitate human-like interactions among agents.</p>
<p>Core Functions of Simulation Tools</p>
<p>Based on a summary of above existing toolkits, as well as the complexity and interactivity of the real world, we have formulated key features of simulation platforms using a functional hierarchy framework to satisfy various needs and summarized and compared the above toolkits based on this framework in Table 4. Specifically, these features can be classified into three levels: single-agent, multi-agent, and environment.</p>
<p>• single-agent, the basic building block of simulation -able to generate human profiles based on key information, enabling researchers to model populations with different characteristics using large language models.</p>
<p>support for using tools to complete some tasks.Using tools is an essential part of human life.The support for using tools can expand the scope of simulation experiments.</p>
<p>able to maintain and internalize its memory, including short memory and long memory.Inspired by Stanford's work (Park et al., 2023a), and based on the roadmap of large language models (Zhao et al., 2023), we believe that prompt and fine-tuning mechanisms exist that can be used to perform short-term and long-term memory tasks respectively.</p>
<p>support for multiple and pluggable models for simulation, enabling researchers to choose different models based on their needs and assumptions.This flexibility can facilitate innovation and customization of research, enabling researchers to better adapt to different research scenarios (Li et al., 2022).</p>
<p>• multi-agent, which interact to form the society able to interact spontaneously or under the influence of the researcher.</p>
<p>support complex interaction rules: serial, parallel, and both.Serial means that two operations cannot be executed at the same time, which means there is an order of precedence.Specifically, we believe there are three modes, sequential, bidding, and specified 10 , referring to langchain 11 .Parallel means that the two actions occur in overlapping periods.Both means supporting both serial rule and parallel rule.</p>
<p>• environment, the container for simulation, including the physical environment and social background able to interact with the physical environment.It means that agents have an impact on the physical environment, such as consumption of water, electricity and food and are also affected by the physical environment.A simple example is visibility.For example, when a report is given in a conference room, people outside the meeting room cannot directly know the specific content and progress of the report.</p>
<p>able to include social backgrounds, such as economic background, political background, cultural background, social norms, institutions, etc.It's necessary because the social background of each era is different, and human beings in the era also have their characteristics.Under different social backgrounds, human beings will have completely different cognition, decision-making behavior, etc.</p>
<p>The above three layers are only a minimum set of our large language models simulation tool imagination.A good and convenient toolkit should also be oriented to researchers, provide good visualization, automatically build profiles and prompts, etc.</p>
<p>We noticed that the current open-source simulation toolkits still have some limitations.Firstly, the current implementation of agents' knowledge learning in these platforms is quite simplistic, primarily relying on prompts while overlooking the more natural learning method of fine-tuning employed by language models.Secondly, these platforms restrict the underlying large language model for agents, which hinders the agent's ability to adapt flexibly to diverse research contexts.Lastly, the current platforms struggle to provide adequate support for effective interaction between agents and their environments.We believe that addressing these issues will contribute to a more comprehensive agent simulation.</p>
<p>Discussions and Future Works</p>
<p>Currently, there are still some common issues in simulation platforms based on large language models.Firstly, there remains a significant gap between large language model-based agents and real-life human behavior.This gap stems partly from the large language model itself.While natural language can express the vast majority of meanings, sometimes information like visuals and sound is indispensable.We believe that future multimodal large models can do better.Even within the scope of natural language itself, large language models still struggle to perfectly replicate the diversity of human behavior, especially when handling complex tasks.This may be due to large language models lacking a "theory of mind," which refers to the ability to understand the mental states and intentions of others.</p>
<p>10 A special case of bidding mode 11 https://python.langchain.com/en/latest/use_cases/agent_simulations.htmlThis deficiency hinders their performance in simulating complex interactions among multiple agents.Secondly, reallife social contexts, physical environments, and operational rules exhibit vast variations, making it challenging to build systems that involve interactions among multiple agents.This complex task often necessitates social science researchers to acquire programming knowledge and natural science researchers to gain an understanding of social science principles.Furthermore, considering temporal aspects adds another layer of complexity.Modeling temporal properties involves accounting for interaction behaviors, dynamic changes, and event sequences, presenting researchers with even greater challenges.</p>
<p>The future of social simulation platforms may involve: 1) Incorporating cognitive theories as frameworks for agent decision-making, thus enhancing the human-like aspects or correctness of agents, as well as providing interpretability for their behavior.2) Harnessing the multimodal capabilities of large language models to improve agents' ability to acquire and express information.3) Establishing an evaluation framework to qualitatively assess simulation platforms.</p>
<p>Conclusion</p>
<p>In this paper, we surveyed the latest developments at the intersection of large language models and social science.We propose a dichotomy to outline the progression in this field, encompassing 'AI for Social Science' and the 'Social Science of AI'.We note that large language models can be integrated into various stages of social science research, serving as auxiliary tools, a source of inspiration, annotation tools, content analysis tools, and so on, thereby enhancing efficiency.While large language models as tools have the advantages of speed, cost-effectiveness, ethically risk-free experimentation, and a low barrier to entry, the reliability and authenticity of their generated text need to be verified.Whether they can replace humans in conducting experiments and surveys also remains an open question.Therefore, researchers need to consider the additional cost of validation and the risk of bias when using these models.Furthermore, both the large language models themselves and the communities formed around them have exhibited some unique and intriguing behaviors.However, the enduring and unresolved issue in the field of social science is whether machines or intelligent agents should be the subject of social science research.We emphasize the promising future of this research direction, which will become increasingly important as AI agents become more prevalent in daily life.These two directions are complementary.The latter can guide the development of the former, while the former can enhance the efficiency of the latter's research.In conclusion, we believe that while AI cannot replace sociologists, it will become deeply integrated into the research process.Social scientists will play a significant role in guiding the development of AI.</p>
<p>As for future works, there is a need for an in-depth study into how and to what extent AI influence human behavior during computer-human interaction, the third intersection of AI with social science.Unlike AI for social science and social science of AI which address social issues within specific groups -the former concentrating on human populations and the latter on AI agent populations -this direction primarily explores new societal issues arising from interactions between AI and humans and introduces new methodologies.It is essential for gaining valuable insights on how to effectively utilize large language models in human-computer interactions and successfully accomplish social-oriented objectives.</p>
<p>Table 1
1Research StagesTraditional Methods Large language modelsHypothesis GenerationSpeedLowHighValidityHighLowNoveltyLowHighHypothesis VerificationExperiment ResearchCostHighLowSpeedLowHighReproducibilityLowHighScalabilityLowHighFidelityEntireNot SureSurvey ResearchCostHighLowEngagementLowEntireInteractionFixedNaturalBiasLowNot SureNonreactive ResearchGeneralitySingle-purposeMultiple-purposeAccessibilityLowHighNumerical analysisAccurateNot Sure</p>
<p>Table 2
2
A disciplinary perspective on AI for Social Science.This table presents a comparison of representative tasks, datasets used, and related work for each discipline.</p>
<p>https://elicit.org/
Research on bias falls within the purview of social psychology, a cross-domain of sociology and psychology. We categorize it under the sociology section to emphasize its interactive nature.
https://chirper.ai/
https://github.com/litanlitudan/skyagi
https://github.com/OpenBMB/AgentVerse
https://github.com/hwchase17/langchain
https://github.com/joonspk-research/generative_agents
https://github.com/aiwaves-cn/agents
https://github.com/renmengjie7/AISimuToolKit
AcknowledgementWe sincerely thank all reviewers for their insightful comments and valuable suggestions.This research work is supported by the National Natural Science Foundation of China under Grants no.62122077, 62106251, 62306303.Xianpei Han is sponsored by CCF-BaiChuan-Ebtech Foundation Model Fund.
Uses and misuses of chatgpt by academic community: An overview and guidelines. Available at SSRN 4402510. M Abbas, C C Aggarwal, C Zhai, 2023. 2012A survey of text classification algorithms. Mining text data</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. G V Aher, R I Arriaga, A T Kalai, International Conference on Machine Learning, PMLR. 2023</p>
<p>Domain adaption of named entity recognition to support credit risk assessment. S Alvarado, J Cesar, K Verspoor, Proceedings of the Australasian Language Technology Association Workshop 2015, Parramatta, Australia. the Australasian Language Technology Association Workshop 2015, Parramatta, Australia2015</p>
<p>A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages. A A Amin, K S Kabir, 10.48550/arXiv.2206.11993arXiv:2206.119932022</p>
<p>Language models as agent models. J Andreas, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Out of one, many: Using language models to simulate human samples. L P Argyle, E C Busby, N Fulda, J R Gubler, C Rytting, D Wingate, Political Analysis. 312023</p>
<p>The HEXACO-60: A Short Measure of the Major Dimensions of Personality. M C Ashton, K Lee, 10.1080/00223890902935878Journal of Personality Assessment. 912009</p>
<p>Openai chatgpt generated literature review: Digital twin in healthcare. Ö Aydın, E Karaarslan, R Backhouse, 2022. 2002. 2023Penguin, London. Bail, C.A.Can generative ai improve social science</p>
<p>Machine-assisted social psychology hypothesis generation. S Banker, P Chatterjee, H Mishra, A Mishra, Barro, R.J. 2023. 1997Macroeconomics. MIT Press</p>
<p>D Besanko, R Braeutigam, Microeconomics, John Wiley &amp; Sons, A Bhattacherjee, Social science research: Principles, methods, and practices. USA2020. 2012</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, 10.1073/pnas.2218523120Proceedings of the National Academy of Sciences. 120e22185231202023</p>
<p>Artificially precise extremism: How internet-trained llms exaggerate our differences. J Bisbee, J Clinton, C Dorff, B Kenkel, J Larson, S Black, G Leo, P Wang, C Leahy, S Biderman, 10.5281/zenodo.5297715doi:10.5281/zenodo.5297715GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. 2023. 2021</p>
<p>Signs of Consciousness in Ai: Can Gpt-3 Tell How Smart it Really is?. L Bojic, I Stojković, Z Jolić Marjanović, 10.2139/ssrn.43994382023</p>
<p>GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities. J Bommarito, M Bommarito, D M Katz, J Katz, 10.48550/arXiv.2301.04408arXiv:2301.044082023</p>
<p>Chatgpt's left-leaning liberal bias. J Brand, A Israeli, D Ngwe, 10.2139/ssrn.4395751SSRN Electronic Journal. 2023. 2023University of Leiden URLUsing GPT for Market Research</p>
<p>GPT3-Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, 10.48550/arXiv.2005.14165arXiv:2005.141652020</p>
<p>A Bryman, Social research methods. Oxford university press2016</p>
<p>Surveying Generative AI's Economic Expectations. L Bybee, 10.48550/arXiv.2305.02823arXiv:2305.028232023</p>
<p>Does ChatGPT resemble humans in language use?. Z G Cai, D A Haslett, X Duan, S Wang, M J Pickering, 10.48550/arXiv.2303.08014arXiv:2303.080142023</p>
<p>The Role of Large Language Models in the Recognition of Territorial Sovereignty: An Analysis of the Construction of Legitimacy. F Castillo-Eslava, C Mougan, A Romero-Reche, S Staab, 10.48550/arXiv.2304.06030arXiv:2304.060302023</p>
<p>Flute: Figurative language understanding through textual explanations. T Chakrabarty, A Saakyan, D Ghosh, S Muresan, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Generative intelligence and news communication: practical empowerment, conceptual challenge and role reshaping. C Chen, 2023aPress Circles</p>
<p>Chatgpt and other artificial intelligence applications speed up scientific writing. T J Chen, Journal of the Chinese Medical Association. 862023b</p>
<p>A Manager and an AI Walk into a Bar: Does ChatGPT Make Biased Decisions Like We Do?. Y Chen, M Andiappan, T Jenkin, A Ovchinnikov, 10.2139/ssrn.43803652023</p>
<p>ConvFinQA: Exploring the chain of numerical reasoning in conversational finance question answering. Z Chen, S Li, C Smiley, Z Ma, S Shah, W Y Wang, 10.18653/v1/2022.emnlp-main.421Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>PaLM: Scaling Language Modeling with Pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, 10.48550/arXiv.2204.02311arXiv:2204.023112022</p>
<p>Language models trained on media diets can predict public opinion. E Chu, J Andreas, S Ansolabehere, D Roy, arXiv:2303.167792023arXiv preprint</p>
<p>Structured, flexible, and robust: Benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks. K M Collins, C Wong, J Feng, M Wei, J B Tenenbaum, 10.48550/arXiv.2205.05718arXiv:2205.057182022</p>
<p>G Coppersmith, M Dredze, C Harman, 10.3115/v1/W15-1204Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality. the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical RealityDenver, ColoradoAssociation for Computational Linguistics2015CLPsych 2015 shared task: Depression and PTSD on Twitter</p>
<p>The benefits and limitations of chatgpt in business education and research: A focus on management science, operations management and data analytics. I Cribben, Y Zeinali, Operations Management and Data Analytics. 2023. March 29, 2023</p>
<p>Artificial intelligence bot chatgpt in medical research: the potential game changer as a double-edged sword. J Dahmen, M E Kayaalp, M Ollivier, A Pareek, M T Hirschmann, J Karlsson, P W Winkler, Knee Surgery, Sports Traumatology, Arthroscopy. 312023</p>
<p>P Danling, General Psychology. 2019Beijing Normal University Publishing Group5 ed.</p>
<p>Language models show human-like content effects on reasoning. I Dasgupta, A K Lampinen, S C Y Chan, A Creswell, D Kumaran, J L Mcclelland, F Hill, 10.48550/arXiv.2207.07051arXiv:2207.070512022</p>
<p>From human writing to artificial intelligence generated text: examining the prospects and potential threats of chatgpt in academic writing. I Dergaa, K Chamari, P Zmijewski, H B Saad, Biology of Sport. 402023</p>
<p>Mind meets machine: Unravelling GPT-4's cognitive psychology. S Dhingra, M Singh, V Sb, N Malviya, S S Gill, 10.48550/arXiv.2303.11436arXiv:2303.114362023</p>
<p>Genlangs" and Zipf's Law: Do languages generated by ChatGPT statistically look human?. J Diamond, 10.48550/arXiv.2304.12191arXiv:2304.121912023</p>
<p>Can ai language models replace human participants?. D Dillion, N Tandon, Y Gu, K Gray, T Donovan, K R Hoover, Trends in Cognitive Sciences. 2023. 2013Cengage LearningThe elements of social scientific thinking</p>
<p>Exploring GPT-3 Model's Capability in Passing the Sally-Anne Test A Preliminary Study in Two Languages. Z Dou, 10.31219/osf.io/8r3ma2023</p>
<p>The Political System: An Inquiry Into the State of Political Science. D Easton, M Elsherief, C Ziems, D A Muchlinski, 10.1086/291002Conference on Empirical Methods in Natural Language Processing. 1955. 202165Latent hatred: A benchmark for understanding implicit hate speech</p>
<p>Machine science. J Evans, A Rzhetsky, Science. 3292010</p>
<p>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. A K Farmer, R A Demers, S Feng, C Y Park, Y Liu, Y Tsvetkov, 10.48550/arXiv.2305.08283arXiv:2305.082832010. 2023aMIT PressA Linguistics Workbook: Companion to Linguistics, Sixth Edition</p>
<p>Body size as a metric for the affordable world. X Feng, S Xu, Y Li, J Liu, 10.1101/2023.03.20.5333362023b</p>
<p>What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory. R Fischer, M Luczak-Roesch, J A Karl, 10.48550/arXiv.2304.03612arXiv:2304.036122023</p>
<p>How ChatGPT is transforming the landscape of social network analysis and community building. M Frąckiewicz, 2023is-transforming-the-landscape-of-social-network-analysis-and-community-building/</p>
<p>The evolution of citation graphs in artificial intelligence research. M R Frank, D Wang, M Cebrian, I Rahwan, Nature Machine Intelligence. 12019</p>
<p>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. Y Fu, H Peng, T Khot, M Lapata, 10.48550/arXiv.2305.10142arXiv:2305.101422023</p>
<p>Misinfo reaction frames: Reasoning about readers' reactions to news headlines. S Gabriel, S Hallinan, M Sap, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. A Gatt, E Krahmer, Journal of Artificial Intelligence Research. 612018</p>
<p>Introduction to sociology. A Giddens, 2007W. W. Norton &amp; CoNew York</p>
<p>Artificial general intelligence: concept, state of the art, and future prospects. F Gilardi, M Alizadeh, M Kubli, B Goertzel, A Goli, A Singh, arXiv:2303.15056Language, time preferences, and consumer behavior: Evidence from large language models. Time Preferences, and Consumer Behavior: Evidence from Large Language Models. 2023. 2014. 2023. May 4, 20235Chatgpt outperforms crowd-workers for text-annotation tasks</p>
<p>Political bias in large language models. L Gover, Puget Sound Journal of Politics. 422023</p>
<p>Susceptibility to Influence of Large Language Models. L D Griffin, B Kleinberg, M Mozes, K T Mai, M Vau, M Caldwell, A Marvor-Parker, 10.48550/arXiv.2303.06074arXiv:2303.060742023</p>
<p>Artificial Intelligence Co-Piloted Auditing. H Gu, M Schreyer, K Moffitt, M A Vasarhelyi, 10.48550/arXiv.2305.05516arXiv:2305.05516Guo, F., 2023. GPT Agents in Game Theory Experiments. 2023</p>
<p>Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. T Hagendorff, 10.48550/arXiv.2303.13988arXiv:2303.139882023</p>
<p>Machine intuition: Uncovering human-like intuitive decision-making in GPT-3. T Hagendorff, S Fabi, M Kosinski, 10.48550/arXiv.2212.05206arXiv:2212.052062022</p>
<p>On Language and Linguistics. M A K Halliday, 2006A&amp;C Black</p>
<p>Using chatgpt to conduct a literature review. M Haman, M Školník, Accountability in Research. 2023</p>
<p>Human-like property induction is a challenge for large language models. S J Han, K J Ransom, A Perfors, C Kemp, Proceedings of the Annual Meeting of the Cognitive Science Society 44. the Annual Meeting of the Cognitive Science Society 442022</p>
<p>The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation. J Hartmann, J Schwenzow, M Witte, 10.48550/arXiv.2301.01768arXiv:2301.017682023</p>
<p>Using chatgpt to fight misinformation: Chatgpt nails 72% of 12,000 verified claims . Horton. E Hoes, S Altay, J Bermeo, Large language models as simulated economic agents: What can we learn from homo silicus. 2023. 2023</p>
<p>Chain of explanation: New prompting method to generate quality natural language explanation for implicit hate speech. F Huang, H Kwak, J An, 10.1145/3543873.3587320doi:10.1145/3543873.3587320Companion Proceedings of the ACM Web Conference 2023. New York, NY, USAAssociation for Computing Machinery2023a</p>
<p>Is ChatGPT better than human annotators? potential and limitations of ChatGPT in explaining implicit hate speech. F Huang, H Kwak, J An, 10.1145/3543873.3587368Companion Proceedings of the ACM Web Conference 2023, ACM. 2023b</p>
<p>G Irving, P Christiano, D Amodei, arXiv:1805.00899Ai safety via debate. 2018arXiv preprint</p>
<p>Political ideology detection using recursive neural networks. M Iyyer, P Enns, J Boyd-Graber, 10.3115/v1/P14-1105Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 52nd Annual Meeting of the Association for Computational LinguisticsBaltimore, MarylandAssociation for Computational Linguistics20141</p>
<p>Theory construction and model-building skills: A practical guide for social scientists. J Jaccard, J Jacoby, 2019Guilford publications</p>
<p>Hypothesis generation from text based on co-evolution of biomedical concepts. K Jha, G Xun, Y Wang, A Zhang, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>MPI: Evaluating and Inducing Personality in Pre-trained Language Models. G Jiang, M Xu, S C Zhu, W Han, C Zhang, Y Zhu, 10.48550/arXiv.2206.07550arXiv:2206.075502022a</p>
<p>Communitylm: Probing partisan worldviews from language models. H Jiang, D Beeferman, B Roy, D Roy, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022b</p>
<p>H Jiang, X Zhang, X Cao, J Kabbara, D Roy, 10.48550/arXiv.2305.02547arXiv:2305.02547PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. 2023</p>
<p>When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment. Z Jin, S Levine, Advances in Neural Information Processing Systems. 352022</p>
<p>Using ChatGPT to evaluate cancer myths and misconceptions: artificial intelligence and cancer information. S B Johnson, A J King, E L Warner, S Aneja, B H Kann, C L Bylund, 10.1093/jncics/pkad015doi:10.1093/jncics/pkad015JNCI Cancer Spectrum. 7152023</p>
<p>Evidence of behavior consistent with self-interest and altruism in an artificially intelligent agent. T Johnson, N Obradovich, 10.48550/arXiv.2301.02330arXiv:2301.023302023</p>
<p>Capturing Failures of Large Language Models via Human Cognitive Biases. E Jones, J Steinhardt, Advances in Neural Information Processing Systems. 352022</p>
<p>. K Joyce, L Smith-Doerr, S Alegria, S Bell, T Cruz, S G Hoffman, S U Noble, B Shestakofsky, 10.1177/2378023121999581Sociology of Artificial Intelligence: A Call for Research on Inequalities and Structural Change. Socius. 723780231219995812021</p>
<p>Forecasting geopolitical conflicts using gpt-3 ai: Reali-ty-check one year into the 2022 ukraine war. D Jungwirth, D Haluza, Social science research methods. 2023. 2017Shandong People's Publishing HouseJuren Lin</p>
<p>Geopolitical forecasting analysis of the russia-ukraine war using the expert's survey, predictioneer's game and gpt-3. Predictioneer's Game and GPT-3. K Kalinin, 2023. April 8, 2023</p>
<p>Estimating the Personality of White-Box Language Models. S R Karra, S T Nguyen, T Tulabandhula, 10.48550/arXiv.2204.12000arXiv:2204.120002023</p>
<p>Pursuing the golem of prague: Jewish culture and the invention of a tradition. H J Kieval, Modern Judaism. 171997</p>
<p>Natural language analyzed with ai-based transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. M King, O Kjell, K Kjell, H A Schwartz, N Sikström, S Kjell, K Schwartz, H A , 10.31224/2974GPT-4 aligns with the New Liberal Party, while other large language models refuse to answer political questions. 2023. 2023. 2022123918Ai-based large language models are ready to transform psychological health assessment</p>
<p>The social construction of technology: Structural considerations. H K Klein, D L Kleinman, Technology, &amp; Human Values. 272002Science</p>
<p>Theory of Mind May Have Spontaneously Emerged in Large Language Models. M Kosinski, 10.48550/arXiv.2302.02083arXiv:2302.020832023</p>
<p>Towards Understanding How Machines Can Learn Causal Overhypotheses. E Kosoy, D M Chan, A Liu, J Collins, B Kaufmann, S H Huang, J B Hamrick, J Canny, N R Ke, A Gopnik, 10.48550/arXiv.2206.08353arXiv:2206.083532022</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. M Krenn, A Zeilinger, Proceedings of the National Academy of Sciences. 1172020</p>
<p>Socially situated artificial intelligence enables learning from human interaction. R Krishna, D Lee, L Fei-Fei, M S Bernstein, Proceedings of the National Academy of Sciences. 1192022. e2115730119</p>
<p>Thus spoke GPT-3: Interviewing a large-language model on climate finance. P R Krugman, R Wells, 10.1016/j.frl.2022.103617arXiv:2303.15727Economics. Macmillan. Lamichhane, B. 531036172009. 2023. 2023arXiv preprintFinance Research Letters</p>
<p>Gpt-d: Inducing dementia-related linguistic anomalies by deliberate degradation of artificial neural language models. C Li, D Knopman, W Xu, T Cohen, S Pakhomov, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective. X Li, Y Li, S Joty, L Liu, F Huang, L Qiu, L Bing, 10.48550/arXiv.2212.10529arXiv:2212.105292023</p>
<p>Mitigating Political Bias in Language Models through Reinforced Calibration. R Liu, C Jia, J Wei, G Xu, L Wang, S Vosoughi, 10.1609/aaai.v35i17.17744Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>InstructGPT-Training language models to follow instructions with human feedback. O Long, W Jeff, J Xu, 10.48550/arXiv.2203.02155arXiv:2203.021552022</p>
<p>Can ChatGPT forecast stock price movements? return predictability and large language models. A Lopez-Lira, Y Tang, 10.2139/ssrn.4412788SSRN Electronic Journal. 2023</p>
<p>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. Y Lu, M Bartolo, A Moore, S Riedel, P Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Gender and Representation Bias in GPT-3 Generated Stories. L Lucy, D Bamman, 10.18653/v1/2021.nuse-1.5Proceedings of the Third Workshop on Narrative Understanding. the Third Workshop on Narrative UnderstandingAssociation for Computational Linguistics2021</p>
<p>Www'18 open challenge: Financial opinion mining and question answering. M Maia, S Handschuh, A Freitas, B Davis, R Mcdermott, M Zarrouk, A Balahur, 10.1145/3184558.31923012018</p>
<p>Good debt or bad debt: Detecting semantic orientations in economic texts. P Malo, A Sinha, P J Korhonen, J Wallenius, P Takala, Journal of the Association for Information Science and Technology. 652013</p>
<p>Sad: A stress annotated dataset for recognizing everyday stressors in sms-like conversational systems. M L Mauriello, E T Lincoln, G Hon, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems URL. 2021</p>
<p>Machines who think: A personal inquiry into the history and prospects of artificial intelligence. P Mccorduck, C Cfe, R W Mcgee, 10.2139/ssrn.4359405Journal of Business Ethics. McGee, R.W.952004. 2023a. 2023bCRC PressUsing chatgpt to conduct literature searches: A case study</p>
<p>G Mialon, R Dessì, M Lomeli, C Nalmpantis, R Pasunuru, R Raileanu, B Rozière, T Schick, J Dwivedi-Yu, A Celikyilmaz, arXiv:2302.07842Augmented language models: a survey. 2023arXiv preprint</p>
<p>Who is GPT-3? An exploration of personality, values and demographics. M Miotto, N Rossberg, B Kleinberg, Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS). the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS)Abu Dhabi, UAEAssociation for Computational Linguistics2022</p>
<p>Politifact fact check dataset. R Misra, 10.13140/RG.2.2.29923.225662022</p>
<p>S Mohammad, S Kiritchenko, P Sobhani, 10.18653/v1/S16-1003Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). the 10th International Workshop on Semantic Evaluation (SemEval-2016)San Diego, CaliforniaAssociation for Computational Linguistics2016SemEval-2016 task 6: Detecting stance in tweets</p>
<p>More human than human: Measuring chatgpt political bias. F Motoki, V P Neto, V Rodrigues, Public Choice. 2023</p>
<p>Available at SSRN 4384861 . OpenAI. P Niszczota, S Abbas, How should AI systems behave, and who should decide? URL. 2023Gpt as a financial advisor</p>
<p>. Openai, 10.48550/arXiv.2303.08774arXiv:2303.087742023GPT-4 Technical Report</p>
<p>J S Park, J C O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, 10.48550/arXiv.2304.03442arXiv:2304.03442Generative Agents: Interactive Simulacra of Human Behavior. 2023a</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. J S Park, L Popowski, C Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Can chatgpt be used to generate scientific hypotheses. Y J Park, D Kaplan, Z Ren, C W Hsu, C Li, H Xu, S Li, J Li, arXiv:2304.122082023barXiv preprint</p>
<p>AI Psychometrics: Using psychometric inventories to obtain psychological profiles of large language models. M Pellert, C Lechner, C Wagner, B Rammstedt, M Strohmaier, 10.31234/osf.io/jv5dt2022</p>
<p>Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics. S Phelps, Y I Russell, 10.48550/arXiv.2305.07970arXiv:2305.079702023</p>
<p>Identifying depression on Reddit: The effect of training data. I Pirina, Ç Çöltekin, 10.18653/v1/W18-5903Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared Task. the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared TaskBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Philosophical and literary sources of frankenstein. B R Pollin, Comparative Literature. 171965</p>
<p>Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models. B Prystawski, P Thibodeau, N Goodman, 10.48550/arXiv.2209.08141arXiv:2209.081412022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 212020</p>
<p>Can chatgpt assess human personalities? a general evaluation framework. H Rao, C Leung, C Miao, S Rathje, D M Mirea, I Sucholutsky, R Marjieh, C Robertson, J J Van Bavel, arXiv:2303.012482023. 2023arXiv preprintGpt is an effective tool for multilingual psychological text analysis</p>
<p>Marketing with ChatGPT: Navigating the Ethical Terrain of GPT-Based Chatbot Technology. P Rivas, L Zhao, 10.3390/ai4020019AI. 42023</p>
<p>A review on sentiment analysis from social media platforms. M Rodríguez-Ibánez, A Casánez-Ventura, F Castejón-Mateos, P M Cuenca-Jiménez, 10.1016/j.eswa.2023.119862Expert Systems with Applications. 2232023. 119862</p>
<p>Empowerment series: Research methods for social work. A Rubin, E R Babbie, Cengage Learning. Russell, S.J. 2016. 2010Pearson Education, IncArtificial intelligence a modern approach</p>
<p>The Self-Perception and Political Biases of ChatGPT. J Rutinowski, S Franke, J Endendyk, I Dormuth, M Pauly, 10.48550/arXiv.2304.07333arXiv:2304.073332023</p>
<p>Synthesizing a talking child avatar to train interviewers working with maltreated children. P Salehi, S Z Hassan, M Lammerse, S S Sabet, I Riiser, R K Røed, M S Johnson, V Thambawita, S A Hicks, M Powell, Big Data and Cognitive Computing. 6622022</p>
<p>Whose Opinions Do Language Models Reflect?. S Santurkar, E Durmus, F Ladhak, C Lee, P Liang, T Hashimoto, 10.48550/arXiv.2303.17548arXiv:2303.175482023</p>
<p>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. M Sap, R Le Bras, D Fried, Y Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, 10.48550/arXiv.2211.05100arXiv:2211.051002023</p>
<p>M Shanahan, arXiv:2212.03551Talking about large language models. 2022arXiv preprint</p>
<p>A Sinha, T Khandait, Impact of News on the Commodity Market: Dataset and Results. K Arai, ChamSpringer International Publishing2021Advances in Information and Communication</p>
<p>Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. G Son, H Jung, M Hahm, K Na, S Jin, 10.48550/arXiv.2305.01505arXiv:2305.015052023</p>
<p>Accurate stock movement prediction with self-supervised learning from sparse noisy tweets. Y Soun, J Yoo, M Cho, 10.1109/BigData55660.2022.100207202022 IEEE International Conference on Big Data (Big Data). 2022</p>
<p>Putting GPT-3's Creativity to the (Alternative Uses) Test. C Stevenson, I Smal, M Baas, R Grasman, H Van Der Maas, 10.48550/arXiv.2206.08932arXiv:2206.089322022</p>
<p>Less likely brainstorming: Using language models to generate alternative hypotheses. L Tang, Y Peng, Y Wang, Y Ding, G Durrett, J F Rousseau, Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20231</p>
<p>Would chat gpt3 get a wharton mba. A prediction based on its performance in the operations management course. C Terwiesch, 2023WhartonMack Institute for Innovation Management/University of Pennsylvania/School Wharton</p>
<p>The Google engineer who thinks the company's AI has come to life. N Tiku, 2023Washington Post URL</p>
<p>LLaMA: Open and Efficient Foundation Language Models. H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, 10.48550/arXiv.2302.13971arXiv:2302.139712023</p>
<p>Do large language models know what humans know?. W M Trochim, J P Donnelly, S Trott, C Jones, T Chang, J Michaelov, B Bergen, S Tu, C Li, J Yu, X Wang, L Hou, J Li, 10.48550/arXiv.2304.14106arXiv:2304.14106ChatLog: Recording and Analyzing ChatGPT Across Time. New YorkMacmillan Publishing Company2001. 2023. 20232Research methods knowledge base</p>
<p>Dreaddit: A reddit dataset for stress analysis in social media. E Turcan, K Mckeown, Conference on Empirical Methods in Natural Language Processing. 2019</p>
<p>Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. P Törnberg, arXiv:2304.065882023</p>
<p>Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. T Ullman, 10.48550/arXiv.2302.08399arXiv:2302.083992023</p>
<p>Chatgpt can distinguish paranoid thoughts in patients with schizophrenia. K Uludag, SSRN 4391941. 2023</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023aarXiv preprint</p>
<p>Can chatgpt write a good boolean query for systematic review literature search?. S Wang, H Scells, B Koopman, G Zuccon, arXiv:2302.034952023barXiv preprint</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 352022</p>
<p>Building experiments: Testing social theory. D Willer, H A Walker, 2007Stanford University Press</p>
<p>Automated literature mining and hypothesis generation through a network of medical subject headings. S J Wilson, A D Wilkins, M V Holt, B K Choi, D Konecki, C H Lin, A Koire, Y Chen, S Y Kim, Y Wang, bioRxiv. 4036672018</p>
<p>D A Wood, M P Achhpilia, M T Adams, S Aghazadeh, K Akinyele, M Akpan, K D Allee, A M Allen, E D Almer, D Ames, The ChatGPT Artificial Intelligence Chatbot: How Well Does It Answer Accounting Assessment Questions? Issues in Accounting Education 38. 2023</p>
<p>Why not a sociology of machines? the case of sociology and artificial intelligence. S Woolgar, Sociology. 191985</p>
<p>Survey research and social science: History, current practice, and future prospects. Handbook of survey research. J D Wright, P V Marsden, 2010</p>
<p>Hybrid deep sequential modeling for social text-driven stock prediction. H Wu, W Zhang, W Shen, Proceedings of the 27th ACM International Conference on Information and Knowledge Management URL. the 27th ACM International Conference on Information and Knowledge Management URL2018</p>
<p>Large language models can be used to scale the ideologies of politicians in a zero-shot learning setting. P Y Wu, J Nagler, J A Tucker, S Messing, arXiv:2303.120572023a</p>
<p>BloombergGPT: A Large Language Model for Finance. S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, 10.48550/arXiv.2303.17564arXiv:2303.175642023b</p>
<p>The wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges. Q Xie, W Han, Y Lai, M Peng, J Huang, arXiv:2304.053512023a</p>
<p>Pixiu: A large language model, instruction data and evaluation benchmark for finance. Q Xie, W Han, X Zhang, ArXiv abs/2306.054432023b</p>
<p>Stock movement prediction from tweets and historical prices. Y Xu, S B Cohen, 10.18653/v1/P18-1183Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis. K Yang, S Ji, T Zhang, Q Xie, S Ananiadou, arXiv:2304.033472023aarXiv preprint</p>
<p>Towards interpretable mental health analysis with large language models. K Yang, S Ji, T Zhang, arXiv:2304.03347Tutorial on Social Research methods. Peking University Press2023b. 2013</p>
<p>How would stance detection techniques evolve after the launch of chatgpt?. B Zhang, D Ding, L Jing, arXiv:2212.145482023</p>
<p>OPT: Open Pre-trained Transformer Language Models. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, T Mihaylov, M Ott, S Shleifer, K Shuster, D Simig, P S Koura, A Sridhar, T Wang, L Zettlemoyer, 10.48550/arXiv.2205.01068arXiv:2205.010682022</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Calibrate Before Use: Improving Few-shot Performance of Language Models. Z Zhao, E Wallace, S Feng, D Klein, S Singh, Proceedings of the 38th International Conference on Machine Learning, PMLR. the 38th International Conference on Machine Learning, PMLR2021</p>
<p>Agents: An open-source framework for autonomous language agents. W Zhou, Y E Jiang, L Li, J Wu, T Wang, S Qiu, J Zhang, J Chen, R Wu, S Wang, arXiv:2309.078702023arXiv preprint</p>
<p>Can large language models transform computational social science?. C Ziems, W Held, O Shaikh, J Chen, Z Zhang, D Yang, arXiv:2305.035142023arXiv preprint</p>
<p>. P G Zimbardo, C Haney, W C Banks, D Jaffe, 1971The Stanford prison experiment. ZimbardoIncorporated</p>            </div>
        </div>

    </div>
</body>
</html>
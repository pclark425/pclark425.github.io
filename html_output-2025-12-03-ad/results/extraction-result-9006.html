<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9006 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9006</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9006</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-263909086</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-emnlp.111.pdf" target="_blank">Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9006.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9006.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-Davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-Davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI GPT-series completion model used as a representative advanced LLM in the experiments; queried via the OpenAI completion API with greedy decoding (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text-Davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI autoregressive transformer model queried via the completion API in this study; treated as one of three advanced closed-source models tested. Experiments set temperature=0 for deterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MoocRadar (Educational Diagnostic Assessment based on Bloom's Taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A fine-grained multi-aspect MOOC exercise repository annotated with Bloom's Taxonomy (knowledge types: factual, conceptual, procedural, meta; cognitive dimensions: remember, understand, apply, analyze, evaluate). Contains 8430 selected exercises (SC, MC, TF) used here to probe cognitive knowledge structure and diagnostic assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy (no context): 0.436 (43.6%); with retrieved context: 0.508 (50.8%). Per-question-type patterns: performed best on True/False and worst on Multiple Choice (see paper Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Average student score from MoocRadar real student behaviors: 0.746 (74.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Text-Davinci-003 performs substantially below the human average overall; context retrieval improves accuracy (~+7.2 percentage points). Shows primacy-related sensitivity to option order and lower similarity to human knowledge-structure (Likeness = 0.262).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot prompting with tailored instructions per question type; two most related subtitles retrieved by BM25 from MOOCCubeX placed as context when used; deterministic sampling (temperature=0); scoring: 1 point per question, MC partial credit 0.5 for partially correct. Human baselines were computed from average student behaviors on MoocRadar. Model answers and explanations were manually annotated by educational experts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Closed-source model with no parameter-level analysis; dataset has imbalanced categories; evaluation is on MOOC-style exercises (educational, Bloom-annotated) rather than classical cognitive-psychology tasks; explanations were judged by human annotators but contexts sometimes hurt explanation accuracy; observed primacy bias and instability in some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9006.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9006.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (Chat-GPT in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI chat-completion model (GPT-3.5-chat family as used in this study) evaluated on MoocRadar to probe cognitive knowledge structure and error/deficit patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat-completion model (referred to as Chat-GPT in the paper) queried via the chat completion API with deterministic decoding (temperature=0); included as a mid-generation model between Text-Davinci-003 and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MoocRadar (Educational Diagnostic Assessment based on Bloom's Taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same as above: MOOC exercise set annotated with Bloom's Taxonomy covering SC, MC, TF items across STEM, social science, humanities, and other disciplines; intended to probe cognitive dimensions and knowledge types.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy (no context): 0.506 (50.6%); with retrieved context: 0.526 (52.6%). Demonstrated better robustness to option-order manipulations than the other tested models and intermediate Likeness to humans (0.396).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Average student score from MoocRadar: 0.746 (74.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ChatGPT outperforms Text-Davinci-003 overall but remains below the human average; it is more robust to option-order changes (less primacy bias) compared to GPT-3.5 and GPT-4; context yields a modest performance gain.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot evaluation; prompts tailored to question type and instruction; retrieval of two BM25 contexts from MOOCCubeX optionally prepended; temperature=0; manual annotation of answers and explanations for evaluation. Scoring and partial-credit rules as above.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Still below human average on overall score; while more robust to option ordering, it can still fail on simpler items (cases shown where ChatGPT had correct knowledge but gave wrong answer depending on option order or question form); like other models, worse performance on MC and intermediate Bloom levels; closed-source so no parameter analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9006.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9006.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The most advanced OpenAI model evaluated in the study; shows highest task accuracy among tested models and the greatest similarity to human knowledge-structure as measured by a Pearson-correlation Likeness metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 model accessed via the chat completion API for this study, with deterministic decoding (temperature=0); included as the strongest-performing closed-source LLM in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MoocRadar (Educational Diagnostic Assessment based on Bloom's Taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MoocRadar: MOOC exercises annotated for Bloom's Taxonomy (knowledge types and cognitive dimensions) across multiple disciplines and question formats (SC, MC, TF). Used to probe deficits, error patterns, robustness, and knowledge-structure similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy (no context): 0.657 (65.7%); with retrieved context: 0.687 (68.7%). Per the paper, GPT-4 can outperform humans in some discipline-specific settings (e.g., STEM) when provided with contextual retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Average student score from MoocRadar: 0.746 (74.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 is the top-performing model but remains below the human average overall; with context it approaches but in many cases does not exceed human mean, although the authors note GPT-4 can outperform humans in STEM when given context. GPT-4 also shows the highest Likeness to human knowledge-structure (0.474).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot evaluation; chat completion API; temperature=0; two BM25-retrieved course-subtitle contexts optionally prepended; manual annotation of both answers and explanations by educational experts; scoring: 1 per question, 0.5 partial credit for MC. Robustness tests included re-ordering options (first vs last) to reveal primacy effects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although strongest of the tested models, GPT-4 exhibits instability (primacy effect) in some single-choice manipulations where changing option order caused it to change answers and produce conflicting but confident explanations; explanation accuracy can lag answer accuracy (particularly on MC); dataset imbalance and the educational nature of the test limit claims about general cognitive-psychology constructs; experiments only include three closed-source models, preventing parameter-level analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mind meets machine: Unravelling gpt-4's cognitive psychology <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>MoocRadar: A fine-grained and multi-aspect knowledge repository for improving cognitive student modeling in moocs <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9006",
    "paper_id": "paper-263909086",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "Text-Davinci-003",
            "name_full": "Text-Davinci-003",
            "brief_description": "An OpenAI GPT-series completion model used as a representative advanced LLM in the experiments; queried via the OpenAI completion API with greedy decoding (temperature=0).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Text-Davinci-003",
            "model_description": "OpenAI autoregressive transformer model queried via the completion API in this study; treated as one of three advanced closed-source models tested. Experiments set temperature=0 for deterministic outputs.",
            "model_size": null,
            "test_battery_name": "MoocRadar (Educational Diagnostic Assessment based on Bloom's Taxonomy)",
            "test_description": "A fine-grained multi-aspect MOOC exercise repository annotated with Bloom's Taxonomy (knowledge types: factual, conceptual, procedural, meta; cognitive dimensions: remember, understand, apply, analyze, evaluate). Contains 8430 selected exercises (SC, MC, TF) used here to probe cognitive knowledge structure and diagnostic assessment.",
            "llm_performance": "Overall accuracy (no context): 0.436 (43.6%); with retrieved context: 0.508 (50.8%). Per-question-type patterns: performed best on True/False and worst on Multiple Choice (see paper Table 5).",
            "human_baseline_performance": "Average student score from MoocRadar real student behaviors: 0.746 (74.6%).",
            "performance_comparison": "Text-Davinci-003 performs substantially below the human average overall; context retrieval improves accuracy (~+7.2 percentage points). Shows primacy-related sensitivity to option order and lower similarity to human knowledge-structure (Likeness = 0.262).",
            "experimental_details": "Zero-shot prompting with tailored instructions per question type; two most related subtitles retrieved by BM25 from MOOCCubeX placed as context when used; deterministic sampling (temperature=0); scoring: 1 point per question, MC partial credit 0.5 for partially correct. Human baselines were computed from average student behaviors on MoocRadar. Model answers and explanations were manually annotated by educational experts.",
            "limitations_or_caveats": "Closed-source model with no parameter-level analysis; dataset has imbalanced categories; evaluation is on MOOC-style exercises (educational, Bloom-annotated) rather than classical cognitive-psychology tasks; explanations were judged by human annotators but contexts sometimes hurt explanation accuracy; observed primacy bias and instability in some conditions.",
            "uuid": "e9006.0",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (Chat-GPT in paper)",
            "brief_description": "OpenAI chat-completion model (GPT-3.5-chat family as used in this study) evaluated on MoocRadar to probe cognitive knowledge structure and error/deficit patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "OpenAI chat-completion model (referred to as Chat-GPT in the paper) queried via the chat completion API with deterministic decoding (temperature=0); included as a mid-generation model between Text-Davinci-003 and GPT-4.",
            "model_size": null,
            "test_battery_name": "MoocRadar (Educational Diagnostic Assessment based on Bloom's Taxonomy)",
            "test_description": "Same as above: MOOC exercise set annotated with Bloom's Taxonomy covering SC, MC, TF items across STEM, social science, humanities, and other disciplines; intended to probe cognitive dimensions and knowledge types.",
            "llm_performance": "Overall accuracy (no context): 0.506 (50.6%); with retrieved context: 0.526 (52.6%). Demonstrated better robustness to option-order manipulations than the other tested models and intermediate Likeness to humans (0.396).",
            "human_baseline_performance": "Average student score from MoocRadar: 0.746 (74.6%).",
            "performance_comparison": "ChatGPT outperforms Text-Davinci-003 overall but remains below the human average; it is more robust to option-order changes (less primacy bias) compared to GPT-3.5 and GPT-4; context yields a modest performance gain.",
            "experimental_details": "Zero-shot evaluation; prompts tailored to question type and instruction; retrieval of two BM25 contexts from MOOCCubeX optionally prepended; temperature=0; manual annotation of answers and explanations for evaluation. Scoring and partial-credit rules as above.",
            "limitations_or_caveats": "Still below human average on overall score; while more robust to option ordering, it can still fail on simpler items (cases shown where ChatGPT had correct knowledge but gave wrong answer depending on option order or question form); like other models, worse performance on MC and intermediate Bloom levels; closed-source so no parameter analysis.",
            "uuid": "e9006.1",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "The most advanced OpenAI model evaluated in the study; shows highest task accuracy among tested models and the greatest similarity to human knowledge-structure as measured by a Pearson-correlation Likeness metric.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 model accessed via the chat completion API for this study, with deterministic decoding (temperature=0); included as the strongest-performing closed-source LLM in the experiments.",
            "model_size": null,
            "test_battery_name": "MoocRadar (Educational Diagnostic Assessment based on Bloom's Taxonomy)",
            "test_description": "MoocRadar: MOOC exercises annotated for Bloom's Taxonomy (knowledge types and cognitive dimensions) across multiple disciplines and question formats (SC, MC, TF). Used to probe deficits, error patterns, robustness, and knowledge-structure similarity to humans.",
            "llm_performance": "Overall accuracy (no context): 0.657 (65.7%); with retrieved context: 0.687 (68.7%). Per the paper, GPT-4 can outperform humans in some discipline-specific settings (e.g., STEM) when provided with contextual retrieval.",
            "human_baseline_performance": "Average student score from MoocRadar: 0.746 (74.6%).",
            "performance_comparison": "GPT-4 is the top-performing model but remains below the human average overall; with context it approaches but in many cases does not exceed human mean, although the authors note GPT-4 can outperform humans in STEM when given context. GPT-4 also shows the highest Likeness to human knowledge-structure (0.474).",
            "experimental_details": "Zero-shot evaluation; chat completion API; temperature=0; two BM25-retrieved course-subtitle contexts optionally prepended; manual annotation of both answers and explanations by educational experts; scoring: 1 per question, 0.5 partial credit for MC. Robustness tests included re-ordering options (first vs last) to reveal primacy effects.",
            "limitations_or_caveats": "Although strongest of the tested models, GPT-4 exhibits instability (primacy effect) in some single-choice manipulations where changing option order caused it to change answers and produce conflicting but confident explanations; explanation accuracy can lag answer accuracy (particularly on MC); dataset imbalance and the educational nature of the test limit claims about general cognitive-psychology constructs; experiments only include three closed-source models, preventing parameter-level analyses.",
            "uuid": "e9006.2",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mind meets machine: Unravelling gpt-4's cognitive psychology",
            "rating": 2,
            "sanitized_title": "mind_meets_machine_unravelling_gpt4s_cognitive_psychology"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "MoocRadar: A fine-grained and multi-aspect knowledge repository for improving cognitive student modeling in moocs",
            "rating": 2,
            "sanitized_title": "moocradar_a_finegrained_and_multiaspect_knowledge_repository_for_improving_cognitive_student_modeling_in_moocs"
        }
    ],
    "cost": 0.01163875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach</p>
<p>Zheyuan Zhang zheyuan-22@mails.tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Jifan Yu 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Juanzi Li lijuanzi@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Lei Hou houlei@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach
AE4766523547252B8112242E46A92A5FRemembering RememberKnowIdentify... Understanding TranslateExplainInduce... Applying ProveEstimateExecute... Analyzing CompareSelectOrganize... Evaluating EvaluateJudgeCriticise... Creating DesignCreateProgram
Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence.Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains.However, cognitive research on the overall knowledge structure of LLMs is still lacking.In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy.We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities.This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs.By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.</p>
<p>Introduction</p>
<p>Large language models (LLMs), such as GPT series (Brown et al., 2020), Flan (Wei et al., 2022), and PaLM (Chowdhery et al., 2022), have gained significant attention worldwide due to their remarkable ability.Given their unprecedented human-like performances, researchers have started to explore alternative evaluation metrics beyond traditional benchmarks like MMLU (Hendrycks et al., 2021) and Big-Bench (Ghazal et al., 2017).</p>
<p>Existing Works on LLMs Evaluation with Exams.Researchers have long sought models capable of passing human exams (Nilsson, 2005).Recently, a new approach simulates professional exams designed for humans to evaluate LLMs.For example, OpenAI (2023) reports the performance of GPT series on a variety of exams, including AP exams, SAT, Leetcode, and so on.There are also emerging benchmarks that comprise common standardized exams, such as AGIEval (Zhong et al., 2023), C-Eval (Huang et al., 2023), M3Exam (Zhang et al., 2023), and CMExam (Liu et al., 2023).However, although standardized exams contain diverse information, these works condense them into a single overall score, lacking structured understanding of LLMs' knowledge and cognitive patterns.</p>
<p>For example, while LLMs demonstrate exceptional performance on tasks challenging for humans, they might still struggle with basic knowledge, as illustrated in Figure 1, which may lead to over-estimation of the validity of model generated contents.Therefore, there is a pressing need for further research of models' knowledge and cognitive distribution in comparison to humans.</p>
<p>Proposed Research.To investigate this problem, we draw inspiration from psychometric methods that use cognitive psychology theories to evaluate LLMs.This topic has gained traction as LLMs continue to demonstrate exceptional performances (Chollet, 2019;Singh et al., 2023;Bubeck et al., 2023).In this work, we adopt the Educational Diagnostic Assessment approach and leverage MoocRadar (Yu et al., 2023), a novel student exercise dataset annotated with Bloom's Taxonomy (Anderson and Krathwohl, 2001), to assess the cognitive capability of LLMs.Specifically, we delve into three primary research questions: 1) Performance Analysis: the proficiency and robustness of LLMs across various question domains; 2) Deficit Assessment: the knowledge structure and the extent to which LLMs are similar with humans; and 3) Error Assessment: the error pattern of LLMs in answers and explanations.Our findings contribute to a deeper understanding of the knowledge structure of LLMs and insights for evaluation.</p>
<p>Contributions.Our main contributions are:</p>
<p>(i) We introduce the topic of the cognitive knowledge structure of LLMs.(ii) We propose a method of Educational Diag-</p>
<p>⭕</p>
<p>Instructions</p>
<p>Single Choice</p>
<p>What's the answer of this single choice question?why?</p>
<p>True or False</p>
<p>Please determine true or false and provide your reason.</p>
<p>Multiple Choice</p>
<p>What's the answer of this multiple choice question?why?</p>
<p>Diagnostic</p>
<p>Performance</p>
<p>Disciplines Contexts</p>
<p>Deficit</p>
<p>Knowledge Cognitive</p>
<p>Error</p>
<p>Answers Explanations</p>
<p>Figure 1: ChatGPT correctly answers a question that is challenging with higher knowledge type and cognitive dimensions right (problem 2) but encounters difficulties in an easier one (problem 1).We design specific instructions to evaluate LLMs and assess their performances in three aspects.</p>
<p>nostic Assessment to evaluate LLMs on their cognitive knowledge structure.(iii) We assess LLMs' performance, deficits, and errors, gaining insights into their capabilities.</p>
<p>Method</p>
<p>Educational Diagnostic Assessment</p>
<p>In education scenarios, Diagnostic Assessment is a widely employed method to gauge students' knowledge structure (Falmagne et al., 2006), discovering their proficiency on certain subject matters and learning styles (Vuong et al., 2021), typically through sets of questions (Leighton and Gierl, 2007).Two main approaches of Diagnostic Assessment include deficit assessment, which focuses on identifying and addressing knowledge gaps in various domains and the degree of knowledge mastery, and error assessment, which focuses on error patterns and strategies for correction (Bejar, 1984).Drawing inspiration from Diagnostic Assessment methods, in this work, based on Bloom's Taxonomy, we use deficit assessment to test the accuracy of models on a wide range of exercises, and error assessment on their answers and explanations.</p>
<p>Experimental Setup</p>
<p>Dataset.In this section, we introduce the dataset utilized for assessment, MoocRadar, offering an extensive overview of its general information.</p>
<p>MoocRadar is a fine-grained and multi-aspect exercise repository designed for cognitive modeling and educational diagnostic assessment.We carefully select 8453 questions appropriate for model evaluation, which fall into three types: single choice (SC), multiple choice (MC), and true or false (TF).Additionally, we exclude the dimension of Create because of the scarcity of related exercises.We further classify these questions into four disciplines by their course information, including STEM, social science, humanity, and others.We test the performance of models on them and analyze the distribution of these features.More details of MoocRadar are illustrated in the appendix.</p>
<p>Model Selection.We carefully choose 3 advanced models that have consistently demonstrated leading performance and are widely recognized in the field, including: Text-Davinci-003, Chat-GPT, and GPT-4, which represent a series of most acknowledged models.All experiments are performed using the APIs provided by OpenAI.Specif-ically, we use the completion API for Text-Davinci-003 and the chat completion API for ChatGPT and GPT-4.To ensure consistency in the quality of the responses, we set the temperature to 0 to get greedy search responses generated by each model.</p>
<p>Experimental Design.As shown in Figure 1, we design different prompts tailored to each type of exercises to query LLMs for both answers and explanation.All tasks are conducted in zero-shot scenario.To simulate human-like behavior that solving exercises with relevant knowledge, we leverage the BM25 algorithm to retrieve the two most related discussions from the subtitles in the corresponding courses in MOOCCubeX (Yu et al., 2021) and test their effect.Moreover, we extract real student behaviors on MoocRadar dataset from MOOCCubeX and calculate their average scores to serve as a reference of humans.Based on both results from human and LLMs, this work provides a road map with investigation to the following research questions:</p>
<p>(RQ1) Performance Analysis: What's the features of LLMs' basic performance on different disciplines and their robustness to these questions?</p>
<p>(RQ2) Deficit Assessment: According to Bloom Taxonomy, compared with humans, what knowledge distribution does LLMs demonstrate?Are they similar to humans in knowledge structure?</p>
<p>(RQ3) Error Assessment: Based on answers and explanations, what's their pattern of errors?</p>
<p>Experiment</p>
<p>In this section, we conduct experiments and analyze the results from three perspectives in the following subsections.We assign a score of 1 to each question type.Following standardized exams, for multiple questions, models receive a score of 0.5 if they fail to select all correct options.We then calculate the average score across questions.</p>
<p>Performance Analysis</p>
<p>Firstly, we assess their performance both with and without contexts, compare their performance in different disciplines, and examine their robustness.</p>
<p>Disciplines and Context.We exhibit scores of model answers with or without context on the four disciplines (STEM, social science, humanity, and others).As shown in Table 2, the later versions of GPT significantly outperform previous models, with GPT-4 being the most advanced, but not better than humans' average.Additional knowledge from context indeed enhances the performance of the models.Comparatively, STEM exercises are more challenging as illustrated in human results, while LLMs demonstrate impressive capability in STEM knowledge.GPT-4 even outperforms humans with context.However, it is surprising that LLMs don't perform as effectively in social science and humanities exercises, even though these disciplines primarily involve natural language.Robustness.In single choice questions, we manipulate the order of the options by either placing the correct answer at the beginning or the end.This allows us to examine if such modifications affect the model's accuracy.As shown in table 3, we find that 1) ChatGPT is more robust to changing of options, while the other two exhibits a cognitive bias as Primacy Effect (Deese and Kaufman, 1957) that early appearance aids performance; 2) if the correct answer appears later in GPT-3.5 and GPT-4, they mistakenly change their answers and explanations; 3) later appearance causes less consistency in answers and explanations in less robust models.</p>
<p>Deficit Assessment</p>
<p>We utilize Bloom's Taxonomy in MoocRadar to demonstrate models' distribution in cognitive dimensions and knowledge types and design a score to measure similarities of models to humans.Bloom Taxonomy Distribution.As shown in Figure 2, we demonstrate the distribution based on Bloom's taxonomy, where deeper colors represent better performance.The 0 and 1 grids are due to the limited number of exercises, typically only one.Generally, both in knowledge types and cognitive dimensions, questions in the intermediate range are more challenging for models and humans.We design a similarity score for deeper understanding.Similarity Score.According to the accuracy of models in various dimensions of knowledge and cognition, we develop a metric to measure their similarity to humans, which primarily considers knowledge structure, beyond mere performance, and estimates the extent to which their cognitive structure is proportional to that of humans.Specifically, given a model M , the 4<em>5 vector of the model distribution in bloom's taxonomy x and human distribution y, convert x and y into 1</em>20 vectors x and ỹ, the similarity between M and human can be defined as: Likeness(M ) = ρ(x, ỹ), where ρ(x, ỹ) represents the Pearson Correlation Coefficient of x and ỹ.We calculate the Likeness of the three models in Table 4.The likeness also exhibits a rising tendency as the models evolve.Models that follow human instructions better are also more similar to humans in knowledge structure.</p>
<p>Error Assessment</p>
<p>In this section, we analyze the error of each models, by delving into their explanation of their answers.Explanation Accuracy.Table 5 demonstrate the accuracy of answers in each type.We mainly find that: 1) Models perform best on TF and worst on MC.MC could be more difficult than SC and TF, because of more thinking steps (determine TF of each options, and select multiple ones).2) Explanations and answers are more consistent in TF than in SC and MC for the same reason, as there are more chances to make errors.3) Accuracy of explanations falls behind answers in MC, where models can select some of the correct options for the wrong reason.4) Context does not necessary aid and even hurt explanation performances, but indeed aids answer accuracy.More advanced models are more consistent in their answers and explanations.</p>
<p>Discussion</p>
<p>In this section, we discuss our findings on the proposed three research questions:</p>
<p>Performance Analysis.We exhibit different models' performance.Comparing with humans, they are less proficient in disciplines primarily involve natural language, but better at STEM.Though with sufficient knowledge, they might have hallucination on specific long-tail concepts in humanity and social science.LLMs are not robust in option orders, and exhibit a cognitive bias as Primacy Effect rather than Recency Effect.</p>
<p>Deficit Assessment.Models are less proficiency in the intermediate range of Bloom's Taxonomy.The reason could be that application-based ques-tions, such as solving mathematical problems and making deductions using chemical theorems, are prone to errors and are inherently challenging for models.For analyzing and evaluating questions, the strong linguistic capabilities of models allow them to excel in these tasks, and perform even better than intermediate-level questions.More advanced models demonstrate more similarity with humans in knowledge structure, which might be an additional effect of human alignment.</p>
<p>Error Assessment.By comparing different kinds of questions, we find that gap exists for models between knowledge and answers.They perform worse in multiple choices, as there are more thinking steps and error chances.Accuracy of explanations can be worse than answers: as models were asked to generate answers first, their explanation could shift due to wrong answers and question orders, and cause their hallucinations.Due to the limitations of autoregressive architecture (Bubeck et al., 2023), their errors could snowball.</p>
<p>Conclusion</p>
<p>In this work, we introduce a new research question on LLMs analyzing, which calls for a deeper understanding of the knowledge structure of these models.We use Educational Diagnostic Assessment as a tool to test the performance of LLMs on various dimensions, and develop a metric to measure the similarity of their knowledge structure with humans.We provide findings and discussion for insight into research on the cognition of LLMs.</p>
<p>Limitations</p>
<p>In this section, we describe the limitations of this work in terms of the dataset and experiments.</p>
<p>Dataset.We investigated the knowledge distribution of LLMs based on the MoocRadar dataset.MoocRadar is a fine-grained, well-structured dataset that is distinct from commonly used benchmarks in terms of knowledge annotation.However, as a dataset for educational diagnostic assessment, it's still limited in the following aspects: 1) Different categories of exercises (e.g.question type, disciplines) have an unbalanced distribution; 2) As demonstrated in the Robustness section, the performance of the models can vary due to different forms of exercises.</p>
<p>Experiment.Due to time and cost constrains, 1) we only included three LLMs by OpenAI, which are all closed-source models.Therefore, we did not conduct experiments at the parameter level.2) though we have discovered some phenomena, further experiments and deeper analysis are not conducted.We include some of them in the case study section in the appendix.</p>
<p>Future Works.Future works include 1) more models for experiments, 2) further exploration on robustness and similarity with humans, and 3) as the next step of diagnostic assessment, investigate how to optimize the knowledge structure of LLMs.</p>
<p>Ethics Statement</p>
<p>We foresee no ethic concerns in this work.The MoocRadar dataset employed in our research is publicly available, and it does not contain any personal information.</p>
<p>A Appendix</p>
<p>A.1 Details of Experiment</p>
<p>This subsection shows details of the dataset we use, and experiment for diagnostic assessment.</p>
<p>Problem Statistics.Table 6 demonstrates the details of the dataset we use, which is selected from the original MoocRadar.Generally, we include three question types (single choice, multiple choice, and true or false), four knowledge types (factual knowledge, conceptual knowledge, procedural knowledge, and meta knowledge), and five cognitive dimensions (remember, understand, apply, analyze, and evaluate), to form a total dataset of 8430 questions.Problem Examples.Table 7 demonstrates examples for each type of questions.There are two or more than two options in single choices and only one correct options, while multiple choices have more than one correct options.True or false questions should be answered as True or False.</p>
<p>Querying Details.For context settings, we use the BM25 algorithm to retrieve the two most related contexts from the subtitles of the corresponding class.As illustrated in Figure 3, for the question about the pioneer of mathematical logic, the BM25 algorithm retrieves context about the emergence and development of logic and the concept of mathematical logic.The two contexts will be placed before instruction, along with the questions and options to form the prompt, and fed into LLMs.In non-context settings, the context position will simply be empty.We also test different instructions to make sure that models will follow them to provide answers and explanations.Annotation details.To accurately assess the models' performance on both answers and explanations, we first invited educational experts to filter questions, because a part of questions in the dataset have accompanying pictures, which can't be input into models.Then we asked human annotators to evaluate the answers and explanations generated by the three models respectively, to prevent misjudgment in automatic evaluation tools.These annotators are familiar with MOOC exercises, and provided with the original questions and correct answers, and have access to the internet to determine accurately.</p>
<p>A.2 Cases</p>
<p>In this subsection, we demonstrate some of the error cases we have seen during our experiments, and hope to provide insights into the model's error patterns.The answer is A. Yes.This is because uniform acceleration refers to ... In this case, the change in velocity is aligned with the direction of acceleration.</p>
<p>Table 9: Case 2: the Primacy Effect of GPT-4.For the exactly same questions, switching options causes a direct change in GPT-4's answer, and both answers are accompanied with solid but conflict explanations, which proves the instability of its knowledge.Statistically, models have a tendency to choose more forward contents, which influence their rationale more, just like the Primacy Effect in cognitive psychology.This phenomenon is also observed in GPT-3.5.</p>
<p>Question</p>
<p>The term "logic" originally derives from: A: English, B: French, C: Latin, D: Greek.</p>
<p>Correct Answer D: Greek Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from the language: C.Latin.The term "logic" originated from the Latin word "logica" which itself was derived from the Greek word "logikē."Therefore, ...</p>
<p>Question</p>
<p>The term "logic" originally derives from: A: English, B: French, C: Greek, D: Latin.</p>
<p>Correct</p>
<p>Answer C: Greek Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from: C: Greek.The Greek philosopher Aristotle is often credited as one of the founders of formal logic and his work on reasoning and ...</p>
<p>Question</p>
<p>Which language does the term 'logic' originally derives from?</p>
<p>Correct Answer Greek</p>
<p>Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from the Greek word "logikē," which is derived from "logos," meaning "word," "reason," or "thought."</p>
<p>The study of logic can be traced back to ... Table 10: Case 3: Easier question forms aid performances.We have discussed the effect of different question types.ChatGPT have the correct knowledge about the origin of this word, but it cannot give correct answers, as the first question demonstrates.The following two questions types improve the performance of ChatGPT, by two different ways: moving the correct option forward which is consistent to Case 2, or ease the burden of models to answer by simplifying the question form.This case corroborates the findings that models are better at TF than SC or MC, because there are fewer thinking steps.</p>
<p>Cognitive Dimension: 2 Question Type: TF [Question] Methane has a center of symmetry.[Options] True, False Problem 2 Knowledge Type: 3 Cognitive Dimension: 4 Question Type: SC [Question] The incorrect combination of quantum numbers for outer atomic electrons among the following is: [Options] A: n=3, l=2, m=0, ms=+1/2, B: n=2, l=1, m=-1, ms=-1/2, C: n=4, l=1, m=2, ms=-1/2, D: n=3, l=1, m=-1</p>
<p>Figure 2 :
2
Figure 2: The distributions of accuracy in Bloom's Taxonomy of different models and average of students.</p>
<p>775 (0.771) 0.605 (0.667) 0.578 (0.691) Table5: Accuracy of explanations.TF, SC, and MC are short for the three question types.The numbers in parentheses represent answer accuracy.</p>
<p>…</p>
<p>Figure 3 :
3
Figure 3: Construction of prompts in experiments.</p>
<p>Question"</p>
<p>When undergoing uniform acceleration, the direction of velocity always lies along the same line as the direction of acceleration."Is it correct?The answer is A: No.The reason is that uniform acceleration only requires the acceleration to remain constant, but the direction of velocity and acceleration can be different.For example, in uniform circular motion, ...Question"When undergoing uniform acceleration, the direction of velocity always lies along the same line as the direction of acceleration."Is it correct?</p>
<p>Table 2 :
2
Models and human performance.S.S. and Human.are short for social science and humanity, and context is the result of models with context.Bold figures denote best performance.
Models Total STEMS.S.Human. OthersGPT-3.5 0.436 -context 0.5080.418 0.4680.461 0.5540.483 0.6140.421 0.504ChatGPT 0.506 -context 0.5260.480 0.4410.547 0.6420.533 0.6390.525 0.601GPT-4 0.657 -context 0.6870.613 0.6290.732 0.7650.684 0.7740.690 0.733Human 0.7460.6250.9240.8540.791</p>
<p>Table 3 :
3
Single choice accuracy of three models.First and last indicate the place of the correct answers.Real is when answers and explanations are both correct.Bold figures denote the best performance.
Models Answer ExplanationRealGPT-3.5 -first -last0.405 0.576 0.3620.436 0.554 0.3900.390 0.476 0.296ChatGPT -first -last0.489 0.440 0.4120.501 0.438 0.4300.487 0.438 0.412GPT-4 -first -last0.635 0.727 0.5200.643 0.728 0.5290.632 0.727 0.473</p>
<p>Table 4 :
4
Models' similarity to human, measured by Pearson Correlation Coefficient of knowledge structures.
ModelsGPT-3.5 ChatGPT GPT-4Likeness0.2620.3960.474</p>
<p>Table 6 :
6
Categories of data.Ex., SC, MC, and TF are short for Exercise, Single Choice, Multiple Choice and True or False questions.
Ex.NKnowledge TypesNCognitive DimensionsNSC5968 Factual2020 Remember1715MC1086 Conceptual4032 Understand4066TF1376 Procedural2268 Apply1667total 8430 Meta110 Analyze640// // Evaluate342Types QuestionsOptionsAnswersSCWhich of the following works was created by Vincent van Gogh?A: Les Nymphéas B: Sunflowers C: Grande OdalisqueBMCWhich of the following are rare earth elements?A: Uranium B: Lutecium D: Samarium C: DysprosiumB, C, DTFLight only exhibits the properties of waves.True FalseFalseTable 7: Question types examples: single choice (SC), multiple choice (MC), and true or false (TF). SC have only one correct option, while MC have 2 or more than 2 correct options. TF should be determined as True or False.</p>
<p>QuestionIf all birds can fly, then ostriches can fly.Ostriches are birds.Therefore, ostriches can fly.Does this reasoning contain any formal errors?
Correct AnswerNo.Model Answer (ChatGPT)Yes. ... While the premises state that every bird is capable of flying, it does not mean that every individual bird, including the ostrich, is capable of flying. Ostriches are actually flightless birds.QuestionIf all birds can fly, then sparrows can fly. Sparrows are birds. Therefore, sparrows can fly. Does this reasoning contain any formal errors?Correct AnswerNo.Model Answer (ChatGPT)No. The reasoning is an example of a valid deductive argument ... from a purely logical standpoint, the argument itself is valid.</p>
<p>Table 8 :
8
Case 1: Changing the entities infects the answer.If the question is consistent with its knowledge, ChatGPT indeed follows the task of focusing on the formal errors or purely logic.Otherwise, it fails to follow the question, as it refuses to override its own knowledge.This phenomenon indicates ChatGPT might be less robust when there are knowledge conflicts.</p>
<p>AcknowledgementsThis work is supported by a grant from the Institute for Guo Tsinghua University (2019GQB0003), and also supported by Tsinghua University Initiative Scientific Research Program.
A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives. W Lorin, David R Anderson, Krathwohl, 2001Longman</p>
<p>Educational diagnostic assessment. I Isaac, Bejar, Journal of Educational Measurement. 2121984</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, ArXiv preprint, abs/2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>On the measure of intelligence. François Chollet, ArXiv preprint. 2019. 1911.01547</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ArXiv preprint, abs/2204.023112022</p>
<p>Serial effects in recall of unorganized and sequentially organized verbal material. James Deese, Roger A Kaufman, Journal of experimental psychology. 5431801957</p>
<p>The assessment of knowledge, in theory and in practice. Jean-Claude Falmagne, Eric Cosyn, Jean-Paul Doignon, Nicolas Thiéry, Formal Concept Analysis: 4th International Conference, ICFCA 2006. Dresden, GermanySpringer2006. February 13-17, 2006Proceedings</p>
<p>Bigbench v2: the new and improved bigbench. Ahmad Ghazal, Todor Ivanov, Pekka Kostamaa, Alain Crolotte, Ryan Voong, Mohammed Al-Kateb, Waleed Ghazal, Roberto V Zicari, Proc. of ICDE. of ICDEIEEE2017</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proc. of ICLR. OpenReview.net. of ICLR. OpenReview.net2021</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, </p>
<p>Jacqueline Leighton, Mark Gierl, Cognitive diagnostic assessment for education: Theory and applications. Cambridge University Press2007</p>
<p>Benchmarking large language models on cmexam -a comprehensive chinese medical exam dataset. Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, Michael Lingzhi, Li , 2023</p>
<p>Human-level artificial intelligence? be serious! AI magazine. Nils J Nilsson, 200526</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Mind meets machine: Unravelling gpt-4's cognitive psychology. Manmeet Singh, S B Vaisakh, Neetiraj Malviya, abs/2303.11436ArXiv preprint. 2023</p>
<p>A data collection on secondary school students' stem performance and reading practices in an emerging country. Quan-Hoang Vuong, Viet-Phuong La, Manh-Toan Ho, Thanh-Hang Pham, Thu-Trang Vuong, Ha-My Vuong, Minh-Hoang Nguyen, Data Intelligence. 322021</p>
<ol>
<li>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, Proc. of ICLR. OpenReview.net. M Dai, V Quoc, Le, of ICLR. OpenReview.net</li>
</ol>
<p>Moocradar: A fine-grained and multi-aspect knowledge repository for improving cognitive student modeling in moocs. Jifan Yu, Mengying Lu, Qingyang Zhong, Zijun Yao, Shangqing Tu, Zhengshan Liao, Xiaoya Li, Manli Li, Lei Hou, Hai-Tao Zheng, ArXiv preprint, abs/2304.022052023</p>
<p>Mooccubex: A large knowledge-centered repository for adaptive learning in moocs. Jifan Yu, Yuquan Wang, Qingyang Zhong, Gan Luo, Yiming Mao, Kai Sun, Wenzheng Feng, Wei Xu, Shulin Cao, Kaisheng Zeng, Zijun Yao, Lei Hou, Yankai Lin, Peng Li, Jie Zhou, Bin Xu, Juanzi Li, Jie Tang, Maosong Sun, 10.1145/3459637.3482010Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21. the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, 2023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
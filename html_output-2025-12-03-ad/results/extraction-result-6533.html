<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6533 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6533</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6533</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-277955511</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.15241v3.pdf" target="_blank">MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual settings, where multilingual safety-aligned data is often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we introduce a multilingual guardrail with reasoning for prompt classification. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-based Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail, MrGuard, consistently outperforms recent baselines across both in-domain and out-of-domain languages by more than 15%. We also evaluate MrGuard's robustness to multilingual variations, such as code-switching and low-resource language distractors in the prompt, and demonstrate that it preserves safety judgments under these challenging conditions. The multilingual reasoning capability of our guardrail enables it to generate explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6533.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6533.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MrGuard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MrGuard: A Multilingual Reasoning Guardrail</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilingual safety guardrail that combines synthetic multilingual reasoning data, supervised fine-tuning, and a curriculum-based Group Relative Policy Optimization (GRPO) stage to produce multilingual reasoning and safety classification for prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B-Instruct (also reported with LLaMA-3.2-3B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B / 3B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Multilingual reasoning via supervised fine-tuning (generative explanations) + curriculum-based GRPO (reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (chain-like explanations) + reinforcement-learning policy optimization (curriculum-guided)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multilingual content-moderation benchmarks (RTP_LX, PTP_wildchat / Wildchat, Aya-red-teaming, MultiJail, XSafety)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary safety classification of user prompts across many languages, including robustness tests with code-switching and sandwich (low-resource distractor) attacks; also generates natural-language reasoning/explanations in the prompt language.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>89.27</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Recent guardrails (GuardR, Aegis-2.0, DUO-Guard, WildGuard)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>15.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>MrGuard uses multilingual generated explanations during SFT and a curriculum that introduces increasingly difficult, culturally-specific variants; authors report that adding reasoning improves generalization to non-English languages and that MrGuard outperforms baselines across in-domain and out-of-domain languages (authors state >15% improvement “consistently”). Reasoning is produced in the input language (language reward) improving interpretability and cross-lingual consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6533.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6533.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Group Relative Policy Optimization (GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning policy-optimization algorithm used to fine-tune LLMs with grouped sampling and relative advantage estimation; applied here with curriculum stages to improve multilingual reasoning and safety classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseekmath: Pushing the limits of mathematical reasoning in open language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π_sft (the supervised-fine-tuned LLaMA guardrail used as reference policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (reported for primary experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Group Relative Policy Optimization (GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>reinforcement learning (policy optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (policy optimization algorithm); used to elicit diverse multilingual reasoning via curriculum data</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RTP_LX / Aya / XSafety (ablation reported on these)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve generative reasoning quality and binary safety classification via RL fine-tuning on multilingual/curriculum data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>5.22</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Supervised fine-tuning only (π_sft)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>5.22</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report that applying GRPO (post SFT) significantly improves multilingual generalization and reasoning capabilities compared to supervised fine-tuning alone; GRPO is argued to outperform offline methods like DPO and be more efficient than on-policy PPO (cited). GRPO is combined with rule-based rewards (format, correctness, uncertainty, language) to shape multilingual reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6533.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6533.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum-based multilingual training (difficulty tiers via back-translation similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage curriculum that progressively introduces more difficult, culturally or linguistically specific prompt variants (derived via augmentation and back-translation) to guide the model from English-dominant competence toward native-like multilingual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Curriculum learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π_sft -> GRPO (fine-tuned LLaMA variants used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Curriculum learning with difficulty buckets (Diff function via back-translation cosine similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>curriculum (training schedule / staged data introduction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RTP_LX / Aya / XSafety (ablation reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Progressively train the guardrail on increasingly difficult multilingual/cultural prompt variants to improve safety classification and reasoning in target languages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>2.25</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>No curriculum (training without staged difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.25</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Ablation shows curriculum learning further improves generalization beyond GRPO and SFT; authors interpret that staged exposure to culturally/linguistically enriched variants helps the model develop language-specific reasoning and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6533.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6533.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanation Fidelity (EF) / Language Match (LM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explanation Fidelity and Language Match metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated (GPT-4.1-mini) and human-evaluated metrics that measure (a) whether generated explanations coherently reflect input and drive safety prediction (EF) and (b) whether reasoning is produced in the same language as the prompt (LM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MrGuard (LLaMA-3.1-8B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Generated multilingual explanations (post-training reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential explanatory generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RTP_LX (reported), human eval subset from RTP_LX (EN, ZH, HI)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate faithfulness and language alignment of generated reasoning/explanations for safety decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Explanation Fidelity (EF) rate and Language Match (LM) rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>87.39</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Automated judge (GPT-4.1-mini) EF rates by language are reported (e.g., EF EN=87.39, AR=80.57, ZH=93.33, RU=86.53, HI=88.97). Language Match rates are very high (LM ~97–99% across languages). Human evaluations on a sampled subset (EN, ZH, HI) give EF: EN 98.0, ZH 87.0, HI 86.0. Authors note these support fidelity of multilingual reasoning but also discuss failure taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6533.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6533.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt-engineering technique that elicits step-by-step intermediate reasoning traces from LLMs to improve complex reasoning performance; discussed as related work and a class of methods for eliciting reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>generic LLMs (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General technique to produce intermediate reasoning chains to solve complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Tree-of-Thought, Graph-of-Thought (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Mentioned in related work as a prompt engineering approach; the paper uses post-training/fine-tuning and RL rather than relying solely on in-context CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6533.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6533.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree/Graph-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought / Graph-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured prompt-based reasoning strategies that organize generation into tree- or graph-structured explorations to improve complex problem solving; cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>generic LLMs (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thought / Graph-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search / graph-search style generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Structured exploration strategies for complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Included in related work as alternative prompt-based reasoning strategies; not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6533.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6533.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GuardR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GuardR (GuardReasoner) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-capable English-centric guardrail cited as a recent baseline; reported to provide reasoning and better cross-dataset generalization than encoder-only classifiers but to perform worse than MrGuard on multilingual data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Guardreasoner: Towards reasoning-based llm safeguards.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B (as reported in Table 1 for GuardR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Post-training reasoning-capable guardrail (SFT with reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential explanatory generation (post-training)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous / similar (English-focused reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MultiJail (code-switching / sandwich attack evaluation shown in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect unsafe prompts and optionally generate reasoning; evaluated on multilingual robustness tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>85.09</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>MrGuard (Ours)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-11.41</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>GuardR is stronger than many encoder-only baselines on English but degrades on non-English inputs; MrGuard yields higher F1 and better robustness to multilingual perturbations. Table 3 (MultiJail) shows GuardR Avg-Orig F1 ~85.09 vs MrGuard ~96.50.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6533.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6533.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DUO-Guard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DUO-Guard (Duoguard)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed RL-based method for multilingual synthetic data generation and guardrail training (reported as a baseline in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Duoguard: A two-player rl-driven framework for multilingual llm guardrails.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QWEN-0.5B (reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.5B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RL-driven synthetic multilingual data generation (two-player framework)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>reinforcement-learning driven data generation / guardrail training</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (targets high-resource languages close to English per paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MultiJail (code-switching evaluation shown in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multilingual safety guardrail trained with an RL-based synthetic data generator and guardrail jointly updated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>51.9</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>MrGuard (Ours)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-44.6</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>DUO-Guard targets primarily high-resource languages; in the MultiJail code-switching evaluation it performs substantially worse than MrGuard, indicating lower multilingual robustness in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Duoguard: A two-player rl-driven framework for multilingual llm guardrails. <em>(Rating: 2)</em></li>
                <li>Aegis2.0: A diverse ai safety dataset and risks taxonomy for alignment of llm guardrails. <em>(Rating: 2)</em></li>
                <li>Guardreasoner: Towards reasoning-based llm safeguards. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6533",
    "paper_id": "paper-277955511",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "MrGuard",
            "name_full": "MrGuard: A Multilingual Reasoning Guardrail",
            "brief_description": "A multilingual safety guardrail that combines synthetic multilingual reasoning data, supervised fine-tuning, and a curriculum-based Group Relative Policy Optimization (GRPO) stage to produce multilingual reasoning and safety classification for prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B-Instruct (also reported with LLaMA-3.2-3B-Instruct)",
            "model_size": "8B / 3B",
            "reasoning_method_name": "Multilingual reasoning via supervised fine-tuning (generative explanations) + curriculum-based GRPO (reinforcement learning)",
            "reasoning_method_type": "sequential (chain-like explanations) + reinforcement-learning policy optimization (curriculum-guided)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Multilingual content-moderation benchmarks (RTP_LX, PTP_wildchat / Wildchat, Aya-red-teaming, MultiJail, XSafety)",
            "task_description": "Binary safety classification of user prompts across many languages, including robustness tests with code-switching and sandwich (low-resource distractor) attacks; also generates natural-language reasoning/explanations in the prompt language.",
            "performance_metric": "F1",
            "performance_value": 89.27,
            "comparison_target_method": "Recent guardrails (GuardR, Aegis-2.0, DUO-Guard, WildGuard)",
            "performance_difference": 15.0,
            "statistical_significance": null,
            "analysis_notes": "MrGuard uses multilingual generated explanations during SFT and a curriculum that introduces increasingly difficult, culturally-specific variants; authors report that adding reasoning improves generalization to non-English languages and that MrGuard outperforms baselines across in-domain and out-of-domain languages (authors state &gt;15% improvement “consistently”). Reasoning is produced in the input language (language reward) improving interpretability and cross-lingual consistency.",
            "ablation_study_present": true,
            "uuid": "e6533.0",
            "source_info": {
                "paper_title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GRPO",
            "name_full": "Group Relative Policy Optimization (GRPO)",
            "brief_description": "A reinforcement-learning policy-optimization algorithm used to fine-tune LLMs with grouped sampling and relative advantage estimation; applied here with curriculum stages to improve multilingual reasoning and safety classification.",
            "citation_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.",
            "mention_or_use": "use",
            "model_name": "π_sft (the supervised-fine-tuned LLaMA guardrail used as reference policy)",
            "model_size": "8B (reported for primary experiments)",
            "reasoning_method_name": "Group Relative Policy Optimization (GRPO)",
            "reasoning_method_type": "reinforcement learning (policy optimization)",
            "reasoning_style_diversity": "single style (policy optimization algorithm); used to elicit diverse multilingual reasoning via curriculum data",
            "benchmark_name": "RTP_LX / Aya / XSafety (ablation reported on these)",
            "task_description": "Improve generative reasoning quality and binary safety classification via RL fine-tuning on multilingual/curriculum data.",
            "performance_metric": "F1",
            "performance_value": 5.22,
            "comparison_target_method": "Supervised fine-tuning only (π_sft)",
            "performance_difference": 5.22,
            "statistical_significance": null,
            "analysis_notes": "Authors report that applying GRPO (post SFT) significantly improves multilingual generalization and reasoning capabilities compared to supervised fine-tuning alone; GRPO is argued to outperform offline methods like DPO and be more efficient than on-policy PPO (cited). GRPO is combined with rule-based rewards (format, correctness, uncertainty, language) to shape multilingual reasoning outputs.",
            "ablation_study_present": true,
            "uuid": "e6533.1",
            "source_info": {
                "paper_title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Curriculum",
            "name_full": "Curriculum-based multilingual training (difficulty tiers via back-translation similarity)",
            "brief_description": "A three-stage curriculum that progressively introduces more difficult, culturally or linguistically specific prompt variants (derived via augmentation and back-translation) to guide the model from English-dominant competence toward native-like multilingual reasoning.",
            "citation_title": "Curriculum learning.",
            "mention_or_use": "use",
            "model_name": "π_sft -&gt; GRPO (fine-tuned LLaMA variants used in experiments)",
            "model_size": "8B",
            "reasoning_method_name": "Curriculum learning with difficulty buckets (Diff function via back-translation cosine similarity)",
            "reasoning_method_type": "curriculum (training schedule / staged data introduction)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "RTP_LX / Aya / XSafety (ablation reported)",
            "task_description": "Progressively train the guardrail on increasingly difficult multilingual/cultural prompt variants to improve safety classification and reasoning in target languages.",
            "performance_metric": "F1",
            "performance_value": 2.25,
            "comparison_target_method": "No curriculum (training without staged difficulty)",
            "performance_difference": 2.25,
            "statistical_significance": null,
            "analysis_notes": "Ablation shows curriculum learning further improves generalization beyond GRPO and SFT; authors interpret that staged exposure to culturally/linguistically enriched variants helps the model develop language-specific reasoning and robustness.",
            "ablation_study_present": true,
            "uuid": "e6533.2",
            "source_info": {
                "paper_title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Explanation Fidelity (EF) / Language Match (LM)",
            "name_full": "Explanation Fidelity and Language Match metrics",
            "brief_description": "Automated (GPT-4.1-mini) and human-evaluated metrics that measure (a) whether generated explanations coherently reflect input and drive safety prediction (EF) and (b) whether reasoning is produced in the same language as the prompt (LM).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MrGuard (LLaMA-3.1-8B-Instruct)",
            "model_size": "8B",
            "reasoning_method_name": "Generated multilingual explanations (post-training reasoning)",
            "reasoning_method_type": "sequential explanatory generation",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "RTP_LX (reported), human eval subset from RTP_LX (EN, ZH, HI)",
            "task_description": "Evaluate faithfulness and language alignment of generated reasoning/explanations for safety decisions.",
            "performance_metric": "Explanation Fidelity (EF) rate and Language Match (LM) rate",
            "performance_value": 87.39,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Automated judge (GPT-4.1-mini) EF rates by language are reported (e.g., EF EN=87.39, AR=80.57, ZH=93.33, RU=86.53, HI=88.97). Language Match rates are very high (LM ~97–99% across languages). Human evaluations on a sampled subset (EN, ZH, HI) give EF: EN 98.0, ZH 87.0, HI 86.0. Authors note these support fidelity of multilingual reasoning but also discuss failure taxonomy.",
            "ablation_study_present": true,
            "uuid": "e6533.3",
            "source_info": {
                "paper_title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "Prompt-engineering technique that elicits step-by-step intermediate reasoning traces from LLMs to improve complex reasoning performance; discussed as related work and a class of methods for eliciting reasoning.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "generic LLMs (referenced)",
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "",
            "task_description": "General technique to produce intermediate reasoning chains to solve complex tasks.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": "Tree-of-Thought, Graph-of-Thought (mentioned)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Mentioned in related work as a prompt engineering approach; the paper uses post-training/fine-tuning and RL rather than relying solely on in-context CoT prompting.",
            "ablation_study_present": false,
            "uuid": "e6533.4",
            "source_info": {
                "paper_title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Tree/Graph-of-Thought",
            "name_full": "Tree-of-Thought / Graph-of-Thought",
            "brief_description": "Structured prompt-based reasoning strategies that organize generation into tree- or graph-structured explorations to improve complex problem solving; cited in related work.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "mention",
            "model_name": "generic LLMs (referenced)",
            "model_size": null,
            "reasoning_method_name": "Tree-of-Thought / Graph-of-Thought",
            "reasoning_method_type": "tree-search / graph-search style generation",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "",
            "task_description": "Structured exploration strategies for complex reasoning tasks.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": "Chain-of-Thought (mentioned)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Included in related work as alternative prompt-based reasoning strategies; not used in experiments.",
            "ablation_study_present": false,
            "uuid": "e6533.5",
            "source_info": {
                "paper_title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GuardR",
            "name_full": "GuardR (GuardReasoner) baseline",
            "brief_description": "A reasoning-capable English-centric guardrail cited as a recent baseline; reported to provide reasoning and better cross-dataset generalization than encoder-only classifiers but to perform worse than MrGuard on multilingual data.",
            "citation_title": "Guardreasoner: Towards reasoning-based llm safeguards.",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B (as reported in Table 1 for GuardR)",
            "model_size": "8B",
            "reasoning_method_name": "Post-training reasoning-capable guardrail (SFT with reasoning)",
            "reasoning_method_type": "sequential explanatory generation (post-training)",
            "reasoning_style_diversity": "homogeneous / similar (English-focused reasoning)",
            "benchmark_name": "MultiJail (code-switching / sandwich attack evaluation shown in Table 3)",
            "task_description": "Detect unsafe prompts and optionally generate reasoning; evaluated on multilingual robustness tests.",
            "performance_metric": "F1",
            "performance_value": 85.09,
            "comparison_target_method": "MrGuard (Ours)",
            "performance_difference": -11.41,
            "statistical_significance": null,
            "analysis_notes": "GuardR is stronger than many encoder-only baselines on English but degrades on non-English inputs; MrGuard yields higher F1 and better robustness to multilingual perturbations. Table 3 (MultiJail) shows GuardR Avg-Orig F1 ~85.09 vs MrGuard ~96.50.",
            "ablation_study_present": false,
            "uuid": "e6533.6",
            "source_info": {
                "paper_title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "DUO-Guard",
            "name_full": "DUO-Guard (Duoguard)",
            "brief_description": "A recently proposed RL-based method for multilingual synthetic data generation and guardrail training (reported as a baseline in the paper).",
            "citation_title": "Duoguard: A two-player rl-driven framework for multilingual llm guardrails.",
            "mention_or_use": "use",
            "model_name": "QWEN-0.5B (reported in Table 1)",
            "model_size": "0.5B",
            "reasoning_method_name": "RL-driven synthetic multilingual data generation (two-player framework)",
            "reasoning_method_type": "reinforcement-learning driven data generation / guardrail training",
            "reasoning_style_diversity": "mixed (targets high-resource languages close to English per paper)",
            "benchmark_name": "MultiJail (code-switching evaluation shown in Table 3)",
            "task_description": "Multilingual safety guardrail trained with an RL-based synthetic data generator and guardrail jointly updated.",
            "performance_metric": "F1",
            "performance_value": 51.9,
            "comparison_target_method": "MrGuard (Ours)",
            "performance_difference": -44.6,
            "statistical_significance": null,
            "analysis_notes": "DUO-Guard targets primarily high-resource languages; in the MultiJail code-switching evaluation it performs substantially worse than MrGuard, indicating lower multilingual robustness in these experiments.",
            "ablation_study_present": false,
            "uuid": "e6533.7",
            "source_info": {
                "paper_title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.",
            "rating": 2,
            "sanitized_title": "deepseekmath_pushing_the_limits_of_mathematical_reasoning_in_open_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Duoguard: A two-player rl-driven framework for multilingual llm guardrails.",
            "rating": 2,
            "sanitized_title": "duoguard_a_twoplayer_rldriven_framework_for_multilingual_llm_guardrails"
        },
        {
            "paper_title": "Aegis2.0: A diverse ai safety dataset and risks taxonomy for alignment of llm guardrails.",
            "rating": 2,
            "sanitized_title": "aegis20_a_diverse_ai_safety_dataset_and_risks_taxonomy_for_alignment_of_llm_guardrails"
        },
        {
            "paper_title": "Guardreasoner: Towards reasoning-based llm safeguards.",
            "rating": 2,
            "sanitized_title": "guardreasoner_towards_reasoningbased_llm_safeguards"
        }
    ],
    "cost": 0.016234,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety
26 Sep 2025</p>
<p>Yahan Yang 
Soham Dan sohamdan@microsoft.com 
Shuo Li lishuo1@seas.upenn.edu 
Dan Roth danr@seas.upenn.edu 
Insup Lee lee@seas.upenn.edu </p>
<p>University of Pennsylvania</p>
<p>University of Pennsylvania</p>
<p>University of Pennsylvania Oracle AI</p>
<p>University of Pennsylvania</p>
<p>MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety
26 Sep 2025E6E215CD530C00E74468DB7B54BF3C6FarXiv:2504.15241v3[cs.CL]
Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors.This vulnerability is exacerbated in multilingual settings, where multilingual safetyaligned data is often limited.Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications.In this work, we introduce a multilingual guardrail with reasoning for prompt classification.Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-based Group Relative Policy Optimization (GRPO) framework that further improves performance.Experimental results demonstrate that our multilingual guardrail, Mr-Guard, consistently outperforms recent baselines across both in-domain and out-of-domain languages by more than 15%.We also evaluate MrGuard's robustness to multilingual variations, such as code-switching and low-resource language distractors in the prompt, and demonstrate that it preserves safety judgments under these challenging conditions.The multilingual reasoning capability of our guardrail enables it to generate explanations, which are particularly useful for understanding languagespecific risks and ambiguities in multilingual content moderation.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities in cross-lingual knowledge transfer, enabling them to perform a variety of tasks across multiple languages even when fine-tuned on primarily monolingual datasets (Touvron et al., 2023;Brown et al., 2020;Qin Figure 1: The guardrails specialized in English (GuardR, (Liu et al., 2025)) are providing different predictions for English and Chinese inputs with the same semantic meaning.Our MrGuard can analyze the Chinese prompts with explanation and provide correct safety prediction.et al., 2024).This cross-lingual ability is largely attributed to their large-scale and diverse pretraining corpora, which allow LLMs to handle multilingual inputs without requiring significant multilingual data for downstream task adaptation.LLMs are increasingly being applied in a wide range of realworld applications, including conversational agents, educational tools, and medical assistants.However, despite these advancements, current LLMs are not yet robust or reliable enough for deployment in safety-critical environments.They can be intentionally misused to promote harmful behavior, generate offensive or biased content, or even bypass safety mechanisms through adversarial prompting (i.e., jailbreaking) (Andriushchenko et al., 2024;Chao et al., 2023).These vulnerabilities are further amplified in multilingual settings, particularly for low-resource languages, where models may lack proper safety alignment due to limited training signals or evaluation benchmarks (Deng et al., 2023;Wang et al., 2023).</p>
<p>To address these challenges, safety alignment strategies, most notably Reinforcement Learning from Human Feedback (RLHF) and Direct Pref-erence Optimization (DPO), which aim to align the behavior of LLMs with human values and thereby mitigate the risk of unsafe or harmful outputs (Ouyang et al., 2022;Rafailov et al., 2023).Another line of work focuses on building standalone safety classifiers or guardrails, which act as filters to detect and block unsafe user prompts or model generations without modifying the LLM itself (Inan et al., 2023;Ghosh et al., 2024).These lightweight safety modules are advantageous in being more efficient and easier to deploy or update (Table 1).Most existing methods are primarily English-centric (Liu et al., 2025;Kang and Li, 2024b;Yuan et al., 2024) which cannot handle multilingual content moderation (Yang et al., 2024).As shown in Figure 1, the guardrail model successfully identifies the unsafe user prompt in English but fails to detect its semantically equivalent counterpart in Chinese.Moreover, without explanations, it becomes difficult to understand the rationale behind the guardrail's decisions 1 .</p>
<p>Base Model</p>
<p>Data R GuardR (Liu et al., 2025) LlaMa-3.1-8B127k EN Yes DUO-Guard (Deng et al., 2025) QWEN-0.5B1679k EN 100k MUL No Aegis-2.0(Ghosh et al., 2025) LlaMa-3.To bridge this gap, our work is the first one to focus on building a guardrail tailored for multilingual safety scenarios with reasoning ability.We aim to design a robust, reasoning-aware safety guardrail that can effectively moderate harmful prompts across diverse languages and cultural contexts.Our contributions can be listed as follows 2 :</p>
<p>• We introduce MrGuard, a multilingual reasoning-enhanced guardrail for prompt 1 We interchangeably use guard and guardrail through the paper.</p>
<p>2 Our code is available at https://github.com/yangy96/mrguard moderation that improves performance and robustness across languages.Our approach combines curriculum learning (Bengio et al., 2009a) with Group Relative Policy Optimization (GRPO) (Shao et al., 2024b)  2 Related Work</p>
<p>Multilingual LLM Safety</p>
<p>While LLMs demonstrate strong cross-lingual capabilities on multilingual downstream tasks, their ability to handle unsafe content in multilingual settings remains largely unknown, and there is still significant room for improving their robustness to multilingual inputs.Prior studies (Wang et al., 2023;Deng et al., 2023) have shown that LLMs are vulnerable to non-English jailbreaking prompts, especially in low-resource languages.Follow-up work (Yoo et al., 2024) uses GPT-4 to combine parallel jailbreaking queries in Deng et al. (2023) from different languages into a single code-switching prompt, demonstrating that such prompts further increase the attack success rate compared to monolingual attacks.Recent work (de Wynter et al., 2024;Jain et al., 2024;Ye et al., 2023) collects multilingual moderation datasets to investigate the ability of LLMs to respond to multilingual harmful prompts/responses and assess whether guardrails effectively filter them out.They all show that existing guard or encoder-only classifiers cannot adequately handle multilingual content moderation.Upadhayay and Behzadan (2024) has introduced an attack against LLMs by embedding jailbreaking prompts within unrelated, low-resource, but safe inputs.Their results show that both proprietary and open-source models are vulnerable to these low-resource distractors, often following the embedded unsafe instructions and generating harmful content.These findings all underscore the urgent need for robust guardrails capable of detecting and mitigating unsafe behavior in multilingual settings.</p>
<p>Guardrails for safeguarding LLMs</p>
<p>Recent guardrail research (Li et al., 2024;Inan et al., 2023;Rebedea et al., 2023;Ghosh et al., 2025;Kang and Li, 2024a) have leveraged pretrained small language models (SLMs), such as LlaMa-2/3.1-7B(Touvron et al., 2023) and Mistral-7B (Jiang et al., 2023), to distinguish between safe and unsafe content.These methods have demonstrated promising results in detecting harmful inputs in English compared to encoder-only models.(Rafailov et al., 2023) to select difficult examples.However, these efforts largely focus on English.To address multilingual safety, Deng et al. (2025) has introduced an RL-based method for generating synthetic multilingual data by iteratively and jointly updating a synthetic data generator and a guardrail model.Yet, their work targets only high-resource languages close to English.In contrast, we focus on building MrGuard: a multilingual guardrail capable of handling cultural nuances and language-specific challenges spanning languages from several different families.</p>
<p>To enhance the reasoning capabilities of language models, we integrate curriculum learning with a reinforcement learning (RL) algorithm known as Group Relative Policy Optimization (GRPO) (Shao et al., 2024b).GRPO demonstrates superior performance compared to offline methods such as Direct Preference Optimization (DPO) (Rafailov et al., 2023), while offering im-proved computational efficiency over on-policy algorithms like Proximal Policy Optimization (PPO).A more comprehensive discussion of these RL approaches and their relationship to curriculum learning is provided in Appendix A.</p>
<p>Multilingual Guard with Reasoning</p>
<p>We detail our algorithm for building MrGuard: a guardrail with reasoning capabilities for multilingual content moderation in this section.The approach consists of three key components as shown in Figure 2: 1. Synthetic Data Generation -Inspired by Liu et al. (2025), we collect the analysis of safety for our seed data from a more powerful proprietary model, GPT-4o-mini3 and use the collected data to train our model.We additionally generate multilingual data and corresponding multilingual analysis using GPT-4o-mini.2. Supervised Fine-Tuning -We fine-tune an instruction-optimized model on the generated data to enable multilingual reasoning capabilities on content moderation, and safety classification.3. Curriculum-Based Optimization -We combine a three-stage curriculum learning framework and GRPO (Shao et al., 2024b) to align the model with desired multilingual moderation behavior.</p>
<p>Synthetic Data Generation</p>
<p>We consider an English safety training dataset, D = {(p l 0 i , y i )} N i=1 , where p l 0 i is an Englishlanguage prompt and y i ∈ {Safe, Unsafe} is its corresponding safety label.For each prompt p l 0 i , we prompt GPT to generate reasoning for why it is labeled as y i .This yields an augmented dataset with model-generated reasoning, denoted as
D l 0 = {(p l 0 i , e l 0 i , y i )} N i=1 .
Next, we subsample a smaller set from the original dataset D, forming a subset D sub .For each target language l k , we prompt GPT to translate each English prompt p l 0 i ∈ D sub into the target language, resulting in p l k i .We assume that the safety label y i is preserved across translations.To further ensure label consistency, we prompt GPT to reassess the safety of each translated prompt p l k i , If the reassessed label conflicts with the original y i , the corresponding example is discarded (3) curriculum-based Group Relative Policy Optimization (GRPO).The upper part illustrates the generation of multilingual translations and reasoning from English seed data using LLMs, followed by supervised fine-tuning.The lower part shows the construction of a curriculum by generating multilingual data with varying difficulty levels, which are then used to train the model via GRPO.</p>
<p>from the training set.We then prompt GPT with (p l k i , y i ) to generate the corresponding reasoning e l k i in language l k and English e l 0 i , yielding the dataset
D l k = {(p l k i , e l 0 i , e l k i , y i )} n i=1 .. Note that D l k contains significantly fewer examples than D l 0 .
Given K target languages {l 1 , . . ., l K }, we obtain the multilingual dataset:
D multi = {D l 0 , D l 1 , . . . , D l K }.</p>
<p>Supervised Fine-tuning</p>
<p>We perform supervised fine-tuning of the base model, denoted by π, using the multilingual dataset D multi , to enable the model to identify safe and unsafe prompts along with their reasoning.Given a data point (p l k i , e l 0 i , e l k i , y i ), we fine-tune the base model by applying cross-entropy loss on the tokens corresponding to both reasoning trajectories and the safety label.This enables the model to leverage the strong generalization capabilities of English while simultaneously developing multilingual reasoning skills, thereby preparing it for the subsequent reinforcement learning stage.We denote the resulting fine-tuned model as π sft .</p>
<p>Curriculum-based GRPO</p>
<p>In this stage, we employ reinforcement learning to further enhance detection performance by eliciting stronger reasoning capabilities.We begin by re-sampling a subset D l ′ 0 from the original English safety training dataset D l 0 .Each prompt in D l ′ 0 is then translated into the target languages l k , for k ∈ {1, . . ., K}.We then introduce a curriculumbased training schedule.The intuition is that, since the base model is initially fine-tuned on an English-dominant corpus, it is more familiar with English-specific nuances, such as slang and native expressions.To guide the model in progressively learning to handle other languages as second languages, we propose a curriculum that gradually introduces more challenging native multilingual variants.These variants are derived from English sentences and are incorporated stage by stage to support step-wise multilingual adaptation.To construct the curriculum, we introduce a novel difficulty function Diff that quantifies the difficulty of prompts in various target languages.Specifically, all the English prompts are assigned a baseline difficulty level of 0. For a prompt p l k in language l k and its corresponding English prompt p l 0 ∈ D l ′ 0 , we use the prompt template shown in Figure 6 to instruct GPT to generate two challenging variants, p l k ′ and p l k ′′ , enriched with slang, references to local places, institutions, foods, and other culturally or linguistically specific elements.A translation model π bt is then used to back-translate p l k ′ and p l k ′′ into English.The semantic similarity between the back-translated prompt and the original English prompt p l 0 is computed using the cosine similarity function cos.The difficulty of a back-translated prompt p ∈ {p l k , p l k ′ , p l k ′′ } is defined as:
Diff(p) =      0, cos(π bt (p), p l 0 ) &gt; t 1 , 1, cos(π bt (p), p l 0 ) ∈ (t 2 , t 1 ], 2, otherwise,
where t 1 and t 2 are threshold hyperparameters.</p>
<p>During training, prompts with difficulty level 0 are introduced in the first epoch.Prompts with levels 1 and 2 are progressively added in the second and third epochs, respectively, following the curriculum learning schedule.</p>
<p>After developing the curriculum, we apply GRPO to optimize the reference model π sft (Shao et al., 2024b).We utilize rule-based reward functions, with the following components:</p>
<p>Format reward (R f ): This reward penalizes formatting errors.If the output does not contain a properly formatted safety prediction (i.e., "Safety: safe" or "Safety: unsafe") which often happens in multilingual generation, the reward is −1.Otherwise, the reward is 1.</p>
<p>Correctness reward (R c ): If the safety prediction is correct, the reward is 1, otherwise, the reward is −1.</p>
<p>Uncertainty reward (R u ): We train an auxiliary encoder-only model π u to use the reasoning to decide whether the input is safe or not (binary classification) and take the softmax score as the reward.
R u = π u (q, ê), if prediction is correct −π u (q, ê) if prediction is incorrect
Language reward (R lang ): For the second and third stages, the input sentences are more native to the target language.We hypothesize that languagespecific reasoning enhances the model's understanding in this setting.To encourage the model to generate reasoning in the target language, we add this language reward,
R lang =      0.5, if difficulty = 1 1.0, if difficulty = 2 0.0, otherwise
Finally, the individual reward signals are combined linearly to produce a single scalar reward value:
R = R f + R c + R u + R lang .
With the reward signals defined, we apply the original GRPO algorithm to optimize the reference model π sft .For a detailed explanation of GRPO, please refer to Appendix E and Shao et al. (2024a).</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>In our experiments, we use the training set from Aegis-2.0-Safety (Ghosh et al., 2025) as the English seed data.Our base model is LLaMA-3.1-8B-Instructand LLaMA-3.2-3B-Instruct(Aaron Grattafiori, 2024), and we apply QLoRA (Dettmers et al., 2023) for parameter-efficient fine-tuning during both the SFT and GRPO stage4 .To construct the curriculum using back-translation, we employ the facebook/nllb-200-3.3B model for translation and use all-MiniLM-L6-v2 to compute the sentence embeddings of both the original and the back-translated sentences.For the difficulty threshold, we set t 1 = 0.85 and t 2 = 0.7.To determine the language of the sampled output, we utilize an xlm-based language detector5 .Our experiments divide the test sets into two categories: in-domain languages, which are covered during training, and out-of-domain languages, which are not seen during training.</p>
<p>Multilingual Content Moderation</p>
<p>Benchmark: Our experiments cover 5 recent multilingual safety benchmarks: PTP_wildchat (Jain et al., 2024) 6 (Wildchat), RTP_LX (de Wynter et al., 2024), aya-red-teaming (Aya) (Aakanksha et al., 2024) , MultiJail (Deng et al., 2023), and XSafety (Wang et al., 2023).We define five indomain languages-English (EN), Arabic (AR), Spanish (ES), Chinese (ZH), and Russian (RU), which are included in the training data.To further assess generalization, we also evaluate on three out-of-domain languages (listed in  with and without reasoning capabilities.The configurations and details of the different baseline models are summarized in Table 1.</p>
<p>Table 2 summarizes MrGuard's performance on several multilingual moderation benchmarks.Across both in-domain and out-of-domain languages, MrGuard consistently outperforms all baselines by a substantial margin.Additionally, we observe that guardrails with reasoning capabilities (GuardR and ours) generalize better across datasets and languages, but with our approach, MrGuard achieves state-of-the-art performance in multilingual scenarios.This showcases that MrGuard effectively captures language-specific nuances, as the test datasets are naturally generated or annotated by native speakers.Note that we restrict our experiments to compact models (≤ 7 B parameters) to ensure low latency and easy deployment, in line with other guardrails (see Table 1).Even at this scale, adding reasoning at the post-training stage yields large gains in multilingual classification.</p>
<p>Moreover, we present a language-wise breakdown in Figure 3 with more results in Appendix B7 .Our model consistently outperforms the baselines across a wide range of languages, although some baselines achieve comparable results on the English subset.We also observe that training on Englishonly data negatively impacts generalization to non-English languages.For example, although Aegis-2.0 and WildGuard perform well on English inputs, their performance degrades significantly on non-English data, whereas MrGuard maintains high performance and shows minimal performance drop, even on unseen languages.This further demonstrates the robustness and cross-lingual generalization capabilities of our model.</p>
<p>Furthermore, Figure 4 presents example reasoning generated by our multilingual guardrail in various languages, showing that the reasoning accurately analyzes and justifies the safety prediction of the input prompts.The reasoning can help users understand the rationale of MrGuard behind its safety decisions.</p>
<p>Robustness to Multilingual Perturbations</p>
<p>In this section, we investigate the potential of using guardrails to identify unsafe prompts that involve perturbations specific to the multilingual setting.We consider two existing multilingual attacks: 1) Yoo et al. (2024) generates code-switching prompts using two parallel datasets, MultiJail and XSafety, and GPT (CSRT) 2) Sandwich attack (Upadhayay and Behzadan, 2024) (Sandwich), where jailbreak- We benchmark several guardrail methods against the adversarial multilingual attacks.Table 3 and 4 reports the F1 scores before and after the attacks, along with the corresponding performance changes.As shown in the tables, all methods experience a decline in F1 score after the attack, demonstrating the effectiveness of both adversarial strategies.Notably, our method not only outperforms the baselines but also exhibits a smaller reduction in F1 score.Our experiments show that incorporating reasoning alongside safety classification significantly enhances the guardrail's robustness against multilingual adversarial prompts.</p>
<p>Discussion</p>
<p>In this section, we conduct a deeper analysis of our framework and results, including ablation experiments of the proposed approach, evaluation of the fidelity of reasoning and safety predictions, and cross-language consistency.</p>
<p>Models</p>
<p>Ablation Study</p>
<p>In this section, we conduct an ablation study to investigate the effectiveness of GRPO and curriculum learning, and various components of the reward function to show that all of them help improve the generalization of our guardrail's performance on different languages across different datasets.</p>
<p>Based on the results in Table 5, we first observe that both GRPO and curriculum learning significantly improve the performance compared to π sft .Consistent with prior work (DeepSeek-AI, 2025), post-training with GRPO improves the generalization across different datasets and enhances reasoning abilities.Moreover, the comparison between models trained with and without curriculum learning shows that gradually increasing the difficulty of training inputs, based on linguistic and cultural complexity, further enhances the model's multilingual understanding.This finding underscores the value of curriculum-based learning strategies in improving robustness and generalization for multi- lingual safety tasks.Furthermore, we show that all components in the reward functions positively contributes to the overall performance of MrGuard.Although removing the language reward leads to slightly better performance across different datasets, we find that the resulting model predominantly generates English reasoning.In practice, however, it is important for the guardrail to produce reasoning in the corresponding input language, making multilingual reasoning generation a valuable capability despite the marginal trade-off in accuracy.We leave the theoretical analysis of curriculum learning and reward function design of GRPO to future work.</p>
<p>Cross-lingual Consistency</p>
<p>One important characteristic for guardrails is that it assigns the same safety label to semantically equivalent prompts in different languages.To quantify this, we define the Cross-Lingual Consistency score as the fraction of parallel examples in which the model's safety predictions agree across languages (She et al., 2024).We report consistency score on XSafety, a parallel dataset, comparing English with each target language across several models in Figure 5. From the results we observe that although our algorithm does not explicitly train for consistency, we still see improved consistency, especially for unsafe prompt classification.As shown in Figure 10, MrGuard exhibits a much smaller performance drop between ID and OOD languages compared to the other baselines.</p>
<p>Quality of Reasoning</p>
<p>LLMs are likely to produce hallucinations, even when guided via chain-of-thought.MrGuard is intended to help users and regulators inspect and trust its decisions, making fidelity measurement crucial.To this end, we employ a stronger LLM, GPT-4.1mini, as a judge to automatically assess whether each explanation faithfully reflects the input and correctly drives the safety prediction.We define the Explanation Fidelity (EF) score as the fraction of reasoning sentences the judge labels as coherent out of the total sentences, and the results are shown in Table 6.Moreover, it is important to maintain the reasoning language in the same language as the prompt.We report the Language Match (LM) rate, which captures the percentage of cases where the generated reasoning is in the same language as the prompt.The results show that our reasoning is aligned with the semantics and the language of the prompt.We additionally perform human evaluation of the quality of the MrGuard's generated reasoning.From the RTP_LX dataset, we subsampled 100 examples and generated reasoning in English, Chinese, and Hindi.Human volunteers then evaluated fidelity using the same instructions provided to GPT-4o in Figure 17.We show that the reasoning remain high fidelity on these languages in Table 7 (Here Hindi is an unseen and relatively-low resource language).We also conducted human evaluation (Appendix C.2) on safety preservation of syntactic data generation.</p>
<p>Lang</p>
<p>% EN ZH HI</p>
<p>Fidelity 98.0 87.0 86.0</p>
<p>Table 7: Explanation fidelity rates for a sampled subset of the RTP_LX dataset, as given by human annotators.</p>
<p>Moreover, we perform a deeper analysis on the failure cases of MrGuard's reasoning and predictions.Below is the refined taxonomy for categorizing reasoning failures: A. Ambiguity: Covers both lexical ambiguity and unclear referents (e.g., pronouns without clear antecedents); B. Cultural Uncertainty: Reliance on culture-specific knowledge or assumptions that may not hold universally; C. Unsupported Inference: Jumps to conclusions without necessary premises or overgeneralizes from a single instance; D. Logical Inconsistency: Contradicts earlier steps or violates basic inference rules; E. Misclassification: Flags risks not present in the prompt.We apply GPT-4.1-mini as an automated judge to analyze the reasoning chains in the RTP_LX dataset.We report the percentage breakdown of reasoning assigned to each category in Table 8.We observe that the model rarely generates logic inconsistent reasoning but sometimes flag risks not present in the inputs.We also provide some qualitative examples of each category in Figure 16</p>
<p>Conclusion</p>
<p>In this work, we introduce MrGuard: a multilingual reasoning-enhanced guardrail for multilingual prompt moderation.Our method consists of three key stages: synthetic data generation, supervised fine-tuning, and reinforcement learning, where we adopt GRPO with a multi-stage curriculum that progressively introduces more cultural and language-specific elements.We conduct comprehensive experiments across multiple diverse and realistic multilingual content moderation benchmarks, including challenging scenarios involving code-switching, and demonstrate that our guardrail achieves state-of-the-art performance with reasoning.We also analyze the generated reasoning to validate its reliability and ensure consistent safety preservation across languages.The reasoning ability enables multilingual users to understand the decision from MrGuard.We believe this work is an important step toward enhancing the safety of LLMs in a multilingual world.</p>
<p>Limitations</p>
<p>Language and resource coverage Due to budget and computational limits, we generated synthetic data only for high-and mid-resource languages, and relied on Aegis-2.0 as our English seed dataset.Expanding to additional seed datasets and low-resource languages could further enhance model performance and broaden the safety taxonomy to better reflect diverse user needs.Additionally, while our guardrail demonstrates strong results on both in-distribution and out-of-distribution dataset and languages, the languages represented in our evaluation remain limited.Potential Bias We use a single LLM (GPT-4omini) as a judge to verify safety labels of translations, which may introduce bias inherent to the LLM.We acknowledge that relying on a single LLM for both generation and evaluation raises reliability concerns.As future work, we will explore ensembles of multiple LLMs for multilingual synthetic data generation and evaluation.Additionally, our current evaluation of reasoning coherence and faithfulness between explanations and final safety predictions relies on automated heuristics, which may not perfectly align with human judgments.Human Annotation To validate our synthetic data and the fidelity of LLM-generated reasoning, we conducted a small human-evaluation study on a subsampled dataset.Due to verification costs, we could not scale to multiple annotators or a larger sample size.</p>
<p>Ethical Statement</p>
<p>Our works aims to improve LLM safety for multilingual users by introducing a multilingual rea-soning guardrail, which is important for building a universally reliable LLM for safety-critical applications.The generated synthetic data and models will be released, accompanied by detailed usage guidelines to prevent misuse.</p>
<p>A Additional Related Work</p>
<p>LLM Reasoning.Several methods have been proposed to enhance the reasoning capabilities of large language models (LLMs), which can broadly be categorized into prompt engineering and posttraining approaches.Prompt engineering methods, such as Chain-of-Thought (Wei et al., 2023), leverage in-context demonstrations to elicit more coherent and structured reasoning trajectories.Building on this idea, Tree-of-Thought (Yao et al., 2023) and Graph-of-Thought (Besta et al., 2024) further improve reasoning by organizing generation within tree-and graph-based logical structures.These prompt-based techniques are post-hoc in nature, enhancing reasoning without modifying the model parameters.</p>
<p>In contrast, post-training approaches aim to directly optimize LLMs for improved reasoning.For instance, Muennighoff et al. (2025) and Chen et al. (2023) apply supervised fine-tuning with highquality, diverse demonstrations, while Xiong et al. (2024) utilize alignment strategies such as Direct Preference Optimization (DPO).More recently, reinforcement learning methods-including PPO and GRPO-have demonstrated strong performance in reasoning tasks (Shao et al., 2024a;DeepSeek-AI, 2025;Qwen et al., 2025;Kazemnejad et al., 2024).Among these, GRPO has gained particular attention for its superior computational efficiency compared to other reinforcement learning algorithms.</p>
<p>Curriculum Learning.Training machine learning models using a progression from easy to hard examples-known as curriculum learning (Bengio et al., 2009b)-has been shown to outperform standard training approaches based on random data shuffling (Soviany et al., 2022).This paradigm has been successfully applied in both supervised learning (Graves et al., 2017;Hacohen and Weinshall, 2019;Matiisen et al., 2020) and reinforcement learning (Narvekar et al., 2020;Ren et al., 2018;Florensa et al., 2017).More recently, curriculum learning has also been explored in the context of LLM alignment (Croitoru et al., 2025).</p>
<p>B Experiment Setup</p>
<p>We used the Huggingface framework (Wolf et al., 2020) to load dataset and evaluate the guardrails and applied the default greedy decoding for all guardrails.Our training is performed on 4 NVIDIA RTX A100 (80G) GPUs, and vLLM (Kwon et al., 2023) is used to optimize inference speed.</p>
<p>For training data configuration, during the SFT stage, we train on the full English seed dataset (30.3K examples) combined with the translations generated from the sampled 2,000 seed examples.At the GRPO stage, we subsample another 2,000 seed examples and generate the challenging variants for curriculum learning, and we additionally include the seed English samples in the first curriculum stage to avoid losing its English ability as described in Section 3.3.</p>
<p>We use the TRL library (von Werra et al., 2020) for both SFT and GRPO stage.For both stages, we set the LoRA rank and alpha to 32, with a dropout rate of 0.1.During SFT, we use a learning rate of SFT is 2e − 5 and train for 3 epochs.For GRPO, we set the learning rate to 1e − 5 and the number of training epoch is 1.We conduct a hyperparameter sweep over LoRA rank and alpha values {8, 16, 32}, and select the best configuration based on performance on the Aegis-2.0 validation dataset.All use of the packages and artifacts are consistent with their intended use and license.We used Chat-GPT to refine short sentences and paragraphs and to check for grammar errors.</p>
<p>C Dataset Details</p>
<p>In our training, we translate English seed data into RU, ES, ZH, AR, so these languages are considered as our in-domain languages.We benchmark our guardrail's performance on five multilingual datasets: PTP_wildchat (Jain et al., 2024), RTP_LX (de Wynter et al., 2024), aya-red-teaming (Aya) (Aakanksha et al., 2024) , MultiJail (Deng et al., 2023), and XSafety (Wang et al., 2023).For each dataset, we list the ID and out-of-domain (OOD) languages in Table 9.Several datasets (Aya, MultiJail, XSafety) are red-teaming datasets that consist solely of unsafe prompts intended to elicit harmful responses; we thus assume all prompts in these datasets are unsafe.For this purpose, we focus on a selected subset of topics in XSafety: Crimes and Illegal Activities, Goal Hijacking, Insult, Role Play Instruction, Unfairness and Discrimination, and Unsafe Instruction Topic.For RTP_LX, we consider prompts with an average toxicity score above 1.0 (on a scale from 1 to 5) as unsafe.For PTP_wildchat, we treat prompts with a prompt toxicity score above 0.1 (on a scale from 0 to 1) as unsafe.The language code is listed in Table 10.</p>
<p>C.1 Generation configuration</p>
<p>We provide the prompts used in our experiments in this section.For synthetic data generation, the prompt template is shown in Figure 6. Figure 7 shows the instruction used during both training and inference.</p>
<p>C.2 Human evaluation</p>
<p>We validated our LLM-generated synthetic translations (Section 3) by comparing the model's safety judgments with human judgments.We have subsampled 100 sentences for ZH, RU, and AR.For each language, we have one volunteer for each language, and the volunteers are native speakers in the target languages and are proficient in English.The guidelines of human evaluation is shown in Figure 8, and the results are listed in Table 11.The safety preservation rate is a three-point scale measurement (0-2).0: The safety label is not preserved; 1: the safety label is preserved, but meaning is altered; 2: both safety label and original meaning are preserved.</p>
<p>G.3 Additional Results on Multilingual Perturbations</p>
<p>We also perform code-switching and sandwich attack on XSafety dataset and the results are shown in Table 16 and Table 17.</p>
<p>G.3.1 Additional Results on Hyperparameter Search</p>
<p>Here we provide additional results of model trained with different difficulty thresholds in Table 18.</p>
<p>G.4 Additional Results on Ablation Study</p>
<p>Moreover, instead of using a curriculum-based language reward, we can assign a fixed reward to promote multilingual reasoning.In our experiments, we set a constant language reward of 0.5 for all non-English explanations (see Table 19).We here reported the language match rate of the model trained without R lang to show the motiva-</p>
<p>H Qualitative Results</p>
<p>We provided some qualitative results generated from our multilingual guardrail in Figure 15.</p>
<p>We have defined a taxonomy of failure cases and provide qualitative examples of each case from different languages.</p>
<p>I Evaluate Reasoning Fidelity</p>
<p>We evaluate the fidelity of reasoning in Section 5.3 and define a taxonomy for a deep analysis of reasoning generated by our guardrail.</p>
<p>Figure 2 :
2
Figure 2: Workflow of our three-stage approach: (1) synthetic data generation, (2) supervised fine-tuning, and(3) curriculum-based Group Relative Policy Optimization (GRPO).The upper part illustrates the generation of multilingual translations and reasoning from English seed data using LLMs, followed by supervised fine-tuning.The lower part shows the construction of a curriculum by generating multilingual data with varying difficulty levels, which are then used to train the model via GRPO.</p>
<p>Figure 3 :
3
Figure 3: F1 score breakdown on the RTP_LX dataset, evaluated across 8 target languages.Here EN, AR, ES, RU, and ZH are in-domain languages, and FR, HI, SW are out-of-domain languages.</p>
<p>Figure 4 :
4
Figure 4: Example of reasoning generated from our multilingual guardrail.</p>
<p>Figure 5 :
5
Figure 5: Consistency score between English and non-English on the XSafety dataset, evaluated across 8 target languages.Here AR, ES, RU, and ZH are in-domain languages, and FR, HI, JA are out-of-domain languages.Higher the score the better.</p>
<p>Figure 6 :
6
Figure 6: The prompt we used for synthetic data generation.</p>
<p>Figure 7 :
7
Figure 7: Instruction prompt used during both training and inference for our guardrail.</p>
<p>Figure 13 :
13
Figure 13: F1 score breakdown on the Multijail dataset, evaluated across 8 target languages.</p>
<p>Figure 14 :
14
Figure 14: F1 score breakdown on additional unseen languages from different datasets.</p>
<p>Figure 15 :
15
Figure 15: Example of reasoning generated from our multilingual guardrail.</p>
<p>Figure 16 :
16
Figure 16: We present qualitative examples across languages illustrating our failure taxonomy.Inputs are from RTP_LX; analysis are generated by MrGuard.English translations via Google Translate are provided.Note that no examples were judged to fall into Category D.</p>
<p>Figure 17 :
17
Figure 17: Instruction for LLM/human annotation to evaluate fidelity of reasoning.</p>
<p>Table 1 :
1
Configurations of recent state-of-the-art guardrails.Base Model refers to the underlying language model used by each guardrail.Data indicates the dataset used for training the guardrail, where EN denotes English-only data and MUL refers to multilingual (non-English) data.R specifies whether the guardrail is trained with reasoning capability.
1-8B -Instruct30k ENNoLlaMa-Guard-3 (Inan et al., 2023)LlaMa-3.1-8BUnknownNoWildGuard (Han et al., 2024)Mistral-7B86.8K ENNoMrGuard (Ours)LlaMa-3.1-8B -Instruct30k EN 6k MULYes</p>
<p>Table 9 )
9
that are not included in the training set but are in the test datasets.Details of the evaluation benchmark are provided in Appendix C. Guard 61.02 45.61 58.58 44.50 66.62 61.68 56.89 67.29 73.60 30.20 GuardR 78.47 61.35 90.14 90.30 83.12 80.80 80.01 82.56 91.89 78.29 LlaMa
Baselines: We compare our guardrail against sev-eral recent content moderation guardrails, both</p>
<p>Table 2 :
2
Performance of different guardrails to identify multilingual safety across five benchmark datasets.We report F1 scores as the evaluation metric and bold the best-performing results for each dataset where ID refers to in-domain languages and OOD refers to out-of-domain languages.Top are baselines and the bottom part is MrGuard (Ours) 8B and 3B.The model size and training dataset size are listed in Table1.</p>
<p>Table 3 :
3
F1 scores on code-switching prompts evaluated on the MultiJail datasets.The best-performing results across models are highlighted in bold.∆ represents the difference between the F1 score on English prompts and the averaged F1 score over all code-switching variants across both ID and OOD languages.
ModelsAvg-Orig↑Avg-Sandwich↑∆↓DUO-Guard51.900.5851.32GuardR85.0978.786.31LlaMa-Guard-3 77.088.6568.43Aegis-2.036.032.4233.61Wildguard62.7945.5717.22MrGuard (Ours) 96.5090.635.83</p>
<p>Table 4 :
4
F1 scores on sandwich attacks evaluated on the MultiJail dataset.The best-performing results across models are highlighted in bold.Avg-Orig indicates the average F1 score on before attack, and the average F1 score after sandwich attack across both ID and OOD languages.∆ represents the difference between them.</p>
<p>Table 5 :
5
F1 scores for the ablation study.Curr denotes GRPO with curriculum learning.R f +a represents the combination of the format reward and accuracy reward.R u corresponds to the uncertainty reward, and R lang denotes the language reward.
RTP-LX Aya XSafetyOurs89.2798.1893.48wo GRPO84.0595.0588.78wo Curr87.0297.2091.55wo R lang , R u88.4897.5992.36wo R lang , R u88.4897.5992.36wo R lang89.5598.3593.07wo R u88.8997.5892.08</p>
<p>Table 6 :
6
Here we report two metrics on RTP_LX dataset to evaluate the quality of the reasoning, where EF is the explanation fidelity rate and LM indicates the language matching rate.Higher score the better.
ENARZHRUHIEF87.39 80.57 93.33 86.53 88.97LM 97.30 98.76 99.52 98.86 99.91</p>
<p>Table 8 :
8
. The percentage breakdown of reasoning for incorrect safety prediction from MrGuard under our failure taxonomy.Note here HI and JA are the unseen languages at the training.
%ENZHARJAHIA 5.046.677.888.996.72B 7.91 26.67 23.65 48.31 44.78C 2.886.674.933.932.99D0.00.00.00.00.0E 94.96 95.78 81.77 73.60 70.15</p>
<p>Table 15 :
15
Performance of different guardrails to identify multilingual safety across five benchmark datasets grouped by resource availability.</p>
<p>Table 16 :
16
F1 scores on code-switching prompts evaluated on the XSafety datasets.The best-performing results across models are highlighted in bold.∆ represents the difference between the F1 score on English prompts and the averaged F1 score over all codeswitching variants across both ID and OOD languages.</p>
<p>Table 17 :
17
F1 scores on sandwich attacks evaluated on the XSafety datasets.The best-performing results across models are highlighted in bold.Avg-Orig indicates the average F1 score on before attack, and the average F1 score after sandwich attack across both ID and OOD languages.∆ represents the difference between them.
ModelsAvg-Orig↑Avg-Sandwich↑∆↓DUO-Guard64.771.2363.53GuardR82.2574.837.42LlaMa-Guard-3 61.506.3755.13Aegis-2.035.321.1234.20Wildguard69.2142.3826.83MrGuard93.4881.1312.36(t2, t1)RTP_LXAyaXsafetyPTPMultiJ(0.6,0.7)90.4697.5692.7390.1696.49(0.6,0.8)90.6997.6793.3290.6897.38(0.6,0.9)90.6197.6492.6690.4896.69(0.7,0.8)90.8197.7593.1890.8296.85(0.7,0.9)90.1697.2092.8190.3396.74(0.8,0.9)90.1397.3091.9189.8396.57</p>
<p>Table 18 :
18
F1 scores on in-domain languages across datasets for models trained with varying difficulty thresholds.
RTP_LXAyaXsafety Wildchat MultijailCurr89.2798.1893.4891.5496.50Fixed89.1398.2293.0291.4696.30</p>
<p>Table 19 :
19
Comparison between fixed and curriculumbased language rewards.Here we report average F1 score across various languages on five datasets.Curriculum refers to the reward as described in Section 3.3 while Fixed denotes a constant reward applied to all non-English reasoning.tion of including the language component in the loss function in Table20.As we show in the Table5, our proposed model has a comparable performance with a model trained without R lang but the generated reasoning is not in the same language as the input.
ENARZHRUHIOurs97.30 98.76 99.52 98.86 99.91W/o Rlang 97.400.00.00.06.64</p>
<p>Table 20 :
20
Language match rate of the model trained with/without R lang in the loss function.</p>
<p>We utilizeGPT-4o-mini (gpt-4o-mini-2024-07-18)   through all data generation stage, referred to as GPT for simplcity, in the next subsection.
More details on the training hyperparameters and configurations are in Appendix B.
  5  The full name of the language detector: papluca/xlm-roberta-base-language-
detection6  We only select wildchat subset.
We also include the breakdown of performance by the language family, language scripts and resource availability in the Appendix G.1.
The configuration and details of the generated attack are in Appendix D.
AcknowledgmentWe thank the anonymous reviewers for their constructive feedback and insightful suggestions.We would also like to thank Dr. Oleg Sokolsky and Dr. Almiqdad Saeed for their help with the synthetic data evaluation.Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-20-1-0080.The views expressed are those of the authors and do not reflect the official policy or position of the Army Research Office or the U.S. Government.D Multilingual VariantsFollowing(Yoo et al., 2024), we generate codeswitching prompts to evaluate the guardrail's robustness to multilingual variations.Rather than mixing 10 different languages, we construct prompts by combining English with a single additional language, reflecting a more realistic codeswitching scenario in practical applications.We generate code-switching prompts for MultiJail and XSafety dataset.Additionally, following (Upadhayay and Behzadan, 2024), we observe that LLMs are more likely to produce harmful responses when distractors from low-resource languages are present.Specifically, we embed jailbreaking prompts between benign questions written in a low-resource language.In our experiments, jailbreaking prompts are drawn from the MultiJail and XSafety datasets, while benign questions are selected from the Vietnamese subset of the XQuAD dataset(Artetxe et al., 2019).Figure9illustrates examples of the generated code-switching (CSRT) and sandwich attacks used to evaluate multilingual guardrail robustness.E GRPO algorithmThe objective function of GRPO is defined as follow.Let π θ denote the language model parameterized by θ, and π old be the model from a previous iteration.Given a prompt p, we sample a group of generations {o 1 , . . ., o G } ∼ π old (• | p), each associated with a reward {r 1 , . . ., r G }. Let D KL denote the KL-divergence between two distributions.GRPO estimates the advantage of a generation o iLanguageLanguageThe GRPO objective is then defined as:F Additional Results on QWEN Table12: Here we report F1 scores of QWEN-2.5-3Bacross different datasets.We take the average across both in-domain and out-of-domain languages.G Additional ResultsG.1 Granular Breakdown of PerformanceHere is the granular breakdown of performance by the language script/family/resource availability across different datasets.By language script (Table13Indo-European: French, Spanish, Italian, English, Russian, Serbian, Hindi Sino-Tibetan: Chinese Japonic: Japanese Koreanic: Korean Niger-Congo: SwahiliBy resource availability (Table15)We group languages according to their webcoverage percentages as reported by Common Crawl.High-Resource (&gt;= 1%): English, Russian, Chinese, Spanish, French, Italian, Japanese Mid-Resource (0.5% -1%): Korean, Arabic Low-Resource (&lt;= 0.5%): Hindi, Serbian, Swahili.We here show detailed break-down results on additional datasets.G.2 Additional Unseen LanguagesHere we report results in Figure14on unseen mid/low-resource languages for different datasets.RTP_LX: Hebrew (HE); Aya: Filipino (FIL);  XSafety: Bengali (BN); PTP_Wildchat: Korean (KO); MultiJail: Bengali (BN).We observe that our multilingual guardrail consistently outperforms the baselines.
The multilingual alignment prism: Aligning global and local preferences to reduce harm. Arash Aakanksha, Beyza Ahmadian, Seraphina Ermis, Julia Goldfarb-Tarrant, Marzieh Kreutzer, Sara Fadaee, Hooker, arXiv:2406.18682arXiv:2407.21783et al. Aaron Grattafiori2024. 2024PreprintThe llama 3 herd of models</p>
<p>Jailbreaking leading safetyaligned llms with simple adaptive attacks. Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, arXiv:2404.021512024arXiv preprint</p>
<p>On the cross-lingual transferability of monolingual representations. Mikel Artetxe, Sebastian Ruder, Dani Yogatama, CoRR, abs/1910.118562019</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, 10.1145/1553374.1553380Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USAAssociation for Computing Machinery2009a</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, 10.1145/1553374.1553380Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USAAssociation for Computing Machinery2009b</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, 10.1609/aaai.v38i16.29720Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Advances in neural information processing systems. 202033Amanda Askell, and 1 others</p>
<p>Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, Eric Wong, arXiv:2310.08419Jailbreaking black box large language models in twenty queries. 2023arXiv preprint</p>
<p>Fireact: Toward language agent fine-tuning. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, arXiv:2310.059152023Preprint</p>
<p>Curriculum direct preference optimization for diffusion and consistency models. Florinel-Alin, Vlad Croitoru, Hondru, Tudor Radu, Nicu Ionescu, Mubarak Sebe, Shah, arXiv:2405.136372025Preprint</p>
<p>Can Gören, and 1 others. Adrian De Wynter, Ishaan Watts, Nektar Ege Altıntoprak, Tua Wongsangaroonsri, Minghui Zhang, Noura Farra, Lena Baur, Samantha Claudet, Pavel Gajdusek, arXiv:2404.14397.etal.DeepSeek-AI.2025arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2024PreprintRtp-lx: Can llms evaluate toxicity in multilingual scenarios?</p>
<p>Duoguard: A two-player rl-driven framework for multilingual llm guardrails. Yihe Deng, Yu Yang, Junkai Zhang, Wei Wang, Bo Li, arXiv:2502.051632025arXiv preprint</p>
<p>Multilingual jailbreak challenges in large language models. Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2305.14314Qlora: Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Reverse curriculum generation for reinforcement learning. Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel, Proceedings of the 1st Annual Conference on Robot Learning. the 1st Annual Conference on Robot LearningPMLR201778</p>
<p>Aegis: Online adaptive ai content safety moderation with ensemble of llm experts. Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien, arXiv:2404.059932024arXiv preprint</p>
<p>Shaona Ghosh, Prasoon Varshney, Narsimhan Makesh, Aishwarya Sreedhar, Traian Padmakumar, Jibin Rebedea, Christopher Rajan Varghese, Parisien, arXiv:2501.09004Aegis2. 0: A diverse ai safety dataset and risks taxonomy for alignment of llm guardrails. 2025arXiv preprint</p>
<p>Automated curriculum learning for neural networks. Alex Graves, Marc G Bellemare, Jacob Menick, Rémi Munos, Koray Kavukcuoglu, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLR201770</p>
<p>On the power of curriculum learning in training deep networks. Guy Hacohen, Daphna Weinshall, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningPMLR201997</p>
<p>Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, Nouha Dziri, arXiv:2406.184952024Preprint</p>
<p>Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, arXiv:2312.06674Davide Testuggine, and 1 others. 2023. Llama guard: Llm-based inputoutput safeguard for human-ai conversations. arXiv preprint</p>
<p>Polyglotoxicityprompts: Multilingual evaluation of neural toxic degeneration in large language models. Devansh Jain, Priyanshu Kumar, Samuel Gehman, Xuhui Zhou, Thomas Hartvigsen, Maarten Sap, arXiv:2405.093732024Preprint</p>
<p>Guillaume Lample, Lucile Saulnier, and 1 others. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Lengyel, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Mintong Kang, Bo Li, arXiv:2407.05557r2-guard: Robust reasoning enabled llm guardrail via knowledgeenhanced logical reasoning. 2024aarXiv preprint</p>
<p>Mintong Kang, Bo Li, arXiv:2407.055572024b. r 2 -guard: Robust reasoning enabled llm guardrail via knowledgeenhanced logical reasoning. arXiv preprint</p>
<p>Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux, arXiv:2410.016792024Preprint</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao, arXiv:2402.05044Salad-bench: A hierarchical and comprehensive safety benchmark for large language models. 2024arXiv preprint</p>
<p>Yue Liu, Hongcheng Gao, Shengfang Zhai, Xia Jun, Tianyi Wu, Zhiwei Xue, Yulin Chen, Kenji Kawaguchi, Jiaheng Zhang, Bryan Hooi, arXiv:2501.18492Guardreasoner: Towards reasoning-based llm safeguards. 2025arXiv preprint</p>
<p>Teacher-student curriculum learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, 10.1109/TNNLS.2019.2934906IEEE Transactions on Neural Networks and Learning Systems. 3192020</p>
<p>Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, arXiv:2501.19393Preprint</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, Journal of Machine Learning Research. 211812020</p>
<p>Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Advances in neural information processing systems. 35</p>
<dl>
<dt>Libo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen, Yinghui Li, Lizi Liao, Min Li, Wanxiang Che, Philip S Yu, arXiv:2404.04925Multilingual large language model: A survey of resources, taxonomy and frontiers. 2024arXiv preprint</dt>
<dd>
<p>Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, arXiv:2412.15115Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint</p>
</dd>
</dl>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>NeMo guardrails: A toolkit for controllable and safe LLM applications with programmable rails. Traian Rebedea, Razvan Dinu, Narsimhan Makesh, Christopher Sreedhar, Jonathan Parisien, Cohen, 10.18653/v1/2023.emnlp-demo.40Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2023 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsSingaporeAssociation for Computational Linguistics2023</p>
<p>Self-paced prioritized curriculum learning with coverage penalty in deep reinforcement learning. Zhipeng Ren, Daoyi Dong, Huaxiong Li, Chunlin Chen, 10.1109/TNNLS.2018.2790981IEEE Transactions on Neural Networks and Learning Systems. 2962018</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, arXiv:2402.033002024aPreprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y Li, Wu, arXiv:2402.03300others. 2024b. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 1arXiv preprint</p>
<p>Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference optimization. Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, Jiajun Chen, arXiv:2401.068382024Preprint</p>
<p>Curriculum learning: A survey. Petru Soviany, Tudor Radu, Paolo Ionescu, Nicu Rota, Sebe, arXiv:2101.103822022Preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Sandwich attack: Multi-language mixture adaptive attack on llms. Bibek Upadhayay, Vahid Behzadan, arXiv:2404.072422024arXiv preprint</p>
<p>Kashif Rasul, and Quentin Gallouédec. Younes Leandro Von Werra, Lewis Belkada, Edward Tunstall, Tristan Beeching, Nathan Thrush, Shengyi Lambert, Huang, Trl: Transformer reinforcement learning. 2020</p>
<p>Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-Tse Huang, Wenxiang Jiao, Michael R Lyu, arXiv:2310.00905All languages matter: On the multilingual safety of large language models. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Xu, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020Teven Le Scao, Sylvain Gugger, and 3 others</p>
<p>Watch every step! llm agent learning via iterative step-level process refinement. Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li, arXiv:2406.111762024Preprint</p>
<p>Yahan Yang, Soham Dan, Dan Roth, Insup Lee, arXiv:2410.22153Benchmarking llm guardrails in handling multilingual toxicity. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023Preprint</p>
<p>Meng Ye, Karan Sikka, Katherine Atwell, Sabit Hassan, Ajay Divakaran, Malihe Alikhani, arXiv:2302.09618Multilingual content moderation: A case study on reddit. 2023arXiv preprint</p>
<p>Csrt: Evaluation and analysis of llms using codeswitching red-teaming dataset. Haneul Yoo, Yongjin Yang, Hwaran Lee, arXiv:2406.154812024arXiv preprint</p>
<p>Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, Bo Li, arXiv:2403.13031Rigorllm: Resilient guardrails for large language models against undesired content. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
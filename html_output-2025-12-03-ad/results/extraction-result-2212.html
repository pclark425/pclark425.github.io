<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2212 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2212</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2212</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-58.html">extraction-schema-58</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <p><strong>Paper ID:</strong> paper-279075076</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.01372v2.pdf" target="_blank">AI Scientists Fail Without Strong Implementation Capability</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2212.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2212.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperBench (AI replication benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that tasks LLM agents with fully reproducing machine-learning papers end-to-end (codebase development, execution, and quantitative matching of reported results) to evaluate implementation and verification capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating ai's ability to replicate ai research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / Computer science</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Agents must reproduce entire ML papers by developing codebases, executing experiments, and matching reported numerical results. Evaluation is decomposed into rubric leaf nodes including 'Code-Development' (code generation), 'Execution' (successfully running the code), and 'Result Match' (quantitatively matching reported metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable — benchmark compares generated computational experiment outputs to the original paper's reported computational results; provides numerical success rates on execution and result match.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Reported low success: e.g., Claude 3.5 Sonnet scored 1.8% on 'Execution' and 0.7% on 'Result Match' (from this paper's Table 1 / discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>PaperBench adopts rubric-based standards (code correctness, runnable execution, and quantitative result agreement) as the computational-domain criteria for sufficient validation.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Implicit: for ML research, reproducing computational experiments and matching reported results counts as sufficient validation; authors note this still requires correct execution and result parity.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Yes — agents frequently fail at running code or matching reported results; very low execution and result-match scores indicate frequent mismatches between generated experiments and original papers.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Benchmark reports discrete success/score metrics (percent success on rubric nodes); no discussion of uncertainty bars or confidence intervals for agent outputs in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not directly addressed by PaperBench itself here; the paper argues for automated detection and labeling of AI-generated content (separate systems) to avoid fabricated/unverified outputs entering the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational reproduction is time-consuming; paper cites that human baseline for a PaperBench task uses ~48 hours (PaperBench statistic) and AI agents face large asynchronous code+execution cycles. No precise per-task cost breakdown beyond reported low success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>PaperBench evaluates computational reproducibility only; it does not cover physical/experimental domains; rubric-level scores can miss deeper methodological or theoretical issues; peer-review-style impact prediction is unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>PaperBench-style reproduction is positioned as important for credibility, but the paper notes that peer review and bibliometrics have limitations and that reproduction success correlates with perceived rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared implicitly to human reproduction/peer validation; agents perform far worse than humans (very low execution/result-match percentages).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2212.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciReplicate-Bench (algorithmic reproduction benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark requiring LLM agents to generate Python code that reproduces algorithms from NLP research papers, focusing on execution correctness of the reproduced implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Computer Science</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Task: read an NLP research paper and produce code that reproduces the algorithmic behavior; evaluation by executing generated code and measuring functional correctness against objective test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable — validation is computational execution vs. paper-reported computational behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Best agent reached only 39% execution accuracy (i.e., passed functional test cases for 39% of tasks) as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Correct execution on test cases derived from the original algorithm/paper is used as the standard for sufficient validation in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>For algorithmic reproduction, execution on benchmark test cases suffices as validation; authors note agents still fail to achieve high execution accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Yes — frequent failures to produce runnable, correct code despite high reasoning-graph accuracy, indicating gaps between conceptual understanding and executable implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reported as execution accuracy percentages; no error bars or probabilistic uncertainty measures discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not a core feature of the benchmark; paper discusses reproduction failures as evidence that many AI-generated papers lack executable evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Execution/evaluation is computational and automated but agents often fail, implying repeated generation+debug cycles; no precise cost/time numbers beyond success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Benchmark focuses on code execution correctness, not broader experimental design, statistical analysis, or domain-specific experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Low execution accuracy undermines credibility of AI-generated algorithmic claims; the paper uses these results to argue AI Scientists lack reliable verification capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared implicitly to human reproduction: humans are expected to achieve much higher execution/reproduction success; agents fall short (39% best execution).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2212.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-Bench (computational reproducibility agent benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that evaluates agents' ability to reproduce computational experiments from papers and then reason about outputs (answer questions based on reproduced outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>CORE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational reproducibility across Computer Science / Social Science / Medicine</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Agents reproduce computational experiments from papers and then perform downstream reasoning (e.g., answer questions) using the reproduced outputs; evaluated on success rates across reproduction and reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Validation is computational reproduction vs. paper-reported computational results; paper reports imperfect success rates indicating partial reproduction difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Example from paper: CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium, indicating moderate reproduction+reasoning success.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Reproduction correctness (running code, matching outputs) plus correct reasoning about outputs constitute the benchmark's validation standard.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>For computational experiments, reproduction and consistent downstream reasoning are treated as sufficient validation within the benchmark's scope.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Yes — incomplete reproduction and imperfect downstream reasoning are common failure modes; paper highlights the difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reported as success percentages per difficulty tier; no probabilistic uncertainty estimates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>CORE-Bench aims to detect unreproducible claims by testing reproduction; indirect detection of fabricated/unsubstantiated computational claims is possible via reproduction failure.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational but nontrivial; reproduction + reasoning pipelines introduce asynchronous costs and require compute resources; no detailed cost numbers beyond success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Focused on computational reproducibility; does not cover physical lab experiments or broader methodological critiques; success rates do not capture all dimensions of scientific rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Reproducibility is presented as crucial to credibility; CORE-Bench success rates are used to argue AI limitations in producing credible scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared against expected human-level reproducibility/reasoning; agents' moderate success (~55.56%) remains lower than ideal human reproduction performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2212.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLE-Bench (Machine Learning Engineering benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark evaluating agents on ML development workflow tasks (training, debugging, submission) to measure practical engineering and verification capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mle-bench: Evaluating machine learning agents on machine learning engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>MLE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning engineering</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Tasks simulate realistic ML engineering steps (e.g., training, debugging, packaging, submission). Success is measured by producing valid submissions and operational models; debugging and iterative verification steps are emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable — validation is within computational ML workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Reported: OpenAI o1-preview had 16.90% accuracy on an ML training task (Table 1). Also noted 20% of o1 preview runs on MLE-Bench failed at debugging steps (discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Successful completion of ML development workflow tasks and producing valid submissions are treated as standards for engineering-level validation.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Within ML engineering, computational execution and submission testing are sufficient to validate results.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Agents frequently fail to debug or produce valid submissions; inability to iterate effectively is a core failure mode highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reported as percent success/failure; no deeper uncertainty quantification discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Indirectly, failures to produce valid submissions reveal unverified or fabricated claims; paper advocates for better oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational resources required for training and debugging are nontrivial; specific time/cost numbers not provided beyond failure statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Benchmarks capture ML engineering tasks but not broader scientific experimental validation; they highlight debugging/verification gaps in agents.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Poor engineering performance undermines credibility of AI-produced ML research artifacts; authors use these metrics to argue for implementation gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared implicitly to competent human ML engineers; agents perform substantially worse (low percent success and debugging failures).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2212.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML-Dev-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ML-Dev-Bench (ML development workflow benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark for completing diverse ML development workflow tasks, assessing success rates across end-to-end development and verification steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ml-dev-bench: Comparative analysis of ai agents on ml development workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ML-Dev-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning engineering</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Evaluates agents on tasks across ML development including coding, training, debugging, and model performance optimization; emphasizes verification steps like model performance measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Comparisons are within computational workflows (agent outputs vs. expected ML results).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Paper reports agents scored 0% on 'Model Performance' tasks across tested agents in ML-Dev-Bench, indicating inability to reach performance targets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Model performance metrics (e.g., validation/test accuracy) and successful deployment/submission are the standards.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Computational experiments are sufficient for validation in ML development, but agents often fail to meet performance benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Agents fail to iteratively optimize model performance and do not debug to acceptable performance levels.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Success/failure reported as percentages; no probabilistic uncertainty estimates for agent predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not explicit; poor model performance indicates lack of verified claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational training and evaluation cycles are resource-intensive; paper highlights asynchronous and costly nature of code execution and experiments (Appendix A provides general sampling time estimates).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Focuses on ML workflows; does not extend to wet-lab or physical experiments; does not capture full spectrum of scientific validation.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Zero model-performance success undermines credibility of agents for producing reliable ML research.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared implicitly to human ML practitioners; agents underperform significantly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2212.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiveCodeBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiveCodeBench (holistic code evaluation benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A coding benchmark collecting problems from competitive programming contests to evaluate code generation, execution, self-repair, and output prediction of code LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Livecodebench: Holistic and contamination free evaluation of large language models for code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LiveCodeBench</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science — code generation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Evaluates code LLMs on generation, executing generated code, self-repair, and correctness of outputs on a variety of contest-derived problems; pass@1 is reported for generation subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Purely computational: generated code is executed and correctness is measured against contest testcases.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Example: o4-mini achieved 52.1% pass@1 on the code generation subtask (paper reports this performance).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Correctness on deterministic contest testcases is the gold criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Execution against standard testcases suffices within the coding domain.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Lower performance on complex code tasks compared to simpler benchmarks suggests failures when moving to realistic research-grade code.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reported via pass@k percentages; no confidence intervals discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable; benchmark focuses on executable correctness rather than detecting fabricated scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Execution and test runs are automated but can be resource-heavy for large-scale evaluation; no numeric cost/time beyond success metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Contest-style tasks differ from multi-file, research-grade codebases; results may not generalize to full scientific experiment implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>High coding pass rates improve credibility for code-generation tasks but do not guarantee end-to-end experimental validity in scientific workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Benchmarks contest-ground truth; agents performing ~50% pass@1 indicate gap relative to human top performers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2212.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-Lab (Autonomous Lab)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A-Lab (autonomous laboratory for materials synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous laboratory system that synthesized 41 novel inorganic materials in 17 days, representing an example of experimental automation achieving validated wet-lab outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>A-Lab (autonomous laboratory)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / experimental chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Physical synthesis of novel inorganic materials in a lab setting with experimental protocols and presumably characterization of materials to confirm synthesis; cited as real experimental validation of automated discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable — this is direct experimental validation (physical synthesis and presumably characterization), presented as a gold-standard example of automated tools achieving experimental success.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Reported outcome: successful synthesis of 41 novel inorganic materials in 17 days (paper citation presented as evidence in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Physical synthesis and characterization (materials validation) are domain standards; the paper cites A-Lab as evidence that automated tools can achieve experimental validation when human-guided.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Not discussed for this system; emphasis is on real experiments rather than simulation alone.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No uncertainty metrics provided in this paper's mention; primary claim is successful synthesis counts as validation.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Reported: 17 days to synthesize 41 materials; this illustrates experimental resource/time scales versus computational work.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Paper notes many automated tools (including A-Lab) still rely on human involvement and are not fully autonomous AI Scientists; A-Lab is an example of domain-specific automation rather than general-purpose AI Scientist.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Experimental wet-lab success confers high credibility in the materials domain; the paper cites A-Lab as an instance where validated experimental outcomes led to impact.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Presented as comparable to established experimental practice — successful synthesis and characterization are gold-standard validation in materials science.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2212.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (protein structure prediction system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational system that predicts 3D protein structures with high accuracy and is cited as an example of an automated scientific tool contributing a major breakthrough, validated against experimental structural data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with alphafold.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Structural biology / bioinformatics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>AlphaFold produces predicted protein structures computationally; validation is performed by comparison against experimentally determined structures (X-ray, cryo-EM, NMR) and community benchmarks, which established its high accuracy (paper cites AlphaFold as a successful automated tool).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity computational modeling of protein folding using learned potentials; the paper references AlphaFold's high accuracy but does not provide numerical fidelity metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>AlphaFold's predictions are compared to experimental structures; the paper uses AlphaFold as an exemplar of computational predictions validated by experimental benchmarks (no numerical comparison provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not specified numerically in this paper excerpt; AlphaFold is characterized qualitatively as accurately determining 3D structures in hours.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Agreement with experimentally determined protein structures and community benchmarks (e.g., CASP) is treated as the validation standard in structural biology.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>In protein structure prediction, high-accuracy computational predictions can be sufficient for many downstream uses when validated against experimental benchmarks; the paper uses AlphaFold as an example of computational approaches achieving high trust.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>AlphaFold provides confidence metrics (in original literature), but this paper does not enumerate them.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>AlphaFold greatly reduced time to obtain structural models (hours vs. years historically) — qualitative claim in the paper; no numeric resource/cost accounting provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Paper notes that many automated tools (including AlphaFold) still depend on human oversight in research pipelines; AlphaFold is domain-specific rather than a full AI Scientist.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>AlphaFold is cited as a high-credibility automated tool because its computational predictions were validated against experimental structures and widely adopted by the community.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>AlphaFold's computational predictions were compared to experimental structural gold-standards (implied in citation), leading to broad acceptance; specific numerical comparisons not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2212.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepReviewer-14B (LLM-as-a-Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepReviewer-14B (LLM-based peer review simulation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art review model used in a simulated peer-review evaluation ('LLM-as-a-Judge') to systematically assess 28 AI-generated research papers for implementation-level reliability and scientific quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepreview: Improving llm-based paper review with human-like deep thinking process.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>DeepReviewer-14B (LLM-based review)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Meta-science / scientific peer review</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Simulated peer-review: DeepReviewer-14B evaluated 28 public AI-generated papers across dimensions (soundness, presentation, contribution) and produced scores and defect categorizations; used to surface implementation/experimental weaknesses in AI-generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable — this is a review/evaluation process rather than experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not applicable; evaluation produced average scores (see Table 2) and found 100% of papers exhibited 'Experimental Weakness' (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Peer-review style rubrics (soundness, presentation, contribution, decision) are used as proxies for community validation and quality assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Authors note that peer-review simulation can filter low-quality papers but is imperfect at predicting long-term impact; hence it is a partial validation method.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Simulated peer review has limitations: selection bias in evaluated papers and known issues in peer review accuracy mean it cannot fully substitute human judgment or downstream experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Scores and percentiles are reported (Table 2); but authors acknowledge review scores are imperfect predictors of future impact.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Authors propose automated detection and labeling systems plus specialized evaluation tools (e.g., DeepReview) to help identify low-quality or fabricated AI-generated content; DeepReviewer exemplified as an evaluation component.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Simulated peer review is faster than physical experiments but requires development of reliable review models and may still be resource-intensive; no explicit timing metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Peer-review simulation may miss long-term impact and can be gamed; authors recommend hybrid automated + human-in-the-loop review for oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>The paper uses DeepReviewer results to argue that AI-generated papers have systematic experimental weaknesses; peer-review-style validation affects acceptance and credibility but has known reliability limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared implicitly to human peer review — DeepReviewer offers a consistent automated judge but the paper notes human review and bibliometrics are imperfect gold standards for predicting impact.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2212.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (method)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge (simulated peer review methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology using large language models as reviewers to simulate peer review and assess the quality and implementation reliability of AI-generated research outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-as-a-Judge (simulated peer review approach)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Meta-science / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Uses a review LLM (DeepReviewer-14B) to systematically evaluate AI-generated papers under unified standards (soundness, presentation, contribution), revealing pervasive experimental weaknesses across AI-generated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable — the approach evaluates reported work rather than reproducing experiments physically or computationally.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not applicable; results showed high rates of detected defects (Table 3: 'Experimental Weakness' in 100% of evaluated papers).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Peer-review criteria are used as proxies for validation in the scholarly domain; authors caution about limitations of peer review for long-term impact assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Authors argue LLM-based review can filter low-quality papers but cannot fully substitute the depth of human experimental verification.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Limitations include selection bias in public AI-generated outputs and known unreliability in peer review to predict long-term impact; LLM reviewers may not detect subtle but important methodological or experimental flaws.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Paper reports scores and percentiles but emphasizes these are imperfect predictors; no calibrated uncertainty intervals given for review judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>LLM-as-a-Judge can flag experimental weaknesses and reproducibility issues, forming part of a detection workflow; authors also call for automated detection tools and transparent labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Faster than physical reproduction but requires careful development and calibration of review models; paper does not provide specific timing.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Peer-review proxies cannot fully replace experiment reproduction or long-term impact metrics; risk of false negatives/positives in detecting groundbreaking work.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>LLM-based review affects immediate acceptance/credibility but authors caution about over-reliance; recommend hybrid automated + human review systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared to human peer review and bibliometrics; authors cite literature showing review scores are not reliable predictors of future impact.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2212.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL surrogate simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation/surrogate environment for RL training (LLM-simulated feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of using LLMs or simulated environments to approximate task execution and provide faster, approximate feedback to RL agents to improve sampling efficiency for long-horizon planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-based environment simulation / surrogate feedback for RL</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Reinforcement learning / agent training methodology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>low-fidelity simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Authors propose leveraging LLMs to simulate aspects of the environment or task execution to give quicker (approximate) feedback during RL training, thereby accelerating long-horizon planning and meta-reasoning training loops.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Low-fidelity/approximate: simulations provide faster but potentially approximate feedback; fidelity and modeled physics/chemistry are not specified and approximations may diverge from real-world dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Paper discusses as a proposal to accelerate RL sampling; does not provide direct empirical comparisons between surrogate-simulated feedback and real-world training outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not provided — proposed as promising direction to improve sampling efficiency, no numeric validation reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Surrogate simulations are considered acceptable for speeding up RL training when approximate feedback is informative; domain-specific sufficiency not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Proposed to be sufficient for RL training feedback if surrogate approximations provide informative signals; authors emphasize potential but caution about approximation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not empirically reported in this paper; authors note potential risk that surrogate feedback may be misleading for real asynchronous, resource-intensive implementation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed; surrogate simulations would require calibration of their uncertainty but paper does not detail methods.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Motivation: surrogate simulations can drastically reduce wall-clock sampling time compared to real asynchronous code+experiment cycles, but exact savings are speculative in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Approximate feedback may not capture crucial real-world constraints; transfer from surrogate to real environment remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Proposed as a training optimization rather than a final validation mechanism; final experimental validation would still be required for scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>No empirical comparison given; gold standard remains real-world asynchronous execution for scientific tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2212.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Co-scientist / Human-in-loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Co-scientist (human-AI collaborative research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pragmatic hybrid approach where AI systems focus on idea generation and humans perform final experimental implementation and validation, proposed as an alternative to fully autonomous AI Scientists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Human-AI co-scientist / human-in-the-loop hybrid workflow</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific research (multi-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Humans execute experimental procedures, rigorous verification, and iterative refinement while AI assists with ideation, decomposition, and partial implementation; validation relies on human-conducted experiments and domain expertise supported by AI-produced artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Hybrid approach uses computational idea-generation plus human-performed experimental validation; the paper advocates this approach to mitigate AI systems' verification weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not quantified; authors argue that even if AI cannot perform full implementation, enabling humans to be ten times more efficient would qualify as successful collaborative scientist.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Domain-specific experimental validation and human peer review remain the standard; co-scientist approach preserves these human-led validation norms.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper suggests simulation/computational outputs alone are insufficient when experiments are required; advocates human execution of experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Paper uses many computational benchmark failures to motivate the co-scientist approach (agents failing execution/result matching), implying computational-only validation often fails.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not specified for this workflow; relies on human statistical and experimental practices for uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Hybrid workflows and human oversight are proposed to reduce the risk of fabricated or unverified AI-generated findings entering the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Human execution remains time/resource intensive; however, co-scientist workflows aim to accelerate human productivity via AI assistance (no numerical time savings provided beyond speculative 'tenfold').</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Relies on continued human expertise and can reduce scalability; does not solve the autonomous implementation challenge but is a practical interim strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Maintains community trust because human-executed experiments and peer review remain central; authors present this as a credible near-term pathway.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Explicitly presented as preserving gold-standard human experimental validation while leveraging AI for ideation/assistance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2212.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2212.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental Weakness finding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experimental Weakness (defect category found by DeepReviewer evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pervasive defect category identified across all 28 evaluated AI-generated papers, indicating failures in experimental design, execution, and result analysis in AI-produced research outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-generated papers evaluation (defect taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Meta-science / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>fabricated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>DeepReviewer evaluation flagged experimental weaknesses across all sampled AI-generated papers; defects include lack of reproducible experiments, flawed experimental protocols, missing or incorrect result analysis, and inability to execute verified experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable — the finding is that experimental validation is missing or weak in the evaluated AI-generated literature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>100% of the 28 evaluated AI-generated papers were found to have 'Experimental Weakness' (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Paper argues domain standards require rigorous experimental execution, reproducibility, and clear result analysis — many AI-generated papers fail these standards.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper implies simulation/computational claims without reproducible execution are insufficient; experimental weaknesses imply missing real validation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Many AI outputs produce code/experiments that cannot be executed or do not match claimed results; the paper documents these replication/execution failures across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not directly addressed per paper-level defects; DeepReviewer provides categorical detections and defect prevalence percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>DeepReviewer/LLM-as-a-Judge flagged these weaknesses; authors call for detection systems and transparent labeling of AI-generated work to prevent fabricated/unverified results from proliferating.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified per paper; the prevalence of experimental weaknesses implies additional human/experimental effort would be needed to verify claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Evaluation limited to publicly available outputs (possible selection bias); nevertheless, pervasive experimental weakness is a robust signal of implementation gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>High prevalence of experimental weaknesses undermines the credibility and acceptance of AI-generated papers in the scientific community.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared implicitly to human-authored work where proper experiments and reproducibility practices are expected; AI papers fail to meet those standards in the evaluated sample.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating ai's ability to replicate ai research. <em>(Rating: 2)</em></li>
                <li>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. <em>(Rating: 2)</em></li>
                <li>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. <em>(Rating: 2)</em></li>
                <li>Mle-bench: Evaluating machine learning agents on machine learning engineering. <em>(Rating: 2)</em></li>
                <li>Ml-dev-bench: Comparative analysis of ai agents on ml development workflows. <em>(Rating: 2)</em></li>
                <li>Livecodebench: Holistic and contamination free evaluation of large language models for code. <em>(Rating: 2)</em></li>
                <li>An autonomous laboratory for the accelerated synthesis of novel materials. <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with alphafold. <em>(Rating: 2)</em></li>
                <li>Deepreview: Improving llm-based paper review with human-like deep thinking process. <em>(Rating: 2)</em></li>
                <li>A survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. <em>(Rating: 1)</em></li>
                <li>Curie: Toward rigorous and automated scientific experimentation with ai agents. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2212",
    "paper_id": "paper-279075076",
    "extraction_schema_id": "extraction-schema-58",
    "extracted_data": [
        {
            "name_short": "PaperBench",
            "name_full": "PaperBench (AI replication benchmark)",
            "brief_description": "A benchmark that tasks LLM agents with fully reproducing machine-learning papers end-to-end (codebase development, execution, and quantitative matching of reported results) to evaluate implementation and verification capability.",
            "citation_title": "Evaluating ai's ability to replicate ai research.",
            "mention_or_use": "mention",
            "system_or_method_name": "PaperBench",
            "scientific_domain": "Machine learning / Computer science",
            "validation_type": "computational validation",
            "validation_description": "Agents must reproduce entire ML papers by developing codebases, executing experiments, and matching reported numerical results. Evaluation is decomposed into rubric leaf nodes including 'Code-Development' (code generation), 'Execution' (successfully running the code), and 'Result Match' (quantitatively matching reported metrics).",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable — benchmark compares generated computational experiment outputs to the original paper's reported computational results; provides numerical success rates on execution and result match.",
            "validation_success_rate": "Reported low success: e.g., Claude 3.5 Sonnet scored 1.8% on 'Execution' and 0.7% on 'Result Match' (from this paper's Table 1 / discussion).",
            "domain_validation_standards": "PaperBench adopts rubric-based standards (code correctness, runnable execution, and quantitative result agreement) as the computational-domain criteria for sufficient validation.",
            "when_simulation_sufficient": "Implicit: for ML research, reproducing computational experiments and matching reported results counts as sufficient validation; authors note this still requires correct execution and result parity.",
            "simulation_failures": "Yes — agents frequently fail at running code or matching reported results; very low execution and result-match scores indicate frequent mismatches between generated experiments and original papers.",
            "uncertainty_quantification": "Benchmark reports discrete success/score metrics (percent success on rubric nodes); no discussion of uncertainty bars or confidence intervals for agent outputs in this paper.",
            "fabrication_detection": "Not directly addressed by PaperBench itself here; the paper argues for automated detection and labeling of AI-generated content (separate systems) to avoid fabricated/unverified outputs entering the literature.",
            "validation_cost_time": "Computational reproduction is time-consuming; paper cites that human baseline for a PaperBench task uses ~48 hours (PaperBench statistic) and AI agents face large asynchronous code+execution cycles. No precise per-task cost breakdown beyond reported low success rates.",
            "hybrid_validation_approach": false,
            "validation_limitations": "PaperBench evaluates computational reproducibility only; it does not cover physical/experimental domains; rubric-level scores can miss deeper methodological or theoretical issues; peer-review-style impact prediction is unreliable.",
            "acceptance_credibility": "PaperBench-style reproduction is positioned as important for credibility, but the paper notes that peer review and bibliometrics have limitations and that reproduction success correlates with perceived rigor.",
            "comparison_to_gold_standard": "Compared implicitly to human reproduction/peer validation; agents perform far worse than humans (very low execution/result-match percentages).",
            "uuid": "e2212.0"
        },
        {
            "name_short": "SciReplicate-Bench",
            "name_full": "SciReplicate-Bench (algorithmic reproduction benchmark)",
            "brief_description": "A benchmark requiring LLM agents to generate Python code that reproduces algorithms from NLP research papers, focusing on execution correctness of the reproduced implementations.",
            "citation_title": "Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers.",
            "mention_or_use": "mention",
            "system_or_method_name": "SciReplicate-Bench",
            "scientific_domain": "Natural Language Processing / Computer Science",
            "validation_type": "computational validation",
            "validation_description": "Task: read an NLP research paper and produce code that reproduces the algorithmic behavior; evaluation by executing generated code and measuring functional correctness against objective test cases.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable — validation is computational execution vs. paper-reported computational behavior.",
            "validation_success_rate": "Best agent reached only 39% execution accuracy (i.e., passed functional test cases for 39% of tasks) as reported in the paper.",
            "domain_validation_standards": "Correct execution on test cases derived from the original algorithm/paper is used as the standard for sufficient validation in this benchmark.",
            "when_simulation_sufficient": "For algorithmic reproduction, execution on benchmark test cases suffices as validation; authors note agents still fail to achieve high execution accuracy.",
            "simulation_failures": "Yes — frequent failures to produce runnable, correct code despite high reasoning-graph accuracy, indicating gaps between conceptual understanding and executable implementations.",
            "uncertainty_quantification": "Reported as execution accuracy percentages; no error bars or probabilistic uncertainty measures discussed.",
            "fabrication_detection": "Not a core feature of the benchmark; paper discusses reproduction failures as evidence that many AI-generated papers lack executable evidence.",
            "validation_cost_time": "Execution/evaluation is computational and automated but agents often fail, implying repeated generation+debug cycles; no precise cost/time numbers beyond success rates.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Benchmark focuses on code execution correctness, not broader experimental design, statistical analysis, or domain-specific experimental validation.",
            "acceptance_credibility": "Low execution accuracy undermines credibility of AI-generated algorithmic claims; the paper uses these results to argue AI Scientists lack reliable verification capability.",
            "comparison_to_gold_standard": "Compared implicitly to human reproduction: humans are expected to achieve much higher execution/reproduction success; agents fall short (39% best execution).",
            "uuid": "e2212.1"
        },
        {
            "name_short": "CORE-Bench",
            "name_full": "CORE-Bench (computational reproducibility agent benchmark)",
            "brief_description": "A benchmark that evaluates agents' ability to reproduce computational experiments from papers and then reason about outputs (answer questions based on reproduced outputs).",
            "citation_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark.",
            "mention_or_use": "mention",
            "system_or_method_name": "CORE-Bench",
            "scientific_domain": "Computational reproducibility across Computer Science / Social Science / Medicine",
            "validation_type": "computational validation",
            "validation_description": "Agents reproduce computational experiments from papers and then perform downstream reasoning (e.g., answer questions) using the reproduced outputs; evaluated on success rates across reproduction and reasoning tasks.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Validation is computational reproduction vs. paper-reported computational results; paper reports imperfect success rates indicating partial reproduction difficulty.",
            "validation_success_rate": "Example from paper: CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium, indicating moderate reproduction+reasoning success.",
            "domain_validation_standards": "Reproduction correctness (running code, matching outputs) plus correct reasoning about outputs constitute the benchmark's validation standard.",
            "when_simulation_sufficient": "For computational experiments, reproduction and consistent downstream reasoning are treated as sufficient validation within the benchmark's scope.",
            "simulation_failures": "Yes — incomplete reproduction and imperfect downstream reasoning are common failure modes; paper highlights the difficulty.",
            "uncertainty_quantification": "Reported as success percentages per difficulty tier; no probabilistic uncertainty estimates reported.",
            "fabrication_detection": "CORE-Bench aims to detect unreproducible claims by testing reproduction; indirect detection of fabricated/unsubstantiated computational claims is possible via reproduction failure.",
            "validation_cost_time": "Computational but nontrivial; reproduction + reasoning pipelines introduce asynchronous costs and require compute resources; no detailed cost numbers beyond success rates.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Focused on computational reproducibility; does not cover physical lab experiments or broader methodological critiques; success rates do not capture all dimensions of scientific rigor.",
            "acceptance_credibility": "Reproducibility is presented as crucial to credibility; CORE-Bench success rates are used to argue AI limitations in producing credible scientific claims.",
            "comparison_to_gold_standard": "Compared against expected human-level reproducibility/reasoning; agents' moderate success (~55.56%) remains lower than ideal human reproduction performance.",
            "uuid": "e2212.2"
        },
        {
            "name_short": "MLE-Bench",
            "name_full": "MLE-Bench (Machine Learning Engineering benchmark)",
            "brief_description": "A benchmark evaluating agents on ML development workflow tasks (training, debugging, submission) to measure practical engineering and verification capabilities.",
            "citation_title": "Mle-bench: Evaluating machine learning agents on machine learning engineering.",
            "mention_or_use": "mention",
            "system_or_method_name": "MLE-Bench",
            "scientific_domain": "Machine learning engineering",
            "validation_type": "computational validation",
            "validation_description": "Tasks simulate realistic ML engineering steps (e.g., training, debugging, packaging, submission). Success is measured by producing valid submissions and operational models; debugging and iterative verification steps are emphasized.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable — validation is within computational ML workflows.",
            "validation_success_rate": "Reported: OpenAI o1-preview had 16.90% accuracy on an ML training task (Table 1). Also noted 20% of o1 preview runs on MLE-Bench failed at debugging steps (discussion).",
            "domain_validation_standards": "Successful completion of ML development workflow tasks and producing valid submissions are treated as standards for engineering-level validation.",
            "when_simulation_sufficient": "Within ML engineering, computational execution and submission testing are sufficient to validate results.",
            "simulation_failures": "Agents frequently fail to debug or produce valid submissions; inability to iterate effectively is a core failure mode highlighted.",
            "uncertainty_quantification": "Reported as percent success/failure; no deeper uncertainty quantification discussed.",
            "fabrication_detection": "Indirectly, failures to produce valid submissions reveal unverified or fabricated claims; paper advocates for better oversight.",
            "validation_cost_time": "Computational resources required for training and debugging are nontrivial; specific time/cost numbers not provided beyond failure statistics.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Benchmarks capture ML engineering tasks but not broader scientific experimental validation; they highlight debugging/verification gaps in agents.",
            "acceptance_credibility": "Poor engineering performance undermines credibility of AI-produced ML research artifacts; authors use these metrics to argue for implementation gaps.",
            "comparison_to_gold_standard": "Compared implicitly to competent human ML engineers; agents perform substantially worse (low percent success and debugging failures).",
            "uuid": "e2212.3"
        },
        {
            "name_short": "ML-Dev-Bench",
            "name_full": "ML-Dev-Bench (ML development workflow benchmark)",
            "brief_description": "A benchmark for completing diverse ML development workflow tasks, assessing success rates across end-to-end development and verification steps.",
            "citation_title": "Ml-dev-bench: Comparative analysis of ai agents on ml development workflows.",
            "mention_or_use": "mention",
            "system_or_method_name": "ML-Dev-Bench",
            "scientific_domain": "Machine learning engineering",
            "validation_type": "computational validation",
            "validation_description": "Evaluates agents on tasks across ML development including coding, training, debugging, and model performance optimization; emphasizes verification steps like model performance measurement.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Comparisons are within computational workflows (agent outputs vs. expected ML results).",
            "validation_success_rate": "Paper reports agents scored 0% on 'Model Performance' tasks across tested agents in ML-Dev-Bench, indicating inability to reach performance targets.",
            "domain_validation_standards": "Model performance metrics (e.g., validation/test accuracy) and successful deployment/submission are the standards.",
            "when_simulation_sufficient": "Computational experiments are sufficient for validation in ML development, but agents often fail to meet performance benchmarks.",
            "simulation_failures": "Agents fail to iteratively optimize model performance and do not debug to acceptable performance levels.",
            "uncertainty_quantification": "Success/failure reported as percentages; no probabilistic uncertainty estimates for agent predictions.",
            "fabrication_detection": "Not explicit; poor model performance indicates lack of verified claims.",
            "validation_cost_time": "Computational training and evaluation cycles are resource-intensive; paper highlights asynchronous and costly nature of code execution and experiments (Appendix A provides general sampling time estimates).",
            "hybrid_validation_approach": false,
            "validation_limitations": "Focuses on ML workflows; does not extend to wet-lab or physical experiments; does not capture full spectrum of scientific validation.",
            "acceptance_credibility": "Zero model-performance success undermines credibility of agents for producing reliable ML research.",
            "comparison_to_gold_standard": "Compared implicitly to human ML practitioners; agents underperform significantly.",
            "uuid": "e2212.4"
        },
        {
            "name_short": "LiveCodeBench",
            "name_full": "LiveCodeBench (holistic code evaluation benchmark)",
            "brief_description": "A coding benchmark collecting problems from competitive programming contests to evaluate code generation, execution, self-repair, and output prediction of code LLMs.",
            "citation_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code.",
            "mention_or_use": "mention",
            "system_or_method_name": "LiveCodeBench",
            "scientific_domain": "Computer science — code generation",
            "validation_type": "computational validation",
            "validation_description": "Evaluates code LLMs on generation, executing generated code, self-repair, and correctness of outputs on a variety of contest-derived problems; pass@1 is reported for generation subtask.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Purely computational: generated code is executed and correctness is measured against contest testcases.",
            "validation_success_rate": "Example: o4-mini achieved 52.1% pass@1 on the code generation subtask (paper reports this performance).",
            "domain_validation_standards": "Correctness on deterministic contest testcases is the gold criterion.",
            "when_simulation_sufficient": "Execution against standard testcases suffices within the coding domain.",
            "simulation_failures": "Lower performance on complex code tasks compared to simpler benchmarks suggests failures when moving to realistic research-grade code.",
            "uncertainty_quantification": "Reported via pass@k percentages; no confidence intervals discussed.",
            "fabrication_detection": "Not applicable; benchmark focuses on executable correctness rather than detecting fabricated scientific claims.",
            "validation_cost_time": "Execution and test runs are automated but can be resource-heavy for large-scale evaluation; no numeric cost/time beyond success metrics.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Contest-style tasks differ from multi-file, research-grade codebases; results may not generalize to full scientific experiment implementations.",
            "acceptance_credibility": "High coding pass rates improve credibility for code-generation tasks but do not guarantee end-to-end experimental validity in scientific workflows.",
            "comparison_to_gold_standard": "Benchmarks contest-ground truth; agents performing ~50% pass@1 indicate gap relative to human top performers.",
            "uuid": "e2212.5"
        },
        {
            "name_short": "A-Lab (Autonomous Lab)",
            "name_full": "A-Lab (autonomous laboratory for materials synthesis)",
            "brief_description": "An autonomous laboratory system that synthesized 41 novel inorganic materials in 17 days, representing an example of experimental automation achieving validated wet-lab outcomes.",
            "citation_title": "An autonomous laboratory for the accelerated synthesis of novel materials.",
            "mention_or_use": "mention",
            "system_or_method_name": "A-Lab (autonomous laboratory)",
            "scientific_domain": "Materials science / experimental chemistry",
            "validation_type": "experimental",
            "validation_description": "Physical synthesis of novel inorganic materials in a lab setting with experimental protocols and presumably characterization of materials to confirm synthesis; cited as real experimental validation of automated discovery.",
            "simulation_fidelity": null,
            "experimental_validation_performed": true,
            "comparison_simulation_vs_experiment": "Not applicable — this is direct experimental validation (physical synthesis and presumably characterization), presented as a gold-standard example of automated tools achieving experimental success.",
            "validation_success_rate": "Reported outcome: successful synthesis of 41 novel inorganic materials in 17 days (paper citation presented as evidence in this paper).",
            "domain_validation_standards": "Physical synthesis and characterization (materials validation) are domain standards; the paper cites A-Lab as evidence that automated tools can achieve experimental validation when human-guided.",
            "when_simulation_sufficient": "Not discussed for this system; emphasis is on real experiments rather than simulation alone.",
            "simulation_failures": "Not discussed.",
            "uncertainty_quantification": "No uncertainty metrics provided in this paper's mention; primary claim is successful synthesis counts as validation.",
            "fabrication_detection": "Not applicable.",
            "validation_cost_time": "Reported: 17 days to synthesize 41 materials; this illustrates experimental resource/time scales versus computational work.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Paper notes many automated tools (including A-Lab) still rely on human involvement and are not fully autonomous AI Scientists; A-Lab is an example of domain-specific automation rather than general-purpose AI Scientist.",
            "acceptance_credibility": "Experimental wet-lab success confers high credibility in the materials domain; the paper cites A-Lab as an instance where validated experimental outcomes led to impact.",
            "comparison_to_gold_standard": "Presented as comparable to established experimental practice — successful synthesis and characterization are gold-standard validation in materials science.",
            "uuid": "e2212.6"
        },
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold (protein structure prediction system)",
            "brief_description": "A computational system that predicts 3D protein structures with high accuracy and is cited as an example of an automated scientific tool contributing a major breakthrough, validated against experimental structural data.",
            "citation_title": "Highly accurate protein structure prediction with alphafold.",
            "mention_or_use": "mention",
            "system_or_method_name": "AlphaFold",
            "scientific_domain": "Structural biology / bioinformatics",
            "validation_type": "computational validation",
            "validation_description": "AlphaFold produces predicted protein structures computationally; validation is performed by comparison against experimentally determined structures (X-ray, cryo-EM, NMR) and community benchmarks, which established its high accuracy (paper cites AlphaFold as a successful automated tool).",
            "simulation_fidelity": "High-fidelity computational modeling of protein folding using learned potentials; the paper references AlphaFold's high accuracy but does not provide numerical fidelity metrics here.",
            "experimental_validation_performed": true,
            "comparison_simulation_vs_experiment": "AlphaFold's predictions are compared to experimental structures; the paper uses AlphaFold as an exemplar of computational predictions validated by experimental benchmarks (no numerical comparison provided here).",
            "validation_success_rate": "Not specified numerically in this paper excerpt; AlphaFold is characterized qualitatively as accurately determining 3D structures in hours.",
            "domain_validation_standards": "Agreement with experimentally determined protein structures and community benchmarks (e.g., CASP) is treated as the validation standard in structural biology.",
            "when_simulation_sufficient": "In protein structure prediction, high-accuracy computational predictions can be sufficient for many downstream uses when validated against experimental benchmarks; the paper uses AlphaFold as an example of computational approaches achieving high trust.",
            "simulation_failures": "Not discussed in this paper.",
            "uncertainty_quantification": "AlphaFold provides confidence metrics (in original literature), but this paper does not enumerate them.",
            "fabrication_detection": "Not applicable.",
            "validation_cost_time": "AlphaFold greatly reduced time to obtain structural models (hours vs. years historically) — qualitative claim in the paper; no numeric resource/cost accounting provided here.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Paper notes that many automated tools (including AlphaFold) still depend on human oversight in research pipelines; AlphaFold is domain-specific rather than a full AI Scientist.",
            "acceptance_credibility": "AlphaFold is cited as a high-credibility automated tool because its computational predictions were validated against experimental structures and widely adopted by the community.",
            "comparison_to_gold_standard": "AlphaFold's computational predictions were compared to experimental structural gold-standards (implied in citation), leading to broad acceptance; specific numerical comparisons not provided in this paper.",
            "uuid": "e2212.7"
        },
        {
            "name_short": "DeepReviewer-14B (LLM-as-a-Judge)",
            "name_full": "DeepReviewer-14B (LLM-based peer review simulation model)",
            "brief_description": "A state-of-the-art review model used in a simulated peer-review evaluation ('LLM-as-a-Judge') to systematically assess 28 AI-generated research papers for implementation-level reliability and scientific quality.",
            "citation_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process.",
            "mention_or_use": "use",
            "system_or_method_name": "DeepReviewer-14B (LLM-based review)",
            "scientific_domain": "Meta-science / scientific peer review",
            "validation_type": "other",
            "validation_description": "Simulated peer-review: DeepReviewer-14B evaluated 28 public AI-generated papers across dimensions (soundness, presentation, contribution) and produced scores and defect categorizations; used to surface implementation/experimental weaknesses in AI-generated outputs.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable — this is a review/evaluation process rather than experimental validation.",
            "validation_success_rate": "Not applicable; evaluation produced average scores (see Table 2) and found 100% of papers exhibited 'Experimental Weakness' (Table 3).",
            "domain_validation_standards": "Peer-review style rubrics (soundness, presentation, contribution, decision) are used as proxies for community validation and quality assessment.",
            "when_simulation_sufficient": "Authors note that peer-review simulation can filter low-quality papers but is imperfect at predicting long-term impact; hence it is a partial validation method.",
            "simulation_failures": "Simulated peer review has limitations: selection bias in evaluated papers and known issues in peer review accuracy mean it cannot fully substitute human judgment or downstream experimental validation.",
            "uncertainty_quantification": "Scores and percentiles are reported (Table 2); but authors acknowledge review scores are imperfect predictors of future impact.",
            "fabrication_detection": "Authors propose automated detection and labeling systems plus specialized evaluation tools (e.g., DeepReview) to help identify low-quality or fabricated AI-generated content; DeepReviewer exemplified as an evaluation component.",
            "validation_cost_time": "Simulated peer review is faster than physical experiments but requires development of reliable review models and may still be resource-intensive; no explicit timing metrics provided.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Peer-review simulation may miss long-term impact and can be gamed; authors recommend hybrid automated + human-in-the-loop review for oversight.",
            "acceptance_credibility": "The paper uses DeepReviewer results to argue that AI-generated papers have systematic experimental weaknesses; peer-review-style validation affects acceptance and credibility but has known reliability limits.",
            "comparison_to_gold_standard": "Compared implicitly to human peer review — DeepReviewer offers a consistent automated judge but the paper notes human review and bibliometrics are imperfect gold standards for predicting impact.",
            "uuid": "e2212.8"
        },
        {
            "name_short": "LLM-as-a-Judge (method)",
            "name_full": "LLM-as-a-Judge (simulated peer review methodology)",
            "brief_description": "A methodology using large language models as reviewers to simulate peer review and assess the quality and implementation reliability of AI-generated research outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "LLM-as-a-Judge (simulated peer review approach)",
            "scientific_domain": "Meta-science / evaluation methodology",
            "validation_type": "other",
            "validation_description": "Uses a review LLM (DeepReviewer-14B) to systematically evaluate AI-generated papers under unified standards (soundness, presentation, contribution), revealing pervasive experimental weaknesses across AI-generated papers.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable — the approach evaluates reported work rather than reproducing experiments physically or computationally.",
            "validation_success_rate": "Not applicable; results showed high rates of detected defects (Table 3: 'Experimental Weakness' in 100% of evaluated papers).",
            "domain_validation_standards": "Peer-review criteria are used as proxies for validation in the scholarly domain; authors caution about limitations of peer review for long-term impact assessment.",
            "when_simulation_sufficient": "Authors argue LLM-based review can filter low-quality papers but cannot fully substitute the depth of human experimental verification.",
            "simulation_failures": "Limitations include selection bias in public AI-generated outputs and known unreliability in peer review to predict long-term impact; LLM reviewers may not detect subtle but important methodological or experimental flaws.",
            "uncertainty_quantification": "Paper reports scores and percentiles but emphasizes these are imperfect predictors; no calibrated uncertainty intervals given for review judgements.",
            "fabrication_detection": "LLM-as-a-Judge can flag experimental weaknesses and reproducibility issues, forming part of a detection workflow; authors also call for automated detection tools and transparent labeling.",
            "validation_cost_time": "Faster than physical reproduction but requires careful development and calibration of review models; paper does not provide specific timing.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Peer-review proxies cannot fully replace experiment reproduction or long-term impact metrics; risk of false negatives/positives in detecting groundbreaking work.",
            "acceptance_credibility": "LLM-based review affects immediate acceptance/credibility but authors caution about over-reliance; recommend hybrid automated + human review systems.",
            "comparison_to_gold_standard": "Compared to human peer review and bibliometrics; authors cite literature showing review scores are not reliable predictors of future impact.",
            "uuid": "e2212.9"
        },
        {
            "name_short": "RL surrogate simulation",
            "name_full": "Simulation/surrogate environment for RL training (LLM-simulated feedback)",
            "brief_description": "The practice of using LLMs or simulated environments to approximate task execution and provide faster, approximate feedback to RL agents to improve sampling efficiency for long-horizon planning tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "LLM-based environment simulation / surrogate feedback for RL",
            "scientific_domain": "Reinforcement learning / agent training methodology",
            "validation_type": "low-fidelity simulation",
            "validation_description": "Authors propose leveraging LLMs to simulate aspects of the environment or task execution to give quicker (approximate) feedback during RL training, thereby accelerating long-horizon planning and meta-reasoning training loops.",
            "simulation_fidelity": "Low-fidelity/approximate: simulations provide faster but potentially approximate feedback; fidelity and modeled physics/chemistry are not specified and approximations may diverge from real-world dynamics.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Paper discusses as a proposal to accelerate RL sampling; does not provide direct empirical comparisons between surrogate-simulated feedback and real-world training outcomes.",
            "validation_success_rate": "Not provided — proposed as promising direction to improve sampling efficiency, no numeric validation reported in this paper.",
            "domain_validation_standards": "Surrogate simulations are considered acceptable for speeding up RL training when approximate feedback is informative; domain-specific sufficiency not detailed.",
            "when_simulation_sufficient": "Proposed to be sufficient for RL training feedback if surrogate approximations provide informative signals; authors emphasize potential but caution about approximation errors.",
            "simulation_failures": "Not empirically reported in this paper; authors note potential risk that surrogate feedback may be misleading for real asynchronous, resource-intensive implementation steps.",
            "uncertainty_quantification": "Not discussed; surrogate simulations would require calibration of their uncertainty but paper does not detail methods.",
            "fabrication_detection": "Not applicable.",
            "validation_cost_time": "Motivation: surrogate simulations can drastically reduce wall-clock sampling time compared to real asynchronous code+experiment cycles, but exact savings are speculative in this paper.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Approximate feedback may not capture crucial real-world constraints; transfer from surrogate to real environment remains a challenge.",
            "acceptance_credibility": "Proposed as a training optimization rather than a final validation mechanism; final experimental validation would still be required for scientific claims.",
            "comparison_to_gold_standard": "No empirical comparison given; gold standard remains real-world asynchronous execution for scientific tasks.",
            "uuid": "e2212.10"
        },
        {
            "name_short": "Co-scientist / Human-in-loop",
            "name_full": "Co-scientist (human-AI collaborative research)",
            "brief_description": "A pragmatic hybrid approach where AI systems focus on idea generation and humans perform final experimental implementation and validation, proposed as an alternative to fully autonomous AI Scientists.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Human-AI co-scientist / human-in-the-loop hybrid workflow",
            "scientific_domain": "General scientific research (multi-domain)",
            "validation_type": "hybrid",
            "validation_description": "Humans execute experimental procedures, rigorous verification, and iterative refinement while AI assists with ideation, decomposition, and partial implementation; validation relies on human-conducted experiments and domain expertise supported by AI-produced artifacts.",
            "simulation_fidelity": null,
            "experimental_validation_performed": null,
            "comparison_simulation_vs_experiment": "Hybrid approach uses computational idea-generation plus human-performed experimental validation; the paper advocates this approach to mitigate AI systems' verification weaknesses.",
            "validation_success_rate": "Not quantified; authors argue that even if AI cannot perform full implementation, enabling humans to be ten times more efficient would qualify as successful collaborative scientist.",
            "domain_validation_standards": "Domain-specific experimental validation and human peer review remain the standard; co-scientist approach preserves these human-led validation norms.",
            "when_simulation_sufficient": "Paper suggests simulation/computational outputs alone are insufficient when experiments are required; advocates human execution of experimental validation.",
            "simulation_failures": "Paper uses many computational benchmark failures to motivate the co-scientist approach (agents failing execution/result matching), implying computational-only validation often fails.",
            "uncertainty_quantification": "Not specified for this workflow; relies on human statistical and experimental practices for uncertainty quantification.",
            "fabrication_detection": "Hybrid workflows and human oversight are proposed to reduce the risk of fabricated or unverified AI-generated findings entering the literature.",
            "validation_cost_time": "Human execution remains time/resource intensive; however, co-scientist workflows aim to accelerate human productivity via AI assistance (no numerical time savings provided beyond speculative 'tenfold').",
            "hybrid_validation_approach": true,
            "validation_limitations": "Relies on continued human expertise and can reduce scalability; does not solve the autonomous implementation challenge but is a practical interim strategy.",
            "acceptance_credibility": "Maintains community trust because human-executed experiments and peer review remain central; authors present this as a credible near-term pathway.",
            "comparison_to_gold_standard": "Explicitly presented as preserving gold-standard human experimental validation while leveraging AI for ideation/assistance.",
            "uuid": "e2212.11"
        },
        {
            "name_short": "Experimental Weakness finding",
            "name_full": "Experimental Weakness (defect category found by DeepReviewer evaluation)",
            "brief_description": "A pervasive defect category identified across all 28 evaluated AI-generated papers, indicating failures in experimental design, execution, and result analysis in AI-produced research outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "LLM-generated papers evaluation (defect taxonomy)",
            "scientific_domain": "Meta-science / evaluation",
            "validation_type": "fabricated",
            "validation_description": "DeepReviewer evaluation flagged experimental weaknesses across all sampled AI-generated papers; defects include lack of reproducible experiments, flawed experimental protocols, missing or incorrect result analysis, and inability to execute verified experiments.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable — the finding is that experimental validation is missing or weak in the evaluated AI-generated literature.",
            "validation_success_rate": "100% of the 28 evaluated AI-generated papers were found to have 'Experimental Weakness' (Table 3).",
            "domain_validation_standards": "Paper argues domain standards require rigorous experimental execution, reproducibility, and clear result analysis — many AI-generated papers fail these standards.",
            "when_simulation_sufficient": "Paper implies simulation/computational claims without reproducible execution are insufficient; experimental weaknesses imply missing real validation.",
            "simulation_failures": "Many AI outputs produce code/experiments that cannot be executed or do not match claimed results; the paper documents these replication/execution failures across benchmarks.",
            "uncertainty_quantification": "Not directly addressed per paper-level defects; DeepReviewer provides categorical detections and defect prevalence percentages.",
            "fabrication_detection": "DeepReviewer/LLM-as-a-Judge flagged these weaknesses; authors call for detection systems and transparent labeling of AI-generated work to prevent fabricated/unverified results from proliferating.",
            "validation_cost_time": "Not quantified per paper; the prevalence of experimental weaknesses implies additional human/experimental effort would be needed to verify claims.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Evaluation limited to publicly available outputs (possible selection bias); nevertheless, pervasive experimental weakness is a robust signal of implementation gaps.",
            "acceptance_credibility": "High prevalence of experimental weaknesses undermines the credibility and acceptance of AI-generated papers in the scientific community.",
            "comparison_to_gold_standard": "Compared implicitly to human-authored work where proper experiments and reproducibility practices are expected; AI papers fail to meet those standards in the evaluated sample.",
            "uuid": "e2212.12"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating ai's ability to replicate ai research.",
            "rating": 2
        },
        {
            "paper_title": "Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers.",
            "rating": 2
        },
        {
            "paper_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark.",
            "rating": 2
        },
        {
            "paper_title": "Mle-bench: Evaluating machine learning agents on machine learning engineering.",
            "rating": 2
        },
        {
            "paper_title": "Ml-dev-bench: Comparative analysis of ai agents on ml development workflows.",
            "rating": 2
        },
        {
            "paper_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code.",
            "rating": 2
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials.",
            "rating": 2
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold.",
            "rating": 2
        },
        {
            "paper_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process.",
            "rating": 2
        },
        {
            "paper_title": "A survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods.",
            "rating": 1
        },
        {
            "paper_title": "Curie: Toward rigorous and automated scientific experimentation with ai agents.",
            "rating": 1
        }
    ],
    "cost": 0.023453500000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI Scientists Fail Without Strong Implementation Capability
May 24, 2025</p>
<p>Minjun Zhu 
Engineering School
Westlake University</p>
<p>Zhejiang University</p>
<p>Qiujie Xie 
Engineering School
Westlake University</p>
<p>Zhejiang University</p>
<p>Yixuan Weng 
Engineering School
Westlake University</p>
<p>Jian Wu 
Engineering School
Westlake University</p>
<p>Zhen Lin 
Engineering School
Westlake University</p>
<p>Linyi Yang yanglinyiucd@gmail.com 
The emergence of Artificial Intelligence (AI) Scientist
University College London</p>
<p>Yue Zhang zhangyue@westlake.edu.cn 
Engineering School
Westlake University</p>
<p>AI Scientists Fail Without Strong Implementation Capability
May 24, 2025F860A8464F3C105E680E4FA57B83AF94arXiv:2506.01372v2[cs.AI]AI ScientistImplementation GapHypothesis and Verification
represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation.Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent.Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools.Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers.To better illustrate the root cause of this implementation gap, we provide an in-depth discussion on the fundamental limitations of AI Scientist.This position paper aims to call for the participants in the community to bridge the implementation gap.</p>
<p>Introduction</p>
<p>The automation of scientific discovery has long been one of humanity's deepest desires (Langley, 1987, King et al., 2009, Radensky et al., 2024, AI, 2025).In recent years, with the advances in deep neural network technology, a range of automated scientific tools has emerged, leading to groundbreaking achievements in fields such as biomedicine (Yang et al., 2025c, Jumper et al., 2021), chemistry (Stokes et al., 2020), and materials science (Szymanski et al., 2023).For instance, DeepMind's AlphaFold can determine the 3D structures of proteins in just a few hours, a task that previously took years to solve (Jumper et al., 2021).In recent, researchers developed an autonomous laboratory, A-Lab, which successfully synthesizes 41 novel inorganic materials within 17 days (Szymanski et al., 2023).However, these scientific tools still rely heavily on human involvement.Researchers must first formulate ideas to be tested, while AI is responsible for the labor-intensive tasks of verification and iterative search.Therefore, these systems cannot be considered as truly automated scientific research.</p>
<p>The emergence of LLM-based AI Scientist has propelled the automation of scientific research to the next level, with AI taking the lead as the primary executor of scientific discovery, managing the entire workflow from idea generation to experiment execution (Lu et al., 2024, Weng et al., 2025).Recent studies have shown that research papers produced by AI Scientist have already reached the level of submissions to major machine learning conferences (Si et al., 2024, Yamada et al., 2025, Intology, 2025).As shown in Figure 1, we demonstrate the progress made by AI Scientist-v2 (Yamada et al., 2025), and the research output has received review scores exceeding the average acceptance threshold for human-authored papers.Similarly, researchers present an empirical validation through multiple peer-reviewed publications accepted at ICLR 2025 workshops and ACL 2025 main conference (Intology, 2025).Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools (e.g., AlphaFold (Jumper et al., 2021)).</p>
<p>In this position paper, we first propose a conceptual framework (Section 2) that defines an AI Scientist as an advanced end-to-end system capable of independently formulating scientific ideas and performing the implementation for verifying these ideas.This definition forms the theoretical foundation of our position, aligns with current research progress (Lu et al., 2024, Weng et al., 2025, Yamada et al., 2025), and emphasizes that the core capability of an AI Scientist lies in generating innovative and feasible ideas at scale (Si et al., 2024, Wang et al., 2024a, Hu et al., 2024, Yang et al., 2025d).The idea-generation capability is a key feature that sets AI Scientists apart from automated scientific tools.While recent advances demonstrate that AI Scientists can generate highly innovative ideas (Si et al., 2025), their implementation capabilities remain constrained (Chan et al., 2024, Starace et al., 2025, Xiang et al., 2025, Siegel et al., 2024, Padigela et al., 2025), creating a significant gap between innovative idea generation and complete implementation.</p>
<p>Our Position:</p>
<p>The fundamental bottleneck for AI Scientists lies in their implementation capability to effectively execute the verification of these ideas.</p>
<p>We defend our argument by analyzing quantitative evidence from existing benchmarks used to evaluate LLMs' abilities in performing complex engineering tasks (Section 3.2).While LLMs can generate highly novel ideas (Si et al., 2024, Chai et al., 2024, Gottweis et al., 2025), their performance in experiment execution is exceptionally poor (Table 1).For instance, a leading LLM like Claude 3.5 Sonnet scored only 1.8% on PaperBench (Starace et al., 2025).This implementation gap is further supported by a systematic evaluation (Section 3.3), which leverages a state-of-the-art review model, DeepReviewer-14B (Zhu et al., 2025), to assess 28 research papers generated by five advanced AI Scientist systems.The results demonstrate that current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers.Finally, to clearly illustrate the root cause of the implementation gap, we provide an in-depth discussion on the fundamental limitations of AI Scientist (Section 4).</p>
<p>In summary, this paper validates and deeply analyzes the implementation gap in existing AI Scientist systems based on extensive quantitative evidence and a simulated peer-review process.Furthermore, as the development of AI Scientists will bring greater regulatory challenges, we comprehensively examine the ethical considerations (Section 5) faced by AI Scientists and suggest directions for future research (Section 6).We hope this position paper will contribute to a clearer understanding of the limitations of current AI Scientist, shedding light on the future development of AI Scientist.</p>
<p>Definition of the AI Scientist</p>
<p>The emergence of automated scientific tools has accelerated scientific discovery across numerous domains (King et al., 2009, Yang et al., 2025c, Jumper et al., 2021, Stokes et al., 2020, Szymanski et al., 2023).However, these tools fundamentally operate within a paradigm where human researchers remain in the dominant position of scientific discovery, and thus cannot be classified as fully automated AI Scientists.In this section, we first provide a detailed discussion of the unique characteristics of the AI Scientist (Section 2.1).Building on this discussion, we then propose a conceptual framework that formally defines the AI Scientist in a mathematical form (Section 2.2).Scientific tools, originating from AI for Science research, represent specialized AI systems designed to solve specific scientific problems by processing data and generating results within defined domains.These tools have demonstrated remarkable success across diverse scientific fields, including protein structure prediction (e.g.Al-phaFold) (Jumper et al., 2021), antibiotic discovery through deep learning approaches (Stokes et al., 2020), and autonomous chemical research with large language models (Boiko et al., 2023).These scientific tools funda-mentally operate within a knowledge-dependent collaborative framework between humans and AI.</p>
<p>Unique Characteristics</p>
<p>AI Scientist represents a research paradigm shift where AI assumes the role of an autonomous scientist capable of conducting independent scientific research.As illustrated in Figure 2, while scientific tools operate under human supervision, receiving data as input and producing predictions as output, AI Scientist goes a step further by demonstrating autonomous scientific reasoning capabilities.It accepts research questions as input and engages in iterative, self-directed interactions with scientific tools to generate comprehensive solutions.Unlike scientific tools that function as sophisticated instruments awaiting human guidance, AI Scientist exhibits genuine scientific agency, conducting end-to-end scientific investigations from question formulation to solution discovery (Yamada et al., 2025).</p>
<p>Conceptualized Framework</p>
<p>Our Definition: An AI Scientist is an advanced end-to-end system capable of independently formulating scientific ideas and executing the requisite verification and falsification procedures.</p>
<p>We define an AI Scientist, denoted as  AI , as a fully autonomous scientific intelligence capable of independently performing diverse scientific research tasks.Different from a general scientific tool, it must possess dual capacities, including idea generation and experimental execution.A complete scientific research task typically originates from an initial scientific question  init and leverages existing domain knowledge  domain .An AI Scientist, denoted  AI , operates within the scope of human ethical constraints ℛ human and resource constraints ℬ res to conduct this task.The primary output is the generation of novel scientific knowledge  new and associated verifiable artifacts  sci .The process through which an AI Scientist aims to achieve the optimal output from a scientific research task can be formally represented as:
(𝒦 new , 𝒜 sci ) ← max{𝒮 AI (𝒬 init , 𝒦 domain , ℛ human |θ AI , ℬ res )} (1)</p>
<p>Arguments for Implementation Capability</p>
<p>We argue that the fundamental bottleneck limiting AI Scientists lies not in their idea generation capabilities, but in their capacity to execute rigorous implementation procedures required for reliable scientific research.To support this position, we present three lines of evidence: systematic analysis of research trends in the AI Scientist literature (Section 3.1), comprehensive benchmark analysis across multiple evaluation frameworks (Section 3.2), and systematic peer review assessment using LLM-as-a-Judge methodology (Section 3.3).</p>
<p>Research Trend of AI Scientist</p>
<p>Our statistical analysis of AI Scientist papers on arXiv up to May 23, 2025 (see Appendix B for details), reveals key trends illustrated in Figure 3.The lower panel of the figure shows that while the total number of publications is growing, studies focusing on idea generation without concrete implementation details consistently outnumber those incorporating such implementations.Despite this disparity in publication numbers, the upper panel indicates a crucial counterpoint: papers that include substantive implementation details achieve a significantly higher average number of citations.This signals strong community valuation for executable advancements and underscores the importance of addressing the implementation gap.This then raises a critical question: if implementation-focused research garners higher impact, why does its volume remain markedly lower?This disparity strongly implies that the path of implementation is fraught with substantial challenges.Empirical Evidence of Implementation Gap.Advanced LLMs achieve near-saturated performance on simple code generation benchmarks like HumanEval (Chen et al., 2021, Liu et al., 2023, Yang et al., 2025a).For example, o3 exhibits excellent problem-solving capabilities in the 99.8th percentile of human performance on algorithmic competition platforms like Codeforces.However, the performance of SoTA LLMs drops dramatically when it comes to real-world research scenarios.As depicted in  (Siegel et al., 2024) (reproducing computational results from scientific papers, determined by accuracy), and ML-Dev-Bench (Padigela et al., 2025) (completing diverse ML development workflow tasks, assessed by success rates).Each takes a different approach to measuring how well AI systems can automate aspects of ML research.These evaluations consistently demonstrate that LLMs face difficulty in translating conceptual understanding or initial plans into verifiably correct and operational code.This "implementation gap" fundamentally limits AI Scientist's verification capabilities.</p>
<p>Quantitative Analysis</p>
<p>Beyond Code Generation.The complexity of real-world research implementation processes extends far beyond simple code generation tasks, often requiring sustained reasoning and multi-step problem-solving.However, current LLMs exhibit relatively weak performance on such complex challenges.LiveCodeBench (LCB) (Jain et al., 2024), a more complex evaluation benchmark than Humaneval (Chen et al., 2021) that collects problems from periodic contests on LeetCode, AtCoder, and Codeforces platforms, evaluates Code LLMs across diverse code-related scenarios, including code generation, execution, self-repair, and output prediction.o4-mini achieves SoTA performance on the code generation subtask with only 52.1% pass@1 score.This poor performance on complex coding tasks reveals that AI scientists lack the implementation ability to handle sophisticated code-based research scenarios.</p>
<p>Implementation and verification.We observe that the verification bottleneck emerges across multiple stages of the research process.SciReplicate-Bench (Xiang et al., 2025), which tasks LLM agents with generating Python code to reproduce algorithms from NLP research papers, reveals that despite agents demonstrating an understanding of algorithmic logic (evidenced by high reasoning graph accuracy), they struggle with code execution.The best agent achieved only 39% execution accuracy, indicating its generated code passed functional test cases for just 39% of the tasks, highlighting a failure to ensure implementation correctness and runtime behavior.Similarly, PaperBench (Starace et al., 2025) requires LLM agents to replicate entire machine-learning papers from scratch by developing codebases and running experiments.While agents can generate code components (e.g., o1-High achieving 43.4% success on weighted "Code-Development" sub-tasks), their performance on subsequent verification stages is poor.On rubric-defined leaf nodes for "Execution" (successfully running the code) and "Result Match" (quantitatively matching the paper's reported results), Claude 3.5 Sonnet scored only 1.8% and 0.7% respectively.This poor performance indicates a breakdown in ensuring the developed solution operates correctly and produces the intended outcomes.</p>
<p>Discussion.The verification challenge extends beyond initial code implementation to debugging, iterative refinement, and validation of experimental outcomes.Evidence from MLE-Bench and ML-Dev-Bench (Chan et al., 2024, Padigela et al., 2025) shows that LLM agents frequently fail to debug their code or produce valid submissions, with 20% of o1 preview runs on MLE Bench failing this step, and struggle to optimize model performance.Debugging, an explicit verification procedure, also indicates persistent agent failures that highlight the verification bottleneck (Chan et al., 2024).The incapacity to iteratively refine solutions towards better performance, illustrated in ML-Dev-Bench where all tested agents scored 0% on "Model Performance" tasks, further signifies deficiencies in robust verification loops essential for scientific advancement (Padigela et al., 2025).Furthermore, CORE-Bench, which requires agents to reproduce results and then answer questions based on these outputs, assesses the verification of entire computational experiments.This process, involving multiple stages of reproduction and reasoning, presents significant challenges.For instance, the imperfect success rates (e.g., CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium) highlight the difficulties in this complex verification process (Siegel et al., 2024).These difficulties across verification tasks suggest that enhancing AI Scientists' systematic verification capability is crucial for their maturation into ideal AI Scientists.Current LLMs, while proficient in content generation, fail to rigorously validate their outputs against explicit criteria, a foundational component of scientific practice.</p>
<p>LLM-as-a-Judge Reveals the Implementation Weaknesses</p>
<p>To further support the existence of implementation gap, we employ a simulated peer review methodology to assess the actual quality of scientific outputs from current AI Scientist systems, particularly their implementation-level reliability.We select 28 publicly available research papers generated independently by five different AI Scientist systems and utilize the SoTA review model DeepReviewer-14B (Zhu et al., 2025) to conduct systematic evaluation under unified standards.We acknowledge that potential selection bias in the public availability of these papers (e.g., researchers may only publish better-performing outputs) means our evaluation results may not fully represent the average output quality of these systems across all scenarios.Nevertheless, this analysis provides valuable insights into the general quality level of current AI-generated research papers.</p>
<p>Rooted Limitations of Execution Capabilities</p>
<p>Our empirical analysis (Section 3) reveals a clear pattern that while AI Scientists are conceptualized as advanced iterations of traditional scientific tools, they consistently fail at implementation and verification procedures across diverse scientific contexts.This raises a critical question: Why do these sophisticated systems fail to achieve consistently strong results, especially when traditional scientific tools, wielded by human researchers, prove highly effective?To understand this paradox, we provide a discussion on the root cause of the implementation gap (Section 4.1 ) and present an in-depth analysis of the fundamental limitations of AI Scientist (Section 4.2).</p>
<p>Two Primary Facets of Implementation Gap</p>
<p>The implementation gap for AI Scientists comprises two primary facets: (1) AI Scientists often exhibit bottlenecks in the planning and execution stages.This manifests in three key areas: failures in longrange logical reasoning required for coherent experimental design, inadequate multi-agent collaboration capabilities including strategic planning across complex multi-file implementations and converting conceptual ideas into working code, and insufficient coordination with external tools and systems; (2) Even when implementation code is generated, AI Scientists demonstrate fundamental weaknesses in evaluation processes.This includes failures in debugging capabilities, experimental validation, result interpretation, and iterative refinement based on experimental feedback.Current systems lack robust mechanisms for assessing implementation quality, validating experimental outcomes, and providing reliable feedback loops that can guide subsequent implementation improvements.</p>
<p>Prevent building "castle in the air".Agent tools often produce difficult-to-verify code and experiments, while evaluation gaps prevent AI Scientists from recognizing and correcting implementation issues through iterative refinement.Without fundamentally enhancing both capabilities, the idealized AI Scientist capable of independent scientific exploration will remain inefficient.</p>
<p>Rooted Limitations</p>
<p>From the current literature on AI scientists, we conclude four major limitations that collectively explain why AI scientists struggle with complex, multi-stage implementation processes:</p>
<p>Limitation 1: fundamental cognitive and execution capabilities.Scientific implementation requires sophisticated long-range logical reasoning across multiple abstraction levels.Existing LLMs demonstrate significantly decreased coherence and robustness as reasoning chains extend (Wu et al., 2025b,a), and increased thinking time does not necessarily yield stronger performance (Ballon et al., 2025).Furthermore, LLM-based agents possess limited capacity to retain past interaction information, with memory deteriorating as text length increases (Pink et al., 2025, Cemri et al., 2025).Most critically, mainstream language models exhibit markedly weaker performance in multi-turn dialogues or multi-step interactive tasks requiring context coherence, deep understanding, and state tracking, with average performance decreases reaching 39% (Laban et al., 2025).This capability degradation in scenarios involving long-range dependencies and complex interactions directly constrains AI Scientist performance in executing complex scientific experiments requiring sustained attention and coherent reasoning chains.</p>
<p>Limitation 2: strategic planning and reasoning.Scientific implementation requires comprehensive abilities for strategic reasoning, continuous monitoring, and dynamic adjustment across all research stages (Lu et al., 2024, Yamada et al., 2025).High-quality research implementation demands global planning abilities spanning entire codebases, which typically contain multiple interdependent files with hundreds of lines requiring coordinated modification (Jimenez et al., 2024, Aleithan et al., 2024).Long-term, complex scientific exploration tasks such as discovering new materials, and modeling complex biological systems particularly require continuous iteration of research directions and experimental strategies over extended time scales based on emerging results and external feedback (Merchant et al., 2023, Brixi et al., 2025, Weng et al., 2023).However, current LLMs demonstrate inadequate adaptive planning and metacognitive abilities when handling highly open, creative scientific research requiring dynamic adjustments to overall research blueprints.While reinforcement learning approaches may potentially enhance LLMs' generalization and metacognitive capabilities, the resource investment required for "inventor" roles like AI Scientists that need to perform complex asynchronous operations and real-world interactions proves enormous.Figure 4 highlights AI's acceleration over human performance in complex tasks such as reasoning and web-based research.While AI Scientists also achieve tasks faster than humans, their estimated single-sample RL training time is orders of magnitude greater than simpler AI agents.This substantial increase in required sampling time (detailed in Appendix A) underscores the immense challenge of developing AI Scientists via standard RL methodologies.(Guo et al., 2024, Qian et al., 2024, Pu et al., 2025b).This requires AI Scientist to not only understand instructions conforming to collaborative protocols but also precisely execute the implementation phases assigned to it within tasks and reliably feed its outputs back to the collaborative network (Bo et al., 2024, Zhang et al., 2024).However, current LLM Agents still have considerable room for improvement in robustness and adaptability when interacting with dynamic environments (Wei et al., 2025).For instance, when calling a series of external APIs to complete a complex scientific computational process, LLM often struggles to handle subtle changes in API interfaces, and other practical engineering issues (Shen et al., 2025).</p>
<p>Limitation 4: evaluation and verification.Existing benchmarks such as MLE-Bench (Chan et al., 2024) and PaperBench (Starace et al., 2025) primarily focus on the complete reproduction of code and experiments from papers.SciReplicate-Bench (Xiang et al., 2025) emphasizes generating necessary code from scientific papers, while ScienceAgentBench (Chen et al., 2025) concentrates on independent and singular data-driven tasks.However, there is currently a lack of a comprehensive benchmark that can evaluate the entire scientific workflow, from initial idea generation through to final implementation and completion.This absence makes it difficult to fairly compare the end-to-end capabilities of different AI Scientist systems.</p>
<p>Additionally, there is a deficiency in evaluation approaches that incorporate measures for external supervision during the AI Scientist's implementation process.The deeper issue is that the quality of scientific discovery often lacks unified objective standards, and the process of scientific exploration is filled with uncertainty and openness, making comprehensive evaluation and effective supervision of AI Scientist's verification capability exceptionally difficult.Evaluating AI Scientist's output (e.g., generated papers) from a peer review perspective, while being a results-oriented assessment method, also has inherent limitations.As in human research systems, even experienced peer reviewers may not always accurately identify the groundbreaking and far-reaching work.A frequently cited example is that the word2vec paper (Mikolov et al., 2013) was initially rejected by ICLR 2013, but later received the "Test of Time Award" at NeurIPS 2023.Extensive experimental analyses have demonstrated that review scores are not reliable indicators for predicting future impact (Abramo et al., 2019, Cortes andLawrence, 2021), suggesting that peer review may be more suitable for filtering low-quality papers rather than identifying the highest quality papers.</p>
<p>Ethical Considerations</p>
<p>Sub-Position: AI scientists are in urgent need of a comprehensive system for generation management and quality evaluation.</p>
<p>As autonomous research agents, AI Scientists lack values and moral constraints.They are incapable of making ethical judgments about the societal impact of their work, and they do not self-regulate based on potential risks associated with their findings (Bengio et al., 2025).As AI Scientists possess stronger capabilities in idea generation and experiment execution, their influence on scientific research and society could far surpass that of current LLMs and scientific tools (e.g., Deep Search, AutoSurvey (Wang et al., 2024c)).In the absence of proper oversight, AI Scientists may: (1) be misused, overwhelming the peer review system, leading to a decline in overall research quality;</p>
<p>(2) enter unethical or dangerous research domains, autonomously generating and publishing sensitive findings that accelerate the development of harmful technologies;</p>
<p>(3) weaken the quality of PhD training, leading to a decline in human research standards and overall scientific literacy.To prevent the above situations, we argue that AI Scientists are in urgent need of a comprehensive system for generation management and quality evaluation, thus enabling effective behavior regulation within the human moral framework (Jobin et al., 2019).This system should include, but not be limited to, the following components:</p>
<p>(1) Implement measures to prevent AI-generated content from disrupting human review systems: Effective strategies should be adopted to ensure that AI-generated articles do not interfere with human peer-review systems while maintaining high standards of quality.This includes establishing a centralized platform to archive scientific outputs generated by AI Scientists, developing automated detection systems to identify such content, and creating specialized evaluation tools (e.g., DeepReview (Zhu et al., 2025)) to assess the quality of AI-generated research outputs.These tools should help identify and filter low-quality content, thereby reducing the burden on the peer review process.All AI-generated outputs must be transparently labeled and reviewed, including information on their origin, generation methods, and scientific tools.</p>
<p>(2) Establish boundaries and strengthen training programs: Implement clear boundaries between human-led and AI-led research processes to ensure that PhD students receive comprehensive training.Key components of doctoral education(e.g., idea testing), should prioritize human involvement to maintain high standards of scientific literacy.Additionally, guidelines should be established to prevent over-reliance on AI Scientists in PhD training, ensuring that AI tools serve as supplements rather than substitutes in the educational process.</p>
<p>(3) Formulate an ethics and responsibility convention: A global convention should be established to define the ethical boundaries and risk management principles for AI-driven research (Huang et al., 2022).</p>
<p>All researchers and institutions utilizing AI Scientists must fully disclose the generation process, algorithmic sources, training data, and potential societal risks of their findings.Additionally, a hybrid mechanism combining automated and human-in-the-loop review should be implemented for continuous ethical oversight and risk evaluation, ensuring that AI Scientist research activities remain within socially safe boundaries (Jobin et al., 2019, Khan et al., 2022).Furthermore, appropriate legislation should be developed to regulate AI Scientists by imposing strict limitations on their use for specific research purposes.</p>
<p>Future Directions</p>
<p>This section outlines feasible pathways to bridge the current implementation capability gap of AI Scientists.</p>
<p>Addressing foundational Basic Abilities is paramount.While scaling laws for pre-training and post-training (Kaplan et al., 2020, Zhang et al., 2025) promise progressive LLM improvements, immediate strategies like well-defined Workflows (Li et al., 2024d, Gu et al., 2024b) also can mitigate current implementation weaknesses.Structuring research processes with human-defined tools allows for guided AI execution and targeted interventions.For instance, Retrieval Augmented Generation (RAG) can counteract limitations in handling long texts or accessing current information (Fan et al., 2024, Arslan et al., 2024), thus expanding the knowledge scope of AI systems.</p>
<p>A significant challenge for sophisticated Strategic Planning is the immense resource consumption of RL (Cao et al., 2024).A promising direction to alleviate this involves leveraging LLMs to simulate aspects of the environment or task execution, thereby accelerating the RL feedback loop (Sun et al., 2025).By allowing the RL agent to receive quicker, albeit potentially approximate, feedback on its actions, particularly for operations that are inherently time-consuming in the real world, the sampling efficiency may be significantly improved.This could reduce the extensive wall-clock time typically required for training robust long-horizon planning and adaptive meta-thinking capabilities in complex scientific domains.</p>
<p>Ensuring Reliable Verification and Fostering Collaboration is crucial.Standardized protocols like MCP and A2A (Yang et al., 2025b, Ray, 2025, Hou et al., 2025) can establish basic interoperability.A promising direction is to build modular multi-agent systems, where specialized AI agents for sub-tasks (e.g., literature review, code generation) are coordinated by a central "Planner Agent" trained via advanced RL, leveraging existing tools (e.g., PASA (He et al., 2025)) rather than reinventing capabilities.Furthermore, enhanced oversight of AI Scientist inference processes is imperative, not just to prevent benchmark "hacking", but also to instill ethical boundaries against unscrupulous data acquisition or other problematic behaviors.</p>
<p>Finally, the Evaluation of AI Scientists (Chang et al., 2024) must evolve towards a holistic, coarse-grained paradigm reflecting real-world scientific discovery's multifaceted nature.Scientific breakthroughs involve both practical utility and novelty.Thus, evaluation frameworks should go beyond single-metric optimization, adopting multi-objective criteria that assess performance gain, originality, experimental rigor, and communication clarity.This multi-faceted approach will offer a more accurate measure of an AI Scientist's true contribution, guiding development toward impactful scientific exploration.</p>
<p>Conclusion</p>
<p>The rise of AI Scientists marks a paradigm shift in scientific discovery, with large language models (LLMs) now driving the workflow from idea generation to experiment execution.Recent systems have shown promise, producing research accepted at ICLR 2025 workshops and sparking discussions on the imminence of human-level AI Scientists.However, despite this progress, AI Scientists have yet to achieve breakthroughs in computer science comparable to traditional automated tools.Based on benchmark analyses and a systematic evaluation of 28 papers from five leading AI Scientist systems, we identify a core bottleneck: the inability to reliably execute and verify experiments.This implementation gap limits both scientific rigor and the quality of the research output.We analyze its root causes and call on the community to address this critical limitation.</p>
<p>Alternative Views.An alternative viewpoint suggests that AI Scientists need not pursue completely autonomous implementation capabilities in the short term, but rather facilitate human-machine collaboration as Co-scientists to assist humans.This approach avoids the deficiencies of LLMs in Dynamic Planning capabilities and Reliable Verification capabilities, instead allowing AI to focus on its strengths, such as idea generation, while humans execute the specific experimental results (Weng et al., 2025).If an AI system, though unable to independently complete all implementation details, can increase human scientists' efficiency tenfold, or help human scientists conceive and verify complex ideas previously beyond reach, then it undoubtedly also qualifies as a successful "collaborative scientist."</p>
<p>Unite</p>
<p>A. Sampling Time Calculation for Different Types of AI Agents</p>
<p>We referenced existing literature (Guo et al., 2025, Yang et al., 2025a, Muennighoff et al., 2025) and our experience to estimate the sampling time potentially required for different types of AI agents trained via reinforcement learning, as illustrated in In contrast, an AI Scientist executing end-to-end scientific discovery tasks has complexity and interaction requirements far exceeding the previous two types.We roughly estimate it might need to generate over 100,000 tokens of content (for example, operational and experimental code approximately 50,000 tokens (T in f er_code ≈ 1250s), research paper writing approximately 30,000 tokens (T in f er_paper ≈ 750s), reviewing and understanding relevant literature approximately 20,000 tokens (T in f er_lit ≈ 500s)), with pure LLM inference time for just this portion being T in f er_CS = T in f er_code + T in f er_paper + T in f er_lit ≈ 2500s.More critically, the "implementation" process of an AI Scientist, such as code writing, debugging, compiling, running experiments, and data analysis, is highly asynchronous and time-consuming.Assuming a rapid research code operation and experimental cycle (from writing to obtaining preliminary results) requires an average of T op_code ≈ 12 hours = 43200s, while in-depth literature research and analysis might require T op_lit ≈ 20 minutes = 1200s.Therefore, the total estimated sampling time to complete a relatively complete scientific exploration loop would be T sample_CS = T in f er_CS + T op_code + T op_lit ≈ 2500s + 43200s + 1200s ≈ 46900s.As intuitively demonstrated in Figure 4, the sampling time required for an AI Scientist (approximately 46,000 seconds) far exceeds that of an AI Reasoner (approximately 250 seconds) and an AI Web Agent (approximately 700 seconds).Notably, AI Reasoners can typically rapidly generate large quantities of training samples through batch generation in parallel, whereas each implementation step of an AI Scientist (especially parts involving code execution and experiment waiting) is almost entirely asynchronous, and requires exclusive computational resources or experimental equipment for learning and feedback collection during operations.Consequently, in actual reinforcement learning training processes, the disparity in real training duration between AI Scientists and the former two types will be even more pronounced.</p>
<p>For the calculation of human duration, we referenced existing metrics.For instance, for reasoning tasks, we referred to the human time from the International Mathematical Olympiad, which is approximately 1.5 hours per problem.For Web Agent tasks, we adopted the average human problem-solving time from BrowseComp (Wei et al., 2025) (2 hours) as the human standard.For Scientist tasks, although each paper often requires months of collaborative work by multiple people, for ease of calculation, we used the human duration of 48 hours from PaperBench (Starace et al., 2025) for statistics; however, even under these conditions, humans achieve a success rate of less than 50%.(Pu et al., 2025a), (Yang et al., 2024), (Su et al., 2024), (Li et al., 2024a), (Hu et al., 2024), (Liu et al., 2025), (Wang et al., 2024b) (Weng et al., 2025), (Xiong et al., 2024) (Gu et al., 2024a), (Li et al., 2024b), (Yu et al., 2024) (Gottweis et al., 2025) (Rabby et al., 2025), (Saeedi et al., 2025) (O'Neill et al., 2025), (Garikaparthi et al., 2025), (Sanyal et al., 2025) w/ Exp (Lu et al., 2024), (Li et al., 2024c) (Liu et al., 2024b), (Liu et al., 2024a) (Yuan et al., 2025), (Schmidgall et al., 2025) (Jiang et al., 2025), (Kon et al., 2025) (Schmidgall et al., 2025), (Jansen et al., 2025) (Yamada et al., 2025), (Seo et al., 2025)</p>
<p>B. Regarding the statistics for the papers</p>
<p>We have conducted a comprehensive search on arXiv to gather relevant publications in the AI Scientist field.This collection includes a series of papers from August 2024 to April 2025 for methods or systems, which are cited in Table 4.It indicates that, to date, a significant number of papers have focused on Idea Generation tasks, often without concrete implementations.</p>
<p>Nevertheless, an encouraging trend has emerged since early 2025.As illustrated in Figure 3, implementationfocused research has demonstrated stronger growth momentum, with incremental growth nearly matching that of non-implementation studies by Spring 2025.This suggests the community is beginning to recognize the critical importance of implementation capabilities for developing truly effective AI Scientists-moving beyond theoretical constructs toward practical systems capable of reliable execution.</p>
<p>Figure 1 :
1
Figure 1: The roadmap of AI Scientist from 2024 to future, highlighting key milestones and fundamental challenges that must be overcome to bridge the implementation gap of AI Scientist.</p>
<p>Figure 3 :
3
Figure 3: Analysis of AI Scientist publications on arXiv.The upper panel displays the average number of citations up to now, categorized by containing implementation details.The lower panel shows the growth in the total number of these papers with the same categorization.</p>
<p>Figure 4 .
4
Using a hypothetical 671B parameter LLM (similar to Deepseek-R1) running on 8 H100 cards (assuming 40 tokens generated per second), the pure inference time T in f er_R for a typical arithmetic reasoning task (generating approximately 10,000 tokens of reasoning content) might be around 250 seconds.For an AI Web Agent, the task might include generating approximately 8,000 tokens of instructions and reports (T in f er_WA ≈ 200s), interspersed with approximately 20 API calls for information search (assuming each search and processing takes T search_API = 10s, totaling T search_total = 20 × 10s = 200s), and potentially requiring reading and comprehension of up to 400,000 tokens of web content (assuming reading and comprehension time T read_WA ≈ 200s).The total sampling time is: T sample_WA ≈ T in f er_WA + T search_total + T read_WA ≈ 600s.</p>
<p>Table 1 :
1
State-of-the-art (SoTA) LLMs show relatively low accuracy on code implementation on different tasks.The listed benchmarks are collected from diverse domains.The table below details their tasks, domains, scale, methods, and performance.
Benchmark Task DescriptionDomainsScaleLLM Acc. PerformanceMLE-Bench(Chan et al., 2024) AI Training taskApplied ML75 OpenAI o1-preview16.90%PaperBench (Starace et al., 2025) ICML paper ReplicatingNLP, CV, ML8,316OpenAI o1-high26.00%SciReplicate-Bench (Xiang et al., 2025) Code GenerationNLP100Claude-Sonnet-3.739.00%CORE-Bench (Siegel et al., 2024) Scientific Paper reproduc-Computer Science,270OpenAI GPT-4o55.56%tionSocial Science, andMedicineML-Dev-Bench (Padigela et al., 2025) AI training taskML30Claude-Sonnet-3.550.00%</p>
<p>Table 2 :
2
DeepReviewer-14B Evaluation of AI-Generated Papers from Various AI Scientist Systems.Scores reflect averages across the 'Num' of available papers.Note: Publicly available papers may be curated and not fully representative of typical system output.
AI Scientist SystemNum Soundness↑Presentation↑Contribution↑ Decision↑Rating↑Percentile↑HKUSD AI Researcher71.751.461.570.02.573.43%AI Scientist102.081.801.750.03.358.22%AI Scientist v231.671.501.500.02.332.04%CycleResearcher-12B62.251.752.130.03.7516.88%Zochi22.382.382.250.04.6329.96%</p>
<p>Table 3 :
3
Defect Categories and Their Issues.
Defect CategoryNumber PercentageExperimental Weakness28100%Methodological Unclarity/Flaws2796.4%Writing &amp; Presentation Issues2692.9%Novelty Concerns2589.3%Theoretical Weakness2485.7%Literature Review Deficiencies2278.6%Practicality &amp; Robustness Gaps2175.0%Reproducibility Issues2071.4%Computational Cost Concerns1864.3%Component Analysis1657.1%Hyperparameter Analysis Lacking1657.1%Ethical Considerations Missing310.7%</p>
<p>Table3shows that among the twelve major defect categories, "Experimental Weakness" appears across all 28 evaluated AI-generated papers, with a 100% occurrence rate.
This finding supports our positions regardingimplementation capability limitations, in experimental design, execution, and result analysis. The secondand third most prevalent issues are "Methodological Unclarity/Flaws" (96.4%) and "Writing &amp; PresentationIssues" (92.9%), which reflect AI Scientists' insufficient ability to clearly articulate and implement researchplans. "Novelty Concerns" (89.3%) and "Theoretical Weakness" (85.7%) occur frequently, indicating that
when AI Scientists generate complete papers, they struggle to propose original scientific contributions with solid theoretical foundations.The prevalence of these high-frequency defects highlights systemic issues in the scientific rigor and implementation quality of current AI-generated research, falling below the standards for reliable and valuable scientific outputs.</p>
<p>Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang.
AI.Google'snewai"co-scientist"aimstoacceleratescien-tificdiscovery.Unite.AI,Feb2025.URLhttps://www.unite.ai/googles-new-ai-co-scientist-aims-to-accelerate-scientific-discovery/.
Swe-bench+: Enhanced coding benchmark for llms.arXiv preprint arXiv:2410.06992,2024.</p>
<p>Table 4 :
4
Timeline of AI Scientist Ideas and Code Implementations by Month
2024-082024-092024-102024-112024-122025-012025-022025-032025-04w/o Exp (Zheng et al.,(Ghafarollahi2024)and Buehler,2024), (Raden-sky et al.,2024)
Minjun Zhu and Qiujie Xie contributed equally to this work. Corresponding author(s): Linyi Yang: yanglinyiucd@gmail.com; Yue Zhang: Email zhangyue@westlake.edu.cn
https://ai-researcher.net/social-iclr-2025
AcknowledgementsThe genesis of this position paper traces back to the insightful discussions and interactions at the AI Co-scientist Discussion held in conjunction with ICLR 2025 on April 26, 20241 .We extend our sincere gratitude to the invited speakers, including Chenglei Si, Jindong Wang, Yutaro Yamada, and David Ha, whose perspectives are invaluable.We also deeply appreciate the contributions of the more than 200 participants who engaged in the vibrant discussions on that day; many of the ideas explored in this work were sparked and refined through those collective interactions.We thank every participant for their engagement and for fostering a stimulating environment that significantly shaped our thinking.
Peer review versus bibliometrics: Which method better predicts the scholarly impact of publications?. Giovanni Abramo, Ciriaco Andrea, D' Angelo, Emanuela Reale, Scientometrics. 1212019</p>
<p>A survey on rag with llms. Muhammad Arslan, Hussam Ghanem, Saba Munawar, Christophe Cruz, Procedia Computer Science. 2462024</p>
<p>The relationship between reasoning and performance in large language models-o3 (mini) thinks harder. Marthe Ballon, Andres Algaba, Vincent Ginis, arXiv:2502.156312025not longer. arXiv preprint</p>
<p>Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path. Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt Macdermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, arXiv:2502.156572025arXiv preprint</p>
<p>Reflective multi-agent collaboration based on large language models. Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, Ji-Rong Wen, Advances in Neural Information Processing Systems. 202437</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Genome modeling and design across all domains of life with evo 2. Garyk Brixi, Matthew G Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A Gonzalez, Samuel H King, David B Li, Aditi T Merchant, BioRxiv. 2025</p>
<p>Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, Yun Li, Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. IEEE Transactions on Neural Networks and Learning Systems. 2024</p>
<p>Why do multi-agent llm systems fail?. Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, arXiv:2503.136572025arXiv preprint</p>
<p>Exploring scientific hypothesis generation with mamba. Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal, Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)2024</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2157-6904153March 2024</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Corinna Cortes, Neil D Lawrence, arXiv:2109.09774Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. 2021arXiv preprint</p>
<p>A survey on rag meeting llms: Towards retrieval-augmented large language models. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Aniketh Garikaparthi, Manasi Patwardhan, arXiv:2504.16728Lovekesh Vig, and Arman Cohan. Iris: Interactive research ideation system for accelerating scientific discovery. 2025arXiv preprint</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024arXiv preprint</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Llms can realize combinatorial creativity: generating creative ideas via llms for scientific research. Tianyang Gu, Jingjin Wang, Zhihao Zhang, Haohong Li, arXiv:2412.141412024aarXiv preprint</p>
<p>Large language models for constructing and optimizing machine learning workflows: A survey. Yang Gu, Hengyu You, Jian Cao, Muran Yu, Haoran Fan, Shiyou Qian, arXiv:2411.104782024barXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Pasa: An llm agent for comprehensive academic paper search. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, arXiv:2501.101202025arXiv preprint</p>
<p>Model context protocol (mcp): Landscape, security threats, and future research directions. Xinyi Hou, Yanjie Zhao, Shenao Wang, Haoyu Wang, arXiv:2503.232782025arXiv preprint</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024arXiv preprint</p>
<p>An overview of artificial intelligence ethics. Changwu Huang, Zeqi Zhang, Bifei Mao, Xin Yao, IEEE Transactions on Artificial Intelligence. 442022</p>
<p>. Intology. Zochi technical report. arXiv. 2025</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, Peter Clark, arXiv:2503.227082025arXiv preprint</p>
<p>Aide: Ai-driven exploration in the space of code. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu, arXiv:2502.131382025arXiv preprint</p>
<p>Swe-bench: Can language models resolve real-world github issues. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, ICLR. 2024</p>
<p>The global landscape of ai ethics guidelines. Anna Jobin, Marcello Ienca, Effy Vayena, Nature machine intelligence. 192019</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, nature. 59678732021</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Ethics of ai: A systematic literature review of principles and challenges. Arif Ali Khan, Sher Badshah, Peng Liang, Muhammad Waseem, Bilal Khan, Aakash Ahmad, Mahdi Fahmideh, Mahmood Niazi, Muhammad Azeem, Akbar , Proceedings of the 26th international conference on evaluation and assessment in software engineering. the 26th international conference on evaluation and assessment in software engineering2022</p>
<p>The automation of science. Jem Ross D King, Stephen G Rowland, Michael Oliver, Wayne Young, Emma Aubrey, Maria Byrne, Magdalena Liakata, Pinar Markham, Larisa N Pir, Soldatova, Science. 32459232009</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. Patrick Tser, Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Ang Chen, arXiv:2502.160692025arXiv preprint</p>
<p>Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville, arXiv:2505.06120Llms get lost in multi-turn conversation. 2025arXiv preprint</p>
<p>Scientific discovery: Computational explorations of the creative processes. Langley, 1987MIT Press</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.131852024aarXiv preprint</p>
<p>Learning to generate research idea with dynamic control. Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du, arXiv:2412.146262024barXiv preprint</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024carXiv preprint</p>
<p>Autoflow: Automated workflow generation for large language model agents. Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, Yongfeng Zhang, arXiv:2407.128212024darXiv preprint</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, 202336</p>
<p>Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration. Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Yingzhou Lu, Yue Zhao, arXiv:2411.156922024aarXiv preprint</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou, arXiv:2503.212482025arXiv preprint</p>
<p>Aigs: Generating science from ai-powered automated falsification. Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, Yang Liu, arXiv:2411.119102024barXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292v32024arXiv preprint</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Muratahan Samuel S Schoenholz, Aykol, Nature. 62479902023</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. 262013</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Răileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciucă, arXiv:2504.129762025arXiv preprint</p>
<p>Ml-dev-bench: Comparative analysis of ai agents on ml development workflows. Harshith Padigela, Chintan Shah, Dinkar Juyal, 2025</p>
<p>Position: Episodic memory is the missing piece for long-term llm agents. Mathis Pink, Qinyuan Wu, Ai Vy, Javier Vo, Jianing Turek, Alexander Mu, Mariya Huth, Toneva, arXiv:2502.069752025arXiv preprint</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, Kevin Kj, Tovi Feng, Tom Grossman, Bhavana Hope, Matt Dalvi Mishra, Jonathan Latzke, Joseph Chee Bragg, Pao Chang, Siangliulue, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025a</p>
<p>Piflow: Principle-aware scientific discovery with multi-agent collaboration. Yingming Pu, Tao Lin, Hongyu Chen, 2025b</p>
<p>Scaling large-language-model-based multi-agent collaboration. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, arXiv:2406.071552024arXiv preprint</p>
<p>Iterative hypothesis generation for scientific discovery with monte carlo nash equilibrium self-refining trees. Gollam Rabby, Diyana Muhammed, Prasenjit Mitra, Sören Auer, arXiv:2503.193092025arXiv preprint</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>A survey on model context protocol: Architecture, state-of-the-art, challenges and future directions. Partha Pratim, Ray , Authorea Preprints. 2025</p>
<p>Astroagents: A multi-agent ai for hypothesis generation from mass spectrometry data. Daniel Saeedi, Denise Buckner, Jose C Aponte, Amirali Aghazadeh, arXiv:2503.231702025arXiv preprint</p>
<p>Aishik Sanyal, Samuel Schapiro, Sumuk Shashidhar, Royce Moon, Lav R Varshney, Dilek Hakkani-Tur, arXiv:2504.20090Spark: A system for scientifically creative idea generation. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227arXiv:2504.17192Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning. 2025. 2025arXiv preprintAgent laboratory: Using llm agents as research assistants</p>
<p>Shortcutsbench: A large-scale real-world benchmark for api-based agents. Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma, 2025</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations, ICLR 2025. SingaporeApril 24-28, 2025</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, arXiv:2409.113632024arXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>A deep learning approach to antibiotic discovery. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Shawn Craig R Macnair, Lindsey A French, Zohar Carfrae, Bloom-Ackermann, Cell. 18042020</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.094032024arXiv preprint</p>
<p>Zerosearch: Incentivize the search capability of llms without searching. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang, arXiv:2505.045882025arXiv preprint</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 62479902023</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 2024a1</p>
<p>Scipip: An llm-based scientific paper idea proposer. Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye, arXiv:2410.231662024barXiv preprint</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, Advances in Neural Information Processing Systems. 2024c37</p>
<p>Jason Wei, Zhiqing Sun, Spencer Papay, Scott Mckinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, Amelia Glaese, arXiv:2504.12516Browsecomp: A simple yet challenging benchmark for browsing agents. 2025arXiv preprint</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee, arXiv:2503.04723Shifting long-context llms research from input to output. 2025aarXiv preprint</p>
<p>When more is less: Understanding chain-of-thought length in llms. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang, arXiv:2502.072662025barXiv preprint</p>
<p>Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He, arXiv:2504.00255Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. 2025arXiv preprint</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, arXiv:2505.09388Chenxu Lv, et al. Qwen3 technical report. 2025aarXiv preprint</p>
<p>A survey of ai agent protocols. Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, arXiv:2504.167362025barXiv preprint</p>
<p>Shennongalpha: an ai-driven sharing and collaboration platform for intelligent curation, acquisition, and translation of natural medicinal material knowledge. Zijie Yang, Yongjing Yin, Chaojun Kong, Tiange Chi, Wufan Tao, Yue Zhang, Tian Xu, Cell Discovery. 111322025c</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>MOOSE-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, The Thirteenth International Conference on Learning Representations. 2025d</p>
<p>Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, arXiv:2412.17767Tao Feng, and Jiaxuan You. Researchtown: Simulator of human research community. 2024arXiv preprint</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou, arXiv:2501.039162025arXiv preprint</p>
<p>Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, arXiv:2503.24235A survey on test-time scaling in large language models: What, how, where, and how well?. 2025arXiv preprint</p>
<p>Chain of agents: Large language models collaborating on long-context tasks. Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, Sercan Arik, Advances in Neural Information Processing Systems. 202437</p>
<p>Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, arXiv:2408.06941Unleashing ai for accelerated scientific research. 2024arXiv preprint</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
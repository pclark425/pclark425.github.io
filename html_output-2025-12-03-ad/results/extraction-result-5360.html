<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5360 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5360</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5360</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-c39eeb0669c727ac606ec7fcb9f0136794739672</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c39eeb0669c727ac606ec7fcb9f0136794739672" target="_blank">NABU - Multilingual Graph-based Neural RDF Verbalizer</a></p>
                <p><strong>Paper Venue:</strong> International Workshop on the Semantic Web</p>
                <p><strong>Paper TL;DR:</strong> This work presents NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English that outperforms state-of-the-art approaches on English and achieves consistent results across all languages on the multilingual scenario.</p>
                <p><strong>Paper Abstract:</strong> The RDF-to-text task has recently gained substantial attention due to continuous growth of Linked Data. In contrast to traditional pipeline models, recent studies have focused on neural models, which are now able to convert a set of RDF triples into text in an end-to-end style with promising results. However, English is the only language widely targeted. We address this research gap by presenting NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English. NABU is based on an encoder-decoder architecture, uses an encoder inspired by Graph Attention Networks and a Transformer as decoder. Our approach relies on the fact that knowledge graphs are language-agnostic and they hence can be used to generate multilingual text. We evaluate NABU in monolingual and multilingual settings on standard benchmarking WebNLG datasets. Our results show that NABU outperforms state-of-the-art approaches on English with 66.21 BLEU, and achieves consistent results across all languages on the multilingual scenario with 56.04 BLEU.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5360.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5360.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NABU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NABU (Multilingual Graph-based Neural RDF Verbalizer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text neural model that encodes RDF knowledge graphs with a Graph Attention Network (GAT)-style encoder over a reified graph representation and decodes with a Transformer to produce multilingual verbalizations (English, German, Russian).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Reified graph encoding + GAT encoder, Transformer decoder (GAT-Trans)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>RDF triples are reified: predicates are converted into nodes and binary edges A0 (subject→predicate) and A1 (predicate→object) are created. The encoder receives node embeddings H, source-node embeddings S, destination-node embeddings D and label embeddings L; S and D are concatenated and projected to form an edge vector E, and the encoder input is H + L + E. A multi-head Graph Attention Network (GAT)-style encoder computes contextual node representations; a Transformer decoder (with BPE/unigram tokenization and optional copy mechanism) generates text. A special language token/node signals the target language for multilingual/bilingual training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (RDF triples / DBpedia subgraphs) with reified predicates</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Language-agnostic (KGs used as language-independent meaning representation); reification reduces parameter explosion by mapping predicates to nodes; encoder produces hidden states for relations (improves relation representation); supports arbitrary number of predicates; enables multilingual decoding via language token/node; improved handling of unseen entities via copy mechanism but still exhibits copying issues across alphabets; reification can reduce explicit discourse-order information.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>RDF-to-text (verbalization) on WebNLG datasets (monolingual, bilingual and multilingual settings over DBpedia-derived triples)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU, METEOR, chrF++; reported monolingual English BLEU=66.21, METEOR≈41.11-41.47, chrF++≈71.98; monolingual German BLEU=53.08 METEOR=37.42 chrF++=64.57; monolingual Russian BLEU=46.86 METEOR=28.84 chrF++=58.37; bilingual ENG-GER BLEU=61.99 METEOR=39.51 chrF++=69.68; bilingual ENG-RUS BLEU=49.15 METEOR=33.41 chrF++=64.00; multilingual (ENG+GER+RUS) BLEU=56.04 METEOR=38.34 chrF++=62.04.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to a Transformer baseline that ingests linearized triples, NABU (GAT-Trans) outperformed the baseline in most settings: +~11 BLEU on English vs. their Transformer baseline (baseline BLEU 54.96), and beat prior English state-of-the-art (other works) by large margins (paper reports NABU > previous SOTA by ≈15 BLEU in some comparisons). NABU outperforms the purely linearized-Transformer baseline on German and multilingual setups; on bilingual ENG-RUS NABU underperforms the baseline on BLEU/METEOR but beats it on chrF++. Authors attribute gains to better graph-structure modeling and reification; losses in discourse ordering relate to loss of explicit linear order from reification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reification harms discourse-order information (ordering errors when multiple identical predicates exist); confusion between similar predicates (dbo:artist vs dbo:producer) due to embedding similarity; tokenization / copying issues for unseen entities (especially cross-alphabet, e.g., Cyrillic Russian entities sometimes copied as English); German inflection/possessive generation errors; bilingual modeling across distant families (English vs Russian) shows inconsistent results and requires further features; need for improved embeddings and ordering/structuring modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5360.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5360.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer_baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer baseline with linearized triples input</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pure Transformer encoder-decoder model that takes linearized (serialized) RDF triples as an input sequence and generates text; used as a strong baseline in NABU experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / sequence serialization of RDF triples</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The set of RDF triples is linearized/serialized into a token sequence (ordered sequence of subject, predicate, object tokens, possibly with delimiters); this string is fed into a standard Transformer encoder which produces a sequence representation used by the Transformer decoder to generate the verbalization. A special language token is prepended for multilingual experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (RDF triples) serialized into a sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicitly encodes triple ordering (helps discourse ordering); compact sequence format compatible with standard seq2seq models; may lose explicit graph-structural inductive biases; easier to implement and benefits from powerful Transformer sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>RDF-to-text verbalization on WebNLG (monolingual, bilingual, multilingual)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU, METEOR, chrF++; reported baseline monolingual ENG BLEU=54.96 METEOR=38.43 chrF++=69.11; GER BLEU=50.07 METEOR=34.51 chrF++=63.48; RUS BLEU=46.42 METEOR=27.74 chrF++=56.80; bilingual ENG-GER BLEU=58.30 METEOR=36.46 chrF++=66.72; bilingual ENG-RUS BLEU=55.30 METEOR=37.90 chrF++=61.63; multilingual ENG-GER-RUS BLEU=53.39 METEOR=36.86 chrF++=60.72.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Performs well at discourse ordering due to explicit linearization; NABU (graph-based) outperforms this baseline in many settings (especially English monolingual and multilingual), but the baseline sometimes surpasses NABU on ordering-sensitive aspects and on some bilingual (ENG-RUS) BLEU/METEOR scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Linearization hides graph structure (less explicit relational inductive bias), potentially limiting modeling of graph-local context; ordering must be provided in the serialization (which can be brittle); may struggle to generalize structure across graphs compared to graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5360.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5360.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reification strategy for RDF graph encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation strategy that maps predicates (relations) to explicit nodes and connects subject→predicate and predicate→object via binary edges (A0/A1), enabling models to produce embeddings for relations and avoid parameter explosion when encoding many predicates as label parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep graph convolutional encoders for structured data to text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Reified RDF graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert each triple (s, p, o) to two binary relations by introducing a predicate node p: create (s, A0, p) and (p, A1, o). This changes edge-labeled graphs into node-labeled graphs where relation instances are explicit nodes, allowing encoders to allocate embeddings/hidden states to predicates as first-class nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (RDF) transformed to node-centric graph via reification</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Alleviates parameter explosion in edge-label encoding; yields hidden states for relations (better relation modeling); allows an arbitrary number of predicates to be represented uniformly; can obscure original triple linear order (affecting discourse ordering); increases node count.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as encoder input format for graph neural encoders (NABU) in RDF-to-text WebNLG experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Indirectly evaluated as part of NABU; enabled NABU to achieve high BLEU (e.g., ENG BLEU=66.21) versus baselines. No standalone numeric metric for reification alone reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared implicitly with edge-labeled graph encodings (parameterized predicate labels) which suffer parameter explosion; reification was favored by NABU and prior GCN work [31] for scalability and relation representation. However, reification can worsen discourse ordering relative to linearized inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reduces explicit discourse-order information leading to ordering errors (e.g., swap of subjects for identical predicates); increases graph node count which may affect encoder complexity; may require additional mechanisms to recover or model linearization/discourse ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5360.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5360.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Network (GAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph neural network that uses self-attention to compute dynamic, normalized attention coefficients over a node's neighbors, enabling the model to weight neighbors differently when aggregating representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph attention networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GAT-based graph encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node's new representation is computed as an attention-weighted sum of neighbor feature vectors, where attention coefficients e_ij = a(h_i, h_j) are computed with a learned attention mechanism and normalized via softmax over neighbors. Multi-head attention heads are used and concatenated/aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs / knowledge graphs (nodes with features, neighborhood aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Dynamic neighbor weighting (better focus on important neighbors), preserves graph connectivity, alleviates equal-weight aggregation issues from GCNs, but can lead to parameter growth for large label sets; supports multi-head attention and improved expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as the encoder in NABU for RDF-to-text (WebNLG) generation; referenced as a graph encoder in graph-to-text literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used inside NABU which achieved high downstream metrics (ENG BLEU=66.21 etc.). No isolated GAT-only numeric ablation is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Cited as improving over GCN's uniform aggregation by introducing attention weights; NABU uses a GAT-inspired encoder and compares favorably (end-to-end) to Transformer baseline and prior GCN-based approaches (Marcheggiani & Perez) in English and multilingual settings. Paper notes potential parameter explosion depending on graph size, mitigated here via reification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Parameter explosion for large relation vocabularies or large graphs if every distinct edge label is parameterized; computational cost; in NABU reification was necessary to mitigate parameter growth. Also possible confusion when predicates/entities have similar embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5360.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5360.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN-based encoder (Marcheggiani & Perez)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep graph convolutional encoders for structured data to text generation (GCN encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Graph Convolutional Network (GCN) based encoder that directly uses the graph structure of RDF input to compute node representations for text generation, shown to outperform LSTM-based encoders on the WebNLG task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep graph convolutional encoders for structured data to text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GCN graph encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Applies graph convolution layers to aggregate normalized sums of neighbor transformed features (h'_i = sigma(sum_{j in N_i} (1/c_ij) g_j)), producing node representations that encode local graph structure; used to represent RDF graphs for downstream decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (RDF triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Respects graph topology and local neighborhood structure; parameter efficient per-layer; may not differentiate neighbor importance well (uses normalization constants rather than learned attention), potentially leading to equal-weight aggregation problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>RDF-to-text generation on WebNLG; used as a baseline in literature comparing graph encoders to sequence encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in cited work to outperform LSTM-based models on WebNLG; NABU cites that GCN-based approaches showed improvements over LSTM models. NABU does not provide GCN numeric values directly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GCN approaches were found by prior work to outperform RNN/GRU encoders on WebNLG; GAT introduces attention to address GCN's equal-weight aggregation limitation. NABU opts for a GAT-inspired encoder (with reification) and shows further gains compared to Transformer baselines and prior approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lacks learned neighbor importance (addressed by GAT), may underperform when certain neighbors are more relevant than others; parameterization choices and graph pre-processing (e.g., reification) can affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5360.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5360.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTR-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GTR-LSTM (Triple encoder for sentence generation from RDF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A triple-level encoder architecture that captures both intra-triple and inter-triple relationships to produce representations for sentence generation from RDF data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gtr-lstm: A triple encoder for sentence generation from rdf data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple-encoder (GTR-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes each triple with mechanisms to capture relationships within a triple and between triples; uses LSTM-based components to aggregate triple-level and global graph information for sentence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (sets of RDF triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Captures triple-local and global inter-triple context; relies on recurrent architectures (LSTM) for sequential aggregation of triple information; may be less parallelizable than Transformer-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>RDF-to-text generation on WebNLG (cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in prior work (citation) to be competitive in graph-to-text tasks; NABU cites GTR-LSTM as part of recent graph-based approaches but does not report exact comparative numbers from that paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared in the literature with GCN and Transformer/sequence baselines; GTR-LSTM captures global KG information but newer architectures (GCN/GAT + Transformer) show strong performance and better parallelism.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RNN/LSTM components can limit parallel training and may be less effective than attention-based models for capturing long-range dependencies across many triples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Gtr-lstm: A triple encoder for sentence generation from rdf data <em>(Rating: 2)</em></li>
                <li>Graph attention networks <em>(Rating: 2)</em></li>
                <li>Modeling global and local node contexts for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Attention is all you need <em>(Rating: 1)</em></li>
                <li>The webnlg challenge: Generating text from dbpedia data <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5360",
    "paper_id": "paper-c39eeb0669c727ac606ec7fcb9f0136794739672",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "NABU",
            "name_full": "NABU (Multilingual Graph-based Neural RDF Verbalizer)",
            "brief_description": "A graph-to-text neural model that encodes RDF knowledge graphs with a Graph Attention Network (GAT)-style encoder over a reified graph representation and decodes with a Transformer to produce multilingual verbalizations (English, German, Russian).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Reified graph encoding + GAT encoder, Transformer decoder (GAT-Trans)",
            "representation_description": "RDF triples are reified: predicates are converted into nodes and binary edges A0 (subject→predicate) and A1 (predicate→object) are created. The encoder receives node embeddings H, source-node embeddings S, destination-node embeddings D and label embeddings L; S and D are concatenated and projected to form an edge vector E, and the encoder input is H + L + E. A multi-head Graph Attention Network (GAT)-style encoder computes contextual node representations; a Transformer decoder (with BPE/unigram tokenization and optional copy mechanism) generates text. A special language token/node signals the target language for multilingual/bilingual training.",
            "graph_type": "Knowledge graph (RDF triples / DBpedia subgraphs) with reified predicates",
            "representation_properties": "Language-agnostic (KGs used as language-independent meaning representation); reification reduces parameter explosion by mapping predicates to nodes; encoder produces hidden states for relations (improves relation representation); supports arbitrary number of predicates; enables multilingual decoding via language token/node; improved handling of unseen entities via copy mechanism but still exhibits copying issues across alphabets; reification can reduce explicit discourse-order information.",
            "evaluation_task": "RDF-to-text (verbalization) on WebNLG datasets (monolingual, bilingual and multilingual settings over DBpedia-derived triples)",
            "performance_metrics": "BLEU, METEOR, chrF++; reported monolingual English BLEU=66.21, METEOR≈41.11-41.47, chrF++≈71.98; monolingual German BLEU=53.08 METEOR=37.42 chrF++=64.57; monolingual Russian BLEU=46.86 METEOR=28.84 chrF++=58.37; bilingual ENG-GER BLEU=61.99 METEOR=39.51 chrF++=69.68; bilingual ENG-RUS BLEU=49.15 METEOR=33.41 chrF++=64.00; multilingual (ENG+GER+RUS) BLEU=56.04 METEOR=38.34 chrF++=62.04.",
            "comparison_to_other_representations": "Compared to a Transformer baseline that ingests linearized triples, NABU (GAT-Trans) outperformed the baseline in most settings: +~11 BLEU on English vs. their Transformer baseline (baseline BLEU 54.96), and beat prior English state-of-the-art (other works) by large margins (paper reports NABU &gt; previous SOTA by ≈15 BLEU in some comparisons). NABU outperforms the purely linearized-Transformer baseline on German and multilingual setups; on bilingual ENG-RUS NABU underperforms the baseline on BLEU/METEOR but beats it on chrF++. Authors attribute gains to better graph-structure modeling and reification; losses in discourse ordering relate to loss of explicit linear order from reification.",
            "limitations_or_challenges": "Reification harms discourse-order information (ordering errors when multiple identical predicates exist); confusion between similar predicates (dbo:artist vs dbo:producer) due to embedding similarity; tokenization / copying issues for unseen entities (especially cross-alphabet, e.g., Cyrillic Russian entities sometimes copied as English); German inflection/possessive generation errors; bilingual modeling across distant families (English vs Russian) shows inconsistent results and requires further features; need for improved embeddings and ordering/structuring modeling.",
            "uuid": "e5360.0",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Transformer_baseline",
            "name_full": "Transformer baseline with linearized triples input",
            "brief_description": "A pure Transformer encoder-decoder model that takes linearized (serialized) RDF triples as an input sequence and generates text; used as a strong baseline in NABU experiments.",
            "citation_title": "Attention is all you need",
            "mention_or_use": "use",
            "representation_name": "Linearization / sequence serialization of RDF triples",
            "representation_description": "The set of RDF triples is linearized/serialized into a token sequence (ordered sequence of subject, predicate, object tokens, possibly with delimiters); this string is fed into a standard Transformer encoder which produces a sequence representation used by the Transformer decoder to generate the verbalization. A special language token is prepended for multilingual experiments.",
            "graph_type": "Knowledge graph (RDF triples) serialized into a sequence",
            "representation_properties": "Explicitly encodes triple ordering (helps discourse ordering); compact sequence format compatible with standard seq2seq models; may lose explicit graph-structural inductive biases; easier to implement and benefits from powerful Transformer sequence modeling.",
            "evaluation_task": "RDF-to-text verbalization on WebNLG (monolingual, bilingual, multilingual)",
            "performance_metrics": "BLEU, METEOR, chrF++; reported baseline monolingual ENG BLEU=54.96 METEOR=38.43 chrF++=69.11; GER BLEU=50.07 METEOR=34.51 chrF++=63.48; RUS BLEU=46.42 METEOR=27.74 chrF++=56.80; bilingual ENG-GER BLEU=58.30 METEOR=36.46 chrF++=66.72; bilingual ENG-RUS BLEU=55.30 METEOR=37.90 chrF++=61.63; multilingual ENG-GER-RUS BLEU=53.39 METEOR=36.86 chrF++=60.72.",
            "comparison_to_other_representations": "Performs well at discourse ordering due to explicit linearization; NABU (graph-based) outperforms this baseline in many settings (especially English monolingual and multilingual), but the baseline sometimes surpasses NABU on ordering-sensitive aspects and on some bilingual (ENG-RUS) BLEU/METEOR scores.",
            "limitations_or_challenges": "Linearization hides graph structure (less explicit relational inductive bias), potentially limiting modeling of graph-local context; ordering must be provided in the serialization (which can be brittle); may struggle to generalize structure across graphs compared to graph encoders.",
            "uuid": "e5360.1",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Reification",
            "name_full": "Reification strategy for RDF graph encoding",
            "brief_description": "A representation strategy that maps predicates (relations) to explicit nodes and connects subject→predicate and predicate→object via binary edges (A0/A1), enabling models to produce embeddings for relations and avoid parameter explosion when encoding many predicates as label parameters.",
            "citation_title": "Deep graph convolutional encoders for structured data to text generation",
            "mention_or_use": "use",
            "representation_name": "Reified RDF graph",
            "representation_description": "Convert each triple (s, p, o) to two binary relations by introducing a predicate node p: create (s, A0, p) and (p, A1, o). This changes edge-labeled graphs into node-labeled graphs where relation instances are explicit nodes, allowing encoders to allocate embeddings/hidden states to predicates as first-class nodes.",
            "graph_type": "Knowledge graph (RDF) transformed to node-centric graph via reification",
            "representation_properties": "Alleviates parameter explosion in edge-label encoding; yields hidden states for relations (better relation modeling); allows an arbitrary number of predicates to be represented uniformly; can obscure original triple linear order (affecting discourse ordering); increases node count.",
            "evaluation_task": "Used as encoder input format for graph neural encoders (NABU) in RDF-to-text WebNLG experiments",
            "performance_metrics": "Indirectly evaluated as part of NABU; enabled NABU to achieve high BLEU (e.g., ENG BLEU=66.21) versus baselines. No standalone numeric metric for reification alone reported.",
            "comparison_to_other_representations": "Compared implicitly with edge-labeled graph encodings (parameterized predicate labels) which suffer parameter explosion; reification was favored by NABU and prior GCN work [31] for scalability and relation representation. However, reification can worsen discourse ordering relative to linearized inputs.",
            "limitations_or_challenges": "Reduces explicit discourse-order information leading to ordering errors (e.g., swap of subjects for identical predicates); increases graph node count which may affect encoder complexity; may require additional mechanisms to recover or model linearization/discourse ordering.",
            "uuid": "e5360.2",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "GAT",
            "name_full": "Graph Attention Network (GAT)",
            "brief_description": "A graph neural network that uses self-attention to compute dynamic, normalized attention coefficients over a node's neighbors, enabling the model to weight neighbors differently when aggregating representations.",
            "citation_title": "Graph attention networks",
            "mention_or_use": "use",
            "representation_name": "GAT-based graph encoding",
            "representation_description": "Each node's new representation is computed as an attention-weighted sum of neighbor feature vectors, where attention coefficients e_ij = a(h_i, h_j) are computed with a learned attention mechanism and normalized via softmax over neighbors. Multi-head attention heads are used and concatenated/aggregated.",
            "graph_type": "General graphs / knowledge graphs (nodes with features, neighborhood aggregation)",
            "representation_properties": "Dynamic neighbor weighting (better focus on important neighbors), preserves graph connectivity, alleviates equal-weight aggregation issues from GCNs, but can lead to parameter growth for large label sets; supports multi-head attention and improved expressivity.",
            "evaluation_task": "Used as the encoder in NABU for RDF-to-text (WebNLG) generation; referenced as a graph encoder in graph-to-text literature.",
            "performance_metrics": "Used inside NABU which achieved high downstream metrics (ENG BLEU=66.21 etc.). No isolated GAT-only numeric ablation is reported in this paper.",
            "comparison_to_other_representations": "Cited as improving over GCN's uniform aggregation by introducing attention weights; NABU uses a GAT-inspired encoder and compares favorably (end-to-end) to Transformer baseline and prior GCN-based approaches (Marcheggiani & Perez) in English and multilingual settings. Paper notes potential parameter explosion depending on graph size, mitigated here via reification.",
            "limitations_or_challenges": "Parameter explosion for large relation vocabularies or large graphs if every distinct edge label is parameterized; computational cost; in NABU reification was necessary to mitigate parameter growth. Also possible confusion when predicates/entities have similar embeddings.",
            "uuid": "e5360.3",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "GCN-based encoder (Marcheggiani & Perez)",
            "name_full": "Deep graph convolutional encoders for structured data to text generation (GCN encoder)",
            "brief_description": "A Graph Convolutional Network (GCN) based encoder that directly uses the graph structure of RDF input to compute node representations for text generation, shown to outperform LSTM-based encoders on the WebNLG task.",
            "citation_title": "Deep graph convolutional encoders for structured data to text generation",
            "mention_or_use": "mention",
            "representation_name": "GCN graph encoding",
            "representation_description": "Applies graph convolution layers to aggregate normalized sums of neighbor transformed features (h'_i = sigma(sum_{j in N_i} (1/c_ij) g_j)), producing node representations that encode local graph structure; used to represent RDF graphs for downstream decoders.",
            "graph_type": "Knowledge graph (RDF triples)",
            "representation_properties": "Respects graph topology and local neighborhood structure; parameter efficient per-layer; may not differentiate neighbor importance well (uses normalization constants rather than learned attention), potentially leading to equal-weight aggregation problems.",
            "evaluation_task": "RDF-to-text generation on WebNLG; used as a baseline in literature comparing graph encoders to sequence encoders.",
            "performance_metrics": "Reported in cited work to outperform LSTM-based models on WebNLG; NABU cites that GCN-based approaches showed improvements over LSTM models. NABU does not provide GCN numeric values directly.",
            "comparison_to_other_representations": "GCN approaches were found by prior work to outperform RNN/GRU encoders on WebNLG; GAT introduces attention to address GCN's equal-weight aggregation limitation. NABU opts for a GAT-inspired encoder (with reification) and shows further gains compared to Transformer baselines and prior approaches.",
            "limitations_or_challenges": "Lacks learned neighbor importance (addressed by GAT), may underperform when certain neighbors are more relevant than others; parameterization choices and graph pre-processing (e.g., reification) can affect performance.",
            "uuid": "e5360.4",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "GTR-LSTM",
            "name_full": "GTR-LSTM (Triple encoder for sentence generation from RDF)",
            "brief_description": "A triple-level encoder architecture that captures both intra-triple and inter-triple relationships to produce representations for sentence generation from RDF data.",
            "citation_title": "Gtr-lstm: A triple encoder for sentence generation from rdf data",
            "mention_or_use": "mention",
            "representation_name": "Triple-encoder (GTR-LSTM)",
            "representation_description": "Encodes each triple with mechanisms to capture relationships within a triple and between triples; uses LSTM-based components to aggregate triple-level and global graph information for sentence generation.",
            "graph_type": "Knowledge graph (sets of RDF triples)",
            "representation_properties": "Captures triple-local and global inter-triple context; relies on recurrent architectures (LSTM) for sequential aggregation of triple information; may be less parallelizable than Transformer-based approaches.",
            "evaluation_task": "RDF-to-text generation on WebNLG (cited prior work)",
            "performance_metrics": "Reported in prior work (citation) to be competitive in graph-to-text tasks; NABU cites GTR-LSTM as part of recent graph-based approaches but does not report exact comparative numbers from that paper.",
            "comparison_to_other_representations": "Compared in the literature with GCN and Transformer/sequence baselines; GTR-LSTM captures global KG information but newer architectures (GCN/GAT + Transformer) show strong performance and better parallelism.",
            "limitations_or_challenges": "RNN/LSTM components can limit parallel training and may be less effective than attention-based models for capturing long-range dependencies across many triples.",
            "uuid": "e5360.5",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2
        },
        {
            "paper_title": "Gtr-lstm: A triple encoder for sentence generation from rdf data",
            "rating": 2
        },
        {
            "paper_title": "Graph attention networks",
            "rating": 2
        },
        {
            "paper_title": "Modeling global and local node contexts for text generation from knowledge graphs",
            "rating": 2
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 1
        },
        {
            "paper_title": "The webnlg challenge: Generating text from dbpedia data",
            "rating": 2
        }
    ],
    "cost": 0.013244249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NABU - Multilingual Graph-based Neural RDF Verbalizer</h1>
<p>Diego Moussallem ${ }^{1 * \dagger}$, Dwaraknath Gnaneshwar ${ }^{2 * \dagger}$, Thiago Castro Ferreira<br>$\otimes^{3,4 \dagger}$, and Axel-Cyrille Ngonga Ngomo ${ }^{1}$<br>${ }^{1}$ Data Science Group, University of Paderborn, Germany<br>first.lastname@upb.de<br>${ }^{2}$ DL group, Manipal Institute of Technology, India<br>dwarakasharma@gmail.com<br>${ }^{3}$ Federal University of Minas Gerais (UFMG), Brazil<br>${ }^{4}$ Tilburg center for Cognition and Communication (TiCC)<br>Tilburg University, The Netherlands<br>tcastrof@tilburguniversity.edu</p>
<h4>Abstract</h4>
<p>The RDF-to-text task has recently gained substantial attention due to continuous growth of Linked Data. In contrast to traditional pipeline models, recent studies have focused on neural models, which are now able to convert a set of RDF triples into text in an end-to-end style with promising results. However, English is the only language widely targeted. We address this research gap by presenting NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English. NABU is based on an encoder-decoder architecture, uses an encoder inspired by Graph Attention Networks and a Transformer as decoder. Our approach relies on the fact that knowledge graphs are language-agnostic and they hence can be used to generate multilingual text. We evaluate NABU in monolingual and multilingual settings on standard benchmarking WebNLG datasets. Our results show that NABU outperforms state-of-the-art approaches on English with 66.21 BLEU, and achieves consistent results across all languages on the multilingual scenario with 56.04 BLEU.</p>
<p>Keywords: Knowledge Graphs $\cdot$ Natural Language Generation $\cdot$ Semantic Web.</p>
<h2>1 Introduction</h2>
<p>Natural Language Generation (NLG) is the process of generating coherent natural language text from non-linguistic data [40]. Despite community agreement on the text and speech output of these systems, there is far less consensus on what the input should be [22]. A large number of inputs have hence been employed for NLG systems, including images [50], numeric data [24], and Semantic Web (SW) data [36]. Practical applications can be found in domains such as weather forecasts [32], feedback for car drivers [8], diet management [1].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Presently, the generation of natural language from Resource Description Framework (RDF) data has gained substantial attention [7]. The RDF-to-text task has hence been proposed to investigate the quality of automatically generated texts from RDF Knowledge Graphs (KGs) [11]. With the emergence of neural methods, end-to-end data-to-text models have been introduced to learn input-output mappings directly. These approaches rely much less on explicit intermediate representations compared to rulebased approaches [23].</p>
<p>Although Neural NLG models have been achieving very good results [20], English is the only language that has been widely targeted. In this work, we alleviate this language limitation by proposing a multilingual approach, named NABU. The motivation behind multilingual models lies in several directions, mainly in (1) transfer learning; when low-resource language pairs are trained together with high-resource languages, the translation quality improves; (2) zero-shot translation, where multilingual models are able to translate between language pairs from similar families that were never seen during training; (3) Easy deploy, a multilingual model achieving same performance on many languages in comparison to several separate language-specific models are much more desirable for companies in terms of deployment [26].</p>
<p>Our approach, NABU, is based on the fact that knowledge graphs are languageagnostic and hence can be used on the encoder side to generate multilingual text. NABU consists of an encoder-decoder architecture which incorporates structural information of RDF triples using an encoding mechanism inspired by Graph Attention Network (GAT) [49]. In contrast to recent related work [41], NABU relies on the use of a reification strategy for modeling the graph structure of RDF input. The decoder part is based on the vanilla Transformer model [48] along with an unsupervised tokenization model.</p>
<p>We evaluate NABU on the standard benchmarking WebNLG datasets[19] in three settings: monolingual, bilingual and multilingual. For the monolingual setting, we compare NABU with state-of-the-art English approaches and also perform experiments on Russian and German. The goal of the bilingual setting is to analyze the performance of NABU for language families. To achieve this goal, we train and evaluate bilingual models using NABU on English-German and on English-Russian. In the multilingual setting, we compare NABU with a multilingual Transformer model on English, German and Russian. Our results show that NABU outperforms state-of-the-art approaches on English and achieves 66.21 BLEU. NABU also achieves consistent results across all languages on multilingual settings with 56.04 BLEU. In addition, NABU presents promising results on the bilingual models with 61.99 BLEU. Our findings suggest that NABU is able to generate multilingual text with similar quality to that generated by humans. The main contributions of this paper can be summarized as follows:</p>
<ul>
<li>We present a novel approach dubbed NABU based on a GAT-Transformer architecture for generating multilingual text from RDF KGs.</li>
<li>NABU outperforms English state-of-the-art approaches with consistent average improvements of +10 BLEU, METEOR and chrF3 on the WebNLG datasets.</li>
<li>NABU exploits the benefits of modeling of language families in the generation task.</li>
</ul>
<p>The version of NABU used in this paper and also all experimental data are publicly available. ${ }^{5}$.</p>
<h1>2 Related Work</h1>
<p>A significant body of research has investigated the generation of Natural Language (NL) texts from RDF data. A plenty of research is based on template- and rule-based approaches such as $[10,14,15,36]$. Recently, the WebNLG [11] challenge made this research area more prominent by providing a benchmark corpus of English texts verbalizing RDF triples in 15 different semantic domains. Among the participating models, the works based on sequence-to-sequence Neural Networks (NNs) achieved some of the best results [45,34]. Moreover, RDF has also been showing promising benefits to the generation of benchmarks for evaluating NLG systems [35].</p>
<p>The choice of neural architectures for RDF-to-text has evolved constantly along the last couple of years. All end-to-end models submitted to the WebNLG challenge [20] received the set of triples in a linearized form as input. However, researchers have recently been experimenting with graph-based approaches, which take the RDF input formatted as a graph, with promising results. Marcheggiane and Perez [31] proposed a structured data encoder based on Graph Convolutional Network (GCN) that directly exploits the graph structure and presented better results than Long Short-Term Memories (LSTM) models. Distiawan et al. [13] presented a GTR-LSTM architecture which captures the global information of a KG by encoding the relationships both within a triple and between the triples. Ferreira et al. [17] introduced a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Although Marcheggiane and Perez [31] showed that the linearisation of the input graph has several drawbacks, the authors implemented Gated recurrent unit (GRU) and Transformer architectures which showed results superior to those of the former architecture. Recently, Ribeiro et.al [41] devised an unified graph attention network structure which investigates graph-to-text architectures that combined global and local graph representations to improve fluency in text generation. Their experiments demonstrated significant improvements on seen categories in the WebNLG dataset.</p>
<p>Despite the plethora of graph-based neural approaches on handling RDF data, English is the only language which has been widely targeted. Recent efforts were made to create German and Russian language versions of WebNLG [16,44]. However, no work that investigates these languages has been published at the time of writing. To the best of our knowledge, NABU is hence the first approach which tackles multilinguality in the RDF-to-text task.</p>
<h2>3 The NABU Approach</h2>
<p>NABU tackles RDF-to-text based on the formal description of a translation problem. The RDF-to-text task takes an RDF graph as input and generates an output text which</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Example of a set of triples (left) and the corresponding verbalization (right).
reflects its meaning. Figure 1 depicts an example of a set of 3 RDF triples and the corresponding text. Therefore, the underlying idea behind our approach is as follows: Given that KGs are language-agnostic and represent facts often extracted from text, we can regard the facts (i.e., RDF triples) as sentences and train a model to translate the facts from a language-agnostic graph representation to several languages. In the following, we give an overview of GAT architecture and Transformer. Thereafter, we present NABU in detail. Throughout the description of our methodology and our experiments, we use DBpedia [2] as reference Knowledge Base (KB) since the benchmarking datasets are based on this KB.</p>
<h1>3.1 Background</h1>
<p>Transformer Transformer-based models consist of an encoder and a decoder, i.e., a two-tier architecture where the encoder reads an input sequence $x=\left(x_{1}, \ldots, x_{n}\right)$ and the decoder predicts a target sequence $y=\left(y_{1}, \ldots, y_{n}\right)$. The encoder and decoder interact via a soft-attention mechanism [3]30, which comprises one or multiple attention layers. We follow the notations from Tang et al. 47] in the subsequent sections: Let $m$ stand for the word embedding size and $n$ for the number of hidden units. Further, let $K$ be the vocabulary size of the source language. Then, $h_{i}^{l}$ corresponds to the hidden state at step $i$ of layer $l . h_{i-1}^{l}$ represents the hidden state at the previous step of layer $l$ while $h_{i}^{l-1}$ means the hidden state at $i$ of layer $l-1 . E \in \mathbb{R}^{m \times K}$ is a word embedding matrix, $W \in \mathbb{R}^{n \times m}, U \in \mathbb{R}^{n \times n}$ are weight matrices, $E_{x_{i}}$ refers to the embedding of $x_{i}$, and $e_{p o s, i}$ indicates a positional embedding at position $i$.</p>
<p>Transformer models rely deeply on self-attention networks. Each token is connected to every other token in the same sentence directly via self-attention. Thus, the path length between any two tokens is 1 . Due to lack of recurrence found in Recurrent Neural Network (RNN), Transformers implement positional encoding to input and output. Additionally, these models rely on multi-head attention to feature attention networks, which are more complex in comparison to the 1-head attention mechanism used in RNNs. In contrast to RNN, the positional information is also preserved in positional embeddings. Equation 1 describes the hidden state $h_{i}^{l}$, which is calculated from all hidden states of the previous layer. $f$ represents a feed-forward network with the rectified</p>
<p>linear unit (ReLU) as the activation function and layer normalization. The first layer is implemented as $h_{i}^{0}=W E_{x_{i}}+e_{p o s, i}$. Moreover, the decoder has a multi-head attention over the encoder's hidden states:</p>
<p>$$
h_{i}^{l}=h_{i}^{l-1}+f\left(\text { self-attention }\left(h_{i}^{l-1}\right)\right)
$$</p>
<p>Graph Attention Networks Deep Learning on non-euclidean data has recently gained substantial research interest due to the abundance of its availability. A plethora of problems can be solved efficiently by representing data in a data structure that can utilize the inherent structure and inter-entity relationships. Kipf and Welling [28] introduced GCN, through which they generalize the convolution operation of Convolutional Neural Network (CNN) to graph structures. Every layer in a GCN has a weight matrix $W$ that transforms nodes feature vectors from a low-dimensional representation space to high-dimensional representation space, which aims to preserve the structure of the graph.</p>
<p>Consider a graph of $z$ nodes and a set of node features $\left(\boldsymbol{h}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \boldsymbol{h}</em>}}, . ., \boldsymbol{h<em _mathbf_1="\mathbf{1">{\mathbf{z}}\right)$. A GCN layer computes a net set of features $\left(\boldsymbol{h}</em>}}^{\prime}, \boldsymbol{h<em _mathbf_z="\mathbf{z">{\mathbf{2}}^{\prime}, . ., \boldsymbol{h}</em>$ to stabilize the update rule. Finally,}}^{\prime}\right)$. First the feature matrix is multiplied with $W \boldsymbol{g}=W \boldsymbol{h}$. Then, the aggregated sum of node features are normalized using normalization constant $\frac{1}{c_{i j}</p>
<p>$$
\boldsymbol{h}<em N__i="N_{i" _epsilon="\epsilon" j="j">{\boldsymbol{i}}^{\prime}=\sigma\left(\sum</em>\right)
$$}} \frac{1}{c_{i j}} \boldsymbol{g}_{j</p>
<p>However, the convolution operation in GCN does not take into account the fact that some nodes are more important than others to generate a particular segment of the target sentence. To alleviate this problem, Velickovic et al. [49] devised GAT, which converts the normalization constant into dynamic attention coefficients. The attention coefficients are calculated by applying self-attention over node features. In one forward pass, a GAT layer calculate a score of a given node that quantifies the importance of neighbors to its representation:</p>
<p>$$
e_{i j}=a\left(\boldsymbol{h}<em _boldsymbol_j="\boldsymbol{j">{\boldsymbol{i}}, \boldsymbol{h}</em>\right)
$$}</p>
<p>The attention scores are then normalized using softmax:</p>
<p>$$
\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k \epsilon N_{i}} \exp \left(e_{i k}\right)}
$$</p>
<h1>3.2 Approach</h1>
<p>Graph-based NNs have been used successfully to parse and support the generation of natural-language sentences from RDF KG. Although GAT models have shown to alleviate the loss of node information, the network still suffers from parameter explosion depending on the size of the graph structure [5]. To alleviate the parameters explosion problem, we follow the same strategy used in [31], named reification, ${ }^{6}$ to slightly</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>modify how the RDF graph is encoded. We describe below how reification is applied. Afterward, we explain the encoder and decoder parts of NABU. An overview of NABU architecture after reification can be found in Figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: NABU architecture</p>
<p>Reification. RDF triples are represented as a graph in which (i) the subjects and objects are nodes and (ii) predicates (relationships) between them are labeled edges. For example, <Albert_Einstein, birthPlace, Germany> can be seen as a sub-KG in DBpedia where Albert_ Einstein and Germany are the nodes and birthPlace is the edge. However, the edges are encoded as parameters by the GAT, and the parameters explosion problem stated by Beck et al. [5] often occurs.</p>
<p>Therefore, we follow the reification strategy, which maps the relations to nodes in the KG and creates new binary relations for each relation in the RDF triples. We rely on two binary relations, which model the relationship between the subject and predicate (A0) and predicate and object (A1) only. For example, 〈Albert_Einstein, birthPlace, Germany〉 becomes 〈Albert_Einstein, A0, birthPlace〉 and〈birthPlace, A1, Germany〉. Apart from handling the parameter explosion problem, reification is useful in two ways. First, the encoder generates a hidden state for each relation in the input. Second, it allows for modeling an arbitrary number of edges (predicates) efficiently. Figure 3 illustrates the reification strategy for our example.</p>
<p>Encoder. Here, the reified graph is sent as input to the GAT that applies a self-attention mechanism to compute the importance of each node in the graph. The GAT encoder represents nodes in a high-dimensional vector space whilst taking into account the representations of their neighbors. Note that NABU follows the same strategy of recent literature on multilingual Neural Machine Translation (NMT) models in which a special token is used in the encoder to determine to what target language to translate [46]. Figure 4 shows how a single forward step/pass works in NABU approach.</p>
<p>In one forward pass of our model, we have four dense vectors as inputs, namely (i) the node vector $\boldsymbol{H}=\left(\boldsymbol{h}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \boldsymbol{h}</em>}}, . ., \boldsymbol{h<em _mathbf_1="\mathbf{1">{\mathbf{z}}\right)$ with embeddings of all nodes in the graphs, (ii) the source vector, $\boldsymbol{S}=\left(s</em>\right)$ with embeddings of source nodes in edges}}, s_{\mathbf{2}}, . ., s_{\mathbf{z}</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Reification used on our example.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: An overview of a single forward pass in NABU.
of the graph, (iii) the destination vector, $\boldsymbol{D}=\left(\boldsymbol{d}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \boldsymbol{d}</em>}}, \ldots, \boldsymbol{d<em _mathbf_1="\mathbf{1">{\mathbf{z}}\right)$ with embeddings of target nodes in edges of the graphs and (iv) the label vector, $\boldsymbol{L}=\left(\boldsymbol{l}</em>}}, \boldsymbol{l<em _mathbf_z="\mathbf{z">{\mathbf{2}}, \ldots, \boldsymbol{l}</em>)$ to form the input vector to our encoder:}}\right)$ with embedding labels. The source $\boldsymbol{S}$ and destination $\boldsymbol{D}$ vectors are concatenated and are passed through dense layer which encodes them into a vector of the same shape as the label vector. We call this new vector the edge vector, $\boldsymbol{E}$. We then add the edge vector $(\boldsymbol{E})$, node vector $(\boldsymbol{H})$ and label vector $(\boldsymbol{L</p>
<p>$$
\begin{gathered}
\boldsymbol{E}=f(\boldsymbol{S}, \boldsymbol{D}), \text { and } \
\boldsymbol{H}^{\prime}=|_{h \epsilon \eta} G(\boldsymbol{H}+\boldsymbol{L}+\boldsymbol{E})
\end{gathered}
$$</p>
<p>where $\eta$ is the number of heads in the multi-head attention layer.</p>
<p>Decoder Our decoder follows the standard architecture of the Transformer decoder, which takes into account the intermediate representation generated by the encoder. The decoder gives a probability distribution over the target language's vocabulary. We also</p>
<p>rely on an unsupervised tokenizer, which implements Byte Pair Encoding (BPE) [43] and unigram language model [29] for handling multilinguality and out-of-vocabulary words. Afterward, we apply a beam search for selecting the most likely word in the output sentence.</p>
<h1>4 Evaluation</h1>
<h3>4.1 Goals</h3>
<p>In our evaluation, we address the following research questions:
Q1: How does our multilingual approach compare with state-of-the-art results in English?
Q2: Is NABU able to generate bilingual text while modelling two languages from distinct families?
Q3: How accurate are the multilingual texts generated by NABU?
We designed our evaluation as follows: First, we measured the performance of NABU on English by using the WebNLG dataset and compared it with state-of-theart approaches. Additionally, we evaluated NABU on two other languages-German and Russian. Second, we evaluated NABU on bilingual models -English-German and English-Russian. Third, we combined all three languages in a multilingual setting and compared it with a multilingual Transformer baseline model. For measuring the quality of our approach, we used the automatic evaluation metrics BLEU, METEOR, and CHRF++ .</p>
<h3>4.2 Data</h3>
<p>The experiments presented in this work were conducted on the WebNLG corpus [18,21], which consists of sets of RDF triples mapped to target texts. In comparison with other popular NLG benchmarks [6,37,33], WebNLG is the most semantically varied corpus. Its English version contains 25,298 texts which describe 9,674 sets of up to 7 RDF triples in 15 domains: Astronaut, University, Monument, Building, Comics Character, Food, Airport, Sports Team, Written Work, City, Athlete, Artist, Means of Transportation, Celestial Body and Politician. Out of these domains, five ((Athlete, Artist, MeanOfTransportation, CelestialBody, Politician)) are exclusively present in the test set, being unseen during the training and validation processes.</p>
<p>For German and Russian, we relied on the translated versions of WebNLG corpus [9,44]. The German version comprises 20,370 texts describing 7,812 sets of up to 7 RDF triples in 15 domains. Additionally, the German datasets provide gold-standard representations for traditional pipeline steps, such as discourse ordering (i.e., the order in which the source triples are verbalized in the target text), text structuring (i.e., the organization of the triples into paragraph and sentences), lexicalization (i.e., verbalization of the predicates) and referring expression generation (i.e., verbalization of the entities). The Russian datasets contain 20,800 texts describing 5,185 sets of up to 7 RDF triples in 9 domains. Both were automatically created and manually analyzed. The English and</p>
<p>Russian datasets abide by the criteria to gold standards as they were manually assessed by several native speakers. The German version can be regarded as a silver standard given that it did not go through the same process and contains some known errors.For the monolingual experiments, we relied on the standard WebNLG parts of train, dev, and test sets across all languages. Note that the German version does not contain a test set originally. Therefore we relied on a k-Fold Cross-Validation technique to create the test set. For the multilingual set of experiments, we concatenated all English, German and Russian datasets and shuffled their training sets randomly to facilitate an end-to-end training of the model.</p>
<h1>4.3 Tasks</h1>
<p>We designed three tasks for carrying out our evaluation, (1) Monolingual, (2) Bilingual, (3) Multilingual. (1) In the monolingual task, we train our models to work in each language separately. Hence, we generate three models, one for English, one for German, and another for Russian. Each model receives RDF triples from its given DBPedia language version. For example, the German model receives triples from the German DBpedia. Afterward, we evaluate the models on each WebNLG language-specific dataset. (2) The bilingual task was divided into two sets; the first set, we train one English-German model. This model receives RDF triples from the English and German DBpedia versions as input and has to generate text in English and German, as output. For the second set, we trained one English-Russian model that receives RDF triples from the English and Russian DBpedia versions and generates text in English and Russian, respectively. (3) In the third task, we train one multilingual model which receives as input the triples from the English, German, and Russian DBpedia versions. This model has to output text in three languages, English, German, and Russian, respectively. The input relies on WebNLG triples containing resources from the English, German, and Russian DBpedia KGs, all entities are found across the three KGs via sameAs relations for the sake of completeness.</p>
<h3>4.4 Model settings</h3>
<p>In this section, we describe the parameters and hyper-parameters used to train NABU models. We experimented with two encoder-decoder architectures for RDF verbalization. First, Transformer ${ }<em A="A" G="G" T-T="T-T" a="a" n="n" r="r" s="s">{\text {baseline }}$ which is an encoder-decoder model with a pure transformer architecture used to both encode triples into intermediate representation and decode it into tokens. Second, NABU $</em>$, which comprises a GAT encoder and Transformer as the decoder.</p>
<p>For both models, we relied on the same settings. We used a Transformer 6-layer encoder-decoder model with an 8 -headed multi-head attention mechanism [48]. The training used a batch size of 32 and Adam optimizer with an initial maximum learning rate of 0.001 . We set a source and target word embedding's size of 256, and hidden layers to size 256, dropout $=0.3$ (naive). We used a vocabulary of 32000 words for the word based models and a beam size of 5 . All our vocabularies were trained using the sentencepiece library. ${ }^{7}$ In addition, we used a copy mechanism for investigating the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>out-of-vocabulary (OOV) words issue. This mechanism first tries to substitute the OOV words with target words that have the highest attention weight according to their source words [30]. If the words are not found, it copies the source words to the position of the not-found target word [25]. Note that we added an extra language token at the beginning of our input sentences for the Transformer model, and a language node to the input graph in our GAT model for performing the bilingual and multilingual experiments. This technique of adding a special language token is in line with [46].</p>
<h1>4.5 Evaluation Metrics</h1>
<p>We used three automatic Machine Translation (MT) standard metrics to ensure consistent and clear evaluation of the common evaluation datasets of the WebNLG challenge. BLEU [38] uses a modified precision metric for comparing the MT output with the reference (human) translation. The precision is calculated by measuring the n-gram similarity $(\mathrm{n}=1, . .4)$ at the word level. BLEU also applies a brevity penalty by comparing the length of the MT output with the reference translation. METEOR [4] was mainly introduced to overcome the semantic weakness of BLEU. To this end, METEOR considers stemming and paraphrasing along with exact standard word (or phrase) matching. The synonymy overlap through a shared WordNet synset of the words. Along with exact standard word (or phrase) matching, it has additional features, i.e., stemming and paraphrasing. CHRF++ [39] exploits the use of character n-gram precision and recall (Fscore) for automatic evaluation of MT outputs. chrF++ has shown a good correlation with human rankings of different MT outputs and is simple and does not require any additional information. Additionally, chrF++ is language- and tokenization-independent.</p>
<h3>4.6 Results</h3>
<p>Monolingual. Our experiments report that NABU consistently outperforms state-of-the-art models on English data. Table 1 shows that NABU achieved a BLEU score of 66.21 , which is $28.15 \%$ higher than the previous state-of-the-art Transformer model [17]. We decided to run our experiments on all WebNLG categories to elucidate the strengths and limitations of NABU. According to [17], the main drawback in current NN models is the incapability of generating text for unseen entities and that the experiments should be on all categories. NABU, in turn, shows that it is capable of predicting correctly both seen and unseen entities and their relations. In addition, NABU shows an improvement in METEOR up to +2 points. We report NABU's chrF++ as our intention is to follow recent literature which has adopted this metric due to its good correlation with human results. We can now answer [Q1] as follows: NABU surpasses state-of-the-art results on WebNLG in English.</p>
<p>Table 2 shows that NABU outperforms the transformer baseline on German and Russian. It is important to note that our Transformer baseline, Transformer ${ }<em _baseline="{baseline" _text="\text">{\text {baseline }}$, already outperforms the previous state-of-the-art approaches on English. The difference between our Transformer ${ }</em>$ and the Transformer presented by [17] is that we rely on BPE and character-level tokenizer on the decoder side. Our results suggest that we can refrain from running the related work (see Table 1) on the German and Russian datasets, especially as they were designed and tested to work on English, thus there is}</p>
<p>Table 1: Results on WebNLG English test set with all categories (seen and unseen), comparison with the state-of-the-art approaches</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UPF-FORGe</td>
<td style="text-align: left;">38.65</td>
<td style="text-align: left;">39.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Melbourne</td>
<td style="text-align: left;">45.13</td>
<td style="text-align: left;">37.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Moryossef et al., 2019)</td>
<td style="text-align: left;">47.40</td>
<td style="text-align: left;">39.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Castro et al. (2019)</td>
<td style="text-align: left;">51.68</td>
<td style="text-align: left;">32.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: left;">$\mathbf{6 6 . 2 1}$</td>
<td style="text-align: left;">$\mathbf{4 1 . 1 1}$</td>
<td style="text-align: left;">$\mathbf{7 1 . 9 8}$</td>
</tr>
</tbody>
</table>
<p>currently no baseline for German and Russian. With these results, NABU demonstrates its language agnosticism and presents improvements in German and Russian over the baseline.</p>
<p>Table 2: Monolingual Results on WebNLG language testsets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Monolingual</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Transformer $_{\text {baseline }}$</td>
<td style="text-align: center;">ENG</td>
<td style="text-align: center;">54.96</td>
<td style="text-align: center;">38.43</td>
<td style="text-align: center;">69.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GER</td>
<td style="text-align: center;">50.07</td>
<td style="text-align: center;">34.51</td>
<td style="text-align: center;">63.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RUS</td>
<td style="text-align: center;">46.42</td>
<td style="text-align: center;">27.74</td>
<td style="text-align: center;">56.80</td>
</tr>
<tr>
<td style="text-align: center;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: center;">ENG</td>
<td style="text-align: center;">66.21</td>
<td style="text-align: center;">41.47</td>
<td style="text-align: center;">71.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GER</td>
<td style="text-align: center;">53.08</td>
<td style="text-align: center;">37.42</td>
<td style="text-align: center;">64.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RUS</td>
<td style="text-align: center;">46.86</td>
<td style="text-align: center;">28.84</td>
<td style="text-align: center;">58.37</td>
</tr>
</tbody>
</table>
<p>Bilingual. Table 3 presents the results of NABU $<em _baseline="{baseline" _text="\text">{G A T-}$ Trans on two bilingual models. The results show that NABU on English-German outperformed the Transformer ${ }</em>$ on all metrics. On English-Russian, NABU $}<em _baseline="{baseline" _text="\text">{G A T-}$ Trans presented worse results on BLEU and METEOR than Transformer ${ }</em>$. However, NABU $}<em A="A" G="G" T-="T-">{G A T-}$ Trans showed superior results on chrF++ which is the metric that best correlates with human results. On the one hand, we analyzed that the English-German model leveraged both languages properly due to their vocabulary overlap. German and English share a word vocabulary of $33 \%$, thus training both languages with NABU $</em>$ Trans, which employs a graph representation on the encoder side and a character level on decoder could actually model both languages correctly and generate coherent text. On the other hand, English-Russian presented inconsistent results because both languages are significantly different, and they do not share any vocabulary. We reckoned these conflicting scores are due to the language family of both languages. Looking manually at the results, we concluded that encoding distinct language families requires additional features, and we, therefore, plan to investigate this phenomenon in the future. The results presented herein answer our sec-</p>
<p>ond research question, [Q2], by showing that NABU is capable of modeling languages from distinct families in a bilingual approach, but a deeper investigation is required.</p>
<p>Table 3: Bilingual Results on WebNLG language test sets</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Bilingual</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Transformer $_{\text {baseline }}$</td>
<td style="text-align: left;">ENG-GER</td>
<td style="text-align: left;">58.30</td>
<td style="text-align: left;">36.46</td>
<td style="text-align: left;">66.72</td>
</tr>
<tr>
<td style="text-align: left;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: left;">ENG-GER</td>
<td style="text-align: left;">$\mathbf{6 1 . 9 9}$</td>
<td style="text-align: left;">$\mathbf{3 9 . 5 1}$</td>
<td style="text-align: left;">$\mathbf{6 9 . 6 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Transformer $_{\text {baseline }}$</td>
<td style="text-align: left;">ENG-RUS</td>
<td style="text-align: left;">$\mathbf{5 5 . 3 0}$</td>
<td style="text-align: left;">$\mathbf{3 7 . 9 0}$</td>
<td style="text-align: left;">61.63</td>
</tr>
<tr>
<td style="text-align: left;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: left;">ENG-RUS</td>
<td style="text-align: left;">49.15</td>
<td style="text-align: left;">33.41</td>
<td style="text-align: left;">$\mathbf{6 4 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>Multilingual. Table 4 shows that NABU $_{G A T-}$ Trans performed better than Transformerbaseline by presenting consistent improvement of +2 BLEU, METEOR, and chrF++. This result exhibits that NABU can effectively generate multilingual text, thus answering our third research question, [Q3]. Comparing the multilingual results of NABU with its bilingual results on English-Russian, we concluded that the characteristics of the German language, namely its three gender types, contributed to the better alignment of the languages in the decoder side of multilingual NABU model. Russian also contains three genders as German; therefore, NABU made use of it as features for generating coherent texts. We also noticed that the English texts generated by the multilingual NABU model are comparable to those of the English state-of-the-art models. NABU's multilingual model is also better than the previous English state-of-the-art by 4 BLEU and presents comparable results on METEOR. This result also reaffirms the capability of NABU for achieving English state-of-the-art results and contributes to our first research question, $[\mathrm{Q} 1]$.</p>
<p>Table 4: Multilingual Results on WebNLG language testsets</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Multilingual</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Transformer $_{\text {baseline }}$</td>
<td style="text-align: left;">ENG-GER-RUS</td>
<td style="text-align: left;">53.39</td>
<td style="text-align: left;">36.86</td>
<td style="text-align: left;">60.72</td>
</tr>
<tr>
<td style="text-align: left;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: left;">ENG-GER-RUS</td>
<td style="text-align: left;">$\mathbf{5 6 . 0 4}$</td>
<td style="text-align: left;">$\mathbf{3 8 . 3 4}$</td>
<td style="text-align: left;">$\mathbf{6 2 . 0 4}$</td>
</tr>
</tbody>
</table>
<p>Time-Performance. All models were trained on NVIDIA Tesla P100. Both NABU$<em _baseline="{baseline" _text="\text">{G A T-}$ Trans and Transformer $</em>$ models took the same amount of time since they contain the same number of weights. Therefore, the monolingual models took 6 hours to be trained, while the multilingual models took 8 hours on average. This difference of 2 hours lies in the size of the multilingual training dataset, which contains all English, German, and Russian training sets.}</p>
<h1>4.7 Error Analysis and Discussion</h1>
<p>In this section, we report some of the errors found in NABU's output while carrying out a human evaluation. First, we analyzed the discrepancy between BLEU, METEOR, and chrF++: NABU outperformed the previous state-of-the-art approach for English by roughly 15 BLEU, while the difference in METEOR is considerable smaller. Our analysis shows that some entities contained typos and were not generated correctly by NABU. In addition, we found a low variance in the generated synonyms. BLEU ignores these aspects while METEOR penalizes based on them, thus explaining the discrepancy between the scores.</p>
<p>Additionally, we noticed some wrong verbalization of similar predicates (edges) that were responsible for decreasing NABU scores across all languages. For example, NABU was sometimes not able to generate text correctly in the Artist domain. The problem lies in the triples which contain both dbo:artist or dbo:producer relations as predicates. Both predicates are often verbalized to "artist". This happens because the predicates share the same domain and range and therefore have a similar vector representation in the embeddings. We plan to address this issue in future work by using a more appropriate embedding model.</p>
<p>We also analyzed the multilingual texts generated by $\mathrm{NABU}<em _baseline="{baseline" _text="\text">{G A T-T r a n s}$ and Transfor$\operatorname{mer}</em>}}$. We noticed that the $\mathrm{NABU<em _baseline="{baseline" _text="\text">{G A T-T r a n s}$ performed better at structuring the RDF graph as input and verbalizing a structured set of RDF triples, whereas Transformer ${ }</em>}}$ presented better results than $\mathrm{NABU<em _baseline="{baseline" _text="\text">{G A T-T r a n s}$ at ordering (also known as Discourse Ordering step) the triples for a better verbalization. The advantage of Transformer ${ }</em>}}$ over $\mathrm{NABU<em A="A" G="G" T-T="T-T" a="a" n="n" r="r" s="s">{G A T-T r a n s}$ in Discourse Ordering seems to be related to the linearized form of its input, which explicitly represents in what order the triples have to be verbalized. Additionally, our reification strategy affected the Discourse Ordering, we noticed it by analyzing the generated text from an input with two equal predicates for different subjects. For example, "Albert_Einstein dbo:birthPlace Germany" and "Michael_Jackson dbo:birthPlace USA". NABU $</em>$ verbalized this two triples as "Albert Einstein was born in the United States of America and Michael Jackson was born in Germany". This problem occurs because NABU can not identify the subjects of each predicate correctly as they are identical in the encoder side. We plan to address this drawback by investigating new approaches for the structuring and ordering steps.</p>
<p>Another interesting insight is related to the inflections of words in German, similar to [9]. The possessive was often a source of errors when verbalizing into German. The translation "Elliot See 's Besatzung war ein Testpilot." is not perfect as the apostrophe ('s) is placed wrongly. However, this problem did not happen when generating the sentence, "Bill Oddies Tochter ist Kate Hardie", where the possessive of "Oddie" is built correctly. Similar insights can be derived pertaining to the preposition "von" (en: of). For example, the entity Texas_University was wrongly verbalized as "Universität von Texas" instead of the correct form "Universität Texas". The possessive and related constructions are well-known challenges in MT from English to German. Therefore, we plan to explore this phenomenon in future research deeply.</p>
<p>On the Russian results, we observed that the main challenge was related to the verbalization of unseen entities. In $\mathrm{NABU}_{G A T-T r a n s}$, some entities were copied from their source sentences due to the use of the copy mechanism in NABU. For example,</p>
<p>the entity "Visvesvaraya_Technological_University" was generated as "Visvesvaraya Technical University" in the English form instead of being verbalized in the Russian language. Additionally, we perceived that $\mathrm{NABU}_{G A T-T v a n s}$ displayed problems similar to those reported in [44] for generating Entities. However, these problems were mostly detected in the unseen category. Our current hypothesis is that the generation of unseen entities in Russian is more challenging than German and English due to the Cyrillic alphabet.</p>
<h1>5 Conclusion</h1>
<p>We presented a multilingual RDF verbalizer which relies on graph attention NN along with a reification strategy. Our experiments suggest that our approach, named NABU, outperforms state-of-the-art approaches in English. Additionally, NABU presented consistent results across the languages used in our evaluation. NABU is language-agnostic, which means it can be ported easily to languages other than those considered in this paper. To the best of our knowledge, we are the first approach to exploit and achieve the multilinguality successfully in the RDF-to-text task. As future work, we aim to exploit other graph-based neural architecture and other reification approaches for improving NABU's performance. Additionally, we plan to investigate how to deal with the similarity of relations by combining language models and new evaluation metrics [42]. Moreover, we plan to investigate our methodology in the context of low-resource scenarios as well as on different KGs [27,12].</p>
<h2>Acknowledgments</h2>
<p>Research funded by the German Federal Ministry of Economics and Technology (BMWI) in the project RAKI (no. 01MD19012D) and by the H2020 KnowGraphs (GA no. 860801). This work also has been supported by the German Federal Ministry of Education and Research (BMBF) within the project DAIKIRI under the grant no 01IS19085B as well as by the German Federal Ministry for Economic Affairs and Energy (BMWi) within the project SPEAKER under the grant no 01MK20011U. Finally, we also would like to thank the funding provided by the Coordination for the Improvement of Higher Education Personnel (CAPES) from Brazil under the grant 88887.367980/2019-00.</p>
<h2>References</h2>
<ol>
<li>Luca Anselma and Alessandro Mazzei. Designing and testing the messages produced by a virtual dietitian. In Proceedings of the 11th International Conference on Natural Language Generation, pages 244-253, Tilburg University, The Netherlands, November 2018. Association for Computational Linguistics.</li>
<li>Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia: A nucleus for a web of open data. The semantic web, pages 722-735, 2007.</li>
<li>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
</li>
<li>
<p>Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 65-72. ACL, 2005.</p>
</li>
<li>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283, 2018.</li>
<li>Anja Belz, Mike White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 217-226, Nancy, France, 2011. Association for Computational Linguistics.</li>
<li>Nadjet Bouayad-Agha, Gerard Casamayor, and Leo Wanner. Natural language generation in the context of the semantic web. Semantic Web, 5(6):493-513, 2014.</li>
<li>Daniel Braun, Ehud Reiter, and Advaith Siddharthan. Saferdrive: An NLG-based behaviour change support system for drivers. Natural Language Engineering, 24(4):551-588, 2018.</li>
<li>Thiago Castro Ferreira, Diego Moussallem, Emiel Krahmer, and Sander Wubben. Enriching the WebNLG corpus. In Proceedings of the 11th International Conference on Natural Language Generation, pages 171-176. Association for Computational Linguistics, 2018.</li>
<li>Philipp Cimiano, Janna Lüker, David Nagel, and Christina Unger. Exploiting ontology lexica for generating natural language texts from rdf data. In Proceedings of the 14th European Workshop on Natural Language Generation, pages 10-19, Sofia, Bulgaria, August 2013. ACL.</li>
<li>Emilie Colin, Claire Gardent, Yassine Mrabet, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th INLG conference, pages 163-167, 2016.</li>
<li>Diego Moussallem, Thiago Castro Ferreira, Marcos Zampieri, Maria Claudia Cavalcanti, Geraldo XexÃ̃o, Mariana Neves, and Axel-Cyrille Ngonga Ngomo. RDF2PT: Generating Brazilian Portuguese Texts from RDF Data. In The 11th edition of the Language Resources and Evaluation Conference, 7-12 May 2018, Miyazaki (Japan), 2018.</li>
<li>Bayu Distiawan, Jianzhong Qi, Rui Zhang, and Wei Wang. Gtr-lstm: A triple encoder for sentence generation from rdf data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1627-1637, 2018.</li>
<li>Daniel Duma and Ewan Klein. Generating natural language from linked data: Unsupervised template extraction. In IWCS, pages 83-94, 2013.</li>
<li>Basil Ell and Andreas Harth. A language-independent method for the extraction of rdf verbalization templates. In INLG, pages 26-34, 2014.</li>
<li>Thiago Castro Ferreira, Diego Moussallem, Emiel Krahmer, and Sander Wubben. Enriching the webnlg corpus. In Proceedings of the 11th International Conference on Natural Language Generation, pages 171-176, 2018.</li>
<li>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 552-562, 2019.</li>
<li>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 179-188. Association for Computational Linguistics, 2017.</li>
<li>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. Creating training corpora for nlg micro-planning. In Proceedings of ACL, 2017.</p>
</li>
<li>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133, 2017.</p>
</li>
<li>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133, 2017.</li>
<li>Albert Gatt and Emiel Krahmer. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. arXiv preprint arXiv:1703.09902, 2017.</li>
<li>Sebastian Gehrmann, Falcon Dai, Henry Elder, and Alexander Rush. End-to-end content and plan selection for data-to-text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 46-56, Tilburg University, The Netherlands, November 2018. Association for Computational Linguistics.</li>
<li>Dimitra Gkatzia, Helen F Hastie, and Oliver Lemon. Comparing multi-label classification with reinforcement learning for summarisation of time-series data. In ACL (1), pages 12311240, 2014.</li>
<li>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1631-1640, 2016.</li>
<li>Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, et al. Googleǖ̌̌̌̌s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339-351, 2017.</li>
<li>Lucie-Aimée Kaffee, Hady Elsahar, Pavlos Vougiouklis, Christophe Gravier, Frédérique Laforest, Jonathon Hare, and Elena Simperl. Mind the (language) gap: Generation of multilingual wikipedia summaries from wikidata for articleplaceholders. In European Semantic Web Conference, pages 319-334. Springer, 2018.</li>
<li>Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.</li>
<li>Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66-75, 2018.</li>
<li>Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attentionbased neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1412-1421. ACL, 2015.</li>
<li>Diego Marcheggiani and Laura Perez. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 1-9. Association for Computational Linguistics, 2018.</li>
<li>Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. What to talk about and how? selective generation using LSTMs with coarse-to-fine alignment. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, HLT-NAACL'16, pages 720-730, San Diego, California, 2016. Association for Computational Linguistics.</li>
<li>Simon Mille, Anja Belz, Bernd Bohnet, Yvette Graham, Emily Pitler, and Leo Wanner. The first multilingual surface realisation shared task (SR'18): Overview and evaluation results. In Proceedings of the First Workshop on Multilingual Surface Realisation, pages 1-12, Melbourne, Australia, July 2018. Association for Computational Linguistics.</li>
<li>
<p>Yassine Mrabet, Pavlos Vougiouklis, Halil Kilicoglu, Claire Gardent, Dina DemnerFushman, Jonathon Hare, and Elena Simperl. Aligning texts and knowledge bases with semantic sentence simplification. WebNLG 2016, 2016.</p>
</li>
<li>
<p>Axel-Cyrille Ngonga Ngomo, Michael Röder, Diego Moussallem, Ricardo Usbeck, and René Speck. Bengal: An automatic benchmark generator for entity recognition and linking. In Proceedings of the 11th International Conference on Natural Language Generation, pages 339-349, 2018.</p>
</li>
<li>Axel-Cyrille Ngonga Ngomo, Diego Moussallem, and Lorenz BÄijhman. A Holistic Natural Language Generation Framework for the Semantic Web. In Proceedings of the International Conference Recent Advances in Natural Language Processing, page 8. ACL (Association for Computational Linguistics), 2019.</li>
<li>Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201-206, Saarbrücken, Germany, 2017.</li>
<li>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on Association for Computational Linguistics, 2002.</li>
<li>Maja Popović. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pages 612-618, 2017.</li>
<li>Ehud Reiter and Robert Dale. Building natural language generation systems. Cambridge university press, 2000.</li>
<li>Leonardo FR Ribeiro, Yue Zhang, Claire Gardent, and Iryna Gurevych. Modeling global and local node contexts for text generation from knowledge graphs. arXiv preprint arXiv:2001.11003, 2020.</li>
<li>Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online, July 2020. Association for Computational Linguistics.</li>
<li>Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL'16, pages 1715-1725, Berlin, Germany, 2016. Association for Computational Linguistics.</li>
<li>Anastasia Shimorina, Elena Khasanova, and Claire Gardent. Creating a corpus for russian data-to-text generation using neural machine translation and post-editing. In Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 44-49, 2019.</li>
<li>Amin Sleimi and Claire Gardent. Generating paraphrases from dbpedia using deep learning. WebNLG 2016, page 54, 2016.</li>
<li>Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu. Multilingual neural machine translation with knowledge distillation. arXiv preprint arXiv:1902.10461, 2019.</li>
<li>Gongbo Tang, Mathias Müller, Annette Rios, and Rico Sennrich. Why self-attention? a targeted evaluation of neural machine translation architectures. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4263-4272, 2018.</li>
<li>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.</li>
<li>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.</li>
<li>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048-2057, 2015.</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/google/sentencepiece&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3149 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3149</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3149</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-258840942</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.14201v1.pdf" target="_blank">Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3149.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3149.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (fine-tuned LLaMA-7B for arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-7B model instruction-fine-tuned on ~1M synthetic arithmetic examples (LoRA fine-tuning) that attains state-of-the-art performance on BIG-bench arithmetic sub-tasks, achieving near-perfect zero-shot addition/subtraction and solving multi-digit multiplication/division via supervised chain-of-thought (CoT) decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-7B base model fine-tuned with instruction-style synthetic dataset (~1M QA pairs) using LoRA (r=64, alpha=64, epoch=1 typical), trained with many instruction templates and producing supervised CoT plus final answer; training can be done on a single 24GB GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction up to 16-digit integers (zero-shot direct answers); multiplication with product limited to 12 digits; division with quotient up to 6 digits (remainder allowed); uses CoT for multi-digit multiplication/division.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Supervised pattern learning from many examples combined with structured intermediate computation (supervised CoT decomposition). Success is attributed primarily to digit-level consistent tokenization enabling stable per-digit representations; complex multi-step tasks are decomposed into learnable subtasks (extraction, split, expansion, product, adding term-by-term).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: near-perfect exact-string-match/digit-match for large-number addition/subtraction after supervised fine-tuning; ablations show multi-digit multiplication/division exact-match remains ~0 without CoT but improves dramatically with the proposed CoT; tokenization comparison shows LLaMA splits digits into single tokens and other LLMs fail under identical fine-tuning; ablation on CoT steps shows 'adding term by term' is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct contradictory internal-representation probes; authors report limited extrapolation (OOD numbers beyond training digit lengths cause performance drops), and note some tasks can be learned only by overfitting (e.g., exhaustive 2-digit×2-digit enumeration), indicating learned pattern limits.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised instruction fine-tuning on synthetic dataset; LoRA parameter-efficient tuning; supervised Chain-of-Thought (CoT) generation as intermediate supervision; prompt-format variation and few-shot templates used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Supervised fine-tuning alone yields near-perfect direct-answer performance for addition/subtraction; supervised CoT decomposition enables accurate multi-digit multiplication and division where direct-answer learning fails; LoRA allowed efficient fine-tuning on modest GPU resources; few-shot/coached prompting using the decomposition helps external models (e.g., GPT-4) but Goat uses the CoT at generation time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported state-of-the-art on BIG-bench arithmetic subtask (zero-shot Goat-7B matches or surpasses few-shot PaLM-540B). Qualitatively: near-perfect exact string match/digit match on large-number addition/subtraction (trained up to 16D+16D); exact-string-match ~0 for complex multiplication/division without CoT, large positive accuracy with CoT (figures shown in ablation plots). Training example: 8-digit addition with 100k samples reaches near-perfect accuracy in ~1.5 hours on an A10 GPU. (Paper gives qualitative and plotted metrics; exact numeric table values are in paper tables/figures.)</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Limited extrapolation beyond training-digit ranges (accuracy degrades gradually for larger digits); multi-digit multiplication/division are 'unlearnable' without CoT; special-case exceptions exist where CoT not needed (e.g., multiplication by power-of-10 patterns); model can overfit small exhaustive domains but not generalize broadly without intermediate supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors compare model behavior to human use of scratchpads/stepwise computation: decomposition mimics human intermediate steps; contrasted with calculator/program solutions (which trivially compute arithmetic) — Goat aims to internalize arithmetic via supervised steps rather than use external tools; humans can compute arbitrarily large numbers given time/space while fine-tuned LLMs show OOD/extrapolation limits.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3149.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3149.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-7B (pretrained foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The open-source LLaMA family model used as the backbone; notable for tokenization scheme that splits digits into individual tokens, which the authors attribute as a key enabler for learning arithmetic when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer foundation model (LLaMA family) used as the base for Goat-7B; by design its tokenizer splits each digit into a single token for decimal numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Examined as base for fine-tuning across addition, subtraction, multiplication, division tasks in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Digit-level consistent tokenization produces stable per-digit embeddings that make mapping from token sequences to arithmetic results learnable via supervised fine-tuning; learning occurs as pattern induction over digit-token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Tokenization table in paper shows LLaMA splits multi-digit numbers into individual digit tokens; empirical fine-tuning results show LLaMA-based models learn addition/subtraction and, with CoT, multi-digit operations, while other models with inconsistent tokenization fail under the same training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper notes that tokenization alone may not explain all learnability differences (task complexity and working-memory-like requirements also hypothesized), but no strong counter-evidence is provided within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised fine-tuning (instruction tuning), LoRA parameter-efficient adaptation, synthetic data generation.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning LLaMA yields Goat that attains high arithmetic performance; LoRA allows efficient resource-limited training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When fine-tuned as Goat, LLaMA-7B achieved near-perfect addition/subtraction and high performance on decomposed multiplication/division; compared to other bases under identical fine-tuning, LLaMA had lower training loss and higher test accuracy (quantitative tables in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Even LLaMA shows limited extrapolation to digits above training range; multi-digit multiplication/division require supervised CoT decomposition to achieve accurate direct solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors compare LLaMA's digit-token representations to the human concept of per-digit manipulation and note that consistent tokenization makes learning arithmetic more similar to human digit-wise procedures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3149.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3149.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI, API-evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong closed-source LLM evaluated as a baseline; despite overall strong capabilities, GPT-4 shows near-zero exact-match performance on many large-number arithmetic tasks and does not reliably exploit chain-of-thought to fix arithmetic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large closed-source autoregressive transformer (API-evaluated by authors) used as a state-of-the-art baseline for arithmetic tasks; evaluated zero-shot and with prompting for CoT or few-shot decomposition examples.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Evaluated on same addition/subtraction/multiplication/division tasks (BIG-bench arithmetic and extra tasks up to 16D+ etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Produces internal stepwise 'long multiplication'/'long division' style CoT by default, but authors report it often fails to effectively leverage intermediate steps to guarantee correct final answers; mechanism appears to be heuristics/implicit patterns rather than systematic per-digit algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Observed behaviors: exact-string-match on large-number tasks is almost identically zero; append 'Solve it step by step' yields marginal improvement; intermediate CoT steps from GPT-4 are sometimes incorrect while final answer can be correct (suggesting inconsistent internal process); few-shot prompting using the authors' decomposition improves GPT-4's correctness relative to its default method.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Although GPT-4 sometimes outputs correct final answers, authors argue its internal CoT often contains alignment/copying/product errors, and the model does not reliably use intermediate supervision to improve final numeric accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting interventions tested: explicit chain-of-thought prompts ('Solve it step by step'), few-shot prompts showing the decomposition method.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Appending CoT instructions produced only marginal accuracy gains; few-shot prompting with the decomposition method increases frequency of correct answers compared to GPT-4's default long multiplication/division, but GPT-4 still underperforms a fine-tuned LLaMA with supervised CoT in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report GPT-4 exact-string-match is nearly zero on large-number arithmetic under standard evaluation; performs reasonably on some balanced tasks (8D+8D, 16D+16D) but fails on many 16D+8D examples. Digit-match metric (1 - char error rate) shows cases where many digits are correct but some are wrong. (Detailed numeric tables are provided in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Common systematic errors: misalignment of corresponding digits, incorrect copying of numbers, incorrect n-digit-by-1-digit intermediate products; inconsistent number tokenization suspected to make digit alignment difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors contrast GPT-4's short working-memory-like behavior (citing Bubeck et al.) with human stepwise methods; they note symbolic calculators/programs trivially compute these tasks and that GPT-4's internal heuristics do not match algorithmic symbolic accuracy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3149.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3149.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other open LLMs (Bloom/OPT/GPT-NeoX/Pythia etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bloom, OPT, GPT-NeoX, Pythia and related open LLMs (fine-tuned in study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of open-source models the authors fine-tuned on the same synthetic arithmetic dataset; these models generally failed to match LLaMA-based performance, attributed primarily to inconsistent number tokenization and higher fine-tuning loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bloom / OPT / GPT-NeoX / Pythia / GPT-J / MPT (as group)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various open-source autoregressive transformer models (sizes vary) with subword tokenization schemes that often do not split each digit into single tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same arithmetic tasks as Goat experiments (addition, subtraction, multiplication, division) under identical fine-tuning dataset and hyperparameters for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Because tokenizers produce inconsistently segmented digit tokens (subword pieces may represent variable-length digit substrings), these models cannot consistently learn digit-wise arithmetic patterns from supervised examples; learning behaves like memorization or inconsistent mapping rather than stable per-digit algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: under identical fine-tuning, these models exhibit much higher loss and far worse test accuracy than LLaMA-based models; prior work (Nogueira et al.) supports the tokenization explanation; Table 5 in paper shows concrete tokenization differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct counter-evidence in this paper; authors note that ChatGLM (not evaluated here) also has digit-splitting tokenizer and could be promising.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Identical supervised fine-tuning on the synthetic dataset and hyperparameters as used for Goat.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning these models on the same data did not produce comparable arithmetic performance to a LLaMA-based model; many learnable tasks for LLaMA remained hard for these models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively: significantly higher training loss and lower accuracy; some models had near-zero accuracy for ≥5-digit addition (as in Nogueira et al.). Exact numeric comparisons are presented in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Inconsistent tokenization causing ambiguous token embeddings for digits (same token can represent different-length digit substrings), inability to align corresponding digits across numbers, resulting in failure to learn generalizable arithmetic mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors argue these models' failures contrast with human digit-wise reasoning and with symbolic algorithms that deterministically compute arithmetic; tokenization differences make these models ill-suited for digitwise arithmetic induction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3149.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3149.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Supervised Chain-of-Thought (CoT) decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Chain-of-Thought decomposition for arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised intermediate-supervision method introduced/used in this paper that decomposes unlearnable composite arithmetic tasks into a sequence of learnable subtasks (for multiplication: extraction, split, expansion, product, adding term-by-term; for division: slow-division recurrence with repeated subtraction of divisor multiples).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Supervised CoT decomposition (intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A structured, human-interpretable CoT format included in training targets. For multiplication the decomposition uses distributive expansion and per-term n-digit-by-1-digit multiplications followed by iterative addition; for division a slow-division recurrence is used (R_j - D*(q*10^j) = R_{j+1}).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit multiplication (n-digit × m-digit) and multi-digit division (long division producing quotient and remainder).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Transforms 'unlearnable' composite tasks into sequences of simpler learnable subtasks; intermediate supervision provides internal scaffolding akin to algorithmic steps the model can learn and then chain together to produce the final result.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation experiments: multi-digit multiplication/division exact-string-match stays ~0 without CoT, while full CoT training yields high accuracy; removing the 'adding term by term' step harms performance most (ablation plots in paper); CoT reduces required epochs/data to reach high accuracy (e.g., 2-digit×2-digit direct learning needed ~10 epochs vs CoT 1 epoch).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No counter-evidence provided; authors note that some CoT steps (split/expand) have minor impact and could be omitted for brevity, but they retain them for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised generation of CoT steps appended before final answer during fine-tuning; also used as few-shot prompt examples when evaluating GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables solving of multi-digit multiplication and division that were otherwise unlearnable via direct supervised mapping; reduces training time and data requirements; ablations indicate specific steps (adding term by term) are critical for success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative/graphical: exact-string-match accuracy goes from ~0 (no CoT) to high values with full CoT on 4D×4D multiplication and 6D÷3D division; specific numeric gains and learning curves are shown in the paper's figures and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>CoT must be designed so intermediate subtasks are themselves learnable; some alternate decomposition choices may be less effective; overall approach is limited by training-distribution coverage and extrapolation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors frame CoT as analogous to human step-by-step arithmetic (scratchpad) and note it makes the model's intermediate computation human-interpretable; compared to symbolic algorithms, CoT is less efficient but allows the model to approximate algorithmic reasoning through learned textually represented steps.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3149.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3149.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Number tokenization (digit-level)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number tokenization: consistent digit-level tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The representational mechanism where each decimal digit is tokenized as a single token; identified as a key enabler for LLMs to learn arithmetic mappings from token sequences to numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>digit-level number tokenization (mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A tokenizer design/property (observed in LLaMA) that maps each decimal digit to a unique token rather than variable-length digit substrings, producing consistent per-digit embeddings across numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Underlies the learning of addition/subtraction/multiplication/division when models are trained on digit-oriented examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Stability of token-to-digit mapping lowers the representational burden: the model learns digit-wise patterns and alignments rather than having to learn variable-length substring semantics; this facilitates learning of per-digit algorithms via supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper shows LLaMA (digit-splitting) fine-tuned models succeed where other tokenizers fail under identical training; cites Nogueira et al. (2021) demonstrating inconsistent tokenization causes failure on 2-digit+ additions and larger; Table 5 compares tokenization across models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not contradicted in paper; authors note tokenization is likely a primary but not sole factor (task complexity and working-memory-like constraints also matter).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Not an intervention but a representational property; authors exploit it by fine-tuning LLaMA-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables more reliable mapping from input token sequences to numeric operations during supervised training; other models with different tokenization required more data or failed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Indirect: models with digit-level tokenization (LLaMA) show much lower fine-tuning loss and higher task accuracy than models with subword numeric tokenization (quantitative comparisons are shown in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Even with digit-level tokenization, complexity and memory limits make some tasks unlearnable without intermediate supervision; digit-tokenization alone does not guarantee extrapolation to larger-than-trained digit lengths.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Sub-task decomposition enables learning in sequence to sequence tasks <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Evaluating transformer language models on arithmetic operations using number decomposition <em>(Rating: 2)</em></li>
                <li>Limitations of language models in arithmetic and symbolic induction <em>(Rating: 1)</em></li>
                <li>Have you seen that number? investigating extrapolation in question answering models <em>(Rating: 1)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3149",
    "paper_id": "paper-258840942",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Goat-7B",
            "name_full": "Goat (fine-tuned LLaMA-7B for arithmetic)",
            "brief_description": "A LLaMA-7B model instruction-fine-tuned on ~1M synthetic arithmetic examples (LoRA fine-tuning) that attains state-of-the-art performance on BIG-bench arithmetic sub-tasks, achieving near-perfect zero-shot addition/subtraction and solving multi-digit multiplication/division via supervised chain-of-thought (CoT) decomposition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Goat-7B",
            "model_description": "LLaMA-7B base model fine-tuned with instruction-style synthetic dataset (~1M QA pairs) using LoRA (r=64, alpha=64, epoch=1 typical), trained with many instruction templates and producing supervised CoT plus final answer; training can be done on a single 24GB GPU.",
            "arithmetic_task_type": "Addition and subtraction up to 16-digit integers (zero-shot direct answers); multiplication with product limited to 12 digits; division with quotient up to 6 digits (remainder allowed); uses CoT for multi-digit multiplication/division.",
            "reported_mechanism": "Supervised pattern learning from many examples combined with structured intermediate computation (supervised CoT decomposition). Success is attributed primarily to digit-level consistent tokenization enabling stable per-digit representations; complex multi-step tasks are decomposed into learnable subtasks (extraction, split, expansion, product, adding term-by-term).",
            "evidence_for_mechanism": "Empirical: near-perfect exact-string-match/digit-match for large-number addition/subtraction after supervised fine-tuning; ablations show multi-digit multiplication/division exact-match remains ~0 without CoT but improves dramatically with the proposed CoT; tokenization comparison shows LLaMA splits digits into single tokens and other LLMs fail under identical fine-tuning; ablation on CoT steps shows 'adding term by term' is critical.",
            "evidence_against_mechanism": "No direct contradictory internal-representation probes; authors report limited extrapolation (OOD numbers beyond training digit lengths cause performance drops), and note some tasks can be learned only by overfitting (e.g., exhaustive 2-digit×2-digit enumeration), indicating learned pattern limits.",
            "intervention_type": "Supervised instruction fine-tuning on synthetic dataset; LoRA parameter-efficient tuning; supervised Chain-of-Thought (CoT) generation as intermediate supervision; prompt-format variation and few-shot templates used for evaluation.",
            "effect_of_intervention": "Supervised fine-tuning alone yields near-perfect direct-answer performance for addition/subtraction; supervised CoT decomposition enables accurate multi-digit multiplication and division where direct-answer learning fails; LoRA allowed efficient fine-tuning on modest GPU resources; few-shot/coached prompting using the decomposition helps external models (e.g., GPT-4) but Goat uses the CoT at generation time.",
            "performance_metrics": "Reported state-of-the-art on BIG-bench arithmetic subtask (zero-shot Goat-7B matches or surpasses few-shot PaLM-540B). Qualitatively: near-perfect exact string match/digit match on large-number addition/subtraction (trained up to 16D+16D); exact-string-match ~0 for complex multiplication/division without CoT, large positive accuracy with CoT (figures shown in ablation plots). Training example: 8-digit addition with 100k samples reaches near-perfect accuracy in ~1.5 hours on an A10 GPU. (Paper gives qualitative and plotted metrics; exact numeric table values are in paper tables/figures.)",
            "notable_failure_modes": "Limited extrapolation beyond training-digit ranges (accuracy degrades gradually for larger digits); multi-digit multiplication/division are 'unlearnable' without CoT; special-case exceptions exist where CoT not needed (e.g., multiplication by power-of-10 patterns); model can overfit small exhaustive domains but not generalize broadly without intermediate supervision.",
            "comparison_to_humans_or_symbolic": "Authors compare model behavior to human use of scratchpads/stepwise computation: decomposition mimics human intermediate steps; contrasted with calculator/program solutions (which trivially compute arithmetic) — Goat aims to internalize arithmetic via supervised steps rather than use external tools; humans can compute arbitrarily large numbers given time/space while fine-tuned LLMs show OOD/extrapolation limits.",
            "uuid": "e3149.0"
        },
        {
            "name_short": "LLaMA-7B (base)",
            "name_full": "LLaMA-7B (pretrained foundation model)",
            "brief_description": "The open-source LLaMA family model used as the backbone; notable for tokenization scheme that splits digits into individual tokens, which the authors attribute as a key enabler for learning arithmetic when fine-tuned.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_description": "Pretrained autoregressive transformer foundation model (LLaMA family) used as the base for Goat-7B; by design its tokenizer splits each digit into a single token for decimal numbers.",
            "arithmetic_task_type": "Examined as base for fine-tuning across addition, subtraction, multiplication, division tasks in the paper's experiments.",
            "reported_mechanism": "Digit-level consistent tokenization produces stable per-digit embeddings that make mapping from token sequences to arithmetic results learnable via supervised fine-tuning; learning occurs as pattern induction over digit-token sequences.",
            "evidence_for_mechanism": "Tokenization table in paper shows LLaMA splits multi-digit numbers into individual digit tokens; empirical fine-tuning results show LLaMA-based models learn addition/subtraction and, with CoT, multi-digit operations, while other models with inconsistent tokenization fail under the same training regime.",
            "evidence_against_mechanism": "Paper notes that tokenization alone may not explain all learnability differences (task complexity and working-memory-like requirements also hypothesized), but no strong counter-evidence is provided within this study.",
            "intervention_type": "Supervised fine-tuning (instruction tuning), LoRA parameter-efficient adaptation, synthetic data generation.",
            "effect_of_intervention": "Fine-tuning LLaMA yields Goat that attains high arithmetic performance; LoRA allows efficient resource-limited training.",
            "performance_metrics": "When fine-tuned as Goat, LLaMA-7B achieved near-perfect addition/subtraction and high performance on decomposed multiplication/division; compared to other bases under identical fine-tuning, LLaMA had lower training loss and higher test accuracy (quantitative tables in paper).",
            "notable_failure_modes": "Even LLaMA shows limited extrapolation to digits above training range; multi-digit multiplication/division require supervised CoT decomposition to achieve accurate direct solutions.",
            "comparison_to_humans_or_symbolic": "Authors compare LLaMA's digit-token representations to the human concept of per-digit manipulation and note that consistent tokenization makes learning arithmetic more similar to human digit-wise procedures.",
            "uuid": "e3149.1"
        },
        {
            "name_short": "GPT-4 (evaluated)",
            "name_full": "GPT-4 (OpenAI, API-evaluated)",
            "brief_description": "A strong closed-source LLM evaluated as a baseline; despite overall strong capabilities, GPT-4 shows near-zero exact-match performance on many large-number arithmetic tasks and does not reliably exploit chain-of-thought to fix arithmetic errors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large closed-source autoregressive transformer (API-evaluated by authors) used as a state-of-the-art baseline for arithmetic tasks; evaluated zero-shot and with prompting for CoT or few-shot decomposition examples.",
            "arithmetic_task_type": "Evaluated on same addition/subtraction/multiplication/division tasks (BIG-bench arithmetic and extra tasks up to 16D+ etc.).",
            "reported_mechanism": "Produces internal stepwise 'long multiplication'/'long division' style CoT by default, but authors report it often fails to effectively leverage intermediate steps to guarantee correct final answers; mechanism appears to be heuristics/implicit patterns rather than systematic per-digit algorithmic computation.",
            "evidence_for_mechanism": "Observed behaviors: exact-string-match on large-number tasks is almost identically zero; append 'Solve it step by step' yields marginal improvement; intermediate CoT steps from GPT-4 are sometimes incorrect while final answer can be correct (suggesting inconsistent internal process); few-shot prompting using the authors' decomposition improves GPT-4's correctness relative to its default method.",
            "evidence_against_mechanism": "Although GPT-4 sometimes outputs correct final answers, authors argue its internal CoT often contains alignment/copying/product errors, and the model does not reliably use intermediate supervision to improve final numeric accuracy.",
            "intervention_type": "Prompting interventions tested: explicit chain-of-thought prompts ('Solve it step by step'), few-shot prompts showing the decomposition method.",
            "effect_of_intervention": "Appending CoT instructions produced only marginal accuracy gains; few-shot prompting with the decomposition method increases frequency of correct answers compared to GPT-4's default long multiplication/division, but GPT-4 still underperforms a fine-tuned LLaMA with supervised CoT in many cases.",
            "performance_metrics": "Authors report GPT-4 exact-string-match is nearly zero on large-number arithmetic under standard evaluation; performs reasonably on some balanced tasks (8D+8D, 16D+16D) but fails on many 16D+8D examples. Digit-match metric (1 - char error rate) shows cases where many digits are correct but some are wrong. (Detailed numeric tables are provided in the paper.)",
            "notable_failure_modes": "Common systematic errors: misalignment of corresponding digits, incorrect copying of numbers, incorrect n-digit-by-1-digit intermediate products; inconsistent number tokenization suspected to make digit alignment difficult.",
            "comparison_to_humans_or_symbolic": "Authors contrast GPT-4's short working-memory-like behavior (citing Bubeck et al.) with human stepwise methods; they note symbolic calculators/programs trivially compute these tasks and that GPT-4's internal heuristics do not match algorithmic symbolic accuracy.",
            "uuid": "e3149.2"
        },
        {
            "name_short": "Other open LLMs (Bloom/OPT/GPT-NeoX/Pythia etc.)",
            "name_full": "Bloom, OPT, GPT-NeoX, Pythia and related open LLMs (fine-tuned in study)",
            "brief_description": "A set of open-source models the authors fine-tuned on the same synthetic arithmetic dataset; these models generally failed to match LLaMA-based performance, attributed primarily to inconsistent number tokenization and higher fine-tuning loss.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bloom / OPT / GPT-NeoX / Pythia / GPT-J / MPT (as group)",
            "model_description": "Various open-source autoregressive transformer models (sizes vary) with subword tokenization schemes that often do not split each digit into single tokens.",
            "arithmetic_task_type": "Same arithmetic tasks as Goat experiments (addition, subtraction, multiplication, division) under identical fine-tuning dataset and hyperparameters for comparison.",
            "reported_mechanism": "Because tokenizers produce inconsistently segmented digit tokens (subword pieces may represent variable-length digit substrings), these models cannot consistently learn digit-wise arithmetic patterns from supervised examples; learning behaves like memorization or inconsistent mapping rather than stable per-digit algorithms.",
            "evidence_for_mechanism": "Empirical: under identical fine-tuning, these models exhibit much higher loss and far worse test accuracy than LLaMA-based models; prior work (Nogueira et al.) supports the tokenization explanation; Table 5 in paper shows concrete tokenization differences.",
            "evidence_against_mechanism": "No direct counter-evidence in this paper; authors note that ChatGLM (not evaluated here) also has digit-splitting tokenizer and could be promising.",
            "intervention_type": "Identical supervised fine-tuning on the synthetic dataset and hyperparameters as used for Goat.",
            "effect_of_intervention": "Fine-tuning these models on the same data did not produce comparable arithmetic performance to a LLaMA-based model; many learnable tasks for LLaMA remained hard for these models.",
            "performance_metrics": "Reported qualitatively: significantly higher training loss and lower accuracy; some models had near-zero accuracy for ≥5-digit addition (as in Nogueira et al.). Exact numeric comparisons are presented in paper tables.",
            "notable_failure_modes": "Inconsistent tokenization causing ambiguous token embeddings for digits (same token can represent different-length digit substrings), inability to align corresponding digits across numbers, resulting in failure to learn generalizable arithmetic mappings.",
            "comparison_to_humans_or_symbolic": "Authors argue these models' failures contrast with human digit-wise reasoning and with symbolic algorithms that deterministically compute arithmetic; tokenization differences make these models ill-suited for digitwise arithmetic induction.",
            "uuid": "e3149.3"
        },
        {
            "name_short": "Supervised Chain-of-Thought (CoT) decomposition",
            "name_full": "Supervised Chain-of-Thought decomposition for arithmetic",
            "brief_description": "A supervised intermediate-supervision method introduced/used in this paper that decomposes unlearnable composite arithmetic tasks into a sequence of learnable subtasks (for multiplication: extraction, split, expansion, product, adding term-by-term; for division: slow-division recurrence with repeated subtraction of divisor multiples).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Supervised CoT decomposition (intervention)",
            "model_description": "A structured, human-interpretable CoT format included in training targets. For multiplication the decomposition uses distributive expansion and per-term n-digit-by-1-digit multiplications followed by iterative addition; for division a slow-division recurrence is used (R_j - D*(q*10^j) = R_{j+1}).",
            "arithmetic_task_type": "Multi-digit multiplication (n-digit × m-digit) and multi-digit division (long division producing quotient and remainder).",
            "reported_mechanism": "Transforms 'unlearnable' composite tasks into sequences of simpler learnable subtasks; intermediate supervision provides internal scaffolding akin to algorithmic steps the model can learn and then chain together to produce the final result.",
            "evidence_for_mechanism": "Ablation experiments: multi-digit multiplication/division exact-string-match stays ~0 without CoT, while full CoT training yields high accuracy; removing the 'adding term by term' step harms performance most (ablation plots in paper); CoT reduces required epochs/data to reach high accuracy (e.g., 2-digit×2-digit direct learning needed ~10 epochs vs CoT 1 epoch).",
            "evidence_against_mechanism": "No counter-evidence provided; authors note that some CoT steps (split/expand) have minor impact and could be omitted for brevity, but they retain them for interpretability.",
            "intervention_type": "Supervised generation of CoT steps appended before final answer during fine-tuning; also used as few-shot prompt examples when evaluating GPT-4.",
            "effect_of_intervention": "Enables solving of multi-digit multiplication and division that were otherwise unlearnable via direct supervised mapping; reduces training time and data requirements; ablations indicate specific steps (adding term by term) are critical for success.",
            "performance_metrics": "Qualitative/graphical: exact-string-match accuracy goes from ~0 (no CoT) to high values with full CoT on 4D×4D multiplication and 6D÷3D division; specific numeric gains and learning curves are shown in the paper's figures and tables.",
            "notable_failure_modes": "CoT must be designed so intermediate subtasks are themselves learnable; some alternate decomposition choices may be less effective; overall approach is limited by training-distribution coverage and extrapolation capability.",
            "comparison_to_humans_or_symbolic": "Authors frame CoT as analogous to human step-by-step arithmetic (scratchpad) and note it makes the model's intermediate computation human-interpretable; compared to symbolic algorithms, CoT is less efficient but allows the model to approximate algorithmic reasoning through learned textually represented steps.",
            "uuid": "e3149.4"
        },
        {
            "name_short": "Number tokenization (digit-level)",
            "name_full": "Number tokenization: consistent digit-level tokenization",
            "brief_description": "The representational mechanism where each decimal digit is tokenized as a single token; identified as a key enabler for LLMs to learn arithmetic mappings from token sequences to numeric results.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "digit-level number tokenization (mechanism)",
            "model_description": "A tokenizer design/property (observed in LLaMA) that maps each decimal digit to a unique token rather than variable-length digit substrings, producing consistent per-digit embeddings across numbers.",
            "arithmetic_task_type": "Underlies the learning of addition/subtraction/multiplication/division when models are trained on digit-oriented examples.",
            "reported_mechanism": "Stability of token-to-digit mapping lowers the representational burden: the model learns digit-wise patterns and alignments rather than having to learn variable-length substring semantics; this facilitates learning of per-digit algorithms via supervised fine-tuning.",
            "evidence_for_mechanism": "Paper shows LLaMA (digit-splitting) fine-tuned models succeed where other tokenizers fail under identical training; cites Nogueira et al. (2021) demonstrating inconsistent tokenization causes failure on 2-digit+ additions and larger; Table 5 compares tokenization across models.",
            "evidence_against_mechanism": "Not contradicted in paper; authors note tokenization is likely a primary but not sole factor (task complexity and working-memory-like constraints also matter).",
            "intervention_type": "Not an intervention but a representational property; authors exploit it by fine-tuning LLaMA-based models.",
            "effect_of_intervention": "Enables more reliable mapping from input token sequences to numeric operations during supervised training; other models with different tokenization required more data or failed.",
            "performance_metrics": "Indirect: models with digit-level tokenization (LLaMA) show much lower fine-tuning loss and higher task accuracy than models with subword numeric tokenization (quantitative comparisons are shown in paper tables).",
            "notable_failure_modes": "Even with digit-level tokenization, complexity and memory limits make some tasks unlearnable without intermediate supervision; digit-tokenization alone does not guarantee extrapolation to larger-than-trained digit lengths.",
            "uuid": "e3149.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Sub-task decomposition enables learning in sequence to sequence tasks",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Evaluating transformer language models on arithmetic operations using number decomposition",
            "rating": 2
        },
        {
            "paper_title": "Limitations of language models in arithmetic and symbolic induction",
            "rating": 1
        },
        {
            "paper_title": "Have you seen that number? investigating extrapolation in question answering models",
            "rating": 1
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 1
        }
    ],
    "cost": 0.01798925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</p>
<p>Tiedong Liu tiedong.liu@u.nus.edu 
National University of Singapore
National University of Singapore</p>
<p>Bryan Kian 
National University of Singapore
National University of Singapore</p>
<p>Hsiang Low lowkh@comp.nus.edu.sg 
National University of Singapore
National University of Singapore</p>
<p>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</p>
<p>We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-ofthe-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation. 1</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown remarkable proficiency across a wide range of natural language processing (NLP) tasks (Brown et al., 2020;Chowdhery et al., 2022;Thoppilan et al., 2022). Notably, GPT-4 (OpenAI, 2023) has achieved state-of-the-art performances in such tasks. However, it is surprising that such powerful language models still struggle with elementary arithmetic tasks. The performance of GPT-4 in 1 https://github.com/liutiedong/goat. arithmetic tasks, particularly multiplication and division of large numbers, currently remains far from optimal, with accuracy levels trending toward zero.</p>
<p>In this paper, we present Goat, a fine-tuned language model that is GOod at Arithmetic Tasks. Goat achieves state-of-the-art performance in elementary arithmetic, including addition, subtraction, multiplication, and division of integers. We adopt an end-to-end supervised instruction-finetuning paradigm on LLaMA (Touvron et al., 2023), leveraging a synthetically generated dataset containing around 1 million samples. Unlike previous research on arithmetic computation (Lee and Kim, 2023;Nogueira et al., 2021;Nye et al., 2021;Qian et al., 2022;Zhou et al., 2022b), our study demonstrates that through supervised fine-tuning alone and without applying any special techniques, our model is capable of generating direct answers for largenumber addition and subtraction with near-perfect accuracy in a zero-shot setting. We attribute this exceptional arithmetic ability to LLaMA's consistent tokenization of numbers and show that this is almost impossible to achieve for previous LLMs such as Bloom (Scao et al., 2022), OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), Pythia (Biderman et al., 2023), etc. However, the model encounters significant difficulties when generating direct answers for arithmetic tasks like large-number multiplication and division. To overcome this challenge, we propose an approach that categorizes various arithmetic tasks into learnable and unlearnable tasks, subsequently decomposing the unlearnable tasks, such as multidigit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. Our approach ensures that the intermediate supervision which facilitates the model's learning is also easily understandable and interpretable by humans. We fine-tune our model to generate the proposed CoT before generating the final answer, similar to sketchpad (Nye et al., 2021). Our method outperforms GPT-4's long multiplication and long division methods by a large margin. We assess the performance of our model using BIG-bench (Srivastava et al., 2022) arithmetic sub-task, and provide a comprehensive evaluation of the effectiveness of our proposed method. Our findings suggest that the model can learn the pattern and generalize to unseen data instead of purely memorizing the computation. Additionally, Goat-7B can be conveniently trained using Low-Rank Adaptation (LoRA) (Hu et al., 2021) technique on a 24GB VRAM GPU, making it easily reproducible for other researchers.</p>
<p>To summarize, our contributions include:</p>
<p>• Our model achieves state-of-the-art performance on various elementary arithmetic tasks, including addition, subtraction, multiplication, and division of positive integers (Section 4).</p>
<p>We show that an open-sourced model finetuned on a synthetically generated dataset has the potential to achieve even higher accuracy on arithmetic tasks compared to GPT-4.</p>
<p>• To the best of our knowledge, we are the first to demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to generate direct answers for certain elementary arithmetic tasks, such as large-number addition and subtraction, without applying any special techniques (Section 3.3). Previously effective chain-of-thought (CoT) methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary. The impressive performance is mainly attributed to LLaMA's consistent tokenization of numbers.</p>
<p>• To solve large-number multiplication and division, we propose a novel decomposition method based on the learnability of the task, leveraging basic arithmetic principles to ensure human interpretability (Section 3.4).</p>
<p>• We systematically investigate the proposed decomposition method and demonstrate its effectiveness (Section 5). We conduct thorough experiments on the decomposition steps in a fully synthetic environment by mitigating many hard-to-control aspects of natural language. Our experimental setup offers an ideal platform to study the impact of CoT and intermediate supervision.</p>
<p>• Our end-to-end instruction tuning pipeline can be easily integrated into existing instructiontuned language models (Chiang et al., 2023;Taori et al., 2023) and potentially enhance their mathematical reasoning for math word problems. We release the model, dataset, and script for generating the dataset.</p>
<p>2 Related Work</p>
<p>Instruction Tuning</p>
<p>Instruction tuning (Chung et al., 2022;Ouyang et al., 2022;Sanh et al., 2021) is a technique used to align pretrained language models with human instructions. It enables targeted customization of LLMs to specific tasks, enhancing their ability to generate more accurate and contextually relevant responses and improving the zero-shot performance. The dataset used for instruction tuning can be human-written (Ouyang et al., 2022), machinegenerated (Peng et al., 2023;Taori et al., 2023;, or collected from web (Geng et al., 2023). Recently, there has been extensive research on fine-tuning LLaMA (Touvron et al., 2023) for various downstream tasks using instruction tuning (Chiang et al., 2023;Geng et al., 2023;Taori et al., 2023;Xu et al., 2023;Yunxiang et al., 2023). Creating high-quality instruction tuning datasets can be expensive and time-consuming. In this study, we utilize a simple Python program to generate input-output pairs for arithmetic tasks.</p>
<p>Arithmetic Reasoning</p>
<p>Arithmetic reasoning has been a topic of interest in NLP research for many years (Lu et al., 2022). Recently, the use of pretrained models (Brown et al., 2020;OpenAI, 2023) has shown great capabilities in solving math word problems. Particularly, chain of thought (CoT) (Kojima et al., 2022;Zhou et al., 2022a) provides the model with the intermediate steps to derive the final answer. However, studies have shown that LLMs struggle with basic arithmetic computation and often make arithmetic mistakes, even though the reasoning process is correct (Cobbe et al., 2021;Gao et al., 2022;Schick et al., 2023). Consequently, one key challenge of arithmetic reasoning, aside from mapping natural language to arithmetic expressions, is how to compute the generated arithmetic expressions with high accuracy.</p>
<p>Arithmetic Computation</p>
<p>Recent studies have explored using external tools to evaluate arithmetic expressions. Toolformer (Schick et al., 2023) and GSM8K (Cobbe et al., 2021) invoke an external calculator to compute the generated arithmetic expression. PoT (Chen et al., 2022) and PAL (Gao et al., 2022) generate programs that can be executed to produce the final answer. While arithmetic can be solved using calculators or programs easily, the ability to perform arithmetic computation is a remarkable trait of human intelligence, and we anticipate LLMs should possess this ability as well.</p>
<p>Previous studies have evaluated the arithmetic abilities of LLMs. Nogueira et al. (2021) have evaluated addition and subtraction tasks. Muffo et al. (2022) have further examined 2-digit multiplication. Yuan et al. (2023) have tested different types of arithmetic operations. CoT seems to be a promising solution for arithmetic computation as well. Similar to humans, autoregressive language model may rely on intermediate supervision to generate the final answer. Scratchpad (Nye et al., 2021) finetunes the language models to produce CoT before generating an answer, and has demon-strated effectiveness on 8-digit addition. However, we show that previously effective CoT methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary for certain arithmetic tasks like addition. By leveraging simple supervised finetuning alone, our model can perform addition and subtraction with sufficiently high accuracy. For challenging tasks like large-number multiplication and division, previous studies (Muffo et al., 2022;Lee and Kim, 2023) either fail to compute or are inefficient. Furthermore, our model is trained endto-end such that it can follow human instructions.</p>
<p>Method</p>
<p>Language Model</p>
<p>LLaMA (Touvron et al., 2023) is a collection of open-source pretrained language models trained on trillions of tokens using publicly available datasets, and achieves state-of-the-art performance on many benchmarks.</p>
<p>Previous studies (Kim et al., 2021;Nogueira et al., 2021) have shown that tokenization is important for LLM's arithmetic ability. Many commonlyused subword tokenization techniques today are not ideal to represent numbers. However, LLaMA splits each digit into an individual token (Yuan et al., 2023), thereby ensuring consistent tokenization of numbers, as shown in Appendix B.</p>
<p>The selection of language models is crucial to our work. We believe the remarkable arithmetic ability demonstrated in this work is mainly attributed to LLaMA's consistent tokenization of numbers. We experimentally verify that other LLMs, such as Bloom, OPT, GPT-NeoX, and Pythia, finetuned on the same arithmetic dataset, cannot match LLaMA's arithmetic ability.</p>
<p>Learnability of Arithmetic Tasks</p>
<p>Wies et al. (2022) have provided a theoretical analysis on the use of intermediate supervision for solving composite tasks. Specifically, they have shown that for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple subtasks, unlearnable composite problems can become learnable by using intermediate supervision or stepby-step CoT.</p>
<p>Building upon their analysis, we first experimentally categorize learnable and unlearnable tasks. In the context of arithmetic computation, learnable tasks generally refer to those for which the model can be successfully trained to generate direct answers, achieving sufficiently high accuracy within a predefined number of training epochs. Conversely, unlearnable tasks are those that the model struggles to learn and generate direct answers correctly even with extensive training. While the exact reason behind the varying learnability of tasks is not yet fully understood and requires further investigation, we hypothesize that it is associated with the complexity of the underlying pattern and the size of working memory required for completing the task (Bubeck et al., 2023).</p>
<p>We experimentally examine the learnability of these tasks by fine-tuning the model specifically for each task in a simplified synthetic environment (Table 7). Our recognized learnable and unlearnable tasks are listed in Table 1.</p>
<p>The categorization of tasks also aligns with human perception. With practice, humans can mentally calculate the addition and subtraction of two large numbers, writing down the final numerical answer directly from the left (most significant figure) to the right (least significant figure) without the need for sketchpad. However, mentally solving large-number multiplication and division is undeniably a challenging task.</p>
<p>We also observe that our classification of tasks is consistent with the performance of GPT-4. In particular, GPT-4 excels in generating direct answers for large-number addition and subtraction. However, its accuracy significantly drops when it comes to multi-digit multiplication and division tasks. Our observation aligns with the claim made by Bubeck et al. (2023) that GPT-4 has a short working memory and performs poorly on composite arithmetic tasks. This is particularly evident in the case of multiplication, which involves multiple steps of addition. The inability of powerful models like GPT-4 to directly solve unlearnable tasks may suggest that generating direct answers for such tasks is extremely challenging, even with extensive training.</p>
<p>It is noteworthy that a task that is learnable for LLaMA may not necessarily be learnable for other LLMs, which is validated in our experiments in Section 5.3. Furthermore, not all tasks classified as unlearnable are entirely impossible for the model to learn. For instance, 2-digit by 2-digit multiplication is considered an unlearnable task in our case. However, the model can still learn to generate the direct answer by overfitting to the training set, which contains an exhaustive enumeration of all possible 2-digit multiplication. Nevertheless, the process takes nearly 10 epochs to achieve around 90% accuracy. In contrast, by inserting our proposed CoT before the final answer, the model can achieve comparable accuracy in 2-digit multiplication with only 1 epoch of training. These findings align with the claim (Wies et al., 2022) that the presence of intermediate supervision facilitates the learning process.</p>
<p>Addition and Subtraction</p>
<p>Addition and subtraction tasks are learnable, as with supervised fine-tuning alone, the model exhibits a remarkable ability to accurately generate direct numerical answers. The model successfully captures the underlying patterns of the arithmetic operations. This is evident from the model's near-perfect accuracy on the unseen test set, despite being trained on a very limited subset of the data. It is worth mentioning that addition and subtraction operations do not require the use of CoT. This contrasts with previous studies that have employed CoT for addition and subtraction tasks (Lee and Kim, 2023;Nye et al., 2021;Qian et al., 2022).</p>
<p>Multiplication</p>
<p>We experimentally verify that n-digit by 1-digit multiplication is learnable. In contrast, multi-digit multiplication poses significant challenges for the model, suggesting it to be an unlearnable task. To overcome this issue, we adopt a similar strategy used in sketchpad (Nye et al., 2021), which finetunes the LLMs to generate CoT before generating the answer. Specifically, we propose a CoT that decomposes the multi-digit multiplication into a series of 5 learnable sub-tasks: (1) extraction: extract the arithmetic expression from the natural language instruction, (2) split: split the smaller number of the two into place values, (3) expansion: expand the sum based on the distributive property, (4) product: compute each product simultaneously, and (5) adding term by term: add the first two terms and copy the rest, and the final sum is obtained.</p>
<p>Consider the example in Fig. 1. Firstly, the arithmetic expression 397 × 4429 is extracted from the instruction, which can be considered as a "copying" task. Secondly, 397×4429 = 4429×(300+90+7) involves two learnable tasks. The larger number of the two is placed in front and then the smaller one is split, which is similar to "ordering" and "split" learnable tasks. The ordering ensures that there are fewer summation terms in the next step, thereby reducing the CoT length. Thirdly, the sum is expanded using distributive law: 4429 × (300 + 90 + 7) = 4429 × 300 + 4429 × 90 + 4429 × 7, which is similar to "copying" task. Next, 4429 × 300 + 4429 × 90 + 4429 × 7 = 1328700 + 398610 + 31003 where the products are computed at once by applying "multiplication n-digit by 1-digit" with zeros copied at the end of each product. Finally, we take the sum of the first two terms at each step, and copy the rest terms, leveraging "addition" and "copying". Hence, a composite unlearnable task is broken down into simpler tasks that are all learnable.</p>
<p>Division</p>
<p>Similarly, we observe that n-digit by 1-digit division is learnable. However, multi-digit division is unlearnable. We design a novel CoT leveraging a modified slow division method based on the following recurrence equation
R j − D × (q n−(j+1) × 10 j ) = R j+1
where R j is the j-th partial remainder of the division, q n−(j+1) is the digit of the quotient in position n − (j + 1) numbered from least significant 0 to most significant n − 1, n is the number of digits in the quotient, and D is the divisor. Specifically, the main idea is to subtract multiples of the divisor from the dividend until the remainder is less than the divisor.</p>
<p>Here is a detailed breakdown of the CoT used in Fig. 1. Consider the first iteration (first equation). The first step 8914−64×100 requires the model to copy the dividend and the divisor, and subsequently generate a number q n−(j+1) × 10 j such that the product of q n−(j+1) × 10 j and the divisor D is less than or equal to the partial remainder R j . This inherently involves two learnable tasks: "n-digit by 1digit multiplication" and "comparison". We experimentally show that this composite task is learnable. The second step 8914 − 64 × 100 = 8914 − 6400 mainly involves a "copying" task and an "n-digit by 1-digit multiplication" task. The third step 8914 − 6400 = 2514 leverages "subtraction". The process iterates until the leftover is less than the divisor, which implies the model has to implicitly learn comparison. Finally, the model generates the quotient by combining all q n−(j+1) 's in previous iterations, which can be considered as the inverse of the "split" task, and finally copies the remainder if it is not zero.</p>
<p>A summary of prompts and expected output for various tasks are shown in Table 2.</p>
<p>Settings</p>
<p>In this paper, we consider the addition and subtraction of two positive integers with each containing up to 16 digits. It is worth noting that the result of subtraction can be negative. To limit the maximum generated sequence length, we consider the multiplication of two positive integers whose product falls within 12 digits, and the division of two positive integers resulting in a quotient within 6 digits where the dividend is less than 12 digits. Since we focus on arithmetic tasks of integers, we aim  to obtain the least positive remainder in the case when it is not divisible. In Section 5.2, we present an analysis showcasing the limited extrapolation capabilities of finetuned LLMs. Consequently, input data that falls outside the distribution of the training data is unlikely to yield reasonable answers. Our method potentially applies to numbers with more digits, though the training cost will increase correspondingly.</p>
<p>Dataset</p>
<p>We generate the dataset synthetically using a Python script. The dataset consists of around 1 million question-answer pairs. The answer contains the proposed CoT as well as the final numerical output. The numbers are randomly generated, hence ensuring a very low probability of instances being duplicated, although small numbers may be sampled multiple times. We sample from log space to ensure the numbers are equally likely to be sampled from different orders of magnitude, which is similar to the sampling method used by Lee and Kim (2023). The details of the dataset are presented in Appendix F.</p>
<p>Fine-tuning</p>
<p>To enable the model to solve arithmetic problems based on instructions and facilitate natural language question answering, we generate hundreds of instruction templates using ChatGPT (Table 6). During the instruction tuning process, we randomly select a template for each arithmetic input from the training set, and fine-tune LLaMA-7B similar to the method used in Alpaca (Taori et al., 2023). We apply various techniques to enhance the model's adaptability to diverse question formats, such as randomly removing spaces between numbers and symbols in the arithmetic expression, replacing "*" with "x" or "times", etc.</p>
<p>Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU. In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy. The training hyperparameters are listed in Appendix A.</p>
<p>Experiments</p>
<p>We evaluate our model using BIG-bench arithmetic dataset (Srivastava et al., 2022), as well as our extra selected tasks. The results are shown in Table 3. Notably, in a zero-shot setting, Goat-7B achieves comparable or even higher accuracy on BIG-bench compared to the few-shot PaLM-540B.</p>
<p>Metric</p>
<p>We first compute the accuracy based on the standard exact string match (Appendix C). We observe that GPT-4's accuracy under exact string match is almost identically zero on tasks involving large numbers. However, in many cases where the final answer is incorrect, the majority of digits in the generated answer align with the target number, with only a few digits being incorrect. Inspired by recent study on the emergent abilities of LLMs (Schaeffer et al., 2023), we include a digit match metric that can reflect the per-token error rate of the output, as each digit is uniquely represented by a token in LLaMA.</p>
<p>Task</p>
<p>BIG-bench</p>
<p>Extra Tasks   ADD  1D  2D  3D  4D  5D  8D+8D 16D+8D 16D+16D GPT  Table 3: The result of GPT-4 and Goat-7B on BIG-bench Arithmetic sub-task and extra selected arithmetic tasks, using metrics Exact String Match/Digit Match (Appendix C), shown in percentage. We test GPT-4 and Goat with exactly the same questions and prompts. We evaluate GPT-4 using the API version on May 10th. For Big-bench tasks, nD refers the n-digit by n-digit operation, except for division where nD means n-digit by m-digit where m ≤ n. BIG-bench only includes division operation without remainder, whereas in extra tasks we include the cases where the remainder is not zero and ask GPT-4 to output the answer in "quotient R remainder" format. It should be noted that we exclude the BIG-bench test data from our training dataset as much as possible, although the overlap is unavoidable for operations involving small numbers.</p>
<p>Comparison</p>
<p>Comparing the performance of Goat and GPT-4 for large-number multiplication and division may seem unfair, as GPT-4 generates direct answers while Goat relies on CoT. Hence, we also evaluate GPT-4's performance with CoT by appending "Solve it step by step" at the end of each prompt. By default, GPT-4 uses long multiplication and long division methods. However, we observe that generating CoT only leads to marginal improvement in accuracy. In some cases, the intermediate steps from long multiplication and division are incorrect, but surprisingly the final answer is correct. This implies that GPT-4 does not effectively take advantage of intermediate supervision from CoT to improve the final output. We identify the following 3 common errors from GPT-4's solution, which results in incorrect final answers: (1) the alignment of corresponding digits, (2) copying of numbers, and (3) the intermediate result from n-digit by 1-digit multiplication.</p>
<p>Additionally, we observe that GPT-4 performs reasonably well on 8D +8D and 16D +16D tasks, but fails on most 16D + 8D tasks, though intuitively 16D + 8D should be relatively easier than 16D+16D. While the exact reason for this remains unclear, one possible factor could be GPT-4's inconsistent number tokenization (Table 5), which makes it difficult to align the corresponding digits of two numbers. full CoT no split no expansion no adding term by term no CoT Figure 2: Accuracy (exact string match) against the number of samples seen during the training of 4D × 4D task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>Here we want to study the usefulness and effectiveness of each intermediate decomposition step. Specifically, for multiplication (Fig. 2), we com- full CoT no product no CoT Figure 3: Accuracy (exact string match) against the number of samples seen during the training of 6D ÷ 3D task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>pare the accuracy of 4-digit by 4-digit multiplication by removing one particular step in the CoT, including split, expansion, adding term by term (referring to G), as well as no CoT. For division (Fig. 3), we compare the accuracy of 6-digit by 3-digit division after removing the middle step that computes the product (referring to G), as well as no CoT. To minimize the impact caused by natural language, we conduct an ablation study in a simplified synthetic environment (Table 7). The multiplication results suggest that the "adding term by term" step plays a crucial role in obtaining the final answer. In contrast, the "split" and "expand" steps have minimal impact, and can potentially be omitted for generating more concise CoT. This can be attributed to the nature of these two intermediate steps, which primarily involve simple and learnable tasks like copying and comparison. Nevertheless, we still retain these steps to ensure human interpretability.</p>
<p>The accuracy of exact string match without CoT remains consistently at zero for both 4D × 4D multiplication and 6D ÷ 3D division. This further showcases the validity of our approach, as breaking down complex arithmetic tasks into a series of learnable tasks can indeed facilitate the training process for LLMs.</p>
<p>Extrapolation</p>
<p>Extrapolation refers to the ability of the model to predict data that lies out-of-distribution (OOD) of training data. We test addition for numbers larger than those in the training data distribution. The results reveal that the model has limited extrapolation capabilities. There is a gradual drop in accuracy, as the test set deviates further from the training set. This observation is consistent with the result reported in (Kim et al., 2021), highlighting a limitation of our fine-tuned model and underscoring the significance of training data distribution.  Figure 4: Accuracy against the number of digits for the addition task. The model is trained up to 16D+16D, and tested on 17D+17D onward.</p>
<p>Comparison with Other LLMs</p>
<p>We conduct comprehensive experiments on a variety of LLMs, including Bloom, OPT, GPT-J, GPT-NeoX, and Pythia. These models are fine-tuned using the identical dataset as that for Goat, maintaining consistency in the training hyperparameters. Our experiment shows that they all struggle with arithmetic tasks. Even for tasks that are considered learnable for LLaMA, such as multi-digit addition, the loss during fine-tuning is significantly higher than that of LLaMA. The observation underscores the claim made in (Nogueira et al., 2021) that tokenization is a crucial factor in the performance of arithmetic tasks.</p>
<p>Few-shot Prompting with GPT-4</p>
<p>GPT-4 demonstrates powerful in-context learning abilities. We further examine the effectiveness of our proposed decomposition method for solving large-number multiplication and division by using few-shot prompting with GPT-4 (see Appendix H). We observe that our decomposition method allows GPT-4 to generate correct answers more frequently than using its default long multiplication and division methods. This further supports the effectiveness and validity of our approach. Examples of the prompt and output are shown in Appendix H.</p>
<p>Limitations</p>
<p>Humans are capable of performing multiplication and division on arbitrarily large numbers, providing sufficient time and space for calculations. In contrast, LLMs often suffer from extrapolation prob-lems. The models are unlikely to generate reasonable answers if the input deviates significantly from the distribution of training data. To enhance the human interpretability of intermediate supervision, we use the straightforward CoT that follows simple basic arithmetic rules. However, this design may not be the most efficient way to facilitate the final answer generation. There are potentially more suitable multiplication and division algorithms for the model to learn. Besides, our research only focuses on elementary arithmetic operations involving integers. Nevertheless, we anticipate that our method could be applicable to decimal computation as well.</p>
<p>Conclusion</p>
<p>In summary, we demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to perform certain basic arithmetic operations with high accuracy. With our proposed CoT, our model achieves state-of-the-art performance on various elementary arithmetic tasks. Our research offers an excellent platform for investigating the mechanism of working memory and the influence of intermediate supervision on text generation. Our method can be easily integrated with other instruction-tuned LLMs and has the potential to further enhance arithmetic reasoning abilities in solving math word problems.</p>
<p>A Hyperparameters</p>
<p>Hyperparameter Value batch size 128 learning rate 0.0003 lora r 64 lora alpha 64 lora target module q, v, k, o lora dropout 0.05 epoch 1  Nogueira et al. (2021) demonstrate that models with inconsistent tokenization of numbers barely learn the addition of 2-digit numbers, and it completely fails to learn the addition of larger numbers. Specifically, it has an accuracy of zero for 5 digits or more. They attribute this failure to the lack of systematic tokenization of individual digits. For instance, "123" might be tokenized as "12" and "3", while "234" might be tokenized as "2" and "34". Consequently, the model is required to learn that the embedding of a token may represent either a single digit or two digits and so on. Hence, it might be challenging for the model to learn to map an embedding to a number when the number of digits it represents changes irregularly. In Table 5, we compare number tokenization across different LLMs.</p>
<p>B Tokenization</p>
<p>C Metric</p>
<p>Exact string match is defined as 1 if the output string exactly matches the target string, and 0 otherwise. Then we take the average of exact string match for each task. Char error rate (CER) is defined as the percentage of characters that were incorrectly predicted. We compute CER using Python torchmetrics package. Then we define digit match accuracy as 1 − cer. We include this metric because, for difficult tasks, the exact string match could be identically zero, making it hard to evaluate the performance. In many cases, both GPT-4 and Goat may have very few incorrect digits in the middle of the generated answer, and the number of digits in the generated answer generally matches the target number. What is the answer to {arithmetic}? . . . . . . Table 6: Example templates to fine-tune arithmetic tasks with natural language instructions, generated by ChatGPT. During training, {arithmetic} is replaced by the randomly generated arithmetic expression, like 3425 * 5823.</p>
<p>D Simplified Synthetic Environment</p>
<p>We use the simplified synthetic environment to study the effectiveness of various CoT, by avoiding many hard-to-control aspects of natural languages. The difference between this and Goat is that we use a more structured prompt without any instruction template and a straightforward completion of the task. This enables easy comparison between the model's performance on different tasks, allowing us to examine the learnability of various sub-tasks and explore the effectiveness of the proposed CoT. The input and output examples for the simplified synthetic environment are shown in Table 7.</p>
<p>E Special Cases</p>
<p>In general, multi-digit multiplication and division are considered unlearnable, and we use the decomposition method to solve them. However, some special cases within multi-digit multiplication and division are learnable, and in these cases, we omit CoT and generate the direct answer:</p>
<p>• For multiplication, one of the two numbers contains only one non-zero digit, such as 857483 × 400 = 342993200. This type of task is similar to learnable n-digit by 1-digit multiplication, with the zeros being copied at the end of the product.</p>
<p>• The dividend is equal to the divisor. In that case, the quotient is identically one. For example, 358 ÷ 358 = 1.</p>
<p>• The dividend is less than the divisor. In that case, the quotient is zero and the remainder equals the dividend. For example, 423 ÷ 968 = 0 R 423.</p>
<p>F Dataset</p>
<p>In general, it is difficult to determine the optimal proportion for each task. The number and composition of data samples also depend on the problem settings (see Section 3.6). We empirically find that n-digit by 1-digit multiplication and division may be easier than other tasks, as it requires fewer samples to reach the same level of accuracy as other tasks during task-specific fine-tuning in the simplified synthetic environment. It is noteworthy that the data samples are all randomly generated, so the probability of the occurrence of duplicated samples is very low for large numbers. Therefore, the train-  Table 7: Examples of input and output for training and testing in the simplified synthetic environment, which is used for testing the learnability of sub-tasks and ablation studies. Specifically, "+", "-", "*", and "\" are used for addition, subtraction, multiplication, and division, respectively. Space is inserted between numbers and symbols. The input and output are formatted to mitigate the influence of natural language.  ing loss can reflect the test accuracy on unseen the test set, if the dataset is only trained for one epoch.</p>
<p>Since the synthetic dataset can be generated very easily, we first create a dataset that contains a sufficient number of data samples for training and then observe the training loss and apply early stopping. We observe that the training loss does not show any significant decrease after training on about one million samples. It should be noted that convergence also depends on other hyper-parameters such as batch size and learning rate. Hence, it is recommended to use a dataset larger than what is necessary and terminate the training process when the training loss no longer decreases.</p>
<p>G Ablation Study</p>
<p>We name the steps (shown in the box below) as (1) extraction, (2) split, (3) expansion, (4) product, and (5, 6, . . . ) adding term by term. The ablation study is performed by removing one particular step while keeping other steps unchanged. We exclude the (1) "extraction" and (4) "product" steps from the ablation study as it is crucial for multi-digit multiplication.</p>
<p>Multiplication</p>
<p>Calculate 397 x 4429 \nAnswer:</p>
<p>× 4429</p>
<p>(1) = 4429 × (300 + 90 + 7)</p>
<p>(2) = 4429 × 300 + 4429 × 90 + 4429 × 7</p>
<p>(3)</p>
<p>= 1328700 + 398610 + 31003 (4) = 1727310 + 31003 (5) = 1758313</p>
<p>For division, the ablation study is performed by removing the middle step (bold) that computes the product for all iterations, while keeping other steps unchanged.</p>
<p>Division</p>
<p>What is 8914/64? \nAnswer: </p>
<p>Figure 1 :
1Example of Goat's response on addition, subtraction, multiplication, and division tasks. Prompts are marked in bold, followed by Goat-7B's response.</p>
<p>Figure 5 :
5Composition of tasks in the dataset.</p>
<p>8914 − 64 × 100 = 8914 − 6400 =2514 2514 − 64 × 30 = 2514 − 1920 =594 594 − 64 × 9 = 594 − 576 =18Therefore, 8914 ÷ 64 = 139 R 18</p>
<p>Table 2 :
2Examples of prompts and targets for fine-tuning LLaMA. "\nAnswer: " is appended at the end of each prompt. It should be noted that there are a few special cases when CoT is not required (see Appendix E).</p>
<p>Table 4 :
4Hyperparameters for fine-tuning LLaMA-7B.
H Few-shot Prompting with  Here are two examples showing 3-shot prompting using GPT-4.3-Shot prompting for multiplicationPrompt:Here are 3 examples of question-solution pairs for solving arithmetic multiplication problems. Solve the last question using the same format given in the solutions."question": "235658 * 91204 = ", "solution": "235658 * 91204 = 235658 * (90000 + 1000 + 200 + 4) = 235658 * 90000 + 235658 * 1000 + 235658 * 200 + 235658 * 4 = 21209220000 + 235658000 + 47131600 + 942632 = 21444878000 + 47131600 + 942632 = 21492009600 + 942632 = 21492952232" "question": "59909636 * 6070 = ", "solution": "59909636 * 6070 = 59909636 * (6000 + 70) = 59909636 * 6000 + 59909636 * 70 = 359457816000 + 4193674520 = 363651490520" "question": "7661 * 9461 = ", "solution": "7661 * 9461 = 9461 * (7000 + 600 + 60 + 1) = 9461 * 7000 + 9461 * 600 + 9461 * 60 + 9461 * 1 = 66227000 + 5676600 + 567660 + 9461 = 71903600 + 567660 + 9461 = 72471260 + 9461 = 72480721" "question": "58061 * 3569 = ",GPT-4:To solve the multiplication problem of 58061 multiplied by 3569 in the same format, we'll distribute 58061 across the sum of the digits of 3569 multiplied by their respective places. So, "solution": "58061 * 3569 = 58061 * (3000 + 500 + 60 + 9) = 58061 * 3000 + 58061 * 500 + 58061 * 60 + 58061 * 9 = 174183000 + 29030500 + 3483660 + 522549 = 203213500 + 3483660 + 522549 = 206697160 + 522549 = 207219709"GPT-4:To solve the division problem of 262122 divided by 591 in the same format, we'll subtract multiples of 591 from 262122 until we reach a number less than 591, then that's our remainder. So, "solution": "262122 -591 * 400 = 262122 -236400 = 25722\n25722 -591 * 40 = 25722 -23640 = 2082\n2082 -591 * 3 = 2082 -1773 = 309\nTherefore, 262122 / 591 = 443 R 309"
Pythia: A suite for analyzing large language models across training and scaling. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, O&apos; Kyle, Eric Brien, Mohammad Aflah Hallahan, Shivanshu Khan, Purohit, Edward Usvsn Sai Prashanth, Raff, arXiv:2304.01373arXiv preprintStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>GPT-NeoX-20B: An opensource autoregressive language model. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach, 10.18653/v1/2022.bigscience-1.9Proceedings of BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models. BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language ModelsAssociation for Computational Linguisticsvirtual+DublinSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Puro- hit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An open- source autoregressive language model. In Proceed- ings of BigScience Episode #5 -Workshop on Chal- lenges &amp; Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. Sébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe- ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general in- telligence: Early experiments with gpt-4.</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.12588arXiv preprintWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reason- ing for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Vicuna: An open-source chatbot impressing gpt-4 with 90%<em> chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%</em> chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Eric Fedus, Xuezhi Li, Mostafa Wang, Dehghani, arXiv:2210.11416Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprintHyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168arXiv preprintKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, arXiv:2211.10435Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprintLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Gra- ham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Xinyang Geng, Arnav Gudibande, Hao Liu, Blog post. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal- lace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic re- search. Blog post, April, 1.</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adap- tation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Have you seen that number? investigating extrapolation in question answering models. Jeonghwan Kim, Giwon Hong, Kyung-Min Kim, Junmo Kang, Sung-Hyon Myaeng, 10.18653/v1/2021.emnlp-main.563Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsJeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. 2021. Have you seen that number? investigating extrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7031-7037, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.11916arXiv preprintTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large lan- guage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Recursion of thought: Divide and conquer reasoning with language models. Soochan Lee, Gunhee Kim, Soochan Lee and Gunhee Kim. 2023. Recursion of thought: Divide and conquer reasoning with language models.</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, arXiv:2212.10535arXiv preprintPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. 2022. A survey of deep learn- ing for mathematical reasoning. arXiv preprint arXiv:2212.10535.</p>
<p>Evaluating transformer language models on arithmetic operations using number decomposition. Matteo Muffo, Aldo Cocco, Enrico Bertino, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources AssociationMatteo Muffo, Aldo Cocco, and Enrico Bertino. 2022. Evaluating transformer language models on arith- metic operations using number decomposition. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 291-297, Mar- seille, France. European Language Resources Asso- ciation.</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin, arXiv:2102.13019arXiv preprintRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2021. Investigating the limitations of transform- ers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. arXiv preprintMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratch- pads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>. OpenAI. 2023. Gpt-4 technical report. OpenAI. 2023. Gpt-4 technical report.</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc- tions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprintBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal- ley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.</p>
<p>Jing Qian, Hong Wang, Zekun Li, Shiyang Li, Xifeng Yan, arXiv:2208.05051Limitations of language models in arithmetic and symbolic induction. arXiv preprintJing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. 2022. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051.</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, H Stephen, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Teven Le Stiegler, Arun Scao, Raja, arXiv:2110.08207arXiv preprintVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training en- ables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Angela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ilić, Roman Hesslow, Alexandra Sasha Castagné, François Luccioni, Yvon, arXiv:2211.05100Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, El- lie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Are emergent abilities of large language models a mirage?. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, arXiv:2304.15004arXiv preprintRylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language mod- els a mirage? arXiv preprint arXiv:2304.15004.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, arXiv:2302.04761Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprintTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, R Adam, Adam Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615arXiv preprintAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, GitHub repositoryRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. GitHub repos- itory.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applica- tions. arXiv preprint arXiv:2201.08239.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and effi- cient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, A Noah, Daniel Smith, Hannaneh Khashabi, Hajishirzi, arXiv:2212.10560arXiv preprintYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al- isa Liu, Noah A Smith, Daniel Khashabi, and Han- naneh Hajishirzi. 2022. Self-instruct: Aligning lan- guage model with self generated instructions. arXiv preprint arXiv:2212.10560.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Sub-task decomposition enables learning in sequence to sequence tasks. Noam Wies, Yoav Levine, Amnon Shashua, arXiv:2204.02892arXiv preprintNoam Wies, Yoav Levine, and Amnon Shashua. 2022. Sub-task decomposition enables learning in sequence to sequence tasks. arXiv preprint arXiv:2204.02892.</p>
<p>Baize: An open-source chat model with parameter-efficient tuning on self-chat data. Canwen Xu, Daya Guo, Nan Duan, Julian Mcauley, arXiv:2304.01196arXiv preprintCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196.</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, arXiv:2304.02015arXiv preprintZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do large lan- guage models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015.</p>
<p>Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, Zhang You, arXiv:2303.14070arXiv preprintLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. 2023. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070.</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, arXiv:2205.01068Mona Diab, Xian Li, Xi Victoria LinarXiv preprintet al. 2022. Opt: Open pre-trained transformer language modelsSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, arXiv:2205.10625Quoc Le, and Ed Chi. 2022aarXiv preprintDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reason- ing in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Model Number Tokenization LLaMA 74815. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, arXiv:2211.09066Teaching algorithmic reasoning via incontext learning. 74arXiv preprint7 [1, 29871, 29955] GPT-4 74815 [20338, 868] 7481 [20338, 16] 748 [20338] , 39373, 996] 7481 [2, 406, 34490] 748 [2, 39373] 74 [2, 5243] 7 [2, 406] Pythia 74815 [24, 2385, 1010] GPT-NeoX-20B 7481 [24, 34474] MPT-7B 748 [24, 2385Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. 2022b. Teaching algorithmic reasoning via in- context learning. arXiv preprint arXiv:2211.09066. Model Number Tokenization LLaMA 74815 [1, 29871, 29955, 29946, 29947, 29896, 29945] 7481 [1, 29871, 29955, 29946, 29947, 29896] 748 [1, 29871, 29955, 29946, 29947] 74 [1, 29871, 29955, 29946] 7 [1, 29871, 29955] GPT-4 74815 [20338, 868] 7481 [20338, 16] 748 [20338] , 39373, 996] 7481 [2, 406, 34490] 748 [2, 39373] 74 [2, 5243] 7 [2, 406] Pythia 74815 [24, 2385, 1010] GPT-NeoX-20B 7481 [24, 34474] MPT-7B 748 [24, 2385] 74</p>
<p>. Gpt-J 74815, 48246, 1314] GPT-Neo 7481 [22, 40271GPT-J 74815 [48246, 1314] GPT-Neo 7481 [22, 40271]</p>
<p>Comparison of number tokenization of various LLMs. It should be noted that ChatGLM also splits each digit into an individual token. Evaluating ChatGLM's arithmetic abilities will be left as future work. 5Table 5: Comparison of number tokenization of various LLMs. It should be noted that ChatGLM also splits each digit into an individual token. Evaluating ChatGLM's arithmetic abilities will be left as future work.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7774 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7774</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7774</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-273532791</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.17309v2.pdf" target="_blank">Literature Meets Data: A Synergistic Approach to Hypothesis Generation</a></p>
                <p><strong>Paper Abstract:</strong> AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7774.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7774.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPOGENIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HYPOGENIC (data-driven hypothesis generation backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven LLM pipeline that initializes a hypothesis bank from training examples, iteratively evaluates hypotheses on examples, updates hypothesis rewards using an accuracy + exploration term, and generates new hypotheses from a pool of mispredicted examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis generation with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences / NLP (deception detection, AIGC detection, stress detection, persuasive language)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>HYPOGENIC iterative reward-and-update evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Initialize hypotheses from example subset; for each training example select top-k hypotheses, evaluate their predictions to compute accuracy-based reward (plus exploration term), track wrong examples into pool W, and periodically generate new hypotheses from W to add to the bank.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and reward (training accuracy per-hypothesis); used in downstream inference to compute final accuracy and macro F1 on held-out IND/OOD sets</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: % correct predictions; Macro F1: macro-averaged F1 across labels. Hypothesis reward r_i = (sum_{(x_j,y_j) in S_i} I(y_j = ŷ_j))/|S_i| + α * log t / |S_i| (unit: dimensionless score combining accuracy and exploration term)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Held-out IND and OOD splits of DECEPTIVE REVIEWS, constructed AIGC sets (LLAMAGC/GPTGC), DREADDIT, PERSUASIVE PAIRS</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>HYPOGENIC outperforms few-shot by +5.61% average accuracy; when combined with literature methods yields further gains (final literature+data best: +3.37% over HYPOGENIC on average OOD accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Compared against few-shot, zero-shot, literature-only, NotebookLM, HyperWrite; HYPOGENIC outperforms few-shot by 5.61% on average and is outperformed by literature+data combined method by 3.37% on average OOD accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>HYPOGENIC's reward focuses on dataset performance and can undervalue literature-based hypotheses; hyperparameters fixed without exhaustive search (authors report using default hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7774.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPOREFINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HYPOREFINE (literature-driven refinement of data hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension that iteratively refines data-generated hypotheses by alternating a data-driven refinement agent and a literature-based refinement agent, incorporating paper summaries to produce more grounded and generalizable hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (refinement pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Iterative refinement with literature and data</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>When new hypotheses are generated from the wrong-example pool W, they are iteratively refined for max_refine rounds by alternating refinement agents that take as input the previous hypotheses plus either literature summaries or the data pool W; final hypotheses are reinserted into HYPOGENIC for reward-based ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and macro F1 on IND/OOD after multi-hypothesis inference; empirical improvement measured as change in accuracy relative to HYPOGENIC and literature-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: % correct predictions on held-out datasets; Macro F1: macro-averaged F1 across classes. Reported improvements e.g., +3.92% average inference accuracy for some tasks when refining with literature.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same held-out IND/OOD splits for the four social-science tasks (DECEPTIVE REVIEWS, LLAMAGC/GPTGC, DREADDIT, PERSUASIVE PAIRS)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Refining with literature improved inference accuracy for DECEPTIVE REVIEWS, PERSUASIVE PAIRS, and DREADDIT (+3.92% on average) but degraded performance for some AIGC tasks (e.g., −13.64% for GPTGC when refined with literature).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>HYPOREFINE (literature+data iterative refinement) often outperforms HYPOGENIC alone and literature-only methods on OOD generalization, but gains are task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Effectiveness depends on quality/relevance of literature; can degrade performance when literature offers weak or irrelevant insights (as seen for some AIGC detection tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7774.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic OOD/IND evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic evaluation on out-of-distribution (OOD) and in-distribution (IND) datasets with cross-model inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic protocol where generated hypotheses are provided to an LLM which (1) selects the most relevant hypotheses for a new example and (2) performs inference using chain-of-thought; performance is reported on held-out IND and OOD splits using accuracy and macro F1 across 5 random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation protocol for hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multi-hypothesis LLM-based inference on IND and OOD</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each method's hypothesis bank and a test example, prompt the LLM to extract relevant hypotheses and reason (chain-of-thought) to predict the label; repeat across held-out IND and OOD sets and report accuracy and macro F1 averaged over 5 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; Macro-averaged F1</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: % correct predictions on test split (IND or OOD). Macro F1: mean of F1 scores computed per class (unit: percentage). Results reported averaged across 5 random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Held-out IND/OOD splits of DECEPTIVE REVIEWS (Ott et al., 2013), AIGC datasets constructed from WRITINGPROMPTS (Fan et al., 2018) into LLAMAGC/GPTGC, DREADDIT (Turcan & McKeown, 2019), PERSUASIVE PAIRS</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Literature+data combined hypotheses achieved the best OOD performance: +8.97% over few-shot, +15.75% over literature-only, +3.37% over HYPOGENIC (data-only) on average OOD accuracy; few-shot outperformed zero-shot by +6.84% average accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Benchmarked against zero-shot, few-shot, zero-shot hypothesis generation, literature-only, HYPOGENIC, NOTEBOOKLM, HYPERWRITE; literature+data best on OOD in most configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Choice of prompts and final hypothesis bank size (20) influence results; chain-of-thought prompting and seed variability can affect reproducibility; literature corpus is small and manually collected.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7774.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation (utility & novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation of hypothesis utility and novelty (decision-making utility and novelty/nuance studies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two user studies measuring whether generated hypotheses improve human decision-making on AIGC and deception detection, and whether literature-driven and data-driven hypotheses provide novel, distinct information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (human subjects studies with LLM-generated hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human-subject evaluation of LLM outputs (psychology / HCI / NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human assessment of hypothesis utility/novelty</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Controlled human-subject experiments (control vs. experimental) and pairwise novelty annotation with majority voting</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Utility study: 60 participants randomly assigned to control (no hypotheses) or experimental (given 3 LLM-generated hypotheses) groups; each participant labels 14 instances; accuracy compared across groups with t-tests. Novelty study: present pairs (one literature-based, one data-driven) to crowd annotators who judge whether the second adds novel information; final labels by majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human task accuracy (%), p-values from t-tests, frequency-of-use statistics, 5-point helpfulness rating, novelty fraction (percent pairs judged novel)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Human accuracy: % of participant responses matching ground truth. p-value: significance of difference between groups via t-test. Helpfulness scale: 5-point Likert ('Not at all' to 'Extremely helpful'). Novelty label: majority vote across annotators per pair, reported as % novel.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AIGC Detection and Deception Detection tasks using the paper's datasets (see datasets entry)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Utility study: 60 participants (30 control, 30 experimental); after attention checks, usable N reported (control: 26 valid, experimental: 27 valid). Participants paid ~$12/hr and $0.10 per correct instance bonus. Novelty study: 50 hypothesis pairs sampled; 10 participants annotated each task; majority vote used to label novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Providing hypotheses improved human accuracy by +14.19% (58.86% → 73.05%, p=0.01) on AIGC detection and +7.44% (57.14% → 64.58%, p=0.04) on deception detection; 84% (deception) and 80% (AIGC) of literature/data hypothesis pairs judged novel to each other.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>No direct comparison to human-generated hypotheses; the study measured improvement of humans when assisted by LLM-generated hypotheses versus no assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small human sample sizes limit statistical power; top-3 hypotheses for user study were selected manually via ablation and subjective judgment; authors note more large-scale experiments needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7774.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward function (UCB-inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward function used to score hypotheses (accuracy + exploration / UCB-inspired term)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Per-hypothesis reward that combines empirical accuracy on evaluated examples with an exploration bonus proportional to α * log t / |S_i|, inspired by the Upper Confidence Bound (UCB) algorithm, to balance exploitation and exploration when selecting hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using confidence bounds for exploitation-exploration trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied within HYPOGENIC (LLM-driven evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>ML methodology used in LLM-driven hypothesis selection</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation/selection criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>UCB-inspired hypothesis reward</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For hypothesis h_i, maintain the set S_i of examples used to evaluate it; compute reward r_i = (sum_{(x_j,y_j) in S_i} I(y_j = ŷ_j))/|S_i| + α * log t / |S_i| where t is training step and α controls exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hypothesis reward (dimensionless score); used to rank and select hypotheses for generation and retention in bank of size H</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>First term: empirical accuracy (fraction correct) on S_i; second term: exploration bonus proportional to α * log t / |S_i|. α reported as 0.5 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied across the training examples within HYPOGENIC and HYPOREFINE on chosen tasks/datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors used α = 0.5 and other hyperparameters (k=10, w_max=10, bank size 20) and reported empirical gains for HYPOGENIC and combined methods; reward formulation enabled exploration-driven generation of new hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Reward focuses on dataset performance and may undervalue literature-derived hypotheses that generalize but do not improve training-set accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7774.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Redundancy checker (LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based redundancy checker / entailment matrix</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module that checks pairwise redundancy (entailment) among hypotheses using an LLM, recording results in a binary matrix A to eliminate near-duplicate hypotheses before assembling final hypothesis banks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM (same family as used for hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / evaluation tooling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>redundancy elimination / pre-processing</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based pairwise entailment redundancy check</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For a hypothesis bank (size 20), the LLM evaluates each hypothesis pair to determine whether one entails the other; results stored in a 20×20 binary matrix A (1 = redundant/entailing, 0 = not redundant); used to construct a no-redundancy bank H_new.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Binary entailment/redundancy labels per hypothesis pair; used indirectly to influence final bank composition and downstream inference metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>A_{i,j} ∈ {0,1} where 1 indicates LLM judged hypothesis i redundant with hypothesis j (entailment).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied inside literature∪HYPOGENIC and other union procedures across task-specific hypothesis banks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to de-duplicate and rank hypotheses; final bank composition (top n2 from HYPOGENIC plus literature) reported to yield best empirical performance in many OOD settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Redundancy judgments depend on LLM quality and prompts; potential to mistakenly mark complementary hypotheses as redundant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7774.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multiple-hypothesis inference (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple-hypothesis-based inference with Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference protocol where an LLM is provided a bank of hypotheses (size 20) and asked to reason step-by-step (chain-of-thought) about which hypotheses apply to a given instance and then produce a label, enabling interpretable, hypothesis-guided predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / interpretability-guided inference</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>inference procedure</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multi-hypothesis LLM inference with CoT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each test instance, feed the LLM the full hypothesis bank and prompt it to (1) consider relevance of hypotheses to the instance, (2) reason step-by-step (chain-of-thought), and (3) output final label. Repeat across instances and compute accuracy/macro F1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; Macro F1</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: % correct predictions. Macro F1: macro-averaged F1 across classes. Reported over 5 random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Held-out IND/OOD splits for tasks in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Chain-of-thought multi-hypothesis inference used throughout automatic evaluation and human ablation studies; contributed to observed performance gains for literature+data methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Performance depends on prompt details and CoT reliability; CoT can be sensitive to seed and LLM used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7774.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation metrics: Accuracy & Macro F1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy and Macro-averaged F1 score (evaluation metrics used for hypothesis-based inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary automated metrics used to assess how well LLM-generated hypotheses support predictive inference: percent correct (accuracy) and macro-averaged F1 to account for class balance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applies to predictions from GPT-4O-MINI and LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / classification</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Accuracy and macro F1 on held-out IND/OOD test sets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute the percentage of correct labels (accuracy) and the macro-average of per-class F1 scores for predictions made by LLMs when using generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%); Macro-averaged F1 (%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = (correct predictions / total) * 100. Macro F1 = mean(F1_class1, F1_class2, ...)*100. Reported per task and averaged across 5 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Held-out IND and OOD test splits for the paper's tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Key reported quantities: OOD accuracy improvements (e.g., literature+data best: +8.97% over few-shot; +15.75% over literature-only; +3.37% over HYPOGENIC). Macro F1 scores reported in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Accuracy on IND can favor HYPOGENIC (data-tailored hypotheses) while OOD better measures generalizability; macro F1 reported but main focus is OOD accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7774.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets used</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task datasets and constructed corpora used for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of IND/OOD datasets used to evaluate hypothesis generalizability and utility: DECEPTIVE REVIEWS (Ott et al., 2013), AIGC datasets built from WRITINGPROMPTS (Fan et al., 2018) producing LLAMAGC and GPTGC, DREADDIT (Turcan & McKeown, 2019), and PERSUASIVE PAIRS (Pauli et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (datasets used to evaluate LLM-generated hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / social-science classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>datasets/benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Held-out IND/OOD splits and cross-model inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Split IND datasets into train/validation/test and sample OOD instances (different sources, different LLM-generated models) to evaluate generalizability of hypotheses; used both for automatic and human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; Macro F1; human accuracy for assisted/unassisted conditions</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Dataset splits: IND with at least 200 train, 300 test, 300 validation; OOD sample size ≥ 300 instances. Metrics defined as above.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DECEPTIVE REVIEWS (Ott et al., 2013 - Negative deceptive opinion spam), WRITINGPROMPTS (Fan et al., 2018 - Hierarchical neural story generation) used to construct LLAMAGC/GPTGC, DREADDIT (Dreaddit: A reddit dataset for stress analysis in social media), PERSUASIVE PAIRS (dataset by Pauli et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported IND/OOD accuracies and macro F1 per dataset; human studies conducted on Deception Detection and AIGC Detection specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Literature corpus small (up to ~10 papers per task) and manually collected; constructed AIGC datasets may favor detection of particular model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7774.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7774.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines: NOTEBOOKLM & HYPERWRITE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NOTEBOOKLM (Google) and HYPERWRITE (OthersideAI) as literature-based baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Commercial/agent frameworks used as literature-driven baselines for hypothesis generation; found to sometimes produce invalid or irrelevant hypotheses that degrade inference performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NotebookLM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NotebookLM; HyperWrite (commercial tools)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Proprietary</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / literature-based hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>baseline hypothesis generators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Compare hypotheses generated by these tools against literature-only, data-only, and literature+data methods in downstream inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Upload collected literature into NotebookLM or use HyperWrite's hypothesis maker to generate hypotheses, then run the same multiple-hypothesis inference pipeline and report accuracy/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; Macro F1; qualitative validity of generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same automated metrics as main pipeline; authors also qualitatively inspected hypotheses and report invalid/irrelevant outputs for these tools (appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated across the paper's IND/OOD datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>NOTEBOOKLM and HYPERWRITE sometimes generated invalid/irrelevant hypotheses leading to degraded downstream inference performance (see Appendix Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Both commercial tools underperformed compared to HYPOGENIC and literature+data methods in many configurations; they produced some invalid hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Proprietary tools' internal details unavailable; their outputs depend on how literature is uploaded and prompts used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hypothesis generation with large language models. <em>(Rating: 2)</em></li>
                <li>Using confidence bounds for exploitation-exploration trade-offs. <em>(Rating: 2)</em></li>
                <li>Negative deceptive opinion spam. <em>(Rating: 2)</em></li>
                <li>Hierarchical neural story generation. <em>(Rating: 2)</em></li>
                <li>Dreaddit: A reddit dataset for stress analysis in social media. <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
                <li>Measuring and benchmarking large language models' capabilities to generate persuasive language. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7774",
    "paper_id": "paper-273532791",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "HYPOGENIC",
            "name_full": "HYPOGENIC (data-driven hypothesis generation backbone)",
            "brief_description": "A data-driven LLM pipeline that initializes a hypothesis bank from training examples, iteratively evaluates hypotheses on examples, updates hypothesis rewards using an accuracy + exploration term, and generates new hypotheses from a pool of mispredicted examples.",
            "citation_title": "Hypothesis generation with large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "model_size": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "scientific_domain": "Social sciences / NLP (deception detection, AIGC detection, stress detection, persuasive language)",
            "theory_type": "hypothesis",
            "evaluation_method_name": "HYPOGENIC iterative reward-and-update evaluation",
            "evaluation_method_description": "Initialize hypotheses from example subset; for each training example select top-k hypotheses, evaluate their predictions to compute accuracy-based reward (plus exploration term), track wrong examples into pool W, and periodically generate new hypotheses from W to add to the bank.",
            "evaluation_metric": "Accuracy and reward (training accuracy per-hypothesis); used in downstream inference to compute final accuracy and macro F1 on held-out IND/OOD sets",
            "metric_definition": "Accuracy: % correct predictions; Macro F1: macro-averaged F1 across labels. Hypothesis reward r_i = (sum_{(x_j,y_j) in S_i} I(y_j = ŷ_j))/|S_i| + α * log t / |S_i| (unit: dimensionless score combining accuracy and exploration term)",
            "dataset_or_benchmark": "Held-out IND and OOD splits of DECEPTIVE REVIEWS, constructed AIGC sets (LLAMAGC/GPTGC), DREADDIT, PERSUASIVE PAIRS",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "HYPOGENIC outperforms few-shot by +5.61% average accuracy; when combined with literature methods yields further gains (final literature+data best: +3.37% over HYPOGENIC on average OOD accuracy).",
            "comparison_to_human_generated": false,
            "comparison_results": "Compared against few-shot, zero-shot, literature-only, NotebookLM, HyperWrite; HYPOGENIC outperforms few-shot by 5.61% on average and is outperformed by literature+data combined method by 3.37% on average OOD accuracy.",
            "limitations_noted": "HYPOGENIC's reward focuses on dataset performance and can undervalue literature-based hypotheses; hyperparameters fixed without exhaustive search (authors report using default hyperparameters).",
            "uuid": "e7774.0",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HYPOREFINE",
            "name_full": "HYPOREFINE (literature-driven refinement of data hypotheses)",
            "brief_description": "An extension that iteratively refines data-generated hypotheses by alternating a data-driven refinement agent and a literature-based refinement agent, incorporating paper summaries to produce more grounded and generalizable hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "model_size": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "scientific_domain": "Social sciences / NLP",
            "theory_type": "hypothesis (refinement pipeline)",
            "evaluation_method_name": "Iterative refinement with literature and data",
            "evaluation_method_description": "When new hypotheses are generated from the wrong-example pool W, they are iteratively refined for max_refine rounds by alternating refinement agents that take as input the previous hypotheses plus either literature summaries or the data pool W; final hypotheses are reinserted into HYPOGENIC for reward-based ranking.",
            "evaluation_metric": "Accuracy and macro F1 on IND/OOD after multi-hypothesis inference; empirical improvement measured as change in accuracy relative to HYPOGENIC and literature-only baselines.",
            "metric_definition": "Accuracy: % correct predictions on held-out datasets; Macro F1: macro-averaged F1 across classes. Reported improvements e.g., +3.92% average inference accuracy for some tasks when refining with literature.",
            "dataset_or_benchmark": "Same held-out IND/OOD splits for the four social-science tasks (DECEPTIVE REVIEWS, LLAMAGC/GPTGC, DREADDIT, PERSUASIVE PAIRS)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Refining with literature improved inference accuracy for DECEPTIVE REVIEWS, PERSUASIVE PAIRS, and DREADDIT (+3.92% on average) but degraded performance for some AIGC tasks (e.g., −13.64% for GPTGC when refined with literature).",
            "comparison_to_human_generated": false,
            "comparison_results": "HYPOREFINE (literature+data iterative refinement) often outperforms HYPOGENIC alone and literature-only methods on OOD generalization, but gains are task-dependent.",
            "limitations_noted": "Effectiveness depends on quality/relevance of literature; can degrade performance when literature offers weak or irrelevant insights (as seen for some AIGC detection tasks).",
            "uuid": "e7774.1",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Automatic OOD/IND evaluation",
            "name_full": "Automatic evaluation on out-of-distribution (OOD) and in-distribution (IND) datasets with cross-model inference",
            "brief_description": "An automatic protocol where generated hypotheses are provided to an LLM which (1) selects the most relevant hypotheses for a new example and (2) performs inference using chain-of-thought; performance is reported on held-out IND and OOD splits using accuracy and macro F1 across 5 random seeds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "model_size": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "scientific_domain": "Social sciences / NLP",
            "theory_type": "evaluation protocol for hypotheses",
            "evaluation_method_name": "Multi-hypothesis LLM-based inference on IND and OOD",
            "evaluation_method_description": "For each method's hypothesis bank and a test example, prompt the LLM to extract relevant hypotheses and reason (chain-of-thought) to predict the label; repeat across held-out IND and OOD sets and report accuracy and macro F1 averaged over 5 seeds.",
            "evaluation_metric": "Accuracy; Macro-averaged F1",
            "metric_definition": "Accuracy: % correct predictions on test split (IND or OOD). Macro F1: mean of F1 scores computed per class (unit: percentage). Results reported averaged across 5 random seeds.",
            "dataset_or_benchmark": "Held-out IND/OOD splits of DECEPTIVE REVIEWS (Ott et al., 2013), AIGC datasets constructed from WRITINGPROMPTS (Fan et al., 2018) into LLAMAGC/GPTGC, DREADDIT (Turcan & McKeown, 2019), PERSUASIVE PAIRS",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Literature+data combined hypotheses achieved the best OOD performance: +8.97% over few-shot, +15.75% over literature-only, +3.37% over HYPOGENIC (data-only) on average OOD accuracy; few-shot outperformed zero-shot by +6.84% average accuracy.",
            "comparison_to_human_generated": false,
            "comparison_results": "Benchmarked against zero-shot, few-shot, zero-shot hypothesis generation, literature-only, HYPOGENIC, NOTEBOOKLM, HYPERWRITE; literature+data best on OOD in most configurations.",
            "limitations_noted": "Choice of prompts and final hypothesis bank size (20) influence results; chain-of-thought prompting and seed variability can affect reproducibility; literature corpus is small and manually collected.",
            "uuid": "e7774.2",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Human evaluation (utility & novelty)",
            "name_full": "Human evaluation of hypothesis utility and novelty (decision-making utility and novelty/nuance studies)",
            "brief_description": "Two user studies measuring whether generated hypotheses improve human decision-making on AIGC and deception detection, and whether literature-driven and data-driven hypotheses provide novel, distinct information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (human subjects studies with LLM-generated hypotheses)",
            "model_size": "N/A",
            "scientific_domain": "Human-subject evaluation of LLM outputs (psychology / HCI / NLP)",
            "theory_type": "human assessment of hypothesis utility/novelty",
            "evaluation_method_name": "Controlled human-subject experiments (control vs. experimental) and pairwise novelty annotation with majority voting",
            "evaluation_method_description": "Utility study: 60 participants randomly assigned to control (no hypotheses) or experimental (given 3 LLM-generated hypotheses) groups; each participant labels 14 instances; accuracy compared across groups with t-tests. Novelty study: present pairs (one literature-based, one data-driven) to crowd annotators who judge whether the second adds novel information; final labels by majority vote.",
            "evaluation_metric": "Human task accuracy (%), p-values from t-tests, frequency-of-use statistics, 5-point helpfulness rating, novelty fraction (percent pairs judged novel)",
            "metric_definition": "Human accuracy: % of participant responses matching ground truth. p-value: significance of difference between groups via t-test. Helpfulness scale: 5-point Likert ('Not at all' to 'Extremely helpful'). Novelty label: majority vote across annotators per pair, reported as % novel.",
            "dataset_or_benchmark": "AIGC Detection and Deception Detection tasks using the paper's datasets (see datasets entry)",
            "human_evaluation_details": "Utility study: 60 participants (30 control, 30 experimental); after attention checks, usable N reported (control: 26 valid, experimental: 27 valid). Participants paid ~$12/hr and $0.10 per correct instance bonus. Novelty study: 50 hypothesis pairs sampled; 10 participants annotated each task; majority vote used to label novelty.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Providing hypotheses improved human accuracy by +14.19% (58.86% → 73.05%, p=0.01) on AIGC detection and +7.44% (57.14% → 64.58%, p=0.04) on deception detection; 84% (deception) and 80% (AIGC) of literature/data hypothesis pairs judged novel to each other.",
            "comparison_to_human_generated": false,
            "comparison_results": "No direct comparison to human-generated hypotheses; the study measured improvement of humans when assisted by LLM-generated hypotheses versus no assistance.",
            "limitations_noted": "Small human sample sizes limit statistical power; top-3 hypotheses for user study were selected manually via ablation and subjective judgment; authors note more large-scale experiments needed.",
            "uuid": "e7774.3",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reward function (UCB-inspired)",
            "name_full": "Reward function used to score hypotheses (accuracy + exploration / UCB-inspired term)",
            "brief_description": "Per-hypothesis reward that combines empirical accuracy on evaluated examples with an exploration bonus proportional to α * log t / |S_i|, inspired by the Upper Confidence Bound (UCB) algorithm, to balance exploitation and exploration when selecting hypotheses.",
            "citation_title": "Using confidence bounds for exploitation-exploration trade-offs.",
            "mention_or_use": "use",
            "model_name": "Applied within HYPOGENIC (LLM-driven evaluation)",
            "model_size": "N/A",
            "scientific_domain": "ML methodology used in LLM-driven hypothesis selection",
            "theory_type": "evaluation/selection criterion",
            "evaluation_method_name": "UCB-inspired hypothesis reward",
            "evaluation_method_description": "For hypothesis h_i, maintain the set S_i of examples used to evaluate it; compute reward r_i = (sum_{(x_j,y_j) in S_i} I(y_j = ŷ_j))/|S_i| + α * log t / |S_i| where t is training step and α controls exploration.",
            "evaluation_metric": "Hypothesis reward (dimensionless score); used to rank and select hypotheses for generation and retention in bank of size H",
            "metric_definition": "First term: empirical accuracy (fraction correct) on S_i; second term: exploration bonus proportional to α * log t / |S_i|. α reported as 0.5 in experiments.",
            "dataset_or_benchmark": "Applied across the training examples within HYPOGENIC and HYPOREFINE on chosen tasks/datasets",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Authors used α = 0.5 and other hyperparameters (k=10, w_max=10, bank size 20) and reported empirical gains for HYPOGENIC and combined methods; reward formulation enabled exploration-driven generation of new hypotheses.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Reward focuses on dataset performance and may undervalue literature-derived hypotheses that generalize but do not improve training-set accuracy.",
            "uuid": "e7774.4",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Redundancy checker (LLM-based)",
            "name_full": "LLM-based redundancy checker / entailment matrix",
            "brief_description": "A module that checks pairwise redundancy (entailment) among hypotheses using an LLM, recording results in a binary matrix A to eliminate near-duplicate hypotheses before assembling final hypothesis banks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM (same family as used for hypothesis generation)",
            "model_size": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "scientific_domain": "NLP / evaluation tooling",
            "theory_type": "redundancy elimination / pre-processing",
            "evaluation_method_name": "LLM-based pairwise entailment redundancy check",
            "evaluation_method_description": "For a hypothesis bank (size 20), the LLM evaluates each hypothesis pair to determine whether one entails the other; results stored in a 20×20 binary matrix A (1 = redundant/entailing, 0 = not redundant); used to construct a no-redundancy bank H_new.",
            "evaluation_metric": "Binary entailment/redundancy labels per hypothesis pair; used indirectly to influence final bank composition and downstream inference metrics",
            "metric_definition": "A_{i,j} ∈ {0,1} where 1 indicates LLM judged hypothesis i redundant with hypothesis j (entailment).",
            "dataset_or_benchmark": "Applied inside literature∪HYPOGENIC and other union procedures across task-specific hypothesis banks",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Used to de-duplicate and rank hypotheses; final bank composition (top n2 from HYPOGENIC plus literature) reported to yield best empirical performance in many OOD settings.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Redundancy judgments depend on LLM quality and prompts; potential to mistakenly mark complementary hypotheses as redundant.",
            "uuid": "e7774.5",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Multiple-hypothesis inference (CoT)",
            "name_full": "Multiple-hypothesis-based inference with Chain-of-Thought prompting",
            "brief_description": "An inference protocol where an LLM is provided a bank of hypotheses (size 20) and asked to reason step-by-step (chain-of-thought) about which hypotheses apply to a given instance and then produce a label, enabling interpretable, hypothesis-guided predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "model_size": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "scientific_domain": "NLP / interpretability-guided inference",
            "theory_type": "inference procedure",
            "evaluation_method_name": "Multi-hypothesis LLM inference with CoT",
            "evaluation_method_description": "For each test instance, feed the LLM the full hypothesis bank and prompt it to (1) consider relevance of hypotheses to the instance, (2) reason step-by-step (chain-of-thought), and (3) output final label. Repeat across instances and compute accuracy/macro F1.",
            "evaluation_metric": "Accuracy; Macro F1",
            "metric_definition": "Accuracy: % correct predictions. Macro F1: macro-averaged F1 across classes. Reported over 5 random seeds.",
            "dataset_or_benchmark": "Held-out IND/OOD splits for tasks in the paper",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Chain-of-thought multi-hypothesis inference used throughout automatic evaluation and human ablation studies; contributed to observed performance gains for literature+data methods.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Performance depends on prompt details and CoT reliability; CoT can be sensitive to seed and LLM used.",
            "uuid": "e7774.6",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Evaluation metrics: Accuracy & Macro F1",
            "name_full": "Accuracy and Macro-averaged F1 score (evaluation metrics used for hypothesis-based inference)",
            "brief_description": "Primary automated metrics used to assess how well LLM-generated hypotheses support predictive inference: percent correct (accuracy) and macro-averaged F1 to account for class balance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applies to predictions from GPT-4O-MINI and LLAMA-3.1-70B-INSTRUCT",
            "model_size": "GPT-4O-MINI; LLAMA-3.1-70B-INSTRUCT",
            "scientific_domain": "NLP / classification",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Accuracy and macro F1 on held-out IND/OOD test sets",
            "evaluation_method_description": "Compute the percentage of correct labels (accuracy) and the macro-average of per-class F1 scores for predictions made by LLMs when using generated hypotheses.",
            "evaluation_metric": "Accuracy (%); Macro-averaged F1 (%)",
            "metric_definition": "Accuracy = (correct predictions / total) * 100. Macro F1 = mean(F1_class1, F1_class2, ...)*100. Reported per task and averaged across 5 seeds.",
            "dataset_or_benchmark": "Held-out IND and OOD test splits for the paper's tasks",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Key reported quantities: OOD accuracy improvements (e.g., literature+data best: +8.97% over few-shot; +15.75% over literature-only; +3.37% over HYPOGENIC). Macro F1 scores reported in appendices.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Accuracy on IND can favor HYPOGENIC (data-tailored hypotheses) while OOD better measures generalizability; macro F1 reported but main focus is OOD accuracy.",
            "uuid": "e7774.7",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Datasets used",
            "name_full": "Task datasets and constructed corpora used for evaluation",
            "brief_description": "A collection of IND/OOD datasets used to evaluate hypothesis generalizability and utility: DECEPTIVE REVIEWS (Ott et al., 2013), AIGC datasets built from WRITINGPROMPTS (Fan et al., 2018) producing LLAMAGC and GPTGC, DREADDIT (Turcan & McKeown, 2019), and PERSUASIVE PAIRS (Pauli et al., 2024).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "N/A (datasets used to evaluate LLM-generated hypotheses)",
            "model_size": "N/A",
            "scientific_domain": "NLP / social-science classification tasks",
            "theory_type": "datasets/benchmarks",
            "evaluation_method_name": "Held-out IND/OOD splits and cross-model inference",
            "evaluation_method_description": "Split IND datasets into train/validation/test and sample OOD instances (different sources, different LLM-generated models) to evaluate generalizability of hypotheses; used both for automatic and human evaluations.",
            "evaluation_metric": "Accuracy; Macro F1; human accuracy for assisted/unassisted conditions",
            "metric_definition": "Dataset splits: IND with at least 200 train, 300 test, 300 validation; OOD sample size ≥ 300 instances. Metrics defined as above.",
            "dataset_or_benchmark": "DECEPTIVE REVIEWS (Ott et al., 2013 - Negative deceptive opinion spam), WRITINGPROMPTS (Fan et al., 2018 - Hierarchical neural story generation) used to construct LLAMAGC/GPTGC, DREADDIT (Dreaddit: A reddit dataset for stress analysis in social media), PERSUASIVE PAIRS (dataset by Pauli et al., 2024)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Reported IND/OOD accuracies and macro F1 per dataset; human studies conducted on Deception Detection and AIGC Detection specifically.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Literature corpus small (up to ~10 papers per task) and manually collected; constructed AIGC datasets may favor detection of particular model outputs.",
            "uuid": "e7774.8",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Baselines: NOTEBOOKLM & HYPERWRITE",
            "name_full": "NOTEBOOKLM (Google) and HYPERWRITE (OthersideAI) as literature-based baselines",
            "brief_description": "Commercial/agent frameworks used as literature-driven baselines for hypothesis generation; found to sometimes produce invalid or irrelevant hypotheses that degrade inference performance.",
            "citation_title": "NotebookLM",
            "mention_or_use": "use",
            "model_name": "NotebookLM; HyperWrite (commercial tools)",
            "model_size": "Proprietary",
            "scientific_domain": "NLP / literature-based hypothesis generation",
            "theory_type": "baseline hypothesis generators",
            "evaluation_method_name": "Compare hypotheses generated by these tools against literature-only, data-only, and literature+data methods in downstream inference",
            "evaluation_method_description": "Upload collected literature into NotebookLM or use HyperWrite's hypothesis maker to generate hypotheses, then run the same multiple-hypothesis inference pipeline and report accuracy/F1.",
            "evaluation_metric": "Accuracy; Macro F1; qualitative validity of generated hypotheses",
            "metric_definition": "Same automated metrics as main pipeline; authors also qualitatively inspected hypotheses and report invalid/irrelevant outputs for these tools (appendix).",
            "dataset_or_benchmark": "Evaluated across the paper's IND/OOD datasets",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "NOTEBOOKLM and HYPERWRITE sometimes generated invalid/irrelevant hypotheses leading to degraded downstream inference performance (see Appendix Table 9).",
            "comparison_to_human_generated": false,
            "comparison_results": "Both commercial tools underperformed compared to HYPOGENIC and literature+data methods in many configurations; they produced some invalid hypotheses.",
            "limitations_noted": "Proprietary tools' internal details unavailable; their outputs depend on how literature is uploaded and prompts used.",
            "uuid": "e7774.9",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hypothesis generation with large language models.",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "Using confidence bounds for exploitation-exploration trade-offs.",
            "rating": 2,
            "sanitized_title": "using_confidence_bounds_for_exploitationexploration_tradeoffs"
        },
        {
            "paper_title": "Negative deceptive opinion spam.",
            "rating": 2,
            "sanitized_title": "negative_deceptive_opinion_spam"
        },
        {
            "paper_title": "Hierarchical neural story generation.",
            "rating": 2,
            "sanitized_title": "hierarchical_neural_story_generation"
        },
        {
            "paper_title": "Dreaddit: A reddit dataset for stress analysis in social media.",
            "rating": 2,
            "sanitized_title": "dreaddit_a_reddit_dataset_for_stress_analysis_in_social_media"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Measuring and benchmarking large language models' capabilities to generate persuasive language.",
            "rating": 1,
            "sanitized_title": "measuring_and_benchmarking_large_language_models_capabilities_to_generate_persuasive_language"
        }
    ],
    "cost": 0.02179,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Literature Meets Data: A Synergistic Approach to Hypothesis Generation
22 Oct 2024</p>
<p>Haokun Liu haokunliu@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Yangqiaoyu Zhou zhouy1@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Mingxuan Li mingxuanl@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Chenfei Yuan 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Chenhao Tan chenhao@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Literature Meets Data: A Synergistic Approach to Hypothesis Generation
22 Oct 20242D7B3BAAC6C7286A72FEB90112AD0DE1arXiv:2410.17309v1[cs.AI]
AI holds promise for transforming scientific processes, including hypothesis generation.Prior work on hypothesis generation can be broadly categorized into theory-driven and datadriven approaches.While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other.To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation.We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over fewshot, 15.75% over literature-based alone, and 3.37% over data-driven alone).Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection.Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively.These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.</p>
<p>Introduction</p>
<p>"It is the theory that decides what can be observed."</p>
<p>-Albert Einstein Large language models (LLMs) excel at synthesizing information and hold promise for transforming hypothesis generation, a critical yet understudied step in scientific discoveries.Many recent studies recognize this potential and use LLMs to generate hypotheses (e.g., Yang et al., 2024b;Batista and Ross, 2024).We broadly categorize them into theory-driven and data-driven methods.* Equal contributions.</p>
<p>On one hand, theory-driven approaches leverage LLMs to review existing literature and generate novel hypotheses (Yang et al., 2024b;Baek et al., 2024).These methods have shown promising results in terms of the hypotheses' novelty, validity, and usefulness to researchers, while remaining grounded in established human knowledge (Si et al., 2024).However, they come with notable limitations: they require high-quality literature, struggle to adapt to new data, and lack empirical support.Data-driven approaches, on the other hand, propose hypotheses by discovering patterns in data (Zhou et al., 2024;Qiu et al., 2024).These hypotheses are data-adaptive and can exhibit strong performance in explaining the data.However, they could be too overly tailored to the specific datasets used, which can hinder their generalizability.</p>
<p>We hypothesize that theory can guide the discovery from data and propose to integrate literaturebased and data-driven hypothesis generation (see Figure 1).For the data-driven component, we use HYPOGENIC as the backbone (Zhou et al., 2024).HYPOGENIC leverages an LLM to initialize hypotheses from a small number of examples and then updates them iteratively to improve the quality of hypotheses.To enhance this process with literature insights, we introduce a literature-based hypothesis agent.This agent interacts with the data-driven hypothesis agent (HYPOGENIC), refining and maintaining a shared pool of hypotheses through continuous collaboration, ensuring that the hypotheses benefit from both data-driven adaptability and the grounding of existing scientific knowledge.In addition to the refinement approach, we also propose to directly unionize literature-based and data-driven hypotheses.</p>
<p>To comprehensively evaluate these hypotheses, we conduct automatic and human evaluation to assess their generalizability, utility, and novelty.We apply our method to address research questions in social sciences: deception detection, AI generated content (AIGC) detection, mental stress detection, and persuasive argument prediction.Automatic evaluation results show that integrating literature and data outperforms other baselines: 8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone in accuracy on out-ofdistribution datasets, a measure of generalizability.</p>
<p>Moreover, we conduct the first study to assess the utility of AI-generated hypotheses in improving human decision-making and show that our generated hypotheses improve human accuracy by 7.44% and 14.19% on deception detection and AIGC detection.Additionally, we find that literature-based and data-driven hypotheses complement each other, as one set often contains novel information not found in the other set.</p>
<p>In addition to our own implementation, we also use commercial ones such as NOTEBOOKLM (Google, 2024) and HYPERWRITE (OthersideAI, 2024) as baselines.</p>
<p>Data-Driven Hypothesis Generation</p>
<p>Our data-driven hypothesis generation largely follows HYPOGENIC in Zhou et al. (2024).Here we give a brief overview.During the initialization stage of HYPOGENIC, an LLM is prompted with a set of initial data instances D init from the training set D and asked to generate initial hypotheses that forms the initial hypothesis bank H D .</p>
<p>In the update stage, for each example s ∈ D, top k high-reward hypotheses from H D are selected and each used to prompt the LLM to make a prediction on s.The accuracy and reward of the k hypotheses are updated accordingly.Among the k hypotheses, if at least w hyp predicted wrong on s, s is added to a wrong examples pool W. Once the size of W reaches w max , a set of new hypotheses are generated from W and added to H D according to their reward.Inspired by the upper confidence bound (UCB) algorithm (Auer, 2003), the reward function of HYPOGENIC is defined as follows:
r i = (x j ,y j )∈S i I(y j = ŷj ) |S i | + α log t |S i | ,
where S i is the set of examples used to evaluate hypothesis h i , t is the training time step, and α is the reward coefficient that controls the exploration term of the reward function.</p>
<p>Integration of Literature-based and Data-driven Hypotheses</p>
<p>One main contribution of our work is proposing the first approach to integrating literature-based and data-driven hypothesis generation so that we can effectively leverage the strengths of each approach, increasing the generalizability and utility of generated hypotheses.We consider two strategies.</p>
<p>Refinement of literature-based hypotheses.HYPOREFINE integrates paper summaries from § 2.1 with HYPOGENIC.In the initialization stage of HYPOGENIC, an LLM is asked to generate initial hypotheses based on both a set of initial examples and paper summaries relevant to the task.</p>
<p>In the update stage, we propose an iterative refinement approach to integrate patterns from data and key findings from literature into new hypotheses.Specifically, each time HYPOGENIC generates a set of new hypotheses H 0 from the wrong examples pool W, these hypotheses are refined multiple rounds by a data-driven refinement agent and a literature-based refinement agent.Take R M as the refinement agent based on model M, each time H 0 is generated from the wrong examples pool W, it is iteratively refined as follows:
H i = R M (q i , H i−1 , P) if i mod 2 = 0 R M (q i , H i−1 , W) if i mod 2 = 1,
where P represents the literature information, W represents the data pool used to generate H 0 , and q being the queries.After max_refine rounds of refinement, the final hypothesis bank H max_refine is fed back to the HYPOGENIC pipeline.</p>
<p>The reward function and update process for the hypothesis bank H remain consistent with those of the original HYPOGENIC.</p>
<p>Union and redundancy elimination.As the reward function of HYPOGENIC focuses only on the hypotheses' performance on the datasets at hand, literature-based hypotheses are sometimes undervalued during the update stage.On occasions they can even be replaced by hypotheses that have especially good performances on data but are not necessarily generalizable on real-world tasks.To counter this issue, we use a union approach to mechanistically combine literature-based and data-based hypotheses.We first generate two hypothesis banks: one literature-based hypothesis bank and another bank using HYPOGENIC or HYPOREFINE.Then we build a redundancy checker to remove hypotheses that express overly similar or repeating information in each bank.Lastly, we construct the final hypothesis bank of size n by randomly choosing</p>
<p>Experiments</p>
<p>In this section, we introduce our evaluation framework and the tasks to operationalize it.</p>
<p>Evaluation Framework</p>
<p>Formally evaluating hypotheses requires rigorous protocols and vast amounts of resources.In this work, we mainly evaluate our generated hypotheses along two dimensions: utility and novelty.We perform both automatic and human evaluations to show that our generated hypotheses can help models and humans in challenging real-world classification tasks and bring novel information.</p>
<p>Automatic evaluation on out-of-distribution (OOD) and in-distribution (IND) datasets and cross-model inference.Since we work with classification tasks, a natural way of evaluating the hypotheses is prompting the LLMs to do inference with the hypotheses.For all the methods that generate hypotheses, we provide a new data example and all the generated hypotheses to the LLMs.Then we prompt the LLMs to first extract the most relevant hypotheses to the new example and make inference using the hypotheses.For detailed information about the prompts, please refer to Appendix A. For each task, we report accuracy and F1 scores on held-out OOD and IND sets for 5 different random seeds.Since we are most interested in the generalizability of the generated hypotheses, we focus on performance on the OOD set in the main paper.</p>
<p>In addition to predicting on out-of-distribution datasets, we test our hypotheses' generalizability by taking the hypotheses generated by one model and performing inference with another model.</p>
<p>Human evaluation on utility and novelty.We design human studies to assess the practical utility and novelty of the generated hypotheses.Specifically, we aim to (1) evaluate whether these hypotheses have meaningful impact on human decisionmaking and (2) determine whether data-driven and literature-driven hypotheses indeed offer distinct perspectives and contribute novel information to the unionized hypotheses pool.We run human studies on Deception Detection and AIGC Detection.Screenshots of the studies are in Appendix C. We pay participants at an average hourly rate of $12.</p>
<p>Human Study I: Utility in human decisionmaking.We recruit 60 participants on prolific.comand randomly assign them into experimental and control groups.The control group performs the task without hypotheses, while the experiment group is given a set of three generated hypotheses to complete the same task.Specifically, each participant is randomly assigned 14 instances, and we include attention check questions to ensure the quality of the collected responses.We evaluate the practical utility of our generated hypotheses by comparing the performance of the two groups.</p>
<p>We pick the hypotheses based on their impact on performance in an ablation setting.Specifically, we choose the top three hypotheses that cause the greatest drop in performance when removed from the hypotheses pool during multi-hypothesis inference.In addition, to motivate participants to perform at their best, we offer a bonus of $0.1 for each correctly predicted instance.</p>
<p>At the end of the study, participants in the experiment group are also asked to give overall ratings and an assessment of the given hypotheses.There are five scales: "Not at all helpful", "Slightly helpful", "Moderately helpful", "Very helpful", and "Extremely helpful".</p>
<p>Human study II: Novelty and nuance.To compare data-driven hypotheses and literature-driven hypotheses, we present one hypothesis of each type to participants and ask them to judge whether the second hypothesis provides meaningfully novel information that is not covered in the first hypothesis.</p>
<p>We sample 50 pairs of hypotheses (h 1 , h 2 ), one from literature-based and one from data-driven, with duplications removed within each group.We recruit 10 Prolific participants to annotate whether h 2 provides new information to h 1 for each pair.Each participant is randomly assigned to annotate 15 pairs.For each pair, we take the majority vote to determine the final novelty label.</p>
<p>Tasks</p>
<p>We consider four tasks in social sciences.Deception Detection is a widely studied problem in psychology and other social sciences (Granhag and Vrij, 2005).We use the dataset introduced by Ott et al. ( 2013) (DECEPTIVE REVIEWS), which consists of 800 genuine hotel reviews and 800 fake hotel reviews, as our IND dataset.For the OOD dataset, we use hotel reviews from different source websites and different cities (Li et al., 2013).AI-Generated Content (AIGC) Detection has attracted significant attention in recent years (Tang et al., 2023).Most existing works focus on developing black-box detection methods and rarely take interpretability into account (Wu et al., 2024).We thus build our own dataset for this task.We take 800 distinct prompts and human-written stories in the WRITINGPROMPTS dataset (Fan et al., 2018).Then we use the same prompts to generate AIwritten stories with LLAMA-3.1-70B-INSTRUCT(Dubey et al., 2024) and GPT-4O-MINI (OpenAI, 2023), constituting our LLAMAGC and GPTGC datasets.The IND data contains stories generated by the corresponding model.The stories generated by the other model are treated as OOD data.Mental Stress Detection from social media content is an important task in mental health (Lupien et al., 2009).We use DREADDIT, a corpus of lengthy Reddit posts with stress status labels developed by Turcan and McKeown (2019).The dataset contains 3.5k post segments annotated using Amazon Mechanical Turk, with labels indicating the presence or absence of stress in posts.Our IND and OOD sets are separated based on subreddits that the posts come from.Persuasive Argument Prediction examines persuasion and social interactions to reveal predictive cues of persuasiveness (Tan et al., 2016).We use PERSUASIVE PAIRS, a dataset with pairs of short texts constructed by Pauli et al. (2024).Within each pair of texts, one is from existing corpora with signals of persuasiveness, while the other one is generated by an LLM with instructions for it to be more/less persuasive than the one from existing corpora.We formulate this task as predicting the more persuasive one of each pair of texts.The dataset contains human-annotated ground-truth labels and is pre-processed by removing examples where there exists disagreement among annotators.The IND and OOD datasets are then created based on different original sources of texts.</p>
<p>For each task, we split the IND dataset with at least 200 examples in train set, 300 in test set (on which we perform inference), 300 in validation set, and sample at least 300 instances from OOD (see Appendix B.1 for more details).</p>
<p>Implementation and Baselines</p>
<p>Our method works with any LLM (M).We use GPT-4O-MINI and LLAMA-3.1-70B-INSTRUCT in this work.Throughout this paper, we refer to GPT-4O-MINI as "GPT-4-MINI" and LLAMA-3.1-70B-INSTRUCT as "LLAMA-70B-I".We compare our method with the following baselines.</p>
<ol>
<li>
<p>Zero-shot and few-shot prompting.We give the LLMs detailed task instructions (zero-shot) and optionally provide three demonstrating examples (few-shot).This approach does not involve any hypothesis.</p>
</li>
<li>
<p>Zero-shot hypothesis generation.Inspired by Qi et al. (2023), we provide specific task descriptions and instructions, and then we prompt the LLMs to generate hypotheses directly without incorporating literature or data.</p>
</li>
<li>
<p>Literature-driven hypothesis generation.We use the implementation in §2.1.In addition to our own implementation, we compare two of the recently released agent frameworks for scientific writing, NOTEBOOKLM (Google, 2024) and HYPERWRITE (OthersideAI, 2024)</p>
</li>
</ol>
<p>Results</p>
<p>We first present automatic evaluation results to demonstrate the utility of generated hypotheses for model inference.We then show that the generated hypotheses can improve human decision-making in challenging tasks and that literature-based and datadriven hypotheses provide unique insights from each other.</p>
<p>Automatic Evaluation</p>
<p>Hypotheses generated by combining information from literature and data achieves the best performance across all task and model configurations (Table 1).First, few-shot inference outperforms zero-shot inference for all task and model configurations, with an average improvement of 6.84% in accuracy.In addition, few-shot inference surpasses zero-shot generation and the best of literature-based methods on average accuracy by 7.21% and 6.78%, respectively, suggesting off-theshelve LLMs or literature alone does not generate effective hypotheses for predictive purposes.In fact, NOTEBOOKLM and HYPERWRITE can generate some invalid or irrelevant hypotheses, which degrades their inference performance (see Table 9 in Appendix).</p>
<p>In contrast, HYPOGENIC consistently outperforms few-shot inference, improving average accuracy by 5.61%, highlighting the advantage of data-driven hypotheses.Compared to few-shot inference, the hypotheses also offer more interpretable insights.Furthermore, our best hypothesis generation method combining literature and data outperforms HYPOGENIC by 3.37% on average (i.e., 8.97% over few-shot and 15.75% over literature-based methods), demonstrating the benefit of incorporating literature with data.</p>
<p>For DECEPTIVE REVIEWS, PERSUASIVE PAIRS, and DREADDIT, refining the hypotheses with literature consistently improves inference accuracy compared to HYPOGENIC, with a 3.92% improvement on average.On the other hand, refining the hypotheses with literature does not help with GPTGC and LLAMAGC, but the union of HYPOGENIC and hypotheses generated from literature consistently performs the best.Comparing with HYPOGENIC for these two tasks, refining the hypotheses with literature actually results in an accuracy drop by 13.64%.This is likely due to that the literature for AIGC detection has relatively few insights on interpretable features to detect AI generated contents, and refining the data-driven hypotheses with that information degrades performance.</p>
<p>To further illustrate our approach, we present a case study of our generated hypotheses in Table 3.For most cases, LITERATURE-ONLY and HYPOGENIC generate different hypotheses as in Case I: one is about first-person singular pronouns, while the other one is about past experiences.We include more details on the differences between hypotheses generated by different methods in § 4.2.More examples of hypotheses generated using LIT-ERATURE∪HYPOREFINE are in Table 8.</p>
<p>Under some cases, the methods can generate similar hypotheses, and HYPOREFINE improves the quality of the hypothesis.In Case II, all three hypotheses focus on balanced perspectives being indicative of truthful reviews.HYPOREFINE incorporates the "reviews that seem to be promoting a competitor" insight from LITERATURE-ONLY, while also capturing the emphasis on "lack of nuance" from HYPOGENIC.By doing so, HYPORE-FINE offers a more nuanced hypothesis that not only explains how deceptive reviews may manipulate reader emotions, but also provides specific examples to illustrate how balanced perspectives can contribute to truthful assessments.This combination of insights from literature and data allows HYPOREFINE to offer a more comprehensive and   1, the hypotheses generated from both literature and data performs the best on all methods for OOD datasets.</p>
<p>Generated hypotheses can be effectively transferred to a different model.To further check the generalization behavior of our generated hypotheses, we take the hypotheses from the bestperforming method with our literature+data approach and then use the other model to perform inference.Table 2 shows that the generated hypotheses from one model remain effective for the other model, the performance exhibits no significant change in most cases (&lt;3% in 10 out of 20 cases).Even with this performance drop, our methods still outperform the few-shot inference baseline by 3.76% and 3.66% on OOD and IND settings.This finding further demonstrates the robustness of our hypothesis generation method and hypothesisbased inference.A significant outlier case is for LLAMAGC OOD: when using LLAMA-70B-I-generated hypotheses for GPTGC (OOD) and ask GPT-4-MINI to perform hypothesis-based inference, the inference per-formance can degrade significantly.This can be due to innate deficits in the task setting, as LLMs tend to favor and better detect their own writing (Panickssery et al., 2024).</p>
<p>Human Evaluation</p>
<p>Generated hypotheses improve human decisionmaking in both AIGC Detection and Deception Detection.In AIGC Detection, the average human accuracy improves by 14.19% (58.86% → 73.05%) when we provide hypotheses as assistance.We perform a statistical t-test and obtain a p-value of 0.01, indicating that the improvement is significant.In Deception Detection, the introduction of hypotheses boosts human accuracy by 7.44% (57.14% → 64.58%), with a p-value of 0.04.</p>
<p>When hypotheses are present, participants would use them to assist decision-making for over 90% of the time.All three presented hypotheses are selected to be used with frequency greater than 30% (Table 4, Table 5 in the Appendix).For example, the most used hypothesis, with frequency of 44.55%, in AIGC detection is "Human-written texts tend to have a more conversational tone and colloquial language, while AI-generated texts tend to be more formal and lack idiomatic expressions."For both tasks, 100% of the participants find the hypotheses to be helpful, and over 40% find them to be "Very helpful" or "Extremely helpful".</p>
<p>Humans rate literature-based and data-driven hypotheses as distinct.We determine the novelty label based on majority vote from three human annotators.84% of the pairs are considered novel to each other for Deception Detection, and 80% are considered novel for AIGC Detection, demonstrating the complementarity between literature-based and data-driven approaches.</p>
<p>Case I: LITERATURE-ONLY and HYPOGENIC generate different hypotheses LITERATURE-ONLY: Deceptive reviews often contain a higher frequency of first-person singular pronouns, while truthful reviews may use these pronouns less frequently.HYPOGENIC: Reviews that reference the reviewer's previous experiences with the hotel brand or similar hotels are more likely to be truthful, while reviews that do not provide any context or comparison to past experiences are more likely to be deceptive.</p>
<p>Case II: LITERATURE-ONLY and HYPOGENIC generate similar hypotheses LITERATURE-ONLY: Truthful reviews often provide a balanced perspective, while deceptive reviews may seem overly promotional or biased towards a competitor.HYPOGENIC: Reviews that express a balanced perspective, mentioning both positive and negative aspects of the stay, are more likely to be truthful, whereas reviews that are overly positive or negative without nuance tend to be deceptive.HYPOREFINE: Reviews that present a balanced perspective by discussing both positive and negative aspects of the stay, particularly with specific examples (e.g., "The location was fantastic, but the air conditioning was broken"), are more likely to be truthful, while reviews that are excessively positive or negative without acknowledging any redeeming qualities (e.g., "This is the best hotel ever!" or "I will never stay here again!") tend to be more deceptive, as they may reflect an attempt to manipulate reader emotions rather than provide an honest assessment.</p>
<p>Related Work</p>
<p>Theory-driven hypothesis generation.Yang et al. (2024b) generates hypotheses from raw web corpus, but their method requires human annotated hypotheses from literature.Baek et al. (2024), Wang et al. (2024), and Ghafarollahi and Buehler (2024) use LLMs to create knowledge graph and generate hypotheses from existing literature.We implement our own literature-based generation because these papers either do not provide sufficient implementation details or require significant effort to adapt to new tasks.2024) uses LLMs to generate hypotheses and conducts comprehensive experiments to study human engagements with headlines.We choose HYPOGENIC as the backbone for data-driven hypothesis generation as their tasks are most similar to ours, and their approach to hypothesis updates integrates naturally into our refinement process.</p>
<p>Data</p>
<p>Automated scientific research with LLMs.</p>
<p>There is growing interest in developing LLMpowered methods and multi-agent frameworks to assist scientific research.Lu et al. (2024) designs an LLM agent to generate full research papers.Li et al. (2024) proposes a method to generate research ideas from existing literature and automatically implement and execute experiments.In contrast, our work focuses primarily on hypothesis generation, as we believe it is crucial to preserve human agency and oversight in the scientific research process.</p>
<p>To evaluate LLM generated hypotheses, Qi et al. ( 2023) examines whether they contain novel information not found in existing literature.Si et al. (2024) asks experts to rate the novelty of LLMproposed research ideas in the NLP domain.While these studies highlight LLMs' ability to generate novel hypotheses, they do not conduct human subject experiments to validate the effectiveness of hypotheses.To this end, we conduct the first human study to test the utility of LLM-generated hypotheses in supporting human decision-making.</p>
<p>Significant efforts have also been made to benchmark multi-agent frameworks on data analysis tasks (Majumder et al., 2024;Gu et al., 2024;Hu et al., 2024;Chen et al., 2024;Huang et al., 2024;Guo et al., 2024), literature processing and informa-</p>
<p>Hypotheses</p>
<p>Frequency of Selection</p>
<p>Hypothesis 1: AI-generated texts tend to use more elaborate and descriptive language, including adjectives and adverbs, to create a sense of atmosphere and immersion.Human-written texts, on the other hand, tend to be more concise and straightforward in their language use.</p>
<p>38.79%</p>
<p>Hypothesis 2: Human-written texts are more likely to contain errors or idiosyncrasies in grammar and punctuation, reflecting the natural imperfections of human writing, while AI-generated texts typically maintain a higher level of grammatical accuracy.</p>
<p>34.55%</p>
<p>Hypothesis 3: Human-written texts tend to have a more conversational tone and colloquial language, while AI-generated texts tend to be more formal and lack idiomatic expressions.</p>
<p>44.55%</p>
<p>No hypothesis selected 3.94%</p>
<p>Table 4: How often participants use hypotheses in AIGC Detection.We allow users to select multiple hypotheses for each instance they make prediction on, so the total frequency can exceed 100%.</p>
<p>tion retrieval tasks (Press et al., 2024;Ajith et al., 2024;Kang and Xiong, 2024;Zhang et al., 2024), and more general research tasks (Tian et al., 2024;Jansen et al., 2024).</p>
<p>Conclusion</p>
<p>We propose a novel approach that integrates literature and data to generate hypotheses, with extensive and systematic evaluations.Our method consistently outperforms all baselines, including existing literature-based and data-driven approaches.Furthermore, human evaluations reveal that our generated hypotheses also improve human decisionmaking in challenging tasks.</p>
<p>Limitations</p>
<p>Our automated evaluation uses two recent models on datasets across various domains, showing the effectiveness of our method across diverse settings.However, we did not further evaluate our hypotheses on some tasks that require representations beyond natural language, such as math problem solving and code generation.The literature corpus used for literature-based hypothesis generation is limited in terms of size and collection method.The collection is carried out by manually searching and collecting up to 10 papers on Semantic Scholar or Google Scholar.Though with the limited literature corpus we already show that our methods yield competent performance, a natural future direction is to enhance the literature component with automatic and scalable retrieval.</p>
<p>Similarly, we achieved satisfactory performance across different models and tasks with the initial set of hyperparameters.However, we did not perform an exhaustive hyperparameter search, which may have yielded further enhancements to the performance of our methods.This represents a limitation of our study that could be addressed in future work.</p>
<p>Our experiments with human subjects is a proof of concept.The number of participants in our human evaluation is relatively small.As a result, we do not believe that we have the statistical power to distinguish, for example, the difference between HYPOGENIC and HYPOREFINE.Although this is not the focus of our study, we encourage future work to conduct large-scale experiments in focused domains to validate the hypotheses generated through human-AI collaboration.</p>
<p>Last but not least, we manually chose three hypotheses through ablation-style study and subjective judgment for experiments with human subjects.We believe this process is the essence of human-AI collaboration in future scientific processes.It requires future exploration to identify the optimal collaboration regime.</p>
<p>A Prompts</p>
<p>All our prompts for LLMs are separated into system prompts and user prompts.System prompts contain role and tone information, followed by detailed descriptions of the task and the expected response format.User prompts contain useful information for hypothesis generation, refinement, or inference, including information from literature, instances from datasets, and previously generated hypotheses.Below are some examples of the prompts that we use for each task.</p>
<p>A.1 Deception Detection</p>
<p>System Prompt You're a professional hotel review analyst.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given examples, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided reviews.System Prompt You're a professional hotel review analyst.Given some key findings from a series of research papers, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given relevant literatures, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided reviews.</p>
<p>System Prompt</p>
<p>You're a social scientist working on a project to identify deceptive hotel reviews.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a review is truthful or deceptive.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided prevailing patterns in data if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 4: Hypothesis Refinement Based on Data.</p>
<p>System Prompt You're a social scientist working on a project to identify deceptive hotel reviews.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given relevant literatures, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided reviews.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a review is truthful or deceptive.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided key findings if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].Refined hypotheses:</p>
<p>Example 5: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt You are a professional deceptive detection agent and your job is to determine whether a hotel review is truthful or deceptive.In other words, we want to know whether the review is written by someone who had real experiences with the hotel.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current hotel review, and also predict whether the current hotel review is truthful or deceptive.Give an answer.The answer should be one word ( truthful or deceptive).</p>
<p>Give your final answer in the format of {Final answer: answer} User Prompt Our learned patterns: <generated_hypotheses> A hotel review is the following: <review> Given the pattern you learned above, give an answer of whether the hotel review above is deceptive or truthful.Think step by step.First step: Think about which pattern can be applied to the hotel review.Second step: Based on the pattern, is this hotel review deceptive or truthful?Final step: give your final answer in the format of {Final answer: answer} Example 6: Multiple-Hypothesis-Based Inference.</p>
<p>A.2 AI-Generated Content (AIGC) Detection</p>
<p>System Prompt You are a professional writer.You will be given a story prompt.Your task is to write a coherent and creative story based on the prompt.Ensure the story has a clear structure, engaging characters, and stays true to the prompt' s core idea.Keep the language clear and appropriate to the story's tone.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a piece of text is written by human or AI.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided prevailing patterns in data if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 12: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt</p>
<p>You are an AI generated content detection agent and want to determine whether a piece of text is written by a human or generated by an AI.In other words, we want to know whether the text is written by a human or generated by AI.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current text, and also predict whether the current text is written by human or AI.</p>
<p>Give an answer. The answer should be one word (AI or HUMAN).</p>
<p>Give your final answer in the format of "Final answer: ANSWER" User Prompt Our learned patterns: <generated_hypotheses> New text: Here is a story: <story> Given the patterns you learned above, give an answer of whether the current text is written by human or AI.Think step by step.First step: Think about which pattern can be applied to the story.Second step: Based on the pattern, is this story written by human or AI?You must give your final answer in the format of " Final answer: ANSWER".</p>
<p>Example 13: Multiple-Hypothesis-Based Inference.</p>
<p>A.3 Mental Stress Detection</p>
<p>System Prompt You're a psychologist and social scientist studying people's stress and their online posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.System Prompt You're a psychologist and social scientist studying people's stress and their online posts.Given some key findings from a series of research papers, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.</p>
<p>Using the given relevant literatures, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided posts.We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for deciding people's stress status (has stress or no stress) based on reddit post.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 17: Hypothesis Refinement Based on Data.</p>
<p>System Prompt You're a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.</p>
<p>Using the given relevant literatures, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided posts.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for deciding people's stress status (has stress or no stress) based on reddit post.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].Refined hypotheses:</p>
<p>Example 18: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt You're a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current reddit post, and also predict whether the poster of the reddit post has stress or not based on the content of the post.Give an answer.The answer should be "has stress" or "no stress".</p>
<p>Give your final answer in the format of {Final answer: answer} User Prompt Our learned patterns: <generated_hypotheses> A reddit post is the following: <post> Given the pattern you learned above, give an answer of whether the poster of the reddit post has stress or not based on the content of the post.</p>
<p>Think step by step.First step: Think about which pattern can be applied to the reddit post.Second step: Based on the pattern, does the poster of a reddit post has stress or not?Answer should be "has stress" or "no stress".</p>
<p>A.4 Persuasive Argument Prediction</p>
<p>System Prompt You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of observations of the format: Argument 1: [argument_1] Argument 2: [argument_2] Observation: The first/second argument uses more persuasive language.Based on the observations, please generate hypotheses that are useful for explaining why one argument uses more persuasive language than the other.</p>
<p>Proposed hypotheses:</p>
<p>Example 20: Data-Based Hypothesis Generation with HypoGeniC.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of literature of the format: Title: [title] Key Findings: [summary] Based on the literature, please generate hypotheses that are useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that you can find in the literature.They should also be generalizable to new instances.Please propose <num_hypotheses> refined hypotheses and generate them in the format of 1. [hypothesis ], 2. [hypothesis], ... <num_hypotheses>.[ hypothesis].</p>
<p>User Prompt</p>
<p>Here are some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• Please generate hypotheses that can help determine which argument uses more persuasive language.Please propose <num_hypotheses> possible hypotheses.</p>
<p>Generate them in the format of 1. [hypothesis], 2.</p>
<p>[hypothesis], ... <num_hypotheses>.[hypothesis].</p>
<p>Proposed hypotheses:</p>
<p>Example 21: Literature-Based Hypothesis Generation.</p>
<p>System Prompt You are a helpful assistant for summarizing key findings in research papers on a given topic.</p>
<p>User Prompt Summarize the following research paper, focusing ONLY on this question: What characterizes texts that use more persuasive language?In other words, how can one determine which one of two sentences uses more persuasive language?Focus on hypotheses of what characterizes texts that use more persuasive language, do not include technical details in the paper.... literature texts here ...</p>
<p>Example 22: Paper Summarization.</p>
<p>System Prompt You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of observations of the format: Argument 1: [argument_1] 2: [argument_2] Observation: The first/second argument uses more persuasive language.Based on the observations, please refine hypotheses provided to make them more useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that occur across the provided examples.They should also be generalizable to new instances.Please propose <num_hypotheses> refined hypotheses and generate them in the format of 1. [</p>
<p>Refined hypotheses:</p>
<p>Example 23: Hypothesis Refinement Based on Data.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of literature of the format: ••• information from literature here ••• Based on the literature, please refine hypotheses provided to make them more useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that you can find in the literature.</p>
<p>Refined hypotheses:</p>
<p>Example 24: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.From past experiences, you learned some patterns.Now, at each time, you should apply the learned patterns to a new pair of arguments and determine which one uses more persuasive language.The answer for the more persuasive language should be of the form "the _ argument" where _ is either first or second.Please give your final answer in the format of { Final answer: the _ argument uses more persuasive language} User Prompt Our learned patterns: <generated_hypotheses> Given the patterns you learned above, determine which of the following arguments uses more persuasive language: Argument 1: <first_argument> Argument 2: <second_argument> Think step by step.</p>
<p>Step 1: Think about which learned patterns can be applied to the arguments.</p>
<p>Step 2: Analyze the difference between "Argument 1" and "Argument 2".</p>
<p>Step 3: Based on the pattern, which argument uses more persuasive language?You MUST give your final answer in the following format: Final answer: the _ argument uses more persuasive language.</p>
<p>Example 25: Multiple-Hypothesis-Based Inference.</p>
<p>B Automated Experiments</p>
<p>Implementation Details  (Goffredo et al., 2023), Persuasion For Good (Wang et al., 2020), and Webis-Clickbait-17 (Potthast et al., 2018), while OOD dataset is from PT-Corpus (Da San Martino et al., 2019).</p>
<p>B.2 Specificity Boost</p>
<p>We further observed that sometimes the solely literature-based hypotheses generated by gpt-4omini are often too short and brief, making it harder to apply during inference.After removing redundancies of hypothesis banks, we unite two hypothesis banks to create a final bank H f inal with a balanced prioritization strategy.We first move the top 10 hypotheses from the HYPOGENIC or HYPOREFINE hypothesis bank to H f inal .If there is less than 10 hypotheses in the banks, we move all hypotheses to H f inal .Then we randomly choose hypotheses from the literature-based hypothesis bank until the size of f inal reaches 20.</p>
<p>B.4 Multiple-Hypothesis Inference Implementation</p>
<p>During multiple-hypothesis based inference, each time we feed a LLM with our final hypothesis bank of size 20 (see Appendix B.5) and an instance of our IND or OOD datasets with labels removed.The LLM is asked to generate an answer for the given instance using Chain-of-Thought prompting (Wei et al., 2022) that considers both the relevance of the hypotheses to the given instance and the utility of the hypothesis bank (see Appendix A for the exact prompts we used).For F1 scores, we report the macro-averaged F1 scores.</p>
<p>B.5 Technical Details of NotebookLM and HyperWrite</p>
<p>NotebookLM is an LLM-powered research assistance tool that generates source-grounding responses to user prompts.Specifically in our case, collected literature are uploaded in the Note-bookLM interface, followed by a hypothesis generation prompt asking to generate hypotheses based on given literature.Given its functionality and our usage, it is placed under the literature-based hypothesis generation category in our evaluations.</p>
<p>For HyperWrite, we use its Hypothesis Maker function, which is an AI-driven tool that generates hypotheses based on a given research question.Though there is no publicly available technical report for this tool, it generally leverages LLM's pretraining knowledge and literature information to produce hypotheses.</p>
<p>B.6 Hyperparameters</p>
<p>We use the same set of hyperparameters across all tasks, models, and methods.</p>
<p>During the training stage of HypoGeniC, the limit of the hypothesis bank size H is set to 20, and the size of training set is set to 200.In the initialization stage, we set num_init = 10.In the update stage, we use reward coefficient α = 0.5, w max = 10, k = 10, and generate 1 hypothesis per update.</p>
<p>In our HYPOREFINE method, the round of refinement max_refine is set to 6.</p>
<p>We use 5 random seeds for multiple-hypothesis inference: 11376, 8271, 39660, 543, 3.</p>
<p>Across all tasks and methods and for both GPT-4o-mini and Llama-3.1-70B-Instruct,we use temperature = 1 × 10 −5 and max_tokens = 4000.</p>
<p>B.7 Licensing Details</p>
<p>DECEPTIVE REVIEWS is released under CC BY-NC-SA 3.0, and PERSUASIVE PAIRS is released under CC BY-NC 4.0.The WRITINGPROMPTS dataset which we use to create the AIGC Detection datasets are under MIT License.The LLA-MAGC and GPTGC datasets will be released under the same licensing as this work, CC BY 4.0 License, should it be accepted.DREADDIT and FOUR-CITIES do not have licenses specified in their original papers, but are considered under CC BY 4.0 and CC BY-NC-SA 3.0 license respectively as they are ACL materials.</p>
<p>For the LLMs, GPT-4-MINI is a proprietary and not released under any open-source license, while LLAMA-70B-I is released under Llama 3.1 Community License Agreement.</p>
<p>Throughout our study, we find that we are in compliance with the licensing agreements of all the datasets and models used in this work.</p>
<p>B.8 Estimated Cost</p>
<p>For LLAMA-70B-I, we run all of our experiments with 4 NVIDIA A100s, and it takes on average 1.5 hours to run all of our hypothesis generation pipelines, including HYPOGENIC, HYPORE-FINE, LITERATURE∪HYPOGENIC , and LITER-ATURE∪HYPOREFINE .With GPT-4-MINI, the average cost for running the same pipelines is $0.6.</p>
<p>C Human Study Details C.1 Decision-making Utility Study Details</p>
<p>The instructions of the practical relevance study can be found in Figure 4 and Figure 6.For the interface, we present an example of the control group interface for Deception Detection in Figure 5, and examples of the experiment group interface in Figure 7.</p>
<p>The subjects of the control group are instructed to perform deception detection or AIGC detection tasks without any assistance from the hypotheses.Subjects in the experiment group are asked to first read the presented 3 hypotheses and then make their predictions on the given instance.They are then required to choose which ones, if any, of the hypotheses that were used in their prediction.At the end of the study, participants in the experiment group are also asked to give overall rating and assessment</p>
<p>Hypotheses</p>
<p>Frequency of Selection Hypothesis 1: Reviews present a balanced perspective by detailing both positive and negative experiences with specific examples (e.g., "the room was spacious and clean, but the noise from the street was disruptive at night") are more likely to be truthful, whereas reviews that express extreme sentiments without acknowledging any redeeming qualities (e.g., "everything was perfect" or "it was a total disaster") are more likely to be deceptive.</p>
<p>50.00%</p>
<p>Hypothesis 2: Reviews that mention specific dates of stay or unique circumstances surrounding the visit (e.g., "We stayed during the busy Memorial Day weekend and faced long lines") are more likely to be truthful, while reviews that use vague temporal references (e.g., "I stayed recently") without concrete details are more likely to be deceptive, as they often lack the specificity that suggests a real and engaged experience.</p>
<p>34.44%</p>
<p>Hypothesis 3: Reviews that provide detailed sensory descriptions of the hotel experience, such as the specific decor of the room, the quality of bedding, and the overall ambiance (e.g., "the room featured luxurious furnishings, highthread-count sheets, and soft lighting that created a relaxing atmosphere") are more likely to be truthful, while reviews that use vague or overly simplistic descriptors (e.g., "the hotel was nice and comfortable") are more likely to be deceptive.</p>
<p>46.39%</p>
<p>No hypothesis selected 7.50%</p>
<p>Table 5: How often humans use hypotheses in Deception Detection human study.We allow users to select multiple hypotheses for each instance they make prediction on, so the total frequency can exceed 100%.</p>
<p>of the helpfulness of the given hypotheses.There are five scales: "Not at all helpful", "Slightly helpful", "Moderately helpful", "Very helpful", and "Extremely helpful".</p>
<p>We choose top 3 hypotheses from the hypothesis bank generated using LITERA-TURE∪HYPOREFINE that cause the greatest drop in performance when removed from the hypotheses pool during multi-hypothesis inference.The chosen hypotheses for Deception Detection and AIGC Detection can be found in Table 4 and  Table 5.</p>
<p>We recruit 30 participants for the control group and 30 for the experimental group.For the control group, 4 people timed out, and 25 out of the remaining 26 participants passed attention checks.For the experimental group, 3 people timed out, and 22 out of the remaining 27 passed attention checks.We compute human accuracy based on responses from people who finished tasks in time and passed attention checks.The average time spent is around 25 minutes and participants are timed out by the system if they spend more than 60 minutes in the study, which can happen when they accidentally leave the study website tab open but forget to do the task.</p>
<p>C.2 Novelty and Nuance Study Details</p>
<p>For the Novelty and Nuance Study, we present the instructions for AIGC Detection in Figure 2. We showcase the interfaces for AIGC Detection in Figure 3.</p>
<p>For both Deception Detection and AIGC Detection, the two hypothesis banks compared are generated using LITERATURE-ONLY and HYPOGENIC respectively.</p>
<p>We recruit 10 participants each task and all particpants passed attention the check question.</p>
<p>C.3 IRB</p>
<p>We received IRB exempt (and will provide study number in the non-anonymous version of the paper).For both of the human studies, we present a detailed description of the study, incentives, risks and benefits, confidentiality, and contacts &amp; questions in our consent form.The study proceeds only if the participant agrees to give consent.</p>
<p>D Examples of Generated Hypotheses and Qualitative Analysis</p>
<p>We include examples of generated hypotheses using our LITERATURE∪HYPOREFINE approach and GPT-4-MINI, together with a brief qualitative analysis of its source in Table 8.We also showcase example hypotheses generated using NOTE-Table 7: Accuracy and F1 scores on the held-out IND datasets.Literature + data outperforms all other methods in 7 out of 10 configurations.For LLAMA-70B-I on GPTGC, LLAMAGC, and PERSUASIVE PAIRS, HYPOGENIC performs the best.This is likely due to that the literature in these tasks do not offer helpful information for the IND data, but they can still provide useful information for the tasks in general.As in Table 6, our approaches with literature + data performs the best in all configurations for the OOD datasets.</p>
<p>Dataset Generated Hypothesis</p>
<p>Literature Source/Novel DECEPTIVE REVIEWS Deceptive reviews often contain a higher frequency of first-person singular pronouns, while truthful reviews may use these pronouns less frequently.Li et al. (2014) The use of repetitive phrasing across multiple reviews is a strong indicator of deception, while truthful reviews are more likely to exhibit unique language and perspectives.</p>
<p>Maurya et al. (2022)</p>
<p>Reviews that provide specific accounts of the checkin and check-out processes, including exact times, the names of staff members involved, and descriptions of any unique features or services utilized (e.g., "I used the self-check-in kiosk at 3 PM"), are more likely to be truthful.Conversely, reviews that mention issues like long wait times or check-in problems without contextual details or specific examples (e.g., "the check-in took too long") are more likely to be deceptive.</p>
<p>Novel (from data)</p>
<p>GPTGC and LLAMAGC AI-generated content may struggle with maintaining coherence over longer passages, while human writing typically maintains clarity and focus.Posts that reflect on personal struggles with mental health or addiction (e.g., "I was a severe addict") are more likely to indicate that the poster has stress, while posts that discuss academic or professional experiences without emotional turmoil (e.g., "I've explained the aforementioned to people") are more likely to indicate that the poster does not have stress.</p>
<p>Novel (from data) PERSUASIVE PAIRS Persuasive texts that incorporate rhetorical devices, such as rhetorical questions and direct appeals, are more likely to engage the reader and compel them to consider the writer's viewpoint.</p>
<p>Wagemans (2023)</p>
<p>Texts that utilize strong, action-oriented verbs are generally more persuasive, as they convey confidence and urgency, compelling the audience to take action.</p>
<p>Novel (from data)</p>
<p>Arguments that include a clear and compelling call to action are more persuasive, as they provide the audience with a specific next step to take, reinforcing the urgency and importance of the message.</p>
<p>Novel (from data)</p>
<p>Table 8: Examples of generated hypotheses using our method accompanied by labels indicating their sources.For hypotheses from literature, we include the specific paper, while for hypotheses that are not explicitly suggested by our literature base, we set the label to "Novel (from data)".</p>
<p>Method</p>
<p>Figure 1 :
1
Figure 1: Illustration of how we combine literature-based and data-driven hypotheses.See algorithmic details in § 2.</p>
<p>n2</p>
<p>hypotheses from the literature-based hypothesis bank and adding the top n 2 hypotheses from the other hypothesis bank based on training accuracies.For detailed information of the implementation, please refer to Appendix B.3.</p>
<p>-driven hypothesis generation.Besides HYPOGENIC, we review additional works on discovering unseen patterns from data.Zhong et al. (2023) discovers patterns by analyzing difference between large corpora.Pham et al. (2024) makes discovery by generating and refining interpretable topics.Romera-Paredes et al. (2024) uncovers new solutions in open math problems by iteratively updating programs.Qiu et al. (2024) and Yang et al. (2024a) evaluate LLMs' ability in performing inductive reasoning in synthetic settings.Batista and Ross (</p>
<p>Using the given examples, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided reviews.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.User Prompt We have seen some hotel reviews: ••• more examples here •••</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.</p>
<p>Using the given examples, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided posts.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.User PromptWe have seen some reddit posts: ••• more examples here ••• Please generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.Propose <num_hypotheses> possible hypotheses.Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].Proposed hypotheses: Example 14: Data-Based Hypothesis Generation with HypoGeniC.</p>
<p>a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.Using the given examples, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided posts.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.User Prompt We have seen some reddit posts: ••• more examples here •••</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.</p>
<p>Final step: give your final answer in the format of {Final answer: answer} Example 19: Multiple-Hypothesis-Based Inference.</p>
<p>Figure 2 :
2
Figure 2: Instruction page for novelty check.</p>
<p>Figure 3 :
3
Figure 3: Annotation page for novelty check.</p>
<p>Figure</p>
<p>Figure Instruction page for prediction task without hypotheses.</p>
<p>Figure 5 :
5
Figure 5: Annotation page for prediction task without hypotheses.</p>
<p>Figure 6 :
6
Figure 6: Instruction page for prediction task with the guide of hypotheses.</p>
<p>Figure 7 :
7
Figure 7: Annotation page for prediction task with the guide of hypotheses.</p>
<p>reviews are more likely to be written in a style and tone that aligns with the reviewer's demographic information available on the platform, if any.<strong> Conversely, deceptive reviews might exhibit inconsistencies between the writing style and the reviewer's claimed demographic, signaling a potential fabrication.</strong>Truthful reviews are more likely to be posted at various times and days, reflecting the organic behavior of genuine guests.<strong>Conversely, deceptive reviews, particularly those orchestrated by paid posters, might be posted in clusters or at unusual times, indicating a coordinated effort.</strong>Truthful reviews are more likely to be written in a way that aligns with the overall sentiment expressed in the review's star rating.<strong>Conversely, deceptive reviews might show inconsistency between the sentiment expressed in the written content and the assigned star rating, indicating a potential attempt to manipulate perception.HYPERWRITE </strong>Relevant Images:<strong> Truthful reviews are more likely to include relevant images.Deceptive reviews less likely to include images.</strong>First-Person Pronouns:<strong> Truthful reviews use first-person pronouns (I, my).Deceptive reviews use third-person (one).</strong>Overly Formal Language:** Deceptive reviews use overly formal language.Truthful reviews use conversational tone.</p>
<p>Table 2 :
2
Cross-model inference performance.Performance on IND held-out datasets.Similarly with Table1, our hypothesis generation methods utilizing literature and data information are able to achieve the best accuracy and F1 scores in most cases on the held-out IND datasets (see Table 7 in the Appendix).For some cases, such as using Llama on the IND datasets for GPTGC, LLA-
explanatory hypothesis.
MAGC, and PERSUASIVE PAIRS, HYPOGENIC gets the top performance compared to other methods.This is not surprising, since HYPOGENIC generates hypotheses by looking at the IND data examples only.In contrast, our methods that take information from both literature and data may generate hypotheses that are more generally applicable but with slightly worse performance on the IND data, whereas in Table</p>
<p>Table 3 :
3
Examples of generated hypotheses from different methods.We show cases where LITERATURE-ONLY and HYPOGENIC generate different hypotheses or similar hypotheses, and how HYPOREFINE combines them in the case if they express unifiable ideas.</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.Example 2: Literature-Based Hypothesis Generation.What is useful for one to decide whether a review is truthful or deceptive in real life?Focus on hypotheses of what kind of reviews tend to be deceptive, do not include technical details in the paper.... literature texts here ...
User PromptWe have some key findings from a series ofresearch papers that might be useful forgenerating the required <num_hypotheses>.hypotheses:••• information from literature here •••Please generate hypotheses that are useful forpredicting whether a review is truthful ordeceptive.When generating hypotheses, remember not tooveruse your own knowledge. Always refer to thekey findings from research papers provided.Directly cite passages in the key findings whengenerating a hypothesis.Propose <num_hypotheses> possible hypotheses.Remember to generate <num_hypotheses> hypotheses!Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].Proposed hypotheses:System PromptYou are a helpful assistant for summarizing keyfindings in research papers on a given topic.User PromptSummarize the following research paper, focusingONLY on this question: Example 3: Paper Summarization.</p>
<p>Example 11: Hypothesis Refinement Based on Data.
list multiple constructs so if there are manyexamples.things changing , pick one);v. usable (i.e., a human equipped with thisGenerate refined hypotheses in the format of 1. [insight could use it to predict if a new piece ofhypothesis], 2. [hypothesis], ... <num_hypotheses>.text is generated AI in a similar way)[hypothesis].The hypotheses should analyze what kind of text isProposed hypotheses:likely to be written by human or AI.Example 8: Data-Based Hypothesis Generation withUser PromptHypoGeniC.We have seen some texts: ••• more examples here •••We have some hypotheses need to be refined:... hypotheses to be refined here ...System PromptPlease refine these hypotheses to make them moreYou're a professional AI content detector.specific and useful for predicting whether a pieceGiven some key findings from a series of researchof text is written by human or AI.papers, we want to generate hypotheses that areWhen refining the hypotheses, feel free to changeuseful for detecting whether a piece of text isthe key information or topic of a hypothesis basedwritten by human or AI.on the provided prevailing patterns in data ifUser Prompt you think it is necessary.Your task is to identify what patterns or traits... story-writing prompt here ... Generate refined hypotheses in the format of 1. [show up more in AI generated texts, and what showsexample: hypothesis], 2. [hypothesis], ... <num_hypotheses>.up more in human written texts. Focus on the[ WP ] You 've been able to read minds since you [hypothesis].generalizable insight that can be applied in otherturned 7 . Mostly you watch people 's thoughts Refined hypotheses:contexts. Ignore things that are specific to thispassively and undetected but one day someone talksstory. Do not make references this story they mayback .\nnot be for others.Using the given relevant literatures, pleaseExample 7: AIGC Detection Dataset Generation.propose <num_hypotheses> possible hypothesis pairs.System PromptYou're a an AI generated content detection expert.These hypotheses should identify specific patternsSystem Prompt You are great at detecting what type of text isthat occur across the provided texts.You're a an AI generated content detection expert. generated by AI.You are great at detecting what type of text is Given a set of texts, we want to generateGenerate them in the format of 1. [hypothesis], 2.generated by AI. hypotheses that are useful for predicting whether[hypothesis], ... <num_hypotheses>. [hypothesis].Given a set of texts, we want to generate a piece of text is generated by AI. In other words,The hypotheses should analyze what kind of text ishypotheses that are useful for predicting whether we want to know whether the text is written by alikely to be written by human or AI.a piece of text is generated by AI. In other words, human or generated by AI.we want to know whether the text is written by aUser Prompthuman or generated by AI. Using the given relevant literatures, refine theWe have some key findings from a series ofhypothesis pairs provided.research papers that might be useful forYour task is to identify what patterns or traits The desired hypotheses should identify specificgenerating the required <num_hypotheses>show up more in AI generated texts, and what shows patterns that occur across the provided texthypotheses:up more in human written texts. Focus on the examples.••• information from literature here •••generalizable insight that can be applied in otherPlease generate hypotheses that are useful forcontexts. Ignore things that are specific to this Generate refined hypotheses in the format of 1. [predicting whether a piece of text is written ofstory. Do not make references this story they may hypothesis], 2. [hypothesis], ... <num_hypotheses>.human or AI.not be for others. [hypothesis].Propose <num_hypotheses> possible hypotheses.The hypotheses should analyze what kind of text isRemember to generate <num_hypotheses> hypotheses!Using the given examples, please propose likely to be written by human or AI.Generate them in the format of 1. [hypothesis], 2.<num_hypotheses> possible hypothesis pairs.[hypothesis], ... <num_hypotheses>. [hypothesis].When proposing hypothesis, look closely into theProposed hypotheses:given examples and identify specific patterns thatoccur across the provided text examples.Example 9: Literature-Based Hypothesis Generation.The hypotheses should be clear, easy to understand, and have specific details such that one can applythe hypotheses to predict whether a piece of textSystem Promptis written by human or AI.You are a helpful assistant for summarizing key findings in research papers on a given topic.Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>. [hypothesis].User Prompt Summarize the following research paper, focusingThe hypotheses should analyze what kind of text is likely to be written by human or AI.ONLY on this question: What is useful for one to detect whether some text is generated by AI? Focus on hypotheses of what kind of text tend to be generated by AI, do not include technical details in the paper. ... literature texts here ...User Prompt We have seen some texts: ... more examples here ... Please generate hypotheses that are useful for predicting predicting whether a piece of text is written by human or AI.Example 10: Paper Summarization.Propose <num_hypotheses> possible hypotheses. Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].System PromptWhen proposing hypothesis, look closely into theYou're a an AI generated content detection expert.given examples and identify specific patterns thatYou are great at detecting what type of text isoccur across the provided text examples.generated by AI.Given a set of texts, we want to generatePlease make sure that the hypotheses are:hypotheses that are useful for predicting whetheri. clear (i.e., precise , not too wordy , and easya piece of text is generated by AI. In other words,to understand);we want to know whether the text is written by aii. generalizable to novel situations (i.e., theyhuman or generated by AI.would make sense if applied to other AI generatedcontent detection experiments or other messagingUsing the given examples, refine the hypothesiscontexts);pairs provided.iii. empirically plausible (i.e., this is aThe desired hypotheses should identify specificdimension on which messages can vary on);patterns that occur across the provided textiv. unidimensional (i.e., avoid hypotheses that</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.Example 15: Literature-Based Hypothesis Generation.
User PromptWe have some key findings from a series ofresearch papers that might be useful forgenerating the required <num_hypotheses>hypotheses:••• information from literature here •••Please generate hypotheses that are useful fordeciding people's stress status (has stress or nostress) based on reddit post.Propose <num_hypotheses> possible hypotheses.Remember to generate <num_hypotheses> hypotheses!Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].Proposed hypotheses:System PromptYou are a helpful assistant for summarizing keyfindings in research papers on a given topic.User PromptSummarize the following research paper, focusingONLY on this question: What is useful for one tojudge whether a reddit poster has stress based onone of their reddit post content?Focus on hypotheses of what kind of posts indicatestress, do not include technical details in thepaper.... literature texts here ...</p>
<p>These hypotheses should identify patterns, phrases, wordings etc. that occur across the provided examples.They should also be generalizable to new instances.Please propose <num_hypotheses> possible hypotheses and generate them in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].
User PromptHere are the Observations:••• more examples here •••Please generate hypotheses that can help determinewhich argument uses more persuasive language.Please propose <num_hypotheses> possiblehypotheses.Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].</p>
<p>Each time we take the best-performing hypothesis h out of the original hypothesis bank H and check if there exists a hypothesis h new in H new such that redundancy is recorded in A for the pair h and h new , i.e., A h,hnew = 1 or A hnew,h = 1.If yes, h is moved out of the original bank H and skipped; if not, h is moved to H new with a rank determined by its training accuracy.
specificity booster is not applied to Llama-3.1-70B-Instruct because it can already generate reasonablyspecific hypotheses.B.3 Refinement and Union ImplementationRefinement is implemented as an extensionbased on the original HypoGeniC pipeline. Dur-ing the initialization stage, an LLM is instructedto generate an initial hypothesis bank H based ona set of initial examples S init and a series of gen-erated paper summaries. These initial hypothesesare then evaluated and re-ranked using the samereward function as in HypoGeniC. In the updatestage, once the size of the wrong examples bankW reaches w max , 1 new hypothesis is generated byfeeding both the wrong examples bank and papersummaries to the LLM. H is then updated with thenew hypothesis according to the reward, followingthe same procedure as HypoGeniC.Union and Redundancy Elimination is imple-mented by combining the hypothesis bank gener-ated using HYPOGENIC or HYPOREFINE and thebank generated by our literature-based hypothe-sis generation method. We first generate the twohypothesis banks separately using HYPOGENIC,HYPOREFINE, and LITERATURE-ONLY, followingthe procedures described above and in Section 3.Each hypothesis bank is then fed to a redundancychecker module. For a hypothesis bank of size20, the LLM-based redundancy checker checkseach pair of hypotheses and see if one entails theother, with results recorded as a 20 × 20 matrix Aof 1 (redundant) or 0 (not redundant). To createthe new no-redundancy hypothesis bank H new , wefirst rank the hypotheses based on their trainingaccuracy.To address this, we adda LLM-based specificity booster after the literature-based hypothesis generation that adds more con-crete illustrations and examples to each of the hy-potheses based solely on its pre-training knowledge.Specifically we apply the specificity booster onour Deception Detection, Mental Stress Detection,and Persuasive Argument Prediction tasks. The</p>
<p>Tang et al. (2023)AI-generated texts are more likely to follow conventional narrative structures, while human-written texts may experiment with form and structure.Novel (from data)DREADDITPosts that show erratic posting behavior or changes in tone (e.g., from positive to negative) are more likely to indicate stress, while consistent posting patterns with a stable tone are more likely to indicate no stress.
Wan and Tian (2024)Posts that exhibit avoidance behaviors (e.g., avoidingDoan et al. (2017)social situations or responsibilities) are more likely toindicate stress, while posts that demonstrate proactiveengagement with challenges are more likely to indicateno stress.</p>
<p>Table 9 :
9
Examples of generated hypotheses using NOTEBOOKLM and HYPERWRITE on DECEPTIVE REVIEWS that are invalid or irrelevant, leading to degraded inference performance for these methods.</p>
<p>AcknowledgmentsWe thank members of the Chicago Human+AI Lab for their helpful comments.We also thank the anonymous participants on Prolific for participating in our study.Methods
Lit-Search: A retrieval benchmark for scientific literature search. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao, arXiv:2407.189402024Preprint</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. Peter Auer, J. Mach. Learn. Res. 32003</p>
<p>ResearchAgent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024Preprint</p>
<p>Words that work: Using language to generate hypotheses. M Rafael, James Batista, Ross, 2024</p>
<p>ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, arXiv:2410.050802024Preprint</p>
<p>Fine-grained analysis of propaganda in news article. Giovanni Da, San Martino, Seunghak Yu, Alberto Barrón-Cedeño, Rostislav Petrov, Preslav Nakov, Proceedings of EMNLP-IJCNLP. EMNLP-IJCNLP2019</p>
<p>How do you #relax when you're #stressed? a content analysis and infodemiology study of stress-related tweets. Son Doan, Amanda Ritchart, Nicholas Perry, Juan D Chaparro, Mike Conway, 10.2196/publichealth.59392017JMIR Public Health and Surveillance3e35</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, arXiv:2407.217832024Preprint</p>
<p>Hierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, Proceedings of ACL. ACL2018</p>
<p>Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024Preprint</p>
<p>Argument-based detection and classification of fallacies in political debates. Pierpaolo Goffredo, Mariana Chaves, Serena Villata, Elena Cabrio, 10.18653/v1/2023.emnlp-main.684Proceedings of EMNLP. EMNLP2023</p>
<p>. Google, 2024NotebookLM</p>
<p>Deception detection. Psychology and law: An empirical perspective. 2005Pär Anders Granhag and Aldert Vrij</p>
<p>BLADE: Benchmarking language model agents for data-driven science. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, M Tianmai, Lanyi Zhang, Mike A Zhu, Jeffrey Merrill, Tim Heer, Althoff, arXiv:2408.096672024Preprint</p>
<p>DS-Agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, Proceedings of ICML. ICML2024</p>
<p>InfiAgent-DABench: Evaluating agents on data analysis tasks. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, Fei Wu, Proceedings of ICML, Proceedings of Machine Learning Research. ICML, Machine Learning Research2024</p>
<p>MLAgentBench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Proceedings of ICML. ICML2024</p>
<p>DISCOVERYWORLD: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, arXiv:2406.067692024Preprint</p>
<p>Hao Kang, Chenyan Xiong, arXiv:2406.10291ResearchArena: Benchmarking llms' ability to collect and organize information as research agents. 2024Preprint</p>
<p>Identifying manipulated offerings on review portals. Jiwei Li, Myle Ott, Claire Cardie, Proceedings of EMNLP. EMNLP2013</p>
<p>Towards a general rule for identifying deceptive opinion spam. Jiwei Li, Myle Ott, Claire Cardie, Eduard Hovy, 10.3115/v1/P14-1147Proceedings of ACL. ACL2014</p>
<p>MLR-Copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024Preprint</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, Proceedings of ACL. ACLOnline2020</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024Preprint</p>
<p>Effects of stress throughout the lifespan on the brain, behaviour and cognition. Sonia J Lupien, Bruce S Mcewen, Megan R Gunnar, Christine Heim, Nature Reviews Neuroscience. 2009</p>
<p>Discov-eryBench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.017252024Preprint</p>
<p>Deceptive opinion spam detection approaches: a literature survey. Sushil Kumar Maurya, Dinesh Singh, Ashish , 10.1007/s10489-022-03427-1Applied Intelligence. 532Kumar Maurya. 2022</p>
<p>GPT-4 technical report. OthersideAI. 2024. HyperWrite. 2023OpenAI</p>
<p>Negative deceptive opinion spam. Myle Ott, Claire Cardie, Jeffrey T Hancock, Proceedings of NAACL. NAACL2013</p>
<p>Llm evaluators recognize and favor their own generations. Arjun Panickssery, R Samuel, Shi Bowman, Feng, arXiv:2404.130762024Preprint</p>
<p>Measuring and benchmarking large language models' capabilities to generate persuasive language. Amalie Brogaard, Pauli , Isabelle Augenstein, Ira Assent, arXiv:2406.177532024Preprint</p>
<p>Topicgpt: A prompt-based topic modeling framework. Minh Chau, Alexander Pham, Simeng Hoyle, Mohit Sun, Iyyer, Proceedings of NAACL. NAACL2024</p>
<p>Crowdsourcing a large corpus of clickbait on Twitter. Martin Potthast, Tim Gollub, Kristof Komlossy, Sebastian Schuster, Matti Wiegmann, Erika Patricia, Garces Fernandez, Matthias Hagen, Benno Stein, Proceedings of COLING. COLING2018</p>
<p>CiteME: Can language models accurately cite scientific claims?. Ori Press, Andreas Hochlehnert, Ameya Prabhu, arXiv:2407.128612024PreprintVishaal Udandarao, Ofir Press, and Matthias Bethge</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren ; Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, 10.1038/s41586-023-06924-6Proceedings of ICLR. Bernardino Romera-Paredes, Mohammadamin Barekatain. ICLR. Bernardino Romera-Paredes, Mohammadamin Barekatain2024. 2024625Mathematical discoveries from program search with large language models</p>
<p>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024Preprint</p>
<p>Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions. Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, Lillian Lee, 10.1145/2872427.2883081Proceedings of WWW. WWW2016</p>
<p>The science of detecting llm-generated texts. Ruixiang Tang, Yu-Neng Chuang, Xia Hu, arXiv:2303.072052023Preprint</p>
<p>Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Shengyan Li, Di Liu, Yutao Luo, Hao Ma, Kha Tong, Chenyu Trinh, Zihan Tian, Bohao Wang, Yanyu Wu, Shengzhu Xiong, Minhui Yin, Kilian Zhu, Yanxin Lieret, Genglin Lu, Yufeng Liu, Tianhua Du, Tao, arXiv:2407.13168SciCode: A research coding benchmark curated by scientists. Jamie CallanEliu Huerta, and Hao Peng2024Preprint</p>
<p>Dreaddit: A reddit dataset for stress analysis in social media. Elsbeth Turcan, Kathleen Mckeown, arXiv:1911.001332019Preprint</p>
<p>How to identify an argument type? on the hermeneutics of persuasive discourse. H M Jean, Wagemans, 10.1016/j.pragma.2022.11.015Journal of Pragmatics. 2032023</p>
<p>User stress detection using social media text: A novel machine learning approach. Xiangxuan Wan, Li Tian, 10.15837/ijccc.2024.5.6772International Journal of Computers Communications &amp; Control. 2024. 19</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proceedings of ACL. ACL2024</p>
<p>Persuasion for good: Towards a personalized persuasive dialogue system for social good. Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, Zhou Yu, arXiv:1906.067252020Preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, Proceedings of NeurIPS. NeurIPS2022</p>
<p>A survey on llm-generated text detection: Necessity, methods, and future directions. Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F Wong, Lidia S Chao, arXiv:2310.147242024Preprint</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of EACL. EACL2024a</p>
<p>Soujanya Poria, and Erik Cambria. 2024b. Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Proceedings of ACL. ACL</p>
<p>MASSW: A new dataset and benchmark tasks for ai-assisted scientific workflows. Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, Qiaozhu Mei, arXiv:2406.063572024Preprint</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Proceedings of NeurIPS. NeurIPS2023</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
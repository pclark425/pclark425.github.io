<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3528 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3528</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3528</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-92173d081b15824d22a9ef070e118744ceee8052</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/92173d081b15824d22a9ef070e118744ceee8052" target="_blank">Show Your Work: Scratchpads for Intermediate Computation with Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, it is shown that scratchpads dramatically improve the ability of language models to perform multi-step computations.</p>
                <p><strong>Paper Abstract:</strong> Large pre-trained language models perform remarkably well on tasks that can be done"in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation"step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a"scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3528.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3528.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpad intermediate computation buffer for Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training and prompting technique that forces decoder-only Transformer language models to emit explicit intermediate computation steps (a 'scratchpad' or program trace) before producing the final answer, enabling adaptive multi-step algorithmic reasoning without changing the underlying architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pre-trained decoder-only Transformer language models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer language models pre-trained on web documents and dialog data and then fine-tuned; used across experiments at sizes from small models (millions of params) up to very large models (137B). For code/program experiments a 137B model with context window 1024 and generation limit 512 tokens was used; fine-tuning used greedy decoding (T=0), typical lr=3e-5 and large token batch sizes (e.g., 8192 tokens) for code tasks. Addition experiments used models in range 2M–1B and fine-tuned for 5k steps on 100k examples (batch size 32).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2M–137B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Algorithmic multi-step computation (long addition, polynomial evaluation, program execution / tracing)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that require strict, multi-step discrete logical computation (grade-school long addition with carries, polynomial arithmetic, and predicting program execution including control flow and local state). These require precise stepwise reasoning and potentially unbounded / variable-length computation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Train / prompt models to emit intermediate steps (scratchpad). For arithmetic tasks: long-add algorithm steps (including carry) encoded as tokens. For program execution: train to emit an alternating sequence of executed source-code lines and JSON-encoded local-variable states (a program trace). Additional interventions: few-shot prompting with scratchpad examples; fine-tuning on traced examples; data augmentation by sampling model-generated programs and tracing them (MBPP-aug); adding large single-line execution datasets and CodeNet traces to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across tasks the scratchpad method substantially improved measured execution/answer accuracy compared to direct prediction baselines. Representative results from the paper: Polynomial evaluation (few-shot, 137B): direct 8.8% vs scratchpad 20.1% (accuracy). Polynomial evaluation (fine-tune, 8B): direct 31.8% vs scratchpad 50.7%. Synthetic Python (few-shot, 137B): direct 11% vs scratchpad 26.5% (note: few-shot scratchpad scoring modified due to variable-name mismatch). Synthetic Python (fine-tuned, 137B): direct 20% vs scratchpad 41.5%. MBPP (real human-written code) on augmented+additional data, best scratchpad results: per-task execution acc 26.6%, per-example execution acc 46.0%, per-example trace acc ~42.1% (see Table 3 for detailed breakdowns). Long addition: scratchpad-enabled models solved long-add problems and generalized to out-of-distribution longer digit lengths (9-10 digits) beyond training (1-8 digits), whereas baseline models failed for OOD lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Direct execution / direct prediction baselines (models asked to predict final outputs without intermediate steps). Representative baseline numbers: Polynomial few-shot 8.8% (direct), polynomial fine-tune 31.8% (direct). Synthetic Python few-shot 11% (direct), fine-tuned 20% (direct). MBPP (no augmentation) direct per-task exec acc 10.3%, per-example execution acc 22.0%; on MBPP-aug direct per-task exec acc fell to 5.1%, per-example 12.3%. For long addition, baseline models trained without scratchpad failed even at largest tested scales (qualitatively 'complete failure' on 9-10 digit OOD).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Quantitative improvements where comparable: Polynomial few-shot +11.3 percentage points (20.1% vs 8.8%); polynomial fine-tune +18.9 pp (50.7% vs 31.8%). Synthetic Python few-shot +15.5 pp (26.5% vs 11%); synthetic Python fine-tune +21.5 pp (41.5% vs 20%). MBPP-aug per-task execution: scratchpad 17.3% vs direct 5.1% (+12.2 pp); MBPP-aug per-example execution: scratchpad 35.5% vs direct 12.3% (+23.2 pp). For long addition, scratchpad models succeeded and generalized to longer numbers where direct models failed (qualitative strong improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Remaining limitations reported: scratchpad requires supervised intermediate-state traces (they do not learn scratchpad usage autonomously in this work); performance still far from perfect (e.g., best per-task MBPP ~26.6%); very-low-data regimes can make scratchpad underperform direct execution (on MBPP with only the original small training set scratchpad got lower per-task exec acc than direct); context / generation window constraints (512/1024 token limits) restrict trace length and prevent scaling to arbitrarily long computations; some systematic few-shot failure modes (e.g., variable-name mismatch where the model returns v0 instead of output, requiring relaxed scoring); scratchpad training can degrade other capabilities somewhat (example: the traced+fine-tuned model achieved 54% few-shot synthesis vs 62% for original few-shot model in Austin et al., indicating some disruption).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations and analysis in the paper: (1) Long-add ablation: fine-tuning a model on scratchpad then subsequently fine-tuning it to do direct execution provided no significant benefit, indicating that mere extra intermediate-step supervision is not the sole cause of scratchpad benefit. (2) MBPP data-augmentation ablation: augmenting training with model-sampled programs (MBPP-aug) significantly improved scratchpad tracing performance but harmed direct execution performance. (3) Adding complementary datasets (single-line execution examples and CodeNet traces) further increased scratchpad performance (e.g., per-task exec acc up to 26.6% and per-example trace acc up to ~42%), suggesting tracing benefits from more diverse execution supervision. (4) Few-shot synthetic experiments revealed consistent variable-name errors; evaluation was adjusted to account for this (checking v0).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show Your Work: Scratchpads for Intermediate Computation with Language Models', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3528.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3528.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long Addition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long integer addition (grade-school algorithm) with scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task: add two multi-digit integers using grade-school long addition (carry propagation); used to evaluate ability to perform precise multi-step arithmetic and to generalize out-of-distribution to longer inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pre-trained decoder-only Transformer language models (various sizes for addition experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned decoder-only Transformers (no architecture change) trained on addition examples with targets either as final sum (direct baseline) or including detailed intermediate long-add steps (scratchpad). Training on 100k examples for 5k steps, batch size 32. Model sizes tested ranged from 2M to 1B parameters for addition experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2M–1B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Long integer addition (1–8 digit training; test on 9–10 digit out-of-distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A strict algorithmic arithmetic task requiring exact carry-by-digit computation across multiple steps; strong test of systematic multi-step logical reasoning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Include explicit long-add intermediate steps (per-digit sums and carries) in the target 'scratchpad' during supervised fine-tuning so the model must emit the step-by-step computation before the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative and plotted results (Figure 3): beyond a critical model size, models trained with scratchpad reliably solved the addition task (both in-distribution up to 8 digits and generalized to 9–10 digit OOD). Models trained without scratchpad failed to learn to add beyond trivial digit lengths and failed on out-of-distribution 9–10 digit cases at the largest tested scale. Exact numeric accuracies for each size are presented in Figure 3 of the paper (not all numeric values listed in text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline models trained without scratchpad (direct prediction of the sum) failed to generalize to larger digit lengths and 'completely fail' on 9–10 digit OOD additions in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Scratchpad enabled correct computation and OOD generalization (9–10 digits) where baseline models failed entirely. Improvement is qualitative and substantial: scratchpad models succeed beyond a critical scale while direct models do not.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limitations: experiments limited to models/sizes tested (2M–1B); Transformer theoretical constraints (quadratic computation in length) may limit perfect simulation of algorithms with higher time complexity; scratchpad length is constrained by generation context size; ablation showed no benefit from merely pretraining with scratchpad then fine-tuning to direct output (i.e., the benefit is not reducible to extra supervision alone).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation: A model first fine-tuned on scratchpad and then fine-tuned to direct execution did not show significant improvement over the baseline direct model, suggesting scratchpad's benefit is not simply extra training-time signal. The paper analyzes scaling with model size and shows a threshold effect where scratchpad enables learning beyond that threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show Your Work: Scratchpads for Intermediate Computation with Language Models', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3528.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3528.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polynomial Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Polynomial evaluation (degree ≤ 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluate integer-coefficient polynomials (degree ≤ 3) at integer inputs, comparing direct final-output prediction versus emitting per-term intermediate computations in a scratchpad.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>137B pre-trained decoder-only model (few-shot), 8B model (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Few-shot evaluation used the 137B pre-trained decoder-only Transformer with 4 in-context examples. Fine-tuning used an 8B pre-trained decoder-only model fine-tuned for 2000 steps on 10k training polynomials.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137B (few-shot); 8B (fine-tune)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Polynomial evaluation (degree ≤ 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Compute per-term values (e.g., -7*x**2, 7*x, constant) and sum them; requires exact arithmetic and multi-step composition (multiplication and addition) across terms.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Scratchpad: force model to emit each term's computed value as intermediate scratchpad lines, then emit the total. Compared two regimes: few-shot prompting vs fine-tuning on scratchpad-annotated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 1: Few-shot: direct prediction 8.8% accuracy vs scratchpad 20.1% accuracy. Fine-tuning: direct prediction 31.8% vs scratchpad 50.7% accuracy. (Percent accuracy on test set of polynomials; scratchpad substantially improves both few-shot and fine-tuned performance.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Direct prediction (no scratchpad): few-shot 8.8%; fine-tune 31.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Few-shot absolute improvement +11.3 percentage points (20.1% vs 8.8%). Fine-tune improvement +18.9 pp (50.7% vs 31.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Absolute accuracy remains limited (best ~50.7% after fine-tuning), indicating scratchpad helps but does not fully solve the task. The experiment limits (degree ≤ 3, input/output range restrictions) narrow scope; scaling to higher degrees, ranges, or longer computations may be limited by context/window size and tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comparison across few-shot and fine-tune regimes shows scratchpad benefits both, with larger absolute improvements in the fine-tuned regime; no further ablation specific to polynomial evaluation beyond the direct vs scratchpad comparison is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show Your Work: Scratchpads for Intermediate Computation with Language Models', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3528.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3528.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic Python Execution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic Python program execution (trace prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predict the execution output of synthetic Python programs (small integers, simple while loops, if statements) by emitting a full program trace (lines executed and JSON-encoded local-variable states) as a scratchpad.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>137B pre-trained decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>137B decoder-only Transformer with 1024 token context and 512 token generation limit; few-shot prompts contained three tracing examples; fine-tuning was run to convergence on synthetic train split (400 training programs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Synthetic program execution (modified Bieber et al. 2020 benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given a small Python program and an input, predict the program's execution trace (the sequence of executed lines) and the local-variable states after each step; correctness judged by whether the final returned output (encoded in trace) semantically matches target.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Scratchpad tracing: represent trace as alternating 'line: <source-line>' and 'state: <JSON dict>' tokens. Evaluate both few-shot (3 examples) and fine-tuning on 400 synthetic programs. Adjusted scoring in few-shot to accept the v0 naming mismatch (model returns v0 instead of 'output').</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 2: Few-shot: direct prediction 11% vs scratchpad 26.5% (note few-shot scratchpad scoring modified to check v0 variable). Fine-tuned: direct prediction 20% vs scratchpad 41.5% execution accuracy on 200 test programs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Direct execution baseline (predict final output without trace): few-shot 11%, fine-tuned 20%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Few-shot +15.5 pp (26.5% vs 11%); fine-tuned +21.5 pp (41.5% vs 20%). Fine-tuning improves scratchpad more than it improves direct execution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Failure modes include consistent few-shot variable-name mismatches (model emits v0 rather than 'output'), requiring relaxed scoring; some traces are nearly correct except for minor naming/format issues. The dataset is synthetic and limited in complexity; scaling to real-world code poses additional challenges (see MBPP experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Observed that fine-tuning yields larger gains for scratchpad than for direct execution. The paper includes example incorrect traces (Appendix D) illustrating types of mistakes (variable-name and final-line mismatches).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show Your Work: Scratchpads for Intermediate Computation with Language Models', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3528.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3528.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBPP Execution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MBPP program execution (human-written Python problems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluate model ability to execute real human-written Python programming problems (MBPP dataset) by predicting execution outputs and/or full execution traces; compare direct prediction baseline to scratchpad tracing and to training on augmented/generated tracing data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program synthesis with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>137B pre-trained decoder-only Transformer (fine-tuned in various data regimes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>137B decoder-only Transformer with 1024 token context, 512 generation limit; fine-tuned on MBPP, MBPP-aug (model-sampled programs), and then additional datasets (CodeNet traces and single-line execution examples). Fine-tuning used batch sizes of 8192 tokens and lr=3e-5 unless otherwise noted.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MBPP program execution / tracing</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given a natural language problem, a ground-truth Python program, and inputs, predict the outputs for the inputs (and optionally the full program trace). MBPP programs use varied data types and control-flow features (ints, strings, floats, dicts, tuples, loops, comprehensions, imports, recursion).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Scratchpad tracing (line-by-line + JSON local state) vs direct execution baseline. Data augmentation: sample candidate programs from the model for each MBPP task (MBPP-aug), run them to produce new I/O examples and traces; include large single-line execution dataset and CodeNet traces; perform staged fine-tuning: (1) MBPP-aug, (2) MBPP-aug + CodeNet + single-line, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3 provides detailed results. Key numbers: On MBPP (original small training set) direct per-task exec acc 10.3% and per-example exec acc 22.0%; scratchpad per-task exec acc 5.1% and per-example exec acc 24.6% (scratchpad underperformed direct on per-task metric in this very-low-data regime). After augmenting with MBPP-aug (17k generated programs): direct per-task exec acc fell to 5.1% (per-example 12.3%), while scratchpad improved to per-task exec acc 17.3% (per-example exec acc 35.5%) and per-example trace acc 26.8%. Further adding CodeNet and single-line data produced best scratchpad results: per-task exec acc 26.6%, per-task trace acc 24.6%, per-example execution acc 46.0%, per-example trace acc 41.9% (see Table 3). The model achieved ~26.8% exact-trace-match raw on MBPP-aug before later dataset additions and ~42% raw exact-trace match in best model.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Direct execution baseline: MBPP per-task exec acc 10.3% (MBPP), MBPP-aug per-task 5.1% (MBPP-aug), per-example 22.0% (MBPP) and 12.3% (MBPP-aug). Direct execution performance degraded when training on MBPP-aug while scratchpad improved.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>With MBPP-aug, scratchpad per-task exec acc improved to 17.3% vs direct 5.1% (+12.2 pp). Per-example 35.5% vs direct 12.3% (+23.2 pp). With the combined MBPP-aug + CodeNet + single-line data, scratchpad achieved up to 26.6% per-task exec acc and ~46.0% per-example exec acc (baseline direct numbers for that final combined training condition are not provided in the table; earlier direct baselines were much lower).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant limitations: in very-low-data regimes scratchpad can underperform direct; the context/generation window limited the test set to 212/500 MBPP tasks (traces must fit generation window); scratchpad still far from perfect (best per-task exec acc ~26.6%); direct-execution models were harmed by MBPP-aug augmentation while scratchpad benefited—indicating sensitivity to training-data distribution; evaluation required a semantic match of JSON-encoded outputs, and some Python objects cannot be perfectly represented in JSON (tuples -> lists, objects represented by placeholders), which complicates scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Analyses show: (1) MBPP-aug (model-sampled candidate programs + executing them to create additional labeled traces) substantially improves scratchpad performance but reduces direct-execution accuracy—indicating that scratchpad benefits from broader execution-trace supervision. (2) Adding CodeNet and single-line execution data further improves scratchpad performance, suggesting that tracing scales with more (even noisy) execution supervision. (3) The paper reports exact-trace-match rates and per-task vs per-example metrics to dissect model behaviour; it also documents practical evaluation adjustments (e.g., handling of JSON encoding limitations and variable-name mismatches).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show Your Work: Scratchpads for Intermediate Computation with Language Models', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Program synthesis with large language models <em>(Rating: 2)</em></li>
                <li>Learning to execute <em>(Rating: 1)</em></li>
                <li>Learning to execute programs with instruction pointer attention graph neural networks <em>(Rating: 1)</em></li>
                <li>Project CodeNet: A Large-Scale AI for code dataset for learning a diversity of coding tasks <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 1)</em></li>
                <li>Neural Turing Machines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3528",
    "paper_id": "paper-92173d081b15824d22a9ef070e118744ceee8052",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "Scratchpad",
            "name_full": "Scratchpad intermediate computation buffer for Transformers",
            "brief_description": "A training and prompting technique that forces decoder-only Transformer language models to emit explicit intermediate computation steps (a 'scratchpad' or program trace) before producing the final answer, enabling adaptive multi-step algorithmic reasoning without changing the underlying architecture.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pre-trained decoder-only Transformer language models",
            "model_description": "Decoder-only Transformer language models pre-trained on web documents and dialog data and then fine-tuned; used across experiments at sizes from small models (millions of params) up to very large models (137B). For code/program experiments a 137B model with context window 1024 and generation limit 512 tokens was used; fine-tuning used greedy decoding (T=0), typical lr=3e-5 and large token batch sizes (e.g., 8192 tokens) for code tasks. Addition experiments used models in range 2M–1B and fine-tuned for 5k steps on 100k examples (batch size 32).",
            "model_size": "2M–137B",
            "reasoning_task_name": "Algorithmic multi-step computation (long addition, polynomial evaluation, program execution / tracing)",
            "reasoning_task_description": "Tasks that require strict, multi-step discrete logical computation (grade-school long addition with carries, polynomial arithmetic, and predicting program execution including control flow and local state). These require precise stepwise reasoning and potentially unbounded / variable-length computation.",
            "method_or_intervention": "Train / prompt models to emit intermediate steps (scratchpad). For arithmetic tasks: long-add algorithm steps (including carry) encoded as tokens. For program execution: train to emit an alternating sequence of executed source-code lines and JSON-encoded local-variable states (a program trace). Additional interventions: few-shot prompting with scratchpad examples; fine-tuning on traced examples; data augmentation by sampling model-generated programs and tracing them (MBPP-aug); adding large single-line execution datasets and CodeNet traces to fine-tuning.",
            "performance": "Across tasks the scratchpad method substantially improved measured execution/answer accuracy compared to direct prediction baselines. Representative results from the paper: Polynomial evaluation (few-shot, 137B): direct 8.8% vs scratchpad 20.1% (accuracy). Polynomial evaluation (fine-tune, 8B): direct 31.8% vs scratchpad 50.7%. Synthetic Python (few-shot, 137B): direct 11% vs scratchpad 26.5% (note: few-shot scratchpad scoring modified due to variable-name mismatch). Synthetic Python (fine-tuned, 137B): direct 20% vs scratchpad 41.5%. MBPP (real human-written code) on augmented+additional data, best scratchpad results: per-task execution acc 26.6%, per-example execution acc 46.0%, per-example trace acc ~42.1% (see Table 3 for detailed breakdowns). Long addition: scratchpad-enabled models solved long-add problems and generalized to out-of-distribution longer digit lengths (9-10 digits) beyond training (1-8 digits), whereas baseline models failed for OOD lengths.",
            "baseline_performance": "Direct execution / direct prediction baselines (models asked to predict final outputs without intermediate steps). Representative baseline numbers: Polynomial few-shot 8.8% (direct), polynomial fine-tune 31.8% (direct). Synthetic Python few-shot 11% (direct), fine-tuned 20% (direct). MBPP (no augmentation) direct per-task exec acc 10.3%, per-example execution acc 22.0%; on MBPP-aug direct per-task exec acc fell to 5.1%, per-example 12.3%. For long addition, baseline models trained without scratchpad failed even at largest tested scales (qualitatively 'complete failure' on 9-10 digit OOD).",
            "improvement_over_baseline": "Quantitative improvements where comparable: Polynomial few-shot +11.3 percentage points (20.1% vs 8.8%); polynomial fine-tune +18.9 pp (50.7% vs 31.8%). Synthetic Python few-shot +15.5 pp (26.5% vs 11%); synthetic Python fine-tune +21.5 pp (41.5% vs 20%). MBPP-aug per-task execution: scratchpad 17.3% vs direct 5.1% (+12.2 pp); MBPP-aug per-example execution: scratchpad 35.5% vs direct 12.3% (+23.2 pp). For long addition, scratchpad models succeeded and generalized to longer numbers where direct models failed (qualitative strong improvement).",
            "limitations_or_failures": "Remaining limitations reported: scratchpad requires supervised intermediate-state traces (they do not learn scratchpad usage autonomously in this work); performance still far from perfect (e.g., best per-task MBPP ~26.6%); very-low-data regimes can make scratchpad underperform direct execution (on MBPP with only the original small training set scratchpad got lower per-task exec acc than direct); context / generation window constraints (512/1024 token limits) restrict trace length and prevent scaling to arbitrarily long computations; some systematic few-shot failure modes (e.g., variable-name mismatch where the model returns v0 instead of output, requiring relaxed scoring); scratchpad training can degrade other capabilities somewhat (example: the traced+fine-tuned model achieved 54% few-shot synthesis vs 62% for original few-shot model in Austin et al., indicating some disruption).",
            "ablation_or_analysis": "Ablations and analysis in the paper: (1) Long-add ablation: fine-tuning a model on scratchpad then subsequently fine-tuning it to do direct execution provided no significant benefit, indicating that mere extra intermediate-step supervision is not the sole cause of scratchpad benefit. (2) MBPP data-augmentation ablation: augmenting training with model-sampled programs (MBPP-aug) significantly improved scratchpad tracing performance but harmed direct execution performance. (3) Adding complementary datasets (single-line execution examples and CodeNet traces) further increased scratchpad performance (e.g., per-task exec acc up to 26.6% and per-example trace acc up to ~42%), suggesting tracing benefits from more diverse execution supervision. (4) Few-shot synthetic experiments revealed consistent variable-name errors; evaluation was adjusted to account for this (checking v0).",
            "uuid": "e3528.0",
            "source_info": {
                "paper_title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Long Addition",
            "name_full": "Long integer addition (grade-school algorithm) with scratchpad",
            "brief_description": "Task: add two multi-digit integers using grade-school long addition (carry propagation); used to evaluate ability to perform precise multi-step arithmetic and to generalize out-of-distribution to longer inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pre-trained decoder-only Transformer language models (various sizes for addition experiments)",
            "model_description": "Fine-tuned decoder-only Transformers (no architecture change) trained on addition examples with targets either as final sum (direct baseline) or including detailed intermediate long-add steps (scratchpad). Training on 100k examples for 5k steps, batch size 32. Model sizes tested ranged from 2M to 1B parameters for addition experiments.",
            "model_size": "2M–1B",
            "reasoning_task_name": "Long integer addition (1–8 digit training; test on 9–10 digit out-of-distribution)",
            "reasoning_task_description": "A strict algorithmic arithmetic task requiring exact carry-by-digit computation across multiple steps; strong test of systematic multi-step logical reasoning and generalization.",
            "method_or_intervention": "Include explicit long-add intermediate steps (per-digit sums and carries) in the target 'scratchpad' during supervised fine-tuning so the model must emit the step-by-step computation before the final answer.",
            "performance": "Qualitative and plotted results (Figure 3): beyond a critical model size, models trained with scratchpad reliably solved the addition task (both in-distribution up to 8 digits and generalized to 9–10 digit OOD). Models trained without scratchpad failed to learn to add beyond trivial digit lengths and failed on out-of-distribution 9–10 digit cases at the largest tested scale. Exact numeric accuracies for each size are presented in Figure 3 of the paper (not all numeric values listed in text).",
            "baseline_performance": "Baseline models trained without scratchpad (direct prediction of the sum) failed to generalize to larger digit lengths and 'completely fail' on 9–10 digit OOD additions in the reported experiments.",
            "improvement_over_baseline": "Scratchpad enabled correct computation and OOD generalization (9–10 digits) where baseline models failed entirely. Improvement is qualitative and substantial: scratchpad models succeed beyond a critical scale while direct models do not.",
            "limitations_or_failures": "Limitations: experiments limited to models/sizes tested (2M–1B); Transformer theoretical constraints (quadratic computation in length) may limit perfect simulation of algorithms with higher time complexity; scratchpad length is constrained by generation context size; ablation showed no benefit from merely pretraining with scratchpad then fine-tuning to direct output (i.e., the benefit is not reducible to extra supervision alone).",
            "ablation_or_analysis": "Ablation: A model first fine-tuned on scratchpad and then fine-tuned to direct execution did not show significant improvement over the baseline direct model, suggesting scratchpad's benefit is not simply extra training-time signal. The paper analyzes scaling with model size and shows a threshold effect where scratchpad enables learning beyond that threshold.",
            "uuid": "e3528.1",
            "source_info": {
                "paper_title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Polynomial Eval",
            "name_full": "Polynomial evaluation (degree ≤ 3)",
            "brief_description": "Evaluate integer-coefficient polynomials (degree ≤ 3) at integer inputs, comparing direct final-output prediction versus emitting per-term intermediate computations in a scratchpad.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "137B pre-trained decoder-only model (few-shot), 8B model (fine-tuned)",
            "model_description": "Few-shot evaluation used the 137B pre-trained decoder-only Transformer with 4 in-context examples. Fine-tuning used an 8B pre-trained decoder-only model fine-tuned for 2000 steps on 10k training polynomials.",
            "model_size": "137B (few-shot); 8B (fine-tune)",
            "reasoning_task_name": "Polynomial evaluation (degree ≤ 3)",
            "reasoning_task_description": "Compute per-term values (e.g., -7*x**2, 7*x, constant) and sum them; requires exact arithmetic and multi-step composition (multiplication and addition) across terms.",
            "method_or_intervention": "Scratchpad: force model to emit each term's computed value as intermediate scratchpad lines, then emit the total. Compared two regimes: few-shot prompting vs fine-tuning on scratchpad-annotated examples.",
            "performance": "Table 1: Few-shot: direct prediction 8.8% accuracy vs scratchpad 20.1% accuracy. Fine-tuning: direct prediction 31.8% vs scratchpad 50.7% accuracy. (Percent accuracy on test set of polynomials; scratchpad substantially improves both few-shot and fine-tuned performance.)",
            "baseline_performance": "Direct prediction (no scratchpad): few-shot 8.8%; fine-tune 31.8%.",
            "improvement_over_baseline": "Few-shot absolute improvement +11.3 percentage points (20.1% vs 8.8%). Fine-tune improvement +18.9 pp (50.7% vs 31.8%).",
            "limitations_or_failures": "Absolute accuracy remains limited (best ~50.7% after fine-tuning), indicating scratchpad helps but does not fully solve the task. The experiment limits (degree ≤ 3, input/output range restrictions) narrow scope; scaling to higher degrees, ranges, or longer computations may be limited by context/window size and tokenization.",
            "ablation_or_analysis": "Comparison across few-shot and fine-tune regimes shows scratchpad benefits both, with larger absolute improvements in the fine-tuned regime; no further ablation specific to polynomial evaluation beyond the direct vs scratchpad comparison is reported.",
            "uuid": "e3528.2",
            "source_info": {
                "paper_title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Synthetic Python Execution",
            "name_full": "Synthetic Python program execution (trace prediction)",
            "brief_description": "Predict the execution output of synthetic Python programs (small integers, simple while loops, if statements) by emitting a full program trace (lines executed and JSON-encoded local-variable states) as a scratchpad.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "137B pre-trained decoder-only Transformer",
            "model_description": "137B decoder-only Transformer with 1024 token context and 512 token generation limit; few-shot prompts contained three tracing examples; fine-tuning was run to convergence on synthetic train split (400 training programs).",
            "model_size": "137B",
            "reasoning_task_name": "Synthetic program execution (modified Bieber et al. 2020 benchmark)",
            "reasoning_task_description": "Given a small Python program and an input, predict the program's execution trace (the sequence of executed lines) and the local-variable states after each step; correctness judged by whether the final returned output (encoded in trace) semantically matches target.",
            "method_or_intervention": "Scratchpad tracing: represent trace as alternating 'line: &lt;source-line&gt;' and 'state: &lt;JSON dict&gt;' tokens. Evaluate both few-shot (3 examples) and fine-tuning on 400 synthetic programs. Adjusted scoring in few-shot to accept the v0 naming mismatch (model returns v0 instead of 'output').",
            "performance": "Table 2: Few-shot: direct prediction 11% vs scratchpad 26.5% (note few-shot scratchpad scoring modified to check v0 variable). Fine-tuned: direct prediction 20% vs scratchpad 41.5% execution accuracy on 200 test programs.",
            "baseline_performance": "Direct execution baseline (predict final output without trace): few-shot 11%, fine-tuned 20%.",
            "improvement_over_baseline": "Few-shot +15.5 pp (26.5% vs 11%); fine-tuned +21.5 pp (41.5% vs 20%). Fine-tuning improves scratchpad more than it improves direct execution.",
            "limitations_or_failures": "Failure modes include consistent few-shot variable-name mismatches (model emits v0 rather than 'output'), requiring relaxed scoring; some traces are nearly correct except for minor naming/format issues. The dataset is synthetic and limited in complexity; scaling to real-world code poses additional challenges (see MBPP experiments).",
            "ablation_or_analysis": "Observed that fine-tuning yields larger gains for scratchpad than for direct execution. The paper includes example incorrect traces (Appendix D) illustrating types of mistakes (variable-name and final-line mismatches).",
            "uuid": "e3528.3",
            "source_info": {
                "paper_title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "MBPP Execution",
            "name_full": "MBPP program execution (human-written Python problems)",
            "brief_description": "Evaluate model ability to execute real human-written Python programming problems (MBPP dataset) by predicting execution outputs and/or full execution traces; compare direct prediction baseline to scratchpad tracing and to training on augmented/generated tracing data.",
            "citation_title": "Program synthesis with large language models",
            "mention_or_use": "use",
            "model_name": "137B pre-trained decoder-only Transformer (fine-tuned in various data regimes)",
            "model_description": "137B decoder-only Transformer with 1024 token context, 512 generation limit; fine-tuned on MBPP, MBPP-aug (model-sampled programs), and then additional datasets (CodeNet traces and single-line execution examples). Fine-tuning used batch sizes of 8192 tokens and lr=3e-5 unless otherwise noted.",
            "model_size": "137B",
            "reasoning_task_name": "MBPP program execution / tracing",
            "reasoning_task_description": "Given a natural language problem, a ground-truth Python program, and inputs, predict the outputs for the inputs (and optionally the full program trace). MBPP programs use varied data types and control-flow features (ints, strings, floats, dicts, tuples, loops, comprehensions, imports, recursion).",
            "method_or_intervention": "Scratchpad tracing (line-by-line + JSON local state) vs direct execution baseline. Data augmentation: sample candidate programs from the model for each MBPP task (MBPP-aug), run them to produce new I/O examples and traces; include large single-line execution dataset and CodeNet traces; perform staged fine-tuning: (1) MBPP-aug, (2) MBPP-aug + CodeNet + single-line, etc.",
            "performance": "Table 3 provides detailed results. Key numbers: On MBPP (original small training set) direct per-task exec acc 10.3% and per-example exec acc 22.0%; scratchpad per-task exec acc 5.1% and per-example exec acc 24.6% (scratchpad underperformed direct on per-task metric in this very-low-data regime). After augmenting with MBPP-aug (17k generated programs): direct per-task exec acc fell to 5.1% (per-example 12.3%), while scratchpad improved to per-task exec acc 17.3% (per-example exec acc 35.5%) and per-example trace acc 26.8%. Further adding CodeNet and single-line data produced best scratchpad results: per-task exec acc 26.6%, per-task trace acc 24.6%, per-example execution acc 46.0%, per-example trace acc 41.9% (see Table 3). The model achieved ~26.8% exact-trace-match raw on MBPP-aug before later dataset additions and ~42% raw exact-trace match in best model.",
            "baseline_performance": "Direct execution baseline: MBPP per-task exec acc 10.3% (MBPP), MBPP-aug per-task 5.1% (MBPP-aug), per-example 22.0% (MBPP) and 12.3% (MBPP-aug). Direct execution performance degraded when training on MBPP-aug while scratchpad improved.",
            "improvement_over_baseline": "With MBPP-aug, scratchpad per-task exec acc improved to 17.3% vs direct 5.1% (+12.2 pp). Per-example 35.5% vs direct 12.3% (+23.2 pp). With the combined MBPP-aug + CodeNet + single-line data, scratchpad achieved up to 26.6% per-task exec acc and ~46.0% per-example exec acc (baseline direct numbers for that final combined training condition are not provided in the table; earlier direct baselines were much lower).",
            "limitations_or_failures": "Significant limitations: in very-low-data regimes scratchpad can underperform direct; the context/generation window limited the test set to 212/500 MBPP tasks (traces must fit generation window); scratchpad still far from perfect (best per-task exec acc ~26.6%); direct-execution models were harmed by MBPP-aug augmentation while scratchpad benefited—indicating sensitivity to training-data distribution; evaluation required a semantic match of JSON-encoded outputs, and some Python objects cannot be perfectly represented in JSON (tuples -&gt; lists, objects represented by placeholders), which complicates scoring.",
            "ablation_or_analysis": "Analyses show: (1) MBPP-aug (model-sampled candidate programs + executing them to create additional labeled traces) substantially improves scratchpad performance but reduces direct-execution accuracy—indicating that scratchpad benefits from broader execution-trace supervision. (2) Adding CodeNet and single-line execution data further improves scratchpad performance, suggesting that tracing scales with more (even noisy) execution supervision. (3) The paper reports exact-trace-match rates and per-task vs per-example metrics to dissect model behaviour; it also documents practical evaluation adjustments (e.g., handling of JSON encoding limitations and variable-name mismatches).",
            "uuid": "e3528.4",
            "source_info": {
                "paper_title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                "publication_date_yy_mm": "2021-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Program synthesis with large language models",
            "rating": 2
        },
        {
            "paper_title": "Learning to execute",
            "rating": 1
        },
        {
            "paper_title": "Learning to execute programs with instruction pointer attention graph neural networks",
            "rating": 1
        },
        {
            "paper_title": "Project CodeNet: A Large-Scale AI for code dataset for learning a diversity of coding tasks",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 1
        },
        {
            "paper_title": "Neural Turing Machines",
            "rating": 1
        }
    ],
    "cost": 0.01811025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Show Your Work: ScratchPads for IntermedIAte COMPutation with Language Models</h1>
<p>Maxwell Nye ${ }^{12 *}$ Anders Johan Andreassen ${ }^{3}$ Guy Gur-Ari ${ }^{3}$ Henryk Michalewski ${ }^{2}$<br>Jacob Austin ${ }^{2}$ David Bieber ${ }^{2}$ David Dohan ${ }^{2}$ Aitor Lewkowycz ${ }^{3}$ Maarten Bosma ${ }^{2}$<br>David Luan ${ }^{2}$ Charles Sutton ${ }^{2}$<br>Augustus Odena ${ }^{2}$<br>${ }^{1}$ MIT<br>${ }^{2}$ Google Research, Brain Team<br>${ }^{3}$ Google Research, Blueshift Team</p>
<h4>Abstract</h4>
<p>Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text (Brown et al., 2020) or synthesizing computer programs (Chen et al., 2021; Austin et al., 2021). However, they struggle with tasks that require unbounded multi-step computation, such as adding integers (Brown et al., 2020) or executing programs (Austin et al., 2021). Surprisingly, we find that these same models are able to perform complex multi-step computations-even in the few-shot regime-when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train Transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.</p>
<h2>1 INTRODUCTION</h2>
<p>Large Transformer-based language models exhibit surprisingly impressive capabilities (Devlin et al., 2019; Brown et al., 2020), including the ability to generate code that solves simple programming problems (Chen et al., 2021; Austin et al., 2021). However, these models struggle to perform multistep algorithmic calculations, especially those that require precise reasoning and unbounded computation. For example, GPT-3 struggles to perform few-shot addition on numbers with greater than three digits (Brown et al., 2020). Similarly, large-scale language models struggle to predict the result of executing Python code, even code which is a solution to a programming task the model is able to solve (Austin et al., 2021). Likewise, standard recurrent and graph neural networks fail to systematically generalize when predicting the output of simple programs with loops (Bieber et al., 2020). So language models can to some extent write code, but do not seem to accurately represent the semantics of the code they write, because they cannot predict its execution. This has motivated research on networks that can perform algorithmic reasoning (Graves et al., 2014; Zaremba \&amp; Sutskever, 2014; Bieber et al., 2020). Neural networks that accurately represent the semantics of programs could enable a variety of downstream tasks, including program synthesis (Devlin et al., 2017), program analysis (Allamanis et al., 2018), and other algorithmic reasoning tasks (Velickovic \&amp; Blundell, 2021).</p>
<p>Why do large language models struggle with algorithmic reasoning tasks? We suggest that this is at least partly due to a limitation of the way the Transformer architecture is applied to these tasks: the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our scratchpad approach applied to predicting code execution and comparison to direct execution prediction. Top: Previous work has shown that large pre-trained models achieve poor performance when asked to directly predict the result of executing given computer code (Austin et al., 2021). Bottom: In this work, we show that training models to use a scratchpad and predict the program execution trace line-by-line can lead to large improvements in execution prediction performance. N.B. Although the example above only has one loop iteration for each loop, all loops are unrolled across time.
model is asked to perform these tasks in one forward pass. Given a fixed number of layers and a fixed amount of computation time, the model cannot adapt the amount of compute spent on a problem to its difficulty before producing an output. ${ }^{1}$ Prior work (Graves, 2016; Banino et al., 2021) has explored neural architectures that explicitly allow for dynamically chosen amounts of computation time to be dedicated to different sub-tasks. In this work, we propose a different approach-one that can exploit existing Transformer architectures and large few-shot-capable language models-we modify the task design rather than the model or training procedure.</p>
<p>Our proposal is simple: Allow the model to produce an arbitrary sequence of intermediate tokens, which we call a scratchpad, before producing the final answer. For example, on addition problems, the scratchpad contains the intermediate results from a standard long addition algorithm (see Figure 2). To train the model, we encode the intermediate steps of the algorithm as text and use standard supervised training.</p>
<p>This paper makes the following contributions:</p>
<ul>
<li>We introduce (Section 2) the notion of a "scratchpad" for Transformers, in order to make them better at performing complex discrete computations without modifying the underlying architecture.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>We show (Section 3) that scratchpads help Transformers learn to perform long addition in the fine-tuning regime, and in particular that they improve out-of-distribution generalization to larger problem instances.</li>
<li>We also find (Section 4) that scratchpads help Transformers perform a somewhat higher level task: polynomial evaluation. This is true in both the few-shot and fine-tuning regimes.</li>
<li>Finally, we move to a much more general context and show (Section 5) that training Transformers to emit full program traces line by line annotated with local variables dramatically improves their ability to predict the result of executing a given computer program on a particular input. This application in some sense subsumes the others.</li>
</ul>
<h1>2 METHOD</h1>
<p>In this work we consider two related problems: algorithm induction (Graves et al., 2014; 2016; Kurach et al., 2016; Kaiser \&amp; Sutskever, 2016) and learning to execute (Zaremba \&amp; Sutskever, 2014; Bieber et al., 2020). The goal of both problems is for the neural network to learn to emulate a function $f$, which is "algorithmic" in the sense that it can be represented by a short program, such as addition or polynomial evaluation, from input-output behavior. In neural algorithm induction, the goal is to learn a single algorithm, and each training example gives a single input and desired output represented as strings. Therefore, the training data is $D=\left{x_{i}, f\left(x_{i}\right)\right}<em i="i">{i=1}^{N}$. For learning to execute, we want the model to produce the result of a program, represented as source code, on some input. If each $\pi</em>\right)\right)\right}}$ is the source code of a program $f_{i}$, then the training data is $D=\left{\left(\pi_{i}, x_{i}, f_{i}\left(x_{i<em i="i">{i=1}^{N}$ (it is common for each $f</em>$ to have multiple input-output examples, but we omit this to lighten notation).</p>
<p>The main idea of this paper is that to solve a given algorithmic task, we simply encode the intermediate steps of the algorithm as text and train the model to emit them to a buffer that we call a "scratchpad." For example, let us consider the algorithmic induction task of learning long addition. To teach a model to add 29 to 57, a training example may look like the text in Figure 2, where the steps of the grade-school long addition algorithm are written out explicitly.</p>
<p>Learning to execute tasks can be encoded in a similar way, except now we add the source code $\pi_{i}$ before the input, scratchpad, and desired output. An example of a training example for a learning to execute task is shown in Figure 1.</p>
<p>At training time, the model will be given the input plus target for standard likelihood-based training. At test time, the model will be given only the input and will be required to predict the target, e.g., by beam search or temperature sampling. In principle, any sequence model could be used for this. In this work, we choose to use decoder-only Transformer language models, but other sequence models could be effective, such as encoder-decoder models (Raffel et al., 2019), or recurrent networks.</p>
<p>Adding a scratchpad has several potential advantages: First, the model has adaptive computation time, that is, it can now process the information for as long as needed, depending on the complexity of the task given the input. Second, the model can store the intermediate state of its computation in the scratch buffer and refer back to it by attending to its context. This removes the need to store all intermediate state in activations. Third, by forcing the model to output concrete intermediate states by sampling from the generative model, we aim to reduce the propagation and compounding of small errors, because states are quantized to token embeddings. Compounded errors can show up in methods-like Neural Turing Machines (Graves et al., 2014)-that use recurrence to support extended computations. Finally, examining a model's scratchpad output can help us identify common errors and correct them by revising the scratchpad format. We found this ability to interpret errors to be useful in this work.</p>
<div class="codehilite"><pre><span></span><code>Input:
2<span class="w"> </span>9<span class="w"> </span>+<span class="w"> </span>5<span class="w"> </span>7
Target:
<span class="nt">&lt;scratch&gt;</span>
2<span class="w"> </span>9<span class="w"> </span>+<span class="w"> </span>5<span class="w"> </span>7,<span class="w"> </span>C:<span class="w"> </span>0
2<span class="w"> </span>+<span class="w"> </span>5,6C:<span class="w"> </span>1<span class="w"> </span>#<span class="w"> </span>added<span class="w"> </span>9<span class="w"> </span>+<span class="w"> </span>7<span class="w"> </span>=<span class="w"> </span>6<span class="w"> </span>carry<span class="w"> </span>1
,86C:<span class="w"> </span>0<span class="w"> </span>#<span class="w"> </span>added<span class="w"> </span>2<span class="w"> </span>+<span class="w"> </span>5<span class="w"> </span>+<span class="w"> </span>1<span class="w"> </span>=<span class="w"> </span>8<span class="w"> </span>carry<span class="w"> </span>0
0<span class="w"> </span>8<span class="w"> </span>6
<span class="nt">&lt;/scratch&gt;</span>
8<span class="w"> </span>6
</code></pre></div>

<p>Figure 2: Example of input and target for addition with a scratchpad. The carry is recorded in the digit following "C:". Comments (marked by #) are added for clarity and are not part of the target.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Using a scratchpad significantly improves the performance of pre-trained Transformerbased models on addition, including their ability to generalize out of the training distribution to numbers with more digits. Models were trained on 1-8 digit addition. The baseline models were trained without intermediate scratchpad steps.</p>
<p>For all experiments, we used pre-trained dense decoder-only Transformer language models, ranging in size from 2 million to 137 billion parameters. These models were pre-trained on web documents and dialog data, and correspond to the models used in Austin et al. (2021).</p>
<h1>3 Addition</h1>
<p>As a first task, we consider integer addition. The baseline addition task presents two numbers as the input, and the target is their sum. For example: ${ }^{2}$</p>
<p>Input: $29+57$
Target: 86
We implement the scratchpad by including the intermediate steps of the long addition algorithm in the target, as in Figure 2. We train several models on integer addition problems with inputs that have 1-8 digits. We then test performance on in-distribution addition problems (with up to 8 digit inputs), and on out-of-distribution problems with 9 and 10 digit inputs. The models were fine-tuned on 100k examples for 5 k steps with batch size 32 . There are 10 k in-distribution test examples, and 1 k test examples for each out-of-distribution task. We examine the performance as a function of model size, ranging from 2 M to 1 B parameters. We compare performance to a baseline which includes the input and target numbers, but no intermediate scratchpad steps.</p>
<p>Results Figure 3 compares the performance of the scratchpad algorithm with the baseline. We see that beyond a critical model size, models are able to solve the addition task using the scratchpad, while models trained without a scratchpad fail to do so even at the largest tested scale. On the out-ofdistribution tasks ( $9-10$ digit addition), we find that models trained without scratchpad completely fail, while models trained with scratchpad show consistent improvement as a function of model size.</p>
<h2>4 Polynomial Evaluation</h2>
<p>Next we focus on a slightly higher-level task: evaluating polynomials. Inspired by the "polynomial evaluation" subproblem in Saxton et al. (2019), we generate a dataset of polynomials of degree less than or equal to three, with integer coefficients and inputs constrained to the range $[-10,10]$. We also restrict outputs to the range $[-1000,1000]$. We generate a training dataset of 10,000 polynomials and a test dataset of size 2,000. An example scratchpad target for this task is shown in Figure 4, with each term of the polynomial evaluated separately. As in the previous section, we compare the results of direct execution with the results of using the scratchpad. In this experiment, we evaluate in the few-shot regime using a 137B parameter pre-trained decoder-only model, as previous work indicates that very large models may be able to perform additions and multiplications with 3 or fewer digits few-shot (Brown et al., 2020). We use $n=4$ example problems in the few-shot prompt. We also evaluate in the fine-tuning regime with an 8B parameter model fine-tuned for 2000 steps on the training set. The results of both evaluations are shown in Table 1. We find that scratchpad execution outperforms direct execution significantly in both the few-shot and fine-tuning regimes.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>Input:
Evaluate<span class="w"> </span>-7*x**2<span class="w"> </span>+<span class="w"> </span>7*x<span class="w"> </span>+<span class="w"> </span>5<span class="w"> </span>at<span class="w"> </span>x<span class="w"> </span>=<span class="w"> </span>1
Target:
<span class="nt">&lt;scratch&gt;</span>
-7*x**2:<span class="w"> </span>-7
7*x:<span class="w"> </span>7
5:<span class="w"> </span>5
<span class="nt">&lt;/scratch&gt;</span>
total:<span class="w"> </span>5
</code></pre></div>

<p>Table 1: Results for polynomial evaluation task. Scratchpad outperforms direct prediction whether using fine-tuning or few-shot.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Few-shot</th>
<th style="text-align: left;">Fine-tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Direct prediction</td>
<td style="text-align: left;">$8.8 \%$</td>
<td style="text-align: left;">$31.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Scratchpad</td>
<td style="text-align: left;">$\mathbf{2 0 . 1 \%}$</td>
<td style="text-align: left;">$\mathbf{5 0 . 7 \%}$</td>
</tr>
</tbody>
</table>
<p>Figure 4: Example of polynomial evaluation with a scratchpad. Each term in the polynomial is computed separately and then added.</p>
<h1>5 EXECUTING Python Programs</h1>
<p>We have shown that scratchpads can help algorithm induction, that is, they can help models learn to implement a particular algorithm with direct algorithm-specific supervision. But needing to handdesign the intermediate states for every new task is sub-optimal. In this section, we evaluate whether a model can learn to implement a new algorithm by executing arbitrary code. To test this capability, we follow the problem setup from Austin et al. (2021), in which language models are asked to predict the result of executing a given Python program on a particular input. Language models performed poorly at this task, even on programs which are solutions to a programming tasks the model is able to solve. Here we show that the scratchpad technique can dramatically improve the ability of language models to execute programs.</p>
<p>Direct execution prediction Our main baseline is the direct execution prediction procedure explored in Austin et al. (2021). Models are shown the source code for a function, and asked to predict the output of running the function on specific inputs. For example, the function in Figure 1 takes as input a string s and a character ch, and removes the first and last instances of the character ch from the string s. The direct execution prompt and target for this task are shown in the "Direct Execution Prediction" box in Figure 1. A task is considered solved under this regime if the model correctly outputs the target string.</p>
<p>Execution prediction via scratchpad tracing As discussed above, direct execution prediction requires the model to correctly output the result of executing the entire function in a single pass. Direct execution prediction has been shown to perform poorly on Python programs in Austin et al. (2021). We therefore design a scratchpad formulation of the execution task, in which models predict the output of a program by first predicting the sequence of intermediate states computed during the program's execution. Formally, we train models to predict an alternating sequence of 1) the ordered sequence of source code lines executed, and 2) the state of the local variables after each line is executed. We call this object the program's trace, and it allows us to track both the control flowthe sequence of operations executed-and how the state changes as a result of each operation. We represent the trace as a string, with the line of code reproduced directly, and the state information represented as a JSON dictionary. ${ }^{3}$ For example, the "Scratchpad Tracing" box in Figure 1 contains the tracing prompt and trace target for the function discussed above.</p>
<p>Concretely, for each function to be traced, the prompt is formed by printing the function definition, followed by a line which calls the function on a particular input: output = fn_name (input_value), where fn_name and input_value are replaced with the corresponding function name and input value. In Figure 1, note how the correct output of remove_Occ("PHP","P") is shown in the last line of the trace, assigned to the variable "output". A tracing example is considered to have the correct execution output if the encoding of the value assigned to the variable output in the last line is a semantic match with the target output value (here, "output": "P"). We consider a task to be executed correctly if all given input-output examples are correctly executed. We can also test</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Example synthetic Python program.</p>
<p>Table 2: Synthetic tracing and execution results. Scratchpad outperforms direct prediction both for few-shot and fine-tuned.
*The accuracy criterion for the few-shot scratchpad condition was slightly modified, see the text of Section 5.1 for more details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Few-shot</th>
<th style="text-align: left;">Fine-tuned</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Direct prediction</td>
<td style="text-align: left;">$11 \%$</td>
<td style="text-align: left;">$20 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Scratchpad</td>
<td style="text-align: left;">$26.5 \%^{*}$</td>
<td style="text-align: left;">$\mathbf{4 1 . 5 \%}$</td>
</tr>
</tbody>
</table>
<p>whether there is a "trace exact match" between the model prediction and the ground truth trace, by a) semantically comparing each state in the trace to the corresponding state in the ground truth trace, and b) comparing the sequence of source code lines predicted with the ground truth sequence.</p>
<p>Experimental setup As a proof-of-concept, we first show that scratchpad tracing greatly improves execution performance on synthetic Python data. Then, we compare scratchpad tracing and execution on the human-written Python problems from Austin et al. (2021). We find that a novel data augmentation technique that uses programs generated by the model as additional training data can significantly increase tracing performance on real data, whereas this augmentation technique hurts performance for direct execution. Finally, we show that incorporating tracing data from additional sources further improves tracing performance, indicating that the scratchpad tracing technique explored here may scale well with more data.
For all experiments on Python code, we use a Transformer model with around 137 billion parameters, a context window of 1024 tokens and a limit of 512 generation tokens. Unless otherwise stated, all fine-tuning runs used a batch size of 8192 tokens and a learning rate of $3 \mathrm{e}-5$, and model inference was performed with decoding temperature set to $T=0$, equivalent to greedy decoding.</p>
<h1>5.1 Scratchpad Beats Direct Execution for Synthetic Python Programs</h1>
<p>In our first experiment, we test the few-shot and fine-tuned execution capabilities of our models on simple synthetic Python programs. This provides a proof-of-concept for our tracing technique.
We use a dataset of synthetic Python programs modified from Bieber et al. (2020). These programs include small integers ( 0,1 , and 2 ), simple while loops, and if statements. We construct a corpus of synthetic programs to mimic the size of the MBPP dataset in Austin et al. (2021), with 400 training programs, 100 validation programs, and 200 test programs. For each program, three random integer inputs are sampled from the range 0 to 9 .
We test execution and scratchpad tracing under both few-shot and fine-tuning conditions. For fewshot experiments, the prompt contains three examples of previous tracing problems, as shown in Appendix C. For fine-tuned experiments, we fine-tune models to convergence on the training split, as judged by validation perplexity.
For the few-shot scratchpad experiment, we noticed that models would not assign the variable name output to the final value in the trace, and would instead continue using v0 (the name of the variable returned in the function $f$ ) as the variable name for the final output line. We therefore modified the accuracy criterion from checking whether the value of output in the last line of the trace is correct, to checking whether the value of v0 is correct. (Under naive scoring, the few-shot tracing accuracy is roughly zero.) An example of this behavior is shown in Appendix D.</p>
<p>Results Table 2 shows our results on synthetic Python problems. In both few-shot and fine-tuned settings, the scratchpad tracing technique leads to higher overall execution accuracy on the 200 test problems. Fine-tuning also improves performance more for the scratchpad tracing technique than it does for direct execution.</p>
<h1>5.2 Scratchpad Beats Direct Execution for Real Programs</h1>
<p>In our second set of experiments, we explore how well the scratchpad performs compared to execution on real data. Our main evaluation dataset is the MBPP dataset, introduced in Austin et al. (2021). MBPP consists of 1000 programming problems, each of which contains a natural language specification, a ground-truth Python program, and three input-output test cases. These programs involve computation using a large variety of types, including ints, strings, floats, dictionaries, tuples, and more, and include many language features and control-flow structures, such as loops, comprehensions, library imports, API calls and recursion. The evaluation split of the MBPP dataset contains 500 tasks. In order to separate out effects of the generation window size, we report all evaluation metrics on the subset of these tasks for which the ground-truth trace fits within the generation window of the model for all three of the input-output examples. This leaves a subset of 212 test tasks. Increasing generation and context window length is an important issue for Transformer-based models, but we view it as orthogonal and leave it for future work.</p>
<h3>5.2.1 Performance is Poor in the Very-Low-Data Regime</h3>
<p>In our first experiment with the MBPP data, we train a scratchpad tracing model on the 374 training tasks ( 3 examples per task, so 1122 overall examples). We discard all training examples which exceed the context window. We compare overall execution results against a model trained on the same 374 training tasks to perform direct execution. The columns labeled "MBPP" for Direct Execution and Scratchpad in Table 3 show the results of this experiment. Neither the scratchpad model or the direct execution model achieve good performance ( $5 \%$ and $10 \%$ output accuracy, respectively), and direct execution outperforms the scratchpad model.</p>
<h3>5.2.2 Sampled Programs Make Good Scratchpad Training Data</h3>
<p>Next, we employ a data augmentation technique to increase the size of the training dataset: We first run few-shot synthesis on the 374 MBPP training tasks using the pre-trained 137B model, as described in Austin et al. (2021). For each task, we sample and record 80 candidate programs $\left{P_{s}\right}$ from the model at temperature $T=0.5$. We can then create a new execution datapoint using the candidate program $P_{s}$, the original three inputs for the task $\left{x_{i}\right}<em _new="{new" i__text="i_{\text">{i=1,2,3}$, and the three new outputs which result from running the candidate program on the original three inputs: $\left{y</em>\right}}}<em _new="{new" i__text="i_{\text">{i=1,2,3}$, where $y</em>$. This process produces much larger tracing and execution datasets with 17 k new programs, which we refer to as MBPP-aug.
Conceptually, we have augmented the dataset using a combination of tools already available to us, namely a) the neural model, and b) program execution via a Python interpreter. We fine-tune direct execution and scratchpad models on this new augmented dataset MBPP-aug, using the same process as above.}}}=P_{s}\left(x_{i}\right)$. We discard any candidate programs for which execution results in an error. Note that the outputs of $y_{i_{\text {new }}}$ may or may not be equal to the original outputs, depending on the computation performed by the generated program $P_{s}$. Therefore, this augmented direct execution dataset has both additional new programs and new outputs compared to the original dataset. We can analogously create a tracing dataset for our scratchpad model by tracing the execution of each candidate program $P_{s}$ on each $x_{i</p>
<p>The "MBPP-aug" columns in Table 3 show the results of this experiment. While the direct execution approach suffers a decrease in accuracy when trained on this additional data, the performance of the scratchpad model is greatly improved; the model trained on the augmented data solves more than three times the number of tasks as the model trained on only the original MBPP programs. We also note that if we measure the raw correctness across samples, the model already achieves 26.8\% exact trace match, which is surprisingly high.</p>
<h3>5.3 Scratchpad Training Makes Good Use of Large Datasets</h3>
<p>In this section, we examine whether collecting additional tracing data from human-written programs further improves tracing performance. This will allow us to understand whether the tracing procedure here is likely to scale well when slightly out-of-distribution tracing data is added to the fine-tuning set. We experiment using two datasets:</p>
<p>Table 3: Comparison of models fine-tuned on different data sets and evaluated on MBPP programs. We report "per-task" execution and tracing accuracies, which require all examples to be correctly executed/traced. We additionally report "per-example" accuracies, which correspond to the total fraction of test examples which are executed/traced correctly across the dataset. We find that training scratchpad models on an dataset augmented with samples from the model significantly improves performance for the scratchpad model, while it harms the direct execution model. Combining tracing training data from several sources further improves scratchpad model performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Direct execution</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Scratchpad</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">MBPP-aug</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">MBPP-aug</td>
<td style="text-align: center;">MBPP-aug <br> +CodeNet <br> +single line <br> (§5.2.1)</td>
<td style="text-align: center;">MBPP-aug <br> +CodeNet <br> +single line <br> (§5.3)</td>
<td style="text-align: center;">MBPP-aug <br> +single line <br> (§5.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(§5.2.1)</td>
<td style="text-align: center;">(§5.2.2)</td>
<td style="text-align: center;">(§5.2.1)</td>
<td style="text-align: center;">(§5.2.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(§5.3)</td>
<td style="text-align: center;">(§5.3)</td>
</tr>
<tr>
<td style="text-align: center;">per-task execution acc:</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">23.4</td>
</tr>
<tr>
<td style="text-align: center;">per-task trace acc:</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">21.5</td>
</tr>
<tr>
<td style="text-align: center;">per-example execution acc:</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">43.5</td>
</tr>
<tr>
<td style="text-align: center;">per-example trace acc:</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">40.2</td>
</tr>
</tbody>
</table>
<div class="codehilite"><pre><span></span><code><span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">15</span><span class="p">;</span><span class="w"> </span><span class="nl">code</span><span class="p">:</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="k">output</span><span class="err">:</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">7</span><span class="p">;</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="n">g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">100, 100, 0, 0, -100, -100</span><span class="o">]</span><span class="p">;</span>
<span class="w">    </span><span class="nl">code</span><span class="p">:</span><span class="w"> </span><span class="n">g</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">l</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">output</span><span class="err">:</span><span class="w"> </span><span class="n">g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">200</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">100, 100, 0, 0, -100, -100</span><span class="o">]</span><span class="p">;</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;aabbcd&#39;</span><span class="p">;</span><span class="w"> </span><span class="nl">code</span><span class="p">:</span><span class="w"> </span><span class="n">o</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">set</span><span class="p">(</span><span class="n">s</span><span class="p">);</span><span class="w"> </span><span class="k">output</span><span class="err">:</span><span class="w"> </span><span class="n">o</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;c&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;d&#39;</span><span class="err">}</span><span class="p">;</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;aabbcd&#39;</span><span class="p">;</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">63</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">11</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">53</span><span class="p">;</span><span class="w"> </span><span class="nl">code</span><span class="p">:</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="err">&#39;</span><span class="w"> </span><span class="n">j</span><span class="p">;</span><span class="w"> </span><span class="k">output</span><span class="err">:</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">62</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">11</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">53</span><span class="p">;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">a</span>,<span class="w"> </span><span class="nv">b</span>,<span class="w"> </span><span class="nv">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">map</span><span class="ss">(</span><span class="nv">int</span>,<span class="w"> </span><span class="nv">input</span><span class="ss">()</span><span class="w"> </span><span class="nv">split</span><span class="ss">())</span>
<span class="k">if</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nv">b</span>:
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">x</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">1000</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span>:
<span class="w">        </span><span class="nv">print</span><span class="ss">(</span><span class="nv">a</span><span class="o">+</span><span class="ss">(</span><span class="nv">x</span><span class="o">//</span><span class="mi">1000</span><span class="ss">))</span>
<span class="w">    </span><span class="k">else</span>:
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ss">(</span><span class="nv">x</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">1000</span><span class="ss">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">500</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span>:
<span class="w">            </span><span class="nv">print</span><span class="ss">(</span><span class="nv">min</span><span class="ss">(</span><span class="nv">a</span><span class="o">+</span><span class="ss">(</span><span class="nv">x</span><span class="o">//</span><span class="mi">1000</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="ss">)</span>,<span class="w"> </span><span class="nv">a</span><span class="o">+</span><span class="ss">(</span><span class="nv">x</span><span class="o">//</span><span class="mi">1000</span><span class="ss">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">b</span><span class="o">+</span><span class="mi">2</span><span class="ss">))</span>
<span class="w">        </span><span class="k">else</span>:
<span class="w">            </span><span class="nv">print</span><span class="ss">(</span><span class="nv">min</span><span class="ss">(</span><span class="nv">a</span><span class="o">+</span><span class="ss">(</span><span class="nv">x</span><span class="o">//</span><span class="mi">1000</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="ss">)</span>,<span class="w"> </span><span class="nv">a</span><span class="o">+</span><span class="ss">(</span><span class="nv">x</span><span class="o">//</span><span class="mi">1000</span><span class="ss">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">b</span><span class="ss">))</span>
<span class="k">else</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">x</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">500</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span>:
<span class="w">        </span><span class="nv">print</span><span class="ss">(</span><span class="nv">b</span><span class="o">+</span><span class="ss">(</span><span class="nv">x</span><span class="o">//</span><span class="mi">500</span><span class="ss">))</span>
<span class="w">    </span><span class="k">else</span>:
<span class="w">        </span><span class="nv">print</span><span class="ss">(</span><span class="nv">b</span><span class="o">+</span><span class="ss">(</span><span class="nv">x</span><span class="o">//</span><span class="mi">500</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="ss">))</span>
</code></pre></div>

<p>Figure 6: Top: examples of single line data. Bottom: example CodeNet submission.
Single-line programs This dataset consists of roughly 9 million examples of single-line Python transformations. Figure 6 (Top) shows examples of these transformations. Each transformation consists of an initial set of variables and corresponding values, a single line of Python (together these form the input), and the new set of variables and values which results from running the line (the target). When training on single-line data, we do not introduce intermediate scratchpad steps. While this dataset does not provide examples of the high-level, multi-line control flow of a trace, the data provides good supervision for modeling the execution of individual lines of code, which is a key component of tracing. This data was collected by Fraser Greenlee, and can be accessed here.</p>
<p>CodeNet The Project CodeNet dataset (Puri et al., 2021) consists of millions of user submissions to approximately 4,000 coding problems. These submissions include both correct and incorrect solutions to programming problems. However, from the experiment with MBPP-aug above, we know that incorrect or broken programs can still provide a useful training signal. We additionally improved our tracing technique to allow tracing programs with errors; when an error is reached, the error message is added to the end of the trace text and tracing is stopped. We extracted a total of 670,904 traces from the CodeNet data. For each dataset, we first fine-tune the model on these datasets, and then perform a second fine-tuning on MBPP-aug until convergence.</p>
<p>Results Results are shown in Table 3. As above, we report execution accuracy across tasks. We additionally report trace accuracy across tasks, to understand the extent to which the entire trace is</p>
<p>accurately predicted. We also report the raw execution and trace accuracy across all test examples, as an additional metric to compare models.</p>
<p>Training on either the single-line dataset or the CodeNet dataset alone seem to both provide gains over MBPP-aug ( $23.4 \%$ and $25.2 \%$ tasks executed correctly, respectively). However, combining both CodeNet and the single-line dataset seems lead to the highest performance; tracing produces the correct final output for $26.6 \%$ of the tasks, and nearly a quarter of the tasks ( $24.6 \%$ ) are traced perfectly for all three examples. These results seem promising: the neural network can often exactly trace programs. In particular, greedily decoding from the best model produces the exact correct trace for almost $42 \%$ of all traces.</p>
<h1>6 Related Work</h1>
<p>The tasks in this paper can be viewed as exploring one criticism of large language models, namely, to what extent do they simply rely on surface-level statistical correlations on text, without learning semantics or world knowledge (Bender \&amp; Koller, 2020)? In response, Li et al. (2021) provide evidence that pre-trained language models do indeed construct approximate representations of the semantics of the situations they describe in text. In the context of programs, Austin et al. (2021) approach this question by exploring the learning to execute task on MBPP, which we consider in Section 5.2. The idea behind this task was to explore whether neural models for synthesis that generate code could also execute it. While that work finds existing models perform poorly at predicting execution, we show that adding a scratchpad allows these models to perform better.</p>
<p>Work in learning to execute has considered whether off-the-shelf recurrent neural networks (Zaremba \&amp; Sutskever, 2014) or more specialized architectures (Dehghani et al., 2018; Bieber et al., 2020; Wang et al., 2020) have an inductive bias that is sufficiently well suited for executing and reasoning about arbitrary code. The related problem of neural algorithm induction has attracted considerable interest (Graves et al., 2014; Kurach et al., 2016; Kaiser \&amp; Sutskever, 2016; Graves et al., 2016; Reed \&amp; de Freitas, 2016; Veličković et al., 2020a;b). This work proposes new neural architectures, inspired by theoretical models of computation, whose inductive bias allows them to more easily learn algorithm induction tasks. Several methods for algorithm induction specifically add adaptive computation time to sequence models (Graves, 2016; Dehghani et al., 2018; Banino et al., 2021). In particular, universal transformers include adaptive computation time, and are evaluated both on algorithm induction and on learning to execute tasks (Dehghani et al., 2018). In contrast, a scratchpad is a simple way both to provide a transformer model with adaptive computation time, and also to provide supervision about how to use that additional computation, without requiring modification to the underlying architecture.</p>
<p>Algorithm induction has also been connected to pre-trained models. Lu et al. (2021) show that Transformers can be used to some extent as universal computation engines, by pre-training on natural language, and fine-tuning a small fraction of the weights on non-language tasks, including simple algorithm induction tasks. Finally, supervised approaches to semantic parsing (Zelle \&amp; Mooney, 1996; Zettlemoyer \&amp; Collins, 2005; Kwiatkowksi et al., 2010; Wong \&amp; Mooney, 2006) predict the text of a database query, which can then be executed to answer a natural language question.</p>
<h2>7 Limitations and Future Work</h2>
<p>Context window size In this work, we limit our experiments to problems where the scratchpad text fits within the model generation window ( 512 tokens). However, many problems require very long scratchpad generations. Therefore, fully realizing the potential of the scratchpad technique may require further improvements in transformer generation window size. This is an active area of research in NLP (Tay et al., 2020), and improvements would be beneficial for the scratchpad technique.</p>
<p>Learning to use the scratchpad without supervision A clear next step is to try to learn to use the scratchpad without direct supervision. A simple method would be to use reinforcement learning (RL) techniques: models would be rewarded for correctly answering questions, with reward inversely proportional to the number of scratchpad tokens used. We would hope that learning to</p>
<p>use the scratchpad would be a transferable skill; for example, a model could potentially use the algorithm it learned to perform long addition to succeed at polynomial evaluation.</p>
<h1>8 CONCLUSION</h1>
<p>In this work we showed-through experiments on long addition, polynomial evaluation, and Python code execution-that allowing models to read from and write to a simple scratchpad can improve their performance on algorithmic tasks. Such models may be a first step toward combining the knowledge-compression capabilities of large language models with reasoning capabilities, in order to build models that understand code as well as write it. This could be useful for a variety of applications that require both working with natural language and reasoning about program semantics, such as program synthesis, neural-guided program analysis, and interactive programming assistants. The scratchpad technique presented here might not take us all the way toward that goal, but we hope it is an important step.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank Fraser Greenlee for constructing the single-line programs dataset, and Kevin Murphy for bringing this dataset to our attention.</p>
<h2>REFERENCES</h2>
<p>Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs. In International Conference on Learning Representations (ICLR), February 2018.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. In 8th ICML Workshop on Automated Machine Learning (AutoML), 2021.</p>
<p>Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5185-5198, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.463. URL https://aclanthology.org/2020. acl-main. 463 .</p>
<p>David Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow. Learning to execute programs with instruction pointer attention graph neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 8626-8637. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/62326dc7c4f7b849d6f013ba46489d6c-Paper.pdf.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, Will Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew,</p>
<p>Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, July 2021. URL http://arxiv.org/abs/2107.03374.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. July 2018.</p>
<p>Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. CoRR, abs/1703.07469, 2017. URL http://arxiv.org/abs/1703.07469.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.</p>
<p>Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014.</p>
<p>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): $471-476,2016$.</p>
<p>Lukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.</p>
<p>Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. In International Conference on Learning Representations, (ICLR), 2016.</p>
<p>Tom Kwiatkowksi, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 1223-1233, October 2010 .</p>
<p>Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural language models. ArXiv, abs/2106.00737, 2021.</p>
<p>Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines. March 2021.</p>
<p>Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, and Ulrich Finkler. Project CodeNet: A Large-Scale AI for code dataset for learning a diversity of coding tasks. May 2021. URL http://arxiv.org/abs/2105.12655.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.</p>
<p>Scott Reed and Nando de Freitas. Neural programmer-interpreters. In International Conference on Learning Representations (ICLR), 2016. URL http://arxiv.org/pdf/1511.06279v3.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. CoRR, abs/1904.01557, 2019. URL http://arxiv.org/abs/ 1904.01557 .</p>
<p>Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. CoRR, abs/2011.04006, 2020. URL https://arxiv.org/abs/2011.04006.</p>
<p>Petar Velickovic and Charles Blundell. Neural algorithmic reasoning. CoRR, abs/2105.02761, 2021. URL https://arxiv.org/abs/2105.02761.</p>
<p>Petar Veličković, Lars Buesing, Matthew C. Overlan, Razvan Pascanu, Oriol Vinyals, and Charles Blundell. Pointer graph networks, 2020a.</p>
<p>Petar Veličković, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of graph algorithms, 2020b.</p>
<p>Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang. Learning semantic program embeddings with graph interval neural network, 2020.</p>
<p>Yuk Wah Wong and Raymond J Mooney. Learning for semantic parsing with statistical machine translation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics -, Morristown, NJ, USA, 2006. Association for Computational Linguistics.</p>
<p>Wojciech Zaremba and Ilya Sutskever. Learning to execute. ArXiv, abs/1410.4615, 2014.
J Zelle and R Mooney. Learning to parse database queries using inductive logic programming. In National Conference on Artificial Intelligence (AAAI), 1996.</p>
<p>Luke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence, July 2005.</p>
<h1>A Effects of Scratchpad Execution Training on Synthesis PERFORMANCE</h1>
<p>To measure the extent to which fine-tuning on the tracing task described above affects the model's ability to perform program synthesis, we ran a few-shot synthesis experiment using the "MBPPaug + CodeNet + single line" model. Specifically, we performed few-shot synthesis on the MBPP dataset, as described in Austin et al. (2021). For each MBPP synthesis task, 80 candidate programs are sampled from the model $(T=0.5)$, and the task is considered solved if any of the candidate programs satisfy all three test cases. For more details, see Austin et al. (2021). The "MBPP-aug + CodeNet + single line" model achieved an overall synthesis accuracy of $54 \%$, compared to the $62 \%$ accuracy of the original few-shot model in Austin et al. (2021). This indicates that the scratchpad execution training does not completely disrup the model's ability to perform other few-shot tasks.</p>
<h2>B Long Addition Ablation Study</h2>
<p>In our long addition experiments in Section 3, we compared a model that was trained to perform "direct execution" (the baseline) vs a model trained to use a scratchpad. Since the model trained to use the scratchpad gets an additional signal from all the intermediate steps shown, we also study what happens if the scratchpad model is subsequently trained to perform direct execution (i.e., directly output the target without using the scratchpad). The result is shown in Figure 7 where we followed the same training procedure as for the original direct execution baseline and scratchpad models. We see no significant benefits from doing any intermediate training using a scratchpad. This indicates that the extra training-time information seen by the scratchpad model does not seem solely responsible for the scratchpad model's improved performance.</p>
<h2>C EXAMPLE FEW-SHOT PROMPT FOR SYNTHETIC PYTHON EXPERIMENTS</h2>
<p>Below is an example of a prompt for few-shot synthetic Python synthesis problems:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Consider</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">Python</span><span class="w"> </span><span class="k">function</span><span class="err">:</span>
<span class="n">def</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">v0</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="n">v4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="w">        </span><span class="n">v4</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">        </span><span class="n">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">v0</span>
<span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">trace</span><span class="vm">?</span>
<span class="o">[</span><span class="n">BEGIN</span><span class="o">]</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">v0</span><span class="p">)</span><span class="err">:</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;f&quot;</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;&lt;callable_object f&gt;&quot;</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">6</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="n">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">0</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">6</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;v4&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">2</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;v4&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">2</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;v4&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">1</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="n">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">12</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;v4&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">1</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">12</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;v4&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">1</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">12</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;v4&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">0</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="n">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">24</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;v4&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">0</span><span class="err">}</span>
<span class="nl">line</span><span class="p">:</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="k">state</span><span class="err">:</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;v0&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">24</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;v4&quot;</span><span class="err">:</span><span class="w"> </span><span class="mi">0</span><span class="err">}</span>
</code></pre></div>

<p>line: return v0
state: {"f": "<callable_object f>", "output": 24}
[DONE]
Consider the following Python function:
def f(v0):
$v 0-00$
$v 0+02$
$v 0-00$
return $v 0$
output $=\mathrm{f}(4)$
What is the execution trace?
[BEGIN]
state: {}
line: def f(v0):
state: {"f": "<callable_object f>"}
line: output $=\mathrm{f}(4)$
state: {"v0": 4}
line: $v 0-00$
state: {"v0": 4}
line: $v 0+02$
state: {"v0": 6}
line: $v 0-00$
state: {"v0": 6}
line: return v0
state: {"f": "<callable_object f>", "output": 6}
[DONE]
Consider the following Python function:
def f(v0):
$v 0-00$
$v 8=2$
while v8 &gt; 0:
$v 8-1$
$v 0+1$
return $v 0$
output $=\mathrm{f}(4)$
What is the execution trace?
[BEGIN]
state: {}
line: def f(v0):
state: {"f": "<callable_object f>"}
line: output $=\mathrm{f}(4)$
state: {"v0": 4}
line: $v 0-00$
state: {"v0": 4}
line: $v 8=2$
state: {"v0": 4, "v8": 2}
line: while v8 &gt; 0:
state: {"v0": 4, "v8": 2}
line: $v 8-1$
state: {"v0": 4, "v8": 1}
line: $v 0+1$
state: {"v0": 4, "v8": 1}
line: while v8 &gt; 0:
state: {"v0": 4, "v8": 1}
line: $v 8-1$
state: {"v0": 4, "v8": 0}
line: $v 0+1$
state: {"v0": 4, "v8": 0}
line: while v8 &gt; 0:
state: {"v0": 4, "v8": 0}
line: return v0
state: {"f": "<callable_object f>", "output": 4}
[DONE]
Consider the following Python function:</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">v0</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="n">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="n">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">v0</span>
<span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">trace</span><span class="vm">?</span>
<span class="o">[</span><span class="n">BEGIN</span><span class="o">]</span>
</code></pre></div>

<h1>D INCORRECT OUTPUT FOR SYNTHETIC PYTHON PROBLEMS</h1>
<p>Below is an example of the failure of the few-shot synthetic experiments discussed in Section 5.1. Given a problem such as:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Consider</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">Python</span><span class="w"> </span><span class="nv">function</span>:
<span class="nv">def</span><span class="w"> </span><span class="nv">f</span><span class="ss">(</span><span class="nv">v0</span><span class="ss">)</span>:
<span class="w">    </span><span class="nv">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="nv">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="nv">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nv">v0</span>
<span class="nv">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">f</span><span class="ss">(</span><span class="mi">8</span><span class="ss">)</span>
<span class="nv">What</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">execution</span><span class="w"> </span><span class="nv">trace</span>?
</code></pre></div>

<p>The model produces the following trace:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">state</span>:<span class="w"> </span><span class="ss">(</span><span class="s2">&quot;v0&quot;</span>:<span class="w"> </span><span class="mi">8</span><span class="ss">)</span>
<span class="nv">line</span>:<span class="w"> </span><span class="nv">def</span><span class="w"> </span><span class="nv">f</span><span class="ss">(</span><span class="nv">v0</span><span class="ss">)</span>:
<span class="nv">state</span>:<span class="w"> </span>{<span class="s2">&quot;v0&quot;</span>:<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="s2">&quot;f&quot;</span>:<span class="w"> </span><span class="s2">&quot;&lt;function f at 0x00BFC1C0&gt;&quot;</span>}
<span class="nv">line</span>:<span class="w"> </span><span class="nv">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="nv">state</span>:<span class="w"> </span>{<span class="s2">&quot;v0&quot;</span>:<span class="w"> </span><span class="mi">10</span>}
<span class="nv">line</span>:<span class="w"> </span><span class="nv">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">2</span>
<span class="nv">state</span>:<span class="w"> </span>{<span class="s2">&quot;v0&quot;</span>:<span class="w"> </span><span class="mi">20</span>}
<span class="nv">line</span>:<span class="w"> </span><span class="nv">v0</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="nv">state</span>:<span class="w"> </span>{<span class="s2">&quot;v0&quot;</span>:<span class="w"> </span><span class="mi">20</span>}
<span class="nv">line</span>:<span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nv">v0</span>
<span class="nv">state</span>:<span class="w"> </span>{<span class="s2">&quot;f&quot;</span>:<span class="w"> </span><span class="s2">&quot;&lt;function f at 0x00BFC1C0&gt;&quot;</span>,<span class="w"> </span><span class="s2">&quot;v0&quot;</span>:<span class="w"> </span><span class="mi">20</span>}
</code></pre></div>

<p>Note that this trace is exactly correct, except for the last line: where the model predicted "v0": 20, the correct output is "output": 20. Because this type of error consistently occurs in the few-shot synthetic Python experiments, we modified the evaluation script slightly to consider this output to be correct.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: Long addition ablation results. Here, we comparing the baseline and scratchpad results to a model that is first fine-tuned on the scratchpad and then subsequently fine-tuned to perform direct execution (the baseline). The intermediate scratchpad training seem to not have any significant effect on the overall performance, indicating that the extra training-time information seen by the scratchpad model does not seem solely responsible for the scratchpad model's performance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Some objects cannot be represented using this JSON representation. Some objects (such as tuples) are represented by the closest JSON data type (in this case, lists). Other objects, such as user-constructed objects, are represented by a placeholder string, e.g., "<object myObject>". Functions are also represented as strings, e.g., "<callable_object f>".&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
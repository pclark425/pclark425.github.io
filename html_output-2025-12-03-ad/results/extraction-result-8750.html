<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8750 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8750</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8750</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271571605</p>
                <p><strong>Paper Title:</strong> An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice</p>
                <p><strong>Paper Abstract:</strong> Continuing advances in Large Language Models (LLMs) in artificial intelligence offer important capacities in intuitively accessing and using medical knowledge in many contexts, including education and training as well as assessment and treatment. Most of the initial literature on LLMs in medicine has emphasized that LLMs are unsuitable for medical use because they are non-deterministic, may provide incorrect or harmful responses, and cannot be regulated to assure quality control. If these issues could be corrected, optimizing LLM technology could benefit patients and physicians by providing affordable, point-of-care medical knowledge. Our proposed framework refines LLM responses by restricting their primary knowledge base to domain-specific datasets containing validated medical information. Additionally, we introduce an actor-critic LLM prompting protocol based on active inference principles of human cognition, where a Therapist agent initially responds to patient queries, and a Supervisor agent evaluates and adjusts responses to ensure accuracy and reliability. We conducted a validation study where expert cognitive behaviour therapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a blind format. Experienced human CBT-I therapists assessed responses to 100 patient queries, comparing LLM-generated responses with appropriate and inappropriate responses crafted by experienced CBT-I therapists. Results showed that LLM responses received high ratings from the CBT-I therapists, often exceeding those of therapist-generated appropriate responses. This structured approach aims to integrate advanced LLM technology into medical applications, meeting regulatory requirements for establishing the safe and effective use of special purpose validated LLMs in medicine.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8750.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8750.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Therapist-Supervisor (Actor-Critic) / VSC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Therapist-Supervisor actor-critic prompting architecture (Virtual Sleep Coach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-reflect prompting protocol implemented as two LLM agent roles: a Therapist (actor) that drafts an answer and a Supervisor (critic) that reviews the Therapist's output, approves it or produces a corrected response; applied with a Retrieval-Augmented Generation (RAG) domain knowledge base for CBT-I patient queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta LLaMA 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta LLaMA 2 (locally run for both embeddings and response generation). Model size and training details are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Actor-Critic generate-then-reflect (Therapist -> Supervisor)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A two-stage single-pass generate-then-review approach: Therapist agent generates an initial response guided by CBT-I manuals; Supervisor agent (same LLM with different prompting) reviews the Therapist response for correctness/relevance and either approves it or generates a revised response to send to the user. The system is RAG-enabled (retrieves domain-specific document chunks prior to generation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Virtual Sleep Coach (CBT-I patient query answering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>100 representative patient queries based on CBT-I manuals (10 intro + 15 for each of six typical CBT-I sessions). Expert CBT-I therapists rated responses blind on a 1-5 Likert scale (1 harmful … 5 expert-level).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>VSC-generated responses: mean rating 4.327 ± 0.883 (Likert 1-5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to instantiate separate Therapist and Supervisor roles (both using the same LLaMA 2 model) combined with Retrieval-Augmented Generation (RAG) from a domain-specific vectorized knowledge base. Supervisor produces critique/corrections via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: VSC (Therapist+Supervisor) responses were rated higher (mean 4.327 ± 0.883) than human 'Appropriate' therapist responses (mean 4.071 ± 0.828); two-tailed t-test p = 0.044. Qualitative: Supervisor responses often corrected Therapist answers to better match patient intent (examples provided).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The apparent improvement is confounded by response length: VSC responses were significantly longer (419.58 ± 136.59 chars) than human responses (243.51 ± 81.98), and an ANCOVA controlling for length removed the significant main effect of response type (p = 0.895), with response length being a significant predictor. The paper did not report a direct ablation comparing Therapist-only vs Therapist+Supervisor, so the incremental value of the Supervisor is not isolated. Other limitations discussed generally: LLM non-determinism, risk of confabulations/hallucinations, need for domain restriction (RAG) and human oversight, and possible false positives from verifiers in other self-critique paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Discussed qualitatively relative to other generate-and-evaluate approaches and active inference theory; no direct quantitative head-to-head evaluation versus other iterative/self-reflective methods (e.g., Reflexion, SelfCheck, CritiqueLLM) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8750.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8750.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bai et al. self-critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bai et al. (2022) generator + verifier self-critiquing approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-and-verify approach in which one LLM generates plans and a separate verifier LLM critiques or evaluates those plans; reported in this paper as an example where self-critiquing reduced performance due to verifier false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generator + Verifier (self-critiquing)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A pipeline where a generator LLM proposes outputs (plans) and a verifier LLM assesses them and rejects or flags outputs; used as an attempted self-critique method in planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Planning tasks (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LLM planning tasks used to evaluate self-critiquing where a generator proposes plans and a verifier checks them.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported in this paper to diminish performance (qualitative statement); no numerical results provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Separate verifier LLM that evaluates/gates outputs from a generator LLM (prompting / chaining of LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>None in this paper; the cited result indicated decreased performance because the verifier produced false positives (i.e., incorrectly judged some correct plans as incorrect).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Verifier false positives that reduced end-task performance; cited as undermining the effectiveness of simple self-critiquing pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as performing worse in some planning settings contrasted with other critique/self-reflection methods discussed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8750.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8750.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CritiqueLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CritiqueLLM: LLM-as-Critic model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A critique-generation model trained to evaluate LLM-generated texts via dialogue-based prompting and supervised fine-tuning, intended to produce scalable, high-quality critique feedback for improving generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CritiqueLLM (critique-generation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A supervised fine-tuned model trained to generate critiques of LLM outputs; uses dialogue-based prompts and various decoding strategies. Exact base model / parameter sizes not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critique generation (LLM-as-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A trained critic model that receives an LLM output and generates evaluative critiques; designed to be used as feedback for improving LLM outputs, trained via supervised fine-tuning and dialogue-style prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Evaluation / critique of generated text (reference-free and referenced settings)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the critic judges quality of LLM outputs; includes reference-free evaluation scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported in this paper to achieve performance comparable to GPT-4 and to outperform it in certain tasks (particularly reference-free settings); no numeric values provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Supervised fine-tuning of a critique-generation model and utilizing dialogue-based prompting to produce critiques; no external memory required (uses learned critique model).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Claimed to provide scalable feedback and improve LLM generation quality; in-text statement notes parity with or better performance than GPT-4 on some metrics (no numeric detail in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper; general concerns about verifier/critic false positives are discussed elsewhere in the manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as a scalable critique option and contrasted qualitatively with simpler generator+verifier pipelines and other self-reflection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8750.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8750.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Actor, Evaluator, and Self-Reflection models framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning style framework that iteratively improves agent behavior by combining Actor, Evaluator, and Self-Reflection models with dynamic memory for solving complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reflexion framework (multi-component RL/agent architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A framework combining an Actor (generates actions), Evaluator (assesses actions), and Self-Reflection model (produces feedback/updates) with memory mechanisms; implemented in RL-like iterative loops. Exact model sizes not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (iterative self-reflection with memory)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative loop: Actor generates actions, Evaluator scores them, Self-Reflection produces feedback used to adapt future actions; includes dynamic memory to store past actions and reflections to guide future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex task solving / autonomous agent tasks (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks for autonomous agent problem solving; in-text description emphasizes iterative learning and memory-enhanced self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Separate module(s) for evaluation and reflection integrated in an RL-style loop; uses memory to store and apply reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites Reflexion as a framework intended to solve complex tasks via iterative self-reflection; this manuscript does not present numerical outcomes from Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as an advanced iterative self-reflection RL-style approach, contrasted qualitatively with simpler generate-then-verify pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8750.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8750.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheck</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheck: zero-shot step-by-step checker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot checker that has LLMs examine their own chain-of-thought reasoning steps to identify errors, using de-correlated generation and checking plus weighted voting to improve final-answer accuracy without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SelfCheck (LLM-based checker)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method that uses the same or additional LLM calls to check step-by-step reasoning outputs in a zero-shot manner; no parameter sizes are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Zero-shot step-by-step self-checking</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate chain-of-thought; use separate LLM calls (or de-correlated generations) to check each step; combine via weighted voting to produce a corrected final answer without any model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning tasks requiring step-by-step chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require multi-step reasoning where intermediate steps can be inspected and checked for errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported qualitatively to improve final answer accuracy via weighted voting; no numeric results provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Multiple LLM calls with de-correlated generation and checking, then aggregation (weighted voting); implemented via prompting rather than fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Authors report improved final-answer accuracy across various reasoning tasks due to de-correlation between generation and checking and aggregation, but no numeric values are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No failure cases quantified in this manuscript; general computational cost of multiple LLM calls is implied.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as an alternative to fine-tuning and single-pass chain-of-thought; claimed to be versatile across tasks without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8750.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8750.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented method that adapts retrieval and generation through self-reflection; adjusts query handling based on complexity to better retrieve relevant evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-RAG (iterative retrieve/generate/critique)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iteratively adapts retrieval and generation steps using self-reflection to decide how to decomposed queries and whether to retrieve additional context; integrates critique into the RAG pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RAG-enhanced knowledge-intensive QA / multi-step queries</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-intensive question-answering tasks where retrieval decisions affect answer correctness, including multi-hop or multi-step queries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>RAG pipeline augmented with self-reflective adjustments to retrieval and query formulation (prompting and retrieval controller).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Described as a technique to improve retrieval handling for complex queries; no numerical evaluations are given in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Iterative retrieval introduces higher computational cost; no empirical failure cases reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Listed among recent RAG advancements (alongside CRAG, Adaptive-RAG, MIGRES, DRAGIN) that address hallucinations and iterative retrieval needs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8750.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8750.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Corrective-RAG (CRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Corrective Retrieval Augmented Generation (CRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RAG variant that employs a retrieval evaluator to detect and correct hallucinations in retrieval-augmented generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Corrective Retrieval Augmented Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Corrective-RAG (retrieval evaluator + generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Uses a retrieval evaluator module to assess retrieved evidence and correct retrieval-induced hallucinations prior to or during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RAG-based knowledge-intensive generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where retrieval quality strongly affects generation correctness (knowledge-grounded QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Addition of a retrieval-evaluator component to the RAG pipeline (external module); evaluation and correction occur at retrieval time.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as addressing hallucinations in RAG; no numeric results provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this manuscript; general increased complexity and computation implied.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside other RAG improvements (Self-RAG, Adaptive-RAG, MIGRES, DRAGIN) as strategies to improve retrieval-driven generation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8750.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8750.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive-RAG / MIGRES / DRAGIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive-RAG, MIGRES, DRAGIN (recent RAG advances)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A group of recent RAG-related methods: Adaptive-RAG routes queries between different RAG approaches, MIGRES guides iterative retrieval to fill knowledge gaps, and DRAGIN dynamically decides retrieval needs during text generation to meet real-time information demands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Adaptive-RAG / MIGRES / DRAGIN (iterative/dynamic retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Adaptive-RAG: routes queries among multiple retrieval strategies; MIGRES: iterative retrieval to fill knowledge gaps; DRAGIN: dynamic retrieval decisions during generation based on real-time needs. All integrate retrieval decisions into an iterative generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive multi-step QA / RAG scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step, multi-document retrieval and generation tasks where iterative retrieval or routing improves answer assembly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>System-level retrieval controllers and dynamic routing integrated with LLM generation; relies on iterative retrieval and controller heuristics or learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Described as promising approaches to improve retrieval efficiency and answer accuracy in multi-step queries; no empirical numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not elaborated here; paper notes higher computational cost for multistep retrieval and complexity of routing policies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Discussed as part of an ecosystem of RAG improvements aiming to reduce hallucinations and increase efficiency over simpler single-hop retrieval approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation. <em>(Rating: 2)</em></li>
                <li>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. <em>(Rating: 2)</em></li>
                <li>Corrective Retrieval Augmented Generation. <em>(Rating: 2)</em></li>
                <li>Bai et al. (2022) on LLM self-critiquing in planning tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8750",
    "paper_id": "paper-271571605",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Therapist-Supervisor (Actor-Critic) / VSC",
            "name_full": "Therapist-Supervisor actor-critic prompting architecture (Virtual Sleep Coach)",
            "brief_description": "A generate-then-reflect prompting protocol implemented as two LLM agent roles: a Therapist (actor) that drafts an answer and a Supervisor (critic) that reviews the Therapist's output, approves it or produces a corrected response; applied with a Retrieval-Augmented Generation (RAG) domain knowledge base for CBT-I patient queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Meta LLaMA 2",
            "model_description": "Meta LLaMA 2 (locally run for both embeddings and response generation). Model size and training details are not specified in this paper.",
            "reflection_method_name": "Actor-Critic generate-then-reflect (Therapist -&gt; Supervisor)",
            "reflection_method_description": "A two-stage single-pass generate-then-review approach: Therapist agent generates an initial response guided by CBT-I manuals; Supervisor agent (same LLM with different prompting) reviews the Therapist response for correctness/relevance and either approves it or generates a revised response to send to the user. The system is RAG-enabled (retrieves domain-specific document chunks prior to generation).",
            "task_name": "Virtual Sleep Coach (CBT-I patient query answering)",
            "task_description": "100 representative patient queries based on CBT-I manuals (10 intro + 15 for each of six typical CBT-I sessions). Expert CBT-I therapists rated responses blind on a 1-5 Likert scale (1 harmful … 5 expert-level).",
            "performance_with_reflection": "VSC-generated responses: mean rating 4.327 ± 0.883 (Likert 1-5).",
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering to instantiate separate Therapist and Supervisor roles (both using the same LLaMA 2 model) combined with Retrieval-Augmented Generation (RAG) from a domain-specific vectorized knowledge base. Supervisor produces critique/corrections via prompting.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: VSC (Therapist+Supervisor) responses were rated higher (mean 4.327 ± 0.883) than human 'Appropriate' therapist responses (mean 4.071 ± 0.828); two-tailed t-test p = 0.044. Qualitative: Supervisor responses often corrected Therapist answers to better match patient intent (examples provided).",
            "limitations_or_failure_cases": "The apparent improvement is confounded by response length: VSC responses were significantly longer (419.58 ± 136.59 chars) than human responses (243.51 ± 81.98), and an ANCOVA controlling for length removed the significant main effect of response type (p = 0.895), with response length being a significant predictor. The paper did not report a direct ablation comparing Therapist-only vs Therapist+Supervisor, so the incremental value of the Supervisor is not isolated. Other limitations discussed generally: LLM non-determinism, risk of confabulations/hallucinations, need for domain restriction (RAG) and human oversight, and possible false positives from verifiers in other self-critique paradigms.",
            "comparison_to_other_methods": "Discussed qualitatively relative to other generate-and-evaluate approaches and active inference theory; no direct quantitative head-to-head evaluation versus other iterative/self-reflective methods (e.g., Reflexion, SelfCheck, CritiqueLLM) in this paper.",
            "ablation_study_results": null,
            "uuid": "e8750.0",
            "source_info": {
                "paper_title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Bai et al. self-critiquing",
            "name_full": "Bai et al. (2022) generator + verifier self-critiquing approach",
            "brief_description": "A generate-and-verify approach in which one LLM generates plans and a separate verifier LLM critiques or evaluates those plans; reported in this paper as an example where self-critiquing reduced performance due to verifier false positives.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Generator + Verifier (self-critiquing)",
            "reflection_method_description": "A pipeline where a generator LLM proposes outputs (plans) and a verifier LLM assesses them and rejects or flags outputs; used as an attempted self-critique method in planning tasks.",
            "task_name": "Planning tasks (as cited)",
            "task_description": "LLM planning tasks used to evaluate self-critiquing where a generator proposes plans and a verifier checks them.",
            "performance_with_reflection": "Reported in this paper to diminish performance (qualitative statement); no numerical results provided here.",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Separate verifier LLM that evaluates/gates outputs from a generator LLM (prompting / chaining of LLMs).",
            "number_of_iterations": null,
            "evidence_for_improvement": "None in this paper; the cited result indicated decreased performance because the verifier produced false positives (i.e., incorrectly judged some correct plans as incorrect).",
            "limitations_or_failure_cases": "Verifier false positives that reduced end-task performance; cited as undermining the effectiveness of simple self-critiquing pipelines.",
            "comparison_to_other_methods": "Mentioned as performing worse in some planning settings contrasted with other critique/self-reflection methods discussed in the paper.",
            "ablation_study_results": null,
            "uuid": "e8750.1",
            "source_info": {
                "paper_title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CritiqueLLM",
            "name_full": "CritiqueLLM: LLM-as-Critic model",
            "brief_description": "A critique-generation model trained to evaluate LLM-generated texts via dialogue-based prompting and supervised fine-tuning, intended to produce scalable, high-quality critique feedback for improving generation.",
            "citation_title": "CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation.",
            "mention_or_use": "mention",
            "model_name": "CritiqueLLM (critique-generation model)",
            "model_description": "A supervised fine-tuned model trained to generate critiques of LLM outputs; uses dialogue-based prompts and various decoding strategies. Exact base model / parameter sizes not provided in this paper.",
            "reflection_method_name": "Critique generation (LLM-as-critic)",
            "reflection_method_description": "A trained critic model that receives an LLM output and generates evaluative critiques; designed to be used as feedback for improving LLM outputs, trained via supervised fine-tuning and dialogue-style prompts.",
            "task_name": "Evaluation / critique of generated text (reference-free and referenced settings)",
            "task_description": "Tasks where the critic judges quality of LLM outputs; includes reference-free evaluation scenarios.",
            "performance_with_reflection": "Reported in this paper to achieve performance comparable to GPT-4 and to outperform it in certain tasks (particularly reference-free settings); no numeric values provided here.",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Supervised fine-tuning of a critique-generation model and utilizing dialogue-based prompting to produce critiques; no external memory required (uses learned critique model).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Claimed to provide scalable feedback and improve LLM generation quality; in-text statement notes parity with or better performance than GPT-4 on some metrics (no numeric detail in this paper).",
            "limitations_or_failure_cases": "Not detailed in this paper; general concerns about verifier/critic false positives are discussed elsewhere in the manuscript.",
            "comparison_to_other_methods": "Presented as a scalable critique option and contrasted qualitatively with simpler generator+verifier pipelines and other self-reflection methods.",
            "ablation_study_results": null,
            "uuid": "e8750.2",
            "source_info": {
                "paper_title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Actor, Evaluator, and Self-Reflection models framework",
            "brief_description": "A reinforcement-learning style framework that iteratively improves agent behavior by combining Actor, Evaluator, and Self-Reflection models with dynamic memory for solving complex tasks.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "model_name": "Reflexion framework (multi-component RL/agent architecture)",
            "model_description": "A framework combining an Actor (generates actions), Evaluator (assesses actions), and Self-Reflection model (produces feedback/updates) with memory mechanisms; implemented in RL-like iterative loops. Exact model sizes not specified here.",
            "reflection_method_name": "Reflexion (iterative self-reflection with memory)",
            "reflection_method_description": "Iterative loop: Actor generates actions, Evaluator scores them, Self-Reflection produces feedback used to adapt future actions; includes dynamic memory to store past actions and reflections to guide future behavior.",
            "task_name": "Complex task solving / autonomous agent tasks (as cited)",
            "task_description": "Benchmarks for autonomous agent problem solving; in-text description emphasizes iterative learning and memory-enhanced self-reflection.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Separate module(s) for evaluation and reflection integrated in an RL-style loop; uses memory to store and apply reflections.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites Reflexion as a framework intended to solve complex tasks via iterative self-reflection; this manuscript does not present numerical outcomes from Reflexion.",
            "limitations_or_failure_cases": "Not detailed in this paper.",
            "comparison_to_other_methods": "Mentioned as an advanced iterative self-reflection RL-style approach, contrasted qualitatively with simpler generate-then-verify pipelines.",
            "ablation_study_results": null,
            "uuid": "e8750.3",
            "source_info": {
                "paper_title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "SelfCheck",
            "name_full": "SelfCheck: zero-shot step-by-step checker",
            "brief_description": "A zero-shot checker that has LLMs examine their own chain-of-thought reasoning steps to identify errors, using de-correlated generation and checking plus weighted voting to improve final-answer accuracy without fine-tuning.",
            "citation_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.",
            "mention_or_use": "mention",
            "model_name": "SelfCheck (LLM-based checker)",
            "model_description": "A method that uses the same or additional LLM calls to check step-by-step reasoning outputs in a zero-shot manner; no parameter sizes are provided in this paper.",
            "reflection_method_name": "Zero-shot step-by-step self-checking",
            "reflection_method_description": "Generate chain-of-thought; use separate LLM calls (or de-correlated generations) to check each step; combine via weighted voting to produce a corrected final answer without any model fine-tuning.",
            "task_name": "Reasoning tasks requiring step-by-step chain-of-thought",
            "task_description": "Tasks that require multi-step reasoning where intermediate steps can be inspected and checked for errors.",
            "performance_with_reflection": "Reported qualitatively to improve final answer accuracy via weighted voting; no numeric results provided in this paper.",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Multiple LLM calls with de-correlated generation and checking, then aggregation (weighted voting); implemented via prompting rather than fine-tuning.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Authors report improved final-answer accuracy across various reasoning tasks due to de-correlation between generation and checking and aggregation, but no numeric values are given here.",
            "limitations_or_failure_cases": "No failure cases quantified in this manuscript; general computational cost of multiple LLM calls is implied.",
            "comparison_to_other_methods": "Presented as an alternative to fine-tuning and single-pass chain-of-thought; claimed to be versatile across tasks without fine-tuning.",
            "ablation_study_results": null,
            "uuid": "e8750.4",
            "source_info": {
                "paper_title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-RAG",
            "name_full": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "brief_description": "A retrieval-augmented method that adapts retrieval and generation through self-reflection; adjusts query handling based on complexity to better retrieve relevant evidence.",
            "citation_title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-RAG (iterative retrieve/generate/critique)",
            "reflection_method_description": "Iteratively adapts retrieval and generation steps using self-reflection to decide how to decomposed queries and whether to retrieve additional context; integrates critique into the RAG pipeline.",
            "task_name": "RAG-enhanced knowledge-intensive QA / multi-step queries",
            "task_description": "Knowledge-intensive question-answering tasks where retrieval decisions affect answer correctness, including multi-hop or multi-step queries.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "RAG pipeline augmented with self-reflective adjustments to retrieval and query formulation (prompting and retrieval controller).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Described as a technique to improve retrieval handling for complex queries; no numerical evaluations are given in this manuscript.",
            "limitations_or_failure_cases": "Iterative retrieval introduces higher computational cost; no empirical failure cases reported here.",
            "comparison_to_other_methods": "Listed among recent RAG advancements (alongside CRAG, Adaptive-RAG, MIGRES, DRAGIN) that address hallucinations and iterative retrieval needs.",
            "ablation_study_results": null,
            "uuid": "e8750.5",
            "source_info": {
                "paper_title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Corrective-RAG (CRAG)",
            "name_full": "Corrective Retrieval Augmented Generation (CRAG)",
            "brief_description": "A RAG variant that employs a retrieval evaluator to detect and correct hallucinations in retrieval-augmented generation.",
            "citation_title": "Corrective Retrieval Augmented Generation.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Corrective-RAG (retrieval evaluator + generation)",
            "reflection_method_description": "Uses a retrieval evaluator module to assess retrieved evidence and correct retrieval-induced hallucinations prior to or during generation.",
            "task_name": "RAG-based knowledge-intensive generation",
            "task_description": "Tasks where retrieval quality strongly affects generation correctness (knowledge-grounded QA).",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Addition of a retrieval-evaluator component to the RAG pipeline (external module); evaluation and correction occur at retrieval time.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as addressing hallucinations in RAG; no numeric results provided in this paper.",
            "limitations_or_failure_cases": "Not detailed in this manuscript; general increased complexity and computation implied.",
            "comparison_to_other_methods": "Mentioned alongside other RAG improvements (Self-RAG, Adaptive-RAG, MIGRES, DRAGIN) as strategies to improve retrieval-driven generation.",
            "ablation_study_results": null,
            "uuid": "e8750.6",
            "source_info": {
                "paper_title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Adaptive-RAG / MIGRES / DRAGIN",
            "name_full": "Adaptive-RAG, MIGRES, DRAGIN (recent RAG advances)",
            "brief_description": "A group of recent RAG-related methods: Adaptive-RAG routes queries between different RAG approaches, MIGRES guides iterative retrieval to fill knowledge gaps, and DRAGIN dynamically decides retrieval needs during text generation to meet real-time information demands.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Adaptive-RAG / MIGRES / DRAGIN (iterative/dynamic retrieval)",
            "reflection_method_description": "Adaptive-RAG: routes queries among multiple retrieval strategies; MIGRES: iterative retrieval to fill knowledge gaps; DRAGIN: dynamic retrieval decisions during generation based on real-time needs. All integrate retrieval decisions into an iterative generation pipeline.",
            "task_name": "Knowledge-intensive multi-step QA / RAG scenarios",
            "task_description": "Multi-step, multi-document retrieval and generation tasks where iterative retrieval or routing improves answer assembly.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "System-level retrieval controllers and dynamic routing integrated with LLM generation; relies on iterative retrieval and controller heuristics or learned policies.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Described as promising approaches to improve retrieval efficiency and answer accuracy in multi-step queries; no empirical numbers provided in this paper.",
            "limitations_or_failure_cases": "Not elaborated here; paper notes higher computational cost for multistep retrieval and complexity of routing policies.",
            "comparison_to_other_methods": "Discussed as part of an ecosystem of RAG improvements aiming to reduce hallucinations and increase efficiency over simpler single-hop retrieval approaches.",
            "ablation_study_results": null,
            "uuid": "e8750.7",
            "source_info": {
                "paper_title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.",
            "rating": 2,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation.",
            "rating": 2,
            "sanitized_title": "critiquellm_scaling_llmascritic_for_effective_and_explainable_evaluation_of_large_language_model_generation"
        },
        {
            "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.",
            "rating": 2,
            "sanitized_title": "selfcheck_using_llms_to_zeroshot_check_their_own_stepbystep_reasoning"
        },
        {
            "paper_title": "Corrective Retrieval Augmented Generation.",
            "rating": 2,
            "sanitized_title": "corrective_retrieval_augmented_generation"
        },
        {
            "paper_title": "Bai et al. (2022) on LLM self-critiquing in planning tasks",
            "rating": 1,
            "sanitized_title": "bai_et_al_2022_on_llm_selfcritiquing_in_planning_tasks"
        }
    ],
    "cost": 0.0166935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice</p>
<p>Roma Shusterman roma.shusterman@bel.company 
Brain Electrophysiology Laboratory Company
EugeneORUSA</p>
<p>Neurosom, Inc. Eugene OR
USA</p>
<p>Allison C Waters 
Icahn School of Medicine at Mount Sinai
New YorkNYUSA</p>
<p>Shannon O'neill 
Icahn School of Medicine at Mount Sinai
New YorkNYUSA</p>
<p>Phan Luu phan.luu@bel.company 
Brain Electrophysiology Laboratory Company
EugeneORUSA</p>
<p>Neurosom, Inc. Eugene OR
USA</p>
<p>Don M Tucker don.tucker@bel.company 
Brain Electrophysiology Laboratory Company
EugeneORUSA</p>
<p>Neurosom, Inc. Eugene OR
USA</p>
<p>An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice
1433943123832CCDE63C9F8D2675E800
Continuing advances in Large Language Models (LLMs) in artificial intelligence offer important capacities in intuitively accessing and using medical knowledge in many contexts, including education and training as well as assessment and treatment.Most of the initial literature on LLMs in medicine has emphasized that LLMs are unsuitable for medical use because they are non-deterministic, may provide incorrect or harmful responses, and cannot be regulated to assure quality control.If these issues could be corrected, optimizing LLM technology could benefit patients and physicians by providing affordable, pointof-care medical knowledge.Our proposed framework refines LLM responses by restricting their primary knowledge base to domain-specific datasets containing validated medical information.Additionally, we introduce an actor-critic LLM prompting protocol based on active inference principles of human cognition, where a Therapist agent initially responds to patient queries, and a Supervisor agent evaluates and adjusts responses to ensure accuracy and reliability.We conducted a validation study where expert cognitive behaviour therapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a blind format.Experienced human CBT-I therapists assessed responses to 100 patient queries, comparing LLM-generated responses with appropriate and inappropriate responses crafted by experienced CBT-I therapists.Results showed that LLM responses received high ratings from the CBT-I therapists, often exceeding those of therapist-generated appropriate responses.This structured approach aims to integrate advanced LLM technology into medical applications, meeting regulatory requirements for establishing the safe and effective use of special purpose validated LLMs in medicine.</p>
<p>Introduction</p>
<p>Given the remarkable capacity of current generation large language models to access knowledge relevant to a wide range of user queries, there is considerable interest in using chatbots for medical applications, which presents both opportunities and concerns.The medical literature at the time of this writing primarily expresses concerns for the use of LLMs.Gilbert and associates (Gilbert, Harvey, Melvin, Vollebregt, &amp; Wicks, 2023) emphasize that chatbots must be regulated like any other medical device to be safe and effective.However, they conclude that current chatbots cannot be effectively regulated because of their nearinfinite range of possible inputs and outputs and because there is no proven method to limit harmful responses.</p>
<p>In a similar analysis, Mesko and Topol (Meskó &amp; Topol, 2023) review the diverse applications in medicine that LLM chatbots, such as GPT4, could support, and they review the range of potential problems with their use, from determining the liability for harmful responses to monitoring validity when the chatbot is updated through continuous learning.Minssen and associates (Minssen, Vayena, &amp; Cohen, 2023) summarized a similar list of challenges to regulating chatbots in medicine, and they emphasized that the uniqueness of the technology poses difficulties for the agencies in the US and Europe that are attempting to develop a rational regulatory strategy.</p>
<p>Recognizing these obvious concerns, we think it is important to consider that successfully addressing the limitations of LLM technology would significantly benefit patients and physicians needing affordable, point-of-care access to medical knowledge, thereby improving widespread access to effective medical care, particularly by underserved populations.In this paper, we review strategies to optimize the application of LLMs for medical devices and services.The inherent non-deterministic nature of the LLM requires a careful strategy of utilizing the inherently variable output for patient assessment and treatment.The LLMs can produce confabulations (factually incorrect responses presented as accurate information) as well as hallucinations (inappropriate or even bizarre responses).Such errors become more common when the LLMs rely on unverified information or when they struggle with processing complex and ambiguous queries.Even when optimally constructed and prompted, LLMs require careful management, given that their response generation process is inherently non-deterministic and may provide somewhat different responses with repeated presentations of the same question.</p>
<p>We propose a structured framework for refining LLM responses, aiming to integrate advanced technology into medical applications more effectively.This includes restricting the LLM chatbot's primary knowledge base to domain-specific datasets containing validated information essential for generating accurate responses to medical queries, alongside methods to enhance domain-specific document processing for accurate model inputs.Additionally, we introduce an actor-critic LLM prompting protocol reflecting active inference principles in human reasoning, where a Therapist agent initially responds to patient queries, and a Supervisor agent evaluates and adjusts responses to ensure appropriateness and reliability.</p>
<p>Using LLMs for Improving Access to Medical Knowledge</p>
<p>Developers interested in utilizing LLMs for medical device and service applications face three main options for integrating LLMs: (1) develop custom LLMs, (2) finetune general-purpose LLMs, or (3) restrict general-purpose LLMs to provide accurate and domain-relevant responses using prompt engineering and restriction base knowledge for response generation to domain-specific knowledge.</p>
<p>The first option, custom LLMs, is tailored for specific tasks and requires significant AI expertise and computational infrastructure, making them appropriate for large organizations with advanced information technology capabilities.This approach is still evolving, with a growing interest in developing domain-specific LLMs in medicine.</p>
<p>The second option, fine-tuning generalpurpose LLMs, involves adjusting existing models with specific datasets, requiring some AI knowledge and considerable resource investment.This area is witnessing developments, such as Low-Rank Adaptation, that facilitate the tuning process without requiring regeneration of the full LLM.In Low-Rank Adaptation, the LLM's weights are frozen, and low-rank trainable decomposition matrices are inserted in each layer of the Transformer architecture, thereby reducing the training demands for domain-specific applications (Hu et al., 2021).</p>
<p>The third approach, prompting generalpurpose LLMs, provides a major advantage when considered within structured application boundaries because of the emphasis on the integration of verified domain-specific databases into the LLM's retrieval process, which also facilitates validation processes that test retrievals against the domain-specific databases.Additionally, incorporating Retrieval-Augmented Generation (RAG) can enhance this approach by allowing the model to retrieve and integrate relevant information from a large corpus of documents, thus providing more accurate and contextually relevant responses.For certain applications, this approach can be completely adequate, offering a balance between flexibility and resource investment.</p>
<p>The present manuscript emphasizes this third approach of using Application Programming Interfaces (APIs) to prompt general-purpose LLMs.This method involves transforming data into vectors and creating specialized embeddings, which primarily requires straightforward programming in Python.Although this approach still utilizes some organizational resources, it is less resource-intensive compared to custom LLM training or finetuning generic models.</p>
<p>We first provide a brief review of existing methods for improving the contextual knowledge base for a domainspecific application, for optimizing the use of the domain-specific knowledge base, and for optimizing the question-and-answer process so that the LLM's behavior remains predictable and reliable.We then propose an actor-critic LLM programming architecture, including a Therapist agent (actor) that generates responses and a Supervisor agent (critic) that evaluates the Therapist agent's response and proposes an alternative response if the original response is found lacking.We emphasize that the current neuropsychological theory of how human verbal associations are organized may provide insight into how LLMs can be instructed to generate appropriate, valid responses that are at the same time considerate of the user's perspective that is often implicit (not explicitly stated) by their query.Remarkably, by reviewing the Supervisor agent responses, we find that improved sensitivity to the patient's concerns (an important factor in patient interactions in the medical setting) is readily engaged by a simple Supervisor programming (prompting) instruction.</p>
<p>Further, we performed a validation study to test our approach by incorporating expert human oversight in validating AI outputs effectively.Expert CBT-I therapists evaluated the LLM responses in a blind format in which the CBT-I therapist did not know whether the response was generated by a human or the LLM.The findings indicated that LLM responses were rated highly, frequently surpassing the ratings of therapist-generated appropriate responses.This structured approach is encouraging for efforts to integrate advanced LLM technology into medical applications safely.</p>
<p>Approach</p>
<p>Overview of LLM Prompting with RAG Architecture Using a Knowledge Domain-Specific Database</p>
<p>The RAG architecture enhances LLMs by restricting the response generation to a limited knowledge base (such as provided by a set of PDFs summarizing that knowledge), thereby enabling more accurate, contextually relevant, and verifiable outputs than would be obtained by generating responses from the large corpus of the LLM itself (Lewis et al., 2020).This architecture is particularly useful in medical question-answering systems where the LLM can be restricted to drawing on knowledge from a known and validated private database to retrieve and generate responses based on the stored information.As with LLM queries generally, the process involves prompting the LLM with user queries, which the model uses to identify and retrieve relevant data before generating an answer (Figure 1).</p>
<p>The initial step in integrating RAG with a private, domain-specific knowledge database involves loading documents into a manageable format.This typically includes converting various data types (e.g., PDFs, text files, databases) into a uniform format such as JSON or plain text.These documents are then pre-processed to ensure clarity and uniformity, which helps optimize the LLM's retrieval and understanding processes.</p>
<p>Once documents are loaded and preprocessed, they are split into segments that are manageable by the encoding process.This splitting could be based on natural divisions within the documents, such as chapters, sections, or paragraphs.The goal is to create chunks that are sufficiently informative but not overly long, ensuring that the retrieval process remains efficient and focused.Each segment is tagged with metadata to assist in the identification and later retrieval process.</p>
<p>After splitting, each document segment is transformed into a vector representation.These vectors are typically created using embeddings-dense representations that capture the semantic meaning of the text.Techniques such as TF-IDF (Term Frequency -Inverse Document Frequency) determine the relevance of specific terms in relation to the document's general semantics.More advanced neural embeddings like those from BERT or RoBERTa can be used (Lui et al 2019).The resulting vectors are stored in a vector database or vector index, facilitating efficient similarity searches and retrieval.</p>
<p>The retrieval component is critical in the RAG architecture.When a query is received, it is converted into a vector using the same embedding method used for document vectors.The system then performs a similarity search between the query vector and the document vectors stored in the index.The most relevant documents or document segments are then retrieved based on semantic closeness to the query.).Next, split the documents into manageable segments, tag them with metadata, and transform these segments into vector representations using techniques like TF-IDF or neural embeddings.When a query is received from the user, the implementation converts it into a vector, performs a similarity search with the document vectors, retrieves the most relevant segments, and uses the LLM to generate a contextually relevant response.</p>
<p>In the final step, the retrieved documents are used by the LLM to understand the context and generate a response.The LLM integrates the information from these documents into its response generation process, ensuring that the answer is accurate and contextually relevant to the query.The integration of RAG allows the LLM to not only generate answers based on a fixed knowledge base but also to dynamically incorporate new and relevant information from the private knowledge database.</p>
<p>In the following section we review the nature of each processing step and then illustrate the overall process with a specific application.</p>
<p>Preparing LLMs and Domain-Specific Knowledge Bases for Reliable Use</p>
<p>Document Parsing and Simplification</p>
<p>LLMs require inputs to be in straightforward text formats.Certain types of documents pose significant challenges for LLMs.For instance, scientific papers, which often include detailed figures and equations, can be problematic (Figure 2).On the first page of such papers, standard PDF parsers may struggle with 1) relating author names to email addresses, 2) understanding that the DOI and publishing date, when printed vertically, are not related to the manuscript's subject, 3) correctly positioning text associated with figures or tables within the manuscript's content, 4) interpreting a twocolumn text, and 5) excluding publisher details from the content for further analysis.Corporate filings, dense with financial data and tables, are interpreted as indecipherable noise.Moreover, even seemingly straightforward texts, such as technical manuals with intricate formatting and diagrams, can disrupt both optical character recognition processes and LLM functionality, demonstrating the models' limitations in handling varied document formats.</p>
<p>For most moderate-size applications, a manual parsing procedure is adequate to assure the specialized domain documents; alternatively, a dedicated parsing engine may be developed for large document inventories.The goal is to convert the documents into a simpler, separated format that is easily parsed by a specialized Domain AI retrieval system.</p>
<p>Context Retention in Text Chunks</p>
<p>When the LLM processes large documents, the text is typically broken into smaller chunks, risking loss of context.This can be particularly problematic in medical documentation, where specific details of the contextual information are often crucial.Labelling data during chunking will add to each segment a metadata wrapper detailing its context-ensuring that even when segments are reviewed in isolation, their significance and connection to the larger document are preserved.</p>
<p>Text chunking is a crucial aspect of retrieval in natural language processing, dictating how information is organized and stored for efficient retrieval.The choice of chunk size presents a challenge, with no one-size-fits-all solution.However, various methods exist to enhance retrieval, including improved chunking strategies.</p>
<p>Fixed Size Chunking represents a widely adopted method in text segmentation, where the chunk size is predetermined based on the number of tokens.Optionally, some overlap between chunks is maintained to preserve semantic coherence.This approach is often preferred due to its computational efficiency and simplicity, and it does not rely on specialized Natural Language Processing (NLP) libraries.</p>
<p>Recursive Chunking, on the other hand, employs an iterative hierarchical process to divide text into smaller segments using designated separators.If the initial splitting fails to produce chunks of the desired size or structure, the method iteratively refines the process until the desired outcome is achieved.While chunks may not be uniform in size, this method retains the essence of fixed size chunking while accommodating more flexible variations.</p>
<p>Document Specific Chunking takes into account the inherent structure of the document, aligning chunk boundaries with logical sections such as paragraphs or subsections.This preserves the author's organizational framework, enhancing coherence and relevance in retrieved information, especially in structured documents like Markdown or HTML formats.</p>
<p>Semantic Chunking focuses on the semantic relationships within the text, dividing it into semantically meaningful segments.Although slower than other methods, semantic chunking ensures information integrity during retrieval, resulting in more accurate and contextually appropriate outcomes.</p>
<p>Both Document Specific Chunking and Semantic Chunking approaches are proving to be particularly promising approaches for improving the domain-specific knowledge base pipeline.</p>
<p>Enhanced Search and Retrieval</p>
<p>Effective search and retrieval are vital for managing extensive medical databases.</p>
<p>By aligning document ingestion with metadata and vector embedding generation, the search mechanism can retrieve relevant text blocks and their contextual metadata.This enhances the accuracy of responses from Domain AI applications, providing precise information retrieval in medical service and device management.LLMs can produce incorrect answers because their knowledge relies solely on parametric memory.Retrieval-augmented LLMs (Borgeaud et al., 2022;Izacard et al., 2023;Shi et al., 2023) address this shortcoming by integrating non-parametric knowledge with additional retrieval modules.Early work focused on single-hop queries (Lazaridou et al., 2022;Ram et al., 2023), involving retrieving a relevant document and integrating this into Questionand-Answer models.However, multistep queries require connecting multiple documents, demanding iterative access to LLMs and retrievers, introducing high computational costs (Press et al., 2023;Trivedi et al., 2023).</p>
<p>Recent advancements in RAG aim to further enhance LLM performance.Corrective-RAG (CRAG) uses a retrieval evaluator to address hallucinations (Yan et al., 2024).Self-RAG adjusts query handling based on complexity (Asai et al., 2023), while Adaptive-RAG routes queries between different RAG approaches (Jeong et al., 2024).MIGRES guides LLMs to fill knowledge gaps through iterative retrieval (Wang, 2024), and DRAGIN dynamically decides on retrieval needs during text generation (Su et al., 2024).These approaches improve the efficiency, accuracy, and adaptability of LLMs in retrieval-augmented text generation, advancing natural language processing in specific information domains.</p>
<p>Refining Question-Answering Capabilities</p>
<p>A recent focus in LLMs emphasizes their question-answering abilities through self-reflection, verification, and errorcorrection mechanisms.One approach by Bai and associates (Bai et al., 2022) explores LLMs' self-critiquing in planning tasks.It uses a generator LLM to create plans and a verifier LLM to assess them.Results indicate self-critiquing diminishes performance due to false positives from the verifier, questioning the effectiveness of this approach.</p>
<p>Conversely, CritiqueLLM introduces a critique generation model for evaluating LLM-generated texts, using dialogue-based prompting to gather high-quality data.Employing supervised fine-tuning and various decoding strategies, CritiqueLLM achieves performance comparable to GPT-4 and outperforms it in certain tasks, especially in reference-free settings, providing scalable feedback to enhance LLM quality (Ke et al., 2023).</p>
<p>Reflexion, a reinforcement learning framework, integrates Actor, Evaluator, and Self-Reflection models for iterative learning.The Actor generates actions, the Evaluator assesses them, and the Self-Reflection model provides feedback.This process aims to solve complex tasks, advancing AI agents' problem-solving abilities through memory mechanisms (Shinn et al., 2023).</p>
<p>SelfCheck presents a zero-shot, step-bystep checker for identifying errors in LLM reasoning chains, leveraging LLMs' generative abilities and de-correlating errors between generation and checking.It improves final answer accuracy through weighted voting without requiring finetuning, making it versatile across various reasoning tasks (Miao et al., 2023).</p>
<p>These studies collectively enhance LLMs' question-answering capabilities by addressing self-critiquing reliability, critique generation, reinforcement learning with selfreflection, and error identification, paving the way for robust AI systems capable of tackling complex tasks across domains.</p>
<p>LLM Generation and LLM Critical Monitoring: Analogy to Active Inference in Human Cognition</p>
<p>A powerful technique in deep learning is the Generative Adversarial Network, in which the generative capacity of a deep learning network is trained by attempting to overcome the critical capacity of a second network that uses the same network statistics to evaluate the output of the generative network (Creswell et al., 2018).In the development of domain-specific LLMs for medical knowledge access, we propose the use of a second (Supervisor) LLM network to critically evaluate the responses of the primary (Therapist) generative LLM.The power of LLMs, in fact, is that a massive array of human linguistic associations can be accessed and then organized coherently by the LLM architecture.By building on the intrinsic mechanisms of active inference in the human brain (Luu, Tucker, &amp; Friston, 2023), we propose that the LLM applications prompting can provide instructions consistent with the dual generative (actor or therapist) and corrective (critic or supervisor) roles that reflect the implicit human understanding of language roles reflected in the LLM associative matrix.Importantly, because LLMs naturally process the semantics of human language, the prompting of LLMs can be achieved with natural language instructions, which are readily interpreted by the LLM as the appropriate context for its responses.</p>
<p>Active inference is the generic theory that originated with the computational neuroscience model of predictive coding (Friston, 2008;Rao &amp; Ballard, 1999).In the network architecture of the human cerebral cortex, this model proposes that the process of perception begins not with sensory input but rather with the brain's prediction or expectancy of this sensory input (Bastos et al., 2012).This is the active part of active inference, in which the generation of predictions presents an implicit hypothesis (a set of Bayesian priors) for what is to be perceived.The sensory data, processed from receptors initially in the primary sensory cortex, then presents the evidence from the external world that matches or does not the initial expectant prediction.The process of active inference proceeds through errorcorrection of the initial prediction by the evidence in order to align with the adaptive context.</p>
<p>The theory of active inference has proven very powerful in both elementary neuroscience, aligning closely with the functions of cortical networks (Adams, Shipp, &amp; Friston, 2013;Bastos et al., 2012), and also neuropsychology, aligning with the dual motivational controls on generative feedforward expectancy and critical feedback error-correction in natural human cognition (Luu &amp; Tucker, 2023;Luu et al., 2023;Tucker &amp; Luu, 2021, 2023).Given the explanatory value of active inference in describing the mechanisms for forming associations (neural connection strengths) in the human cortex, we can hypothesize that similar (generative feedforward and critical, error-correcting feedback) processes underly the implicit associative semantics of LLMs.By designing the applications programming (instruction and prompting) of dual actor (generative) and critic (error-correcting) LLMs in ways that align with these intrinsic natural biases of human associations, it may be possible to optimize both the creative generative capacities of LLMs (when instructed to emphasize this creative generation) and the critical error-correction capacities (when these are explicitly instructed).</p>
<p>Furthermore, the theory of active inference, when elaborated in relation to the adaptive mechanisms of human motivational control (Luu &amp; Tucker, 2023;Luu et al., 2023;Tucker &amp; Luu, 2021, 2023) explains why actor (generative) functions are intrinsically separated from critic (errorcorrecting) functions.These functions have inherently different forms of motive control in the human brain.The generative actor function operates under a feedforward (loose and impulsive) control bias, proceeding from limbic regions at the core of the hemisphere through limbifugal (fromlimbic) connections through the association cortex toward primary sensory and motor cortices.This is a very useful form of motive control for creative generation, but it is likely to be over-inclusive and error-prone (Tucker &amp; Luu, 2023).</p>
<p>In contrast, the error-correcting critic function in the active inference within cortical networks operates under feedback control (constraint), which is inherently suited to the constraint that sensory data imposes on expectant predictions, thereby achieving effective-error correction.In more general cognition, this form of motive control is integral to critical thinking (Tucker &amp; Luu, 2023).It is, therefore, wellsuited to an evaluative LLM that is critical of responses that are not well-constrained by the domain-specific knowledge base.</p>
<p>In a Bayesian analysis, these dual forms of control are optimized for the variational Bayes adjustments: holding the priors (feedforward) constant while the evidence (feedback) is adjusted, then holding the feedback mechanism constant while the errors in the prediction are adjusted.</p>
<p>Although the underlying network architecture of LLMs is an engineering construction, and not particularly neuromorphic, the intrinsic associations of the human language corpus were indeed generated by human brains.We propose these inherent adaptive biases are implicit within the language corpus, and thus the LLMs, and are therefore best engaged separately for semantic generation (with the actor or Therapist prompting) and semantic constraint (with the critic or Supervisor prompting).</p>
<p>Our theoretical model is that these instructional biases are able to optimize the performance of the dual actor (here, Therapist) and critic (here, Supervisor) LLM instructions because these roles are integral to the way people think.We propose that they are deeply implicit in the semantic associations of each of the large language corpuses that form the basis of LLMs.</p>
<p>Methods</p>
<p>LLM Architecture and Implementation</p>
<p>Content extraction from PDF files was done with the PdfReader class from the PyPDF2 library.Post extraction, the extracted text undergoes a chunking process to divide the large text into manageable pieces.This is implemented using the RecursiveCharacterTextSplitter from the langchain.text_splittermodule.The splitter was configured to segment the text into chunks of approximately 1000 characters, allowing for an overlap of 200 characters between consecutive chunks.The text chunks were then embedded into vector representations using the locally-run LLaMA 2 model, which provides state-ofthe-art language understanding capabilities.These embeddings were stored in Chroma DB, a high-performance vector database designed for efficient storage, retrieval, and similarity searches among large sets of vectors.This enables rapid retrieval and analysis of text data.</p>
<p>The actor-critic architecture for LLM programming of the Virtual Sleep Coach is shown in Figure 3.The LLM for each condition was the Meta LLaMa 2. At left is the Therapist agent (actor) that generates the initial response to the patient, with simple instructions to organize the response from the CBT-I manual (or manuals).An additional specification of the context may be a focus on the specific session of the CBT-I training, but for this illustration and validation experiment we will not segment the context into specific sessions.</p>
<p>At right is the Supervisor agent, which uses the same LLM but with prompting instructions to act as a supervisor, to review the Therapist agent's response and decide if it is appropriate to both the context knowledge base and the intent of the patient's query.</p>
<p>Instructing Patients in Cognitive Behavioral Therapy for Insomnia</p>
<p>To provide a use case to show how these general principles might be implemented, we apply the proposed framework to an education and training chatbot, the Virtual Sleep Coach (VSC), for improving sleep through Cognitive Behavior Therapy for Insomnia (CBT-I).Like other behaviour therapies, CBT-I is highly structured, such that the therapist follows an evidence-based manual, which outlines a prescribed protocol for training the patient to improve sleep habits and practices.Although it is typically administered by a clinical psychologist, the principles are easily understood by physicians and healthcare professionals in general, so an LLM-based CBT-I chatbot can become a cost-effective method for teaching patients about the therapy principles as well as instructing them on the sequential training goals.Although CBT-I software apps are now widely available, patient compliance is often poor due to the considerable discipline required to understand and follow the often challenging treatment regimen.In contrast with a passive software application, the VSC interacts with the patient through natural language, with an engaging and interpersonally sensitive conversation that provides immediate answers to questions and concerns that arise during the therapy process.These features offer important advantages in improving patient compliance.</p>
<p>For example, a key behavioral principle of CBT-I is stimulus control (Perlis et al., 2005), in which the experience of being in bed must be limited to sleeping (and/or sex).Instead, those with insomnia often learn to associate being in bed with anxiety, including worrying about being awake.The discipline to get out of bed when not sleeping is very challenging.It often requires the CBT-I therapist to work carefully with the patient to understand and commit to difficult behaviour changes.Whereas a phone app is not suited to this kind of structured yet compassionate interpersonal interaction, we propose that well-programmed LLMs can be highly successful with the necessary therapeutic interaction to improve patient understanding and compliance, thus achieving real and perceptible therapeutic progress.</p>
<p>Human Expert Validation Study of the Virtual Sleep Coach</p>
<p>Once the Therapist-Supervisor prompting protocol for the VSC was established, we designed a validation study in which expert CBT-I therapists evaluated the VSC responses in a blind format in which the CBT-I therapist did not know whether the response was generated by a human or the VSC.The experienced CBT-I therapists rated each response on a 1-to-5 Lykert scale (1 = potentially harmful, 2 = inappropriate or irrelevant, 3 = adequate but inexpert, 4 = adequate to the therapeutic context, and 5 = consistent with expert therapy).Patient queries (N = 100) were generated by two clinical psychologists experienced with patient interactions in psychotherapy and behavior therapy settings.The queries were based on typical patient queries illustrated in this study's two CBT-I therapist manuals (Perlis, Jungquist, Smith, &amp; Posner, 2005;Taylor et al., 2019).Ten queries were generated relevant to the introduction of CBT-I, and fifteen were generated for each of the six typical CBT-I sessions, resulting in 100 representative patient queries.The typical CBT-I sessions deal with the sleep diary and sleep baseline recording, stimulus control (using the bed only for sleep or sex), sleep restriction, relaxation training, and cognitive restructuring for worry and concerns that interfere with sleep.</p>
<p>For each query, three responses were generated: one by the VSC, monitored and corrected by the Supervisor agent before presentation to the patient; a second appropriate response by an experienced CBT-I therapist; and a third inappropriate response also by a CBT-I therapist, which could be irrelevant, non-therapeutic, insensitive, or harmful.To ensure unbiased evaluation, a second independent expert CBT-I therapist rated all three responses on a 1-5 scale to assess the proportion of VSC responses that were appropriate versus inappropriate.Each of the two CBT-I therapists evaluated 50 queries and rated a total of 150 responses (50 VSC-generated, 50 appropriate CBT-I therapist responses, and 50 inappropriate CBT-I therapist responses), with the order of the responses randomized for each query (Supplementary Table 1).</p>
<p>Results</p>
<p>Consistent with the literature on LLM prompt engineering, we have observed that the LLM's responses are highly sensitive to the context for the communication conveyed by the prompt.Thus, simply assigning the Supervisor role to the LLM's responses, without any specific prompt to be sensitive to the patient's intentions, resulted in the Supervisor agent making a more careful, accurate interpretation of the patient's intended meaning than was initially provided by the Therapist agent.The following example (also Supplementary Table 2) illustrates this sensitivity the LLM showed to the context of the patient's response implied by taking the Supervisor role.The Therapist agent's initial response in this instance focused on improving sleep hygiene, which indirectly addresses worrying but does not directly offer strategies for managing or halting worrying thoughts during waking hours.In contrast, the Supervisor agent provided a revised response emphasizing cognitive restructuring, problem-solving, designated 'worry time,' mindfulness, and grounding techniques.This feedback aimed to ensure the response directly addressed the patient's query on how to stop worrying, aligning more closely with the patient's needs for managing daytime anxiety.</p>
<p>Patient</p>
<p>The results in Figure 4A present the 1-5 Likert scale ratings for responses to 100 patient queries: "Appropriate" CBT-I therapist-generated responses (mean ± std: 4.071 ± 0.828), VSC-generated responses (4.327 ± 0.883), and "Inappropriate" CBT-I therapist-generated responses (1.847 ± 0.923).As anticipated, most "Appropriate" CBT-I therapist responses were rated 4 or 5 by another CBT-I therapist (Figure 4A,left).Unexpectedly, the VSC-generated appropriate responses received even higher ratings from the blind CBT-I therapist (Figure 4A, middle), with very few inappropriate ratings, indicating a statistically significant difference from "Appropriate" responses (two-tailed t-test, p = 0.044).Consistent with expectations, the CBT-I therapist-generated "Inappropriate" responses mainly were rated as inappropriate (Figure 4A,right).</p>
<p>Figure 4B shows the cumulative distribution of difference scores between VSC-generated and CBT-I therapistgenerated appropriate responses, reflecting the distributions seen in Figure 4A, with a greater number of positive scores (indicating higher ratings for VSC responses).Figure 4C similarly displays the distribution of difference scores, corroborating the trend observed in Figure 4B, with VSC responses being rated more favorably overall.</p>
<p>The higher ratings of the VSC responses might be attributed to the length of the responses.On average, the VSCgenerated responses were longer than the human therapists (419.58 ± 136.59 vs 243.51±81.98characters; t-test p=0.0038).These longer responses could have been perceived as more comprehensive or enhanced, contributing to the higher ratings.</p>
<p>To control for the effect of the response length, we performed an analysis of covariance.The main effect of response type ('Appropriate' vs. VSC) on therapist ratings was not statistically significant after controlling for response length (p-value = 0.895).The covariate, response length, was found to be a significant predictor of therapist ratings, indicating that the length of the response has a significant impact on the ratings.</p>
<p>Discussion</p>
<p>The extensive information within general-purpose LLMs can provide relevant and useful medical information but may also produce meaningless, inappropriate, or harmful responses due to the inclusion of inaccurate data.A simple strategy to mitigate this risk is the RAG method, which restricts the knowledge base to verified medical information.In our study, the VSC generated responses based on well-known CBT-I manuals (Perlis et al., 2005;Taylor et al., 2019), ensuring the responses were aligned with typical CBT-I therapist guidance.</p>
<p>To further ensure the appropriateness of VSC responses, we implemented an actorcritic architecture.The Therapist role generated the initial response, which was then reviewed by the Supervisor role for accuracy and relevance.This dual-pass system adds a layer of redundancy and quality control.Literature on active inference suggests human reasoning involves a creative generation phase balanced by error correction through environmental feedback (Bastos et al., 2012;Friston, 2008;Tucker &amp; Luu, 2023).We propose that the separation of actor (Therapist) and critic (Supervisor) roles in LLMs mirrors this cognitive process, optimizing response generation while minimizing errors.</p>
<p>The rationale behind this strategy is rooted in neuropsychological theories of human learning.LLMs, developed from vast linguistic associations, exhibit a form of inferential reasoning similar to human reasoning, which evolves from cumulative, Bayesian experiences rather than formal rules.This is analogous to statistical learning in developmental psychology, where predictive concepts are formed from concrete experiences (Posner &amp; Keele, 1968;Rosch, 1975;Saffran &amp; Kirkham, 2018).</p>
<p>Drawing from active inference principles, programming LLMs with distinct actor and critic roles aligns with natural human linguistic associations.The actor role in human cognition operates in a feedforward manner, generating expectations, while the critic role corrects errors through feedback (Friston, 2008;Luu, Tucker, &amp; Friston, 2023;Tucker &amp; Luu, 2021).This structured approach harnesses the creative potential of LLMs while limiting inappropriate responses in medical contexts.</p>
<p>The inherent sensitivity of the LLM to the implicit interpersonal perspective of a supervisor illustrates the complex implicit human psycholinguistic information that is integral to the LLM corpus and is therefore reflected by the simple associational adjacency of human language captured by the LLM.Humans do not restrict their interpretations to the explicit statements in conversations, but they rather infer the intention of the speaker.This is a wellknown capacity of human linguistic inference described as Gricean implicature (Grice, 1957).The fact that an LLM recognizes that a therapist Supervisor agent must be sensitive to the patient's intention, and not only their explicit utterance, points to the complexity of human linguistic reasoning that canwith appropriate promptingbe coaxed from the LLM.</p>
<p>This observation may be consistent with our general interpretation that the structure of active inference, differentiating the roles of generating responses versus critically validating them, is inherent to natural human linguistic reasoning.Separating the roles of the responding Therapist and the critical Supervisor, and then integrating them within the response given to the person (patient or physician) by the VSC leads to well-formed and accurate responses to the patient's queries.The patient sees only the final responses, but both Therapist and Supervisor responses are logged for each session and can be reviewed by the human responsible for the accurate performance of the VSC.</p>
<p>Our study tested this approach using Meta LLaMa 2, generating responses to queries typical of CBT-I sessions.Blind ratings by trained CBT-I therapists indicated that LLM responses were consistently appropriate, often more so than those of human therapists.This may be attributed to the LLM responses being longer and closely aligned with the CBT-I manuals.</p>
<p>The key finding is that restricting LLM responses to a specific knowledge domain and implementing a secondary evaluation layer ensures relevance and appropriateness in medical applications.While further research is needed to refine LLM design and prompting strategies for medical domains, our results suggest that even basic precautions can enable powerful, validated knowledge delivery technologies, potentially supporting FDA and other regulatory approvals for routine medical use.</p>
<p>Supplemental Materials</p>
<p>Table 1: Examples of ten generated queries, "Appropriate" and "Inappropriate" responses generated by one clinical therapist, "VSC" generated response and 3 ratings of the three responses by the second therapist.</p>
<p>Q: What is the evidence that following Good Sleep Practices really works?A1: CBT-I can be so impactful that it is actually the first line of treatment recommended by the National Sleep Foundation.</p>
<p>Fig 1 .
1
Fig 1.LLM with RAG.Various document types in domain-specific knowledge databases are converted into a uniform format (e.g., JSON).Next, split the documents into manageable segments, tag them with metadata, and transform these segments into vector representations using techniques like TF-IDF or neural embeddings.When a query is received from the user, the implementation converts it into a vector, performs a similarity search with the document vectors, retrieves the most relevant segments, and uses the LLM to generate a contextually relevant response.</p>
<p>Figure 2 .
2
Figure 2. Example of challenges in parsing a PDF page.Numbers 1-5 highlight the problematic pieces of text for off-the-shelf parsers.</p>
<p>Figure 3 .
3
Figure3.Proposed Therapist-Supervisor Framework.The Therapist agent drafts a response and sends it to the Supervisor agent for verification.The Supervisor agent checks the response for correctness.If the response is deemed correct, it is sent to the patient.If the response is incorrect, the Supervisor generates the correct response, which is then sent to the patient.</p>
<p>Figure 4 .
4
Figure 4. A. Number of responses (total = 100) rated (1 = potentially harmful, 2 = inappropriate or irrelevant, 3 = adequate but inexpert, 4 = adequate to the therapeutic context, and 5 = consistent with expert therapy) for each of the sources (CBT-I therapist generated to be appropriate, VSC generated, and CBT-I therapist generated to be inappropriate).B. Cumulative distribution of the difference between VSC responses and therapist appropriate responses.C. Difference between ratings of CBT-I therapist appropriate and VSC responses.</p>
<p>Table 2 :
2
It<code>s hard for me to understand why I have to restrict my sleep time more when I have problem sleeping.A1: The importance of sleep restriction is first address quality.When compressing sleep with sleep restriction, it offers the body the chance to go through a natural sleep cycle and receive the deeper stages.If quality sleep improves, the next step is to add time back to your earliest bedtime for sleep quantity to improve.Sleep restriction therapy aims to consolidate your sleep by limiting the time you spend in bed to match your actual sleep time.By restricting your time in bed, you can improve sleep efficiency and reduce the time spent awake in bed.Initially, this may lead to mild sleep loss, but it can help enhance your sleep drive and strengthen the association between being in bed and sleeping.The goal is to improve the quality and efficiency of your sleep so that you can experience better restorative sleep in the long run.It's a structured approach to help regulate your sleep patterns and improve overall sleep quality.A1: It is recommended that a sleep study be pursued after attempting CBT-i.If you notice insomnia symptoms are improving, but you continue have significant daytime fatigue, it may be time to schedule that appointment.A sleep study, specifically polysomnography (PSG) with EEG, can be beneficial for you at this point.It provides detailed information on different sleep stages, including deep sleep and dream sleep, which can help in understanding your sleep patterns and potential issues like insomnia.By objectively measuring brain activity during sleep, PSG can identify any underlying sleep disorders.It is considered the 'gold standard' in sleep research and can provide valuable insights for developing an effective treatment plan.However, the decision to undergo a sleep study should be made in consultation with a healthcare provider who can assess your individual needs and recommend the most appropriate course of action.Examples of the « Therapist » agent responses to patient queries and « Supervisor » agent correction when needed.Query 1: I often have something on my mind that I worry about.How can I stop worrying?To stop worrying, try not to fall asleep while worrying as it can interfere with sleep.Turn on the light, leave the bedroom, and engage in a different activity like reading a book to distract yourself.Avoid stimulating activities before bed.Put the clock under the bed or out of sight to prevent clock watching, which can lead to frustration and interfere with sleep.Avoid taking naps during the day to help you fall asleep at night.Practicing relaxation techniques may also help alleviate worrying and promote better sleep.Therapist's RESPONSE seems to be wrong.The response provided information on how to improve sleep hygiene and address insomnia, rather than directly addressing the issue of how to stop worrying.While improving sleep can indirectly help reduce worrying, it does not directly address techniques for managing or stopping worrying thoughts.Supervisor Response: "To stop worrying during the day, you can try cognitive restructuring techniques like identifying and challenging negative thoughts, engaging in problem-solving to address your worries, and setting aside specific 'worry time' during the day.Additionally, practicing mindfulness and grounding techniques can help you stay present and reduce anxious thoughts.It's important to address worrying during daytime hours to prevent it from interfering with your sleep at night."Is it ok for me to lay in bed, say during the day, and just scroll on my tablet and relax?Therapist Response: It's not recommended to engage in non-sleep activities in bed, even during the day.It's best to reserve the bed for sleep and intimacy only.Stimulus control instructions suggest leaving the bed if you're awake for more than 15 minutes.Relaxing activities like scrolling on your tablet can hinder the association between bed and sleep.It's advised to find a different spot for activities like reading or using electronic devices to help improve sleep quality.The therapist</code>s RESPONSE is not exactly what I would expect.The provided response does not directly address the patient's question about whether it's okay to scroll on a tablet in bed during the day.The response should
Appropriate3
Q: How important is it for me to complete the sleep diary daily?A1: It`s okay if you miss a few days a week.As long as there is a general reflection of your sleep patterns across 3 days then treatment will still work.Q: At this point, is a sleep study, with EEG, a good thing for me?</p>
<p>focus more on the importance of maintaining the association between bed and sleep by avoiding non-sleep activities in bed.Supervisor Response: It's not recommended to engage in non-sleep activities in bed, even during the day.It's best to reserve the bed for sleep and intimacy only.Stimulus control instructions suggest leaving the bed if you're awake for more than 15 minutes.To improve sleep quality, it's advised to find a different spot for activities like reading or using electronic devices.Query 3: Have you ever had sleep problems yourself?Therapist Response: I am trained to answer questions related to behavioral therapy for insomnia.Supervisor feedback: The therapist<code>s RESPONSE is good Query 8: I read that traffic accidents are often caused by people who fall asleep.Aren</code>t I at risk of a crash when I<code>m restricting sleep in this therapy?Therapist Response: It's understandable to have concerns about sleep restriction therapy and its impact on your safety.However, the therapy is carefully structured to ensure that you still get a safe amount of sleep.By limiting your time in bed to match your average total sleep time, you're likely to experience improved sleep quality and reduced daytime sleepiness, decreasing the risk of accidents due to drowsiness.If you have specific concerns about safety or driving, it's important to discuss them with your therapist for personalized guidance.Supervisor feedback: The therapist</code>s RESPONSE seems to be wrong because it doesn't directly address the patient's concern about the risk of traffic accidents due to sleep restriction in therapy.Supervisor Response: It's important to acknowledge your concerns about the risk of traffic accidents due to sleep restriction in therapy.While sleep restriction therapy is carefully structured to ensure you still get a safe amount of sleep, it's essential to prioritize safety.Discussing any specific concerns you have about safety or driving with your therapist can help tailor the therapy to address these issues and ensure your well-being.Query 9: Of all the patients you<code>ve seen, how many fail the therapy completely?Am I the only one who has such a problem?Therapist Response: I don't have access to individual patient data or records, so I can't provide specific numbers on therapy outcomes.Each individual's response to therapy can vary.It's common for some patients to face challenges during therapy, and you are not alone in experiencing difficulties.It's important to remember that progress in therapy can fluctuate, and setbacks are a normal part of the process.If you're finding therapy challenging, it might be helpful to discuss your concerns with your therapist.Supervisor feedback: The therapist</code>s RESPONSE is good.Query 10: Now that I<code>m sleeping better, can I stay up late sometimes on the weekend?Therapist Response: It's essential to maintain a consistent sleep schedule, including on weekends, to support your circadian rhythm.Staying up late can disrupt your progress in improving your sleep patterns.Consistency in sleep timing is crucial for long-term benefits.It's advisable to stick to your regular sleep schedule to continue experiencing better sleep.Supervisor feedback: The therapist</code>s RESPONSE is good.
Predictions not commands: active inference in the motor system. R A Adams, S Shipp, K J Friston, Brain Structure and Function. 21832013</p>
<p>. A Asai, Z Wu, Y Wang, A Sil, </p>
<p>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. H Hajishirzi, ArXiv, abs/2310.11511Bastos, A. M., Usrey, W. M.2023Adams, R</p>
<p>Canonical microcircuits for predictive coding. A Mangun, G R Fries, P Friston, K J , Neuron. 7642012</p>
<p>Large language model AI chatbots require approval as medical devices. X Chen, M Lin, N Schärli, D Zhou, S Gilbert, H Harvey, T Melvin, E Vollebregt, P Wicks, A Creswell, T White, V Dumoulin, K Arulkumaran, B Sengupta, A A Bharath, ArXiv, abs/2304.05128IEEE signal processing magazine. 3512023. 2023. 2018Nature medicine</p>
<p>Large language model AI chatbots require approval as medical devices. K Friston, S Gilbert, H Harvey, T Melvin, E Vollebregt, P Wicks, PLoS Comput Biol. 4112008. 2023Nature medicine</p>
<p>. P Grice, Meaning. Philosophical Review. 661957</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, . . Chen, W Ke, P Wen, B Feng, Z Liu, X , arXiv:2106.09685Lora: Low-rank adaptation of large language models. Lei, X.2021arXiv preprint</p>
<p>J Cheng, S Wang, A Zeng, Y Dong, H Wang, J Tang, M Huang, CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation. 2023</p>
<p>P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Yih, T Rocktäschel, S Riedel, D Kiela, https:/doi.org/10.48550/arXiv.2005.11401Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. 2020</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019</p>
<p>Continuity and change in neural plasticity through embryonic morphogenesis, fetal activitydependent synaptogenesis, and infant memory consolidation. P Luu, D M Tucker, Developmental Psychobiology. 658e224392023</p>
<p>Vertical Integration of Motivational Control Across the Evolved Levels of the Human Neuraxis. P Luu, D M Tucker, K Friston, N Miao, Y W Teh, T Rainforth, SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. 2023. 2023</p>
<p>. A Madaan, N Tandon, P Gupta, </p>
<p>The imperative for regulatory oversight of large language models (or generative AI) in healthcare. S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Welleck, B Majumder, S Gupta, A Yazdanbakhsh, P Clark, B Meskó, E J Topol, T Minssen, E Vayena, I G Cohen, M L Perlis, C Jungquist, M T Smith, D Posner, M Zhang, S Min, L Schmidt, N A Smith, M Lewis, R P Rao, D H Ballard, ArXiv, abs/2303.17651Nature Neuroscience. 612023. 2023. 2023. 2005. 2023. 1999EMNLPDigital Medicine</p>
<p>DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models. N Shinn, B Labash, A Gopinath, W Su, Y Tang, Q Ai, Z Wu, Y Liu, D Taylor, A Peterson, J Goodie, ArXiv, abs/2403.100812023. 2024Reflexion: an autonomous agent with dynamic memory and self-reflection</p>
<p>Adaptive control of functional connectivity: dorsal and ventral limbic divisions regulate the dorsal and ventral neocortical networks. E Grieser, A Hryshko-Mullen, A Rowan, . . Hall-Clark, B ; Tucker, D M Luu, P Tucker, D M Luu, P Trivedi, T Balasubramanian, N Khot, T Sabharwal, A , Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsTucson, AZ2019. 2021. 2023. 2023128University of ArizonaInterleaving retrieval with chain-of-thought reasoning for knowledge intensive multi-step questions</p>
<p>Large Language Models are reasoners with Self-Verification. Y Weng, M Zhu, B Li, S He, K Liu, J Zhao, S Yan, J Gu, Y Zhu, Z Ling, ArXiv, abs/2212.095612022. 2024</p>
<p>. ArXiv, abs/2401.15884Corrective Retrieval Augmented Generation. </p>            </div>
        </div>

    </div>
</body>
</html>
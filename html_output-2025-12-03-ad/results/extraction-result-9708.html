<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9708 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9708</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9708</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-277313413</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.18865v3.pdf" target="_blank">Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models offers new possibilities for structured exploration of scientific knowledge. Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights. Specifically, we investigate how knowledge unit--especially those tied to methodological design--can be modeled and recombined to yield research breakthroughs. Our proposed framework addresses two key challenges. First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problem-driven contexts. Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential. This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9708.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9708.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disruption Index</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bibliometric metric that quantifies the extent to which a focal scientific work displaces prior work versus reinforcing it, computed from citation relationships between the focal paper, its references, and subsequent citing papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Qwen-7B (fine-tuned) as primary prediction LLM; compared against GPT-4o, GPT-4 Turbo, Claude, SciBERT, RoBERTa, LLaMA 3, Qwen-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>The primary experimental LLM is Qwen-7B fine-tuned with LoRA/PEFT adapters; experiments also compare general-purpose LLMs (GPT-4o/GPT-4 Turbo/Claude) and various PLMs (SciBERT, RoBERTa, LLaMA 3, Qwen-32B).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-domain (computer science/AI via DBLP, biomedical depression research via PubMed, medical-robotics patents via PatSnap).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated predictive model that maps problem-method summaries and extracted reference information to a predicted Disruption Index (DI); compared predictions to ground-truth DI values computed from citation graphs. Also used as feedback signal in iterative method optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary target metric is Disruption Index (D); prediction quality assessed by regression errors (MSE, MAE) and entropy-weighted variants (WMSE, WMAE). For search/selection tasks, hit rate of high-disruptiveness (DI > 0.5) is used.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Three citation datasets: DBLP (14,533 CCF-A AI publications 2011–2021), PubMed (96,612 depression-related articles 2015–2025), PatSnap (6,677 active medical-robotics patents 2020–2025).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The proposed DI-based evaluation framework achieves substantially lower prediction errors than baselines (e.g., Our Framework on DBLP: MSE=0.0093, MAE=0.0111, WMSE=0.0941, WMAE=0.1364; PubMed: MSE=0.0154, MAE=0.0342; PatSnap: MSE=0.0218, MAE=0.0412). When used as optimization feedback, the Greedy+GPP search yielded hit rates for DI>0.5 of 26.3% (DBLP), 28.1% (PubMed), and 24.6% (PatSnap), improving ~9.1 percentage points over best single-LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on historical citation data — performance degrades in nascent fields with little prior literature; multi-step pipeline increases computational cost and latency; DI is a retrospective proxy and depends on citation behavior which can be biased.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Paper argues DI better captures transformative impact than raw citation counts and complements human preference/alignment evaluations; empirically, DI-driven automated evaluation produced more discriminative predictive performance than general LLM outputs and standard bibliometrics.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use DI as an objective complement to human assessment for evaluating LLM-generated scientific theories; fine-tune summarization and DI predictors on domain data; apply entropy-weighted loss and secondary learning to emphasize rare high-DI examples; use DI as an iterative reward signal when optimizing method combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9708.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9708.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Problem-Method Combination Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Problem-Driven Method Exploration + Disruptive Knowledge Prediction + Dynamic Method Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured pipeline that (1) retrieves candidate methods relevant to a new research problem, (2) summarizes and extracts key reference information, (3) predicts Disruption Index for problem-method pairs, and (4) iteratively optimizes method combinations using DI feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LoRA-fine-tuned LLMs (summarization & prediction models; experiments based on Qwen-7B/Qwen-32B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Task-specific LLMs are fine-tuned with Low-Rank Adaptation (LoRA) adapters via PEFT; summarization model handles up to 1000 tokens and DI predictor up to 7000 tokens per configuration described.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-domain application (AI, biomedical research, patents) as above.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>End-to-end evaluation: (a) summarize problem-method pairs; (b) retrieve top-k candidate references; (c) feed summaries and extracted reference info into DI predictor; (d) optionally iterate with optimization (GPP). Measured with summarization similarity (cosine/ROUGE), DI prediction errors, and hit rate for high-DI discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Retrieval relevance (semantic similarity threshold δ, top-k), summarization quality (cosine similarity, ROUGE), DI prediction errors (MSE/MAE/WMSE/WMAE), and optimization hit rate (DI>0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Same DBLP, PubMed, PatSnap datasets; structured literature index of problem and method entries used for retrieval and candidate construction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Framework components together outperform baselines across summarization and DI-prediction tasks (see summarization and DI metrics). Ablation shows each module (summarization fine-tuning, relevance/extraction, secondary learning, deviation-aware alignment) contributes materially to final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires a structured literature database with problem-method indexing; multi-step design increases runtime; performance tied to quality of extraction and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Framework emphasizes traceability to source literature and quantitative DI-based assessment rather than pure free-form LLM ideation and human-preference-only evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Index literature by problem and method to enable rapid retrieval; fine-tune summarization models for concise, logic-preserving summaries; perform relevance assessment and information extraction before DI prediction to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9708.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9708.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DI Prediction Techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy-weighted Loss, Secondary Learning, Deviation-aware Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of training and stabilization techniques applied to the DI prediction model: entropy-weighted loss upweights rare high-DI samples, secondary learning re-trains on top-error samples, and deviation-aware iterative feedback mitigates bias and stabilizes predictions with a KL-balancing term.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Qwen-7B (fine-tuned with LoRA adapters) as prediction backbone in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Predictor is a frozen-pretrained LLM with LoRA fine-tuning; training uses Adam optimizer and PEFT-inserted adapters described in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-domain (same experimental domains).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Supervised regression to ground-truth DI with specialized losses and training curricula; ablation tests evaluate the contribution of each technique via changes in MSE/MAE/WMSE/WMAE.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>MSE, MAE, WMSE, WMAE; secondary learning selects top ~20% highest-error examples; KL-divergence used to balance primary vs secondary distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Training/test splits on DBLP, PubMed, PatSnap with DI labels computed from citation graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using entropy-weighted loss, secondary learning, and deviation-aware alignment substantially reduced prediction errors; e.g., on DBLP full model MSE=0.0093 vs w/o secondary learning MSE=0.0187 and w/o deviation-aware alignment MSE=0.0216 (Table 4), demonstrating sizable ablation impacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Secondary learning risks prediction drift without KL balancing; entropy weighting depends on accurate class/rarity estimates; requires careful hyperparameter tuning (weights, top-percent selection).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These techniques are automated strategies to emphasize rare disruptive examples — an alternative to manual curating of high-impact cases or human reweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Apply entropy-weighted loss to emphasize rare disruptive samples, use a secondary learning pass on the hardest examples with KL regularization to prevent drift, and feed iterative deviation feedback into model updates to improve calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9708.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9708.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summarization Evaluation (cosine / ROUGE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Problem-Method Summarization Evaluation via Cosine Similarity and ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative evaluation of generated concise problem-method summaries by measuring semantic similarity (cosine between embeddings) and n-gram overlap metrics (ROUGE) against ground-truth summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Qwen-32B (fine-tuned) used for summarization evaluation in reported experiments; compared to GPT-4o, GPT-4 Turbo, Claude variants.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Qwen-32B is a larger pre-trained variant of Qwen used both pre-trained and fine-tuned (LoRA) for summarization; fine-tuning improved alignment to ground-truth summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied to AI (DBLP), biomedical (PubMed), and patent (PatSnap) corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate problem-method summary S = G_theta(P, M, Prompt) and compute cosine similarity against ground-truth summary vector and ROUGE scores for n-gram overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Cosine similarity of embedding vectors and ROUGE (reported values), used to evaluate logical consistency and completeness of summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>DBLP, PubMed, PatSnap subsets with ground-truth problem-method summaries created from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned Qwen-32B improved summarization over baselines: DBLP similarity 0.558 / ROUGE 0.325 (fine-tuned) vs Qwen-32B pre-trained similarity 0.552 / ROUGE 0.315; PubMed 0.500/0.324; Patents 0.612/0.404 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ROUGE and cosine similarity capture surface and embedding-level alignment but do not fully measure scientific validity, plausibility, or traceable support to references.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Metrics provide reproducible quantitative signal but must be paired with downstream DI prediction and reference-based validation to assess scientific usefulness beyond lexical similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Fine-tune summarization models on task-specific ground truth; use both embedding-based and lexical metrics; follow summarization with relevance-assessment to ensure traceable literature grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9708.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9708.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPP (Greedy with Probabilistic Perturbation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy with Probabilistic Perturbation search algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search strategy that primarily takes greedy steps to increase the objective (predicted DI) but with small probability ε accepts a random non-optimal neighbor to escape local optima, combined with adaptive regularization and temporal smoothing of DI feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>DI predictor + search controller implemented around fine-tuned models (experiments centered on Qwen-7B-based predictors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Search uses the DI prediction model as a scoring oracle; method neighborhoods generated by replacing/augmenting/modifying components of candidate methods.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied to method combination search in AI, biomedical, and patent domains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Iterative optimization: at each iteration evaluate candidate neighborhood N(M_t) with DI predictor and update method M_t+1 by greedy argmax with probability 1-ε or random neighbor with probability ε. Regularization C(M) prevents overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Hit rate of identified method combinations with true DI>0.5 (measured against ground-truth DI), plus convergence behavior and ability to escape local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>DBLP, PubMed, PatSnap candidate search spaces constructed from indexed problem-method pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPP outperformed standard greedy: hit rates improved from Standard Greedy 19.3%/21.5%/18.2% to Greedy+GPP 26.3%/28.1%/24.6% on DBLP/PubMed/PatSnap respectively (Table 5), an improvement of ≈+9.1 percentage points over best LLM baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires careful tuning of perturbation probability ε and penalty/regularization terms; random perturbations may increase wall-clock evaluations; success depends on DI predictor fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>GPP automates exploration/exploitation trade-offs that human search heuristics perform manually, showing measurable improvements in locating historically disruptive combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use small ε to balance exploration; incorporate regularization C(M) and temporal smoothing of DI feedback; couple with high-fidelity DI predictor to guide global search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9708.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9708.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior LLM/Ideation Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IdeaBench, Creativity Index, MOOSE-Chem, Chain-of-Ideas, Idea Arena, ResearchAgent (mentioned benchmarks/frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Existing benchmarks and frameworks for evaluating LLM-generated research ideas and ideation quality, typically measuring originality, feasibility, impact, reliability, or using multi-agent and human-alignment protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various (benchmarks/frameworks evaluate different LLMs; examples referenced include GPT family, multi-agent LLM systems and specialized fine-tuned models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not a single model — these works propose evaluation protocols (IdeaBench), indices (Creativity Index with DPO), chemistry-focused multi-agent frameworks (MOOSE-Chem), chain-structuring systems (Chain-of-Ideas), and iterative reviewer/agent systems (ResearchAgent).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Primarily cross-domain ideation benchmarks; MOOSE-Chem is chemistry-specific; ResearchAgent and Chain-of-Ideas apply across scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human preference alignment, supervised fine-tuning with DPO, multi-agent decomposition, chain-structuring of literature, and structured reviewer agents; mostly text/idea-quality-centric evaluations rather than citation-based disruptiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Originality, feasibility, impact, reliability, human preference alignment, and idea coherence; these differ from DI and do not directly measure long-term citation-driven disruptiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>IdeaBench (idea evaluation benchmark), Creativity Index datasets/methods (supervised labels), MOOSE-Chem chemistry hypotheses datasets, Chain-of-Ideas literature chains, ResearchAgent retrieval+review corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>These prior benchmarks provide complementary evaluation axes (creativity, feasibility, human preference) but the paper highlights they lack an objective disruptiveness metric like DI and often emphasize textual quality over traceable, citation-grounded impact.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Predominantly rely on human judgments or semantic similarity measures, risk of hallucinated unsupported ideas, and limited objective quantification of long-term scientific impact.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Prior benchmarks generally align with human evaluators; paper positions DI-based evaluation as a quantitative complement to these human-centered metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Combine idea-quality benchmarks with citation-grounded metrics (like DI) and enforce traceability to literature to reduce hallucination and increase verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A dynamic network measure of technological change <em>(Rating: 2)</em></li>
                <li>Idea-bench: How far are generative models from professional designing? <em>(Rating: 2)</em></li>
                <li>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models. <em>(Rating: 2)</em></li>
                <li>Chain of ideas: Revolutionizing research via novel idea development with llm agents. <em>(Rating: 2)</em></li>
                <li>Empowering AI as autonomous researchers: Evaluating LLMs in generating novel research ideas through automated metrics. <em>(Rating: 2)</em></li>
                <li>Combination of research questions and methods: A new measurement of scientific novelty. <em>(Rating: 1)</em></li>
                <li>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9708",
    "paper_id": "paper-277313413",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "DI",
            "name_full": "Disruption Index",
            "brief_description": "A bibliometric metric that quantifies the extent to which a focal scientific work displaces prior work versus reinforcing it, computed from citation relationships between the focal paper, its references, and subsequent citing papers.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Qwen-7B (fine-tuned) as primary prediction LLM; compared against GPT-4o, GPT-4 Turbo, Claude, SciBERT, RoBERTa, LLaMA 3, Qwen-32B",
            "llm_description": "The primary experimental LLM is Qwen-7B fine-tuned with LoRA/PEFT adapters; experiments also compare general-purpose LLMs (GPT-4o/GPT-4 Turbo/Claude) and various PLMs (SciBERT, RoBERTa, LLaMA 3, Qwen-32B).",
            "scientific_domain": "Multi-domain (computer science/AI via DBLP, biomedical depression research via PubMed, medical-robotics patents via PatSnap).",
            "evaluation_method": "Automated predictive model that maps problem-method summaries and extracted reference information to a predicted Disruption Index (DI); compared predictions to ground-truth DI values computed from citation graphs. Also used as feedback signal in iterative method optimization.",
            "evaluation_criteria": "Primary target metric is Disruption Index (D); prediction quality assessed by regression errors (MSE, MAE) and entropy-weighted variants (WMSE, WMAE). For search/selection tasks, hit rate of high-disruptiveness (DI &gt; 0.5) is used.",
            "benchmark_or_dataset": "Three citation datasets: DBLP (14,533 CCF-A AI publications 2011–2021), PubMed (96,612 depression-related articles 2015–2025), PatSnap (6,677 active medical-robotics patents 2020–2025).",
            "results_summary": "The proposed DI-based evaluation framework achieves substantially lower prediction errors than baselines (e.g., Our Framework on DBLP: MSE=0.0093, MAE=0.0111, WMSE=0.0941, WMAE=0.1364; PubMed: MSE=0.0154, MAE=0.0342; PatSnap: MSE=0.0218, MAE=0.0412). When used as optimization feedback, the Greedy+GPP search yielded hit rates for DI&gt;0.5 of 26.3% (DBLP), 28.1% (PubMed), and 24.6% (PatSnap), improving ~9.1 percentage points over best single-LLM baselines.",
            "limitations_or_challenges": "Relies on historical citation data — performance degrades in nascent fields with little prior literature; multi-step pipeline increases computational cost and latency; DI is a retrospective proxy and depends on citation behavior which can be biased.",
            "comparison_to_human_or_traditional": "Paper argues DI better captures transformative impact than raw citation counts and complements human preference/alignment evaluations; empirically, DI-driven automated evaluation produced more discriminative predictive performance than general LLM outputs and standard bibliometrics.",
            "recommendations_or_best_practices": "Use DI as an objective complement to human assessment for evaluating LLM-generated scientific theories; fine-tune summarization and DI predictors on domain data; apply entropy-weighted loss and secondary learning to emphasize rare high-DI examples; use DI as an iterative reward signal when optimizing method combinations.",
            "uuid": "e9708.0",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Problem-Method Combination Framework",
            "name_full": "Problem-Driven Method Exploration + Disruptive Knowledge Prediction + Dynamic Method Optimization",
            "brief_description": "A structured pipeline that (1) retrieves candidate methods relevant to a new research problem, (2) summarizes and extracts key reference information, (3) predicts Disruption Index for problem-method pairs, and (4) iteratively optimizes method combinations using DI feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "LoRA-fine-tuned LLMs (summarization & prediction models; experiments based on Qwen-7B/Qwen-32B variants)",
            "llm_description": "Task-specific LLMs are fine-tuned with Low-Rank Adaptation (LoRA) adapters via PEFT; summarization model handles up to 1000 tokens and DI predictor up to 7000 tokens per configuration described.",
            "scientific_domain": "Multi-domain application (AI, biomedical research, patents) as above.",
            "evaluation_method": "End-to-end evaluation: (a) summarize problem-method pairs; (b) retrieve top-k candidate references; (c) feed summaries and extracted reference info into DI predictor; (d) optionally iterate with optimization (GPP). Measured with summarization similarity (cosine/ROUGE), DI prediction errors, and hit rate for high-DI discoveries.",
            "evaluation_criteria": "Retrieval relevance (semantic similarity threshold δ, top-k), summarization quality (cosine similarity, ROUGE), DI prediction errors (MSE/MAE/WMSE/WMAE), and optimization hit rate (DI&gt;0.5).",
            "benchmark_or_dataset": "Same DBLP, PubMed, PatSnap datasets; structured literature index of problem and method entries used for retrieval and candidate construction.",
            "results_summary": "Framework components together outperform baselines across summarization and DI-prediction tasks (see summarization and DI metrics). Ablation shows each module (summarization fine-tuning, relevance/extraction, secondary learning, deviation-aware alignment) contributes materially to final performance.",
            "limitations_or_challenges": "Requires a structured literature database with problem-method indexing; multi-step design increases runtime; performance tied to quality of extraction and retrieval.",
            "comparison_to_human_or_traditional": "Framework emphasizes traceability to source literature and quantitative DI-based assessment rather than pure free-form LLM ideation and human-preference-only evaluation.",
            "recommendations_or_best_practices": "Index literature by problem and method to enable rapid retrieval; fine-tune summarization models for concise, logic-preserving summaries; perform relevance assessment and information extraction before DI prediction to reduce noise.",
            "uuid": "e9708.1",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DI Prediction Techniques",
            "name_full": "Entropy-weighted Loss, Secondary Learning, Deviation-aware Alignment",
            "brief_description": "A set of training and stabilization techniques applied to the DI prediction model: entropy-weighted loss upweights rare high-DI samples, secondary learning re-trains on top-error samples, and deviation-aware iterative feedback mitigates bias and stabilizes predictions with a KL-balancing term.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Qwen-7B (fine-tuned with LoRA adapters) as prediction backbone in experiments",
            "llm_description": "Predictor is a frozen-pretrained LLM with LoRA fine-tuning; training uses Adam optimizer and PEFT-inserted adapters described in experiments.",
            "scientific_domain": "Multi-domain (same experimental domains).",
            "evaluation_method": "Supervised regression to ground-truth DI with specialized losses and training curricula; ablation tests evaluate the contribution of each technique via changes in MSE/MAE/WMSE/WMAE.",
            "evaluation_criteria": "MSE, MAE, WMSE, WMAE; secondary learning selects top ~20% highest-error examples; KL-divergence used to balance primary vs secondary distributions.",
            "benchmark_or_dataset": "Training/test splits on DBLP, PubMed, PatSnap with DI labels computed from citation graphs.",
            "results_summary": "Using entropy-weighted loss, secondary learning, and deviation-aware alignment substantially reduced prediction errors; e.g., on DBLP full model MSE=0.0093 vs w/o secondary learning MSE=0.0187 and w/o deviation-aware alignment MSE=0.0216 (Table 4), demonstrating sizable ablation impacts.",
            "limitations_or_challenges": "Secondary learning risks prediction drift without KL balancing; entropy weighting depends on accurate class/rarity estimates; requires careful hyperparameter tuning (weights, top-percent selection).",
            "comparison_to_human_or_traditional": "These techniques are automated strategies to emphasize rare disruptive examples — an alternative to manual curating of high-impact cases or human reweighting.",
            "recommendations_or_best_practices": "Apply entropy-weighted loss to emphasize rare disruptive samples, use a secondary learning pass on the hardest examples with KL regularization to prevent drift, and feed iterative deviation feedback into model updates to improve calibration.",
            "uuid": "e9708.2",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Summarization Evaluation (cosine / ROUGE)",
            "name_full": "Problem-Method Summarization Evaluation via Cosine Similarity and ROUGE",
            "brief_description": "Quantitative evaluation of generated concise problem-method summaries by measuring semantic similarity (cosine between embeddings) and n-gram overlap metrics (ROUGE) against ground-truth summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Qwen-32B (fine-tuned) used for summarization evaluation in reported experiments; compared to GPT-4o, GPT-4 Turbo, Claude variants.",
            "llm_description": "Qwen-32B is a larger pre-trained variant of Qwen used both pre-trained and fine-tuned (LoRA) for summarization; fine-tuning improved alignment to ground-truth summaries.",
            "scientific_domain": "Applied to AI (DBLP), biomedical (PubMed), and patent (PatSnap) corpora.",
            "evaluation_method": "Generate problem-method summary S = G_theta(P, M, Prompt) and compute cosine similarity against ground-truth summary vector and ROUGE scores for n-gram overlap.",
            "evaluation_criteria": "Cosine similarity of embedding vectors and ROUGE (reported values), used to evaluate logical consistency and completeness of summaries.",
            "benchmark_or_dataset": "DBLP, PubMed, PatSnap subsets with ground-truth problem-method summaries created from literature.",
            "results_summary": "Fine-tuned Qwen-32B improved summarization over baselines: DBLP similarity 0.558 / ROUGE 0.325 (fine-tuned) vs Qwen-32B pre-trained similarity 0.552 / ROUGE 0.315; PubMed 0.500/0.324; Patents 0.612/0.404 (Table 1).",
            "limitations_or_challenges": "ROUGE and cosine similarity capture surface and embedding-level alignment but do not fully measure scientific validity, plausibility, or traceable support to references.",
            "comparison_to_human_or_traditional": "Metrics provide reproducible quantitative signal but must be paired with downstream DI prediction and reference-based validation to assess scientific usefulness beyond lexical similarity.",
            "recommendations_or_best_practices": "Fine-tune summarization models on task-specific ground truth; use both embedding-based and lexical metrics; follow summarization with relevance-assessment to ensure traceable literature grounding.",
            "uuid": "e9708.3",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPP (Greedy with Probabilistic Perturbation)",
            "name_full": "Greedy with Probabilistic Perturbation search algorithm",
            "brief_description": "A search strategy that primarily takes greedy steps to increase the objective (predicted DI) but with small probability ε accepts a random non-optimal neighbor to escape local optima, combined with adaptive regularization and temporal smoothing of DI feedback.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "DI predictor + search controller implemented around fine-tuned models (experiments centered on Qwen-7B-based predictors)",
            "llm_description": "Search uses the DI prediction model as a scoring oracle; method neighborhoods generated by replacing/augmenting/modifying components of candidate methods.",
            "scientific_domain": "Applied to method combination search in AI, biomedical, and patent domains.",
            "evaluation_method": "Iterative optimization: at each iteration evaluate candidate neighborhood N(M_t) with DI predictor and update method M_t+1 by greedy argmax with probability 1-ε or random neighbor with probability ε. Regularization C(M) prevents overfitting.",
            "evaluation_criteria": "Hit rate of identified method combinations with true DI&gt;0.5 (measured against ground-truth DI), plus convergence behavior and ability to escape local optima.",
            "benchmark_or_dataset": "DBLP, PubMed, PatSnap candidate search spaces constructed from indexed problem-method pairs.",
            "results_summary": "GPP outperformed standard greedy: hit rates improved from Standard Greedy 19.3%/21.5%/18.2% to Greedy+GPP 26.3%/28.1%/24.6% on DBLP/PubMed/PatSnap respectively (Table 5), an improvement of ≈+9.1 percentage points over best LLM baseline performance.",
            "limitations_or_challenges": "Requires careful tuning of perturbation probability ε and penalty/regularization terms; random perturbations may increase wall-clock evaluations; success depends on DI predictor fidelity.",
            "comparison_to_human_or_traditional": "GPP automates exploration/exploitation trade-offs that human search heuristics perform manually, showing measurable improvements in locating historically disruptive combinations.",
            "recommendations_or_best_practices": "Use small ε to balance exploration; incorporate regularization C(M) and temporal smoothing of DI feedback; couple with high-fidelity DI predictor to guide global search.",
            "uuid": "e9708.4",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prior LLM/Ideation Benchmarks",
            "name_full": "IdeaBench, Creativity Index, MOOSE-Chem, Chain-of-Ideas, Idea Arena, ResearchAgent (mentioned benchmarks/frameworks)",
            "brief_description": "Existing benchmarks and frameworks for evaluating LLM-generated research ideas and ideation quality, typically measuring originality, feasibility, impact, reliability, or using multi-agent and human-alignment protocols.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_name": "Various (benchmarks/frameworks evaluate different LLMs; examples referenced include GPT family, multi-agent LLM systems and specialized fine-tuned models)",
            "llm_description": "Not a single model — these works propose evaluation protocols (IdeaBench), indices (Creativity Index with DPO), chemistry-focused multi-agent frameworks (MOOSE-Chem), chain-structuring systems (Chain-of-Ideas), and iterative reviewer/agent systems (ResearchAgent).",
            "scientific_domain": "Primarily cross-domain ideation benchmarks; MOOSE-Chem is chemistry-specific; ResearchAgent and Chain-of-Ideas apply across scientific domains.",
            "evaluation_method": "Human preference alignment, supervised fine-tuning with DPO, multi-agent decomposition, chain-structuring of literature, and structured reviewer agents; mostly text/idea-quality-centric evaluations rather than citation-based disruptiveness.",
            "evaluation_criteria": "Originality, feasibility, impact, reliability, human preference alignment, and idea coherence; these differ from DI and do not directly measure long-term citation-driven disruptiveness.",
            "benchmark_or_dataset": "IdeaBench (idea evaluation benchmark), Creativity Index datasets/methods (supervised labels), MOOSE-Chem chemistry hypotheses datasets, Chain-of-Ideas literature chains, ResearchAgent retrieval+review corpora.",
            "results_summary": "These prior benchmarks provide complementary evaluation axes (creativity, feasibility, human preference) but the paper highlights they lack an objective disruptiveness metric like DI and often emphasize textual quality over traceable, citation-grounded impact.",
            "limitations_or_challenges": "Predominantly rely on human judgments or semantic similarity measures, risk of hallucinated unsupported ideas, and limited objective quantification of long-term scientific impact.",
            "comparison_to_human_or_traditional": "Prior benchmarks generally align with human evaluators; paper positions DI-based evaluation as a quantitative complement to these human-centered metrics.",
            "recommendations_or_best_practices": "Combine idea-quality benchmarks with citation-grounded metrics (like DI) and enforce traceability to literature to reduce hallucination and increase verifiability.",
            "uuid": "e9708.5",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A dynamic network measure of technological change",
            "rating": 2,
            "sanitized_title": "a_dynamic_network_measure_of_technological_change"
        },
        {
            "paper_title": "Idea-bench: How far are generative models from professional designing?",
            "rating": 2,
            "sanitized_title": "ideabench_how_far_are_generative_models_from_professional_designing"
        },
        {
            "paper_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "rating": 2,
            "sanitized_title": "moosechem_large_language_models_for_rediscovering_unseen_chemistry_scientific_hypotheses"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models.",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Chain of ideas: Revolutionizing research via novel idea development with llm agents.",
            "rating": 2,
            "sanitized_title": "chain_of_ideas_revolutionizing_research_via_novel_idea_development_with_llm_agents"
        },
        {
            "paper_title": "Empowering AI as autonomous researchers: Evaluating LLMs in generating novel research ideas through automated metrics.",
            "rating": 2,
            "sanitized_title": "empowering_ai_as_autonomous_researchers_evaluating_llms_in_generating_novel_research_ideas_through_automated_metrics"
        },
        {
            "paper_title": "Combination of research questions and methods: A new measurement of scientific novelty.",
            "rating": 1,
            "sanitized_title": "combination_of_research_questions_and_methods_a_new_measurement_of_scientific_novelty"
        },
        {
            "paper_title": "Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references.",
            "rating": 1,
            "sanitized_title": "creativity_in_science_and_the_link_to_cited_references_is_the_creative_potential_of_papers_reflected_in_their_cited_references"
        }
    ],
    "cost": 0.01740775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations
14 Apr 2025</p>
<p>Junlan Chen 
Sun Yat-sen University
GuangzhouChina</p>
<p>Kexin Zhang 
Sun Yat-sen University
GuangzhouChina</p>
<p>Daifeng Li lidaifeng@mail.sysu.edu.cn 
Sun Yat-sen University
GuangzhouChina</p>
<p>Yangyang Feng 
Sun Yat-sen University
GuangzhouChina</p>
<p>Yuxuan Zhang 
Sun Yat-sen University
GuangzhouChina</p>
<p>Bowen Deng 
Sun Yat-sen University
GuangzhouChina</p>
<p>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations
14 Apr 2025567B202A2AAC0C2C15CCA7E343CA733EarXiv:2503.18865v3[cs.AI]Scientific InnovationKnowledge RecombinationLLM ReasoningProblem-Method Structure
The emergence of large language models (LLMs) offers new possibilities for structured exploration of scientific knowledge.Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights.Specifically, we investigate how knowledge units-especially those tied to methodological design-can be modeled and recombined to yield research breakthroughs.Our proposed framework addresses two key challenges.First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problemdriven contexts.Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential.This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</p>
<p>Introduction</p>
<p>Recent advances in Large Language Models (LLMs) have significantly enhanced text understanding and generation capabilities [27,33,2], demonstrating expertlevel performance across various domains [3].In scientific discovery, LLMs have been applied to generate research ideas and synthesize existing knowledge [3,39,8,17], confirming their potential in assisting scientific exploration.However, despite these advancements, existing approaches still exhibit several limitations: (1) the inability to systematically identify and integrate fine-grained knowledge components, resulting in scientific discovery that remains at a macro-level of idea generation rather than precise matching of research problems and methods; (2) the hallucination phenomenon in LLMs, where models generate problem-solving approaches that lack actual literature support, potentially leading research efforts astray; and (3) the absence of objective metrics to assess the transformative impact of newly proposed discoveries, as current methods predominantly rely on subjective expert alignment rather than quantitative evaluations of scientific breakthroughs.</p>
<p>Research has shown that the most influential scientific discoveries primarily stem from the combination-particularly atypical combinations-of traditional ideas from prior work [32,34,36].Innovation emerges when prior innovations or their components are assembled into an original design [16,24,35].Schumpeter [31], a pioneer in innovation theory, posited that innovation is fundamentally "the recombination of elements of production", meaning a novel combination of production elements or conditions.Similarly, Nelson and Winter [25] argued that "the creation of novelty in art, science, and technology largely depends on the recombination of pre-existing conceptual and physical materials."In scientific research, research questions and methods serve as fundamental building blocks, and their combination determines the scientific novelty of a publication [22].Despite this, existing studies largely focus on LLM-driven idea generation rather than systematically identifying, filtering, and combining problem-method pairs to enhance the effectiveness of scientific discovery.</p>
<p>To address this gap, we introduce the Disruptive Index (DI) to quantify whether a scientific discovery drives a paradigm shift.Disruptive innovation represents fundamental transformations in scientific and technological progress, distinct from incremental improvements that merely refine existing paradigms.Traditional impact metrics, such as citation counts, primarily measure the extent of a technology's adoption rather than its transformative potential.The Disruptive Index (DI), proposed by Funk and Owen-Smith [11], captures whether a scientific discovery supersedes previous approaches rather than merely reinforcing the status quo.A prominent example is Watson and Crick's (1953) discovery of the DNA double-helix structure, which superseded previous approaches, such as Pauling's triple-helix model, and fundamentally altered the field of molecular biology.Their study, with a DI score of 0.62 [28], exemplifies a highly disruptive scientific breakthrough, validated extensively through expert assessments [11,37].Therefore, beyond relying on LLM-generated research ideas, it is essential to construct a framework that integrates DI-based evaluations to systematically assess the transformative impact of problem-method combinations.</p>
<p>Building upon these insights, we propose a problem-method combination framework for scientific discovery, inspired by how scientific breakthroughs emerge from the recombination of existing knowledge.Given a research question, our framework first retrieves and synthesizes relevant papers, then employs an LLM assistant to determine whether specific papers can serve as sources for new scientific discoveries related to the question and extracts a candidate set of methods.Subsequently, we introduce an innovative disruptive index evaluation framework to quantify the disruptiveness of problem-method combinations.Specifically, through model fine-tuning, our assistant generates combination strategies based on research questions and candidate methods.To evaluate the disruptiveness of these strategies, we identify potential source literature in our database, analyze differences between source strategies and current strategies, and propose an adaptive bias-aware alignment model to predict disruptive indices based on these differences.Finally, we iteratively explore candidate method sets to identify the most disruptive problem-method combinations.</p>
<p>We conduct extensive experiments on publication databases across three scientific domains.Our results demonstrate that the proposed framework outperforms state-of-the-art methods in predicting the disruptiveness of problemmethod combinations.Furthermore, validation on real-world high-disruptiveness publications confirms the framework's ability to identify highly disruptive scientific discoveries.</p>
<p>The primary contributions of this research are as follows:</p>
<p>1.A novel framework for scientific discovery that systematically identifies and integrates problem-method combinations rather than relying solely on LLM-generated research ideas.2. A disruptive index evaluation framework that quantitatively assesses the potential disruptiveness of new scientific discoveries, improving upon traditional impact metrics.3. Extensive experimental validation demonstrating the effectiveness of our approach in identifying high-disruptiveness discoveries across multiple scientific domains.</p>
<p>2 Related Work</p>
<p>LLMs in Scientific Discovery and Research Ideation</p>
<p>Large Language Models (LLMs) have demonstrated significant potential in scientific discovery, particularly in generating novel research ideas.Various benchmarks and frameworks have been developed to evaluate the quality of LLMgenerated research hypotheses.One approach involves the establishment of Ide-aBench, a benchmark designed to standardize the assessment of research ideas produced by LLMs [18].Another method introduces the Creativity Index, which incorporates supervised fine-tuning and Direct Preference Optimization (DPO) to evaluate originality, feasibility, impact, and reliability in idea generation [9].Additionally, the MOOSE-Chem multi-agent framework has been implemented, utilizing problem decomposition strategies to improve the quality of LLM-generated research hypotheses in chemistry [39].These efforts reflect a broader trend toward leveraging LLMs for structured scientific ideation.However, despite advancements in benchmark development and evaluation methodologies, existing approaches remain largely reliant on textual synthesis rather than structured reasoning or methodological integration.</p>
<p>The need for systematic frameworks that enhance the logical progression of research ideation and ensure scientific rigor continues to be a critical challenge.</p>
<p>Beyond direct idea generation, LLMs have been incorporated into structured frameworks to enhance the logical progression of research ideation.One such approach is the Chain-of-Ideas (CoI) Agent, an LLM-based system that organizes relevant literature into a structured chain, simulating the progressive development of a research domain and strengthening ideation capabilities [17].To systematically evaluate generated ideas, the Idea Arena protocol has been designed, ensuring that evaluation criteria align with human research preferences.Experimental results indicate that the CoI Agent outperforms conventional methods and produces research ideas of comparable quality to those generated by human researchers.</p>
<p>Additionally, ResearchAgent has been introduced as an iterative framework that refines research ideas through the integration of an academic graph and knowledge retrieval mechanisms.Multiple LLM-powered reviewing agents are employed to provide structured feedback, aligning evaluations with human-defined criteria.This structured review process enhances the clarity, novelty, and validity of generated ideas, demonstrating effectiveness across multiple disciplines [3].</p>
<p>Despite the substantial advancements of Large Language Models (LLMs) in scientific discovery, existing methods still exhibit several critical limitations.First, current research primarily focuses on generating research hypotheses or ideas but fails to systematically explore the finer-grained composition of methodological elements tailored to specific research questions.This limitation results in a lack of effective knowledge recombination mechanisms, rendering LLMs incapable of constructing genuinely innovative research pathways that align with established scientific methodologies.</p>
<p>Second, current LLM-driven research ideation methods heavily rely on providing extensive background information [17], where LLMs are exposed to a large volume of related literature to enhance their inferential capabilities.While this strategy enriches the contextual breadth of generated content, it compromises the traceability of research ideas, making it difficult to directly associate LLM-generated hypotheses with specific prior studies.Consequently, researchers often face challenges in verifying the theoretical foundations and scientific validity of these generated ideas.</p>
<p>Finally, when evaluating LLM-generated research hypotheses, existing approaches predominantly rely on human preference alignment, where assessments are based on subjective ratings or semantic similarity measures.However, there is a notable lack of objective metrics to rigorously quantify the scientific impact of these ideas.In particular, current research lacks systematic methods to evaluate the transformative potential or disruptive impact of newly generated methodologies, thereby making it challenging to distinguish LLM-generated research ideas from truly groundbreaking scientific discoveries.</p>
<p>These limitations underscore the necessity of developing a more systematic and intelligent scientific discovery framework based on problem-method matching, ensuring traceability, scientific rigor, and quantitative evaluation of research ideas.Such a framework would enable a more structured integration of methodological elements while incorporating quantitative analysis to assess the potential impact of newly generated methodologies.</p>
<p>Knowledge Combination and Recombinant Innovation in Scientific Discovery</p>
<p>Innovation fundamentally arises from the combination and recombination of knowledge [38,30].The concept of knowledge recombination has gained increasing attention in the literature, with over 1,000 articles in top management journals leveraging this framework to analyze scientific innovation [38].Recombinant innovation is regarded as a major driver of new idea generation, and its frequent occurrence in scientific research underscores the necessity of understanding how scientific knowledge is integrated and combined in academic publications [7].</p>
<p>Within the domain of scientific discovery, research questions and research methods serve as the fundamental building blocks that determine the novelty and impact of scientific contributions [22].Existing studies have examined scientific novelty through various combination-based perspectives, but have yet to fully address the temporal evolution and semantic complexity of research questions and methods.To bridge this gap, recent work has proposed a life-index novelty measurement, incorporating the frequency and age of research questions and methods, alongside semantic novelty assessment using deep learning and representation learning techniques [22].These advancements highlight the importance of systematically integrating research questions and methods to characterize scientific novelty.Despite these insights, current methodologies predominantly focus on evaluating novelty at the level of individual concepts, rather than systematically modeling how methodological elements are combined to address specific research questions.While research questions and methods both constitute integral knowledge elements in scientific articles, existing studies rarely explore how their structured integration contributes to groundbreaking discoveries.This limitation suggests the need for a systematic framework that formalizes the combination of research questions and methods to assess their transformative potential.</p>
<p>Moreover, the lack of structured mechanisms for problem-method matching has hindered the ability to predict which methodological innovations lead to significant scientific breakthroughs.Although LLM-based approaches have been employed to generate research ideas, they primarily rely on retrieving or synthesizing prior knowledge, rather than systematically aligning methods with research questions to facilitate novel knowledge recombination.This gap underscores the necessity of developing an intelligent framework that systematically integrates research methods as core knowledge elements and models their structured composition for scientific discovery.</p>
<p>The Necessity of the Disruption Index (DI) in Evaluating Scientific Breakthroughs</p>
<p>Traditional metrics for assessing the impact of scientific research, such as citation counts, h-index, and journal impact factor, have long been used as standard measures of scientific influence.However, these indicators primarily capture the magnitude of a study's dissemination rather than its ability to challenge existing paradigms [13? , 40].While citation counts are intuitive and widely adopted, they suffer from inherent limitations, including bias toward incremental research, ignoring negative citations, and reinforcing conservative citation behavior [6,26,10].Consequently, traditional bibliometric indicators often fail to distinguish between studies that reinforce the status quo and those that disrupt established knowledge structures.</p>
<p>To address these limitations, Funk and Owen-Smith (2017)introduced the Disruption Index (DI) as a metric to quantify the extent to which new technological advancements displace or reinforce existing knowledge [11].Inspired by prior literature on technological shifts, they argued that the dichotomy between competence-enhancing and competence-destroying innovations was insufficient for characterizing real-world technological evolution.Instead, they proposed that disruptiveness exists on a continuum, where some innovations incrementally improve existing knowledge, while others render previous technologies obsolete [1].</p>
<p>Originally developed to measure technological innovation using vast patent databases such as the U.S. Patent Citations Data File, the Disruption Index was later extended to scientific research by Wu et al. (2019), who applied the metric to bibliometrics [37].They demonstrated that DI could effectively differentiate groundbreaking discoveries from incremental advancements by analyzing its values in Nobel Prize-winning papers and comparing disruption levels between review papers and their original research articles.</p>
<p>The DI quantifies disruptiveness using the following formulation:
D = n i − n j n i + n j + n k ,(1)
where n i is the number of papers that cite the focal paper exclusively, n j represents papers citing both the focal paper and its references, and n k denotes papers that cite only the references of the focal paper [37].This formulation allows DI to capture the extent to which a research contribution redefines its field, rather than merely accumulating citations.</p>
<p>The necessity of DI stems from its ability to quantitatively evaluate scientific breakthroughs, offering a more precise alternative to traditional citationbased measures.As disruptive innovation is characterized by a paradigm shift that redirects collective attentio*, DI provides a robust framework for distinguishing transformative research from incremental progress [20].Given that existing LLM-based research ideation models primarily focus on generating novel ideas without evaluating their potential to challenge existing scientific conventions, integrating DI into scientific discovery frameworks can significantly enhance the assessment of research novelty and impact.These considerations highlight the limitations of existing citation-based indicators in capturing scientific breakthroughs and underscore the importance of incorporating DI into intelligent scientific discovery frameworks.A disruptionaware approach could enable more systematic evaluations of research impact, ensuring that novel problem-method combinations are assessed not just for their feasibility but also for their potential to drive substantial scientific advancements.</p>
<p>Our research proposes a novel scientific discovery paradigm that not only provides methodological support for evaluating scientific innovation but also establishes a new technological paradigm for intelligent research tools.Through question-methodology combinatorial logic, we have constructed a framework that achieves objective quantitative assessment of the disruptive potential of research ideas and generates corresponding reasoning chains.This study introduces an improved framework with three core modules to enhance disruptive knowledge prediction and method combination exploration.</p>
<p>Methodology</p>
<p>The Problem-Driven Method Exploration Module identifies potential method combinations based on specific research questions, providing innovative and targeted strategies.</p>
<p>The Disruptive Knowledge Prediction Module predicts the disruptive potential of given problems and methods using a deviation-awareness mechanism and a secondary learning approach to ensure accuracy and reliability.</p>
<p>The Dynamic Method Optimization Module iteratively refines method combinations based on disruptive index feedback, enhancing their disruptive potential.</p>
<p>This framework offers a systematic approach for researchers to uncover disruptive knowledge and drive scientific innovation.</p>
<p>Problem-Driven Method Exploration Module</p>
<p>To enhance the efficiency of method exploration and reduce resource consumption, this study designs a Problem-Driven Method Exploration Module that constructs a paper database indexed by problems and methods.Traditional approaches often require repeated evaluation of the relationship between new problems and paper abstracts, which not only increases computational costs but also reduces retrieval efficiency.To address this, we propose an efficient retrieval mechanism that rapidly identifies potential method candidates relevant to specific research problems.</p>
<p>The paper database is built upon a large-scale collection of academic literature.First, we preprocess textual information such as paper titles, abstracts, and keywords to remove redundant and noisy data, ensuring data accuracy and consistency.Subsequently, natural language processing (NLP) techniques are employed to extract the research problem and research method from each paper.These two elements are then indexed as key entries to facilitate efficient retrieval through a problem-driven approach.</p>
<p>During the method exploration process, when a new research problem P new is proposed, the system embeds it into a semantic vector space:
v Pnew = Embed(P new )
Then, a similarity function is applied to retrieve the top-k similar problems:
P sim = {P i | sim(v Pnew , v Pi ) ≥ δ}
The associated methods M sim = {M i | P i ∈ P sim } are collected, and a heuristic filtering mechanism H is applied:
M final = H(M sim , sim(•), rule(•))
Finally, the system constructs candidate problem-method pairs:
C = {(P new , M ) | M ∈ M final }
These selected candidates support downstream disruptive knowledge prediction and method optimization.</p>
<p>Disruptive Index Prediction Model Methodology</p>
<p>This study introduces a disruptive index prediction model consisting of interconnected sub-modules designed to precisely evaluate the innovative potential of specific problem-method combinations.Specifically, the overall module comprises three sub-modules: problem-method summary generation, identification, extraction, and refinement of key reference information, and final prediction of the disruptive index.</p>
<p>The first sub-module aims to automatically generate highly concise summaries for given problem-method pairs.To enhance the accuracy and logical coherence of generated summaries, we employ a summary generation model fine-tuned using Low-Rank Adaptation (LoRA) [15].This model effectively learns from existing real-world literature summaries within a targeted downstream task context.Additionally, a set of meticulously crafted prompts guide the model stepby-step in describing the logical relationships and details involved in combining problems and methods (detailed prompt design is provided in the Appendix).</p>
<p>Formally, given a problem-method pair (P, M ) and a task-specific prompt Prompt, the summary generation model G θ produces a concise summary S:
S = G θ (P, M, Prompt)
Through this approach, our trained model significantly outperforms current state-of-the-art methods, demonstrating superior logical consistency and completeness of information.</p>
<p>The second sub-module integrates both the identification of key reference documents and the extraction of essential information.Given the computationally intensive nature of precise matching across large literature databases, we initially filter candidate references based on the semantic similarity between the given problem-method combination and the problem-method indices from a structured literature database.</p>
<p>Let D be the structured database and (P, M ) the input pair.We first compute the similarity and select the top-k references:
R top = {R i ∈ D | sim((P, M ), (P i , M i )) ≥ δ}
This approach ensures that only references with highly relevant problemmethod associations are considered, constraining the selection to 100 references.Subsequently, we employ a frozen pre-trained model to perform a fine-grained semantic comparison between the generated summaries and candidate reference summaries, thereby accurately identifying the key source references relevant to the problem-method combination.</p>
<p>Let F denote the frozen model and S the generated summary.We obtain the extracted information I:
I = F (S, R top )
Once the key references are identified, we further refine the extracted information to reduce computational burden and minimize noise introduced by large-scale raw textual inputs.Inspired by the RAHA approach [19], we leverage a frozen pre-trained model to compare the generated summary with the identified reference summaries and extract only the most critical, relevant information required for the model input.This integrated method effectively captures complex citation relationships while ensuring the quality of the extracted input information, ultimately improving the efficiency and accuracy of subsequent predictions.</p>
<p>The third sub-module utilizes the generated problem-method summaries and extracted critical information to predict the disruptive index for problem-method combinations.This sub-module employs a prediction model fine-tuned via LoRA.During fine-tuning, supervision is provided using real summaries and extracted reference information.Formally, the prediction model D ϕ outputs a disruption score y based on S and I: y = D ϕ (S, I)</p>
<p>Due to the scarcity of highly disruptive papers in real-world datasets, potentially biasing predictions towards low-disruptive outcomes, we introduce an entropy-based weighted evaluation metric [4].</p>
<p>The weighted loss for entropy-aware learning is given by:
L entropy = N i=1 w i • ℓ(y i , ŷi ), w i = − log(p(ŷ i ))
This metric assigns higher weights to rare but highly disruptive samples, enhancing the model's capability to identify high-disruptiveness cases.</p>
<p>Furthermore, we design a secondary learning mechanism for challenging-toclassify samples, selecting the top 20% of instances with the highest prediction errors for reinforcement training.To prevent prediction drift caused by secondary training, we introduce a KL-divergence-based balancing mechanism to stabilize training outcomes.The KL divergence loss between primary and secondary distributions is defined as:
L KL = D KL (D primary ϕ ∥ D secondary ϕ )
To further stimulate the cognitive and evaluative capabilities of the model, we propose an iterative deviation-awareness mechanism.Specifically, after each prediction iteration, the results are fed back to the model, prompting self-awareness and evaluation of prediction deviations.Let θ t be the model parameters at iteration t, then parameter update is guided by the deviation gradient:
θ t+1 = θ t − η • ∇Deviation(ŷ t , y t )
Experimental results demonstrate that, compared to existing state-of-the-art methods, our proposed framework significantly and comprehensively improves performance in predicting the disruptive index of problem-method combinations.</p>
<p>Dynamic Method Optimization Module</p>
<p>The dynamic method optimization module is designed to iteratively refine method combinations based on feedback from the disruptive index, enhancing their disruptive potential over multiple optimization cycles.This process ensures that method selection and adjustments remain continuously optimized within the problem-method space to maximize impact.</p>
<p>At the core of this module lies a feedback-driven iterative optimization mechanism.Specifically, the system evaluates the disruptive index of a given problemmethod combination at each iteration and utilizes this information to guide subsequent modifications.</p>
<p>Formally, let (P, M t ) represent the problem and current method configuration at iteration t, and let y t be the corresponding disruptive index:
y t = D ϕ (S t , I t )
The optimization process follows a greedy algorithm, selecting adjustments in each iteration that locally maximize the disruptive index:
M t+1 = arg max M ′ ∈N (Mt) D ϕ (S ′ , I ′ )
where N (M t ) denotes the neighborhood of candidate method variants generated by replacing, augmenting, or modifying components of M t .By progressively favoring configurations that yield higher indices, the framework systematically converges towards method combinations with enhanced disruptive impact.</p>
<p>However, traditional greedy algorithms are prone to getting trapped in local optima, leading to stagnation in suboptimal solutions.To address this limitation, we incorporate a Greedy with Probabilistic Perturbation (GPP) approach [14], enhancing the global search capability.</p>
<p>In GPP, with a small probability ϵ, a non-optimal method M rand ∈ N (M t ) is accepted:
M t+1 = arg max M ′ D ϕ (S ′ , I ′ ), with probability 1 − ϵ M rand , with probability ϵ
This probabilistic mechanism allows the optimization process to escape local optima and explore method configurations with long-term disruptive potential.By leveraging this stochastic perturbation strategy, the optimization process effectively balances local search with global exploration.</p>
<p>Furthermore, to prevent stagnation in local optima and ensure comprehensive exploration of the method space, the optimization process integrates adaptive constraints.</p>
<p>Let C(M t ) denote constraint-based penalty terms for overfitting, we define the total objective with regularization as:
L opt = −D ϕ (S t , I t ) + λ • C(M t )
This ensures that method updates remain meaningful and generalizable across problem contexts.</p>
<p>The optimization module also includes an adaptive learning component that dynamically adjusts the weight of disruptive index feedback based on observed trends over multiple iterations.Let w t denote the weight at time t, updated based on temporal smoothing:
w t+1 = α • w t + (1 − α) • y t
This mechanism enables the system to prioritize consistently improving adjustments while attenuating noise introduced by short-term evaluation anomalies.</p>
<p>Empirical evaluations indicate that the incorporation of the GPP mechanism effectively enhances the global search capability, enabling the algorithm to escape local optima while maintaining efficient optimization performance.Overall, this dynamic optimization strategy significantly improves the identification and refinement of disruptive method combinations.By integrating iterative feedback, local optimization, adaptive constraints, and stochastic perturbation, this module provides a more efficient and systematic solution for method selection, ultimately fostering the discovery of high-impact scientific innovations.</p>
<p>Experiments</p>
<p>Data Sources</p>
<p>To evaluate the effectiveness of our proposed framework, we conduct experiments on three citation-based datasets: DBLP, PubMed, and PatSnap.Given the broad scope of these datasets, we focus on specific domains to ensure targeted analysis.</p>
<p>For DBLP, we extract records from 2011 to 2021 covering 14,533 publications from CCF-A conferences in the field of artificial intelligence.From PubMed, we select 96,612 research articles related to depression, published between 2015 and 2025.Lastly, for PatSnap, we use 6,677 patent records on medical robotics, with legal status marked as active, covering the period from 2020 to 2025.Further details on dataset characteristics and preprocessing are provided in Appendix A.</p>
<p>Baselines</p>
<p>To comprehensively assess the performance of our framework, we compare it against a set of established baselines, including both general-purpose large language models and specialized pre-trained models for scientific and technical domains.</p>
<p>We consider the following baselines:</p>
<p>(1) General-purpose LLMs: GPT and Claude, widely used for natural language understanding and text generation tasks.</p>
<p>(2) SciBERT [29], a pre-trained language model designed specifically for scientific text processing, which has demonstrated strong performance in scientific literature comprehension and reasoning tasks.</p>
<p>(3) RoBERTa [21], an optimized variant of BERT that enhances training robustness and performance across multiple NLP tasks.</p>
<p>(4) LLaMA 3 [12], the latest iteration in the LLaMA series of large-scale language models, which offers improved efficiency and reasoning capabilities.</p>
<p>(5) Qwen-7B [5], an autoregressive generative language model based on masked language modeling, optimized for diverse text generation and completion tasks.</p>
<p>All of these models are publicly accessible, allowing for reproducible benchmarking and comparative evaluation of our proposed framework.</p>
<p>Experimental Setup</p>
<p>Our experiments are conducted using PyTorch on four NVIDIA A800 GPUs.The ablation study is performed based on Qwen-7B.The model optimization is implemented using the Adam optimizer, with a learning rate set to 1e-5 and a gradient clipping threshold fixed at 0.2.</p>
<p>For model configurations, the problem-method summary generation model is set to handle a maximum input length of 1000 tokens, while the disruptive index prediction model is configured with a maximum input length of 7000 tokens.The batch size is consistently set to 4 across all experiments.The adapter used in the second LLM is configured with a low-rank dimension of 64.</p>
<p>We employ the PEFT (Parameter-Efficient Fine-Tuning) library to insert adapters into the last attention or feedforward layers of the LLM [23].This analysis is performed based on a principled examination of the forward components.</p>
<p>Both training and testing iterations are set to K = 5.For other baseline models, the number of training epochs is fixed at 5, and the optimal model checkpoint is selected based on validation set performance metrics.</p>
<p>Main Results</p>
<p>We present the main results on the DBLP, PubMed, and PatSnap datasets in Table 1.Our framework, which integrates two models along with the overall system, consistently outperforms existing state-of-the-art LLMs and PLMs across multiple evaluation metrics.Table 1 reports the cosine similarity and ROUGE scores between problem-method summaries generated by our framework and their corresponding ground-truth summaries.We use Qwen-32B as an example for evaluating the summarization performance.The fine-tuned model not only surpasses existing general-purpose LLMs such as GPT and Claude but also demonstrates superior performance over non-fine-tuned pre-trained models.</p>
<p>This confirms the effectiveness of our approach in refining the alignment between problem-method pairs and their corresponding textual representations.As shown in Table 2, we evaluate the effectiveness of our disruptive index prediction model based on four key metrics: MSE, MAE, weighted MSE (WMSE), and weighted MAE (WMAE).Our model, incorporating adaptive bias awareness and secondary sample learning, achieves lower error rates across all metrics, demonstrating its superiority over general-purpose LLMs, pre-trained language models (PLMs), and large language models (LLMs) fine-tuned on scientific tasks.The improvements in these evaluation metrics indicate that our framework effectively captures the disruptive potential of problem-method combinations with higher accuracy and robustness.As shown in Table 3, our full framework, designed for disruptive index prediction, outperforms general LLMs across all evaluation metrics.The consistent improvements in MSE, MAE, WMSE, and WMAE confirm the effectiveness of integrating problem-method pairs and disruptive index prediction to enhance scientific discovery.</p>
<p>Ablation Study</p>
<p>To analyze the contributions of individual components within our framework, we conduct an ablation study, as shown in Table 4. (1) Framework w/o fine-tuning the problem-method summarization model: Removing fine-tuning from the problem-method summarization model results in a noticeable performance degradation across all datasets.This demonstrates the importance of task-specific adaptation in improving the alignment between problem-method pairs and their textual representations.Without fine-tuning, the generated summaries exhibit lower quality, impacting the overall framework's ability to capture meaningful research insights.</p>
<p>(2) Framework w/o relevance assessment and information extraction: Excluding the step of relevance judgment and structured information extraction significantly reduces the effectiveness of disruptive index prediction.The removal of this module leads to an increase in MSE and MAE, as the model is unable to accurately capture contextual knowledge essential for evaluating problem-method disruptiveness.This highlights the necessity of refining input information before disruptive potential estimation.</p>
<p>(3) Framework w/o secondary learning: The absence of secondary learning-where high-error samples undergo further training-results in higher prediction errors across all four key evaluation metrics: MSE, MAE, WMSE, and WMAE.This confirms that the secondary learning mechanism enhances model robustness by mitigating the impact of hard-to-classify instances, ensuring better generalization.</p>
<p>(4) Framework w/o deviation-aware alignment: Removing deviationaware alignment leads to a decline in performance by increasing error rates and reducing model consistency.Without this module, the framework struggles to adjust predictions based on previously observed discrepancies, limiting its ability to refine predictions dynamically.</p>
<p>Overall, the ablation study confirms that each component plays a crucial role in improving the framework's predictive performance.The degradation in results when removing any of these modules underscores their importance in systematically enhancing problem-method integration and disruptive potential assessment.</p>
<p>Greedy Algorithm Optimization Results</p>
<p>To evaluate the effectiveness of our dynamic method optimization module, we conduct experiments measuring its ability to identify high-disruptiveness method combinations.Table 5 reports the hit rate (Disruptive Index &gt; 0.5) on three datasets.</p>
<p>Our framework leverages a Greedy with Probabilistic Perturbation (GPP) approach to iteratively optimize problem-method combinations based on the disruptive index.Compared to a standard greedy algorithm, GPP significantly improves the ability to escape local optima, resulting in superior method selection.Specifically, the model incorporating GPP achieves a higher disruptive index score across all datasets, demonstrating improved long-term optimization capabilities.The observed performance gains are attributed to two key advantages of GPP: (1) Balancing exploration and exploitation, where the probabilistic acceptance of non-optimal choices prevents premature convergence, and (2) Adaptive weighting of disruptive index feedback, which enables a more refined and responsive optimization trajectory.These mechanisms collectively enhance the ability of our framework to discover novel, high-impact problem-method combinations.</p>
<p>Conclusion</p>
<p>In this study, we propose a novel framework for scientific discovery that systematically integrates problem-method combinations with disruptive index prediction.Our approach leverages fine-tuned LLMs for problem-method summarization, an adaptive bias-aware alignment model for disruptive index estimation, and a dynamic optimization strategy incorporating Greedy with Probabilistic Perturbation (GPP) to iteratively refine method selection.</p>
<p>Empirical results on DBLP, PubMed, and PatSnap confirm the effectiveness of our framework.Compared to existing general-purpose LLMs, pre-trained language models, and baseline methods, our approach consistently achieves higher accuracy in problem-method summarization, disruptive index prediction, and high-impact method identification.The introduction of GPP significantly enhances search efficiency by balancing exploitation and exploration, ensuring the discovery of truly novel and disruptive scientific insights.</p>
<p>Our findings contribute to the advancement of AI-driven scientific discovery by demonstrating the value of structured problem-method integration and adaptive learning strategies.Future research may explore expanding the framework to broader domains and improving interpretability to further assist researchers in generating groundbreaking discoveries.</p>
<p>Limitations</p>
<p>While our proposed framework demonstrates strong performance in integrating problem-method combinations with disruptive index prediction, it has two primary limitations.</p>
<p>First, for entirely emerging scientific fields with minimal prior work, our framework may encounter challenges due to a lack of sufficient historical data.The effectiveness of the problem-method integration and disruptive index prediction relies on existing structured research literature.In domains with scarce prior knowledge, the search space for potential method combinations becomes significantly larger, reducing search efficiency and increasing the likelihood of suboptimal results.</p>
<p>Second, our framework involves a multi-step process that includes problemmethod summarization, source validation, information extraction, secondary learning, and deviation-aware alignment.While each step enhances accuracy, it also increases computational complexity and execution time.The sequential nature of these processes results in higher processing overhead, which may limit the scalability of our approach when applied to large-scale real-time applications.</p>
<p>Future research should explore ways to mitigate these limitations, including optimizing search strategies for data-scarce fields and improving computational efficiency through parallelization and adaptive learning techniques.</p>
<p>Fig. 1 .
1
Fig. 1.Enter Caption</p>
<p>Table 1 .
1
Comparison of Question-Method Pair Summarization Performance Across Different Datasets Using Cosine Similarity and ROUGE
ModelDBLPPubMedPatentSimilarity ROUGE Similarity ROUGE Similarity ROUGEGPT-4o0.5030.2060.4320.2250.4470.133GPT-4 Turbo0.5200.2050.3450.1870.4360.133Claude 3.50.4690.2040.3810.1540.3430.095Claude 3.70.4600.2140.4910.1390.2990.085Qwen-32B (Pre-trained) 0.5520.3150.4230.2420.4560.158Qwen-32B (Fine-tuned)0.5580.3250.5000.3240.6120.404</p>
<p>Table 2 .
2
Disruptive Index Prediction Using Summarization and Information Across Different Datasets Using MSE, MAE, WMSE, and WMAE
ModelDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEGPT-4o0.3607 0.5728 0.5821 0.7175 0.3255 0.5369 0.2321 0.4026 0.3018 0.5162 0.0884 0.1770GPT-4 Turbo0.3607 0.5728 0.5821 0.7175 0.3549 0.5621 0.1742 0.2919 0.3455 0.5594 0.6520 0.7707Claude 3.50.4346 0.6115 0.3162 0.4318 0.4269 0.6077 0.3029 0.4219 0.4242 0.5887 0.9875 0.9295Claude 3.70.4699 0.6580 0.2653 0.4131 0.4751 0.6593 0.2728 0.3773 0.4242 0.5887 0.9875 0.9295SciBERT0.3125 0.4153 0.3641 0.3817 0.4218 0.1642 0.3486 0.4357 0.4871 0.5092 0.4217 0.4561RoBERTa0.4351 0.5715 0.3105 0.3751 0.4017 0.4521 0.3465 0.4213 0.4154 0.5184 0.4156 0.4364LLaMA 30.5612 0.6143 0.3155 0.4182 0.4832 0.5961 0.5942 0.4118 0.5624 0.7334 0.3284 0.3912Qwen-7B0.6845 0.8223 0.4558 0.4526 0.4839 0.6587 0.1587 0.2908 0.6843 0.8151 0.5172 0.5241SciBERT(Fine-tuned) 0.0142 0.0247 0.5472 0.6781 0.0093 0.0145 0.3148 0.4207 0.0135 0.0241 0.4151 0.5124RoBERTa(Fine-tuned) 0.0091 0.0154 0.6245 0.6578 0.0075 0.0921 0.2947 0.4814 0.0183 0.0214 0.4521 0.4873LLaMA 3(Fine-tuned) 0.0124 0.0325 0.5175 0.5412 0.0091 0.0124 0.3541 0.4168 0.0265 0.3457 0.5142 0.5321Qwen-7B (Fine-tuned) 0.0052 0.0121 0.6172 0.6739 0.0020 0.0072 0.2533 0.4604 0.0144 0.0181 0.4325 0.4512</p>
<p>Table 3 .
3
Utilizing Question-Method Pairs to Predict Disruptive Index Results with MSE, MAE, WMSE, and WMAE Metrics
ModelDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEGPT-4o0.1191 0.3062 0.5887 0.7399 0.2053 0.4182 0.6545 0.7036 0.1216 0.3044 1.0043 0.9041GPT-4 Turbo 0.1597 0.3628 0.4771 0.6453 0.3289 0.5196 0.3690 0.5558 0.1523 0.3556 1.0803 0.9312Claude 3.50.2291 0.4268 0.9919 0.8767 0.1731 0.3742 1.0523 0.9006 0.2176 0.4201 1.2142 0.9428Claude 3.70.4319 0.6143 0.1355 0.3382 0.1146 0.3021 0.6827 0.7224 0.4594 0.6559 0.1437 0.3431Our Framework 0.0093 0.0111 0.0941 0.1364 0.0154 0.0342 0.1147 0.2142 0.0218 0.0412 0.1241 0.2962</p>
<p>Table 4 .
4
Ablation study of our framework using Qwen-7B across DBLP, PubMed, and Patent datasets with MSE, MAE, WMSE, and WMAE metrics.
Model VariantDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEFull Framework (Ours)0.0093 0.0111 0.0941 0.1364 0.0154 0.0342 0.1147 0.2142 0.0218 0.0412 0.1241 0.2962w/o Summarization Fine-tuning 0.0274 0.0461 0.1403 0.2093 0.0412 0.0623 0.1889 0.2563 0.0586 0.0721 0.2034 0.3121w/o Relevance + Extraction0.0351 0.0592 0.1881 0.2672 0.0526 0.0783 0.2102 0.2907 0.0735 0.0897 0.2345 0.3374w/o Secondary Learning0.0187 0.0329 0.1187 0.1766 0.0291 0.0503 0.1506 0.2384 0.0375 0.0594 0.1741 0.3022w/o Deviation-Aware Alignment 0.0216 0.0382 0.1279 0.1917 0.0318 0.0562 0.1663 0.2496 0.0414 0.0638 0.1862 0.3195</p>
<p>Table 5 .
5
Hit Rate (%) of High-Disruptiveness Method Combinations (Disruptive Index &gt; 0.5) Identified by Various Methods
MethodDBLP PubMed PatSnapGPT-4o (ChatGPT)16.4% 18.2% 14.9%Claude 3.515.7% 17.1% 14.3%Claude 3.717.2% 19.0% 15.6%Standard Greedy19.3% 21.5% 18.2%Greedy + GPP (Ours)26.3% 28.1% 24.6%Improvement over best LLM +9.1% +9.1% +9.0%
J. Chen et al.</p>
<p>Extra credit for disruption: Trend of disruption in radiology academic journals. A Abu-Omar, P Kennedy, M Yakub, J Robbins, A Yassin, N Verma, M Scaglione, F Khosa, Clinical Radiology. 77122022</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>A characterization of entropy in terms of information loss. J C Baez, T Fritz, T Leinster, Entropy. 13112011</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, B Hui, L Ji, M Li, J Lin, R Lin, D Liu, G Liu, C Lu, K Lu, J Ma, R Men, X Ren, X Ren, C Tan, S Tan, J Tu, P Wang, S Wang, W Wang, S Wu, B Xu, J Xu, A Yang, H Yang, J Yang, S Yang, Y Yao, B Yu, H Yuan, Z Yuan, J Zhang, X Zhang, Y Zhang, Z Zhang, C Zhou, J Zhou, X Zhou, T Zhu, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>The incidence and role of negative citations in science. C Catalini, N Lacetera, A Oettl, Proceedings of the National Academy of Sciences. 112452015</p>
<p>Scientific knowledge combination in networks: New perspectives on analyzing knowledge absorption and integration. H Chen, J Liu, Z Liu, EEKE/AII@ JCDL. 2023</p>
<p>Empowering ai as autonomous researchers: Evaluating llms in generating novel research ideas through automated metrics. D Dasgupta, A Mondal, P P Chakrabarti, 2nd AI4Research Workshop: Towards a Knowledgegrounded Scientific Research Lifecycle. </p>
<p>Empowering AI as autonomous researchers: Evaluating LLMs in generating novel research ideas through automated metrics. D Dasgupta, A Mondal, P P Chakrabarti, 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle. 2024</p>
<p>Tradition and innovation in scientists' research strategies. J G Foster, A Rzhetsky, J A Evans, American sociological review. 8052015</p>
<p>A dynamic network measure of technological change. R J Funk, J Owen-Smith, Management science. 6332017</p>
<p>A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>The impact of collaboration and knowledge networks on citations. J Guan, Y Yan, J J Zhang, Journal of Informetrics. 1122017</p>
<p>A probabilistic greedy search algorithm for combinatorial optimisation with application to the set covering problem. M Haouari, J Chaouachi, Journal of the Operational Research Society. 5372002</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, ICLR. 1232022</p>
<p>The burden of knowledge and the "death of the renaissance man": Is innovation getting harder?. B F Jones, The Review of Economic Studies. 7612009</p>
<p>L Li, W Xu, J Guo, R Zhao, X Li, Y Yuan, B Zhang, Y Jiang, Y Xin, R Dang, arXiv:2410.13185Chain of ideas: Revolutionizing research via novel idea development with llm agents. 2024arXiv preprint</p>
<p>C Liang, L Huang, J Fang, H Dou, W Wang, Z F Wu, Y Shi, J Zhang, X Zhao, Y Liu, arXiv:2412.11767Idea-bench: How far are generative models from professional designing?. 2024arXiv preprint</p>
<p>C Lin, J Ren, G He, Z Jiang, H Yu, X Zhu, arXiv:2402.08874Recurrent alignment with hard attention for hierarchical text rating. 2024arXiv preprint</p>
<p>New directions in science emerge from disconnection and discord. Y Lin, J A Evans, L Wu, Journal of Informetrics. 1611012342022</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Combination of research questions and methods: A new measurement of scientific novelty. Z Luo, W Lu, J He, Y Wang, Journal of Informetrics. 1621012822022</p>
<p>Peft: State-of-the-art parameter-efficient fine-tuning methods. S Mangrulkar, S Gugger, L Debut, Y Belkada, S Paul, B Bossan, Peft: State-of-the-art parameter-efficient fine-tuning methods. 2022</p>
<p>A new method for identifying recombinations of existing knowledge associated with high-impact innovation. S Mukherjee, B Uzzi, B Jones, M Stringer, Journal of Product Innovation Management. 3322016</p>
<p>An evolutionary theory of economic change. R R Nelson, S G Winter, 1985harvard university press</p>
<p>Global citation inequality is on the rise. M W Nielsen, J P Andersen, Proceedings of the National Academy of Sciences. 1187e20122081182021</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 352022</p>
<p>Papers and patents are becoming less disruptive over time. M Park, E Leahey, R J Funk, Nature. 61379422023</p>
<p>N Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Search and recombination process to innovate: a review of the empirical evidence and a research agenda. T Savino, A Messeni Petruzzelli, V Albino, International Journal of Management Reviews. 1912017</p>
<p>Business cycles: A theoretical, historical and statistical analysis of the capitalist process. J A Schumpeter, Acessado em. 41939. 1964</p>
<p>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references. I Tahamtan, L Bornmann, Journal of informetrics. 1232018</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Atypical combinations and scientific impact. B Uzzi, S Mukherjee, M Stringer, B Jones, Science. 34261572013</p>
<p>Collaboration and creativity: The small world problem. B Uzzi, J Spiro, American journal of sociology. 11122005</p>
<p>Bias against novelty in science: A cautionary tale for users of bibliometric indicators. J Wang, R Veugelers, P Stephan, Research Policy. 4682017</p>
<p>Large teams develop and small teams disrupt science and technology. L Wu, D Wang, J A Evans, Nature. 56677442019</p>
<p>A knowledge recombination perspective of innovation: review and new research directions. T Xiao, M Makhija, S Karim, Journal of Management. 4862022</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Z Yang, W Liu, B Gao, T Xie, Y Li, W Ouyang, S Poria, E Cambria, D Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>Team size, research variety, and research performance: do coauthors' coauthors matter. N Zhu, C Liu, Z Yang, Journal of Informetrics. 1541012052021</p>            </div>
        </div>

    </div>
</body>
</html>
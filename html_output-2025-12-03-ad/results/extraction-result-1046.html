<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1046 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1046</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1046</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-247748869</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.13733v2.pdf" target="_blank">Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Assembly of multi-part physical structures is both a valuable end product for autonomous robotics, as well as a valuable diagnostic task for open-ended training of embodied intelligent agents. We introduce a naturalistic physics-based environment with a set of connectable magnet blocks inspired by children's toy kits. The objective is to assemble blocks into a succession of target blueprints. Despite the simplicity of this objective, the compositional nature of building diverse blueprints from a set of blocks leads to an explosion of complexity in structures that agents encounter. Furthermore, assembly stresses agents' multi-step planning, physical reasoning, and bimanual coordination. We find that the combination of large-scale reinforcement learning and graph-based policies -- surprisingly without any additional complexity -- is an effective recipe for training agents that not only generalize to complex unseen blueprints in a zero-shot manner, but even operate in a reset-free setting without being trained to do so. Through extensive experiments, we highlight the importance of large-scale training, structured representations, contributions of multi-task vs. single-task learning, as well as the effects of curriculums, and discuss qualitative behaviors of trained agents.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1046.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1046.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN-Structured Multi-Blueprint Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Attention Network Structured Agent trained with Large-Scale PPO on Multiple Blueprints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated bimanual assembly agent using a graph-attention encoder and dot-product attention decoding, trained with Proximal Policy Optimization (PPO) on many blueprint assembly tasks to assemble magnet blocks into target blueprints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GNN-Structured Multi-Blueprint Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy learned with on-policy RL (PPO + GAE). Observations encoded as a directed graph over blocks + global gripper node; encoded by 3 GAT (graph attention) layers. Actions use per-block movement proposals and dot-product attention between gripper queries and block keys to choose which block each gripper moves.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Magnetic Block Assembly Environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3D physics environment (MuJoCo) with up to 16 cuboid blocks having positive/negative magnetic attachment points. The agent controls virtual grippers that directly set positional/rotational velocities of chosen blocks. Tasks are to assemble one of many target blueprints (165 total blueprints; 141 train / 24 test). Episodes last 100 environment steps. Complexity arises from combinatorial composition of parts (blueprints of 2–16 blocks), multi-step planning, physical contact/collision reasoning, and bimanual coordination; variation arises from many blueprint instances and randomized initial states (random scatter or pre-constructed blueprint resets).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of blocks in blueprint (2–16), number of required magnetic connections, blueprint combinatorial complexity (165 total; 141 train / 24 test), episode length (100 steps). Training difficulty measured by 'steps until success' (empirical ranges: ~100M–500M env steps for first reliable solutions depending on blueprint size).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Range low→high: low for 2-block blueprints, high for 16-block blueprints; overall environment contains high complexity instances (up to 16 blocks, combinatorial assembly).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of blueprint instances (165 total; 141 training instances / 24 held-out tests), initial-state variation (random scatter or preconstructed blueprint with probability 0.2), curriculum sampling over blueprint difficulty, and randomized block placements and orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (141 distinct training blueprints spanning many structural patterns plus randomized initial states and blueprint-reset mode to increase state diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (per-blueprint and aggregated), 'steps until success' (first timestep when a blueprint is reliably solved), and episodic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>After large-scale training (1B env steps default; extended runs to 2.5B): examples from Table 1 — 6-block blueprint: 100% success (multi-task) by ~180M steps; 12-block: 98.8% success (multi-task) by ~220M steps; 16-block: 90.9% success (multi-task) by ~240M steps. Simpler 2-block tasks can require up to ~100M steps to be reliably solved; some very complex tasks first solved by ~500M steps in extended runs.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: multi-task training across a wide variation of blueprints (complexities) scaffolds learning — exposure to many tasks of varying complexity speeds learning and substantially improves generalization to unseen complex blueprints. High environment complexity (large blueprints) is hard or impossible to learn in low-variation (single-task) regimes; large-scale training + relational inductive bias (GAT) + multi-task exposure trades computation for ability to generalize across high complexity and high variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>6-block multi-task: 100% success (after ~180M steps) when trained across the diverse training set.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>16-block multi-task: 90.9% success after 1B steps (trained on full set of training blueprints).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>6-block single-task baseline: 99.6% success by ~100M steps (single-blueprint training).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Large-scale on-policy RL (PPO + GAE) using a graph-attention network encoder; multi-task training across many blueprints; curriculum that increases sampling probability of harder blueprints; episodic initial states that with prob 0.2 reset from preconstructed blueprints; dot-product attention decoding for gripper→block selection.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Multi-task GNN agent generalized zero-shot to held-out complex blueprints (held-out success rates reported as high; multi-task agents could transfer to complex unseen blueprints). Specific empirical evidence: multi-task agent achieves high success on held-out structures and after extended training solved complex unseen blueprints.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low sample efficiency: training required large scale compute — default runs used 1 billion environment steps (1 B steps ≈ 48 hours in their distributed setup), extended experiments up to 2.5 billion steps; simpler 2-block tasks can require ~100M steps; complex tasks often require hundreds of millions of steps (100–500M) to first be solved and ~200–500M to reach high success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Multi-task training across diverse blueprints is crucial: it speeds learning and enables generalization to unseen complex blueprints. 2) Relational inductive bias implemented via graph-attention networks (and attention decoding) is necessary for success and generalization. 3) Massive scale of training (hundreds of millions to billions of env steps) is a key enabling factor. 4) Resetting from preconstructed blueprints increases state variation and enables strong reset-free operation. 5) Curriculum effects are ambiguous but may improve held-out generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1046.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1046.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Task Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Blueprint Trained Structured Agent (single-task)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent of similar architecture trained only on a single blueprint (single-task) to evaluate the effect of low task-variation training on learning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Single-Task Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same structured policy architecture (GNN + attention decoding) but trained only on a single target blueprint (single-task training) using PPO + GAE.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Magnetic Block Assembly Environment (single-blueprint training)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same assembly environment; training episodes sampled only from one blueprint (low task variation), episodes still have randomized initial states unless constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Blueprint size (6, 12, 16 blocks used in experiments) and number of required magnetic connections; measured by steps-until-success and final success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies by blueprint: experiments included moderate (6-block), medium (12-block), and high (16-block) complexity single-blueprint instances.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Low task variation (single blueprint); standard environment stochasticity (initial randomization) but no multi-blueprint sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Low (single target blueprint)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate and 'steps until success'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 results after 1B steps: 6-block single-task: 99.6% success (steps until success ~100M); 12-block single-task: 99.9% success (steps until success ~480M); 16-block single-task: 0% success (did not learn after 1B steps).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Demonstrates that low variation (single-task) training can suffice for low-to-moderate complexity tasks but fails for high complexity: single-task agents could not learn the 16-block blueprint even after 1B steps, highlighting a trade-off where high task complexity demands exposure to variation (multi-task) to scaffold learning.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>16-block single-task: 0% success after 1B environment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>6-block single-task: 99.6% success by ~100M steps.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-task PPO training on a single blueprint (same architecture otherwise).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Limited: single-task agents transferred to some blueprints of equal or lower complexity than trained-on, but mostly failed to transfer to any blueprints they were not trained to solve (i.e., poor generalization to held-out/novel structures).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Sample-inefficient for higher complexity: while 6-block succeeded by ~100M steps, 12-block required ~480M steps to learn, and 16-block did not learn within 1B steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Single-task training is sufficient for simple/moderate blueprints but fails for high-complexity blueprints; multi-task exposure across variations is necessary to learn and generalize to complex assembly tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1046.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1046.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResNet Baseline (No-Relational)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flattened Observations + ResNet Encoder Baseline (No Relational Inductive Bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline agent that flattens the environment observations and uses a residual network encoder instead of a graph neural network, to test the role of relational inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ResNet Baseline (No-Relational)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent uses flattened observations (no explicit graph structure) and a residual network encoder; action and value decoders are similar to the structured agent. Trained with PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Magnetic Block Assembly Environment (flattened observation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same assembly environment but with the agent deprived of relational (object-centric) input inductive bias by flattening the observations.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same (blueprint size 2–16 blocks), but absence of relational encoder tests sensitivity to relational complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>High complexity instances expose the weakness of no-relational encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Trained in multiple variants: (1) full training set, (2) subset ≤6 blocks, (3) single blueprint of 6 blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Varied across the three ablated experiments; still includes some environment randomness but lacks explicit relational modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (per-blueprint) after training (1B steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Catastrophic failure: after 1B env steps all three ResNet variants had 0% success rate on all blueprints, and never exceeded 2.5% success on any train or held-out blueprint.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Shows that removing relational inductive biases is catastrophic across variation and complexity: the model cannot exploit the compositional structure of tasks, failing even when variation is limited. This implies relational structure is critical when environment complexity and variation are high.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>0% success (catastrophic) after 1B steps when trained on full training set and also when trained on limited subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>0%–≤2.5% success (did not meaningfully learn) in evaluated setups.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>0% success in all tested settings after 1B steps.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Also failed in most low-variation settings tested (0% after 1B in some cases; never exceeded 2.5% on any blueprint).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>PPO training with flattened inputs and a residual network encoder (various training subsets tested).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Poor: ResNet variants failed to learn and thus could not generalize; relational inductive bias (graph structure + attention) is necessary for any meaningful generalization or performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Extremely poor — after 1B environment steps there was effectively no learning (0% success across blueprints).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Relational/object-centric inductive biases (graph neural networks with attention) are essential for learning and generalization in this compositional assembly domain; removing them leads to catastrophic failure regardless of training variation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1046.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1046.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Gripper Agent (Ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Gripper Structured Agent (one virtual gripper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of the structured agent restricted to a single virtual gripper to test the necessity of bimanual coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Single-Gripper Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same GNN-based architecture but with only one gripper (action space restricted), trained with PPO on the assembly tasks to evaluate effect of reduced manipulation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Magnetic Block Assembly Environment (single-gripper ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same environment but agent has only one gripper (instead of two), increasing difficulty for tasks requiring coordinated two-handed maneuvers.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Blueprint size (2–16 blocks) and inherent requirement for bimanual maneuvers (some blueprints require simultaneous positioning or holding of substructures).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Medium-to-high complexity particularly for blueprints requiring bimanual coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Same blueprint set as main experiments (multi-task vs single-task variants tested).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (when evaluated across multi-blueprint set) but single-gripper capacity reduces effective ability to handle variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (per-blueprint, aggregated).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Quantitative numbers not provided in main text; reported qualitatively as 'overall success rate is lower than that of a dual-gripper agent, particularly on the more complex blueprints.' Exact per-blueprint rates are not stated in the paper text provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Ablation shows that reducing the agent's embodiment (fewer effectors) reduces capability on high-complexity blueprints; certain strategies exist for single-gripper to complete some structures but overall lower performance indicates a trade-off between agent embodiment (gripper count) and ability to solve complex/varied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Same large-scale PPO training as main agent but with single-gripper action space.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Single-gripper agent finds unique strategies to complete some structures, but generally exhibits lower success and weaker performance on complex held-out blueprints compared to the bimanual agent.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantitatively reported; qualitative comparison indicates less efficient/less successful learning on complex tasks when using a single gripper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bimanual capability (two grippers) significantly improves success on complex tasks; agent embodiment (number of effectors) is an important factor for solving high-complexity assembly problems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1046.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1046.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Blueprint-Reset Augmented Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Trained with Preconstructed-Blueprint Resets (Reset-Augmented Training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of the structured agent trained with episodic initial states that include preconstructed blueprints (with probability 0.2), which increases initial-state diversity and enables stronger reset-free operation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Blueprint-Reset Augmented Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same GNN-based PPO agent but with environment initialization modified: with probability 0.2 episodes start from a randomly sampled pre-constructed blueprint (unused blocks scattered), enabling learning of disassembly and improved continuous operation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Magnetic Block Assembly Environment with blueprint resets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environment identical to main but initial states sometimes start from pre-built blueprints to increase state diversity and to train agents on disassembly and reassembly behaviors; used to evaluate reset-free continuous operation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>As main environment: blueprint sizes (≥12 used for reset-free evaluation), number of sequential tasks in reset-free run (10 consecutive blueprints per reset-free episode), episode length (100 steps per blueprint).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>High for reset-free evaluation (sequences of high-complexity blueprints, minimum 12 blocks per blueprint in evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Initial-state variation increased by sampling preconstructed blueprints (probability 0.2 during training); reset-free evaluation sequences sample from training set blueprints (min 12 blocks).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (training includes intentionally varied initial states and disassembly scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Reset-free success rate aggregated over sequences of consecutive blueprints (build 10 consecutive blueprints per reset-free episode).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reset-free evaluation (50 reset-free episodes, building 10 blueprints each, sampled from training set structures requiring ≥12 blocks): Without blueprint resets during training: 69.4% ± 17.0% aggregated success. With blueprint resets during training: 93.1% ± 7.5% aggregated success.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Training with blueprint resets increases initial-state variation and forces agents to learn disassembly and reassembly; this improves resilience and performance in high-complexity, continuous (reset-free) scenarios — demonstrating that increasing variation in initial states can substantially improve performance on long-horizon/reset-free operation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Reset-free sequences of high-complexity blueprints (≥12 blocks): 93.1% ± 7.5% success when trained with blueprint resets vs 69.4% ± 17.0% without.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-task PPO with blueprint-reset augmentation (probability 0.2 of starting from preconstructed blueprints), curriculum sampling, and standard GNN architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agents trained with blueprint resets generalize to reset-free continuous operation much better than agents trained without such initial-state variation; training with resets allows agents to disassemble incorrect constructions and continue operating across multiple sequential goals.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not separately quantified beyond overall training scale; improvement in reset-free performance comes from the same large-scale training but with initial-state reset augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Including pre-constructed blueprint resets during episodic training (increasing initial-state variation) substantially improves performance in practical reset-free continuous operation, indicating that exposing agents to diverse initial conditions (including partially-built structures) is beneficial for robustness and long-horizon performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1046.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1046.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gripper-Transition-Delay Finetuned Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Fine-tuned with Gripper-Change Transition Delays (to reduce unrealistic fast re-grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent that was first trained in the default direct-manipulation environment and then fine-tuned in a modified environment where a gripper is disabled for 2 steps upon changing which object it holds, to mitigate unrealistic behaviors induced by direct manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Gripper-Transition-Delay Finetuned Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pretrained GNN-PPO agent is further trained (fine-tuned) in an environment variant that enforces a 2-step disablement of a gripper when it switches the object it's holding, to emulate a more realistic gripper transition constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Magnetic Block Assembly Environment with gripper-transition delay</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same assembly environment but when a gripper changes the object it holds, that gripper is disabled for 2 environment steps. This penalizes unrealistic rapid object switching and simulates a more realistic actuator constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same blueprint complexity measures; added constraint increases temporal coordination requirement and effectively increases problem difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Increased difficulty relative to default because of enforced gripper handover/transition delay constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Single modification to action-effect mapping (gripper-change delay) applied during fine-tuning; other environment variation unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Low-to-moderate (single type of environment constraint variation applied during fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate during fine-tuning and recovery time measured in env steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When the delay was applied after default training and the agent continued training (fine-tuning), the agent's success rate initially dropped significantly but recovered to strong performance within less than 100M environment steps. Training from scratch with the gripper-transition delay is far more challenging: after 1.5B steps the agent trained from scratch with delays still failed to make significant progress on many blueprints.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Demonstrates that imposing a more realistic constraint (reducing an unrealistic capability) increases effective task difficulty, but that sequential training (train in simpler setting then finetune on constrained setting) is an efficient trade-off: initial training in a higher-variation/easier setting followed by finetuning on constrained variants recovers performance with modest additional sample cost.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Pretrain in default easier direct-manipulation environment (large-scale PPO), then fine-tune in environment with 2-step gripper-change disablement; comparison to training-from-scratch with delay.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Fine-tuning from a pretrained agent recovers strong performance within <100M additional steps; training from scratch under the gripper-change delay is much harder and unsuccessful even after 1.5B steps for many blueprints. This supports a strategy of curriculum/ecosystem where agents are first trained in simpler settings then adapted to more realistic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Finetuning efficient (recovery in <100M env steps); training from scratch under the constraint is inefficient (1.5B steps still insufficient for substantial progress).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Realistic actuator constraints (gripper-change delay) increase task difficulty, but finetuning from agents trained in simpler settings is an efficient approach to adapt to such constraints. 2) Training from scratch with realistic constraints can be prohibitively expensive compared to two-stage train-then-finetune workflows.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured agents for physical construction. <em>(Rating: 2)</em></li>
                <li>Learn2assemble with structured representations and search for robotic architectural construction. <em>(Rating: 2)</em></li>
                <li>Ikea furniture assembly environment for long-horizon complex manipulation tasks. <em>(Rating: 2)</em></li>
                <li>Building lego using deep generative models of graphs. <em>(Rating: 1)</em></li>
                <li>Towards practical multi-object manipulation using relational reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1046",
    "paper_id": "paper-247748869",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "GNN-Structured Multi-Blueprint Agent",
            "name_full": "Graph-Attention Network Structured Agent trained with Large-Scale PPO on Multiple Blueprints",
            "brief_description": "A simulated bimanual assembly agent using a graph-attention encoder and dot-product attention decoding, trained with Proximal Policy Optimization (PPO) on many blueprint assembly tasks to assemble magnet blocks into target blueprints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GNN-Structured Multi-Blueprint Agent",
            "agent_description": "Policy learned with on-policy RL (PPO + GAE). Observations encoded as a directed graph over blocks + global gripper node; encoded by 3 GAT (graph attention) layers. Actions use per-block movement proposals and dot-product attention between gripper queries and block keys to choose which block each gripper moves.",
            "agent_type": "simulated agent",
            "environment_name": "Magnetic Block Assembly Environment",
            "environment_description": "3D physics environment (MuJoCo) with up to 16 cuboid blocks having positive/negative magnetic attachment points. The agent controls virtual grippers that directly set positional/rotational velocities of chosen blocks. Tasks are to assemble one of many target blueprints (165 total blueprints; 141 train / 24 test). Episodes last 100 environment steps. Complexity arises from combinatorial composition of parts (blueprints of 2–16 blocks), multi-step planning, physical contact/collision reasoning, and bimanual coordination; variation arises from many blueprint instances and randomized initial states (random scatter or pre-constructed blueprint resets).",
            "complexity_measure": "Number of blocks in blueprint (2–16), number of required magnetic connections, blueprint combinatorial complexity (165 total; 141 train / 24 test), episode length (100 steps). Training difficulty measured by 'steps until success' (empirical ranges: ~100M–500M env steps for first reliable solutions depending on blueprint size).",
            "complexity_level": "Range low→high: low for 2-block blueprints, high for 16-block blueprints; overall environment contains high complexity instances (up to 16 blocks, combinatorial assembly).",
            "variation_measure": "Number of blueprint instances (165 total; 141 training instances / 24 held-out tests), initial-state variation (random scatter or preconstructed blueprint with probability 0.2), curriculum sampling over blueprint difficulty, and randomized block placements and orientations.",
            "variation_level": "High (141 distinct training blueprints spanning many structural patterns plus randomized initial states and blueprint-reset mode to increase state diversity).",
            "performance_metric": "Success rate (per-blueprint and aggregated), 'steps until success' (first timestep when a blueprint is reliably solved), and episodic reward.",
            "performance_value": "After large-scale training (1B env steps default; extended runs to 2.5B): examples from Table 1 — 6-block blueprint: 100% success (multi-task) by ~180M steps; 12-block: 98.8% success (multi-task) by ~220M steps; 16-block: 90.9% success (multi-task) by ~240M steps. Simpler 2-block tasks can require up to ~100M steps to be reliably solved; some very complex tasks first solved by ~500M steps in extended runs.",
            "complexity_variation_relationship": "Explicitly discussed: multi-task training across a wide variation of blueprints (complexities) scaffolds learning — exposure to many tasks of varying complexity speeds learning and substantially improves generalization to unseen complex blueprints. High environment complexity (large blueprints) is hard or impossible to learn in low-variation (single-task) regimes; large-scale training + relational inductive bias (GAT) + multi-task exposure trades computation for ability to generalize across high complexity and high variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": "6-block multi-task: 100% success (after ~180M steps) when trained across the diverse training set.",
            "high_complexity_high_variation_performance": "16-block multi-task: 90.9% success after 1B steps (trained on full set of training blueprints).",
            "low_complexity_low_variation_performance": "6-block single-task baseline: 99.6% success by ~100M steps (single-blueprint training).",
            "training_strategy": "Large-scale on-policy RL (PPO + GAE) using a graph-attention network encoder; multi-task training across many blueprints; curriculum that increases sampling probability of harder blueprints; episodic initial states that with prob 0.2 reset from preconstructed blueprints; dot-product attention decoding for gripper→block selection.",
            "generalization_tested": true,
            "generalization_results": "Multi-task GNN agent generalized zero-shot to held-out complex blueprints (held-out success rates reported as high; multi-task agents could transfer to complex unseen blueprints). Specific empirical evidence: multi-task agent achieves high success on held-out structures and after extended training solved complex unseen blueprints.",
            "sample_efficiency": "Low sample efficiency: training required large scale compute — default runs used 1 billion environment steps (1 B steps ≈ 48 hours in their distributed setup), extended experiments up to 2.5 billion steps; simpler 2-block tasks can require ~100M steps; complex tasks often require hundreds of millions of steps (100–500M) to first be solved and ~200–500M to reach high success.",
            "key_findings": "1) Multi-task training across diverse blueprints is crucial: it speeds learning and enables generalization to unseen complex blueprints. 2) Relational inductive bias implemented via graph-attention networks (and attention decoding) is necessary for success and generalization. 3) Massive scale of training (hundreds of millions to billions of env steps) is a key enabling factor. 4) Resetting from preconstructed blueprints increases state variation and enables strong reset-free operation. 5) Curriculum effects are ambiguous but may improve held-out generalization.",
            "uuid": "e1046.0"
        },
        {
            "name_short": "Single-Task Agent",
            "name_full": "Single-Blueprint Trained Structured Agent (single-task)",
            "brief_description": "Agent of similar architecture trained only on a single blueprint (single-task) to evaluate the effect of low task-variation training on learning and generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Single-Task Agent",
            "agent_description": "Same structured policy architecture (GNN + attention decoding) but trained only on a single target blueprint (single-task training) using PPO + GAE.",
            "agent_type": "simulated agent",
            "environment_name": "Magnetic Block Assembly Environment (single-blueprint training)",
            "environment_description": "Same assembly environment; training episodes sampled only from one blueprint (low task variation), episodes still have randomized initial states unless constrained.",
            "complexity_measure": "Blueprint size (6, 12, 16 blocks used in experiments) and number of required magnetic connections; measured by steps-until-success and final success rate.",
            "complexity_level": "Varies by blueprint: experiments included moderate (6-block), medium (12-block), and high (16-block) complexity single-blueprint instances.",
            "variation_measure": "Low task variation (single blueprint); standard environment stochasticity (initial randomization) but no multi-blueprint sampling.",
            "variation_level": "Low (single target blueprint)",
            "performance_metric": "Success rate and 'steps until success'.",
            "performance_value": "Table 1 results after 1B steps: 6-block single-task: 99.6% success (steps until success ~100M); 12-block single-task: 99.9% success (steps until success ~480M); 16-block single-task: 0% success (did not learn after 1B steps).",
            "complexity_variation_relationship": "Demonstrates that low variation (single-task) training can suffice for low-to-moderate complexity tasks but fails for high complexity: single-task agents could not learn the 16-block blueprint even after 1B steps, highlighting a trade-off where high task complexity demands exposure to variation (multi-task) to scaffold learning.",
            "high_complexity_low_variation_performance": "16-block single-task: 0% success after 1B environment steps.",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "6-block single-task: 99.6% success by ~100M steps.",
            "training_strategy": "Single-task PPO training on a single blueprint (same architecture otherwise).",
            "generalization_tested": true,
            "generalization_results": "Limited: single-task agents transferred to some blueprints of equal or lower complexity than trained-on, but mostly failed to transfer to any blueprints they were not trained to solve (i.e., poor generalization to held-out/novel structures).",
            "sample_efficiency": "Sample-inefficient for higher complexity: while 6-block succeeded by ~100M steps, 12-block required ~480M steps to learn, and 16-block did not learn within 1B steps.",
            "key_findings": "Single-task training is sufficient for simple/moderate blueprints but fails for high-complexity blueprints; multi-task exposure across variations is necessary to learn and generalize to complex assembly tasks.",
            "uuid": "e1046.1"
        },
        {
            "name_short": "ResNet Baseline (No-Relational)",
            "name_full": "Flattened Observations + ResNet Encoder Baseline (No Relational Inductive Bias)",
            "brief_description": "A baseline agent that flattens the environment observations and uses a residual network encoder instead of a graph neural network, to test the role of relational inductive biases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ResNet Baseline (No-Relational)",
            "agent_description": "Agent uses flattened observations (no explicit graph structure) and a residual network encoder; action and value decoders are similar to the structured agent. Trained with PPO.",
            "agent_type": "simulated agent",
            "environment_name": "Magnetic Block Assembly Environment (flattened observation baseline)",
            "environment_description": "Same assembly environment but with the agent deprived of relational (object-centric) input inductive bias by flattening the observations.",
            "complexity_measure": "Same (blueprint size 2–16 blocks), but absence of relational encoder tests sensitivity to relational complexity.",
            "complexity_level": "High complexity instances expose the weakness of no-relational encoders.",
            "variation_measure": "Trained in multiple variants: (1) full training set, (2) subset ≤6 blocks, (3) single blueprint of 6 blocks.",
            "variation_level": "Varied across the three ablated experiments; still includes some environment randomness but lacks explicit relational modeling.",
            "performance_metric": "Success rate (per-blueprint) after training (1B steps).",
            "performance_value": "Catastrophic failure: after 1B env steps all three ResNet variants had 0% success rate on all blueprints, and never exceeded 2.5% success on any train or held-out blueprint.",
            "complexity_variation_relationship": "Shows that removing relational inductive biases is catastrophic across variation and complexity: the model cannot exploit the compositional structure of tasks, failing even when variation is limited. This implies relational structure is critical when environment complexity and variation are high.",
            "high_complexity_low_variation_performance": "0% success (catastrophic) after 1B steps when trained on full training set and also when trained on limited subsets.",
            "low_complexity_high_variation_performance": "0%–≤2.5% success (did not meaningfully learn) in evaluated setups.",
            "high_complexity_high_variation_performance": "0% success in all tested settings after 1B steps.",
            "low_complexity_low_variation_performance": "Also failed in most low-variation settings tested (0% after 1B in some cases; never exceeded 2.5% on any blueprint).",
            "training_strategy": "PPO training with flattened inputs and a residual network encoder (various training subsets tested).",
            "generalization_tested": true,
            "generalization_results": "Poor: ResNet variants failed to learn and thus could not generalize; relational inductive bias (graph structure + attention) is necessary for any meaningful generalization or performance.",
            "sample_efficiency": "Extremely poor — after 1B environment steps there was effectively no learning (0% success across blueprints).",
            "key_findings": "Relational/object-centric inductive biases (graph neural networks with attention) are essential for learning and generalization in this compositional assembly domain; removing them leads to catastrophic failure regardless of training variation.",
            "uuid": "e1046.2"
        },
        {
            "name_short": "Single-Gripper Agent (Ablation)",
            "name_full": "Single-Gripper Structured Agent (one virtual gripper)",
            "brief_description": "A version of the structured agent restricted to a single virtual gripper to test the necessity of bimanual coordination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Single-Gripper Agent",
            "agent_description": "Same GNN-based architecture but with only one gripper (action space restricted), trained with PPO on the assembly tasks to evaluate effect of reduced manipulation capability.",
            "agent_type": "simulated agent",
            "environment_name": "Magnetic Block Assembly Environment (single-gripper ablation)",
            "environment_description": "Same environment but agent has only one gripper (instead of two), increasing difficulty for tasks requiring coordinated two-handed maneuvers.",
            "complexity_measure": "Blueprint size (2–16 blocks) and inherent requirement for bimanual maneuvers (some blueprints require simultaneous positioning or holding of substructures).",
            "complexity_level": "Medium-to-high complexity particularly for blueprints requiring bimanual coordination.",
            "variation_measure": "Same blueprint set as main experiments (multi-task vs single-task variants tested).",
            "variation_level": "High (when evaluated across multi-blueprint set) but single-gripper capacity reduces effective ability to handle variation.",
            "performance_metric": "Success rate (per-blueprint, aggregated).",
            "performance_value": "Quantitative numbers not provided in main text; reported qualitatively as 'overall success rate is lower than that of a dual-gripper agent, particularly on the more complex blueprints.' Exact per-blueprint rates are not stated in the paper text provided.",
            "complexity_variation_relationship": "Ablation shows that reducing the agent's embodiment (fewer effectors) reduces capability on high-complexity blueprints; certain strategies exist for single-gripper to complete some structures but overall lower performance indicates a trade-off between agent embodiment (gripper count) and ability to solve complex/varied tasks.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Same large-scale PPO training as main agent but with single-gripper action space.",
            "generalization_tested": true,
            "generalization_results": "Single-gripper agent finds unique strategies to complete some structures, but generally exhibits lower success and weaker performance on complex held-out blueprints compared to the bimanual agent.",
            "sample_efficiency": "Not quantitatively reported; qualitative comparison indicates less efficient/less successful learning on complex tasks when using a single gripper.",
            "key_findings": "Bimanual capability (two grippers) significantly improves success on complex tasks; agent embodiment (number of effectors) is an important factor for solving high-complexity assembly problems.",
            "uuid": "e1046.3"
        },
        {
            "name_short": "Blueprint-Reset Augmented Agent",
            "name_full": "Agent Trained with Preconstructed-Blueprint Resets (Reset-Augmented Training)",
            "brief_description": "A version of the structured agent trained with episodic initial states that include preconstructed blueprints (with probability 0.2), which increases initial-state diversity and enables stronger reset-free operation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Blueprint-Reset Augmented Agent",
            "agent_description": "Same GNN-based PPO agent but with environment initialization modified: with probability 0.2 episodes start from a randomly sampled pre-constructed blueprint (unused blocks scattered), enabling learning of disassembly and improved continuous operation.",
            "agent_type": "simulated agent",
            "environment_name": "Magnetic Block Assembly Environment with blueprint resets",
            "environment_description": "Environment identical to main but initial states sometimes start from pre-built blueprints to increase state diversity and to train agents on disassembly and reassembly behaviors; used to evaluate reset-free continuous operation.",
            "complexity_measure": "As main environment: blueprint sizes (≥12 used for reset-free evaluation), number of sequential tasks in reset-free run (10 consecutive blueprints per reset-free episode), episode length (100 steps per blueprint).",
            "complexity_level": "High for reset-free evaluation (sequences of high-complexity blueprints, minimum 12 blocks per blueprint in evaluation).",
            "variation_measure": "Initial-state variation increased by sampling preconstructed blueprints (probability 0.2 during training); reset-free evaluation sequences sample from training set blueprints (min 12 blocks).",
            "variation_level": "High (training includes intentionally varied initial states and disassembly scenarios).",
            "performance_metric": "Reset-free success rate aggregated over sequences of consecutive blueprints (build 10 consecutive blueprints per reset-free episode).",
            "performance_value": "Reset-free evaluation (50 reset-free episodes, building 10 blueprints each, sampled from training set structures requiring ≥12 blocks): Without blueprint resets during training: 69.4% ± 17.0% aggregated success. With blueprint resets during training: 93.1% ± 7.5% aggregated success.",
            "complexity_variation_relationship": "Training with blueprint resets increases initial-state variation and forces agents to learn disassembly and reassembly; this improves resilience and performance in high-complexity, continuous (reset-free) scenarios — demonstrating that increasing variation in initial states can substantially improve performance on long-horizon/reset-free operation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Reset-free sequences of high-complexity blueprints (≥12 blocks): 93.1% ± 7.5% success when trained with blueprint resets vs 69.4% ± 17.0% without.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-task PPO with blueprint-reset augmentation (probability 0.2 of starting from preconstructed blueprints), curriculum sampling, and standard GNN architecture.",
            "generalization_tested": true,
            "generalization_results": "Agents trained with blueprint resets generalize to reset-free continuous operation much better than agents trained without such initial-state variation; training with resets allows agents to disassemble incorrect constructions and continue operating across multiple sequential goals.",
            "sample_efficiency": "Not separately quantified beyond overall training scale; improvement in reset-free performance comes from the same large-scale training but with initial-state reset augmentation.",
            "key_findings": "Including pre-constructed blueprint resets during episodic training (increasing initial-state variation) substantially improves performance in practical reset-free continuous operation, indicating that exposing agents to diverse initial conditions (including partially-built structures) is beneficial for robustness and long-horizon performance.",
            "uuid": "e1046.4"
        },
        {
            "name_short": "Gripper-Transition-Delay Finetuned Agent",
            "name_full": "Agent Fine-tuned with Gripper-Change Transition Delays (to reduce unrealistic fast re-grasping)",
            "brief_description": "Agent that was first trained in the default direct-manipulation environment and then fine-tuned in a modified environment where a gripper is disabled for 2 steps upon changing which object it holds, to mitigate unrealistic behaviors induced by direct manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Gripper-Transition-Delay Finetuned Agent",
            "agent_description": "Pretrained GNN-PPO agent is further trained (fine-tuned) in an environment variant that enforces a 2-step disablement of a gripper when it switches the object it's holding, to emulate a more realistic gripper transition constraint.",
            "agent_type": "simulated agent (fine-tuned)",
            "environment_name": "Magnetic Block Assembly Environment with gripper-transition delay",
            "environment_description": "Same assembly environment but when a gripper changes the object it holds, that gripper is disabled for 2 environment steps. This penalizes unrealistic rapid object switching and simulates a more realistic actuator constraint.",
            "complexity_measure": "Same blueprint complexity measures; added constraint increases temporal coordination requirement and effectively increases problem difficulty.",
            "complexity_level": "Increased difficulty relative to default because of enforced gripper handover/transition delay constraint.",
            "variation_measure": "Single modification to action-effect mapping (gripper-change delay) applied during fine-tuning; other environment variation unchanged.",
            "variation_level": "Low-to-moderate (single type of environment constraint variation applied during fine-tuning).",
            "performance_metric": "Success rate during fine-tuning and recovery time measured in env steps.",
            "performance_value": "When the delay was applied after default training and the agent continued training (fine-tuning), the agent's success rate initially dropped significantly but recovered to strong performance within less than 100M environment steps. Training from scratch with the gripper-transition delay is far more challenging: after 1.5B steps the agent trained from scratch with delays still failed to make significant progress on many blueprints.",
            "complexity_variation_relationship": "Demonstrates that imposing a more realistic constraint (reducing an unrealistic capability) increases effective task difficulty, but that sequential training (train in simpler setting then finetune on constrained setting) is an efficient trade-off: initial training in a higher-variation/easier setting followed by finetuning on constrained variants recovers performance with modest additional sample cost.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Pretrain in default easier direct-manipulation environment (large-scale PPO), then fine-tune in environment with 2-step gripper-change disablement; comparison to training-from-scratch with delay.",
            "generalization_tested": true,
            "generalization_results": "Fine-tuning from a pretrained agent recovers strong performance within &lt;100M additional steps; training from scratch under the gripper-change delay is much harder and unsuccessful even after 1.5B steps for many blueprints. This supports a strategy of curriculum/ecosystem where agents are first trained in simpler settings then adapted to more realistic constraints.",
            "sample_efficiency": "Finetuning efficient (recovery in &lt;100M env steps); training from scratch under the constraint is inefficient (1.5B steps still insufficient for substantial progress).",
            "key_findings": "1) Realistic actuator constraints (gripper-change delay) increase task difficulty, but finetuning from agents trained in simpler settings is an efficient approach to adapt to such constraints. 2) Training from scratch with realistic constraints can be prohibitively expensive compared to two-stage train-then-finetune workflows.",
            "uuid": "e1046.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured agents for physical construction.",
            "rating": 2,
            "sanitized_title": "structured_agents_for_physical_construction"
        },
        {
            "paper_title": "Learn2assemble with structured representations and search for robotic architectural construction.",
            "rating": 2,
            "sanitized_title": "learn2assemble_with_structured_representations_and_search_for_robotic_architectural_construction"
        },
        {
            "paper_title": "Ikea furniture assembly environment for long-horizon complex manipulation tasks.",
            "rating": 2,
            "sanitized_title": "ikea_furniture_assembly_environment_for_longhorizon_complex_manipulation_tasks"
        },
        {
            "paper_title": "Building lego using deep generative models of graphs.",
            "rating": 1,
            "sanitized_title": "building_lego_using_deep_generative_models_of_graphs"
        },
        {
            "paper_title": "Towards practical multi-object manipulation using relational reinforcement learning.",
            "rating": 2,
            "sanitized_title": "towards_practical_multiobject_manipulation_using_relational_reinforcement_learning"
        },
        {
            "paper_title": "Causalworld: A robotic manipulation benchmark for causal structure and transfer learning.",
            "rating": 1,
            "sanitized_title": "causalworld_a_robotic_manipulation_benchmark_for_causal_structure_and_transfer_learning"
        }
    ],
    "cost": 0.0219505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning</p>
<p>Seyed Kamyar kamyar@google.com 
Google Research</p>
<p>Seyed Ghasemipour 
Google Research</p>
<p>Daniel Freeman cdfreeman@google.com 
Google Research</p>
<p>Byron David byrondavid@google.com 
Google Research</p>
<p>Shane Shixiang shanegu@google.com 
Google Research</p>
<p>Gu 
Google Research</p>
<p>Satoshi Kataoka 
Google Research</p>
<p>Igor Mordatch imordatch@google.com 
Google Research</p>
<p>Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning</p>
<p>Assembly of multi-part physical structures is both a valuable end product for autonomous robotics, as well as a valuable diagnostic task for open-ended training of embodied intelligent agents. We introduce a naturalistic physics-based environment with a set of connectable magnet blocks inspired by children's toy kits. The objective is to assemble blocks into a succession of target blueprints. Despite the simplicity of this objective, the compositional nature of building diverse blueprints from a set of blocks leads to an explosion of complexity in structures that agents encounter. Furthermore, assembly stresses agents' multi-step planning, physical reasoning, and bimanual coordination. We find that the combination of large-scale reinforcement learning and graph-based policies -surprisingly without any additional complexity -is an effective recipe for training agents that not only generalize to complex unseen blueprints in a zero-shot manner, but even operate in a reset-free setting without being trained to do so. Through extensive experiments, we highlight the importance of large-scale training, structured representations, contributions of multi-task vs. single-task learning, as well as the effects of curriculums, and discuss qualitative behaviors of trained agents. Our accompanying project webpage can be found at: sites.google.com/view/learning-direct-assembly</p>
<p>Introduction</p>
<p>Robotic assembly of objects from a given set of parts is an incredibly intriguing avenue for research in artificial intelligence (AI). Not only is it a valuable capability we would like autonomous robots to possess, but it is a challenging problem statement with open-ended complexity, touching upon many fruitful avenues of AI research. Agents that can assemble structures can reshape their surroundings, which creates dynamic environments with more possibilities for open-ended learning (Baker et al., 2019;Wang et al., 2019;Co-Reyes et al., 2020). A key feature of assembly is that due to its compositional and modular nature, given a set of parts, one can create objects on a broad spectrum of complexity. Furthermore, in order to solve the assembly problem, agents must acquire a diverse set of skills and capabilities. They must learn to grasp and attach components in an order that is amenable to successful completion. They must develop physical reasoning capabilities to avoid collision, and they need to learn bi-manual coordination. In addition, once agents have acquired such skills, it is expected that they can rapidly learn to create new desired objects.</p>
<p>To study assembly, we create a simulated but naturalistic environment which consists of blocks of varying shapes that can be magnetically attached to one-another. Agents are then tasked with constructing a desired structure from one of almost 200 pre-designed blueprints (although generative models can be used to automate this process (Thompson et al., 2020)). In this environment, in lieu of grasping robotic arms, we enable agents to directly move desired blocks. Such direct manipulation and the use of magnetic connections (instead of more complex joining mechanisms) abstracts away some details of the assembly problem while retaining many of the challenges that make the problem hard and interesting. Compared robot manipulation tasks which arXiv:2203.13733v2 [cs.RO] 12 Apr 2022 Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning Figure 1. Examples of target blueprints we consider. We train on variety of target structures, ranging from structures of 2 to 16 blocks. emphasize rearrangement and stacking Ope-nAI et al., 2021), the compositional nature of assembly from a set of blocks leads to an agent continually encountering new structures of varying complexities, which necessitates developing a richer representation of what constitutes an "object" (Spelke, 1990). In addition, our magnetic assembly benchmark stresses multi-step planning, physical reasoning, and bimanual coordination.</p>
<p>Despite the complexity of this problem, we find that it is possible to train a single agent that can simultaneously assemble all given blueprint tasks, generalize to complex unseen blueprints in a zero-shot manner, and even operate in a reset-free manner despite being trained in an episodic fashion. Our solution relies on a combination of large-scale reinforcement learning, structured (graph-based) agent representations, and simultaneous multi-blueprint training. Simultaneously training on diverse blueprints of varying complexity scaffolds the agent's learning by enabling it to first make progress of simpler tasks (such as simply joining two blocks), while structured policy representations enable the agent to generalize and transfer its solutions towards solving more complex -and even unseen -blueprints. We empirically observe a progression of learning increasingly large blueprints -many of which were not solvable with singleblueprint training. Surprisingly, we find other components such as planning or hierarchical approaches to be unnecessary for this task. Through experiments, we highlight the contributions of various components such as structured policies, episodic initial state distribution, curriculum that emphasize training on harder blueprints, and discuss qualitative behaviors and maneuvers discovered by the trained agents.</p>
<p>Our contributions are as follows:</p>
<p>• We introduce an assembly domain that allows for a controlled study of generalization in reinforcement learning (RL).</p>
<p>• We demonstrate a single agent that can simultaneously solve all seen assembly tasks and generalize to unseen tasks.</p>
<p>• We demonstrate the importance of combining largescale RL, structured policies, and multi-task training grippers magnets block Figure 2. Our magnetic assembly domain. Two virtual grippers directly manipulate the available blocks to magnetically assemble a desired blueprint structure.</p>
<p>as a route to arrive at generally capable agents.</p>
<p>We hope this work further encourages study of assembly as an open-ended means to develop and evaluate embodied agent learning.</p>
<p>Magnetic Block Assembly Environment</p>
<p>Our goal is to design a minimal tractable assembly environment to study generalization in a naturalistic, multi-step, combinatorial, dynamic problem requiring bi-hand coordination. We construct a three-dimensional environment containing a fixed set of 16 cuboid blocks of 6 different types. Blocks contain positive and negative magnet points, rendered as red and blue respectively, positioned on the block surface. Positive and negative magnets "snap" together when sufficiently close, and disconnect when adequate pulling force is applied. Magnets enable creation of arbitrarily complex composed structures from the given building blocks. Additionally, magnets can be implemented in the real-world, unlike more abstract locking constraints (e.g. instantaneous weld constraints), yet are tolerant enough to join objects without tackling the problem of high-precision insertion that would be required for other connection mechanisms such as pegs or screws. To simplify the problem, in lieu of robotic arms we opt for the use of virtual grippers which can directly manipulate desired blocks. More specifically, each gripper can decide which block to move, and set its positional and rotational velocities. The use of direct manipulation abstracts away the challenges of grasping and manipulation with a robotic arm, and enables us to focus on research questions concerning higher-level assembly behaviors such as planning and generalization to unseen structures. While the number of grippers is parameterizable, unless otherwise specified, throughout this work we will use 2 virtual grippers.</p>
<p>To specify the assembly task, we designed 165 blueprints (split into 141 train, 24 test) describing interesting structures to be built, although the blueprints can potentially be procedurally generated (Thompson et al., 2020). The complexity of the created blueprints range from requiring only a single magnetic connection, up to challenging structures that make use of all 16 available blocks. The problem statement in our magnetic assembly environment is simple to describe: In each episode, the agent must assemble the blocks to create the desired blueprint. Each episode begins with either all blocks randomly scattered around the environment, or from a randomly sampled pre-constructed blueprint -with unused blocks dispersed on the ground. Episodes are 100 environment steps long, translating to a length of 10 seconds in the real world. In each step agents receive rewards based on how close blocks are to their intended configurations, as well as correct and incorrect magnetic connections. Episodes terminate when exactly correct magnetic connections are made and blueprint blocks are in the correct relative position and orientation, or when 100 steps has passed. Our simulated assembly task is implemented in the open-source Mujoco (Todorov et al., 2012) physics engine. A detailed description of observation space, action space, rewards, and success criterion used can be found in Appendix B.</p>
<p>Methodology</p>
<p>While solving the magnetic assembly task can be approached through a variety of solutions such as hierarchical reinforcement learning and geometric planning algorithms (e.g. RRT (LaValle et al., 2001)), in past work it has been demonstrated that many tasks which intuitively require complex planning strategies can be solved through large-scale application of reinforcement learning algorithms using the right training setups, and appropriate neural network architectures and inductive biases (Silver et al., 2017;Berner et al., 2019;Vinyals et al., 2019;Baker et al., 2019). Motivated by such results, in this work we explore the ingredients necessary to train effective agents for magnetic assembly through RL. In the subsequent sections we describe the main ingredients of training successful agents and study the contribution of each component. Using dot-product attention between the keys and queries, the grippers decide which block to hold, and output its proposed move. The global latent representation is used to predict a baseline value for the PPO (Schulman et al., 2017) algorithm.</p>
<p>Agents</p>
<p>In this section we describe our structured agents, including observations and action spaces, and graph-based network architecture. The Python code describing our agent architecture can be found in Appendix C.</p>
<p>Structured Observations</p>
<p>The observations provided to the agent can be divided into two broad categories, those concerning the blocks, and those concerning the grippers. When designing observations to provide to agents, we have taken the effort to ensure observations are invariant to the global position and orientation. This is a valuable inductive bias that provides agents with the flexibility build desired blueprint anywhere and in any rotation.</p>
<p>Block Observations In the assembly task, observations pertaining to the blocks can be naturally organized into a directed graph, with each node containing information about a particular block, and each directed edge representing relative information about the two blocks. The information contained in each node is very minimal: the z height of the block from the ground, and whether it was being held by each gripper in the previous timestep. The majority of observations are placed on the directed edges. An edge connecting two blocks contains the information regarding: relative position and orientation of their magnets that need to be connected, change in relative position and orientation of the blocks needed to match the blueprint, relative position of center of mass of the blocks, whether the blocks are magnetically attached, and whether the blocks should be magnetically attached according to the blueprint. All these observations can be automatically extracted from the simulator state and the target blueprint configuration, and can realistically be computed in a real-world setting as well by simply obtaining each blocks position and orientation. Detailed information regarding exact observations can be found in Appendix B.</p>
<p>Gripper Observations For each gripper we include its orientation, positional and rotational velocities, and which block the gripper was holding in the previous timestep.</p>
<p>Graph Neural Network Encoder</p>
<p>Given that our magnetic assembly task can be naturally set up using graph-based observations, prior to extracting actions and critic values, we first encode inputs using a graph neural network architecture , specifically graph attention networks (Veličković et al., 2017). The two inputs to our encoder are (1) a directed graph containing all block observations (2) a "global node" containing gripper observations. After linearly embedding all input features, they are passed through N = 3 graph attention layers whose design is inspired by Transformers (Vaswani et al., 2017) and Graph Attention Networks (Veličković et al., 2017). Concisely, in each layer, each node aggregates information by attending to incoming edge and node features, and subsequently the global node features are updated by aggregating information from the graph nodes. An intuitive diagram describing the architecture used can bee seen in Figure 3.</p>
<p>Policy</p>
<p>Through experimentation, we have discovered that in addition to a graph neural network encoder, a key design choice is how to extract policy actions from the encoded inputs. The outputs of the graph neural network encoder are hidden features per node corresponding to the blocks, and hidden features corresponding to the global node. Using linear layers, from each block node we obtain 2 vectors: (1) a vector representing how the block would like to be moved if a gripper chooses to move it, and (2) a vector representing a key vector for the block. From the global hidden features, using linear layers we obtain one query vector per gripper. To obtain logits representing which gripper decides to move which block, we use dot-product attention between the block keys and the gripper query vectors, akin to the popular attention mechanism used in Transformers (Vaswani et al., 2017). In addition to the use of graph neural networks, using this form of decoding from the graph encoder has been a key enabler in training effective policies.</p>
<p>Critic</p>
<p>To train our RL agents we also require critic value estimates, which we obtain by passing global features obtained from the graph encoder to a 3 layer MLP, with 512 dimensional hidden layers, and relu activation function.</p>
<p>Training and Evaluation</p>
<p>Large-Scale PPO We train our agents using Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Generalized Advantage Estimation (GAE) (Schulman et al., 2015), and follow the practical PPO training advice of (Andrychowicz et al., 2020a). As will be shown below, one of the most key ingredient in enabling the training of our magnetic assembly agents is the scale of training. Unless otherwise specified, our agents are trained for 1 Billion environment timesteps, using 1 Nvidia V100 GPU for training, and 3000 preemptible CPUs for generating rollouts in the environment. 1 Billion steps in our setup amounts to about 48 hours of training. The key libraries used for training are Jax (Bradbury et al., 2018), Jraph (Godwin* et al., 2020), Haiku (Hennigan et al., 2020), and Acme (Hoffman et al., 2020).</p>
<p>Multi-Task Training</p>
<p>The blueprints that we have designed range from very simple 2 block structures, up to complex blueprints containing all blocks. To train assembly agents, we have split blueprints into training and testing structures, and unless otherwise specified, agents are trained on the full training set of blueprints; in each episode, we sample a training blueprint and task the agent with creating that structure.</p>
<p>Initial State Episodes start from either (1) all the blocks randomly dispersed on the ground, or (2) a randomly chosen preconstructed blueprint structure with unused blocks randomly arranged on the ground. Resetting from blueprints increases the diversity of initial states, forces the agent to learn how to disassemble structures, and as we found enables a reset-free mode of operation where the agent can continually construct, deconstruct, and reconstruct differ-ent blueprints. Unless otherwise specified, we reset from training blueprints with probability 0.2.</p>
<p>Curriculum We have observed that throughout training, some blueprints can be quickly learned while others can be much more challenging. We believe an interesting feature of the assembly problem is that even if a blueprint is currently unsolvable, due to the modular aspect of building complex structures, agents can learn more effectively if we emphasize focus on more challenging blueprints rather than allocating resources to rolling out policies on blueprints that they are already capable of solving. For this reason, in each episode we sample goal blueprints based on a curriculum whose detailed description can be found in Appendix D.</p>
<p>Performance Evaluation During training, we evaluate trained policies continuously approximately every 10 minutes by freezing the policy and computing average success rate over 40 episodes. This continuous evaluation is executed on both training and test environments. Also, in each evaluation cycle, we generate a video to visualize the agents' behavior. Such visualizations have been a valuable asset in iterating over the design of our agents, observations, reward functions, and training setups.</p>
<ol>
<li>Experiments 6.1. Importance of Large-Scale Training</li>
</ol>
<p>We begin by verifying that our training procedure leads to capable assembly agents. To this end, we train our structured agents (Section 4), using the training procedure described in Section 5, for 2.5 billion environment steps to observe training patterns that may arise over a long period of training. Figure 4 presents the success rates of our agent (averaged across two runs) throughout training, on blueprints the agent was trained on as well as held-out structures (per blueprint success rates presented in Figure 11 in the appendix).</p>
<p>The first key observation is the compute scale necessary for effectively training our structured agents using PPO (Schulman et al., 2017). The simplest 2 block structures can take up to 100 million steps to be reliably solved, while it can take up to 500 million environment steps until the first time some of the most complex blueprints are solved. The second observation is that after a long period of training, not only can agents reliably solve all training blueprints, but they can also generalize well to complex held-out blueprints.</p>
<p>Multi-Task vs. Single-Task</p>
<p>As noted in Section 5, agents are simultaneously trained to construct all blueprints in the training split. To understand the contribution of this "Multi-Task" training, we train three agents in a "Single-Task" setting: one for learning to con-struct a particular 6 block blueprint, one for constructing a particular 12 block blueprint, and one for constructing a particular 16 block blueprint. The success rates for these three agents can be found in Figures 14, 15, and 16 respectively.</p>
<p>Our key observations are the following: (1) While the 6 and 12 block blueprints are eventually learned, the 16 block blueprint is not learned, (2) In the single-task setting, the 12 block blueprint requires approximately 500 million environment steps to be learned, while in the multi-task setting ( Figure 11) it is learned within 300 million steps, (3) the single-task agents can transfer to some blueprints of equal or lower complexity than they were trained on, but mostly fail to transfer to any blueprints they were not trained to solve. This is in sharp contrast to the multi-task agents which can even transfer to complex held-out blueprints. These results highlight the necessity of multi-task training, not only for generalization to unseen blueprints, but for quickly and reliably solving complex tasks, despite the fact that agent architectures are well-matched to the problem domain.</p>
<p>Structured Agent Architecture</p>
<p>As discussed in section 4, given that state information for the assembly task can be naturally organized into a graph representation, the use of graph neural networks imbues agents with an inductive bias that is well-matched to the domain. Indeed, prior work (Bapst et al., 2019;Li et al., 2020) have observed that the use of agents with relational structures is a key ingredient in solving object-oriented tasks.</p>
<p>In this section we aim to understand the contribution of various components of our agents' architecture. Figure 10 demonstrates the effect of removing the attention mechanism in the graph neural network layers, meaning that while our agents continue have a graph inductive bias, the hidden representations for each block are updated by treating other blocks equally, rather than deciding which blocks to attend to. The results in Table 2 and Figure 10 clearly demonstrate the necessity of the attention mechanism.</p>
<p>Removing Attention in Graph Layers</p>
<p>Removing Relational Inductive Biases We also attempt to train agents without relational inductive biases. Instead, we flatten the environment observations and use a residual network (He et al., 2016) encoder, with a similar action and value decoder as described in Section 4. Details of the residual network architecture are described in Appendix E. We train three variants of the residual network agents: (1) trained on the full training set of blueprints, (2) trained on a subset of the training blueprints requiring ≤ 6 blocks, and (3) trained on a single training blueprint requiring 6 blocks. Figures 21, 22, and 23 demonstrate that in all three scenarios, removing the relational inductive bias of graph neural networks is catastrophic. After 1 billion steps, all  (1) the key role of scale in training successful agents, and that (2) after a long period of training, agents generalize well to complex held-out blueprints. Per blueprint results presented in Figure 11. Steps Until Success" denotes approximately the first timestep at which the respective agent was able to successfully create the blueprint. While for very small structures single-blueprint training can be effective, for more complex blueprints, single-blueprint agents take significantly longer or are entirely unable to learn the task within 1 billion timesteps. agents have a 0% success rate on all blueprints, and never accomplish higher than 2.5% success rate on any train or held-out blueprint.</p>
<p>Bimanual Manipulation</p>
<p>Compared to most robotics tasks and benchmarks (Levine et al., 2018;Kalashnikov et al., 2018;Andrychowicz et al., 2020b;Chen et al., 2021;Huang et al., 2021;Yu et al., 2020;Li et al., 2020;Batra et al., 2020;OpenAI et al., 2021), part-based assembly stresses the bimanual coordination of agents. To verify that our magnetic assembly domain stresses this skill, we compare success rates between bimanual and single-gripper agents in Table 2 and Figure 9. We find that while our single-gripper agents finds unique strategies to complete some of the structures, its overall success rate is lower than that of a dual-gripper agent, particularly on the more complex blueprints. This indicates the necessity of using two grippers in our domain.</p>
<p>Reset-Free Evaluation</p>
<p>As described in Section 5, with probability 0.2, the initial state of an episode is set to be a randomly selected preconstructed blueprint, with remaining blocks dispersed on the ground. This choice has two advantages: (1) it provides an opportunity for agents to learn how to disassemble incorrect constructions, and (2) it enables the evaluation of our agents in a reset-free manner, where we continually task agents with constructing new blueprints without resetting the environment to an initial state.</p>
<p>To evaluate the contribution of this choice, we compare the success rate of two agents, one with and one without blueprint resets, in a reset-free setting. Specifically, within one reset-free episode we ask agents to build 10 consecutive blueprints without resetting to an initial state: once an agent successfully constructs a blueprint, or the maximum of 100 steps has elapsed, we change the target blueprint. As an additional challenge, we sample blueprints from the training set structures requiring a minimum of 12 blocks. We report the success rate aggregated across 50 reset-free episodes (i.e. 50 × 10 total episodes).</p>
<p>When resetting from blueprints is disabled, our agent achieves a sucess rate of 69.4% ± 17.0%. In constrast, with blueprint resets, the success rate increases to 93.1% ± 7.5%. This is an exciting finding as it demonstrates a scenario where episodic training enables agents to be deployed in the practically-relevant reset-free scenario.</p>
<p>Curriculum</p>
<p>As described in Section 5, throughout training we make use of a curriculum that increases the likelihood of sampling more challenging blueprints. To analyze its contribution, we compare two runs of agents with curriculum, to an agent trained without curriculum. Our results in Figures 17 and 18 indicate that our curriculums do not have a clear-cut benefit, but may be leading to improvements in generalization to blueprints in the held-out test set.</p>
<p>Curbing Unrealistic Behaviors</p>
<p>Due to the use of direct manipulation, agents can rapidly switch which block they are holding, which can result in sometimes unrealistic maneuvers not achievable by physical robot grippers. Thus, in the event we wanted to transfer the success from our direct manipulation environment to more realistic settings using robotic arms, it is important to understand how one can mitigate unrealistic behaviors. To this end, after training an agent using the default training procedure described in Section 5, we modify the environment as follows: Whenever a gripper chooses to change the object it is holding, we disable that gripper for 2 steps. After this change, we continue to train our agent. Figure 19 demonstrates that while initially the agent's success rate drops very significantly, within less than 100 million environment steps the agent recovers its strong performance. This is a small amount of steps compared to the 2.5+ billion environment steps to train the agent.</p>
<p>Given this result, one might ask whether we could have started training our agents using this environment modification from the beginning. Figure 20 compares this approach to our default training setup. As can be seen, training agents from scratch using gripper transition delays is a significantly more challenging problem, and even after 1.5 billion environment steps, the agent is still unable to make significant progress on many of the blueprints in the training set. These results demonstrate that an efficient approach towards training practical agents is to first train agents in the simplest settings, and continue to finetune those agents in more realistic scenarios. Videos demonstrating behaviors of agents discussed in this section can be found in the accompanying project webpage.</p>
<p>Analyzing Learned Solutions</p>
<p>In this section our goal is to obtain a qualitative understanding of the strategies learned by our trained agents.</p>
<p>Learned Attention Patterns As shown in Section 6.3, the attention mechanism is a key ingredient in the graph neural network architecture we used. To understand what the attention heads in the different layers have learned to focus on, we visualize agents' attention patterns throughout different episodes. We observe the following interpretable patterns: (1) Some attention heads focus on the blocks that should be connected according to the blueprint, (2) Some keep account of which blocks are currently connected, and (3) Some focus on which blocks are currently being held by the two grippers. Other attention heads, particularly in the later layers, are more challenging to interpret, but appear to contain a combination of the previously described attention motifs. Videos visualizing learned attention patterns can be found in the accompanying website.</p>
<p>Qualitative Behaviours Rolling out trained agents, we observe a number of interesting learned behaviors. Examples of such behaviors include: (1) Despite environment observations not providing fine-grained detail about freespace, agents appear to have learned robust collision avoidance skills.</p>
<p>(2) When building complex structures, agents appear to first build separate smaller substructures, and subsequently attach the substructures to construct the full blueprint.</p>
<p>Related Work</p>
<p>Assembly and Construction Bapst et al. (2019) previously studied two-dimensional construction environments, training an agent to assemble a structure for an open-ended goal, such as a connecting or a covering structure. Certain details of assembly are abstracted away, as the agent has the ability to directly summon a block of choice anywhere in the scene and weld blocks via an explicit action. By contrast, our environment contains a fixed set of blocks that must be moved -or reassembled from a previous structure -in three-dimensional space, and where block connections are made via magnet forces. Such a design makes the environment more easily implementable as a real-world robot setup. Lee et al. (2019; also introduce a three-dimensional assembly environment for furniture design from a blueprint. By contrast, we use generic blocks, which leads to combinatorial complexity in the space of structures, and enables a more controlled study of generalization. A key point of differentiation from the above works is that we use assembly as a domain for the study of generalization, and train a single agent to solve all -seen and unseen -assembly tasks simultaneously. Additionally, we introduce the use of multiple grippers in assembly which allows us to evaluate the bimanual coordination of trained agents. Chung et al.</p>
<p>(2021) present a task where given side view images of a desired structure, Lego blocks must be stacked to create a structure with a similar silhouette. At each step, the state of the Lego structure is encoded using graph neural networks, and through deep RL, a policy learns where the next Lego block must be placed. In contrast, in our work we focus on the dynamic task of assembly, where discrete decisions and continuous control are solved simultaneously using largescale deep RL. Funk et al. (2022) introduce a 3D task where blocks must be re-arranged to create stable structures that occupy randomly sampled target regions. They present a long-horizon manipulation algorithm combining deep reinforcement learning and Monte-Carlo tree search, that at each step decides which block should be moved to which location. Similar to our work, multi-head attention graph neural networks are used, which enable generalization to new settings with larger number of blocks. By comparison, we train a single policy to jointly consider the low-level manipulation of the blocks and block selection. Suárez-Ruiz et al. (2018) studied assembly of a single chair with realworld bimanual robots using offline planning methods. Kim &amp; Seo (2019); Cabi et al. (2019) studied real-world insertion problem, which is an operation in the broader assembly process. Hartmann et al. (2021) present a planning system to solve long-horizon multi-robot construction problems, consisting of stacking parts to create architectural structures. In contrast, in this work we focused on the assembly problem and were motivated to understand whether deep reinforcement learning can be used so simultaneously solve discrete planning and continuous control.</p>
<p>Generalization in Robotic Manipulation</p>
<p>Much recent work in robot manipulation focused on the tasks of object grasping (Levine et al., 2018;Kalashnikov et al., 2018), in-hand object manipulation (Andrychowicz et al., 2020b;Chen et al., 2021;Huang et al., 2021), or execution of a motor skill (Yu et al., 2020), where variation comes from diversity of object shapes and arrangements involved. By contrast, while assembly uses a fixed set of blocks, the compound structures that must be manipulated have a combinatorial diversity of shape that dynamicaly changes during the episode. Li et al. (2020); Batra et al. (2020);OpenAI et al. (2021) propose scene re-arrangement as a universal task for embodied AI. While re-arrangement is general, we caution that many instances of rearrangement can be solved as a sequence of largely independent sub-tasks that do not influence each other, and can be performed in any order. By contrast, assembly steps are coupled and must be performed in a specific order that must be discovered by the agent. Ahmed et al. (2020); Funk et al. (2022) also present robotics benchmarks based on block rearrangement, with stronger emphasis on long-horizon planning, learning from structured representations, and generalization to unseen scenarios. Gupta et al. (2019) introduce a kitchen environment with an implicit dependency structure among sub-tasks (i.e. opening a container before putting something in it), but such pre-conditions are harder to scale.</p>
<p>Structured and Object-Centric Policies Besides our work, there have been numerous efforts to parameterize policies through structured models such as graph neural networks or Transformers, leveraging intrinsic objectness and invariances of the physical world (Spelke, 1990). They have been used for controlling agents of different morphologies for locomotion and manipulation tasks (Wang et al., 2018;Sanchez-Gonzalez et al., 2018;Chen et al., 2018;Pathak et al., 2019;Huang et al., 2020;Kurin et al., 2020), as well as enabling agents to learn a compositionally-challenging task like stacking . Other relevant works include parameterizing manipulation actions, especially of 2D tasks, as object-based spatial actions, which have enabled breakthroughs in vision-based manipulation (Zeng et al., 2020;Noguchi et al., 2021;Shridhar et al., 2022). While inspired by similar motivations, our work tackles a uniquely challenging task, 3D bi-manual assembly, compared to single-arm or 2D tasks in prior work Zeng et al., 2020).</p>
<p>Limitations, Discussion, &amp; Future Directions</p>
<p>We introduced a new blueprint assembly environment for studying bimanual assembly of multi-part physical struc-tures, and demonstrated training of a single agent that can simultaneously solve all seen and unseen assembly tasks via a combination of large-scale RL, structured policies, and multi-task training. While our work showed that a solution to our problem exists, it is by no means efficient -requiring billions of training episodes. It is likely that by incorporating planning or hierarchical methods, the training time can be significantly shortened. Additionally, upon maturity of accelerated simulation engines (Liang et al., 2018;Freeman et al., 2021;Makoviychuk et al., 2021), our agents may be trained at a similar compute scale using much more modest hardware infrastructures. Beyond more efficient training, in this work we chose to abstract away complexities of manipulation and perception. A more detailed treatment of these elements, such as constraints that prevent overly aggressive behaviors, can bring this work closer to robotics applications. And lastly, while we focused on blueprint assembly in this work, more open-ended assembly goals as well as multiagent interaction present further research opportunities for developing agents of increasing complexity.</p>
<p>Author Contributions</p>
<p>Kamyar Ghasemipour: First-author, ran all experiments, wrote manuscript draft, developed the agent architectures (e.g. Graph Neural Network architectures, policy action representations, etc.), contributed to the environment creation (improved and added observations, rewards, success conditions, moved observations from node-based towards edge-based structure, bug fixes).</p>
<p>Byron David, Daniel Freeman, Shane Gu: Helped experiment implementations, creating blueprints used for creating the magnetic parts assembly, helped with discussing ideas.</p>
<p>Satoshi Kataoka: Co-supervising author, created majority of robust infrastructure (evaluation infrastructure, and backup mechanisms), contributed to the environment creation (improved robustness, usability and reliability), reviewed all code modifications.</p>
<p>Igor Mordatch: Co-supervising author, initiated the research direction, created the environment as well benchmark task definition, created first proof of concept results motivating more focused investigation, guided project direction, rendered figures and videos.</p>
<p>A. Blueprints</p>
<p>In this section, we describe details of all blueprints. Figure 5 and 6 are used for training while figure 7 and 8 are used for testing.  In each step, the following rewards are computed and added together. The cumulative reward is then subtracted from the cumulative reward in the prior timestep, and the difference in cumulative rewards is returned as the environment reward for the current timestep.</p>
<p>• force penalty if two blocks, or a block and the ground, make heavy contact • -1 for each magnetic connection that should not be connected</p>
<p>• dense reward based on position and orientation of each two magnets that should be connected • +1 if two magnets that should be connected are connected</p>
<p>• dense reward based on position and orientation of each two blocks that should be attached to one-another</p>
<p>B.4. Success Criterion</p>
<p>An episode is deemed successful when in some timestep all of the following success criterion are satisfied.  </p>
<p>D. Curriculum</p>
<p>The following python script represents how our training curriculum is defined. 1 import numpy as np 2 from scipy import special </p>
<p>E. ResNet Baseline Details</p>
<p>When ablating the role of relation inductive biases in the agent architecture, we replace the graph neural network encoder in our default agent (described in Section 4) with a residual network encoder defined as follows:</p>
<p>1 import jax 2 import haiku as hk </p>
<p>Figure 3 .
3Diagram depicting our structured agent. Inputs to the agent are graph-structured block observations as well as gripper observations. A graph neural network processes the observations and produces: (1) per block moves and per block attention keys, (2) per gripper attention queries, (3) a global latent representation.</p>
<p>Figure 4 .
4Plot presenting success rates for different groups of blueprints throughout training. Each square represents the success rate on 40 episodes, evaluated at that point in training, averaged across two training runs. Our results demonstrate</p>
<p>•
magnetic connections required by the blueprint are made • no extra magnets outside the blueprint definition are connected • all blocks in the blueprint structure are in the correct relative position and orientationC. Graph Neural Network Architecture 1 import jax 2 import jax.numpy as jnp 3 import haiku as hk</p>
<p>s, b_i in zip(successes, blueprint_indices): 30 self.success_rates[i] = (1. -self.tau) * self.success_rates[i] + self.tau * s 31 32 self.success_rates * = self.decay # ensures there are not things that never get sampled 33 self.probs = special.softmax((1. -self.success_rates) / self.temp) 34 35 def sample(self): 36 return int(np.random.choice(np.arange(len(blueprint_indices)), p=self.probs))</p>
<p>Table 1 .
1Comparison of Single-Blueprint vs. Multi-Blueprint training for blueprints of various complexities. Success rates are calculated after 1 billion steps of training for the respective agents, and "Blueprint Size 
Single-Blueprint Training 
Multi-Blueprint Training (ours) 
Success Rate Steps Until Success Success Rate Steps Until Success 
6 Blocks 
99.6% 
100M 
100% 
180M 
12 Blocks 
99.9% 
480M 
98.8% 
220M 
16 Blocks 
0% 
-
90.9% 
240M </p>
<p>Table 3
3shows distribution for block sizes for different types.Number of blocks 
Types 
Training Test Test Literal Test Longest Test Tallest 
2 
14 
3 
31 
4 
36 
5 
5 
6 
9 
7 
6 
8 
6 
9 
6 
10 
5 
1 
11 
4 
12 
4 
2 
1 
1 
1 
13 
3 
2 
1 
1 
14 
5 
2 
1 
1 
1 
15 
4 
2 
1 
1 
16 
4 
2 
1 
1 
1 </p>
<p>Table 3 .
3Number of blueprint block distributionsB. Descriptions of Observation Space, Action Space, Rewards, Success Criterion </p>
<p>B.1. Observation Space </p>
<p>Per block observations on graph nodes </p>
<p>• z height from ground </p>
<p>• binary indicator for whether the block was being held in the previous timestep by some gripper </p>
<p>Observations on directed graph edge from block A to block B </p>
<p>• change in position for desired magnets to align </p>
<p>• change in orientation for desired magnets to align </p>
<p>• the difference between current block positional delta, and block positional delta in the blueprint </p>
<p>• the difference between current block orientation delta, and block orientation delta in the blueprint </p>
<p>• positional delta </p>
<p>• 0-1 indicator for whether blocks should be connected according to the blueprint </p>
<p>• 0-1 indicator for whether blocks should are currently connected </p>
<p>Per gripper observations </p>
<p>• orientation </p>
<p>• positional velocity </p>
<p>• rotational velocity </p>
<p>• 16-dimensional 0-1 vector showing which block the gripper was holding in the previous timestep </p>
<p>B.2. Action Space </p>
<p>• one 16-dimensional one-hot vector per gripper, representing which block the gripper wants to manipulate </p>
<p>• a 16 × 6 dimensional matrix, representing desired position and rotational velocity per block, were it to be chosen by a 
gripper to be manipulated </p>
<p>B.3. Rewards </p>
<p>return (jnp.reshape(m, list(m.shape[:-1]) + [NUM_HEADS, M_DIM]), edge_features) input_globals = jnp.squeeze(g.globals, axis=1) ######### The policy part 103 # first predict an action distribution per block 104 # for now lets try without further processing of the g_nodes # now for each gipper generate logits for which block it wants to move 119 # for now lets try without further processing of g_globals 120 global_feats = jnp.concatenate([input_globals, g_globals], axis=-1) block_active = jnp.reshape(input_g.globals, [input_g.globals.shape[0], NUM_GRIPS, -1]) block_active = block_active[:, :, -node_feats.shape[1]:] ######### The baseline value part 142 # for now just doing it based on the globals 143 144 # get the baseline value input_globals = jnp.squeeze(input_g.globals, axis=1) input_globals = hk.Linear(GLOBAL_EMBED_DIM, with_bias=False)(input_globals) g_globals = jnp.squeeze(output_g.globals, axis=1) g_globals = hk.Linear(GLOBAL_EMBED_DIM, with_bias=False)(g_globals) all_globals = jnp.concatenate([input_globals, g_globals], axis=-1) grip_logits_softmax = jax.lax.stop_gradient(jax.nn.softmax(grip_logits, axis=-1)) # dont want the value loss to influence gripper choice grip_logits_softmax = jnp.reshape(grip_logits_softmax, [grip_logits_softmax.shape[0], -1]) all_globals = jnp.concatenate([all_globals, grip_logits_softmax], axis=-1) value_network = hk.nets.MLP(def update_edge_fn(edge_features, sender_features, receiver_features, global_features): </p>
<p>27 </p>
<p>feats = jnp.concatenate([sender_features, edge_features], axis=-1) </p>
<p>28 </p>
<p>m = hk.Linear(NUM_HEADS * M_DIM, with_bias=False, name='linear_message')(feats) </p>
<p>29 </p>
<p>30 </p>
<p>31 </p>
<p>def update_node_fn(node_features, sender_features, receiver_features, global_features): </p>
<p>32 </p>
<p>receiver_features = jnp.reshape(receiver_features, list(receiver_features.shape[:-2]) 
+ [-1]) </p>
<p>33 </p>
<p>x = jnp.concatenate([receiver_features], axis=-1) </p>
<p>34 </p>
<p>residual = hk.Linear(node_features.shape[-1], with_bias=False, name='linear_node')(x) </p>
<p>35 </p>
<p>y = node_features + residual </p>
<p>36 </p>
<p>y = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True,)(y) </p>
<p>37 </p>
<p>38 </p>
<p>h = hk.nets.MLP( </p>
<p>39 </p>
<p>output_sizes=[NODE_MLP_HID] * NODE_MLP_NUM_LAYERS + [y.shape[-1]], </p>
<p>40 </p>
<p>activation=jax.nn.relu, </p>
<p>41 </p>
<p>name='mlp_node')(jnp.concatenate([y, global_features], axis=-1)) </p>
<p>42 </p>
<p>h = h + y </p>
<p>43 </p>
<p>h = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True,)(h) </p>
<p>44 </p>
<p>return h </p>
<p>45 </p>
<p>46 </p>
<p>def update_global_fn(node_features, edge_features, global_features): </p>
<p>47 </p>
<p>x = jnp.concatenate([global_features, node_features], axis=-1) </p>
<p>48 </p>
<p>h = hk.nets.MLP( </p>
<p>49 </p>
<p>output_sizes=[GLOBAL_MLP_HID] * GLOBAL_MLP_NUM_LAYERS + [GLOBAL_EMBED_DIM], </p>
<p>50 </p>
<p>activation=jax.nn.relu, </p>
<p>51 </p>
<p>name='mlp_global')(x) 
h = h + global_features </p>
<p>53 </p>
<p>h = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True,)(h) </p>
<p>54 </p>
<p>return h </p>
<p>55 </p>
<p>56 </p>
<p>def attention_logit_fn(edge_features, sender_features, receiver_features, 
global_features): </p>
<p>57 </p>
<p>edge_features = edge_features[1] </p>
<p>58 </p>
<p>q = hk.Linear(NUM_HEADS * QK_DIM, with_bias=False, name='linear_query')( </p>
<p>59 </p>
<p>jnp.concatenate([sender_features, edge_features], axis=-1)) </p>
<p>60 </p>
<p>q = jnp.reshape(q, list(q.shape[:-1]) + [NUM_HEADS, QK_DIM]) </p>
<p>61 </p>
<p>k = hk.Linear(NUM_HEADS * QK_DIM, with_bias=False, name='linear_key')( 
receiver_features) </p>
<p>62 </p>
<p>k = jnp.reshape(k, list(k.shape[:-1]) + [NUM_HEADS, QK_DIM]) </p>
<p>63 </p>
<p>return jnp.sum(q * k, axis=-1, keepdims=True) / (QK_DIM ** 0.5) </p>
<p>64 
def attention_reduce_fn(edge_features, weights): </p>
<p>66 </p>
<p>return edge_features[0] * weights </p>
<p>67 </p>
<p>68 </p>
<p>def encode_graph(g): </p>
<p>69 </p>
<p>return jraph.GraphMapFeatures( </p>
<p>70 </p>
<p>embed_edge_fn=hk.Linear(EDGE_EMBED_DIM, name='linear_embed_edge'), </p>
<p>71 </p>
<p>embed_node_fn=hk.Linear(NODE_EMBED_DIM, name='linear_embed_node'), </p>
<p>72 </p>
<p>embed_global_fn=hk.Linear(GLOBAL_EMBED_DIM, name='linear_embed_global') </p>
<p>73 </p>
<p>)(g) </p>
<p>74 </p>
<p>75 </p>
<p>@jax.vmap </p>
<p>76 </p>
<p>def _ppo_graph_part(g): </p>
<p>77 </p>
<p>g = encode_graph(g) </p>
<p>78 </p>
<p>enc_edges = g.edges </p>
<p>79 </p>
<p>80 </p>
<p>for _ in range(NUM_GN_GAT_LAYERS): </p>
<p>81 </p>
<p>g = jraph.GraphNetGAT( </p>
<p>82 </p>
<p>update_edge_fn=update_edge_fn, </p>
<p>83 </p>
<p>update_node_fn=update_node_fn, </p>
<p>84 </p>
<p>attention_logit_fn=attention_logit_fn, </p>
<p>85 </p>
<p>attention_reduce_fn=attention_reduce_fn, </p>
<p>86 </p>
<p>update_global_fn=update_global_fn, </p>
<p>87 </p>
<p>)(g) </p>
<p>88 </p>
<p>g = g._replace(edges=enc_edges) </p>
<p>89 </p>
<p>90 </p>
<p>return g </p>
<p>91 </p>
<p>92 </p>
<p>def _ppo_graph_network(g): </p>
<p>93 </p>
<p>input_g = g </p>
<p>94 </p>
<p>95 </p>
<p>input_globals = hk.Linear(GLOBAL_EMBED_DIM, with_bias=False)(input_globals) </p>
<p>96 </p>
<p>97 </p>
<p>g = _ppo_graph_part(g) </p>
<p>98 </p>
<p>output_g = g </p>
<p>99 </p>
<p>g_globals = jnp.squeeze(g.globals, axis=1) </p>
<p>100 </p>
<p>g_nodes = g.nodes # B x n_nodes x NODE_DIM </p>
<p>101 </p>
<p>102 </p>
<p>105 </p>
<p>node_feats = g_nodes </p>
<p>106 </p>
<p>107 </p>
<p>BLOCK_ACT_DIM = 6 </p>
<p>108 </p>
<p>block_act_locs = hk.Linear( </p>
<p>109 </p>
<p>BLOCK_ACT_DIM, </p>
<p>110 </p>
<p>w_init=hk.initializers.VarianceScaling(1e-4), </p>
<p>111 </p>
<p>b_init=hk.initializers.Constant(0.))(node_feats) </p>
<p>112 </p>
<p>block_act_scales = hk.Linear( </p>
<p>113 </p>
<p>BLOCK_ACT_DIM, </p>
<p>114 </p>
<p>w_init=hk.initializers.VarianceScaling(1e-4), 
b_init=hk.initializers.Constant(0.))(node_feats) </p>
<p>116 </p>
<p>block_act_scales = jax.nn.softplus(block_act_scales) + 1e-6 </p>
<p>117 </p>
<p>118 </p>
<p>121 </p>
<p>122 </p>
<p>NUM_GRIPS = 2 </p>
<p>123 </p>
<p>grip_keys = hk.Linear(NUM_GRIPS * QK_DIM, with_bias=False)(global_feats) </p>
<p>124 </p>
<p>grip_keys = jnp.reshape(grip_keys, [grip_keys.shape[0], NUM_GRIPS, 1, QK_DIM]) # B x G 
x 1 x QK_DIM </p>
<p>125 </p>
<p>126 </p>
<p>127 </p>
<p>128 </p>
<p>block_active = jnp.transpose(block_active, [0, 2, 1]) </p>
<p>129 </p>
<p>block_keys = hk.Linear(QK_DIM, with_bias=False)( </p>
<p>130 </p>
<p>jnp.concatenate([node_feats, block_active], axis=-1)) # B x n_nodes x QK_DIM </p>
<p>131 </p>
<p>132 </p>
<p>block_keys = jnp.reshape(block_keys, [block_keys.shape[0], 1, block_keys.shape[1], 
QK_DIM]) # B x 1 x n_nodes x QK_DIM </p>
<p>133 </p>
<p>134 </p>
<p>grip_logits = jnp.sum(grip_keys * block_keys, axis=-1) / (QK_DIM ** 0.5) # B x G x 
n_nodes </p>
<p>135 </p>
<p>136 </p>
<p>policy_output = PolicyOutput( </p>
<p>137 </p>
<p>block_act_locs=block_act_locs, </p>
<p>138 </p>
<p>block_act_scales=block_act_scales, </p>
<p>139 </p>
<p>grip_logits=grip_logits,) </p>
<p>140 </p>
<p>141 </p>
<p>145 </p>
<p>146 </p>
<p>147 </p>
<p>148 </p>
<p>149 </p>
<p>150 </p>
<p>151 </p>
<p>152 </p>
<h1>trying something</h1>
<p>153 </p>
<p>154 </p>
<p>155 </p>
<p>156 </p>
<p>157 </p>
<p>158 </p>
<p>mlp_inputs = all_globals </p>
<p>159 </p>
<p>160 </p>
<p>161 </p>
<p>output_sizes=VALUE_MLP_LAYER_SIZES + [1], </p>
<p>162 </p>
<p>activation=jax.nn.relu) </p>
<p>163 </p>
<p>value = value_network(mlp_inputs) </p>
<p>164 </p>
<p>value = jnp.squeeze(value, axis=-1) </p>
<p>165 </p>
<p>166 </p>
<p>return (policy_output, value) </p>
<p>167 </p>
<p>168 </p>
<p>return _ppo_graph_network </p>
<p>uniform_initializer = hk.initializers.VarianceScaling( scale=0.333, mode='fan_out', distribution='uniform') return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)3 </p>
<p>4 NUM_RESNET_BLOCKS = 4 
5 HIDDEN_DIM = 1024 </p>
<p>6 </p>
<p>7 8 </p>
<p>9 </p>
<p>10 def residual_block(x): </p>
<p>11 </p>
<p>h = x </p>
<p>12 </p>
<p>h = hk.Linear(HIDDEN_DIM, w_init=uniform_initializer)(h) </p>
<p>13 </p>
<p>h = jax.nn.relu(h) </p>
<p>14 </p>
<p>h = hk.Linear(HIDDEN_DIM, w_init=uniform_initializer)(h) </p>
<p>15 </p>
<p>x = x + h </p>
<p>16 </p>
<p>def resnet_encoder(x):</p>
<p>O Ahmed, F Träuble, A Goyal, A Neitz, Y Bengio, B Schölkopf, M Wüthrich, S Bauer, Causalworld, arXiv:2010.04296A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprintAhmed, O., Träuble, F., Goyal, A., Neitz, A., Bengio, Y., Schölkopf, B., Wüthrich, M., and Bauer, S. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprint arXiv:2010.04296, 2020.</p>
<p>What matters for on-policy deep actor-critic methods? a large-scale study. M Andrychowicz, A Raichuk, P Stańczyk, M Orsini, S Girgin, R Marinier, L Hussenot, M Geist, O Pietquin, M Michalski, International Conference on Learning Representations. Andrychowicz, M., Raichuk, A., Stańczyk, P., Orsini, M., Girgin, S., Marinier, R., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., et al. What matters for on-policy deep actor-critic methods? a large-scale study. In International Conference on Learning Representations, 2020a.</p>
<p>Learning dexterous in-hand manipulation. O M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, The International Journal of Robotics Research. 391Andrychowicz, O. M., Baker, B., Chociej, M., Jozefowicz, R., McGrew, B., Pachocki, J., Petron, A., Plappert, M., Powell, G., Ray, A., et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020b.</p>
<p>Emergent tool use from multi-agent autocurricula. B Baker, I Kanitscheider, T Markov, Y Wu, G Powell, B Mcgrew, I Mordatch, arXiv:1909.07528arXiv preprintBaker, B., Kanitscheider, I., Markov, T., Wu, Y., Pow- ell, G., McGrew, B., and Mordatch, I. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528, 2019.</p>
<p>Structured agents for physical construction. V Bapst, A Sanchez-Gonzalez, C Doersch, K Stachenfeld, P Kohli, P Battaglia, J Hamrick, International Conference on Machine Learning. PMLRBapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K., Kohli, P., Battaglia, P., and Hamrick, J. Structured agents for physical construction. In International Confer- ence on Machine Learning, pp. 464-474. PMLR, 2019.</p>
<p>D Batra, A X Chang, S Chernova, A J Davison, J Deng, V Koltun, S Levine, J Malik, I Mordatch, R Mottaghi, arXiv:2011.01975A challenge for embodied ai. arXiv preprintBatra, D., Chang, A. X., Chernova, S., Davison, A. J., Deng, J., Koltun, V., Levine, S., Malik, J., Mordatch, I., Mot- taghi, R., et al. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020.</p>
<p>P W Battaglia, J B Hamrick, V Bapst, A Sanchez-Gonzalez, V Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, arXiv:1806.01261Relational inductive biases, deep learning, and graph networks. arXiv preprintBattaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez- Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., et al. Rela- tional inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.</p>
<p>Dota 2 with large scale deep reinforcement learning. C Berner, G Brockman, B Chan, V Cheung, P Debiak, C Dennison, D Farhi, Q Fischer, S Hashme, C Hesse, arXiv:1912.06680arXiv preprintBerner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.</p>
<p>JAX: composable transformations of Python+NumPy programs. J Bradbury, R Frostig, P Hawkins, M J Johnson, C Leary, D Maclaurin, G Necula, A Paszke, J Vanderplas, S Wanderman-Milne, Q Zhang, Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.</p>
<p>Scaling data-driven robotics with reward sketching and batch reinforcement learning. S Cabi, S G Colmenarejo, A Novikov, K Konyushkova, S Reed, R Jeong, K Zolna, Y Aytar, D Budden, M Vecerik, arXiv:1909.12200arXiv preprintCabi, S., Colmenarejo, S. G., Novikov, A., Konyushkova, K., Reed, S., Jeong, R., Zolna, K., Aytar, Y., Budden, D., Vecerik, M., et al. Scaling data-driven robotics with reward sketching and batch reinforcement learning. arXiv preprint arXiv:1909.12200, 2019.</p>
<p>Hardware conditioned policies for multi-robot transfer learning. T Chen, A Murali, A Gupta, arXiv:1811.09864arXiv preprintChen, T., Murali, A., and Gupta, A. Hardware conditioned policies for multi-robot transfer learning. arXiv preprint arXiv:1811.09864, 2018.</p>
<p>A system for general inhand object re-orientation. T Chen, J Xu, Agrawal , P , Conference on Robot Learning. Chen, T., Xu, J., and Agrawal, P. A system for general in- hand object re-orientation. Conference on Robot Learn- ing, 2021.</p>
<p>Brick-by-brick: Combinatorial construction with deep reinforcement learning. H Kim, J Knyazev, B Lee, J Taylor, G W Park, J Cho, M , Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning Chung. 342021Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning Chung, H., Kim, J., Knyazev, B., Lee, J., Taylor, G. W., Park, J., and Cho, M. Brick-by-brick: Combinatorial construction with deep reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>. J D Co-Reyes, S Sanjeev, G Berseth, A Gupta, S Levine, arXiv:2006.12478Ecological reinforcement learning. arXiv preprintCo-Reyes, J. D., Sanjeev, S., Berseth, G., Gupta, A., and Levine, S. Ecological reinforcement learning. arXiv preprint arXiv:2006.12478, 2020.</p>
<p>Brax -a differentiable physics engine for large scale rigid body simulation. C D Freeman, E Frey, A Raichuk, S Girgin, I Mordatch, O Bachem, Freeman, C. D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., and Bachem, O. Brax -a differentiable physics engine for large scale rigid body simulation, 2021. URL http: //github.com/google/brax.</p>
<p>Learn2assemble with structured representations and search for robotic architectural construction. N Funk, G Chalvatzaki, B Belousov, J Peters, Conference on Robot Learning. PMLRFunk, N., Chalvatzaki, G., Belousov, B., and Peters, J. Learn2assemble with structured representations and search for robotic architectural construction. In Confer- ence on Robot Learning, pp. 1401-1411. PMLR, 2022.</p>
<p>Jraph: A library for graph neural networks in jax. * Godwin, J Keck, * , T Battaglia, P Bapst, V Kipf, T Li, Y Stachenfeld, K Veličković, P Sanchez-Gonzalez, A , Godwin<em>, J., Keck</em>, T., Battaglia, P., Bapst, V., Kipf, T., Li, Y., Stachenfeld, K., Veličković, P., and Sanchez- Gonzalez, A. Jraph: A library for graph neural net- works in jax., 2020. URL http://github.com/ deepmind/jraph.</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, arXiv:1910.11956arXiv preprintGupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019.</p>
<p>Long-horizon multi-robot rearrangement planning for construction assembly. V N Hartmann, A Orthey, D Driess, O S Oguz, M Toussaint, arXiv:2106.02489arXiv preprintHartmann, V. N., Orthey, A., Driess, D., Oguz, O. S., and Toussaint, M. Long-horizon multi-robot rearrange- ment planning for construction assembly. arXiv preprint arXiv:2106.02489, 2021.</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.</p>
<p>T Hennigan, T Cai, T Norman, I Babuschkin, Haiku, Sonnet for JAX. Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/ deepmind/dm-haiku.</p>
<p>M Hoffman, B Shahriari, J Aslanides, G Barth-Maron, F Behbahani, T Norman, A Abdolmaleki, A Cassirer, F Yang, K Baumli, S Henderson, A Novikov, S G Colmenarejo, S Cabi, C Gulcehre, T L Paine, A Cowie, Z Wang, B Piot, N De Freitas, Acme, arXiv:2006.00979A research framework for distributed reinforcement learning. arXiv preprintHoffman, M., Shahriari, B., Aslanides, J., Barth-Maron, G., Behbahani, F., Norman, T., Abdolmaleki, A., Cas- sirer, A., Yang, F., Baumli, K., Henderson, S., Novikov, A., Colmenarejo, S. G., Cabi, S., Gulcehre, C., Paine, T. L., Cowie, A., Wang, Z., Piot, B., and de Freitas, N. Acme: A research framework for distributed reinforce- ment learning. arXiv preprint arXiv:2006.00979, 2020. URL https://arxiv.org/abs/2006.00979.</p>
<p>One policy to control them all: Shared modular policies for agent-agnostic control. W Huang, I Mordatch, D Pathak, International Conference on Machine Learning. PMLRHuang, W., Mordatch, I., and Pathak, D. One policy to con- trol them all: Shared modular policies for agent-agnostic control. In International Conference on Machine Learn- ing, pp. 4455-4464. PMLR, 2020.</p>
<p>Generalization in dexterous manipulation via geometry-aware multi-task learning. W Huang, I Mordatch, P Abbeel, D Pathak, arXiv:2111.03062arXiv preprintHuang, W., Mordatch, I., Abbeel, P., and Pathak, D. Gener- alization in dexterous manipulation via geometry-aware multi-task learning. arXiv preprint arXiv:2111.03062, 2021.</p>
<p>Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, arXiv:1806.10293arXiv preprintKalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018.</p>
<p>Shallow-depth insertion: Peg in shallow hole through robotic in-hand manipulation. C H Kim, J Seo, IEEE Robotics and Automation Letters. 42Kim, C. H. and Seo, J. Shallow-depth insertion: Peg in shallow hole through robotic in-hand manipulation. IEEE Robotics and Automation Letters, 4(2):383-390, 2019.</p>
<p>My body is a cage: the role of morphology in graph-based incompatible control. V Kurin, M Igl, T Rocktäschel, W Boehmer, S Whiteson, arXiv:2010.01856arXiv preprintKurin, V., Igl, M., Rocktäschel, T., Boehmer, W., and White- son, S. My body is a cage: the role of morphology in graph-based incompatible control. arXiv preprint arXiv:2010.01856, 2020.</p>
<p>Rapidlyexploring random trees: Progress and prospects. S M Lavalle, J J Kuffner, B Donald, Algorithmic and computational robotics: new directions. 5LaValle, S. M., Kuffner, J. J., Donald, B., et al. Rapidly- exploring random trees: Progress and prospects. Algo- rithmic and computational robotics: new directions, 5: 293-308, 2001.</p>
<p>Ikea furniture assembly environment for long-horizon complex manipulation tasks. Y Lee, E S Hu, Z Yang, A Yin, J J Lim, arXiv:1911.07246arXiv preprintLee, Y., Hu, E. S., Yang, Z., Yin, A., and Lim, J. J. Ikea furniture assembly environment for long-horizon complex manipulation tasks. arXiv preprint arXiv:1911.07246, 2019.</p>
<p>Adversarial skill chaining for long-horizon robot manipulation via terminal state regularization. Y Lee, J J Lim, A Anandkumar, Y Zhu, arXiv:2111.07999arXiv preprintLee, Y., Lim, J. J., Anandkumar, A., and Zhu, Y. Ad- versarial skill chaining for long-horizon robot manipu- lation via terminal state regularization. arXiv preprint arXiv:2111.07999, 2021.</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, The International Journal of Robotics Research. 374-5Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J., and Quillen, D. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. The International Journal of Robotics Research, 37(4-5):421- 436, 2018.</p>
<p>Towards practical multi-object manipulation using relational reinforcement learning. R Li, A Jabri, T Darrell, Agrawal , P , 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEELi, R., Jabri, A., Darrell, T., and Agrawal, P. Towards prac- tical multi-object manipulation using relational reinforce- ment learning. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 4051-4058. IEEE, 2020.</p>
<p>Gpu-accelerated robotic simulation for distributed reinforcement learning. J Liang, V Makoviychuk, A Handa, N Chentanez, M Macklin, D Fox, Conference on Robot Learning. PMLRLiang, J., Makoviychuk, V., Handa, A., Chentanez, N., Macklin, M., and Fox, D. Gpu-accelerated robotic simula- tion for distributed reinforcement learning. In Conference on Robot Learning, pp. 270-282. PMLR, 2018.</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprintMakoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., Hoeller, D., Rudin, N., Allshire, A., Handa, A., et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021.</p>
<p>Tool as embodiment for recursive manipulation. Y Matsushima, T Matsuo, Y Gu, S S , arXiv:2112.00359Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning Noguchi. arXiv preprintBlocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning Noguchi, Y., Matsushima, T., Matsuo, Y., and Gu, S. S. Tool as embodiment for recursive manipulation. arXiv preprint arXiv:2112.00359, 2021.</p>
<p>Asymmetric self-play for automatic goal discovery in robotic manipulation. O Openai, M Plappert, R Sampedro, T Xu, I Akkaya, V Kosaraju, P Welinder, R D&apos;sa, A Petron, H P D O Pinto, arXiv:2101.04882arXiv preprintOpenAI, O., Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., Pinto, H. P. d. O., et al. Asymmetric self-play for automatic goal discovery in robotic manipulation. arXiv preprint arXiv:2101.04882, 2021.</p>
<p>Learning to control self-assembling morphologies: a study of generalization via modularity. D Pathak, C Lu, T Darrell, P Isola, A A Efros, arXiv:1902.05546arXiv preprintPathak, D., Lu, C., Darrell, T., Isola, P., and Efros, A. A. Learning to control self-assembling morphologies: a study of generalization via modularity. arXiv preprint arXiv:1902.05546, 2019.</p>
<p>Graph networks as learnable physics engines for inference and control. A Sanchez-Gonzalez, N Heess, J T Springenberg, J Merel, M Riedmiller, R Hadsell, P Battaglia, International Conference on Machine Learning. PMLRSanchez-Gonzalez, A., Heess, N., Springenberg, J. T., Merel, J., Riedmiller, M., Hadsell, R., and Battaglia, P. Graph networks as learnable physics engines for infer- ence and control. In International Conference on Machine Learning, pp. 4470-4479. PMLR, 2018.</p>
<p>High-dimensional continuous control using generalized advantage estimation. J Schulman, P Moritz, S Levine, M Jordan, Abbeel , P , arXiv:1506.02438arXiv preprintSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. PMLRShridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pp. 894-906. PMLR, 2022.</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, nature. 5507676Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. nature, 550(7676):354-359, 2017.</p>
<p>Principles of object perception. E S Spelke, Cognitive science. 141Spelke, E. S. Principles of object perception. Cognitive science, 14(1):29-56, 1990.</p>
<p>Can robots assemble an ikea chair?. F Suárez-Ruiz, X Zhou, Q.-C Pham, Science Robotics. 3176385Suárez-Ruiz, F., Zhou, X., and Pham, Q.-C. Can robots as- semble an ikea chair? Science Robotics, 3(17):eaat6385, 2018.</p>
<p>Building lego using deep generative models of graphs. R Thompson, E Ghalebi, T Devries, G W Taylor, arXiv:2012.11543arXiv preprintThompson, R., Ghalebi, E., DeVries, T., and Taylor, G. W. Building lego using deep generative models of graphs. arXiv preprint arXiv:2012.11543, 2020.</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEETodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, pp. 5026-5033. IEEE, 2012.</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>P Veličković, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, arXiv:1710.10903Graph attention networks. arXiv preprintVeličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, Nature. 5757782Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575 (7782):350-354, 2019.</p>
<p>Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. R Wang, J Lehman, J Clune, K O Stanley, arXiv:1901.01753arXiv preprintWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (poet): Endlessly generating in- creasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019.</p>
<p>Learning structured policy with graph neural networks. T Wang, R Liao, J Ba, S Fidler, Nervenet, International Conference on Learning Representations. Wang, T., Liao, R., Ba, J., and Fidler, S. Nervenet: Learning structured policy with graph neural networks. In Interna- tional Conference on Learning Representations, 2018.</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Conference on Robot Learning. PMLRYu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A benchmark and evalua- tion for multi-task and meta reinforcement learning. In Conference on Robot Learning, pp. 1094-1100. PMLR, 2020.</p>
<p>A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, arXiv:2010.14406Transporter networks: Rearranging the visual world for robotic manipulation. arXiv preprintZeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., et al. Transporter networks: Rearranging the visual world for robotic manipulation. arXiv preprint arXiv:2010.14406, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
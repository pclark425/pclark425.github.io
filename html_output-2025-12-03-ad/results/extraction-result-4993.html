<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4993 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4993</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4993</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270710993</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.17169v3.pdf" target="_blank">Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types — propositional, first-order, and non-monotonic consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs such as GPT-4, ChatGPT, Gemini-Pro, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4993.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4993.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary large language model from OpenAI evaluated on Multi-LogiEval for multi-step logical reasoning using zero-shot chain-of-thought and neurosymbolic pipelines; among the best-performing models in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary transformer-based LLM from OpenAI evaluated in April 2024; used in zero-shot Chain-of-Thought (Zero-shot-CoT) prompting and in a neurosymbolic pipeline (conversion to FOL and Prover9).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A newly introduced benchmark (Multi-LogiEval) of ~1.6k binary QA instances spanning propositional logic (PL), first-order logic (FOL), and non-monotonic (NM) reasoning with depths 1–5, 33 inference rules, and >60 rule-combinations; task is to judge whether a conclusion follows from a natural-language context.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with zero-shot Chain-of-Thought prompting (Zero-shot-CoT); also used to translate NL to formal FOL for a neurosymbolic pipeline where Prover9 performs formal proof; additionally evaluated in a 3-shot (few-shot) prompting condition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports overall trend: average accuracy across evaluated LLMs drops from ~68% at depth-1 to ~43% at depth-5; GPT-4 is among the top performers on PL and FOL and shows improved performance at depth-3 relative to depth-2 but declines at depths 4–5. In a single-step multi-variable FOL evaluation (Appendix H) GPT-4 achieved 80% average accuracy. In neurosymbolic experiments (GPT-4 → Prover9) Table 7 reports overall accuracies by depth (e.g., d1 overall acc 55.83%, d2 46.67%, d3 38.89%, d4 40.00%, d5 60.00%) for the executable pipeline, indicating accuracy and executable-accuracy decline with depth until d5 where an improvement was observed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with longer multi-step chains: performance drops substantially at depths 4–5; errors arise from incorrect evidence interpretation and information mapping across steps. Models can produce overly long or incorrect reasoning chains (error propagation). Neuorsymbolic pipeline shows reduced executable accuracy with depth (until d5 exception).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms ChatGPT and most open-source models on PL and FOL in this benchmark; comparable or better than Gemini in many settings; in single-step multi-variable FOL, GPT-4 (80%) was below Gemini (90%) and below ChatGPT (84.3%) per the appendix numbers. In neurosymbolic experiments, overall behavior follows general decline with depth but shows particular patterns reported in Table 7.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Manual chain analysis: at d2 ~27.4% of wrong reasoning chains due to mapping failures, which drops to ~22% at d3; average generated reasoning-chain lengths at d5: GPT-4 ~10.87 steps (paper-reported). Few-shot (3-shot) prompting improves performance for GPT-4 across depths but trends (drop at high depth) remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4993.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4993.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI chat-oriented LLM evaluated on Multi-LogiEval with zero-shot-CoT and few-shot prompting; strong at shallow depths but shows weaknesses at highest depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat-oriented LLM (version tested: April 2024 releases) evaluated using zero-shot Chain-of-Thought prompting and 3-shot few-shot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary entailment classification over natural-language contexts for PL, FOL, and NM reasoning at depths 1–5.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting (Zero-shot-CoT); additionally evaluated in 3-shot few-shot setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-paper trends: performs well at depth-1 and shows a peak at depth-3 for PL (improved from d2 to d3) but performs worse than GPT-4 and Gemini at depth-5 for PL and FOL. Reported average reasoning-chain length at d5 for ChatGPT is ~13.85 (longer than GPT-4 and Gemini). In single-step multi-variable FOL (Appendix H) ChatGPT achieved ~84.3% average accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Tendency to generate longer reasoning chains (average ~13.85 at d5) which do not correlate with better outcomes; higher error propagation for long chains; lower performance than GPT-4/Gemini at highest depths; struggles to map evidence correctly in some steps leading to incorrect conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms GPT-4 and Gemini at highest depths (d4–d5) for PL and FOL despite strong shallow-depth performance; in multi-variable single-step FOL ChatGPT (84.3%) was better than GPT-4 (80%) but below Gemini (90%). Few-shot prompting improves performance but does not fully remove weaknesses at high depths.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Manual analysis indicates ChatGPT has longer produced chains and that mapping failures account for a substantial share of errors (~27.4% at d2 decreasing at d3). Few-shot (3-shot) improves accuracy relative to zero-shot but high-depth drops persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4993.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4993.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini-Pro evaluated as a proprietary model on Multi-LogiEval using zero-shot-CoT; shows high performance and similar behavior to GPT-4 at mid depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary LLM (Google Gemini-Pro) evaluated in April 2024 experiments using zero-shot Chain-of-Thought prompting; treated as a top-tier proprietary baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary entailment QA covering PL, FOL, NM, depths 1–5.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting; also evaluated in 3-shot few-shot setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Gemini shows competitive performance with GPT-4, with improvements from depth-2 to depth-3 in PL and FOL and declines at depths 4–5; in single-step multi-variable FOL (Appendix H) Gemini achieved ~90% average accuracy. Few-shot (3-shot) leads to notable performance improvements similar to other proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance drops at deeper reasoning depths (d4–d5); subject to the same error-propagation issues as other LLMs; can mis-map premises across steps in multi-step chains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Comparable to GPT-4 at midsized depths (d2–d3); often outperforms ChatGPT at highest depths. In multi-variable single-step FOL Gemini (90%) outperformed both ChatGPT and GPT-4 in the single-step evaluation reported in the appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Manual chain analysis: Gemini generates shorter reasoning chains at d5 (avg ~8.85) compared to ChatGPT and GPT-4, and shorter chains correlated with better outcomes in some cases; few-shot prompting improves overall accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4993.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4993.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 7B-parameter LLM evaluated as one of the best-performing open-source models on Multi-LogiEval, especially at higher depths relative to larger open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B-parameter LLM (Mistral-7B-Instruct) evaluated with zero-shot-CoT and in few-shot (3-shot) settings. Paper cites training/architecture emphasis on reasoning (Jiang et al., 2023 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary entailment QA across PL, FOL, and NM at reasoning depths 1–5.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting; also evaluated in 3-shot few-shot setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Among open-source models, Mistral-7B performed best across most classical-logic depths (except the simplest d1). At d5 for PL and FOL Mistral-7B outperformed Orca-13B by roughly 3x according to the authors' analysis. For NM tasks, Mistral-7B had similar performance to Yi-34B across depths. Few-shot improved performance but high-depth drops persisted.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although strong for an open-source 7B model, performance still declines with higher reasoning depth; struggles on certain FOL mappings and on non-monotonic base cases at d1; overall absolute performance lower than top proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperformed larger open-source models (Orca-13B, Yi-34B) consistently at deeper reasoning depths despite being smaller; still lags behind proprietary models like GPT-4 and Gemini on PL and FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Authors attribute Mistral-7B's strong behavior to architecture/training choices that emphasize reasoning (citing Jiang et al., 2023); few-shot (3-shot) improves accuracy but does not fully close gap to proprietary models at highest depths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4993.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4993.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Orca-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Orca-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 13B-parameter LLM (Orca family) evaluated on Multi-LogiEval; shows weaker multi-step reasoning than smaller Mistral-7B in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 13B-parameter LLM (Orca-2-13B) evaluated with zero-shot-CoT and few-shot prompting in April 2024 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary entailment QA over PL, FOL, and NM across depths 1–5.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting; also evaluated in 3-shot few-shot setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance declines strongly with increasing reasoning depth; at the hardest depth (d5) for PL and FOL Orca-13B performed substantially worse than Mistral-7B (Mistral achieved roughly 3x the performance of Orca at d5 per authors). Few-shot improved performance but relative ordering among open-source models persisted.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Pronounced degradation at higher depths; error propagation and mis-mapping of premises across steps; overall performs worse than smaller Mistral-7B in these multi-step logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperformed Mistral-7B (7B) at deeper reasoning tasks despite larger parameter count; inferior to proprietary models GPT-4, Gemini, and ChatGPT on PL and FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Authors note a counterintuitive trend where larger open-source models show decreased performance compared to smaller ones; suggests architecture/training differences (not merely size) drive reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4993.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4993.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-34B (Yi)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 34B-parameter LLM (Yi-34B-Chat) evaluated on Multi-LogiEval; shows inconsistent behavior and generally lower multi-step reasoning performance compared to proprietary models and some smaller open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 34B-parameter model (Yi-34B-Chat) evaluated with zero-shot-CoT and in 3-shot settings (citation: Young et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary entailment evaluation spanning PL, FOL, NM with depths 1–5.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting and 3-shot few-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Shows variable performance: for some depths (e.g., certain NM settings) Yi-34B improved with depth, but in classical PL/FOL it showed decreasing performance as depth increases; in one comparison Mistral-7B outperformed Yi-34B. Overall few-shot helps but does not eliminate high-depth failures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Larger parameter count did not translate to better multi-step logical reasoning; suffers from error propagation, mapping failures, and lower accuracy at higher depths in PL/FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performed worse than proprietary models and worse than Mistral-7B at many depths; marginally better than Orca-13B at some mid depths in the authors' reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Authors note a trend that larger open-source models (Yi-34B, Orca-13B) can perform worse than smaller ones (Mistral-7B), indicating size alone isn't predictive; few-shot (3-shot) yields improvements but high-depth challenges persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language. <em>(Rating: 2)</em></li>
                <li>RuleTaker <em>(Rating: 2)</em></li>
                <li>Folio: Natural language reasoning with firstorder logic. <em>(Rating: 2)</em></li>
                <li>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models. <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. <em>(Rating: 2)</em></li>
                <li>Benchmark problems for formal nonmonotonic reasoning: Version 2.00. <em>(Rating: 1)</em></li>
                <li>Learning deductive reasoning from synthetic corpus based on formal logic. <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
                <li>Transformers as soft reasoners over language. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4993",
    "paper_id": "paper-270710993",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "Proprietary large language model from OpenAI evaluated on Multi-LogiEval for multi-step logical reasoning using zero-shot chain-of-thought and neurosymbolic pipelines; among the best-performing models in the study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary transformer-based LLM from OpenAI evaluated in April 2024; used in zero-shot Chain-of-Thought (Zero-shot-CoT) prompting and in a neurosymbolic pipeline (conversion to FOL and Prover9).",
            "model_size": null,
            "logical_reasoning_task": "Multi-LogiEval",
            "task_description": "A newly introduced benchmark (Multi-LogiEval) of ~1.6k binary QA instances spanning propositional logic (PL), first-order logic (FOL), and non-monotonic (NM) reasoning with depths 1–5, 33 inference rules, and &gt;60 rule-combinations; task is to judge whether a conclusion follows from a natural-language context.",
            "method_or_approach": "Evaluated with zero-shot Chain-of-Thought prompting (Zero-shot-CoT); also used to translate NL to formal FOL for a neurosymbolic pipeline where Prover9 performs formal proof; additionally evaluated in a 3-shot (few-shot) prompting condition.",
            "performance": "Paper reports overall trend: average accuracy across evaluated LLMs drops from ~68% at depth-1 to ~43% at depth-5; GPT-4 is among the top performers on PL and FOL and shows improved performance at depth-3 relative to depth-2 but declines at depths 4–5. In a single-step multi-variable FOL evaluation (Appendix H) GPT-4 achieved 80% average accuracy. In neurosymbolic experiments (GPT-4 → Prover9) Table 7 reports overall accuracies by depth (e.g., d1 overall acc 55.83%, d2 46.67%, d3 38.89%, d4 40.00%, d5 60.00%) for the executable pipeline, indicating accuracy and executable-accuracy decline with depth until d5 where an improvement was observed.",
            "limitations_or_failure_cases": "Struggles with longer multi-step chains: performance drops substantially at depths 4–5; errors arise from incorrect evidence interpretation and information mapping across steps. Models can produce overly long or incorrect reasoning chains (error propagation). Neuorsymbolic pipeline shows reduced executable accuracy with depth (until d5 exception).",
            "comparison": "Outperforms ChatGPT and most open-source models on PL and FOL in this benchmark; comparable or better than Gemini in many settings; in single-step multi-variable FOL, GPT-4 (80%) was below Gemini (90%) and below ChatGPT (84.3%) per the appendix numbers. In neurosymbolic experiments, overall behavior follows general decline with depth but shows particular patterns reported in Table 7.",
            "ablation_or_analysis_results": "Manual chain analysis: at d2 ~27.4% of wrong reasoning chains due to mapping failures, which drops to ~22% at d3; average generated reasoning-chain lengths at d5: GPT-4 ~10.87 steps (paper-reported). Few-shot (3-shot) prompting improves performance for GPT-4 across depths but trends (drop at high depth) remain.",
            "uuid": "e4993.0",
            "source_info": {
                "paper_title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI chat model)",
            "brief_description": "OpenAI chat-oriented LLM evaluated on Multi-LogiEval with zero-shot-CoT and few-shot prompting; strong at shallow depths but shows weaknesses at highest depths.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "OpenAI chat-oriented LLM (version tested: April 2024 releases) evaluated using zero-shot Chain-of-Thought prompting and 3-shot few-shot variants.",
            "model_size": null,
            "logical_reasoning_task": "Multi-LogiEval",
            "task_description": "Binary entailment classification over natural-language contexts for PL, FOL, and NM reasoning at depths 1–5.",
            "method_or_approach": "Zero-shot Chain-of-Thought prompting (Zero-shot-CoT); additionally evaluated in 3-shot few-shot setup.",
            "performance": "Per-paper trends: performs well at depth-1 and shows a peak at depth-3 for PL (improved from d2 to d3) but performs worse than GPT-4 and Gemini at depth-5 for PL and FOL. Reported average reasoning-chain length at d5 for ChatGPT is ~13.85 (longer than GPT-4 and Gemini). In single-step multi-variable FOL (Appendix H) ChatGPT achieved ~84.3% average accuracy.",
            "limitations_or_failure_cases": "Tendency to generate longer reasoning chains (average ~13.85 at d5) which do not correlate with better outcomes; higher error propagation for long chains; lower performance than GPT-4/Gemini at highest depths; struggles to map evidence correctly in some steps leading to incorrect conclusions.",
            "comparison": "Underperforms GPT-4 and Gemini at highest depths (d4–d5) for PL and FOL despite strong shallow-depth performance; in multi-variable single-step FOL ChatGPT (84.3%) was better than GPT-4 (80%) but below Gemini (90%). Few-shot prompting improves performance but does not fully remove weaknesses at high depths.",
            "ablation_or_analysis_results": "Manual analysis indicates ChatGPT has longer produced chains and that mapping failures account for a substantial share of errors (~27.4% at d2 decreasing at d3). Few-shot (3-shot) improves accuracy relative to zero-shot but high-depth drops persist.",
            "uuid": "e4993.1",
            "source_info": {
                "paper_title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gemini-Pro",
            "name_full": "Gemini-Pro",
            "brief_description": "Google's Gemini-Pro evaluated as a proprietary model on Multi-LogiEval using zero-shot-CoT; shows high performance and similar behavior to GPT-4 at mid depths.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-Pro",
            "model_description": "Proprietary LLM (Google Gemini-Pro) evaluated in April 2024 experiments using zero-shot Chain-of-Thought prompting; treated as a top-tier proprietary baseline.",
            "model_size": null,
            "logical_reasoning_task": "Multi-LogiEval",
            "task_description": "Binary entailment QA covering PL, FOL, NM, depths 1–5.",
            "method_or_approach": "Zero-shot Chain-of-Thought prompting; also evaluated in 3-shot few-shot setup.",
            "performance": "Gemini shows competitive performance with GPT-4, with improvements from depth-2 to depth-3 in PL and FOL and declines at depths 4–5; in single-step multi-variable FOL (Appendix H) Gemini achieved ~90% average accuracy. Few-shot (3-shot) leads to notable performance improvements similar to other proprietary models.",
            "limitations_or_failure_cases": "Performance drops at deeper reasoning depths (d4–d5); subject to the same error-propagation issues as other LLMs; can mis-map premises across steps in multi-step chains.",
            "comparison": "Comparable to GPT-4 at midsized depths (d2–d3); often outperforms ChatGPT at highest depths. In multi-variable single-step FOL Gemini (90%) outperformed both ChatGPT and GPT-4 in the single-step evaluation reported in the appendix.",
            "ablation_or_analysis_results": "Manual chain analysis: Gemini generates shorter reasoning chains at d5 (avg ~8.85) compared to ChatGPT and GPT-4, and shorter chains correlated with better outcomes in some cases; few-shot prompting improves overall accuracy.",
            "uuid": "e4993.2",
            "source_info": {
                "paper_title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral-7B-Instruct",
            "brief_description": "Open-source 7B-parameter LLM evaluated as one of the best-performing open-source models on Multi-LogiEval, especially at higher depths relative to larger open-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_description": "Open-source 7B-parameter LLM (Mistral-7B-Instruct) evaluated with zero-shot-CoT and in few-shot (3-shot) settings. Paper cites training/architecture emphasis on reasoning (Jiang et al., 2023 discussion).",
            "model_size": "7B",
            "logical_reasoning_task": "Multi-LogiEval",
            "task_description": "Binary entailment QA across PL, FOL, and NM at reasoning depths 1–5.",
            "method_or_approach": "Zero-shot Chain-of-Thought prompting; also evaluated in 3-shot few-shot setup.",
            "performance": "Among open-source models, Mistral-7B performed best across most classical-logic depths (except the simplest d1). At d5 for PL and FOL Mistral-7B outperformed Orca-13B by roughly 3x according to the authors' analysis. For NM tasks, Mistral-7B had similar performance to Yi-34B across depths. Few-shot improved performance but high-depth drops persisted.",
            "limitations_or_failure_cases": "Although strong for an open-source 7B model, performance still declines with higher reasoning depth; struggles on certain FOL mappings and on non-monotonic base cases at d1; overall absolute performance lower than top proprietary models.",
            "comparison": "Outperformed larger open-source models (Orca-13B, Yi-34B) consistently at deeper reasoning depths despite being smaller; still lags behind proprietary models like GPT-4 and Gemini on PL and FOL.",
            "ablation_or_analysis_results": "Authors attribute Mistral-7B's strong behavior to architecture/training choices that emphasize reasoning (citing Jiang et al., 2023); few-shot (3-shot) improves accuracy but does not fully close gap to proprietary models at highest depths.",
            "uuid": "e4993.3",
            "source_info": {
                "paper_title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Orca-2-13B",
            "name_full": "Orca-2-13B",
            "brief_description": "Open-source 13B-parameter LLM (Orca family) evaluated on Multi-LogiEval; shows weaker multi-step reasoning than smaller Mistral-7B in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Orca-2-13B",
            "model_description": "Open-source 13B-parameter LLM (Orca-2-13B) evaluated with zero-shot-CoT and few-shot prompting in April 2024 experiments.",
            "model_size": "13B",
            "logical_reasoning_task": "Multi-LogiEval",
            "task_description": "Binary entailment QA over PL, FOL, and NM across depths 1–5.",
            "method_or_approach": "Zero-shot Chain-of-Thought prompting; also evaluated in 3-shot few-shot setup.",
            "performance": "Performance declines strongly with increasing reasoning depth; at the hardest depth (d5) for PL and FOL Orca-13B performed substantially worse than Mistral-7B (Mistral achieved roughly 3x the performance of Orca at d5 per authors). Few-shot improved performance but relative ordering among open-source models persisted.",
            "limitations_or_failure_cases": "Pronounced degradation at higher depths; error propagation and mis-mapping of premises across steps; overall performs worse than smaller Mistral-7B in these multi-step logical tasks.",
            "comparison": "Underperformed Mistral-7B (7B) at deeper reasoning tasks despite larger parameter count; inferior to proprietary models GPT-4, Gemini, and ChatGPT on PL and FOL.",
            "ablation_or_analysis_results": "Authors note a counterintuitive trend where larger open-source models show decreased performance compared to smaller ones; suggests architecture/training differences (not merely size) drive reasoning ability.",
            "uuid": "e4993.4",
            "source_info": {
                "paper_title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Yi-34B",
            "name_full": "Yi-34B (Yi)",
            "brief_description": "Open-source 34B-parameter LLM (Yi-34B-Chat) evaluated on Multi-LogiEval; shows inconsistent behavior and generally lower multi-step reasoning performance compared to proprietary models and some smaller open-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi-34B",
            "model_description": "Open-source 34B-parameter model (Yi-34B-Chat) evaluated with zero-shot-CoT and in 3-shot settings (citation: Young et al., 2024).",
            "model_size": "34B",
            "logical_reasoning_task": "Multi-LogiEval",
            "task_description": "Binary entailment evaluation spanning PL, FOL, NM with depths 1–5.",
            "method_or_approach": "Zero-shot Chain-of-Thought prompting and 3-shot few-shot evaluation.",
            "performance": "Shows variable performance: for some depths (e.g., certain NM settings) Yi-34B improved with depth, but in classical PL/FOL it showed decreasing performance as depth increases; in one comparison Mistral-7B outperformed Yi-34B. Overall few-shot helps but does not eliminate high-depth failures.",
            "limitations_or_failure_cases": "Larger parameter count did not translate to better multi-step logical reasoning; suffers from error propagation, mapping failures, and lower accuracy at higher depths in PL/FOL.",
            "comparison": "Performed worse than proprietary models and worse than Mistral-7B at many depths; marginally better than Orca-13B at some mid depths in the authors' reported comparisons.",
            "ablation_or_analysis_results": "Authors note a trend that larger open-source models (Yi-34B, Orca-13B) can perform worse than smaller ones (Mistral-7B), indicating size alone isn't predictive; few-shot (3-shot) yields improvements but high-depth challenges persist.",
            "uuid": "e4993.5",
            "source_info": {
                "paper_title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language.",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "RuleTaker",
            "rating": 2
        },
        {
            "paper_title": "Folio: Natural language reasoning with firstorder logic.",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "LogicBench: Towards systematic evaluation of logical reasoning ability of large language models.",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers.",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning.",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Benchmark problems for formal nonmonotonic reasoning: Version 2.00.",
            "rating": 1,
            "sanitized_title": "benchmark_problems_for_formal_nonmonotonic_reasoning_version_200"
        },
        {
            "paper_title": "Learning deductive reasoning from synthetic corpus based on formal logic.",
            "rating": 1,
            "sanitized_title": "learning_deductive_reasoning_from_synthetic_corpus_based_on_formal_logic"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Transformers as soft reasoners over language.",
            "rating": 1,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        }
    ],
    "cost": 0.015648,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models
7 Oct 2024</p>
<p>Nisarg Patel nppatel7@asu.edu 
Arizona State University
USA</p>
<p>Mohith Kulkarni 
Arizona State University
USA</p>
<p>Mihir Parmar mparmar3@asu.edu 
Arizona State University
USA</p>
<p>Aashna Budhiraja 
Arizona State University
USA</p>
<p>Mutsumi Nakamura 
Arizona State University
USA</p>
<p>Neeraj Varshney 
Arizona State University
USA</p>
<p>Chitta Baral chitta@asu.edu 
Arizona State University
USA</p>
<p>Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models
7 Oct 20248CD6405F43BD3E2ED2D05DB69C7F1542arXiv:2406.17169v3[cs.CL]Rule Combination Context and Question PL Rules: MT, DS Propositions: p: Capture shots in golden hours. q: Photo wins awards. r: Focus on rare wildlife.BDDS Predicates: p: Work extra hours. q: Meet project deadlines. r: Take minimal breaks. s: Increase productivity HSCDDSMP Propositions: P: Studied hard for the exam. Q: Feel confident. R: Score well. S: Cooked nice dinner. T: Feel relaxed. U: Sleep soundly HSMTDSMPMP Propositions: P: Train consistently. Q: Increase endurance and stamina. R: complete the 26.2 mile marathon. S: Ate nutritious food. T: More steady energy. U: Train harder staying injury free BDDSMT Predicates: P: It's Monday. Q: There is a staff meeting. R: Finish report. S: Submit the report. T: Good Employee BDDSMTDS Predicates: P: First day of school. Q: students feel nervous and excited. R: Study Hard. S: get good grades. T: teacher is very strict. U: class textbook is very long HSMTDSMPMP Predicates: P: Practice drawing techniques. Q: improve artistic skills. R: sell their artwork. S: studies art history and famous artists. T: gain inspiration. U: develop creative style Basic Default Reasoning Default Reasoning with Irrelevant Information
As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning.Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multistep reasoning with a limited set of inference rules.Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning.To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multistep logical reasoning with various inference rules and depths.Multi-LogiEval covers three logic types -propositional, first-order, and non-monotonic consisting of more than 30 inference rules and more than 60 of their combinations with various depths.Leveraging this dataset, we conduct evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca, and Mistral, employing a zero-shot chain-of-thought.Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ∼ 68% at depth-1 to ∼ 43% at depth-5).We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings.We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs 1 .</p>
<p>Introduction</p>
<p>The ability to perform multi-step reasoning -drawing conclusions from provided multiple premises -is a hallmark of human intelligence.Recently, Large Language Models (LLMs) such as GPT-4, ChatGPT, Gemini, and Mistral (Jiang et al., 2023)  have achieved impressive performance on a variety of language tasks that were previously thought to be exclusive to humans (OpenAI, 2023;Brown et al., 2020;Zhao et al., 2023).However, the ability of these LLMs to perform multi-step logical reasoning over natural language remains underexplored, despite its various real-world applications (Khashabi, 2019;Beygi et al., 2022).Although several datasets have been proposed (Luo et al., 2023) to evaluate the logical reasoning capabilities of LLMs, these datasets are limited in their scope by (1) evaluating simplistic single-step logical reasoning such as ProntoQA (Saparov and He, 2023) and (2) evaluating multi-step logical reasoning, but only on a single type of logic and covering only a few logical inference rules as done in FO-LIO (Han et al., 2022) and ProofWriter (Tafjord et al., 2021).Furthermore, there are only a few benchmarks, such as LogicBench (Parmar et al., 2024) and BoardgameQA (Kazemi et al., 2023), that cover reasoning such as non-monotonic which is closer to human-like reasoning.Motivated by this, our work aims to bridge these gaps by creating a more comprehensive and logically complex eval-uation dataset by incorporating varying numbers of reasoning depths (i.e., multi-steps) to reach conclusions.In addition, past attempts have been made to evaluate multi-hop reasoning of language models (Mavi et al., 2022).In contrast, our work systematically evaluates multi-hop logical reasoning over various inference rules and their combinations.</p>
<p>To this end, we propose Multi-LogiEval, a systematically created Question-Answering (QA) dataset covering multi-step logical reasoning across three different logic types: Propositional Logic (PL), First-Order Logic (FOL), and Non-Monotonic (NM) reasoning.Our objective is to present a preliminary analysis of the LLMs' ability to perform multi-step logical reasoning and demonstrate their failures even when performing simple reasoning.We believe that, regardless of whether such reasoning is available in some existing natural data (e.g., examinations), LLMs should do proper logical reasoning.Thus, we systematically compiled data using various inference rules and varying numbers of reasoning depths.In particular, our proposed dataset provides ∼ 1.6k high-quality instances that cover 33 inference rules and reasoning patterns and more than 60 complex combinations of these inference rules with a different number of reasoning steps (1 ∼ 5).Our choice of inference rules is further explained in section 3.1.To evaluate LLMs, we formulate a binary classification task in Multi-LogiEval where the context represents a natural language story consisting of logical statements, and the models have to determine whether the story logically entails a conclusion given in the question.Examples of instances are presented in Table 4.To develop Multi-LogiEval, we propose a two-stage procedure: (i) creating meaningful combinations of inference rules to generate data instances with different reasoning depths, and (ii) prompt LLMs to generate <context, question, answer> triplets consisting of different 'ontologies' (i.e., a collection of concepts such as car, person, and animals).In the end, we perform human validation of each generated instance to ensure the quality.</p>
<p>We evaluate a range of LLMs, including GPT-4, ChatGPT, Gemini-Pro, Yi-34B (Young et al., 2024), Orca-2-13B (Mitra et al., 2023), and Mistral-7B (Wei et al., 2021) on Multi-LogiEval using Zeroshot Chain-of-Thought (Zero-shot-CoT) prompting (Wei et al., 2022).The zero-shot CoT approach allows us to determine LLM's ability to do logical reasoning based on parametric knowledge (acquired during pre-training) since we can not ex-
Dataset Logic Covered Multi-Step Logical Reasoning PL FOL NM LogicNLI ✗ ✓ ✗ ✗ ProofWriter ✓ ✓ ✗ ✓ FOLIO ✗ ✓ ✗ ✓ SimpleLogic ✓ ✗ ✗ ✓ ProntoQA ✗ ✓ ✗ ✗ RuleTaker ✗ ✓ ✗ ✓ LogicBench ✓ ✓ ✓ ✗ Multi-LogiEval ✓ ✓ ✓ ✓
Table 1: Comparison of Multi-LogiEval with existing datasets and benchmarks pect in-context examples of inference rules for various reasoning depths will always be available in prompts.We measure the accuracy of LLMs' predictions on the binary classification task.As illustrated in Figure 1, our experimental results indicate that LLMs performance decreases as the depth of reasoning increases, indicating mistakes in the initial reasoning step propagate further in the reasoning chain.The rationale behind the choice of binary classification task is that it provides systematic standard metric-based evaluation (i.e., direct comparison of LLMs' performance in terms of accuracy), which could be more challenging with open-ended question-answer formats.However, we also provide a manual and thorough analysis of the reasoning chain generated by LLMs revealing several findings such as the importance of contextual information, the lack of correlation between longer reasoning chains and better outcomes, and the lower performance of larger-scale open-source LLMs compared to smaller ones.</p>
<p>Related Work</p>
<p>Past attempts have been made to assess the logical reasoning ability of language models.For instance, LogiQA (Liu et al., 2021) and ReClor (Yu et al., 2020) evaluate diverse forms of logical reasoning by compiling multi-choice questions from standardized examinations, including multi-step reasoning.However, in contrast to our Multi-LogiEval, these datasets involve mixed forms of reasoning and do not focus on assessing logical reasoning independently.Past attempts have been made to create datasets focusing on logical reasoning (Luo et al., 2023).In terms of task formulation, our proposed dataset is similar to ProofWriter (Tafjord et al., 2021), RuleTaker (Clark et al., 2021), FOLIO (Han et al., 2022), ProntoQA (Saparov and He, 2023), and LogicBench (Parmar et al., 2024) which are QA datasets designed to evaluate logical reasoning ability independently.ProofWriter provides multihop proofs for each example, RuleTaker mainly covers the simple implication rules such as modus ponens, while FOLIO gives diverse and complex logical expressions and covers multi-step reasoning.However, it is only limited to FOL.ProntoQA (Saparov and He, 2023) provides a QA dataset with explanation and reasoning steps but is limited to single-step modus ponens in FOL.Although Log-icBench (Parmar et al., 2024) covers various inference rules and reasoning patterns comprehensively, it only contains single-step logical reasoning (see Table 1 for comparison).Additional datasets for evaluating multi-step logical reasoning also exist, such as SimpleLogic (Zhang et al., 2022), which only covers modus ponens inference rule, and Rule-Bert (Saeed et al., 2021) which covers only soft logical rules.In contrast, Multi-LogiEval evaluates logical reasoning independently beyond modus ponens.In addition, FLD (Formal Logic Deduction) (Morishita et al., 2023) has formal logic theorybased inference rules, and their combinations to create multi-step reasoning, but limited to PL and FOL.However, Multi-LogiEval offers a broader set of inference rules for PL and FOL, along with their meaningful combinations for multi-step reasoning, in addition to NM reasoning.
Rule Propositional Logic First-order Logic MP ((p → q) ∧ p) ⊢ q (∀x(p(x) → q(x)) ∧ p(a)) ⊢ q(a) MT ((p → q) ∧ ¬q) ⊢ ¬p (∀x(p(x) → q(x)) ∧ ¬q(a)) ⊢ ¬p(a) HS ((p → q)) ∧ (q → r)) ⊢ (p → r) (∀x((p(x) → q(x)) ∧ (q(x) → r(x))) ⊢ (p(a) → r(a)) DS ((p ∨ q) ∧ ¬p) ⊢ q (∀x(p(x) ∨ q(x)) ∧ ¬p(a)) ⊢ q(a) CD ((p → q) ∧ (r → s) ∧ (p ∨ r)) ⊢ (q ∨ s) (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (p(a) ∨ r(a))) ⊢ (q(a) ∨ s(a)) DD ((p → q) ∧ (r → s) ∧ (¬q ∨ ¬s)) ⊢ (¬p ∨ ¬r) (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (¬q(a) ∨ ¬s(a))) ⊢ (¬p(a) ∨ ¬r(a)) BD ((p → q) ∧ (r → s) ∧ (p ∨ ¬s)) ⊢ (q ∨ ¬r) (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (p(a) ∨ ¬s(a))) ⊢ (q(a) ∨ ¬r(a)) CT (p ∨ q) ⊣⊢ (q ∨ p) ∀x(p(x) ∨ q(x)) ⊣⊢ ∀x(q(x) ∨ p(x)) DMT ¬(p ∧ q) ⊣⊢ ¬p ∨ ¬q ¬∀x(p(x) ∧ q(x)) ⊣⊢ ∃x(¬p(x) ∨ ¬q(x)) CO ((p → q) ∧ (p → r) ⊢ (p → (q ∧ r) ∀x((p(x) → q(x)) ∧ (p(x) → r(x))) ⊢ ∀x(p(x) → (q(x) ∧ r(x))) IM (p → (q → r)) ⊣⊢ ((p ∧ q) → r) ∀x(p(x) → (q(x) → r(x))) ⊣⊢ ∀x((p(x) ∧ q(x)) → r(x)) MI (p → q) ⊣⊢ (¬p ∨ q) - EG - p(a) ⊢ ∃x(p(x)) UI - ∀x(p(x)) ⊢ p(a)</p>
<p>Multi-LogiEval</p>
<p>In developing Multi-LogiEval, we leverage the capabilities of LLMs while employing different methods to generate data for NM compared to PL and FOL since the formulations for PL and FOL differ from NM.In particular, our data creation process consists of two major stages: (i) Generation of rule combination and (ii) Generation of data instances.</p>
<p>Generation of rule combination</p>
<p>We create a meaningful combination of inference rules to achieve reasoning depths and define the complex question for each combination that will require multiple reasoning steps to answer.Here, each step generally corresponds to one inference rule.</p>
<p>Generation of data instances</p>
<p>Using the combinations of inference rules generated in the above step, we prompt the LLM to generate a more human-like natural language story embedded with logical rules as a context and then the following complex reasoning question.In this way, we generate data in the form of <context, question> pairs for each combination of inference rules at each depth.</p>
<p>Data Generation for Monotonic Logic</p>
<p>Here, we provide details of the data generation process for PL and FOL (further details are in Appendix A).Specifically, we delve into 14 distinct inference rules of PL and FOL, detailed in Table 2. Choice of inference rules Since entailment (concluding a formula in logic from another formula in that logic) in PL is Co-NP Complete, and entailment in FOL is undecidable.Even though we are interested in multi-step reasoning, our aim is not to build a "complete" reasoning system (the system that can make all possible entailments in that logic), rather, our goal is to make LLMs be able to at least mimic some key inference rules up to a depth of five, which itself is challenging.Thus, we start with the set of 25 inference rules used in (Parmar et al., 2024) and add eight more inference rules, resulting in 33 inference rules (with zero or one variable).For a depth of five that would mean a 33 5 possible combination, which is already quite big (&gt; 39 million).In addition, we also consider seven FOL inference rules involving three variables and binary, ternary relations (Appendix H).</p>
<p>In adding the new inference rules, our main consideration was how well they match human intuition.</p>
<p>For example, we left out p ∧ ¬p ⊢ q as that is not very intuitive to non-logician humans.Similarly, we left out inference rules such as simplification ((p ∧ q) ⊢ p), conjunction (p, q ⊢ (p ∧ q)), and addition (p ⊢ (p ∨ q)), as they would lead to infinite reasoning chains and it did not make sense to add them as an additional step of reasoning to arrive at a meaningful conclusion.Conversely, we added the DMT (¬(p ∧ q) ⊣⊢ ¬p ∨ ¬q), and show its use in multi-step, as shown in Table 9 (Appendix B).</p>
<p>Generation of Rule Combination</p>
<p>We apply sequential inference rules for multi-step reasoning, as illustrated in Figure 2. To ensure a comprehensive approach to answering a question, . . .we employ a method that involves leveraging contextual information and explicit details provided in the question.This process requires a logical chain of reasoning, combining knowledge from the given context with the information presented in the question.Each step in the reasoning chain corresponds to an inference rule, with combinations ensuring each step aligns with a single rule.To generate the combinations, we start with the initial rule and assess whether the conclusion of this rule aligns with the premise of other rules.This iterative process results in multi-step combinations/reasoning, with the conclusion of each step serving as a part of the premise for the subsequent rule.</p>
<p>We create 71 rule combinations, ranging from 2step to 5-step reasoning chains.We use each single inference rule as depth-1.Examples of rule combinations in classical logic are presented in Table 3. Let's consider a specific combination involving the Modus Tollens (((p → q) ∧ ¬q) ⊢ ¬p) and Disjunctive Syllogism (((p ∨ r) ∧ ¬p) ⊢ r) rules for creating combination for depth-2.Given the context, including natural language statements for (p → q) and (p∨r) and information in the question as ¬q, we ask about the truth value of r.Applying Modus Tollens, we deduce ¬p from the (p → q) from context and ¬q in question, giving the first step.Subsequently, using ¬p as the premise for Disjunctive Syllogism, we conclude that r is indeed true based on the (p ∨ r) and ¬p, giving the second step.Creating rule combinations at higher depths, and validating the quality of generated instances is challenging, hence, we limit the number of rule combinations at d 5 for the scope of this study.More examples provided in Appendix B.</p>
<p>Generation of Data Instances</p>
<p>We generate natural language (NL) data at different depths by prompting Claude-2 in a few-shot setting with instructions for various rule combinations.The prompt schema, shown in Figure 3, comprise five crucial components:</p>
<p>Rule Definition We provide generalized rules for various combinations containing propositions represented by labels such as P and Q.For instance, Rule 1: "If P is true, then Q is true."Utilizing these defined rules, we construct the contextual premise by combining them.Subsequently, we formulate a question that requires a step-by-step deduction using all the established rules to derive the answer.</p>
<p>Format We provide model-specific instructions for generating outputs in a designated format, simplifying the process of parsing it on a large scale.Task Definitions We provide definitions to perform two tasks.First, to generate the context that serves as a human-like illustration of generalized rules.This task instructs the generation of a reallife story with sentences exemplifying the specified rules, where entity labels such as P, Q, R, S, T, and U are replaced with actual entities.To ensure clarity, entity labels are excluded from the context.Additionally, the context generation for FOL incorporates instructions specifying the use of generalized sentences with indefinite pronouns for quantification.The second task focuses on question generation, which entails formulating questions in the format: "[(....) is true/not true, then is (....) true?]"This dual-task approach ensures the generation of <context, question> pair.We provide examples of generated NL instances in Table 4.</p>
<p>Generalized Rule Definition</p>
<p>Formatting</p>
<p>Examples We present five in-context exemplars for every rule combination.Each instance comprises propositions such as P, Q, R, a contextual narrative, and an associated question.An example prompt for depth-3 is presented in Appendix C, and we use a similar structure for all other prompts.</p>
<p>Non-Monotonic Reasoning</p>
<p>We utilize eight NM reasoning patterns defined in Lifschitz (1989) (Appendix E), and have generated data for depths 1 to 5. To increase reasoning depth, we integrated NM with classical logic, using only one NM rule per depth due to the 4-5 assumptions each pattern involves.Thus, combining two NM patterns with classical logic creates lengthy contexts, challenging for LLMs to generate quality instances.Our rule combinations avoid overly long contexts while requiring reasoning up to depth-5.</p>
<p>Generation of Rule Combination</p>
<p>We consider reasoning patterns corresponding to default reasoning for depth-1.We generalize the rule to generate simple sentence pairs independently before combining them according to the template-based NM rule.After generating sentence pairs, we combined the sentences based on the defined rule and formulated the question-answer pair accordingly.We have manually generated 12, 2, 2, and 1 rule combinations for depth-2, depth-3, depth-4, and depth-5, provided in Appendix E. While formulating depthwise rule combinations, a logical relationship between the context and question is followed.The rule combinations for all depths from 2 to 5 include 6 reasoning rules from NM-BDR, PBD, DRO, PBD, REII, and REIII-and 3 inference rules from PL-MP, MT, and DS.The data for depths 2 to 5 is generated by forming a logical connection between two NM rules' conclusions and the PL rules.</p>
<p>Generation of Data Instances</p>
<p>In creating prompts for data generation, we use a four-part structure: (1) define the task, (2) explain each rule as an assumption and conclusion, (3) provide instructions for creating context and questions to ensure logical connections, and (4) establish formatting guidelines for systematic output.Appendix E shows an example of the prompt.</p>
<p>Qualitative Analysis</p>
<p>After data generation, we conducted a manual qualitative analysis, resulting in 1,552 high-quality samples for Multi-LogicEval.</p>
<p>Logic</p>
<p>Reasoning Statistics Multi-LogicEval has 5 different logical reasoning depths.Table 5 shows the depth-wise statistics of samples present for each logic type after validation.After manual validation, from the generated data, we selected/updated high-quality 10 data instances for each inference rule in depth 1 and 15 or 20 data instances for each rule combination, which resulted in 410, 442, 310, 280, and 110 samples for depth-1, depth-2, depth-3, depth-4, and depth-5, respectively.For evaluation, of the total 1552 samples, 1126 samples have the answer yes, and the remaining 426 samples have the answer no.</p>
<p>Quality of Data Instances</p>
<p>We examine each context for potential discrepancies throughout the data generation phase, ensuring they are logically correct and represent the intended logical relations.</p>
<p>We also dedicated considerable effort to eliminating typos and validating the grammar.While validating, we encountered a few errors within the synthetically generated story-based context.We manually mitigate these errors to ensure integrity and utility (Analysis presented in Appendix F).</p>
<p>Results and Analysis</p>
<p>Experimental Setup</p>
<p>Task Formulation We formulate a binary classification task using Multi-LogiEval.Let us consider a set of data instances I D,L corresponding to depth D and logic type L. In this set, i th instance is represented as I i D,L = {(c i , q i )} where c i represents context and q i represents question corresponding to i th instance.Here, each context and question pair is created so that the conclusion provided in the question always entails context.However, you require different reasoning steps to conclude.We prompt the model to assign a label Yes if the conclusion logically entails the context; otherwise, No.</p>
<p>To evaluate any LLMs, we provide &lt; p, c, q &gt; as input to predict a label Yes or No where p is a natural language prompt.</p>
<p>Random Baseline</p>
<p>We calculated a random baseline for each depth from Multi-LogiEval as below:
Acc random = p 2 yes + p 2 no ,
where p yes and p no represent the probabilities of predicting "yes" and "no," respectively.The random baselines for depths d 1 , d 2 , d 3 , d 4 , and d 5 , with corresponding Acc random of 86.63%, 67.35%, 53.71%, 58.33%, and 83.33%, respectively.From Table 6, we can observe that these models perform lower in terms of average accuracy compared to the random baseline.</p>
<p>Findings Table 6 reveal that open-source models experience a significant performance drop from d 4 to d 5 .Also, there is an increasing performance trend in NM.For PL and FOL, GPT-4, ChatGPT, and Gemini show improved performance from d 2 to d 3 , whereas the performance of open-source models consistently decreases.In addition, larger open-source models demonstrate decreasing performance.Furthermore, ChatGPT performs lower than GPT-4 and Gemini at d 5 in PL and FOL.Also, FOL performance is lower compared to PL at d 5 .</p>
<p>Analysis and Discussion</p>
<p>In this section, we manually analyze the generated reasoning chains2 by different LLMs and investigate the above-mentioned findings in detail.</p>
<p>Performance Improvement from d 2 to d 3 in PL and FOL for GPT-4, ChatGPT, and Gemini GPT-4, ChatGPT, and Gemini excel at d 3 for PL, with a performance decrease at d 4 and d 5 .This trend is also observed in FOL for the same models except ChatGPT.Systematic analysis of all the reasoning chains with wrong predictions for PL and FOL shows these models reach incorrect conclusions often due to the wrong interpretation of evidence.In d 3 , increasing context length improves LLMs accuracy in information mapping, thus achieving peak performance (comparison with d 2 to d 5 ).At d 2 , around ∼ 27.4% of reasoning chains with incorrect conclusions were due to the models' failure to correctly map information, either from context to conclusion or the premise from one step to the next step.This number drops to ∼ 22% at d 3 and we observed that a larger context length at d 3 helps in reducing this problem.However, at d 4 and d 5 , the length of the reasoning chain increases further.Since longer reasoning steps are more prone to error propagation at later stages, causing the models to deviate further from the true conclusion, hence, lower performance at d 4 and d 5 .</p>
<p>Lower Performance of ChatGPT compared to GPT-4 and Gemini at Higher Depths This pattern is particularly evident in FOL and PL at d 5 for ChatGPT compared to Gemini, and GPT-4.At d5, manual analysis shows that ChatGPT tends to generate longer reasoning chains compared to Gemini, and GPT-4 when answering question.For PL and FOL, the average reasoning chain length for ChatGPT at d 5 is 13.85, while for Gemini and GPT-4 at d 5 is 8.85 and 10.87, respectively.Longer reasoning chains do not necessarily correlate with better reasoning outcomes, highlighting the complexity of complex reasoning task.This suggests that optimizing reasoning chain length is crucial for improving model accuracy in complex scenarios.</p>
<p>Increasing Performance Trend in NM In our analysis of ChatGPT and the open-source model Yi-34B, we've observed consistent performance improvements with increasing depth in NM reasoning.This trend diverges from classical logic PL and FOL.Specifically, at depths d 2 to d 5 , NM exhibits novel performance due to unique rule combinations in reasoning patterns.For instance, at d 2 , NM combines one PL rule with one NM reasoning pattern, progressing to two PL rules with one NM pattern at d 3 , and so forth.The addition of NM reasoning patterns complements PL and FOL by providing supplementary evidence and improving contextual understanding.Notably, as depth increases, integrating basic classical rules with NM significantly enhances model accuracy, particularly evident at depths 4 and 5.This integration is pivotal for the notable performance gains observed in NM compared to classical logic at higher depths.</p>
<p>Larger Open-Source Models Show Decreased Performance Compared to Smaller Models Here, we examine Mistral-7B, Orca-13B, and Yi-34B, which differ significantly in parameter size.Mistral-7B, the smallest, performed best across various depths of classical logic, except at the simplest d 1 .As reasoning depth increased, Mistral-7B consistently outperformed Orca-13B and Yi-34B, with Yi-34B only marginally better (1.5%) at d 3 .For NM tasks, Mistral-7B and Yi-34B showed similar performance across all depths.At the most challenging depth (d 5 ) for both PL and FOL, Mistral-7B outperformed Orca-13B by achieving 3x performance despite Orca-13B's larger size.We believe that this capability of Mistral-7B is attributed to its architecture and training, enhancing its reasoning abilities, as discussed in Jiang et al. (2023).In particular, the training of Mistral-7B focused on enhancing its reasoning capabilities.</p>
<p>Lower Average Performance in FOL than PL at d 1 to d 5 Upon observing the reasoning chains with wrong final predictions for the FOL and PL, we find that the generic rules in FOL contexts lead to deviations from the correct reasoning path.In some cases, it assigns predicates incorrectly to the FOL inference rule.This pattern is more prominent at d 5 , highlighting the large gap (∼ 10%) in average performance between PL and FOL.</p>
<p>Lower Performance in d 1 of NM Reviewing the reasoning chains, we noted that models struggled to accurately map information.Interpreting various assumptions is crucial for effective reasoning at d 1 .However, we observed that models have difficulty concluding based solely on assumptions present in the context when explicit knowledge is absent.</p>
<p>Preliminary Discussion on Multi-variable FOL Since our work focuses on evaluating LLMs' multistep reasoning with simple FOL inference rules, we conducted only a preliminary study on their reasoning abilities for multi-variable FOL rules, discussed in Appendix H.This study reveals that creating natural language instances is challenging for this kind of setup.</p>
<p>Case Study on Evaluating Multi-LogiEval using Neural Symbolic Motivated by Olausson et al. (2023), we evaluate GPT-4 using the neurosymbolic approach where we utilized the Prover9 3 .It first converts FOL statements into conjunctive normal form (CNF) and then performs resolution.Thus, we only evaluated data samples with FOL from Multi-LogiEval.For this study, we adapt the evaluation approach presented in Pan et al. (2023) where GPT-4 is used to convert the context and question in a formal executable program and use Prover9 to solve it.We used an implementation with a similar GPT-4 version compatible with Pan et al. (2023).We use the below prompt to convert natural language to an executable logic program:</p>
<p>Given a problem description and a question.The task is to parse the problem and the question into first-order logic formulas.The grammar of the first-order logic formula is defined as follows :</p>
<ol>
<li>logical conjunction: expr1 ∧ expr2 2. logical disjunction: expr1 ∨ expr2 3. logical exclusive disjunction: expr1 ⊕ expr2 4. logical negation: ¬expr1 5. expr1 implies expr2: expr1 → expr2 6.</li>
</ol>
<p>expr1 if and only if expr2: expr1 ↔ expr2 7. logical universal quantification: ∀x 8. logical existential quantification: ∃x Output format: <logic form ::: description> We evaluate the performance in terms of the accuracy of selecting the correct answer.We also 3 https://www.cs.unm.edu/~mccune/prover9/report the "Executable Rate", which reflects the grammar correctness of the logical form, and the "Executable Accuracy" of the executable samples to measure the semantic correctness (Pan et al., 2023).As the reasoning depth increases from d 1 to d 4 (Table 7), there is a general trend of decreasing overall accuracy and executable accuracy, indicating that higher reasoning depth poses more challenges for executing code correctly.Interestingly, at d 5 , both overall accuracy and executable accuracy show a significant improvement.</p>
<p>Human Evaluation and Further Discussion</p>
<p>In this study, we performed a human evaluation on a selected subset of the Multi-LogiEval.Additionally, we explored potential strategies for enhancing LLMs' reasoning capabilities based on the findings of our current analysis.Please refer to Appendix I for further details on the human evaluation process and discussion.</p>
<p>Conclusions</p>
<p>In this work, we introduced Multi-LogiEval, a comprehensive multi-step logical reasoning benchmark consisting of three types of logic and over 60 combinations of inference rules.Our approach utilized two-stage methodology to construct data instances for our benchmark consisting of ∼ 1.6k data instances with 1 ∼ 5 reasoning depth.We evaluated a range of LLMs, including GPT-4, ChatGPT, Gemini, Yi, Orca, and Mistral on Multi-LogiEval.Experimental results revealed that these models struggle to perform logical reasoning, and their performance drops as the depth of logical reasoning increases (average accuracy of ∼ 68% at d 1 to ∼ 43% at d 5 ) for classical and non-classical logic.Furthermore, we systematically analyzed the reasoning chain generated by LLMs at various depths and presented interesting findings.We hope that Multi-LogiEval will facilitate future research in evaluating and enhancing the ability of existing and upcoming LLMs for multi-step logical reasoning.</p>
<p>Limitations</p>
<p>Though Multi-LogiEval facilitates the evaluation of the multi-step logical reasoning ability of LLMs, the complexity of reasoning depth presented in Multi-LogiEval can be improved by adding reasoning depth beyond five steps.Multi-LogiEval can be further extended by incorporating other inference rules and logic types, for instance, the inference rules in first-order logic that capture n-ary relations between multiple variables.We also note that this research is limited to the English language and can be extended to multilingual scenarios for evaluating the logical reasoning ability of LLMs.</p>
<p>Ethics Statement</p>
<p>We have used AI assistants (Grammarly and ChatGPT) to address the grammatical errors and rephrase the sentences.</p>
<p>A Monotonic Logic Description</p>
<p>Propositional Logic (PL) PL serves as a foundational framework for reasoning about truth values of statements, represented as propositions denoted by symbols like p, q, r, etc. Employing logical connectives such as '∧' (conjunction), '∨' (disjunction), and '→' (implication), it establishes relationships between these propositions.PL incorporates various inference rules, guiding the derivation of conclusions from given propositions.For instance, Modus Ponens is an example of such inference rules where if presented with the premises ((p → q) ∧ p)-interpreted as "if p, then q, and p is true"-we can deduce the truth of q, denoted as ((p → q) ∧ p) ⊢ q.</p>
<p>First-order Logic (FOL) FOL builds upon the foundations of PL by introducing predicates and quantifiers.Predicates allow us to express relationships involving variables, and quantifiers such as the universal (∀) and existential (∃) quantifiers enable us to make statements about all or some elements in a domain.For instance, instead of stating "John is a student," we can express it in FOL as "There exists x such that x is John and x is a student."This logic extends the rules of PL, such as the Modus Ponens rule, which lets us infer conclusions for specific instances from general premises.</p>
<p>B Combinations of rules for Monotonic Logic</p>
<p>We created 27 multi-step reasoning inference rule combinations for Propositional Logic (PL), with depths ranging from 2 to 5. We use the same rule combinations for First Order Logic (FOL) for each depth.All rule combinations for 2-step, 3-step, 4-step, and 5-step reasoning for PL and FOL are presented in Tables 8, 9, 10, and 11 respectively.</p>
<p>For each combination, we provide the inference rules to be used for reasoning, the premises present in the context and in the question, and the complex reasoning question-answer pair.</p>
<p>C Example of Prompt</p>
<p>Figure 4 illustrates an example prompt for combination of rules from propositional logic, namely 'constructive dilemma' (CD), 'disjunctive syllogism' (DS), and 'modus ponens' (MP).CD is represented as (p → q) ∧ (r → s) ∧ (p ∨ r)) ⊢ (q ∨ s), which can be understood in natural language as "If p implies q, and if r implies s, and either p or r or</p>
<p>Formatting Instruction:</p>
<p>Complete the following tasks, only returning text in exactly the format given in the following examples.</p>
<p>Diversity Instruction:</p>
<p>Generate 5 more examples from multiple domains</p>
<p>Task Definition:</p>
<p>Task 1: Generate a short real life story that includes sentences to illustrate the above rules, replacing the entities P, Q, R, S, T with real values.Do not include the entity labels like P, Q, R, S, T in the story.Task 2: Generate the following complex reasoning question using the story and the rules, by replacing the respective entities.Q1: [If Q is not true, then is T true?] both are true, then we can conclude that either q or s or both are true."DS is formally represented as (p ∨ q) ∧ ¬p) ⊢ q, which can be understood in natural language as "If p or q are true, and we know ¬p, then we can conclude q." MP is formally represented as (p → q) ∧ p) ⊢ q, which can be understood in natural language as "If p implies q, and we know p, then we can conclude q."In this prompt, the generalized rule definitions provide a description of the premises given in the story in natural language.The prompt includes instructions on how the generated samples should be formatted, instructions to generate samples from diverse domains, and detailed definitions for generating propositions, and then using them to generate a context and question for each sample.To enhance the quality of samples in terms of relevance and coherence, the prompt includes an examples section that demonstrates these tasks.In Figure 4, we present three examples with their respective propositions, contexts, and questions.</p>
<p>D NL Examples for PL and FOL</p>
<p>In this section, we illustrate multi-step reasoning for PL and FOL using natural language examples for depths 2 through 5. Table 12 provides examples in natural language for PL.We provide one example of rule combinations for each depth.For each example, we provide the inference rules and propositions, as well as the respective context and complex reasoning question.Table 13 provides examples in natural language for FOL, with one combination for each depth.Similar to PL, we provide the inference rules, predicates, and the contextquestion pair for each example.(Lifschitz, 1989), specifically chosen for depth-1 non-monotonic logic.Out of the 11 default non-classical reasoning rules mentioned in the paper, we opted for 8.These include Default Reasoning with Several Defaults (DRS), Default Reasoning with Irrelevant Information (DRI), Default Reasoning with a Disabled Default (DRD), Default Reasoning in an Open Domain (DRO), Reasoning about Unknown Expectations I (RE1), Reasoning about Unknown Expectations II (RE2), Reasoning about Unknown Expectations III (RE3), and Reasoning about Priorities (RAP).These rules constitute our selection for depth-1 non-monotonic logical reasoning.Moving on to depths 2 through 5, we integrated classical and non-classical logic.Tables 16, 17, 18, and 19 outline the combinations of rules prepared respectively for depth-2, depth-3, depth-4, and depth-5 logical reasoning tasks.In this context, we combined BDR, DRD, PBD, DRO, REII, and REIII from non-monotonic logic with MP, MT, and DS from propositional logic to form combinations for depths 2 to 5 of data.Tables 20, 21, 22, and 23 show the prompts that we used to generate data instances respectively for depths 2, 3, 4, and 5.The instruction-based data generation can be seen in Tables 20,21,22,and 23.In addition to instructionbased generation, one-shot prompts were used for depth-3, depth-4, and depth-5 data generation as seen in Tables 21, 22, and 23.</p>
<p>E More Details on NM</p>
<p>F Validation of Data Instances</p>
<p>We involved four evaluators (who are also authors of this paper) for data validation.Each evaluator holds a graduate degree in computer science and has knowledge of logical reasoning.As discussed in Section 3.3, each sample is evaluated by one evaluator to ensure its logical correctness.We categorized errors into three distinct groups.The categories of errors identified are (i) Incorrect Logical Premises (ILP) which indicates that premises generated by the model in the context are logically incorrect (i.e., did not align with the intended conclusion), (ii) Leaking Conclusion (LC) where the context inadvertently revealed the conclusion, bypassing the need for the logical deduction, and (iii) Repetition of Samples (RS) where identical or nearly identical contexts are present, reducing dataset diversity.We found ∼ 14.3% (223 samples) of the total 1552 samples with ILP, ∼ 3.7% (57 samples) with LC, and ∼ 3.7% (57 samples) with RS.We mitigated all these errors manually from the generated data instances to provide a highquality evaluation set.Furthermore, we also analyzed the number of samples we corrected for PL (∼ 22% -115/525), FOL (∼ 19% -102/535), and NM (∼ 25.9% -127/492), highlighting the difficulty of generating instances for specific logics.Similarly, we also analyzed depth-wise instance correction where we corrected ∼ 17.8% (73/410), ∼ 21% (93/442), ∼ 23.5% (73/310), ∼ 33.2% (93/280), and ∼ 21% (23/110) for the depth d 1 , d 2 , d 3 , d 4 , and d 5 , respectively, indicating the challenges of generating and validating multi-step reasoning context with increasing depth.</p>
<p>G Few shot evaluation Multi-LogiEval</p>
<p>We evaluate models in a few-shot setting (specifically, 3-shot) on Multi-LogiEval, revealing a notable enhancement in performance, as depicted in Table 24.In the 3-shot evaluation results, we observe notable improvements in the performance of various LLMs.GPT-4 consistently exhibits high accuracy across all depths, particularly excelling in PL and FOL.Though showing significant enhancements compared to its zero-shot performance across all the models, they still underperform in NM, highlighting a persistent challenge in this area.Open-source models such as Yi-34B and Mistral-7B, while benefiting from the 3-shot setup, still display noticeable performance drops in higher depths.Comparing these findings to the zero-shot results from Table 6, we see a general trend of improved performance in the 3-shot setting, indicating the effectiveness of few-shot prompting.However, the observed performance drop from d 4 to d 5 in open-source models comparable across both settings, suggesting that while few-shot examples enhance overall accuracy, they do not fully mitigate the inherent challenges these models face in higher depths.Moreover, the performance trends identified in the zero-shot evaluation, such as the consistent decrease in accuracy for larger opensource models and the superior performance of proprietary models such as GPT-4 and ChatGPT in PL and FOL, remain similar in the 3-shot setting.</p>
<p>H Extended first-order logic with n-ary relations</p>
<p>First-order logic often involves handling n-ary relations involving more than two variables-such as the ternary relation in "If P (a, b, c) ∧ Q(c, d) then R(a, d)".Moreover, one can alternate for all (∀), and there exists (∃) for any number of times in FOL, and that means there are an infinite number of such rules in first-order logic.As discussed in section 3.1, our aim is not to build a comprehensive set covering all the possible inference rules but rather to evaluate the reasoning ability of language models up to a reasoning depth of five on a systematically curated set of inference rules.However, to evaluate the ability of LLMs to reason with such complex rules, we explore 7 such inference rules for which we generated data using a similar prompt structure as depicted in Figure 3.We generate 10 instances for each of the inference rule, resulting in 70 instances for evaluation.The choice of inference rules can be found in Table 14.We evaluate the large-scale models GPT-4, ChatGPT, and Gemini.These models achieve an average accuracy of 80%, 84.3%, and 90%, respectively.This demonstrates that these LLMs can comprehend multi-variable FOL, but the rules currently involve only singlestep reasoning.Our work also shows that these models perform well with single-step reasoning.</p>
<p>Exploring multi-step reasoning with multi-variable FOL presents an interesting direction for future research direction.</p>
<p>Context: Someone wanted to become an artist.They learned that if they practiced drawing techniques consistently, they would improve their artistic skills.With improved artistic skills, they could sell their artworks.Either someone practices drawing techniques consistently, or someone studies art history and famous artists, or they do both.If someone studies art history and famous artists, then they gain inspiration for their own art.If they gain inspiration, then they can develop their own creative style.</p>
<p>Question: If Emma cannot sell her artworks yet, then has she developed her own creative style?</p>
<p>Table 13: Natural language examples of rule combinations of each depth for FOL.</p>
<p>I Human Evaluation and Discussion</p>
<p>We have conducted a human evaluation on a subset of Multi-LogiEval.Specifically, we selected 15 unique instances covering all 5 depths (5 instances for each logic type) from Multi-LogiEval.This selection resulted in a total instances of 75 <context, question> pairs.We hired three graduate student volunteers to provide the evaluations.Task instructions provided to all three annotators are similar to prompts provided to LLMs.Each instance pair is answered/annotated by three different annotators with 0.853 inter-annotator agreement (measured with raw/observed agreement).Here are the results for three logic types averaged across three annotators for each depth.(Rafailov et al., 2024), and KTO (Ethayarajh et al., 2024) for their suitability in improving logical reasoning also can be an interesting future direction.Task 1: Generate a short generic story that should only contain the natural language sentences for assumptions 1, 2, 3, and 4 using propositions to replace the labels A, B and so on.The story should not include labels like p or q and so on.</p>
<p>Task 2: Generate the question by replacing them with the entities with respective propositions.Task 1: Generate a short context paragraph by replacing all the entity labels A, B, and so on in the above context with propositions and real entities.The generated context should have natural language sentences for all the sentences 1-4.It should not include label representations like A or B and should not mention the words "property".</p>
<p>Task 2: Generate questions 1 and 2 by replacing the respective labels from the generated context.</p>
<p>Example 1: Assumptions: Apple tree and Orange tree are fruit trees.Normally, fruit trees produce edible fruit.If Garden is regularly watered, then its plants are flourishing.If Orchard receives enough sunlight, then it yields high-quality fruit.Either Garden has regular watering or Orchard yields high-quality fruit or both.</p>
<p>If the apple tree produces edible fruit, then Orchard receives enough sunlight.</p>
<p>Question 1: Can we conclude if Garden does not have flourishing plants then the orange tree produces edible fruit?</p>
<p>Question 2: Can we conclude if Garden does not have flourishing plants then the orange tree does not produce edible fruit?</p>
<p>GPT</p>
<p>Figure 1 :
1
Figure 1: Performance (avg.accuracy across each depth for PL &amp; FOL) of various LLMs on Multi-LogiEval.</p>
<p>Figure 2 :
2
Figure 2: Process for combining multiple logical inference rules for PL and FOL: Premise 1 is the set of premises for the first inference rule, leading to Conclusion 1.Conclusion 1 and Premise 2 derive Conclusion 2, and so on.⊢: Entails.</p>
<p>Figure 3 :
3
Figure 3: Data generation prompt for PL and FOL</p>
<p>Figure 4 :
4
Figure 4: An example prompt for 3-step combination of inference rules CD, DS, and MP from propositional logic.</p>
<p>p(x) ∧ q(x)) → r(x, y)) ∧ ∃u∃v(p(u) ∧ ¬r(u, v)) ⊢ ∃y¬q(y)2 ∀x∀y((p(x) ∧ q(x)) → ¬s(x, y)) ∧ ∀z(r(z) → p(z)) ∧ r(a) ∧ s(a, b) ⊢ ¬q(b) 3 ∀x∃y((p(x) → q(x, y)) ∧ ∀u∀v((q(u, v) ∧ r(u, v)) → s(v)) ∧ ∃z∃k(p(z) ∧ r(z, k)) ⊢ ∃ws(w)4 ∀x∀y∀z(p(x, y, z) → (q(x, z) ∨ r(y))) ∧ ∃u∃v∃w(p(u, v, w) ∧ ¬q(u, w)) ⊢ ∃sr(s) 5 ∀x((p(x) → ∃yr(y, x)) ∧ p(a) ⊢ ∃zr(z, a) 6 ∀x∀y(p(x, y) ∨ q(x, y)) ∧ ∃u∃v¬q(u, v) ⊢ ∃z∃wp(z, w) 7 ∀x∀y(p(x, y) → (q(x) ∧ r(y)) ∧ p(a, b) ⊢ q(a) ∧ r(b))</p>
<p>Table 2 :
2
Parmar et al. (2024) establish the relationship between premises and their corresponding conclusions.A subset of these inference rules is adapted fromParmar et al. (2024).MP: Modus Ponens, MT: Modus Tollens, HS: Hypothetical Syllogism, DS: Disjunctive Syllogism, CD: Constructive Dilemma, DD: Destructive Dilemma, BD: Bidirectional Dilemma, CT: Commutation, DMT: De Morgan's Theorem, CO: Composition, IM: Importation, MI: Material Implication, EG: Existential Generalization, UI: Universal Instantiation</p>
<p>Table 3 :
3
Examples of multi-step reasoning rule combinations for PL.Similar combinations are used for FOL.</p>
<p>Table 4 :
4
NL examples of different rule combinations for all three logic types.Appendix D provides more examples.</p>
<p>Table 5 :
5
Statistics of Multi-LogiEval
DepthTotal12345PL120 105 135 120 45525FOL130 105 135 120 45535NM160 232 404020492Total 410 442 310 280 110 1552</p>
<p>Table 6 :
6
.52 82.22 71.67 66.67 83.85 70.48 71.85 59.17 66.67 36.88 51.67 65.00 67.50 60.00  ChatGPT 91.67 56.19 63.70 62.50 44.44 97.69 59.05 57.78 50.83 37.78 33.75 41.1150.0062.5060.00Gemini 90.00 62.86 68.15 65.83 60.00 76.92 62.86 65.93 57.50 53.33 46.25 46.11 62.50 55.00 60.00 Yi-34B 85.00 65.71 58.52 46.67 26.67 90.00 55.24 57.94 48.33 13.33 37.50 41.11 55.00 62.50 65.00 OrcaEvaluation of LLMs in terms of accuracy on Multi-LogiEval.
ModelsPropositionalFirst-OrderNon-Monotonicd 1d 2d 3d 4d 5d 1d 2d 3d 4d 5d 1d 2d 3d 4d 5GPT-4 89.17 69Avg 85.42 60.79 61.61 55.83 42.96 83.21 59.84 58.79 51.39 32.96 35.63 41.02 51.67 51.67 55.83Experiments We evaluate a range of proprietarymodels (i.e., GPT-4, ChatGPT, and Gemini-Pro)and open-source models (i.e., Yi-34B-Chat, Orca-2-13B, and Mistral-7B-Instruct) on Multi-LogiEval.The evaluation is conducted on the versions of Ope-nAI and Google models released in April 2024.
Each model is evaluated in a zero-shot-CoT setting.The prompt used for experiments is provided below.We evaluate LLMs in a zero-shot setting to show the logical reasoning ability of the model based on knowledge acquired during pre-training since we can not expect in-context examples corresponding to different reasoning patterns and depths during inference.However, we also evaluate LLMs in a 3-shot setting (results are in Appendix G).Metrics Since the objective is to assess the model's ability to arrive at the correct conclusion, we measure the accuracy associated with a Yes and No label.Apart from accuracy, we provide an indepth analysis of reasoning chains in section 4.3 to gain insights into models' performance.In addition, we would like to mention that the binary labels Yes and No indicate whether the conclusion presented in the question can be derived from the context.Hence, accuracy is an important evaluation metric, reflecting the model's reasoning ability. 1 to 66.67%, 44.44%, and 60.00% at d 5 for PL, respectively, indicating the challenge encountered even by larger-scale LLMs when handling longer chains of logical reasoning.In summary, for PL and FOL, LLMs perform well on d 1 compared to other depths.While they show competitive performance for d 2 and d 3 , there is a significant drop in performance for d 4 and d 5 in most cases.In contrast, moving on to NM, going from d 1 to d 5 , there is an increase in the performance of LLMs from an average of 35.63% to 55.83%.</p>
<p>Table 7 :
7
Performance Metrics by Reasoning Depths
ReasoningOverall AccExecutable RateExecutable AccDepth(%)(%)(%)d 155.8385.8351.46d 246.6776.6747.83d 338.8977.7830.00d 440.0077.7841.43d 560.0077.7862.86</p>
<p>Rule 1: [if {P} is true then {Q} is true, and if {R} is true then {S} is true, and either {P} or {R} or both are true] Rule 2: [if {S} is true, then {T} is true]
Generalized Rule Definition:</p>
<p>Table 15 displays instances of general rules discussed in the paper by Lifschitz</p>
<p>Table 12 :
12
Natural language examples of rule combinations of each depth for PL.</p>
<p>Table 14 :
14
FOL inference rules that establish the relationship between multiple variables John said the shirt was blue.Mary said the shirt was not blue.Normally people are correct when they make assertions.John had a closer look at the shirt than Mary.If the shirt was purple, it could not be blue.Ben, Mark, and Jacob took a history test.Students who study many hours usually pass history tests.Ben and Mark did not study many hours.If Jacob passed the history test, he must have paid attention in class.John, Peter and Kate are students in math class.Students in math class normally do homework.John and Peter did not do their math homework.If Kate missed class then she did not do her math homework.Kate booked a room at hotel Y. Rooms at hotel Y are usually clean.There is at least one room at hotel Y that is not clean.If Kate's room has mold, then it is probably not clean.
Question: Can we conclude the dog made a mess? (Yes)PBD_MT Conclusion of PBD: X MT: (X → Y) ∧ ¬Y ⊢ ¬XContext: Question: Can we conclude the shirt was purple? (No)REI_MP Conclusion of REI: X MP: (X → Y) ∧ X ⊢ YContext: Question: Can we conclude Jacob paid attention in class? (Yes)REI_MT Conclusion of REI: X MT: (X → Y) ∧ ¬Y ⊢ ¬XContext: Question: Can we conclude Kate missed class? (No)REII_MPContext: John bought a new phone. New phones usually come with a warranty. However, some new phones do not come with a warranty. If a phone has a warranty, then it hasConclusion of REII: Xcustomer support.MP: (X → Y) ∧ X ⊢ YQuestion: Can we conclude John's new phone has customer support? (Yes)REII_MTContext:Conclusion of REII: XMT: (X → Y) ∧ ¬Y ⊢ ¬X
Question: Can we conclude Kate's room has mold?(No)</p>
<p>Table 16 :
16
Natural language examples of rule combinations of depth-2 for NM.Context: Smartphone A and Smartphone B both have GPS technology.Normally, smartphones with GPS technology also have internet connectivity.If smartphone A has internet connectivity, then Mike can access online maps.If Mike can access online maps, then Emily can get driving directions from Mike.Context: Car A and car B are electric vehicles.Normally, electric vehicles (cars) have fast-charging capabilities.If car C is a hybrid, then car C has good fuel efficiency.If car A has a fast-charging capability, then it implies that the environment is very eco-friendly.Either car C is a hybrid or the environment is not very eco-friendly, or both.
RuleExamplesRule: d3_1Assumptions:1: A and B are objects of type T and have property S.2: Normally objects of type T with property S have property U.3: if A has property U implies C has property D4: if C has property D implies E has property FQuestion 1: Can we conclude if E does not have F then B has U? (YES) Question 2: Can we conclude if E does not have F then B does not have U?Question 1: Can we conclude if Emily can not get driving di-rections from Mike, then smartphone B has internet connectivity? (Yes) Question 2: Can we conclude if Emily can not get driving directions from Mike, then smartphone B does not have internet connectivity? (No)(NO)Rule: d3_2Assumptions:1: A and B are objects of type T and have property S.2: Normally objects of type T with property S have property U.3: if C has property G implies C has property D4: if A has property U implies E has property F5: either C has property G or E does not have property F or bothQuestion 1: Can we conclude if car C is not a good fuel effi-Question 1:cient then Car B has a fast-charging capability? (Yes)Can we conclude if C does not have D then B has U? (YES)Question 2: Can we conclude if car C is not a good fuel efficient thenQuestion 2:Car B does not have a fast-charging capability? (No)Can we conclude if C does not have D then B does not have U?(NO)</p>
<p>Table 17 :
17
Natural language examples of rule combinations of depth-3 for NM.Context: Assume A and B are plants of species T and they both produce flowers.Normally, flowering plants of species T also bear fruit.If an animal C is a bird, then it can fly.If an environment has a lot of sunlight, then it supports plant growth.Either the bird cannot fly or the environment does not support plant growth or both.If plant A bears fruit, then the environment has a lot of sunlight.
RuleExamplesRule: d4_1Assumptions:1: A and B are objects of type T and have property S.Context: Apple tree and Orange tree are fruit trees. Normally, fruit2: Normally objects of type T with property S have property U.trees produce edible fruit. If Garden is regularly watered, then its plants3: if C has property G implies C has property Dare flourishing. If Orchard receives enough sunlight, then it yields4: if E has property L implies E has property Fhigh-quality fruit. Either Garden has regular watering or Orchard yields5: either C has property G or E has property F or bothhigh-quality fruit or both. If the apple tree produces edible fruit, then6: if A has property U then E has property LOrchard receives enough sunlight.Question 1: Can we conclude if Garden does not have flourish-Question 1:ing plants then the orange tree produces edible fruit? (Yes)Can we conclude if C does not have D then B has U? (YES)Question 2: Can we conclude if Garden does not have flourishing plantsQuestion 2:then the orange tree does not produce edible fruit? (No)Can we conclude if C does not have D then B does not have U?(NO)Rule: d4_2Assumptions:1: A and B are objects of type T and have property S.2: Normally objects of type T with property S have property U.3: if C has property G implies C has property D4: if E has property L implies E has property F5: either C does not have property D or E does not have propertyF or both6: if A has property U then E has property LQuestion 1:Can we conclude if the bird is capable of flying then plant B bears fruit?Question 1:(Yes)Can we conclude if C has property G then B has U? (YES)Question 2:Question 2:Can we conclude if C does not have G then B does not have U?(NO)
Can we conclude if the bird can fly then plant B does not bear fruit?(No)</p>
<p>Table 18 :
18
Natural language examples of rule combinations of depth-4 for NM. Rose and Lily are plants that flower.Normally, plants that flower also produce seeds.If a plant is a Cactus, and it has thorns, then it can survive in the desert.If a plant is an Orchid, and it has broad leaves, then it can grow in tropical areas.Either a Cactus has thorns, or an Orchid can grow in tropical areas, or both.If a Lotus has flowers, then an Orchid has broad leaves.If a Rose produces seeds, then a Lotus has flowers.
RuleExamplesRule: d5_1Assumptions: 1: A and B are objects of type T and have property S. 2: Normally objects of type T with property S have property U. 3: if C has property G implies C has property D 4: if E has property L implies E has property F 5: either C has property G or E has property F or both 6: if I has property H then E has property L 7: if A has property U then I has property HContext: Question 1:Can we conclude if a Cactus cannot survive in the desert then a LilyQuestion 1:produces seeds? (YES)Can we conclude if C does not have D then B has U? (YES)Question 2:Question 2:Can we conclude if a Cactus cannot survive in the desert then a Lily doesCan we conclude if C does not have D then B does not have U?not produce seeds? (NO)(NO)</p>
<p>Table 19 :
19
Natural language examples of rule combinations of depth-5 for NM.
Rule:Assumptions:1: A and B are objects of type T and have property P.2: Normally objects of type T with property P have property Q.3: A does not have property Q.4: If B has property Q then it implies B has property C.
Question: Can we conclude B has property C?</p>
<p>Table 20 :
20
An example of prompt used to generate data instance for depth-2 using NM-BDR and PL-MP Generate a short context paragraph by replacing all the entity labels A, B, and so on in the above context with propositions and real entities.The generated context should have natural language sentences for all the sentences 1-4.It should not include label representations like A or B and should not mention the words "property".Generate questions 1 and 2 by replacing the respective labels from the generated context.Smartphone A and Smartphone B both have GPS technology.Normally, smartphones with GPS technology also have internet connectivity.If smartphone A has internet connectivity, then Mike can access online maps.If Mike can access online maps, then Emily can get driving directions from Mike.Can we conclude if Emily can not get driving directions from Mike, then smartphone B has internet connectivity?Can we conclude if Emily can not get driving directions from Mike, then smartphone B does not have internet connectivity?
Rule: d3_1Assumptions:1: A and B are objects of type T and have property S.2: Normally objects of type T with property S have property U.3: if A has property U implies C has property D4: if C has property D implies E has property FQuestion 1:Can we conclude if E does not have F then B has U? (YES)Question 2:Can we conclude if E does not have F then B does not have U? (NO)Task 1: Task 2: Example 1:Assumptions:Question 1:Question 2:</p>
<p>Table 21 :
21
An example of prompt used to generate data instance for depth-3 for NM and B are objects of type T and have property S. 2: Normally objects of type T with property S have property U. 3: if C has property G implies C has property D 4: if E has property L implies E has property F 5: either C has property G or E has property F or both 6: if A has property U then E has property L Question 1: Can we conclude if C does not have D then B has U? (YES) Can we conclude if C does not have D then B does not have U? (NO)
Rule: d4_1Assumptions:1: A Question 2:</p>
<p>Table 22 :
22
An example of prompt used to generate data instance for depth-4 for NM Can we conclude if C does not have D then B has U? (YES) Can we conclude if C does not have D then B does not have U? (NO)Task 1: Generate a short context paragraph by replacing all the entity labels A, B, and so on in the above context with propositions and real entities.The generated context should have natural language sentences for all the sentences 1-4.It should not include label representations like A or B and should not mention the words "property".Task 2: Generate questions 1 and 2 by replacing the respective labels from the generated context.Rose and Lily are plants that flower.Normally, plants that flower also produce seeds.If a plant is a Cactus, and it has thorns, then it can survive in the desert.If a plant is an Orchid, and it has broad leaves, then it can grow in tropical areas.Either a Cactus has thorns, or an Orchid can grow in tropical areas, or both.If a Lotus has flowers, then an Orchid has broad leaves.If a Rose produces seeds, then a Lotus has flowers.Can we conclude if a Cactus cannot survive in the desert then a Lily produces seeds?(YES) Question 2: Can we conclude if a Cactus cannot survive in the desert then a Lily does not produce seeds?(NO)
Rule: d5_1Assumptions:1: A and B are objects of type T and have property S.2: Normally objects of type T with property S have property U.3: if C has property G implies C has property D4: if E has property L implies E has property F5: either C has property G or E has property F or both6: if I has property H then E has property L7: if A has property U then I has property HQuestion 1:Question 2:Example 1:Assumptions:Question 1:</p>
<p>Table 23 :
23
An example of prompt used to generate data instance for depth-5 for NM
ModelsPropositionalFirst-OrderNon-Monotonicd 1d 2d 3d 4d 5d 1d 2d 3d 4d 5d 1d 2d 3d 4d 5</p>
<p>45.00 57.50 70.00 Avg 84.22 75.05 74.52 74.33 70.67 90.67 75.62 69.48 59.00 54.66 50.75 42.78 60.83 62.92 60.83</p>
<p>Table 24 :
24
Few-shot Evaluation of LLMs in terms of accuracy on Multi-LogiEval.</p>
<p>Data is available at https://github.com/Mihir3009/ Multi-LogiEval * Equal Contribution
https://github.com/Mihir3009/Multi-LogiEval
AcknowledgementWe thank the anonymous reviewers for their constructive suggestions and feedback.We extend our gratitude to the Research Computing (RC), and Enterprise Technology at ASU for providing computing resources, and access to the ChatGPT enterprise version for experiments.We acknowledge support by a 2023 Spring Amazon Research Award (ARA).This material is also based upon work supported by the Engineering Research and Development Center -Information Technology Laboratory (ERDC-ITL) under Contract No. W912HZ24C0022.Premises in StoryPremise in Question Answer DS:Rule CombinationsPremises in Story Premise in Question Answer CD:
Logical reasoning for task oriented dialogue systems. Sajjad Beygi, Maryam Fazel-Zarandi, Alessandra Cervone, Prakash Krishnan, Siddhartha Jonnalagadda, 10.18653/v1/2022.ecnlp-1.10Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5). the Fifth Workshop on e-Commerce and NLP (ECNLP 5)Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Transformers as soft reasoners over language. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial IntelligenceOyvind Tafjord, and Kyle RichardsonCurran Associates, Inc. Peter Clark2020. 202133Advances in Neural Information Processing Systems</p>
<p>Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela, arXiv:2402.01306Kto: Model alignment as prospect theoretic optimization. 2024arXiv preprint</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, arXiv:2306.07934Vaiva Imbrasaite, and Deepak Ramachandran. 2023. Boardgameqa: A dataset for natural language reasoning with contradictory information. arXiv preprint</p>
<p>Reasoning-Driven Question-Answering for Natural Language Understanding. Daniel Khashabi, 2019University of Pennsylvania</p>
<p>Benchmark problems for formal nonmonotonic reasoning: Version 2.00. Vladimir Lifschitz, Non-Monotonic Reasoning: 2nd International Workshop Grassau, FRG. Springer1989. June 13-15. 19882</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Towards LogiGLUE: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models. Man Luo, Shrinidhi Kumbhar, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, Chitta Baral, arXiv:2310.008362023arXiv preprint</p>
<p>A survey on multi-hop question answering and generation. Vaibhav Mavi, Anubhav Jangra, Adam Jatowt, arXiv:2204.091402022arXiv preprint</p>
<p>Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, arXiv:2311.11045Teaching small language models how to reason. 20232arXiv preprint</p>
<p>Learning deductive reasoning from synthetic corpus based on formal logic. Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa, International Conference on Machine Learning. PMLR2023</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Xinyi Wang, William Wang, 10.18653/v1/2023.findings-emnlp.248Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. 2023OpenAI ; Liangming Pan, Alon Albalak,Gpt-4 technical report</p>
<p>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL). the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)Bangkok, Thailand2024</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>RuleBERT: Teaching soft rules to pre-trained language models. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, 10.18653/v1/2021.emnlp-main.110Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, ICLR</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Yi: Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, arXiv:2403.046522024arXiv preprint</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2020</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, arXiv:2205.11502On the paradox of learning to reason from data. 2022arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223BD: (P → Q) ∧ (R → S) ∧ (P ∨ ¬S) ⊢ (Q ∨ ¬R) DS: (Q ∨ ¬R) ∧ ¬Q ⊢ ¬R MT: (T → R) ∧ ¬R ⊢ ¬T MT: (U → T) ∧ ¬T ⊢ ¬U (P → Q). 2023arXiv preprintA survey of large language models. U → T) ¬Q U: ✗ IM: (P → (Q ∧ R)</p>
<p>⊢ (p ∧ Q) → R Mt ; P ∧ Q) → R) ∧ ¬r ⊢ ¬(p ∧ Q, DMT: ¬(P ∧ Q) ⊢ (¬P ∨ ¬Q) DS: (¬P ∨ ¬Q) ∧Q ⊢ ¬P. P → (Q ∧ R)</p>
<p>Q , ¬r P ; P → Q) ∧ (q → R) ⊢ (p → R) Mt ; P → R) ∧ ¬r ⊢ ¬p Ds ; P ∨ S) ∧ ¬p ⊢ S Mp ; S → T) ∧ S ⊢ T Mp, Table 10: 4-step reasoning rule combinations for PL and FOL. Rule Combinations Premises in Story Premise in Question Answer HS. T → U) ∧ T ⊢ U (P → Q), (Q → R). P ∨ S), (S → T), (T → U</p>
<p>¬r U: ✓ Bd ; P → Q) ∧ (r → S) ∧ (p ∨ ¬s) ⊢ (q ∨ ¬r) Ct, Q ∨ ¬R) ⊣⊢ (¬R ∨ Q) DS: (¬R ∨ Q) ∧ R ⊢ Q MP: (Q → T) ∧ Q ⊢ T MP: (T → U) ∧ T ⊢ U (P → Q), (R → S). P ∨ ¬S), (Q → T), (T → U</p>
<p>R U: ✓ Cd ; P → Q) ∧ (r → S) ∧ (p ∨ R) ⊢ (q ∨ S) Ct ; Q ∨ S) ⊣⊢ (s ∨ Q) Ds ; S ∨ Q) ∧ ¬s ⊢ Q Mp, Q → T) ∧ Q ⊢ T MP: (T → U) ∧ T ⊢ U (P → Q), (R → S). P ∨ R), (Q → T), (T → U</p>
<p>¬S U: ✓ Table 11: 5-step reasoning rule combinations for PL and FOL. </p>            </div>
        </div>

    </div>
</body>
</html>
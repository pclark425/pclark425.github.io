<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3532 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3532</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3532</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-5a9001cdccdb8b1de227a45eccc503d32d1a2464</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5a9001cdccdb8b1de227a45eccc503d32d1a2464" target="_blank">What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation, and confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge.</p>
                <p><strong>Paper Abstract:</strong> Abstract Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing whether multiple-choice QA models know definitions and taxonomic reasoning—two skills widespread in existing benchmarks and fundamental to more complex reasoning. We introduce a methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation. We include ways to carefully control for artifacts that may arise during this process. Our evaluation confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge. However, it also reveals a more nuanced picture: their performance notably degrades even with a slight increase in the number of “hops” in the underlying taxonomic hierarchy, and with more challenging distractor candidates. Further, existing models are far from perfect when assessed at the level of clusters of semantically connected probes, such as all hypernym questions about a single concept.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3532.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3532.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa (QA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large fine-tuned as a multiple-choice QA model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based pretrained language model (RoBERTa-large) fine-tuned on aggregated science multiple-choice QA datasets and evaluated on automatically generated WordNetQA and DictionaryQA probes measuring definition, synonymy, hypernymy, hyponymy, and word-sense tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder model (Robustly optimized BERT retraining). Used here as a multiple-choice QA encoder (question+[SEP]+answer input) and fine-tuned on aggregated science QA datasets and on small samples of probe data (inoculation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>WordNetQA / DictionaryQA (Definitions, Synonymy, Hypernymy, Hyponymy, WordSense)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Automatically generated multiple-choice probes from expert lexical resources (WordNet and GCIDE) testing taxonomic (ISA/hypernymy/hyponymy) and definitional reasoning, synonym detection, and word-sense disambiguation, with controlled distractor types and multi-hop inference (number of ISA hops).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation from QA-finetuned weights; inoculation (continued fine-tuning) on small probe samples (k up to 3k) including 'add-some' balancing with natural science QA examples to reduce forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot instance-level (dev/test): Definitions 74.1% / 77.1%; Synonymy 61.1% / 64.2%; Hypernymy 53.2% / 71.0%; Hyponymy 48.5% / 58.6%. After inoculation (best aggregate M*, k up to 3k) instance-level (dev/test): Definitions 89.0% / 89.3% (inoculation cost -1.33); Synonymy 81.2% / 81.3 (-1.31); Hypernymy 77.7% / 87.7 (-0.74); Hyponymy 81.2% / 89.4 (-1.64). Cluster-level (strict) accuracies (dev): Definitions 75.0%; Synonymy 61.7%; Hypernymy 54.0%; Hyponymy 36.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random chance ~20% (5-way). Task-specific LSTM Question-to-Choice-GloVe (directly trained on probes): Definitions 53.6% / 51.8%; Synonymy 57.3% / 55.3%; Hypernymy 50.4% / 47.0%; Hyponymy 61.6% / 64.2%. Choice-only baselines and non-transformer models mostly near random or substantially lower.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Zero-shot RoBERTa outperforms task-specific LSTM baselines on many probes (e.g., Definitions dev: 74.1% vs 53.6% = +20.5 pp). Inoculated RoBERTa further improves substantially (Definitions inoculated 89.0% vs LSTM ~53% ≈ +35 pp). Against random baseline (~20%), zero-shot gains are large (~+30–55 pp depending on probe).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance degrades with multi-hop reasoning: accuracy decreases as number of ISA hops k increases (e.g., hyponymy with sister distractors k'=1 drops from 47% to 15% as k increases from 1 to 4 in reported analyses). Performance also drops when distractors are semantically close (e.g., RoBERTa zero-shot example: 88% with random distractors → 64% with up/down (sister/ISA) distractors at k'=1). Cluster-level evaluation reveals inconsistency: hyponymy cluster strict accuracy is low (36.7%), indicating models often fail to know all facts about a concept. Despite strong scores, RoBERTa remains 2–10% behind conservative human estimates on several probes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Detailed analyses include heatmaps by number of hops k and distractor type showing sensitivity to distractor closeness and hop count; cluster-level (strict) accuracy metric demonstrating consistency issues; inoculation experiments showing small data (even 100 examples) can substantially boost performance (example: 2-hop hyponymy random distractors improved from 59% to 77% after inoculating with 100 examples). Inoculation cost measured (~1–2% loss on original QA task for transformer models). 'Add-some' inoculation balancing probes with natural science questions reduces forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3532.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3532.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (QA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-large uncased (whole-word masking) fine-tuned as a multiple-choice QA model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based pretrained language model (BERT-large) fine-tuned on science QA datasets and evaluated on WordNetQA/DictionaryQA probes for definitional and taxonomic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large (uncased, whole-word masking)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder (BERT-large) fine-tuned for multiple-choice QA by encoding question+answer pairs and optimizing classifier over CLS token; evaluated zero-shot and after inoculation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>WordNetQA / DictionaryQA (Definitions, Synonymy, Hypernymy, Hyponymy, WordSense)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same set of lexical/taxonomic probes as above testing recognition of definitions, synonyms, and ISA relations, including multi-hop inference and distractor perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot from QA-finetuned model; inoculation by fine-tuning on up to 3k probe examples using the modified inoculation objective to reduce forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot (dev/test): Definitions 54.1% / 55.7%; Synonymy 58.8% / 60.9%; Hypernymy 43.2% / 51.0%; Hyponymy 24.0% / 27.0%. After inoculation (best aggregate): Definitions 84.0% / 84.1 (inoculation cost -1.15); Synonymy 79.6% / 79.7 (-0.44); Hypernymy 73.8% / 82.7 (-0.49); Hyponymy 79.8% / 88.0 (-0.92). Cluster-level accuracies (dev): Definitions 68.5%; Synonymy 58.1%; Hypernymy 49.0%; Hyponymy 34.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random ~20%. Task-specific LSTM Question-to-Choice-GloVe: Definitions ~53.6% etc. Non-transformer baselines (ESIM) near random on many probes.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Zero-shot BERT exceeds non-transformer baselines on several probes (e.g., Synonymy: 58.8% vs LSTM 57.3% dev). Inoculation substantially improves BERT across probes (e.g., Definitions to 84.0% vs LSTM ~53% ≈ +31 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Strong sensitivity to hop count and distractor closeness similar to RoBERTa; zero-shot BERT much weaker than RoBERTa on many probes. Cluster-level evaluation reveals partial knowledge (especially hyponymy cluster strict accuracies are low). Still below human estimates by several percentage points after inoculation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Inoculation learning curves show BERT learns probes quickly with low inoculation cost; comparison with add-some inoculation shows balancing with natural QA examples reduces forgetting. Heatmap analyses by hops and distractor types highlight where performance falls off.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3532.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3532.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESIM (QA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ESIM-based QA model (non-transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-transformer (ESIM) model using GloVe or ELMo embeddings evaluated as a science QA model and on WordNetQA/DictionaryQA probes; generally underperforms compared to transformer models and struggles with inoculation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ESIM (with GloVe / ELMo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LSTM-based entailment-style model (ESIM) used as a QA encoder (question-to-choice architecture) with either GloVe or ELMo embeddings; trained on science QA and evaluated on probes both zero-shot and after inoculation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>WordNetQA / DictionaryQA (Definitions, Synonymy, Hypernymy, Hyponymy)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See above; ESIM is evaluated on definitional and taxonomic probes to assess its ability to capture relational lexical knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot from science-trained ESIM; inoculation (continued training) on probe examples and add-some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot (dev/test): ESIM-GloVe Definitions 27.5% / 28.3%; Synonymy 25.1% / 26.1%; Hypernymy 27.0% / 33.0%; Hyponymy 23.6% / 24.8%. After inoculation (best aggregate): ESIM-GloVe Definitions 46.2% / 42.4 (inoculation cost -6.27); Synonymy 50.4% / 47.3 (-6.84); Hypernymy 56.6% / 52.9 (-5.69); Hyponymy 59.1% / 61.1 (-5.10).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random ~20%. Task-specific LSTM models trained on probes (Question-to-Choice-GloVe/ELMo) generally outperform ESIM zero-shot but ESIM after inoculation can improve yet at the expense of larger inoculation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>ESIM shows limited zero-shot competence (near random). Inoculation improves ESIM substantially in absolute terms (e.g., Definitions to ~46% dev) but with high inoculation cost (degradation on original science task ~5–7 percentage points), contrasting with transformers which have lower cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>ESIM struggles to learn probes without degrading performance on original tasks (high inoculation cost). It is not robust to add-some inoculation; more science data during inoculation tends to confuse ESIM on both tasks. Overall low zero-shot competence on structured lexical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comparison of learning curves vs transformer models shows ESIM cannot absorb probe data without forgetting; inoculation cost measured and reported. Analysis highlights the necessity of high-capacity transformer architectures for quick low-cost adaptation to probe formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3532.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3532.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Question-to-Choice LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific LSTM Question-to-Choice models (GloVe / ELMo embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LSTM-based multiple-choice QA architectures trained directly on the probe data (silver-standard) and used as a baseline to test whether models trained specifically on probes outperform pretrained transformers zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Question-to-Choice LSTM (GloVe / ELMo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BiLSTM encoders over question and choice with max-pooling to create contextual representations and attention scoring between question and choice; trained directly on probe datasets (up to 3k examples for direct training baselines reported).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>WordNetQA / DictionaryQA (Definitions, Synonymy, Hypernymy, Hyponymy)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same lexical/taxonomic multiple-choice probes; these LSTM models are task-specific baselines trained on the probe distribution rather than leveraging large-scale pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Direct training on probes (3k examples) with GloVe or ELMo embeddings; no pretraining beyond embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Direct-training results (Group 2 in Table 7): Question-to-Choice-GloVe dev/test: Definitions 53.6% / 51.8%; Synonymy 57.3% / 55.3%; Hypernymy 50.4% / 47.0%; Hyponymy 61.6% / 64.2%. Question-to-Choice-ELMo mixed: Definitions 42.3% / 41.6%; Synonymy 58.6% / 56.0%; Hypernymy 56.0% / 51.5%; Hyponymy 54.8% / 56.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to random (~20%) and choice-only baselines, these LSTM models are substantially stronger when trained directly on probes but are generally weaker than RoBERTa after inoculation and weaker than RoBERTa zero-shot for some probes (e.g., Definitions).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Directly trained LSTM baselines substantially exceed random and choice-only baselines (e.g., Definitions ~53% vs 20% random), but are outperformed by RoBERTa zero-shot on some probes (Definitions) and by inoculated transformers on most probes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Task-specific LSTM models lack the large-scale pretraining inductive bias and generally underperform transformer QA models, especially after transformer inoculation; they do not generalize as well across distractor perturbations and multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comparisons across embedding choices (GloVe vs ELMo) show variability; the paper uses these models as sanity checks to ensure probe datasets are non-trivial and not dominated by artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3532.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3532.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Choice-Only Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Choice-only and choice-to-choice partial-input baselines (GloVe / BERT / RoBERTa embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that use only the candidate answer choices (Choice-Only) or inter-choice comparisons (Choice-to-Choice) to predict answers, used to detect annotation artifacts and biases in probe construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Choice-Only / Choice-to-Choice (GloVe, BERT, RoBERTa variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that encode only answer choices (not the question) or compare choices pairwise; implemented with different embeddings (GloVe, BERT, RoBERTa) to test for dataset artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>WordNetQA / DictionaryQA (sanity-check baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>These are not reasoning systems per se; they are used as controls to detect exploitable artifacts in automatically generated multiple-choice probes.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Train-only-on-choices baselines and choice-to-choice attention models; used to probe for annotation artifacts and distractor weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Choice-Only-GloVe dev/test example: Definitions 26.6% / 26.1%; Synonymy 36.9% / 36.1%; Hypernymy 42.5% / 46.0%; Hyponymy 34.3% / 34.4%. Choice-Only-BERT and RoBERTa generally higher (Choice-Only-RoBERTa: Definitions 26.8% / 28.6%, Synonymy ~41% etc.). Some unexpected high scores occurred in early DictionaryQA due to distractor biases (Choice-Only reached 56% before corrections). Choice-to-Choice-BERT/RoBERTa exceeded 60% on some hypernymy probes in spite of efforts to filter artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random ~20% ; these baselines quantify the degree to which answer choices alone leak signal. High Choice-only scores indicate dataset artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Choice-only models show modest improvements over random on some probes indicating non-trivial artifacts; their performance motivated further filtering of distractors (e.g., GCIDE distractor filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High choice-only performance in some settings (particularly in initial DictionaryQA and certain hypernymy splits) revealed that expert-resource-based automatic generation can still introduce systematic artifacts; highlights the need for rigorous baselines. These models, by design, do not perform logical reasoning (no question input).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Used as sanity checks; prompted iterative improvements to distractor generation (filtering entries without example sentences, de-duplication). The paper reports that despite filtering, some hypernymy splits still allow choice-only models to reach unexpectedly high accuracy, indicating residual bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>oLMpics - On what Language Model Pre-training Captures <em>(Rating: 2)</em></li>
                <li>Language Models as Knowledge Bases? <em>(Rating: 2)</em></li>
                <li>Probing Natural Language Inference Models through Semantic Fragments <em>(Rating: 2)</em></li>
                <li>Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets <em>(Rating: 2)</em></li>
                <li>RoBERTa: A Robustly Optimized Bert Retraining Approach <em>(Rating: 1)</em></li>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 1)</em></li>
                <li>Language Models as Knowledge Bases? In Proceedings of EMNLP <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3532",
    "paper_id": "paper-5a9001cdccdb8b1de227a45eccc503d32d1a2464",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "RoBERTa (QA)",
            "name_full": "RoBERTa-large fine-tuned as a multiple-choice QA model",
            "brief_description": "A transformer-based pretrained language model (RoBERTa-large) fine-tuned on aggregated science multiple-choice QA datasets and evaluated on automatically generated WordNetQA and DictionaryQA probes measuring definition, synonymy, hypernymy, hyponymy, and word-sense tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Transformer encoder model (Robustly optimized BERT retraining). Used here as a multiple-choice QA encoder (question+[SEP]+answer input) and fine-tuned on aggregated science QA datasets and on small samples of probe data (inoculation).",
            "model_size": null,
            "reasoning_task_name": "WordNetQA / DictionaryQA (Definitions, Synonymy, Hypernymy, Hyponymy, WordSense)",
            "reasoning_task_description": "Automatically generated multiple-choice probes from expert lexical resources (WordNet and GCIDE) testing taxonomic (ISA/hypernymy/hyponymy) and definitional reasoning, synonym detection, and word-sense disambiguation, with controlled distractor types and multi-hop inference (number of ISA hops).",
            "method_or_intervention": "Zero-shot evaluation from QA-finetuned weights; inoculation (continued fine-tuning) on small probe samples (k up to 3k) including 'add-some' balancing with natural science QA examples to reduce forgetting.",
            "performance": "Zero-shot instance-level (dev/test): Definitions 74.1% / 77.1%; Synonymy 61.1% / 64.2%; Hypernymy 53.2% / 71.0%; Hyponymy 48.5% / 58.6%. After inoculation (best aggregate M*, k up to 3k) instance-level (dev/test): Definitions 89.0% / 89.3% (inoculation cost -1.33); Synonymy 81.2% / 81.3 (-1.31); Hypernymy 77.7% / 87.7 (-0.74); Hyponymy 81.2% / 89.4 (-1.64). Cluster-level (strict) accuracies (dev): Definitions 75.0%; Synonymy 61.7%; Hypernymy 54.0%; Hyponymy 36.7%.",
            "baseline_performance": "Random chance ~20% (5-way). Task-specific LSTM Question-to-Choice-GloVe (directly trained on probes): Definitions 53.6% / 51.8%; Synonymy 57.3% / 55.3%; Hypernymy 50.4% / 47.0%; Hyponymy 61.6% / 64.2%. Choice-only baselines and non-transformer models mostly near random or substantially lower.",
            "improvement_over_baseline": "Zero-shot RoBERTa outperforms task-specific LSTM baselines on many probes (e.g., Definitions dev: 74.1% vs 53.6% = +20.5 pp). Inoculated RoBERTa further improves substantially (Definitions inoculated 89.0% vs LSTM ~53% ≈ +35 pp). Against random baseline (~20%), zero-shot gains are large (~+30–55 pp depending on probe).",
            "limitations_or_failures": "Performance degrades with multi-hop reasoning: accuracy decreases as number of ISA hops k increases (e.g., hyponymy with sister distractors k'=1 drops from 47% to 15% as k increases from 1 to 4 in reported analyses). Performance also drops when distractors are semantically close (e.g., RoBERTa zero-shot example: 88% with random distractors → 64% with up/down (sister/ISA) distractors at k'=1). Cluster-level evaluation reveals inconsistency: hyponymy cluster strict accuracy is low (36.7%), indicating models often fail to know all facts about a concept. Despite strong scores, RoBERTa remains 2–10% behind conservative human estimates on several probes.",
            "ablation_or_analysis": "Detailed analyses include heatmaps by number of hops k and distractor type showing sensitivity to distractor closeness and hop count; cluster-level (strict) accuracy metric demonstrating consistency issues; inoculation experiments showing small data (even 100 examples) can substantially boost performance (example: 2-hop hyponymy random distractors improved from 59% to 77% after inoculating with 100 examples). Inoculation cost measured (~1–2% loss on original QA task for transformer models). 'Add-some' inoculation balancing probes with natural science questions reduces forgetting.",
            "uuid": "e3532.0",
            "source_info": {
                "paper_title": "What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "BERT (QA)",
            "name_full": "BERT-large uncased (whole-word masking) fine-tuned as a multiple-choice QA model",
            "brief_description": "A transformer-based pretrained language model (BERT-large) fine-tuned on science QA datasets and evaluated on WordNetQA/DictionaryQA probes for definitional and taxonomic reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-large (uncased, whole-word masking)",
            "model_description": "Transformer encoder (BERT-large) fine-tuned for multiple-choice QA by encoding question+answer pairs and optimizing classifier over CLS token; evaluated zero-shot and after inoculation.",
            "model_size": null,
            "reasoning_task_name": "WordNetQA / DictionaryQA (Definitions, Synonymy, Hypernymy, Hyponymy, WordSense)",
            "reasoning_task_description": "Same set of lexical/taxonomic probes as above testing recognition of definitions, synonyms, and ISA relations, including multi-hop inference and distractor perturbations.",
            "method_or_intervention": "Zero-shot from QA-finetuned model; inoculation by fine-tuning on up to 3k probe examples using the modified inoculation objective to reduce forgetting.",
            "performance": "Zero-shot (dev/test): Definitions 54.1% / 55.7%; Synonymy 58.8% / 60.9%; Hypernymy 43.2% / 51.0%; Hyponymy 24.0% / 27.0%. After inoculation (best aggregate): Definitions 84.0% / 84.1 (inoculation cost -1.15); Synonymy 79.6% / 79.7 (-0.44); Hypernymy 73.8% / 82.7 (-0.49); Hyponymy 79.8% / 88.0 (-0.92). Cluster-level accuracies (dev): Definitions 68.5%; Synonymy 58.1%; Hypernymy 49.0%; Hyponymy 34.0%.",
            "baseline_performance": "Random ~20%. Task-specific LSTM Question-to-Choice-GloVe: Definitions ~53.6% etc. Non-transformer baselines (ESIM) near random on many probes.",
            "improvement_over_baseline": "Zero-shot BERT exceeds non-transformer baselines on several probes (e.g., Synonymy: 58.8% vs LSTM 57.3% dev). Inoculation substantially improves BERT across probes (e.g., Definitions to 84.0% vs LSTM ~53% ≈ +31 pp).",
            "limitations_or_failures": "Strong sensitivity to hop count and distractor closeness similar to RoBERTa; zero-shot BERT much weaker than RoBERTa on many probes. Cluster-level evaluation reveals partial knowledge (especially hyponymy cluster strict accuracies are low). Still below human estimates by several percentage points after inoculation.",
            "ablation_or_analysis": "Inoculation learning curves show BERT learns probes quickly with low inoculation cost; comparison with add-some inoculation shows balancing with natural QA examples reduces forgetting. Heatmap analyses by hops and distractor types highlight where performance falls off.",
            "uuid": "e3532.1",
            "source_info": {
                "paper_title": "What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "ESIM (QA)",
            "name_full": "ESIM-based QA model (non-transformer)",
            "brief_description": "A non-transformer (ESIM) model using GloVe or ELMo embeddings evaluated as a science QA model and on WordNetQA/DictionaryQA probes; generally underperforms compared to transformer models and struggles with inoculation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ESIM (with GloVe / ELMo)",
            "model_description": "An LSTM-based entailment-style model (ESIM) used as a QA encoder (question-to-choice architecture) with either GloVe or ELMo embeddings; trained on science QA and evaluated on probes both zero-shot and after inoculation.",
            "model_size": null,
            "reasoning_task_name": "WordNetQA / DictionaryQA (Definitions, Synonymy, Hypernymy, Hyponymy)",
            "reasoning_task_description": "See above; ESIM is evaluated on definitional and taxonomic probes to assess its ability to capture relational lexical knowledge.",
            "method_or_intervention": "Zero-shot from science-trained ESIM; inoculation (continued training) on probe examples and add-some variants.",
            "performance": "Zero-shot (dev/test): ESIM-GloVe Definitions 27.5% / 28.3%; Synonymy 25.1% / 26.1%; Hypernymy 27.0% / 33.0%; Hyponymy 23.6% / 24.8%. After inoculation (best aggregate): ESIM-GloVe Definitions 46.2% / 42.4 (inoculation cost -6.27); Synonymy 50.4% / 47.3 (-6.84); Hypernymy 56.6% / 52.9 (-5.69); Hyponymy 59.1% / 61.1 (-5.10).",
            "baseline_performance": "Random ~20%. Task-specific LSTM models trained on probes (Question-to-Choice-GloVe/ELMo) generally outperform ESIM zero-shot but ESIM after inoculation can improve yet at the expense of larger inoculation cost.",
            "improvement_over_baseline": "ESIM shows limited zero-shot competence (near random). Inoculation improves ESIM substantially in absolute terms (e.g., Definitions to ~46% dev) but with high inoculation cost (degradation on original science task ~5–7 percentage points), contrasting with transformers which have lower cost.",
            "limitations_or_failures": "ESIM struggles to learn probes without degrading performance on original tasks (high inoculation cost). It is not robust to add-some inoculation; more science data during inoculation tends to confuse ESIM on both tasks. Overall low zero-shot competence on structured lexical reasoning.",
            "ablation_or_analysis": "Comparison of learning curves vs transformer models shows ESIM cannot absorb probe data without forgetting; inoculation cost measured and reported. Analysis highlights the necessity of high-capacity transformer architectures for quick low-cost adaptation to probe formats.",
            "uuid": "e3532.2",
            "source_info": {
                "paper_title": "What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "Question-to-Choice LSTM",
            "name_full": "Task-specific LSTM Question-to-Choice models (GloVe / ELMo embeddings)",
            "brief_description": "LSTM-based multiple-choice QA architectures trained directly on the probe data (silver-standard) and used as a baseline to test whether models trained specifically on probes outperform pretrained transformers zero-shot.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Question-to-Choice LSTM (GloVe / ELMo)",
            "model_description": "BiLSTM encoders over question and choice with max-pooling to create contextual representations and attention scoring between question and choice; trained directly on probe datasets (up to 3k examples for direct training baselines reported).",
            "model_size": null,
            "reasoning_task_name": "WordNetQA / DictionaryQA (Definitions, Synonymy, Hypernymy, Hyponymy)",
            "reasoning_task_description": "Same lexical/taxonomic multiple-choice probes; these LSTM models are task-specific baselines trained on the probe distribution rather than leveraging large-scale pretraining.",
            "method_or_intervention": "Direct training on probes (3k examples) with GloVe or ELMo embeddings; no pretraining beyond embeddings.",
            "performance": "Direct-training results (Group 2 in Table 7): Question-to-Choice-GloVe dev/test: Definitions 53.6% / 51.8%; Synonymy 57.3% / 55.3%; Hypernymy 50.4% / 47.0%; Hyponymy 61.6% / 64.2%. Question-to-Choice-ELMo mixed: Definitions 42.3% / 41.6%; Synonymy 58.6% / 56.0%; Hypernymy 56.0% / 51.5%; Hyponymy 54.8% / 56.3%.",
            "baseline_performance": "Compared to random (~20%) and choice-only baselines, these LSTM models are substantially stronger when trained directly on probes but are generally weaker than RoBERTa after inoculation and weaker than RoBERTa zero-shot for some probes (e.g., Definitions).",
            "improvement_over_baseline": "Directly trained LSTM baselines substantially exceed random and choice-only baselines (e.g., Definitions ~53% vs 20% random), but are outperformed by RoBERTa zero-shot on some probes (Definitions) and by inoculated transformers on most probes.",
            "limitations_or_failures": "Task-specific LSTM models lack the large-scale pretraining inductive bias and generally underperform transformer QA models, especially after transformer inoculation; they do not generalize as well across distractor perturbations and multi-hop reasoning.",
            "ablation_or_analysis": "Comparisons across embedding choices (GloVe vs ELMo) show variability; the paper uses these models as sanity checks to ensure probe datasets are non-trivial and not dominated by artifacts.",
            "uuid": "e3532.3",
            "source_info": {
                "paper_title": "What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "Choice-Only Baselines",
            "name_full": "Choice-only and choice-to-choice partial-input baselines (GloVe / BERT / RoBERTa embeddings)",
            "brief_description": "Baselines that use only the candidate answer choices (Choice-Only) or inter-choice comparisons (Choice-to-Choice) to predict answers, used to detect annotation artifacts and biases in probe construction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Choice-Only / Choice-to-Choice (GloVe, BERT, RoBERTa variants)",
            "model_description": "Models that encode only answer choices (not the question) or compare choices pairwise; implemented with different embeddings (GloVe, BERT, RoBERTa) to test for dataset artifacts.",
            "model_size": null,
            "reasoning_task_name": "WordNetQA / DictionaryQA (sanity-check baselines)",
            "reasoning_task_description": "These are not reasoning systems per se; they are used as controls to detect exploitable artifacts in automatically generated multiple-choice probes.",
            "method_or_intervention": "Train-only-on-choices baselines and choice-to-choice attention models; used to probe for annotation artifacts and distractor weaknesses.",
            "performance": "Choice-Only-GloVe dev/test example: Definitions 26.6% / 26.1%; Synonymy 36.9% / 36.1%; Hypernymy 42.5% / 46.0%; Hyponymy 34.3% / 34.4%. Choice-Only-BERT and RoBERTa generally higher (Choice-Only-RoBERTa: Definitions 26.8% / 28.6%, Synonymy ~41% etc.). Some unexpected high scores occurred in early DictionaryQA due to distractor biases (Choice-Only reached 56% before corrections). Choice-to-Choice-BERT/RoBERTa exceeded 60% on some hypernymy probes in spite of efforts to filter artifacts.",
            "baseline_performance": "Random ~20% ; these baselines quantify the degree to which answer choices alone leak signal. High Choice-only scores indicate dataset artifacts.",
            "improvement_over_baseline": "Choice-only models show modest improvements over random on some probes indicating non-trivial artifacts; their performance motivated further filtering of distractors (e.g., GCIDE distractor filtering).",
            "limitations_or_failures": "High choice-only performance in some settings (particularly in initial DictionaryQA and certain hypernymy splits) revealed that expert-resource-based automatic generation can still introduce systematic artifacts; highlights the need for rigorous baselines. These models, by design, do not perform logical reasoning (no question input).",
            "ablation_or_analysis": "Used as sanity checks; prompted iterative improvements to distractor generation (filtering entries without example sentences, de-duplication). The paper reports that despite filtering, some hypernymy splits still allow choice-only models to reach unexpectedly high accuracy, indicating residual bias.",
            "uuid": "e3532.4",
            "source_info": {
                "paper_title": "What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge",
                "publication_date_yy_mm": "2019-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "oLMpics - On what Language Model Pre-training Captures",
            "rating": 2
        },
        {
            "paper_title": "Language Models as Knowledge Bases?",
            "rating": 2
        },
        {
            "paper_title": "Probing Natural Language Inference Models through Semantic Fragments",
            "rating": 2
        },
        {
            "paper_title": "Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets",
            "rating": 2
        },
        {
            "paper_title": "RoBERTa: A Robustly Optimized Bert Retraining Approach",
            "rating": 1
        },
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 1
        },
        {
            "paper_title": "Language Models as Knowledge Bases? In Proceedings of EMNLP",
            "rating": 1
        }
    ],
    "cost": 0.01760575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge</h1>
<p>Kyle Richardson and Ashish Sabharwal<br>Allen Institute for AI, Seattle, WA, USA<br>{kyler,ashishs}@allenai.org</p>
<h4>Abstract</h4>
<p>Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing whether multiple-choice QA models know definitions and taxonomic reasoning-two skills widespread in existing benchmarks and fundamental to more complex reasoning. We introduce a methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation. We include ways to carefully control for artifacts that may arise during this process. Our evaluation confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge. However, it also reveals a more nuanced picture: their performance notably degrades even with a slight increase in the number of "hops" in the underlying taxonomic hierarchy, and with more challenging distractor candidates. Further, existing models are far from perfect when assessed at the level of clusters of semantically connected probes, such as all hypernym questions about a single concept.</p>
<h2>1 Introduction</h2>
<p>Automatically answering questions, especially in the open-domain setting where minimal or no contextual knowledge is explicitly provided, requires considerable background knowledge and reasoning abilities. For example, answering the two questions in the top gray box in Figure 1 requires identifying a specific ISA relation (that 'cooking' is a type of 'learned behavior') as well as recalling a concept definition (that 'global warming' is defined as a 'worldwide increase in temperature').
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of our experimental setup and probing methodology. The gray box at the top shows questions from existing open-domain QA benchmarks, requiring background knowledge. The yellow box shows simple examples of multiple-choice questions in our proposed Definition and ISA probes.</p>
<p>Recent success in QA has been driven largely by new benchmarks (Zellers et al., 2018; Talmor et al., 2019b; Bhagavatula et al., 2020; Khot et al., 2020, etc.) and advances in model pretraining (Radford et al., 2018; Devlin et al., 2019). This raises a natural question: Do state-of-the-art multiple-choice QA (MCQA) models that excel at standard benchmarks truly possess basic knowledge and reasoning skills expected in these tasks?</p>
<p>Answering this question is challenging due to limited understanding of heavily pre-trained complex models and the way existing MCQA datasets are constructed. We focus on the second aspect, which has two limitations: Large-scale crowdsourcing leaves little systematic control over question semantics or requisite background knowledge (Welbl et al., 2017), while questions from real exams tend to mix multiple challenges in a single dataset, often even in a single ques-</p>
<p>tion <em>Clark et al. (2018); Boratko et al. (2018)</em>.</p>
<p>To address this challenge, we propose systematically constructing model competence probes by exploiting structured information contained in <em>expert knowledge sources</em> such as knowledge graphs and lexical taxonomies. Importantly, these probes are diagnostic tasks, designed not to impart new knowledge but to assess what models trained on standard QA benchmarks already know; as such, they serve as proxies for the types of questions that a model might encounter in its original task, but involve a single category of knowledge under various controlled conditions and perturbations.</p>
<p>Figure 1 illustrates our methodology. We start with a set of standard MCQA benchmark tasks $\mathcal{D}$ and a set of models $\mathcal{M}$ trained on $\mathcal{D}$. Our goal is to assess how competent these models are relative to a particular knowledge or reasoning skill $S$ (e.g., definitions) that is generally deemed important for performing well on $D$. To this end, we systematically and automatically generate a set of <em>dataset probes</em> $P_{S}$ from information available in expert knowledge sources. Each probe is an MCQA rendering of the target information (see examples in Figure 1, yellow box). We then use these probes $P_{S}$ to ask two empirical questions: (1) How well do models in $\mathcal{M}$ already trained on $\mathcal{D}$ perform on probing tasks $P_{S}$? (2) With additional nudging, can models be re-trained, using only a modest amount of additional data, to perform well on each probing task $P_{S}$ with minimal performance loss on their original tasks $D$ (thus giving evidence of prior model competence on $S$)?</p>
<p>While our methodology is general, our experiments focus on probing state-of-the-art MCQA models in the domain of grade-school level science, which is considered particularly challenging with respect to background knowledge and inference <em>Clark (2015); Clark et al. (2019); Khot et al. (2020)</em>. In addition, existing science benchmarks are known to involve widespread use of definition and taxonomic knowledge (see detailed analysis by <em>Clark et al. (2018); Boratko et al. (2018)</em>), which is also fundamental to deeper reasoning. Accordingly, we employ the most widely used lexical ontology WordNet <em>Miller (1995)</em> and publicly available dictionaries as sources of expert knowledge to construct our probes, WordNetQA (Section 3.1) and DictionaryQA (Section 3.2). These probes measure competence in various settings including hypernymy, hyponymy, and synonymy detection, as well as word sense disambiguation.</p>
<p>Our exploration is closely related to the recent work of <em>Talmor et al. (2019a)</em>. However, a key difference is that they study language models (LMs), for which there is <em>no clear a priori expectation</em> of specific knowledge or reasoning skills. In contrast, we focus on models heavily trained for benchmark QA tasks, where such tasks are known to require certain types of knowledge and reasoning skills. We probe whether such skills are actually learned by QA models, either during LM pre-training or when training for the QA tasks.</p>
<p>Recognizing the need for suitable controls in any synthetic probing methodology <em>Hewitt and Liang (2019); Talmor et al. (2019a)</em>, we introduce two controls: (a) the probe must be challenging for any model that lacks contextual embeddings, and (b) strong models must have a <em>low inoculation cost</em>, i.e., when fine-tuned on a few probing examples, the model should mostly retain its performance on its original task. This ensures that the probe performance of a model, even when lightly inoculated on probing data, reflects its knowledge as originally trained for the benchmark task, which is precisely what we aim to uncover.</p>
<p>Constructing a wide range of systematic tests is critical for having definitive empirical evidence of model competence on any given phenomenon. Such tests should cover a broad set of concepts and question <em>variations</em> (i.e., systematic adjustments to how the questions are constructed). When assessing <em>ISA</em> reasoning, not only is it important to recognize in the question in Figure 1 that <em>cooking</em> is a <em>learned behavior</em>, but also that <em>cooking</em> is a general type of <em>behavior</em> or, through a few more inferential steps, a type of <em>human activity</em>. Our automatic use of expert knowledge sources allows constructing such high-coverage probes, circumventing pitfalls of solicitation bias and reporting bias.</p>
<p>Our results confirm that transformer-based QA models have a remarkable ability to recognize the types of knowledge captured in our probes—even without additional fine-tuning (i.e., in a <em>zero-shot</em> setting). Such models can even outperform strong</p>
<p><sup>1</sup>All data and code are available at https://github.com/allenal/semantic_fragments</p>
<p><sup>2</sup>Standard inoculation <em>Liu et al. (2019a)</em> is known to drop performance on the original task. We use a modified objective <em>Richardson et al. (2020)</em> to alleviate this issue.</p>
<p><sup>3</sup>Different from <em>Talmor et al. (2019a)</em>, we find BERT and RoBERTa based QA models to be qualitatively similar, performing within 5% of each other on nearly all probes.</p>
<p>task-specific non-transformer models trained directly on our probing tasks (e.g., $+26 \%$ compared to a task-specific LSTM). We also show that the same models can be effectively re-fine-tuned on small samples (even 100 examples) of probe data, and that high performance on the probes tends to correlate with a smaller drop in the model's performance on the original QA task.</p>
<p>Our comprehensive assessment also reveals important nuances to the positive trend. For example, we find that the best models still perform 2-10\% (absolute) below conservative estimates of human performance (Section 3.1.3) on these tasks. Further, the accuracy of even the best QA model degrades substantially on our hyponym probes (by $8-15 \%$ ) when going from 1-hop hyponym links to 2-hops. The accuracy on the WordNetQA probe drops by $14-44 \%$ under our cluster-level analysis (Section 3.1.1), which assesses whether a model knows several facts about each individual concept, rather than only answering correctly isolated questions. This shows that state-of-the-art QA models have much room to improve even in some fundamental building blocks (definitions and taxonomic hierarchies) of more complex forms of reasoning.</p>
<h2>2 Related Work</h2>
<p>We follow recent work on constructing challenge datasets for probing neural models, which has primarily focused on the task of natural language inference (NLI) (Glockner et al., 2018; McCoy et al., 2019; Rozen et al., 2019; Warstadt et al., 2019). Most of this work looks at constructing data through adversarial generation methods, which have also been found useful for creating stronger models (Kang et al., 2018). There has also been work on using synthetic data of the type we consider in this paper (Poliak et al., 2018a; Geiger et al., 2019; Yanaka et al., 2020; Clark et al., 2020). We closely follow the methodology of Richardson et al. (2020), who use handconstructed linguistic fragments to probe NLI models and study model re-training using a variant of the inoculation by fine-tuning strategy of Liu et al. (2019a). In contrast, we focus on probing open-domain MCQA models (see Si et al. (2019) for a study on reading comprehension) as well as constructing data from much larger sources of structured knowledge.</p>
<p>Our main study focuses on probing the BERT model and fine-tuning approach of Devlin et al.
(2019), and other variants thereof, which are all based on the transformer architecture of Vaswani et al. (2017). There have been recent studies into the types of relational knowledge contained in large-scale knowledge models (Schick and Schütze, 2020; Petroni et al., 2019; Jiang et al., 2019), which also probe models using structured knowledge sources. These studies, however, primarily focus on unearthing the knowledge contained in the underlying language models as is without further training, using simple (single token) cloze-style probing tasks and templates. Most of these results only provide a lowerbound estimate of model performance, since the probing templates being employed potentially deviate from what the model has observed during pre-training. In contrast, we focus on understanding the knowledge contained in language models after they have been trained for a QA end-task using benchmark datasets in which such knowledge is expected to be widespread. Further, our evaluation is done before and after these models are finetuned on our small samples of target data. This has the advantage of allowing each model to become informed about the format of each probe. We also explore a more complex set of probing templates.</p>
<p>The use of lexical resources such as WordNet to construct datasets has a long history, and has recently appeared in work on adversarial attacks (Jia and Liang, 2017) and general task construction (Pilehvar and Camacho-Collados, 2019). In the area of MCQA, there is related work on constructing questions from tuples (Jauhar et al., 2016; Talmor et al., 2019b), both of which involve standard crowd annotation to elicit question-answer pairs (see also Seyler et al. (2017); Reddy et al. (2017)). In contrast to this work, we focus on generating data in an entirely automatic and silver-standard fashion (i.e., in a way that potentially introduces a little noise), which obviates the need for expensive annotation and gives us the flexibility to construct much larger datasets that control a rich set of semantic aspects of the target questions. Following standard practices in MCQA dataset creation (e.g., Khot et al., 2020), however, we perform crowdsourcing to obtain conservative (in the sense of Nangia and Bowman (2019)) estimates of human performance on our main evaluation sets, to compare against model performance.</p>
<p>While our probing methodology is amenable to any domain, we focus on probing open-domain</p>
<table>
<thead>
<tr>
<th>Set</th>
<th>WordNet (WN)</th>
<th>GCIDE</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathcal{R}$</td>
<td>{isa$\uparrow$, isa⋮, def, ex, lemma}</td>
<td>{def, ex, lemma}</td>
</tr>
<tr>
<td>$\mathcal{C}$</td>
<td>{WN synsets}</td>
<td>{entry ids}</td>
</tr>
<tr>
<td>$\mathcal{D}$</td>
<td>{synset glosses}</td>
<td>{unique defs}</td>
</tr>
<tr>
<td>$\mathcal{S}$</td>
<td>{synset sentences}</td>
<td>{entry examples}</td>
</tr>
<tr>
<td>$\mathcal{W}$</td>
<td>{synset lemmas}</td>
<td>{all words}</td>
</tr>
<tr>
<td>Atomic Triple Types</td>
<td></td>
<td>Definition</td>
</tr>
<tr>
<td>Concept Senses and Definitions</td>
<td></td>
<td>$T_{\text {a }} \subseteq{\text { def }} \times \mathcal{C} \times \mathcal{D}$</td>
</tr>
<tr>
<td>Concepts with Example Sentences</td>
<td></td>
<td>$T_{\text {s }} \subseteq{\text { ext }} \times \mathcal{C} \times \mathcal{S}$</td>
</tr>
<tr>
<td>Concepts with Words</td>
<td></td>
<td>$T_{\text {i }} \subseteq{\text { lemma }} \times \mathcal{C} \times \mathcal{W}$</td>
</tr>
<tr>
<td>ISA Relations (WN only)</td>
<td></td>
<td>$T_{\text {s }} \subseteq\left{\text { isa }{ }^{1}, \text { isa }{ }^{1}\right} \times \mathcal{C} \times \mathcal{C}$</td>
</tr>
</tbody>
</table>
<p>Table 1: A description of the different resources used to construct the probes, represented as abstract triples.</p>
<p>QA models in the domain on grade-school level science using a standard suite of benchmark QA datasets (see Table 6). Our choice of this domain is based on the following considerations: it is wellstudied qualitatively <em>Davis (2016)</em>, making it relatively easy to know the types of probes and diagnostic tests to construct using existing expert knowledge. For example, the manual analysis of Mihaylov et al. (2018) found that explicit definitional and ISA knowledge occurred in around 20\% and $18 \%$, respectively, of the questions sampled in one benchmark task. Clark et al. (2013) and Boratko et al. (2018) provide similar results involving other benchmarks used in our study.</p>
<p>We also examined MCQA models trained on closely-related datasets tailored to commonsense and situational reasoning <em>Zellers et al. (2018); Talmor et al. (2019b); Bhagavatula et al. (2020); Sap et al. (2019)</em>. However, there has been a limited study of the kinds of knowledge needed in this domain, as well as expert knowledge sources for creating corresponding probes. MCQA models trained in this domain exhibit lower performance on our definition and ISA probes.</p>
<h2>3 Dataset Probes and Construction</h2>
<p>Our probing methodology starts by constructing challenge datasets (Figure 1, yellow box) from a target set of knowledge resources. Each probing dataset consists of multiple-choice questions that include a question $\mathbf{q}$ and a set of answer choices or candidates $\left{a_{1}, \ldots a_{N}\right}$. This section describes in detail the 5 datasets we build (grouped into WordNetQA and DictionaryQA), drawn from two publicly-available resources: WordNet <em>Miller (1995)</em> and the GNU Collaborative International Dictionary of English (GCIDE). ${ }^{4}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A portion of the WordNet ISA graph (top) and an example distractor function $\operatorname{Distr}(\tau)$ (bottom) used to generate distractor choices $\left{a_{1}^{\prime}, a_{2}^{\prime}\right}$ for a question $\mathbf{q}$ based on information in the graph.</p>
<p>For convenience, we will describe each source of expert knowledge as a directed, edge-labeled graph $G$. The nodes of this graph are $\mathcal{V}=\mathcal{C} \cup$ $\mathcal{W} \cup \mathcal{S} \cup \mathcal{D}$, where $\mathcal{C}$ is a set of atomic concepts, $\mathcal{W}$ a set of words, $\mathcal{S}$ a set of sentences, and $\mathcal{D}$ a set of definitions (see Table 1 for details for WordNet and GCIDE). Each edge of $G$ is directed from an atomic concept in $\mathcal{C}$ to another node in $V$, and is labeled with a relation, such as hypernym or isa ${ }^{\uparrow}$, from a set of relations $\mathcal{R}$ (see Table 1).</p>
<p>When defining our probe question templates, it will be useful to view $G$ as a set of (relation, source, target) triples $\mathcal{T} \subseteq \mathcal{R} \times \mathcal{C} \times \mathcal{V}$. Due to their origin in an expert knowledge source, such triples preserve semantic consistency. For instance, when the relation in a triple is def, the corresponding edge maps a concept in $\mathcal{C}$ to a definition in $\mathcal{D}$.</p>
<p>We rely on two heuristic functions, defined below for each individual probe: $\operatorname{GEN}<em 1="1">{\mathcal{Q}}(\tau)$, which generates gold question-answer pairs $(\mathbf{q}, \mathbf{a})$ from a set of triples $\tau \subseteq \mathcal{T}$ and question templates $\mathcal{Q}$, and $\operatorname{Distr}\left(\tau^{\prime}\right)$, which generates distractor answers choices $\left{a</em>(\tau)$.}^{\prime}, \ldots a_{N-1}^{\prime}\right}$ based on another set of triples $\tau^{\prime}$ (where usually $\tau \subset \tau^{\prime}$ ). For brevity, we will use $\operatorname{GEN}(\tau)$ to denote $\operatorname{GEN}_{\mathcal{Q}</p>
<p>In generating our dataset probes, our general strategy is to build automatic silver-standard training and developments sets, in the latter case at a large scale to facilitate detailed and controlled analysis of model performance. As discussed be-</p>
<table>
<thead>
<tr>
<th>Probe Type</th>
<th>Triple Input $\tau$</th>
<th>Generation Templates from $\mathcal{Q}$</th>
<th>Example Questions and Answers $(q, s)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Definitions: Defining words in context.</td>
<td>$\left(\operatorname{def}, c_{i}, d\right)$ $\left(\mathrm{ex}, c_{i}, s\right)$ $\left(\right.$ word, $c_{i}, w$</td>
<td>q. In the sentence $[s]$, the word $[w]$ is best defined as: $\boldsymbol{a} .[d]$</td>
<td>q. In the sentence The baby nestled her head, the word nestled is best defined as: a. position comfortably</td>
</tr>
<tr>
<td>Hypernymy: $\mathrm{ISA}^{\uparrow}$ reasoning in context (symbolically $c_{i} \sim&gt;c_{i^{\prime}}$ )</td>
<td>$\left(\operatorname{def}, c_{i^{\prime}}, d\right)$ $\left(\operatorname{isa}^{\uparrow}, c_{i}, c_{i^{\prime}}\right)$ $\left(\mathrm{ex}, c_{i}, s\right)$ $\left(\right.$ word, $c_{i}, w$ ) $\left(\right.$ word, $\left.c_{i^{\prime}}, w^{\prime}\right)$</td>
<td>q. In $[s]$, the word or concept $[w]$ is best described as a type of $\boldsymbol{a}$. $\left[\boldsymbol{w}^{\prime}\right]$ defined as $[d]$</td>
<td>q. In The thief eluded the police, the word or concept eluded is best described as a type of a. escape event defined as to run away from..</td>
</tr>
<tr>
<td>Hyponymy: $\mathrm{ISA}^{\downarrow}$ reasoning given context. (symbolically $c_{i}&lt;\sim c_{i^{\prime}}$ )</td>
<td>$\left(\operatorname{def}, c_{i^{\prime}}, d\right)$ $\left(\operatorname{isa}^{\downarrow}, c_{i}, c_{i^{\prime}}\right)$ $\left(\mathrm{ex}, c_{i}, s\right)$ $\left(\right.$ word, $c_{i}, w$ ) $\left(\right.$ word, $\left.c_{i}, w^{\prime}\right)$</td>
<td>q. Given the context $[s]$, which of the following word or concept is a specific type of $[w]$ a. $\left[\boldsymbol{w}^{\prime}\right]$ defined as $[d]$</td>
<td>q. Given the context they awaited her arrival, which of the following word or concept is a specific type of arrival? a. crash landing, defined as an emergency landing under circumstances where...</td>
</tr>
<tr>
<td>Synonymy: Related words.</td>
<td>$\left(\operatorname{def}, c_{i}, d\right)$ $\left(\right.$ word, $c_{i}, w_{1}$ ) $\left(\right.$ word, $\left.c_{i}, w_{2}\right)$</td>
<td>q. Which words best correspond to $[d]$ ? a. $\left[\left{w_{1}, w_{2}, \ldots\right}\right]$</td>
<td>q. Which set of words best corresponds to the definition a grammatical category in inflected languages governing agreement $\ldots$ a. gender,</td>
</tr>
</tbody>
</table>
<p>Table 2: Details of the GEN $(\tau)$ function used to construct gold question-answer pairs $(\mathbf{q}, \mathbf{a})$ from a triple graph $G$.
low, we also provide estimates of human performance on our test sets, and in some cases introduce smaller gold-standard test sets to allow for a direct comparison with model performance.</p>
<h3>3.1 WordNetQA</h3>
<p>WordNet is a publicly-available English lexical database consisting of around 117 k concepts, which are organized into groups of synsets that each contain a gloss (i.e., a definition), a set of representative English words (called lemmas), and, in around 33 k synsets, example sentences. In addition, many synsets have ISA links to other synsets that express complex taxonomic relations. Figure 2 shows an example and Table 1 summarizes how we formulate WordNet as a set of triples $\mathcal{T}$ of various types. These triples together represent a directed, edge-labeled graph $G$.</p>
<p>Our main motivation for using WordNet, as opposed to a resource such as ConceptNet (Havasi et al., 2007), is the availability of glosses $(\mathcal{D})$ and example sentences $(\mathcal{S})$, which allows us to construct natural language questions that contextualize the types of concepts we want to probe. For example, when probing whether a model has knowledge of a concept such as bank (a financial institution), we provide an example sentence he cashed a check at the bank, to help disambiguate the particular sense of bank we are probing. Sentential contexts also provide additional hints to models in cases of rare or infrequent concepts. ${ }^{5}$ Since</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>WordNet is the most authoritative and widelyused knowledge resource in NLP, it also has the advantage of having mappings into other knowledge resources (Niles and Pease, 2001; Navigli and Ponzetto, 2010; Tandon et al., 2017), which allows for easily extending our probes to other domains and phenomena.</p>
<p>Example Generation GEN $(\tau)$. We build 4 individual datasets based on semantic relations native to WordNet: hypernymy (i.e., generalization or ISA reasoning up a taxonomy, $\mathrm{ISA}^{\uparrow}$ ), hyponymy (ISA ${ }^{\downarrow}$ ), synonymy, and definitions. To generate a set of questions in each case, we employ a number of rule templates $\mathcal{Q}$ that operate over tuples. A subset of such templates is shown in Table 2 and were designed to mimic naturalistic (i.e., human authored) questions we observed in our science benchmarks.</p>
<p>For example, suppose we wish to create a question $\mathbf{q}$ about the definition of a target concept $c \in \mathcal{C}$. We first select a question template from $\mathcal{Q}$ that first introduces the concept $c$ and its lemma $l \in \mathcal{W}$ in context using the example sentence context $s \in \mathcal{S}$, and then asks to identify the corresponding WordNet gloss $d \in \mathcal{D}$, which serves as the gold answer a. The same is done for ISA reasoning; each question about a hypernym/hyponym relation between two concepts $c \rightarrow^{\uparrow / \downarrow} c^{\prime} \in \mathcal{T}_{i}$ (e.g., $\operatorname{dog} \rightarrow^{\uparrow / \downarrow}$ animal/terrier) first introduces a context for $c$ and then asks for an answer that identifies $c^{\prime}$ (which is also provided with a gloss so as to contain all available context).</p>
<p>In the latter case, the rules $\left(\right.$ isa $\left.^{r}, c, c^{\prime}\right) \in \mathcal{T}_{i}$ in Table 2 cover only direct ISA links from $c$ in direction $r \in{\uparrow, \downarrow}$. In practice, for each $c$ and $r$, we construct tests that cover the set $\operatorname{HOPS}(c, r)$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Target Concept</th>
<th style="text-align: center;">Example Question</th>
<th style="text-align: center;">Inferences <br> (target answers in symbolic form)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">trouser.n.01, gloss: a garment extending from the waist to the knee or ankle covering each leg...</td>
<td style="text-align: center;">q. In he had a sharp crease in his trousers, the word/phrase trousers is best defined as a type of</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { trouser.n. } 01 \Rightarrow \text { consumer_goods.n. } 01 \ &amp; \text { trouser.n. } 01 \Rightarrow \text { garment.n. } 01 \ &amp; \text { trouser.n. } 01 \Rightarrow \text { commodity.n. } 01 \ &amp; \text { trouser.n. } 01 \Rightarrow \text { clothing.n. } 01 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">oppose.v.06, gloss: be resistant to</td>
<td style="text-align: center;">q. In the sentence or expression The board opposed his motion, the following is a more specific type of opposed [or opposition]</td>
<td style="text-align: center;">oppose.v.06 <br> weto.v. 01 <br> oppose.v.06 <br> weto.v. 01 <br> oppose.v.06 <br> demonstrate.v. 04</td>
</tr>
<tr>
<td style="text-align: center;">poet_laureate.n.01, gloss: a poet who is ... holding an honorary position...</td>
<td style="text-align: center;">q. Given the fragment he is the poet laureate of Arkansas, poet laureate ... is best described as a type of</td>
<td style="text-align: center;">poet_laureate.n.01=&gt;poet.p poet_laureate.n.01=&gt;communicator.n. 01 poet_laureate.n.01=&gt;writer.n. 01</td>
</tr>
</tbody>
</table>
<p>Table 3: Semantic clusters for three target concepts, involving ISA reasoning.
of all direct as well as derived ISA relations of $c$ :
$\operatorname{HOPS}(c, r):=\left{\left(\right.\right.$ is $\left.\left.a^{r}, c, c^{\prime}\right) \in \mathcal{T}_{i}\right} \cup \operatorname{HOPS}\left(c^{\prime}, r\right)$
This allows us to evaluate the extent to which models are able to handle complex forms of reasoning that require several inferential steps or hops. ${ }^{6}$</p>
<p>Distractor Generation: $\operatorname{DISTR}\left(\tau^{\prime}\right)$. Figure 2 shows an example of how distractors are generated, relying on similar principles as above. For each concept $c$, we choose 4 distractor answers that are close in the WordNet semantic space. For example, when constructing hypernymy tests for $c$ from the set $\operatorname{HOPS}(c, \uparrow)$, we draw distractors from $\operatorname{HOPS}(c, \downarrow)$, as well as from the $\ell$-deep sister family of $c$, defined as follows. The 1-deep sister family is simply $c$ 's siblings or sisters, i.e., the other children $\tilde{c} \neq c$ of the parent node $c^{\prime}$ of $c$. For $\ell&gt;1$, the $\ell$-deep sister family also includes all descendants of each $\tilde{c}$ up to $\ell-1$ levels deep, denoted $\operatorname{HOPS}_{\ell-1}(\tilde{c}, \downarrow)$. Formally:</p>
<p>$$
\begin{aligned}
\operatorname{SISTER}<em _ell-1="\ell-1">{\ell}(c): &amp; =\left{x \in \operatorname{HOPS}</em>, \downarrow) \mid\right. \
&amp; \left(\text { isa } \left.\uparrow, c, c^{\prime}\right) \in \mathcal{T}}(\tilde{c<em i="i">{i}\right. \
&amp; \left.\left(\text { isa } \left.\uparrow, \tilde{c}, c^{\prime}\right) \in \mathcal{T}</em> \neq c\right}
\end{aligned}
$$}, \tilde{c</p>
<p>For definitions and synonyms, we build distractors from all of these sets (with a similar depth limit for SISTER distractors), enabling a systematic investigation via a wide range of distractors.</p>
<h3>3.1.1 Perturbations and Semantic Clusters</h3>
<p>For each concept $c$ (an atomic WordNet synset) and probe type (definitions, hypernymy, etc.), we have a wide variety of questions related to $c$ that manipulate 1) the complexity of reasoning that is involved (e.g., the number of inferential hops) and</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2) the types of distractors (or distractor perturbations) that are employed. We call such sets semantic clusters.</p>
<p>Table 3 shows three examples, capturing ISA reasoning about the following target concepts: trousers, opposing, and poet laureate. Such clusters enable new types of evaluation of the comprehensiveness and consistency of a model's knowledge of target concepts.</p>
<h3>3.1.2 Summary of Probe Datasets</h3>
<p>Details of the individual datasets, including average cluster sizes, are summarized in Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Probe</th>
<th style="text-align: center;"># Questions <br> (Unique / w Perturb.)</th>
<th style="text-align: center;">Cluster Size <br> (Avg.)</th>
<th style="text-align: center;"># Synsets <br> (or concepts)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Hypernymy</td>
<td style="text-align: center;">$19,705 / 35,094$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">7,849</td>
</tr>
<tr>
<td style="text-align: left;">Hyponymy</td>
<td style="text-align: center;">$6,697 / 35,243$</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">3,452</td>
</tr>
<tr>
<td style="text-align: left;">Synonymy</td>
<td style="text-align: center;">$28,254 / 91,069$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">15,632</td>
</tr>
<tr>
<td style="text-align: left;">Definitions</td>
<td style="text-align: center;">$31,380 / 148,662$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">15,159</td>
</tr>
<tr>
<td style="text-align: left;">WordSense</td>
<td style="text-align: center;">$\sim 7,000 /-$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\sim 7,000$</td>
</tr>
</tbody>
</table>
<p>Table 4: Details of our dataset probes, including both the number of unique (q.a) pairs (for WordNetQA) and the number of all questions including distractor choice perturbations (w Perturb.).</p>
<p>From these sets, we follow Richardson et al. (2020) in allocating a maximum of 3 k examples for inoculating the models in the manner described in the next section (i.e., for continuing to train QA models and introduce them to the format of our probes), and reserve the rest for development and testing. In particular, we build large development sets, which are important for performing detailed analysis and cluster-based evaluation.</p>
<h3>3.1.3 Human Performance</h3>
<p>We report human scores on the individual test sets in WordNetQA (see bottom of Table 7). This is done in two ways.</p>
<p>First, for our test sets generated for definitions and synonyms that cover a large set of disconnected concepts in the WordNet graph and where</p>
<p>it is infeasible to annotate individual instances of concepts, we estimate human performance by having crowd-workers on Amazon Mechanical Turk answer a random sample of 500 test questions. Scores are computed by taking the majority vote for each question among 5 annotators. This follows exactly the evaluation protocol employed by nangia2019evaluating and is a conservative estimate in that crowd annotators received virtually no training and no qualification exam before participating in the task.</p>
<p>Second, for our hypernymy and hyponymy test sets, which cover a smaller number of denselyconnected concepts, we annotated smaller goldstandard test sets that include a sample of around 2,000 random questions that cover a large proportion of the concepts being probed and that have high human performance. To do this, we follow the annotation strategy described above, and greedily apply filtering to remove questions incorrectly answered by human annotators, which follows prior work on building evaluation sets for MCQA <em>Mihaylov et al. (2018); Talmor et al. (2019b); Khot et al. (2020)</em>.</p>
<h3>3.2 DictionaryQA</h3>
<p>The DictionaryQA dataset is created from the English dictionary GCIDE built largely from the Webster’s Revised Unabridged Dictionary webster1913, which has previously been used in other NLP studies due to its large size and public availability hill2016. Each dictionary entry consists of a word, its part-of-speech, its definition, and an optional example sentence, as shown for an example in Table 5.</p>
<table>
<thead>
<tr>
<th>GCIDE Dictionary Entries</th>
</tr>
</thead>
<tbody>
<tr>
<td>word: gift, pos: n., definition: Anything given; anything</td>
</tr>
<tr>
<td>voluntarily transferred by one person to another without</td>
</tr>
<tr>
<td>compensation; a present; entry example: None.</td>
</tr>
<tr>
<td>word: gift, pos: n., definition: A bribe; anything given to</td>
</tr>
<tr>
<td>corrupt. entry example: None.</td>
</tr>
<tr>
<td>word: gift, pos: n., definition: Some exception inborn qual-</td>
</tr>
<tr>
<td>ity or characteristic; a striking or special talent or aptitude;..</td>
</tr>
<tr>
<td>entry example: the gift of wit; a gift for speaking.</td>
</tr>
</tbody>
</table>
<p>Table 5: Example dictionary entries for the word gift.</p>
<p>Overall, 33k entries (out of a total of 155k) contain example sentences/usages. As with the WordNet probes, we focus on this subset so as to contextualize each word being probed. Since GCIDE does not have ISA relations or explicit synsets, we take each unique entry to be a distinct sense. Our probe centers around word-sense disambiguation.</p>
<p>To buildQA examples, we use the same generation templates for definitions exemplified in Table 2 for WordNetQA. To construct distractors, we simply take alternative definitions for the target words that represent a different word sense (e.g., the alternative definitions of gift in Table 5), and randomly chosen definitions if needed to create a 5-way multiple choice question. As above, we reserve a maximum of 3k examples for training, and use the same amount for development.</p>
<p>Our initial attempts at building this dataset via standard random splitting resulted in certain systematic biases, revealed by high performance of the choice-only model we used as a control. Among other factors, we found the use of definitions from entries without example sentences as distractors (see again Table 5) to have a surprising correlation with such biases. Filtering such distractors helped improve the quality of this probe.</p>
<p>For assessing human performance, we annotated a smaller gold-standard test set consisting of around 1,100 questions using the crowd-sourcing elicitation setup described in Section 3.1.</p>
<h2>4 Probing Methodology and Modeling</h2>
<p>Given the probes above, we now can start to answer the empirical questions posed at the beginning. Our main focus is on looking at transformer-based MCQA models trained on science benchmarks in Table 6. We start with our target MCQA models, as well as several control baselines.</p>
<h3>4.1 Task Definition and Modeling</h3>
<p>Given a dataset $D=\left{\left(\mathbf{q}^{(d)},\left{a_{1}^{(d)}, \ldots, a_{N}^{(d)}\right}\right)\right}<em i="i">{d}^{|D|}$ consisting of pairs of questions stems $\mathbf{q}$ and answer choices $a</em>$. Throughout this paper, we look at 5-way multiple-choice problems (i.e., where each $N=5$ ).}$, the goal is to find the correct answer $a_{i^{*}}$ that correctly answers each $\mathbf{q</p>
<p>Question+Answer Encoder. Our investigation centers around the use of the transformer-based BERT encoder and fine-tuning approach of devlin2019 (see also radford2018). For each question and individual answer pair $q_{a_{i}}^{(j)}$, we assume the following rendering of this input:</p>
<p>$$
q_{a_{i}}^{(j)}:=\left[\begin{array}{llll}
\mathrm{CLS}
\end{array}\right] \mathbf{q}^{(j)} \quad[\mathrm{SEP}] \quad a_{i}^{(j)} \quad[\mathrm{SEP}]
$$</p>
<p>This is run through the pre-trained BERT encoder to generate a representation for $q_{a_{i}}^{(j)}$ using the hidden state representation for CLS (i.e., the classifier</p>
<p>token): $\mathbf{c}<em _theta__i="\theta_{i">{i}^{(j)}=\operatorname{BERT}\left(q</em>}}^{(j)}\right) \in \mathbb{R}^{H}$. The probability of a given answer $p_{i}^{(j)}$ is then standardly computed using an additional classification layer over $\mathbf{c<em>{j}$, which is optimized (along with the full transformer network) by taking the final loss of the probability of each correct answer $p</em>{i^{<em>}}$ over all answer choices, i.e., $\mathcal{L}=\sum_{d \in|D|}-\log p_{i^{</em>}}^{(d)}$.</p>
<p>We specifically use BERT-large uncased with whole-word masking, as well as the RoBERTa-large model from Liu et al. (2019b), which is a more robustly trained version of the original BERT model. Our system uses the implementations provided in AllenNLP <em>Gardner et al. (2018)</em> and Huggingface <em>Wolf et al. (2019)</em>.</p>
<p>Baselines and Sanity Checks. When creating synthetic datasets, it is important to ensure that systematic biases, or annotation artifacts <em>Gururangan et al. (2018)</em>, are not introduced into the resulting probes and that the target datasets are sufficiently challenging (or good, in the sense of <em>Hewitt and Liang (2019)</em>). To test for this, we use several of the MCQA baseline models first introduced in Mihaylov et al. (2018), which take inspiration from the LSTM-based models used in Conneau et al. (2017) for NLI and various partial-input baselines based on these models.</p>
<p>Following Mihaylov et al. (2018)'s notation, for any sequence $s$ of tokens in $\left{q^{(j)}, a_{1}^{(j)}, \ldots, a_{N}^{(j)}\right} \in$ $D$, an encoding of $s$ is given as the following:</p>
<p>$$
h_{s}^{(j)}=\operatorname{BiLSTM}(\operatorname{EMBED}(s)) \in \mathbb{R}^{|s| \times 2 h}
$$</p>
<p>where $h$ is the dimension of the hidden state in each directional network, and $\operatorname{EMBED}(\cdot)$ assigns a token-level embeddings to each token in $s$. A contextual representation for each $s$ is then built by applying an element-wise max operation over $h_{s}$ as follows:</p>
<p>$$
r_{s}^{(j)}=\max \left(h_{s}^{(j)}\right) \in \mathbb{R}^{2 h}
$$</p>
<p>With these contextual representations, different baseline models can be constructed. For example, a Choice-Only model, a variant of the well-known hypothesis-only baseline used in NLI <em>Poliak et al. (2018b)</em>, scores each choice $c_{i}$ in the following way: $\alpha_{i}^{(j)}=\mathbf{W}^{T} r_{c_{i}}^{(j)} \in \mathbb{R}$ for $\mathbf{W}^{T} \in \mathbb{R}^{2 h}$ independently of the question and assigns a probability to each answer $p_{i}^{(j)} \propto e^{\alpha_{i}^{(j)}}$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>A slight variant of this model, the Choice-to-choice model, tries to single out a given answer choice relative to other choices by scoring all choice pairs $\alpha_{i, i^{\prime}}^{(j)}=\operatorname{Att}\left(r_{c_{i}}^{(j)}, r_{c_{i^{\prime}}}^{(j)}\right) \in \mathbb{R}$ using a learned attention mechanism ATT and finding the choice with the minimal similarity to other options (for full details, see their original paper). In using these partial-input baselines, which we train directly on each target probe, we can check whether systematic biases related to answer choices were introduced into the data creation process.</p>
<p>A Question-to-choice model, in contrast, uses the contextual representations for each question and individual choice and an attention model ATT model to get a score $\alpha_{q, i}^{(j)}=\operatorname{Att}\left(r_{q}^{(j)}, r_{c_{i}}^{(j)}\right) \in \mathbb{R}$ as above. Here we also experiment with using ESIM <em>Chen et al. (2017)</em> to generate the contextual representations for $q, c_{i}$ (which includes token-wise attention), as well as a VecSimilarity model that measures the average (cosine) vector similarity between question and answer tokens: $\alpha_{q, i}^{(j)}=\operatorname{Sim}\left(\operatorname{EMBED}\left(q^{(j)}\right), \operatorname{EMBED}\left(c_{i}^{(j)}\right)\right)$. These sets of baselines, which have been shown to be weak on other benchmark MCQA tasks, are primarily used not as competitive models but to check for artifacts between questions and answers that are not captured in the partial-input baselines. This helps ensure that the overall MCQA probing tasks are sufficiently difficult.</p>
<h3>4.2 Inoculation and Pre-training</h3>
<p>Using the various models introduced above, we train these models on benchmark tasks in the science domain and look at model performance on our probes with and without additional training on samples of probe data, building on the idea of inoculation from Liu et al. (2019a). Model inoculation is the idea of continuing to train models on new challenge tasks (in our cases, separately for each probe) using only a small amount of examples. Unlike in ordinary fine-tuning, the goal is not to learn an entirely re-purposed model, but to improve on (or vaccinate against) particular phenomena (e.g., our synthetic probes) that potentially deviate from a model's original training distribution.</p>
<p>Following a variant proposed by Richardson et al. (2020), for each pre-trained (science) model and architecture $M_{a}$ we continue training the model on $k$ new probe examples (with a maximum of $k=3000$ ) under a set of hyper-parameter configurations ${1, \ldots, J}$ and identify, for each $k$, the</p>
<p>model $M_{*}^{a, k}$ with the best aggregate performance $S$ on the original (orig) and new task:</p>
<p>$$
M_{*}^{a, k}=\underset{M \in\left{M_{1}^{a, k}, \ldots, M_{J}^{a, k}\right}}{\arg \max } \operatorname{AVG}\left(S_{\text {new }}(M), S_{\text {orig }}(M)\right)
$$</p>
<p>As in Richardson et al. (2020), we performed comprehensive hyper-parameter searches that target especially learning rates and # training iterations.</p>
<p>Using this methodology, we can see how much exposure to new data it takes for a given model to master a new task, and whether there are phenomena that stress particular models (e.g., lead to catastrophic forgetting of the original task). Given the restrictions on the number of fine-tuning examples, our assumption is that when models are able to maintain good performance on their original task during inoculation, the quickness with which they are able to learn the inoculated task provides evidence of prior competence, which is precisely what we aim to probe. To measure past performance, we define a model's inoculation cost as the difference in the performance of this model on its original task before and after inoculation, which serves as a control on the target QA model.</p>
<p>We pre-train on an aggregated training set of all benchmark science exams in Table 6. ${ }^{b}$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Science Datasets</th>
<th style="text-align: center;">#Questions</th>
<th style="text-align: center;">$N$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OpenBookQA Mihaylov et al. 2018</td>
<td style="text-align: center;">4,957</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">SciQ Welbl et al. 2017</td>
<td style="text-align: center;">11,675</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">TextBookQA Kembhavi et al. 2017</td>
<td style="text-align: center;">7,611</td>
<td style="text-align: center;">$4 / 5$</td>
</tr>
<tr>
<td style="text-align: left;">ARC Dataset++ Clark et al. 2018</td>
<td style="text-align: center;">4,035</td>
<td style="text-align: center;">$4 / 5$</td>
</tr>
<tr>
<td style="text-align: left;">MCQL Liang et al. 2018</td>
<td style="text-align: center;">6,318</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Science Collection (total)</td>
<td style="text-align: center;">34,596</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>Table 6: The MCQA training datasets used. #Question denotes the number of training samples in our version of each dataset, $N$ the number of choices.</p>
<p>In line with our goal of obtaining insights into the strongest QA models, we first pre-trained our RoBERTa-large model on the RACE dataset (Lai et al., 2017), a recipe used by several leading models on science benchmarks. and created an aggregate development set of $\sim 4 \mathrm{k}$ science questions for evaluating overall science performance and inoculation cost. To handle a varying number of answer choices in these sets, we made all sets 5 -way by adding empty answers as needed. We also experimented with a slight variant of inoculation, called</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>add-some inoculation, which involves balancing the inoculation training sets with naturalistic science questions. We reserve the MCQL dataset in Table 6 for this purpose, and experiment with balancing each probe example with one science example ( $x 1$ matching) and adding twice as many science questions ( $x 2$ matching, up to 3 k ) for each new example.</p>
<h3>4.3 Evaluating Model Competence</h3>
<p>We use instance-level accuracy, the standard overall accuracy of correct answer prediction (as in Table 7). In addition, we also propose to measure a model's cluster-level (or strict cluster) accuracy, which requires correctly answering all questions in a semantic cluster (cf. Section 3.1.1).</p>
<p>Our cluster-based analysis is motivated by the idea that if a model truly knows the meaning of a given concept then it should be able to answer arbitrary questions about this concept without sensitivity to varied distractors. While our strict cluster metric is simplistic, it takes inspiration from work on visual QA (Shah et al., 2019), and allows us to evaluate a model's consistency and robustness across our different probes, and to get insight into whether errors are concentrated on a small set of concepts or widespread across different clusters.</p>
<p>The ability of a model to answer several questions about a single concept can be thought of as a type of certificate (i.e., further justification and demonstration) of general understanding of that concept in the sense of Ranta (2017).</p>
<h2>5 Results and Findings</h2>
<p>We begin with an assessment to ensure that our probes are sufficiently difficult to provide meaningful insights into strong models (Section 5.1), then assess the strength of pre-trained QA models (Section 5.2) and whether they can be effectively inoculated (Section 5.3), and finally present a cluster-based consistency analysis (Section 5.4).</p>
<h3>5.1 Are our probes sufficiently challenging?</h3>
<p>Partial-input baseline models, Choice-Only and Choice-to-Choice, generally performed poorly on our probes (cf. Table 7, group 1), indicating limited biases in distractor generation. Initial versions of DictionaryQA had unforeseen biases partly related to distractors sampled from entries without example sentences (cf. Section 3.2), which resulted in high (56\%) Choice-Only-GloVe scores</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">WordNetQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DictionaryQA Word sense (Dev/Test)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Definitions (Dev/Test)</td>
<td style="text-align: center;">Synonymy <br> (Dev/Test)</td>
<td style="text-align: center;">Hypernymy <br> (Dev/Test)</td>
<td style="text-align: center;">Hyponymy <br> (Dev/Test)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Group 1: Baselines (direct training on 3k probes)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">19.9 / 20.0</td>
<td style="text-align: center;">19.8 / 19.8</td>
<td style="text-align: center;">19.9 / 20.0</td>
<td style="text-align: center;">20.2 / 21.0</td>
<td style="text-align: center;">20.0 / 19.0</td>
</tr>
<tr>
<td style="text-align: center;">Choice-Only-GloVe</td>
<td style="text-align: center;">26.6 / 26.1</td>
<td style="text-align: center;">36.9 / 36.1</td>
<td style="text-align: center;">42.5 / 46.0</td>
<td style="text-align: center;">34.3 / 34.4</td>
<td style="text-align: center;">35.0 / 32.1</td>
</tr>
<tr>
<td style="text-align: center;">Choice-Only-BERT</td>
<td style="text-align: center;">22.9 / 23.2</td>
<td style="text-align: center;">41.1 / 39.4</td>
<td style="text-align: center;">63.8 / 54.4</td>
<td style="text-align: center;">35.7 / 35.1</td>
<td style="text-align: center;">36.6 / 31.7</td>
</tr>
<tr>
<td style="text-align: center;">Choice-Only-RoBERTa</td>
<td style="text-align: center;">26.8 / 28.6</td>
<td style="text-align: center;">40.9 / 40.1</td>
<td style="text-align: center;">62.3 / 57.3</td>
<td style="text-align: center;">37.8 / 37.5</td>
<td style="text-align: center;">38.0 / 31.7</td>
</tr>
<tr>
<td style="text-align: center;">Choice-to-Choice-GloVe</td>
<td style="text-align: center;">26.4 / 28.1</td>
<td style="text-align: center;">40.1 / 35.0</td>
<td style="text-align: center;">47.0 / 35.5</td>
<td style="text-align: center;">35.4 / 36.1</td>
<td style="text-align: center;">37.3 / 33.3</td>
</tr>
<tr>
<td style="text-align: center;">Question-to-Choice-VecSimilarity</td>
<td style="text-align: center;">33.4 / 32.1</td>
<td style="text-align: center;">31.7 / 30.7</td>
<td style="text-align: center;">28.9 / 33.0</td>
<td style="text-align: center;">26.2 / 28.8</td>
<td style="text-align: center;">29.5 / 33.1</td>
</tr>
<tr>
<td style="text-align: center;">Group 2: Task-Specific (non-transformer) Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Question-to-Choice-GloVe</td>
<td style="text-align: center;">53.6 / 51.8</td>
<td style="text-align: center;">57.3 / 55.3</td>
<td style="text-align: center;">50.4 / 47.0</td>
<td style="text-align: center;">61.6 / 64.2</td>
<td style="text-align: center;">53.2 / 53.5</td>
</tr>
<tr>
<td style="text-align: center;">Question-to-Choice-ELMO</td>
<td style="text-align: center;">42.3 / 41.6</td>
<td style="text-align: center;">58.6 / 56.0</td>
<td style="text-align: center;">56.0 / 51.5</td>
<td style="text-align: center;">54.8 / 56.3</td>
<td style="text-align: center;">51.6 / 52.1</td>
</tr>
<tr>
<td style="text-align: center;">Group 3: Science Models (no fine-tuning or direct training on probes)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ESIM-GloVe</td>
<td style="text-align: center;">27.5 / 28.3</td>
<td style="text-align: center;">25.1 / 26.1</td>
<td style="text-align: center;">27.0 / 33.0</td>
<td style="text-align: center;">23.6 / 24.8</td>
<td style="text-align: center;">31.9 / 32.5</td>
</tr>
<tr>
<td style="text-align: center;">ESIM-ELMO</td>
<td style="text-align: center;">23.1 / 24.0</td>
<td style="text-align: center;">21.1 / 21.5</td>
<td style="text-align: center;">27.1 / 32.7</td>
<td style="text-align: center;">18.0 / 18.5</td>
<td style="text-align: center;">28.3 / 31.5</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">54.1 / 55.7</td>
<td style="text-align: center;">58.8 / 60.9</td>
<td style="text-align: center;">43.2 / 51.0</td>
<td style="text-align: center;">24.0 / 27.0</td>
<td style="text-align: center;">43.0 / 42.9</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">74.1 / 77.1</td>
<td style="text-align: center;">61.1 / 64.2</td>
<td style="text-align: center;">53.2 / 71.0</td>
<td style="text-align: center;">48.5 / 58.6</td>
<td style="text-align: center;">53.0 / 55.1</td>
</tr>
<tr>
<td style="text-align: center;">Group 4: Science Models (best aggregate model $M_{*}$ fine-tuned on probes; inoculation cost is shown in parenthesis)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ESIM-GloVe</td>
<td style="text-align: center;">46.2 / 42.4 (-6.27)</td>
<td style="text-align: center;">50.4 / 47.3 (-6.84)</td>
<td style="text-align: center;">56.6 / 52.9 (-5.69)</td>
<td style="text-align: center;">59.1 / 61.1 (-5.10)</td>
<td style="text-align: center;">50.0 / 55.3 (-7.09)</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">84.0 / 84.1 (-1.15)</td>
<td style="text-align: center;">79.6 / 79.7 (-0.44)</td>
<td style="text-align: center;">73.8 / 82.7 (-0.49)</td>
<td style="text-align: center;">79.8 / 88.0 (-0.92)</td>
<td style="text-align: center;">75.6 / 79.1 (-2.84)</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">89.0 / 89.3 (-1.33)</td>
<td style="text-align: center;">81.2 / 81.3 (-1.31)</td>
<td style="text-align: center;">77.7 / 87.7 (-0.74)</td>
<td style="text-align: center;">81.2 / 89.4 (-1.64)</td>
<td style="text-align: center;">80.0 / 85.9 (-2.23)</td>
</tr>
<tr>
<td style="text-align: center;">Human Performance (estimates)</td>
<td style="text-align: center;">$-/ 91.2 \%$</td>
<td style="text-align: center;">$-/ 87.4 \%$</td>
<td style="text-align: center;">$-/ 96 \%$ !</td>
<td style="text-align: center;">$-/ 95.5 \%$ !</td>
<td style="text-align: center;">$-/ 95.6 \%$ !</td>
</tr>
</tbody>
</table>
<p>Table 7: Instance-level accuracy (\%) of all baselines (group 1), task-specific non-transformer QA models (group 2), pre-trained MCQA models (zero-shot, group 3), and MCQA models after fine-tuning on our probes (group 4). Human scores marked with ${ }^{\dagger}$ represent scores on gold-standard annotated test sets.
before such distractors were filtered out.
One exception is our hypernymy probe where, despite several attempts at filtering data and de-duplicating splits (w.r.t. correct answer and distractor types), the Choice-to-ChoiceBERT/RoBERTa models achieve over $60 \%$ accuracy. The nature of the biases here remains unclear, highlighting the importance of having rigorous baselines as unintended biases in expert knowledge can carry over to resulting datasets. We also note the large gap between the BERT/RoBERTa versus GloVe choice-only models, emphasizing the need for using the best available models even in partial-input baselines.</p>
<p>A more conventional set of Task-Specific QA models (i.e., the LSTM-based Question-toChoice models trained directly on the probes) is not particularly strong on any of the datasets (cf. Table 7, group 2), suggesting our probes are indeed sufficiently challenging and largely immune from overt artifacts. The poor performance of the VecSimilarity (which uses pretrained Word2Vec embeddings without additional training) provides additional evidence of the insufficiency of elementary lexical matching strategies.</p>
<h3>5.2 How strong are pre-trained QA models?</h3>
<p>Non-transformer science models, such as ESIM with GloVe or ELMo, struggle with all probes (cf. Table 7, group 3), often scoring near ran-
dom chance. In sharp contrast, the transformer models have mixed results, the most striking being RoBERTa QA models on the definitions, synonymy and hypernymy test probes (achieving $77 \%, 64 \%$, and $71 \%$ resp.), which substantially outperform even task-specific LSTM models trained directly on the probes. Throughout all of these results, however, model performance is significantly behind human performance.</p>
<p>At first glance, these zero-shot results suggest RoBERTa's high competence on these phenomena. A closer scrutiny enabled by our controlled probes, however, provides a more subtle picture. Each heat map in Figure 3 breaks down the performance of an ESIM or RoBERTa QA model based on the difficulty of the probe dataset (rows) and the nature of the distractors (columns).</p>
<p>Across all datasets and number of hops in the question (i.e., all rows), zero-shot model performance for RoBERTa (bottom-left heat map) is consistently highest among examples with random distractors (the first column) and lowest when distractors are closest in WordNet space (e.g., sister and ISA, or up/down, distractors at distance $k^{\prime}=1$ ). For example, RoBERTa's zero-shot score drops from $88 \%$ to $64 \%$ when going from random distractors to up/down distractors at $k^{\prime}=1$.</p>
<p>Further, model performance also clearly degrades for hypernymy and hyponymy as $k$, the number of hops in the question, increases (see</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />Figure 3: Combined model accuracies on the different WordNetQA datasets (divided by 4 bold lines) broken down (where possible) into number of hops $k$ (rows) and types of distractor sets and hops $k^{\prime}$ (rows) across the different stages of inoculation (# ex.). The 4 dashed lines show some trends related to multi-hop inference.</p>
<p>red dashed boxes). For example, the accuracy on questions involving hyponym reasoning with sister distractors of $k^{\prime}=1$ (column 2) degrades from 47% to only 15% as $k$ increases from 1 to 4. This general tendency persists despite additional finetuning, providing evidence of the limited ability of these models to perform multi-hop inference.</p>
<h3>5.3 Can models be effectively inoculated?</h3>
<p>How well do probe generation templates align with the science training distribution (which we know little about) can significantly impact zeroshot performance <em>Petroni et al. (2019)</em>. Zero-shot results above thus provide a lower bound on model competence on the probed phenomena. We next consider a probe-specific fine-tuning or inoculation step, allowing models to learn target templates and couple this with knowledge acquired during pre-training and science training.</p>
<p>Accuracy after inoculation on 3K probe instances is shown (with inoculation cost in parenthesis) in group 4 of Table 7, for the model with the highest aggregate score on the original task and new probe. Transformer-based models again outperform non-transformer ones, and better models correlate with lower inoculation costs. E.g., on synonymy, ESIM’s inoculation cost is 7%, but only $\sim$1% for BERT and RoBERTa. This emphasizes the high capacity of transformer QA models to absorb new phenomena at minimal cost, as observed earlier for NLI <em>Richardson et al. (2020)</em>.</p>
<p>Figure 4 shows the corresponding learning curves. Transformer QA models learn most tasks quickly while maintaining constant scores on their original tasks (flat dashed lines, plots 1-4), providing evidence of high competence. For BERT and RoBERTa, add-some inoculation (a) improves scores on the probing tasks (solid black and blue lines, plot 1) and (b) minimizes loss on the original task (dashed blue and black lines, plots 2-4).</p>
<p>ESIM behaves quite the opposite (plots 5-6), generally unable to learn individual probes without degrading on its original task. More science data during inoculation confuses it on both tasks.</p>
<p>As the middle-bottom plot of Figure 3 shows, RoBERTa’s performance improves significantly (e.g., from 59% to 77% on 2-hop hyponymy with random distractors) even after inoculation with a mere 100 examples, providing strong evidence of prior competence. After 3k examples, it performs well on virtually all probes. However, results still notably degrade with the number of hops and distractor complexity, as discussed earlier, and we still find its performance to be between 2%-10% behind human performance.</p>
<h3>5.4 Are models consistent across clusters?</h3>
<p>Table 8 shows mixed results for cluster-level accuracy across the different WordNetQA probes. Our best model is rather robust on the definitions</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Inoculation plots with accuracy on challenge tasks (red/circle solid lines) and original tasks (red/circle dashed lines) using the best aggregate model M<sup>w,k</sup> at each k challenge examples (x axis). The effect of using <strong>add-some inoculation</strong> is shown in the blue/square (x1 match) and black/triangle (x2 match) lines.</p>
<p>probe. RoBERTa QA's cluster accuracy is 75%, meaning it can answer <em>all</em> questions correctly for 75% of the target concepts, and that errors are concentrated on a small minority (25%) of concepts. On synonymy and hypernymy, both BERT and RoBERTa are less strong but appear robust on a majority of concepts. In contrast, our best model on hyponymy has an accuracy of only 36%, indicating that the RoBERTa QA models know only partially about a vast majority of concepts, leaving substantial room for further improvement.</p>
<p>We emphasize that these results only provide a crude look into model consistency and robustness. Recalling dataset details in Table 4, probes differ in terms of the average size of clusters. For example, hyponymy, in virtue of having many more questions per cluster, might simply be a much more difficult dataset for our cluster-based evaluation. In addition, such a strict evaluation does not take into account potentially erroneous questions within clusters, which is an important issue that we leave for future work.</p>
<h3>6 Discussion</h3>
<p>We presented a new methodology for automatically building challenge datasets from knowledge</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Definitions</th>
<th>Synonymy</th>
<th>Hypernymy</th>
<th>Hyponymy</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td><em>Strict Cluster Accuracy</em> (Δ)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Choice-Only</td>
<td>14.7 (-12.0)</td>
<td>18.5 (-22.3)</td>
<td>34.6 (-27.6)</td>
<td>4.1 (-33.7)</td>
</tr>
<tr>
<td>ESIM</td>
<td>30.2 (-15.9)</td>
<td>23.3 (-26.9)</td>
<td>29.2 (-27.3)</td>
<td>15.2 (-43.8)</td>
</tr>
<tr>
<td>BERT</td>
<td>68.5 (-15.5)</td>
<td>58.1 (-21.5)</td>
<td>49.0 (-24.8)</td>
<td>34.0 (-45.4)</td>
</tr>
<tr>
<td>RoBERTa</td>
<td>75.0 (-13.9)</td>
<td>61.7 (-19.4)</td>
<td>54.0 (-23.2)</td>
<td>36.7 (-44.4)</td>
</tr>
</tbody>
</table>
<p>Table 8: <strong>Cluster-level</strong> accuracies (%) on the WordNetQA dev. sets for inoculated models and best <strong>Choice-only</strong> model. Δ show the absolute difference in percentage points with instance-level accuracies.</p>
<p>graphs and taxonomies. We introduced several new silver-standard datasets for systematically probing state-of-the-art open-domain QA models. While our focus was on probing definitions and ISA reasoning, the methodology is amendable to any target knowledge resource or QA domain. We see synthetic datasets and our general methodology as an inexpensive supplement to recent large-scale investment in <em>naturalistic</em> QA dataset construction (Zellers et al., 2018; Sakaguchi et al., 2020) to help better understand today's models.</p>
<p>We found transformer-based QA models to have a remarkable ability to reason with complex forms of relational knowledge, both <em>with</em> and <em>without</em> exposure to our new tasks. In the latter case (zero-shot), a newer RoBERTa QA model trained only on benchmark data outperforms several <em>task-specific</em> LSTM-based models trained directly on our probes. When <em>inoculated</em> using small samples (e.g., 100 examples) of probing data, RoBERTa masters many aspects of our probes with virtually no performance loss on its original QA task—which we use as a control on the probing quality.</p>
<p>Since these models seem to already contain considerable amounts of relational knowledge, our simple inoculation strategy, which nudges models to bring out this knowledge explicitly while retaining performance on their original task (hence allowing a fairer probe of its knowledge by giving the model the opportunity to learn the probe format), could serve as a simpler alternative to designing new model architectures explicitly encoding such knowledge (Peters et al., 2019).</p>
<p>Regarding our focus on preserving a model performance on its original task, one might expect that re-training on relevant knowledge should <em>improve</em> performance. Following other work in this area (Richardson et al., 2020; Yanaka et al., 2020), we found that maintaining performance after additional fine-tuning on specialized datasets is already a tall order given that models are susceptible to over-specialization; indeed, similar issues</p>
<p>have been noticed in recent work on large-scale transfer learning (Raffel et al., 2019). We believe that using inoculation for the sole purpose of improving model performance, which is beyond the scope of this paper, would likely require a more sophisticated inoculation protocol. Devising more complex loss functions extending our inoculation strategy to help balance old and new information could help in this endeavor.</p>
<p>The main appeal of automatically generated probes is the ability to systematically manipulate probe complexity, which in turn enables more controlled experimentation as well as new forms of evaluation. It allowed us to study in detail the effect of different types of distractors and the complexity of required reasoning. This study showed that even the best QA models, despite additional fine-tuning, struggle with harder categories of distractors and with multi-hop inferences. For some probes, our cluster-based analysis revealed that errors are widespread across concept clusters, suggesting that models are not always consistent and robust. These results, taken together with our findings about the vulnerability of synthetic datasets to systematic biases and comparison with human scores, suggest that there is much room for improvement and that the positive results should be taken with a grain of salt. Developing better ways to evaluate semantic clusters and model robustness would be a step in this direction.</p>
<p>We emphasize that using synthetic versus naturalistic QA data comes with important trade-offs. While we are able to generate large amounts of systematically controlled data at virtually no cost or need for manual annotation, it is much harder to validate the quality of such data at such a scale and such varying levels of complexity. Conversely, with benchmark QA datasets, it is much harder to perform the type of careful manipulations and cluster-based analyses we report here. While we assume that the expert knowledge we employ, in virtue of being hand-curated by human experts, is generally correct by design, we know that such resources are fallible and error-prone. We propose measuring human performance via small samples of probing data, and leave more scalable methods of removing potential noise and adding human annotation to future work.</p>
<p>One of the overarching goals of our approach to model probing is to uncover whether black box models are able to reason in a consistent and cor-
rect manner. Our assumption, similar to Clark et al. (2020), is that the ability of a model to mimic the input-output behavior of data generated using expert knowledge gives some evidence of correctness in virtue of such data being correct by construction (see discussion by Ranta (2017)). We emphasize, however, that there are limits to how much we can learn through this type of behavioral testing, given that models are susceptible to exploiting systematic biases in synthetic data and the general difficulty of disentangling a model's knowledge acquired during pre-training versus fine-tuning (Talmor et al., 2019a). We therefore see efforts to combine behavioral testing with various other analysis methods (Belinkov and Glass, 2019) that aim to uncover correlations and causal patterns between internal model representations and discrete structures (Chrupała and Alishahi, 2019; Vig et al., 2020; Geiger et al., 2020) as a promising direction for future work. This, in combination with extending our probing strategy to other forms of expert knowledge, could prove to be an effective way to engage others working on linguistics and other areas of AI in state-of-theart NLP research.</p>
<h2>Acknowledgments</h2>
<p>We thank the Action Editor and the three anonymous reviewers for their thoughtful comments and feedback. Thanks also to our colleagues at AI2, in particular Peter Clark, Daniel Khashabi, Tushar Khot, Oyvind Tafjord, and Alon Talmor for feedback on earlier drafts of this work and assistance with various aspects of modeling. Special thanks to Daniel Khashabi for helping with some of the earlier human evaluation experiments.</p>
<h2>References</h2>
<p>Yonatan Belinkov and James Glass. 2019. Analysis Methods in Neural Language Processing: A Survey. Transactions of the Association for Computational Linguistics, 7:49-72.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. 2020. Abductive Commonsense Reasoning. Proceedings of ICLR.</p>
<p>Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, et al. 2018. A</p>
<p>Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset. In Proceedings of Machine Reading for Question Answering (MRQA) Workshop at ACL.</p>
<p>Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for Natural Language Inference. In Proceedings of $A C L$.</p>
<p>Grzegorz Chrupała and Afra Alishahi. 2019. Correlating Neural and Symbolic Representations of Language. In Proceedings of ACL.</p>
<p>Peter Clark. 2015. Elementary School Science and Math Tests as a Driver for AI: take the Aristo Challenge! In Twenty-Seventh IAAI Conference.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457.</p>
<p>Peter Clark, Oren Etzioni, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, Sumithra Bhakthavatsalam, et al. 2019. From 'F' to 'A' on the NY Regents Science Exams: An Overview of the Aristo Project. arXiv preprint arXiv:1909.01958.</p>
<p>Peter Clark, Philip Harrison, and Niranjan Balasubramanian. 2013. A Study of the Knowledge Base Requirements for Passing an Elementary Science Test. In Proceedings of $A K B C$.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as Soft Reasoners over Language. Proceedings of IJCAI.</p>
<p>Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. Proceedings of EMNLP.</p>
<p>Ernest Davis. 2016. How to Write Science Questions that are Easy for People and Hard for Computers. AI magazine, 37(1):13-22.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Didirectional Transformers for Language Understanding. In Proceedings of NAACL.</p>
<p>Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A Deep Semantic Natural Language Processing Platform. arXiv preprint arXiv:1803.07640.</p>
<p>Atticus Geiger, Ignacio Cases, Lauri Karttunen, and Chris Potts. 2019. Posing Fair Generalization Tasks for Natural Language Inference. In Proceedings of EMNLP.</p>
<p>Atticus Geiger, Kyle Richardson, and Christopher Potts. 2020. Modular Representation Underlies Systematic Generalization in Neural Natural Language Inference Models. arXiv preprint arXiv:2004.14623.</p>
<p>Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI Systems with Sentences that Require Simple Lexical Inferences. In Proceedings of $A C L$.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation Artifacts in Natural Language Inference Data. In Proceedings of NAACL.</p>
<p>Catherine Havasi, Robert Speer, and Jason Alonso. 2007. ConceptNet 3: a Flexible, Multilingual Semantic Network for Common Sense Knowledge. In Proceedings of Recent Advances in NLP.</p>
<p>John Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of EMNLP.</p>
<p>Felix Hill, Kyunghyun Cho, Anna Korhonen, and Yoshua Bengio. 2016. Learning to Understand Phrases by Embedding the Dictionary. TACL, 4:1730.</p>
<p>Sujay Kumar Jauhar, Peter Turney, and Eduard Hovy. 2016. Tables as Semi-Structured Knowledge for Question Answering. In Proceedings of ACL.</p>
<p>Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Reading Comprehension Systems. In Proceedings of EMNLP.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2019. How Can We Know What Language Models Know? arXiv preprint arXiv:1911.12543.</p>
<p>Dongyeop Kang, Tushar Khot, Ashish Sabharwal, and Eduard H. Hovy. 2018. AdvEntuRe: Adversarial Training for Textual Entailment with KnowledgeGuided Examples. In Proceedings of ACL.</p>
<p>Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Are you Smarter than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension. In Proceedings of CVPR.</p>
<p>Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. QASC: A Dataset for Question Answering via Sentence Composition. In Proceedings of AAAI.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale Reading Comprehension Dataset from Examinations. In Proceedings of EMNLP.</p>
<p>Chen Liang, Xiao Yang, Neisarg Dave, Drew Wham, Bart Pursel, and C Lee Giles. 2018. Distractor Generation for Multiple Choice Questions Using Learning to Rank. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications.</p>
<p>Nelson F Liu, Roy Schwartz, and Noah A Smith. 2019a. Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets. Proceedings of NAACL.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. RoBERTa: A Robustly Optimized Bert Retraining Approach. arXiv preprint arXiv:1907.11692.</p>
<p>R Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. Proceedings of ACL.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a Suit of Armor Conduct Electricity? a New Dataset for Open Book Question Answering. In Proceedings of EMNLP.</p>
<p>George A Miller. 1995. Wordnet: a Lexical Database for English. Communications of the ACM, 38(11):39-41.</p>
<p>Nikita Nangia and Samuel R Bowman. 2019. Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark. arXiv preprint arXiv:1905.10425.</p>
<p>Roberto Navigli and Simone Paolo Ponzetto. 2010. Babelnet: Building a Very Large Multilingual Semantic Network. In Proceedings of ACL.</p>
<p>Ian Niles and Adam Pease. 2001. Towards a Standard Upper Ontology. In Proceedings of the International Conference on Formal Ontology in Information Systems-Volume.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of EMNLP.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Representations. In Proceedings of NAACL.</p>
<p>Matthew E Peters, Mark Neumann, IV Logan, L Robert, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. 2019. Knowledge Enhanced Contextual Word Representations. In Proceedings of EMNLP.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language Models as Knowledge Bases? In Proceedings of EMNLP.</p>
<p>Mohammad Taher Pilehvar and Jose CamachoCollados. 2019. WiC: the Word-in-Context Dataset for Evaluating Context-sensitive Meaning Representations. In Proceedings of NAACL.</p>
<p>Adam Poliak, Aparajita Haldar, Rachel Rudinger, J Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. 2018a. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation. In Proceedings of EMNLP.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018b. Hypothesis Only Baselines in Natural Language Inference. In Proceedings of *SEM.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Understanding by Generative Pre-training.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Aarne Ranta. 2017. Explainable Machine Translation with Interlingual Trees as Certificates. In Proceedings of the Conference on Logic and Machine Learning in Natural Language.</p>
<p>Sathish Reddy, Dinesh Raghu, Mitesh M Khapra, and Sachindra Joshi. 2017. Generating Natural Language Question-Answer Pairs from a Knowledge Graph using a RNN based Question Generation Model. In Proceedings of EACL.</p>
<p>Kyle Richardson, Hai Hu, Lawrence S Moss, and Ashish Sabharwal. 2020. Probing Natural Language Inference Models through Semantic Fragments. In Proceedings of AAAI.</p>
<p>Ohad Rozen, Vered Shwartz, Roee Aharoni, and Ido Dagan. 2019. Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets. In Proceedings of CoNLL.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale. Proceedings of AAAI.</p>
<p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. SocialIQA: Commonsense Reasoning about Social Interactions. In Proceedings of EMNLP.</p>
<p>Timo Schick and Hinrich Schütze. 2020. Rare words: A Major Problem for Contextualized Embeddings and How to Fix it by Attentive Mimicking. Proceedings of AAAI.</p>
<p>Dominic Seyler, Mohamed Yahya, and Klaus Berberich. 2017. Knowledge Questions from Knowledge Graphs. In Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval.</p>
<p>Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. 2019. Cycle-consistency for Robust Visual Question Answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</p>
<p>Chenglei Si, Shuohang Wang, Min-Yen Kan, and Jing Jiang. 2019. What does BERT Learn from MultipleChoice Reading Comprehension Datasets? arXiv preprint arXiv:1910.12391.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2019a. oLMpics - On what Language Model Pre-training Captures. ArXiv:, arXiv preprint arXiv:1912.13283.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019b. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of NAACL.</p>
<p>Niket Tandon, Gerard De Melo, and Gerhard Weikum. 2017. Webchild 2.0: Fine-grained Commonsense Knowledge Distillation. In Proceedings of ACL.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of NeurIPS, pages 59986008 .</p>
<p>Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias. arXiv preprint arXiv:2004.12265.</p>
<p>Alex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining Nie, Anna Alsop, Shikha Bordia, Haokun Liu, Alicia Parrish, et al. 2019. Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs. In Proceedings of EMNLP.</p>
<p>Noah Webster. 1913. Webster's Revised Unabridged Dictionary of the English Language. G. \&amp; C. Merriam Company.</p>
<p>Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017. Crowdsourcing Multiple Choice Science Questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃl'mi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, and Kentaro Inui. 2020. Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language? In Proceedings of ACL.</p>
<p>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In Proceedings of EMNLP.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\text {b }}$ To save space, we do not report scores for each individual science dataset, but we did verify that our best models achieve results comparable to the state of the art for each dataset.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
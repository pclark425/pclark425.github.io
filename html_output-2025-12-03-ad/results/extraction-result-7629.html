<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7629 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7629</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7629</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-273993773</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.08278v2.pdf" target="_blank">Knowledge Bases in Support of Large Language Models for Processing Web News</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have received considerable interest in wide applications lately. During pre-training via massive datasets, such a model implicitly memorizes the factual knowledge of trained datasets in its hidden parameters. However, knowledge held implicitly in parameters often makes its use by downstream applications ineffective due to the lack of common-sense reasoning. In this article, we introduce a general framework that permits to build knowledge bases with an aid of LLMs, tailored for processing Web news. The framework applies a rule-based News Information Extractor (NewsIE) to news items for extracting their relational tuples, referred to as knowledge bases, which are then graph-convoluted with the implicit knowledge facts of news items obtained by LLMs, for their classification. It involves two lightweight components: 1) NewsIE: for extracting the structural information of every news item, in the form of relational tuples; 2) BERTGraph: for graph convoluting the implicit knowledge facts with relational tuples extracted by NewsIE. We have evaluated our framework under different news-related datasets for news category classification, with promising experimental results.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7629",
    "paper_id": "paper-273993773",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004223749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge Bases in Support of Large Language Models for Processing Web News
14 Nov 2024</p>
<p>Yihe Zhang 
University of Louisiana at Lafayette Lafayette
LouisianaUSA</p>
<p>Nabin Pakka 
University of Louisiana at Lafayette Lafayette
LouisianaUSA</p>
<p>Nian-Feng Tzeng 
University of Louisiana at Lafayette Lafayette
LouisianaUSA</p>
<p>Knowledge Bases in Support of Large Language Models for Processing Web News
14 Nov 2024A382C4E64A48176D75D637E329DF141FarXiv:2411.08278v2[cs.CL]
Large Language Models (LLMs) have received considerable interest in wide applications lately.During pre-training via massive datasets, such a model implicitly memorizes the factual knowledge of trained datasets in its hidden parameters.However, knowledge held implicitly in parameters often makes its use by downstream applications ineffective due to the lack of common-sense reasoning.In this article, we introduce a general framework that permits to build knowledge bases with an aid of LLMs, tailored for processing Web news.The framework applies a rule-based News Information Extractor (NewsIE) to news items for extracting their relational tuples, referred to as knowledge bases, which are then graph-convoluted with the implicit knowledge facts of news items obtained by LLMs, for their classification.It involves two lightweight components: 1) NewsIE: for extracting the structural information of every news item, in the form of relational tuples; 2) BERTGraph: for graphconvoluting the implicit knowledge facts with relational tuples extracted by NewsIE.We have evaluated our framework under different news-related datasets for news category classification, with promising experimental results.</p>
<p>INTRODUCTION</p>
<p>Large Language Models have garnered widespread enthusiasm lately as conversational agents for diverse applications.Noticeably, ChatGPT [46] takes queries and creates responses, Google's Bard [32] can produce creative replies due to its coding and multilingual capabilities, ChatSonic [9] generates update-to-date replies with the aid of Google Search for accurate and informative content creation, among others.Meanwhile, a simple and yet powerful language representation model, known as Bidirectional Encoder Representations from Transformers (BERT) [20], was introduced.As an LLM family member, BERT comprises a stack of transformer encoders and is pre-trained via unlabeled texts.Most LLMs are fineturnable using small amounts of domain-specific data for improving the performance of the given domain tasks.In particular, the BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks [20], such as question answering and language inference, with little changes to the models.</p>
<p>Conventional LLMs usually take massive amounts of unstructured text data for pre-training, before made them available for general applications.Although a pre-trained LLM can produce replies to its inputs (such as Web news items), it does not inherently capture structural and relational information of tokens existing in its every input, presenting an opportunity for improving the quality of replies.To this end, we resort to knowledge bases (KBs), which are created by pre-processing Web news items individually to extract structural and relational information of token in each</p>
<p>Knowledge Base</p>
<p>Large Language Model</p>
<p>[CLS]</p>
<p>[SEP]</p>
<p>[NSP]</p>
<p>Figure 1: Illustration of Knowledge Base (left) and Large Language Model (right), with the former typically storing structured knowledge explicitly and the latter holding unstructured knowledge implicitly.</p>
<p>item, such as the triggers, the arguments, the temporal relations, etc.This KB creation is undertaken in the rule-based manner automatically without requiring any relation-specific training data.Pre-processing news items extracts a number of relational tuples (1, , 2) per item, realized by the News Information Extractor (NewsIE) we have developed.The KBs created out of news items make it possible to produce better replies to inputs (i.e., news items) that are fed to a given LLM for processing, by complementing the LLM outputs.This results from the fact that LLM outputs contain no structural information while the KBs hold structural and relational information of individual news items.</p>
<p>In this article, we consider the framework that lets KBs support LLMs for improving Web news processing performance.The framework consists of an LLM and a graph convolutional network (GCN) [12], which is inputted with the LLM output and KBs for graph-convolutive operations to let KBs complement the LLM output.The GCN serve to convolute relational tuples with the implicit knowledge facts obtained by the LLM, which is fine-tuned by Web news items for improving the performance of news processing.The fine-tuned LLM takes raw Web news themselves directly as its input.While any LLM may be adopted to form the framework, we use BERT [20] as an example LLM in this article, realized our BERTGraph.Without making any change to BERT for use in BERTGraph, our framework is in contrast to prior designs, like KG-BERT [59] and K-BERT [29], which change the BERT input format, and Relphormer [3], which modifies the BERT encoder structure.However, any change to the input format or the encoder structure of BERT destroys its original embedding, requiring prohibitively expensive LLM model re-training, making our BERTGraph more favorable in practice and able to fully utilize implicit knowledge learned by BERT.More details of the BERTGraph framework can be found in Subsection 3.1.1For the BERT output and KBs to be compatible with the GCN input, a text-to-graph adapter is devised, as stated in Subsection 3.1.2.</p>
<p>We have implemented BERTGraph for experimentally evaluating its news category classification performance under three publicly available Web news datasets, N24News [54], SNOPES [47], and politifact [41].Our evaluation results demonstrate that BERTGraph outperforms its BERT counterpart for evaluated news datasets, in terms of all performance metrics but the precision of politifact, with 0.27 versus 0.28.</p>
<p>The remainder of this paper is organized as follows.Section 2 presents related background and prior work.Section 3 overviews the proposed BERTGraph design, with its two key components detailed in sequence.Section 4 describes the implementation of BERTGraph and then provides experimental results under different news datasets for news category classification.Section 5 concludes this paper.</p>
<p>RELATED BACKGROUND AND PRIOR WORK</p>
<p>Background and prior work related to our proposed BertGraph design include knowledge bases, large language models, graph convolution networks, and information extraction, as provided next in sequence.</p>
<p>Knowledge Bases</p>
<p>Knowledge bases (KBs) are structured repositories of information and data.Such a repository contains facts and relationships among its entities.The Resource Description Format (RDF) [36] has been considered to be the standard format for knowledge bases in Natural Language Processing (NLP).Various KBs have been created out of different data sources, and they are not all of high quality.</p>
<p>In particular, DBpedia [27] created a multilingual knowledge base, extracted from Wikipedia.It uses the DBpedia ontology, comprising 320 classes, to store the facts in the RDF format.However, the lack of a consensus on the contributors of DBpedia poses an issue about its quality [39].WordNet [37] is an online database, which is an effective combination of traditional lexicographic information and modern computing.Parts of Speech of English are organized into sets of synonyms, with their semantic relations linking the synonym sets.Later knowledge bases, like YAGO [14], are of higher quality due to the addition of knowledge about individuals like persons, organizations, products, etc., with their semantic relationships also in existence.YAGO claims its accuracy approaching 95%, based on empirical evaluation of fact correctness.Meanwhile, DEAP-FAKED [34] and Boshko et al. [25] use an existing knowledge base, Wikidata5m, for Named Entity Disambiguation (NED).</p>
<p>Embedding methods, such as TransE and ComplEX, are used to obtain embeddings for news classification.</p>
<p>The proposed BERTGraph framework relies on quality knowledge bases that are generated out of Web News datasets automatically, by our devised News Information Extractor (NewsIE).Generating knowledge bases at scale without manual effort, NewsIE is detailed in Section 3.2.</p>
<p>Large Language Models</p>
<p>With the introduction of Transformer [50] in 2017, the NLP field has seen a rising number of efficient language models.Pre-trained Large Language Models (LLMs), like ChatGPT [46], Bard [32], Chat-Sonic [9], BERT [20], XLNet [58], RoBERTa [30], etc., have been used extensively for NLP and other tasks.Since pre-trained models sll contain huge amounts of textual information, they can handle a variety of tasks easily and often effectively by down-streaming the models for tasks at hand, especially after model fine-tuning via task-specific datasets.</p>
<p>Bidirectional Encoder Representation from Transformers (BERT) [20] has been fine-tuned for NLP tasks, such as sentiment analysis [16,57], text classification [25,43,49], etc. Upon training, BERT introduces masks to input sentences and predicts masked tokens.Due to its reliance on masks, BERT neglects dependency between the masked tokens positions [58].XLNet [58] overcomes the limitation of BERT using its autoregressive formulation.BERT is trained modestly to achieve sound results [30].RoBERTa ehnances BERT by training with bigger batches of more data, dynamicaly changing the masking pattern, training on longer word sequences, and removing the next sentence prediction.Although pre-trained models are capable for NLP tasks, they are not always efficient for knowledgedriven tasks due to the discrepancy of fine-tuning's specific domain and pre-training's wide domains [29].</p>
<p>Graph Convolution Networks</p>
<p>The generalization of convolutional neural networks (CNNs) to signals defined on more general domains was first attempted in [4], treating two onstructions of deep neural networks for processing graphic data efficiently.Later, gneralizing CNNs for use in highdimensional irregular domains, such as graphs, was demonstrated in [12] to learn local, stationary, and compositional features on graphs.Meanwhile, the Graph Convolutional Network (GCN) was proposed for semi-supervised learning on graph-structured data, based on a variant of CNNs [23].It scales linearly with the graph edge count and learns hidden layer representations that encode both the local graph structure and node features node, to yield superior performance.Relational Graph Convolutional Networks (R-GCNs) were introduced specifically to handle the highly multi-relational data characteristic of realistic knowledge bases, able to soundly outperform their decoder-only baselines.A comprehensive review on GCNs is made by grouping existing models into two categories, based on the types of convolutions, and also by categorizing them according to the areas of their applications [61].</p>
<p>Information Extraction</p>
<p>Various information extraction (IE) mechanisms have been considered.Specifically, TextRunner [60] pursues Open Information Extraction (OIE), that makes a single, data-driven pass over the entire corpus to extracts a large set of relational tuples autonomously.It utilizes a set of patterns in order to obtain propositions but does not capture the 'context' of each clause for effective extraction.A follow-up study relies on semantic features (semantic roles) for the OIE task, demonstrating that Semantic role labeling (SRL) can be used to increase the precision and recall of OIE [8].Separately, a greedy parser, which relies on a classifier to predict the correct transition based on a small number of dense features, is treated for speedy parsing [6].Subsequent OIE systems include Stanford OPENIE [2], OPENIE4 [33], and Neural OIE [10], which have been used in various applications, alebit to their abilities on sentence level extraction only.</p>
<p>The OpenIE methods are in three lines.The first lines of the methods start from TextRunner [60],WOE [56], and OLLIE [44], using traditional machine learning methods to learn the informative relation structures.For example, TextRunner extract unlexicalized Part-of-Speech (POS) and noun phrase (NP) chunk features and input them into the Naive Bayes classifier.WOE trains a linearchain Conditional Random Field model by learning POS tags and Dependency Parsing (DP) tags extracted from the text.OLLIE relies on bootstrap learning of patterns based on dependency parse paths.Different from the first line of simple machine learning methods, the second line uses deep learning methods for information extraction based on deep features.LSTM-based models are first explored in this area by considering the information as a sequence, such as BiLSTM-CRF [19], BiLSTM-CNNs-CRF [31] and Freedom [28].Table Convolutional Networks is proposed [51,52] for extracting tablet information from the Web.Recently, BERT-based models have emerged by formulating the IE problem as structural reading comprehension, such as WebSRC [6], and webformer [53] build on transformer aims to extract HTML patterns.However, these machine learning methods usually require a labeled dataset targeted to a specific domain.The third line of OpenIE is rule-based methods.This line of methods does not rely on any training data but only on previously defined rules.Reverb [15], extract manually selected syntactical and lexical features.KRAKEN [1] is designed for capturing complete facts.The clause type is defined in ClausIE [13] PROPS [48] and PredPatt [55] parsing the information based on a list of rules defined on Universal Dependency.Rule-based methods do not require any training process and can easily operate on large-scare datasets.</p>
<p>BERTGRPAH FRAMEWORK AND KNOWLEDGE BASES</p>
<p>This section first overviews the proposed BertGraph framework, which comprises an LLM (e.g., BERT [20]) followed by a GCN, as shown in Fig. 2. A key component in support of BERTGraph, called News information extractor (NewsIE), is then detailed.NewsIE is responsible for extracting structural information out of every Web news item individually to constitute the quality knowledge bases of news datasets.Such a knowledge base and the BERT's output are fed to the GCN for processing.</p>
<p>BERTGraph Overview</p>
<p>Pre-trainned LLMs show robust performance when fine-tuned with only some domain-specific examples [5] before putting them for use.Such robust performance nature stems from models' implicit knowledge.However, materializing the implicit knowledge of a given LLM (e.g., BERT) is challenging.This work proposes to address the challenge by appending a Graph Convolution Layer to BERT, constituting BERTGraph.The Graph Convolution Layer is realized by a typical GCN, which is inputted with BERT's output plus the dataset-specific knowledge base produced automatically by our NewsIE.Our BERTGraph framework aims to answer the key question of: Can structured KBs enhance the performance of LLM's downstream tasks, given that LLM's ouputs are unstructured?To this end, we provide the model and architecture of BERTGraph, in Sec 3.1.1,followed by a light-weighted solution for dealing with the incompatibility issue of the BERT output format and the GCN input format, called the Text-to-Graph Adapter, in Sec.3.1.2.</p>
<p>3.1.1Model and Architecture.BERTGraph takes the BERT Base [28] as its backbone.It should be noted that all other transformerbased LLM models are also suitable for BERTGraph, making it a general design framework.BERT consists of transformer encoders that focus on understanding the meanings of its inputted texts, sufficing our purpose of news category classification.The BERT Base is a pre-trained language model that leverages 12 transformer encoder layers to capture semantic-rich information from inputted texts.Its pre-training is on a large-scale unlabeled general domain corpus.Each input (with no more than 512 words) fed to BERT is first split into a list of tokens, by a tokenizer (which comes with a pre-trained BERT model), to become compatible with the BERT architecture.A token first transfers to an input embedding by adding its 1) token embedding, 2) segment embedding, and 3) positional embedding, where the token embedding captures the semantics of individual tokens, the segment embedding distinguish tokens across different inputs, and the positional embedding indicates the positions of the tokens situated in the input.An input embedding can then be forwarded to BERT for encoding, starting from its first transformer encoder layer.The stacked transformer layers of BERT learn the relationships among input tokens and output the following: 1) hidden states, which represent the contextualized representations and attentions, and 2) Attention scores, which denote attention probabilities assigned to input tokens.Notice that BERT outputs do not contain any structural information.Without making any change to BERT, BERTGraph adds structure information, derived from NewsIE, directly to the BERT output, for graph-convolutive processing.Hence, the BERTGraph architecture is in contrast to prior designs, like KG-BERT [71] and K-BERT [38], which change the BERT input format, and Relphormer [3], which modifies the BERT encoder structure.The rationale behind our architecture is as follows.First, any change to the input format or the encoder structure destroys the original embedding, requiring prohibitively expensive LLM model re-training.Second, an input format chance calls for developing a new suitable data format, possibly a daunting task even for a specific domain.Conversely, BERTGraph makes full use of implicit knowledge learned by BERT, by forwarding its outputs, together with structure information (created by NewsIE; see Sec. 3.2 below), to the appended GCN for graph-convolutive processing.For the BERT output to be compatible with the GCN input, a text-to-graph adapter is devised, as stated next.called the Customized Pooling Layer, which averages the token embeddings of its associated nodes, where the embeddings are from the BERT outputs, as depicted in Fig. 3.If there are a total of  distinct nodes extracted by NewsIE, T2G Adapter creates the Customized Pooling Layer  times.Each pooling output is then assigned with a new label by the T2G Adapter, starting form 0 and with an increment of .This helps to train the appended GCN in the batch mode upon BERTGraph fine-tuning, when the GCN re-indexes knowledge bases.Finally, the pooling output and the edge list (present in knowledge bases) serve as the GCN's input.</p>
<p>News Information Extractor</p>
<p>As a key component that supports our BertGraph framework, the News Information Extractor (NewsIE) aims to identify the informative and structural aspects of each raw Web news item individually, to arrive at knowledge bases.NewsIE operates in a customized rulebased manner, relying on a set of rules derived from traditional NLP taggings, such as Dependency Parsing (DP) [11] and Part of Speech (POS) [40].It automatically and efficiently identifies the informative and structural aspects of news items at scale, comprising predicate-argument structure extraction, clause type identification, and element aggregation.Specifically, the subject, predicate, object, complement and adverbial informative aspects of individual news items are identified and extracted.Next, based on the extracted informative aspects, NewsIE categories the extracted informative aspects per news item into their proper clause types.Finally, the structural aspects of each news item are obtained by applying our developed rules to its categoried clause types.Realized by using Python NLP library SpaCy [18], our NewsIE is detailed below.</p>
<p>Predicate-argument Structure Extraction.</p>
<p>Web news evolve constantly, with new subjects and events emerging daily.However, there are two drawbacks if deploying LLMs directly for processing Web news.First, LLMs require substantial computational resources in their training, making it impractical to retrain them even after abundant new subjects and events have emerged.Second, LLMs lack scalability when exploring previously unknown subjects or events.For instance, the BERT model assigns an 'UNKNOWN' token to any word non-existing in its dictionary (upon prefix search failures).To tackle these drawbacks, we extract factual information from the unstructured text of each news item as pre-processing, bapplying Part-of-speech tagging (POS) to get token-level grammatical category information, e.g., 'NOUN' and 'VERB'.This factual information extraction is inspired by the Open Information Extraction (OpenIE) technique.Following the rule-based OpenIE method, clausIE [7,13], our structure extractor employs dependency paring (DP) [17] to get token-level relationship tagging.Note that Stanford DP (StanDP) [5] may be suitable for extraction as well.Unless stated otherwise, we adopt DP as the default method.After DP, a plain news sentence is transferred to a tree-like syntactic structure, where each node is a word in the sentence, and edges or arcs between pairs of nodes represent grammatical relationships.The root node of the whole tree structure is usually the main verb of the sentence.However, if the verb doesn't exist, the first noun (or the subject complement of a copular verb in StanDP) of the sentence becomes the root node.A group of nodes within a sub-tree, which denotes one single concept or synonym, is called a chunk.The goal of our NewsIE is to extract the subject, predicate, object, complement, and adverbial chunks from news items.Given that subjects are indispensable in most news items, we start by extracting subjects.From the root node, we look for the node in the dependency tree labeled as either 'NSUBJ' (subject), 'CSUBJ' (clausal subject), 'NSUBJPASS' (passive subject), and 'CSUB-JPASS' (clausal passive subject), as the subject head.Notice that multiple subject heads extracted from the single dependency tree are possible.If no 'VERB' exists in a sentence, the 'ROOT' node serves as the subject head.We search the sub-tree for each subject head by traversing the compound (tagged as 'COMPOUND') relationship, or 'NOUN' (person, place, thing, animal, or idea) and 'PROPN' (name or place) POS tag, recursively in its child nodes.Words tagged with 'NOUN' and 'PROPN' are open-class words that can be connected to make larger chunks.During travasal, eligible child nodes must be connected to the subject head (one or more hops) only with a compound relationship or belonging to any of the 'NOUN' and 'PROPN' POS tags.This rule separates the hierarchy subjects that may exist in the same sub-tree.After obtaining the search results, we sort the nodes according to their original order in the text and consider them together as a subject chunk.Since DP and stanDP may produce ambiguous parsing results [26], we apply Named Entity Recognizer (NER), supported by SpaCy Stanza [42], to rectify the results.</p>
<p>Then, we extract predicates from the sentences of each news item.Predicates are usually verbs (words with 'VERB' POS tag) or copulas (words with 'AUX' POS tag) in a sentence.From the dependency tree, we can easily locate the predicate for each subject chunk by checking the parent node of the subject head, denoted as the predicate head.Besides the single-word verb, six different cases of a predicate chunk are considered: 1) Phrasal verb, formed with a verb and an adverb particle, e.g., 'take off'; 2) Prepositional verb, formed with a verb and a preposition, e.g., 'worry about'; 3) Phrasal-prepositional verb, formed with a verb, a particle, and a preposition, e.g., 'catch up with'; 4) negative verb, formed by a word with the 'not' meaning and a verb, e.g., 'never go'; 5) tense verb, formed with 'have' or 'be' and a verb, e.g., 'is playing'; 6) verb with passive voice, formed with 'be' and a verb, e.g., 'is done'.For Cases 1) and 3), NewsIE traverses the sub-tree of the predicate head, probing the node with a particle relationship that is tagged as 'PRT' in its child(ren).For Case 4), NewsIE simply finds the child node that has a negative relationship tagged as 'NEG' with the predicate head.For Cases 5) and 6), NewsIE traverses the sub-tree to find the node respectively with auxiliary (tagged as 'AUX') and with passive auxiliary (tagged as 'AUXPASS') relationships.</p>
<p>Notice that the valid search result of every case must be directly connected to its predicate head.All valid results (i.e., nodes) of a predicate head sub-tree are denoted as its predicate chunk.Since the prepositional verbs for Case 2) all have their semantic meanings very similar to that of their key verb, e.g., 'worry' and 'worry about', they are considered to be a single verb word when extracting the predicate head, with its prepositional words (e.g., 'about' of 'worry about') regarded as its objects.</p>
<p>As objects are associated with their respective verb chunks, NewsIE can extract objects from verb chunks.In general, the verb of a sentence may be followed by 1) no objects, 2) one object (direct object), 3) two objects (direct object and indirect object), and 4) prepositional objects.To extract objects from a sentence, we start from the predicate head to extract its direct, indirect, and prepositional objects separately.In particular, extracting the direct (or indirect) object is to follow the node with a 1DOBJ' (or 'DATIVE') DP tag but without an 'ADP' (adposition) POS tag.The extracted direct (or indirect) object head then allows to get the subject chunks by searching nodes with 'COMPOUND' DP tags.A prepositional object is obtained by probing the sub-tree along a node with its DP tag being 'POBJ' (preposition object), 'PREP' (preposition), 'AGENT' (agent), or 'DATIVE' with 'ADP', to serve as its POS tag.Notice that a sentence may have multiple prepositional object heads.The sub-tree of the obtained prepositional object head is then traversed along the node(s) tagged with 'COMPOUND' DP, to serve as the corresponding prepositional object(s).If no such tagged node is found, the verb has no linked object.</p>
<p>A complement refers to a predicate argument for completing the meaning of a predicate.Extracting the complement chunks starts from the predicate heads.For each head, we consider three types of complements, including 'ATTR' for attribute, 'OPRD' for object predicate, and 'COMP' family for complements.The 'ATTR' is a noun phrase usually following a copula verb.The 'OPRD' is an object predicate in a small clause that functions like the predicate of an object.The 'COMP' family contains the complement of a preposition (tagged as 'PCOMP'), the clausal complement (tagged as 'CCOMP'), and the open clausal complement (tagged as 'XCOMP').The complement chunks of a predicate are derived by starting from the predicate head node to traverse the sub-tree along its child nodes with the aforementioned DP tags.</p>
<p>The last kind of elements to be extracted are adverbials, which contain such important information as space and time.Although many adverbials possibly exist in a sentence, we consider only those adverbials related to the predicate.In a dependency tree, adverbials usually start from the nodes tagged with DPs which include the substring of 'ADV', i.e., adverbial clause modifiers ('ADVCL'), adverbial modifiers ('ADVMOD'), and noun phrases as adverbial modifiers ('NPADVMOD').Hence, the complement chunks of a given adverbial can be obtained by probing its dependency tree's nodes with the aforementioned DP tags.</p>
<p>Clause Type Identification.</p>
<p>After extracting five kinds of chunks, i.e., subjects (''), predicates (' '), objects ('  ' for direct object, '  ' for indirect object, '  ' for prepositional object), complements (''), and adverbials (''), we next classify the types of clauses via a rule-based manner, where a clause comprises multiple extracted chunks.The clause type indicates how those extracted chunks are syntactically connected among one another.Before clause type identification for a given news item, we first apply Verb-Net [45] and PropBank [22] on the news item to categorize its verbs and get the argument frames of categorized verbs.We then map the extracted chunks of the news item to the argument frames given by VerbNet and PropBank.This way can quickly identify different   types of verbs without managing an extensive dictionary of every kind of verbs.Given all the clause types contain '' and ' ', with their differences lying in the parts of '', '', and ' ', we identify different clause types in the sequence shown in Fig. 4. First, if there is '', we look for a '  '.If it exists but no '  ' is present, the clause type of ' ' is designated.Otherwise, the clause type is of ''.If both '  ' and '  ' are present, the clause type belongs to ' '.
Q i z c j m g c k S 5 i d 9 C Z D Z m e X m V k h L P k E L x 4 U 8 e o X e f N v n C R 7 0 G h B Q 1 H V T X d X k A i u j e t + O Y W l 5 Z X V t e J 6 a W N z a 3 u n v L v X 1 H G q G D Z Y L G L V D q h G w S U 2 D D c C 2 4 l C G g U C W 8 H o e u q 3 H l F p H s s H M 0 7 Q j + h A 8 p A z a q x 0 f 9 v j v X L F r b o z k L / E y 0 k F c t RI V V y 3 t r M S U E Q Z Y T D K Q N L 3 8 G N v m Q = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i z c j m g c k S 5 i d 7 U 2 G z M 4 u M 7 N C C P k E L x 4 U 8 e o X e f N v n C R 7 0 G h B Q 1 H V T X d X k A q u j e t + O Y W l 5 Z X V t e J 6 a W N z a 3 u n v L v X 1 E m m G D Z Y I h L V D q h G w S U 2 D D c C 2 6 l C G g c C W 8 H w e u q 3 H l F p n s g H M 0 r R j 2 l f 8 o g z a q x 0 f 9 s L e + W K W 3 V n I H + J l 5 M K 5 K j 3 y p / d M G F Z j N I w Q b X u e G 5 q / D F V h j O B k 1 I 3 0 5 h S N q R 9 7 F g q a Y z a H 8 9 O n Z A j q 4 Q k S p Q t a c h M / T k x p r H W o z i w n T E 1 A 7 3 o T c X / v E 5 m o k t / z G W a G Z R s v i j K B D E J m f 5 N Q q 6 Q G T G y h D L F 7 a 2 E D a i i z N h 0 S j Y E b / H l v 6 R 5 U v X O q 2 d 3 p 5 X a V R 5 H E Q 7 g E I 7 B g w u o w Q 3 U o Q E M + v A E L / D q C O f Z
In the case of '  ' existing together with '  ' or ' ', the clause type is identified as ' ' if its verb is labeled as a linking verb according to VerbNet; otherwise, the clause type is of ' '.In the absence of both '  ' and '  ', the clause type belongs to ' ', only if either "  ' or ' ' exists and the verb is a linking verb.The remaining cases are assigned with the clause type of ' '.</p>
<p>Element Aggregation.</p>
<p>After the clause types are identified, we then aggregate the chunks with different clause types, aiming to transform structured information into knowledge bases.Each clause type can generate one (or two) tuple(s).An element is denoted by a single node in the knowledge base.Since an ' ' clause involves only two elements, a dummy node (dented as 'DUMMY'), which contains no information, is added to it, enabling its predicate node (' ') to link with its subject node () and the dummy node ('DUMMY').For the clause type of ' ', its predicate node (' ') is directly linked with its subject node ('') and its object node ('').Likewise, for the clause type of , its predicate node (' ') is directly linked with its subject node ('') and its complement node ('').The clause type of ' ' has its predicate node (' ') linked directly with its subject node () and its adverbial node (' ').The ' ' clause type has its predicate node (' ') linked with its subject node () and its object node (''), with its complement node ('') appended to the object node ('').Similarly, an   clause type has its predicate node ( ') linked with its subject node () and its object node (''), with its adverbial node ('') appended to object node ('').An ' ' clause type combines its predicate node (' ') and its direct object node ('  ') to become a new node that denotes the relationship between the subject node () and the indirect object node ('  ').Hence, knowledge bases are generated automatically at scale without labeling effort.</p>
<p>EXPERIMENTAL EVALUATION AND RESULT DISCUSSION</p>
<p>This section first introduces the datasets used in our evaluation study, followed by the experimental setup for experimental evaluation.Experimental results and discussion are then provided.</p>
<p>Datasets</p>
<p>Three datasets N24News [54], SNOPES [47], and politifact [41] are considered for the evaluation of the proposed model.N24News is a multimodal news dataset containing both text and images from New York Times with 24 categories.The dataset is comprised of 60K image-text pairs, but this paper uses only the text part of the dataset.SNOPES is a rumor dataset consisting of a number of variety of news categories.Among the categories, only 12 categories are selected for the classification task totalling 7060 instances of news.</p>
<p>Politifact is a fact-check dataset collected from PolitiFact website.There are 21152 instances and 6 categories of expert verified facts.</p>
<p>Metric</p>
<p>We use F1-score (F1), accuracy (Acc), and precision (Pre) as evaluation metrics.Precision measures the accuracy of positive inference made by the model.In classification, it is the ratio of True Positive (TP) of a class to the sum of TP and False Positive (FP) of the same class.Recall is another metric which measures the ability of a model to correctly predict the positive instances.It is the ratio of TP to the sum of TP and False Negative (FN).F1-score is used to evaluate model on imbalanced data.It is the harmonic mean of Precision and Recall which provides balance between them.Accuracy measures the number of correctly inferred instances from the whole dataset.</p>
<p>Experimental Setup</p>
<p>The models were trained on DELL PRECISION TOWER 7910 containing 2 NVIDIA TITAN RTX TU102 GPUs with 24GB of memory each.Batch size of 16 is used with the learning rate of 1e-5 and Adam [21] is used as optimizer.The size of the training dataset varies, ranging from 5% to 80%, with 10% for validation dataset, and the remainder for test datset.There are 4 layers of GNN with hidden layer dimension of 768.For evaluation, Accuracy is the default metric for classification tasks but due to its lack for imbalanced dataset [35], F1 score is also considered.</p>
<p>Experimental Results and Discussion</p>
<p>We conduct a series of experiments on the three dataset, i.e., Snopes, N24News, and Politifact to evaluate the performance of our proposed BERTGraph.The above results indicate that BERTGraph, outperforms BERT, since structural information helps BERT get better representations.Now let's examine two variants of BERTGraph, i.e.,BERTGraph-l, and BERTGraph-s.We can see BERTGraph-s performs the worst among all four models, the reason is that this model does not learns any structural information, which is crucial for improving performance.BERTGraph-l performs slightly superior than BERT on Snopes and N24News dataset, but inferior than BERT on Politifact dataset.These results suggest that fine-tuning only one layer in BERT can lead to improved performance in some cases.</p>
<p>Next, we conducted fine-tuning both BERTGraph and BERT by using 80% of the dataset, the results are presented in Table .2. From the table, we can see BERTGraph consistently outperforms BERT across all F1, Acc and Pre metrics, on the Snopes, N24News, and Politifact datasets.Specificlly, on the Snopes dataset, BERTGraph achieves F1, Acc, and Pre scores of 0.74, 0.75, and 0.74, respectively, whereas BERT only attains scores of 0.68, 0.69, and 0.67.This demonstrates that BERTGraph surpasses BERT, up to 6%, 6%, and 7%, for F1, Acc and Pre, respectively, under 80% of the training dataset setting.For N24News dataset, BERTGraph maintains its superiority, with 6%, 6%, and 7% higher than BERT, on the three metrics.On Politifact, BERTGraph achieves the F1, Acc, and Pre, as 0.29, 0.3, and 0.3, which outperforms  by 6%, 1%, and 2%, respectively.</p>
<p>Trianing Size impacts.</p>
<p>A comparative study of the baseline model and BERTGraph on variable training size of datasets is shown in Fig. 5.It is clear that our proposed model performs better than the baseline on both F1 score and accuracy.But the difference is reduced, or even BERT performs better when the test and train datasets are nearly equal in proportion.However, this is rarely the case in real-world scenarios.The real-world problems have very little data to learn from.Sometimes, the model has to learn in one shot.The BERTGraph learns even from less amount of data due to the additional information obtained from NewsIE.The semantic and structural information obtained from the NewsIE focuses the model on attending to the important token rather than dividing the attention to all tokens.Fig. 5(a) illustrates the BERTGraph and BERT performance on the Snopes dataset, where the training size varies from 5% to 80% The utilization of only 5% of training data mimics the real-world scenarios where labeled data is scarce, while the use of 80% training data allows us to explore the overall learning capabilities of the models.In Fig. 5(a), the red lines with square markers denote the results from the BERTGraph model, and the blue lines with circular markers denote results from the BERT model.The solid, dashed, and dotted lines correspond to F1, Acc, and Pre score, respectively.Notably, BERTGraph outperforms BERT in nearly all the cases of the F1, Acc, and Pre scores.BERTGraph shows an upward trend in performance as the training data size increases.Whereas, BERT shows the performance improved initially but subsequently suffers a performance decline, with scores for F1, Acc, and Pre decreasing from 0.71, 0.72, and 0.7 to 0.68, 0.69, and 0.67, respectively.The reason behind this is fine-tuning BERT lets the model overfit the specific tasks.However, BERTGraph do not suffer such problems according to the observation.Fig. 5(b) illustrates the BERTGraph and BERT performance on the N24News dataset.The legends remain the same as the previous figure.We observe that only a small portion of the data is available for finetuning.When training data increase from 10% to 40% or 60%, the BERT model suddenly achieves its best performance.Nevertheless, as the training data increased, the BERT model performance began to drop.Specifically, its F1 drops from 0.74 to 0.69 , Acc decreases from 0.75 to 0.69, and Pre decreases from 0.74 to 0.7.In contrast, BERTGraph stays an upward trend in performance, increasing its F1 score from 0.66 to 0.75, Acc from 0.65 to 0.75, and Pre from 0.7 to 0.74, when the training dataset increase from 5% to 80% This suggests fine-tuning with structural information can help mitigate the overfitting problem since noise may be ignored in the process.[24] and GraphConv [38].They are compared only based on the accuracy.The dataset is trained for a maximum of 50 epochs with various layers for GCNs.Only the best-performing model is used for evaluation.We have observed that structural information plays a pivotal role during the fine-tuning process.It can avoid the model overfitting on specific tasks.The LLMs utilizing the attention mechanism, denoted as full attention, maybe overkill and excessive, leading to an overemphasis on input noise, which hurts the overall quality of the representations.Structure information constrains the LLMs to fine-tune according to specific passes, encouraging the model to ignore the input noise.This discovery holds the potential for LLMs to be more intelligent.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce the BertGraph frameworks and evaluate its effectiveness in downstream news-related classification tasks.The BERTGraph is an extension of BERT, taking both original text and structured information extracted by NewsIE as its inputs.</p>
<p>Our evaluation demonstrates that, the BERTGraph outperforms its BERT Base counterpart in terms of almost all performance metrics for three news datasets available to the public, achieving up to 5% of accuracy increase on the Snopes dataset.The evaluation results underscore the importance of incorporating structural information for LLMs.Furthermore, we observe that as the training dataset size grows, Bert suffers the over-fitting problem, which is not observed in BERTGraph.Consequently, our model presents a more robust fine-tuning structure overall.Additionally, we highlight the value of LLMs as rich sources of knowledge.Our lightweight BERTGraph can easily utilize the implicit knowledge of LLMs.In the future, we will evaluate BertGraph on larger datasets for a broader range of tasks, further exploring its potentials.</p>
<ol>
<li>1 . 2 Figure 2 :Figure 3 :
1223
Figure 2: Overview of BERTGraph framework.</li>
</ol>
<p>t e x i t s h a 1 _ b a s e 6 4 = " H k V B w m m Y z d I m t G 3 N 5 L g 1 i g J 8 p G 8 = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P</p>
<p>7 5 c 9 u P 2 Z p h N I w Q b X u e G 5 i / I w q w 5 n A S a m b a k w o G 9 E B d i y V N E L t Z 7 N T J + T I K n 0 S x s q W N G S m / p z I a K T 1 O A p s Z 0 T N U C 9 6 U / E / r 5 O a 8 N L P u E x S g 5 L N F 4 W p I C Y m 0 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B W 3 z 5 L 2 m e V L 3 z 6 t n d a a V 2 l c d R h A M 4 h G P w 4 A J q c A N 1 a A C D A T z B C 7 w 6 w n l 2 3 p z 3 e W v B y W f 2 4 R e c j 2 8 l W o 2 5 &lt; / l a t e x i t &gt; O i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p>
<p>Figure 4 :
4
Figure 4: The sequence of clause type identification.</p>
<p>Fig. 5 (
5
c) illustrates the performance on the Politifact dataset.The</p>
<p>Figure 5 :
5
Figure 5: F1 score, accuracy, precision under various training amounts (in %) of datasets.</p>
<p>Table 1 :
1
We first use 10% of the data to fine-tune BERT, BERTGraph, BERTGraph-0 and BERTGraph-1, the results are shown in Table. 1. From the table, it is evident that BERTGraph outperforms the other models on Snopes and N24News dataset across all performance Comparative Results of BERTGraph and BERT on different datasets, under 10% of data for training and 10% for validation Acc, and Pre).Specifically, on Snopes dataset, BERT-Graph achieves F1, Acc, and Pre scores of 0.68, 0.71 of 0.68, respectively, whereas the counterpart BERT, only achieves 0.66, 0.66, and 0.67 of F1 score, Acc, and Pre.BERTGraph improvements of 2%, 5%, and 1% in F1 score, Acc, and Pre, respectively.On N24News dataset, BERTGraph achieves F1, Acc, and Pre scores of 0.68, 0.71 of 0.68, making improvements of 4%, 5%, and 2%, respectively, over BERT model.On the Politifact data, both BERTGraph and BERT exhibit performance below 0.3 in terms of F1, Acc, and Pre.However, BERTGraph outperforms BERT in terms of F1 and Acc, two metric, achieving values of 0.29, and 0.3, respectively.The Pre score is slightly in inferior than BERT, at 0.27, compared to 0.28 for BERT.
4.4.1 Comparing to the BERT baseline. To compare BERTGraphwith BERT baseline, we employ various fine-tuning configurations.Besides BERT and BERTGraph, we explore two variations of theBERTGraph model: 1) BERTGraph-s, this variant maintains the sta-bility of BERT without conducting any fine-tuning; 2) BERTGraph-l, this variant, fine-tuning is applied solely to the last layer ofBERT.</p>
<p>Table 2 :
2
Comparative Results of BERTGraph and BERT on different datasets, under 80% of data for training and 10% for validation
DatasetBERTGraphBERTMetricsF1Acc PreF1 Acc PreSnopes0.74 0.75 0.74 0.68 0.69 0.67N24News 0.75 0.75 0.74 0.69 0.69 0.70Politifact 0.29 0.29 0.3 0.24 0.28 0.26</p>
<p>Table 3 :
3
Comparative accuracy of BERT based models and GCN based models on different datasets Comparison between BERT based and GCN based models.All three datasets are trained in two Graph Neural Network models, GCN
DatasetBERTGraph BERT BERTGraph-s GCN GraphConvSnopes0.710.660.660.610.22N24News0.690.640.570.490.46Politifact0.30.280.270.310.324.4.3</p>
<p>Table 3 clearly shows that the accuracy of the GCN model depends on the dataset.GCN has 61% accuracy on Snopes while GraphConv only has 22% accuracy.The proposed model either outperforms GCN-based models or is comparable in accuracy.Besides the Politifact dataset, BERT-based models outperform GCN-based models.The pre-trained model captures general information from the sentences by embedding.The embeddings contain grammatical and structural information, contributing to their accuracy.4.4.4 Discussion.In this project, we explore a crucial question that was evolving alongside the LLMs: Can structured KBs enhance the performance of LLM's downstream tasks, given that LLM's outputs are unstructured?Our experiments, as described in Subsections 4.4.1 and Subsections 4.4.2, have yielded valuable insights.</p>
<p>Kraken: N-ary facts in open information extraction. Alan Akbik, Alexander Löser, Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction. the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge ExtractionAKBC-WEKEX2012</p>
<p>Leveraging linguistic structure for open domain information extraction. Gabor Angeli, Melvin Jose , Johnson Premkumar, Christopher D Manning, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing20151</p>
<p>Relphormer: relational graph transformer for knowledge graph representation. Zhen Bi, Siyuan Cheng, Ningyu Zhang, Xiaozhuan Liang, Feiyu Xiong, Huajun Chen, arXiv:2205.108522022. 2022arXiv preprint</p>
<p>Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Lecun, arXiv:1312.6203Spectral networks and locally connected networks on graphs. 2013. 2013arXiv preprint</p>
<p>A fast and accurate dependency parser using neural networks. Danqi Chen, Christopher D Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>WebSRC: A Dataset for Web-Based Structural Reading Comprehension. Xingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, Kai Yu, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Grammar Informed Sound Effect Retrieval for Soundscape Generation. E Chourdakis, J D Reiss, DMRN+ 13: Digital Music Research Network Oneday Workshop. London, UK, 92018</p>
<p>Semantic role labeling for open information extraction. Janara Christensen, Stephen Soderland, Oren Etzioni, Proceedings of the NAACL HLT 2010 first international workshop on formalisms and methodology for learning by reading. the NAACL HLT 2010 first international workshop on formalisms and methodology for learning by reading2010</p>
<p>Chatsonic -Best ChatGPT Alternative for Content Creation. Mike Coombe, 2023</p>
<p>Lei Cui, Furu Wei, Ming Zhou, arXiv:1805.04270Neural open information extraction. 2018. 2018arXiv preprint</p>
<p>Universal dependencies. Marie-Catherine De Marneffe, Christopher D Manning, Joakim Nivre, Daniel Zeman, Computational Linguistics. 472021. 2021</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in neural information processing systems. 292016. 2016</p>
<p>Clausie: clause-based open information extraction. Luciano Del, Corro , Rainer Gemulla, Proceedings of the 22nd international conference on World Wide Web. the 22nd international conference on World Wide Web2013</p>
<p>Yago: A core of semantic knowledge unifying wordnet and wikipedia. Kasneci Fabian, Gjergji, Gerhard, 16th International world wide web conference. WWW2007</p>
<p>Identifying relations for open information extraction. Anthony Fader, Stephen Soderland, Oren Etzioni, Proceedings of the 2011 conference on empirical methods in natural language processing. the 2011 conference on empirical methods in natural language processing2011</p>
<p>Aspect-based sentiment analysis using bert. Mickel Hoang, Oskar Alija Bihorac, Jacobo Rouces, Proceedings of the 22nd Nordic Conference on Computational Linguistics. the 22nd Nordic Conference on Computational Linguistics2019</p>
<p>An improved non-monotonic transition system for dependency parsing. Matthew Honnibal, Mark Johnson, Proceedings of the 2015 conference on empirical methods in natural language processing. the 2015 conference on empirical methods in natural language processing2015</p>
<p>spaCy: Industrial-strength natural language processing in python. Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, 2020. 2020</p>
<p>Zhiheng Huang, Wei Xu, Kai Yu, arXiv:1508.01991Bidirectional LSTM-CRF models for sequence tagging. 2015. 2015arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT20191</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014. 2014arXiv preprint</p>
<p>From TreeBank to PropBank. R Paul, Martha Kingsbury, Palmer, LREC. 2002. 1989-1993</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>Knowledge graph informed fake news classification via heterogeneous representation ensembles. Boshko Koloski, Timen Stepišnik Perdih, Marko Robnik-Šikonja, Senja Pollak, Blaž Škrlj, Neurocomputing. 4962022. 2022</p>
<p>Nathaniel Krasner, Miriam Wanner, Antonios Anastasopoulos, arXiv:2203.12815Revisiting the Effects of Leakage on Dependency Parsing. 2022. 2022arXiv preprint</p>
<p>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia. 2015. 20156</p>
<p>Freedom: A transferable neural architecture for structured information extraction on web documents. Ying Bill Yuchen Lin, Nguyen Sheng, Sandeep Vo, Tata, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>K-bert: Enabling language representation with knowledge graph. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, Ping Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>End-to-end sequence labeling via bidirectional lstm-cnns-crf. Xuezhe Ma, Eduard Hovy, arXiv:1603.013542016. 2016arXiv preprint</p>
<p>An overview of Bard: an early experiment with generative AI. James Manyika, 2023</p>
<p>Open information extraction systems and downstream applications. Mausam Mausam, Proceedings of the twenty-fifth international joint conference on artificial intelligence. the twenty-fifth international joint conference on artificial intelligence2016</p>
<p>DEAP-FAKED: Knowledge graph based approach for fake news detection. Mohit Mayank, Shakshi Sharma, Rajesh Sharma, 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE2022</p>
<p>DEAP-FAKED: Knowledge graph based approach for fake news detection. Mohit Mayank, Shakshi Sharma, Rajesh Sharma, 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE2022</p>
<p>The resource description framework (RDF) and its vocabulary description language RDFS. Brian Mcbride, Handbook on Ontologies. Springer2004</p>
<p>WordNet: a lexical database for English. George A Miller, Commun. ACM. 381995. 1995</p>
<p>Weisfeiler and leman go neural: Higher-order graph neural networks. Christopher Morris, Martin Ritzert, Matthias Fey, Jan William L Hamilton, Gaurav Eric Lenssen, Martin Rattan, Grohe, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Type Prediction for Entities in DBpedia by Aggregating Multilingual Resources. Thi-Nhu Nguyen, Hideaki Takeda, Khai Nguyen, Ryutaro Ichise, Tuan-Dung Cao, ISWC (Posters &amp; Demos). Citeseer2016</p>
<p>A Universal Part-of-Speech Tagset. Slav Petrov, Dipanjan Das, Ryan Mcdonald, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). the Eighth International Conference on Language Resources and Evaluation (LREC'12)2012</p>
<p>. Politifact, 2023</p>
<p>Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, Christopher D Manning, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2020</p>
<p>Fake News Classification using transformer based enhanced LSTM and BERT. Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, Ahad Ali, International Journal of Cognitive Computing in Engineering. 32022. 2022</p>
<p>Open language learning for information extraction. Michael Schmitz, Stephen Soderland, Robert Bart, Oren Etzioni, Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. the 2012 joint conference on empirical methods in natural language processing and computational natural language learning2012</p>
<p>VerbNet: A broad-coverage, comprehensive verb lexicon. Karin Kipper Schuler, 2005University of Pennsylvania</p>
<p>. John Schulman, Introducing ChatGPT. 2022</p>
<p>. Snopes, 2023</p>
<p>Gabriel Stanovsky, Jessica Ficler, Ido Dagan, Yoav Goldberg, arXiv:1603.01648Getting more out of syntax with props. 2016. 2016arXiv preprint</p>
<p>How to fine-tune bert for text classification?. Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang, Chinese Computational Linguistics: 18th China National Conference. Springer2019</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 302017. 2017</p>
<p>TCN: table convolutional network for web table interpretation. Daheng Wang, Prashant Shiralkar, Colin Lockard, Binxuan Huang, Xin Luna Dong, Meng Jiang, Proceedings of the Web Conference 2021. the Web Conference 20212021</p>
<p>Tag, copy or predict: a unified weakly-supervised learning framework for visual information extraction using sequences. Jiapeng Wang, Tianwei Wang, Guozhi Tang, Lianwen Jin, Weihong Ma, Kai Ding, Yichao Huang, arXiv:2106.106812021. 2021arXiv preprint</p>
<p>Webformer: The web-page transformer for structure information extraction. Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, Dongfang Liu, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022</p>
<p>Zhen Wang, Xu Shan, Xiangxie Zhang, Jie Yang, arXiv:2108.13327N24news: A new dataset for multimodal news classification. 2021. 2021arXiv preprint</p>
<p>Universal decompositional semantics on universal dependencies. Aaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, Benjamin Van Durme, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>Open information extraction using wikipedia. Fei Wu, Daniel S Weld, Proceedings of the 48th annual meeting of the association for computational linguistics. the 48th annual meeting of the association for computational linguistics2010</p>
<p>BERT post-training for review reading comprehension and aspect-based sentiment analysis. Hu Xu, Bing Liu, Lei Shu, Philip S Yu, arXiv:1904.022322019. 2019arXiv preprint</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Advances in Neural Information Processing Systems. 322019. 2019</p>
<p>KG-BERT: BERT for knowledge graph completion. Liang Yao, Chengsheng Mao, Yuan Luo, arXiv:1909.031932019. 2019arXiv preprint</p>
<p>Textrunner: open information extraction on the web. Alexander Yates, Michele Banko, Matthew Broadhead, Michael J Cafarella, Oren Etzioni, Stephen Soderland, Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics. Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational LinguisticsNAACL-HLT2007</p>
<p>Graph convolutional networks: a comprehensive review. Si Zhang, Hanghang Tong, Jiejun Xu, Ross Maciejewski, Computational Social Networks. 62019. 2019</p>            </div>
        </div>

    </div>
</body>
</html>
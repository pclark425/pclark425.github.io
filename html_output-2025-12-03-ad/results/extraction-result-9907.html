<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9907 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9907</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9907</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-d96e228ce2e10d2215fa1d9833238bb1f2157656</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d96e228ce2e10d2215fa1d9833238bb1f2157656" target="_blank">The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics</a></p>
                <p><strong>Paper Venue:</strong> EVAL4NLP</p>
                <p><strong>Paper TL;DR:</strong> A human evaluation of the plausibility of explanations given by the LLMs and its effect on model performance is performed and parts of the code and datasets are made available.</p>
                <p><strong>Paper Abstract:</strong> Generative large language models (LLMs) have seen many breakthroughs over the last year. With an increasing number of parameters and pre-training data, they have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Strategies employed in this context differ in the choice of input prompts, the selection of samples for demonstration, and the methodology used to construct scores grading the generations. Approaches often differ in the input prompts, the samples that are selected for demonstration and the construction process of scores from the output. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore such approaches for machine translation evaluation and summarization eval- uation. Specifically, we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We test the approaches of the participants on a new reference-free test-set spanning 3 language pairs for machine transla- tion as well as a summarization dataset. Further, we present an overview of the approaches taken by the participants, present their results on the test set and analyze paths for future work. Fi- nally, as a separate track, we perform a human evaluation of the plausibility of explanations given by the LLMs and its effect on model performance. We make parts of our code and datasets available.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9907.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9907.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MQM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multidimensional Quality Metrics (MQM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-grained human annotation scheme for translation quality that assigns error categories and severities (e.g., minor/major) which are aggregated into segment/system-level scores; used as the ground-truth annotation format for MT in this shared task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multidimensional quality metrics (mqm): A framework for declaring and describing translation quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>various allowed LLMs (see paper Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM; MQM is a human annotation scheme. In the shared task the LLMs listed (e.g., Platypus2-70B, Guanaco-65B, OpenOrca-Platypus2-13B, orca_mini_v3_7b, WizardLM-13B, Nous-Hermes-13b) were used as metrics and compared against MQM human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine translation evaluation / Natural language generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human annotators performed MQM fine-grained error annotations per segment; aggregated MQM scores serve as the gold standard to compute correlations with LLM-produced metric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Error categories and severities defined by MQM (e.g., major/minor errors), aggregated into a single numeric MQM score per segment.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>WMT MQM partitions (en-de, zh-en) used as train/dev; MQM annotations used as ground truth for MT evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MQM annotations were used as the reference human judgments against which LLM-based metrics were correlated (Kendall/Pearson/Spearman). Inter-annotator Kendall agreement values are reported (e.g., en-de 0.458). The top automatic metrics reached correlations comparable to small-sample inter-annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>MQM is costly and requires trained annotators; some early annotation batches had tokenization issues and a low-agreement subset (notably en-es). MQM depends on annotator expertise and can be time-consuming.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>MQM represents the human/traditional ground truth used to evaluate LLM-based metrics; the best LLM-based metrics achieved correlation levels approaching human inter-annotator agreement on small subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use MQM for fine-grained human evaluation when building/evaluating LLM-based metrics; avoid major source errors in test set; perform annotator qualification and iterative annotation batches to improve consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9907.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9907.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KendallTau</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kendall's tau (segment-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rank correlation coefficient used to measure agreement between segment-level automatic metric scores and human MQM judgments; used as the primary evaluation metric in the shared task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>THE TREATMENT OF TIES IN RANKING PROBLEMS.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>n/a (statistical metric)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM; Kendall's tau is a statistical correlation measure. The paper applies it to compare LLM-generated metric scores with human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation statistics for NLG metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute Kendall's tau between model-produced scores and human MQM scores at segment level; also report Pearson and Spearman as secondary measures. Significance assessed with a permute-both significance test.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correlation coefficients (Kendall as primary; Pearson and Spearman as supplementary). Statistical significance (p ≤ 0.05) via permutation tests.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Correlations computed on the shared task test-sets (MT en-de/en-es/en-zh and summarization dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Kendall's tau was used to rank systems. Example: HIT-MI&T Lab probability-based method obtained top Kendall correlations in the small MT track (values in Table 6); top summarization systems had Kendall ~0.61–0.63.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Kendall's tau can be influenced by many ties and the distribution of scores; paper notes alternative Kendall variants may be explored (Deutsch et al., 2023). Small test sizes in some language pairs reduced reliability (NaNs appear for some entries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Used to compare automatic metric outputs to human MQM judgments; also compared to inter-annotator Kendall agreement to contextualize performance.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use Kendall's tau as a primary segment-level metric for NLG metric evaluation; supplement with Pearson and Spearman and use permutation tests for significance; consider modern variants of Kendall for ties.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9907.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9907.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probability-based</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probability-based generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation approach that uses LLM generation probabilities (e.g., likelihood of paraphrases or translations) as metric scores; often implemented by prompting the model to generate paraphrases/translations and computing conditional probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic machine translation evaluation in many languages via zero-shot paraphrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>OpenOrca-Platypus2-13B (used successfully by top MT team), other allowed LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>OpenOrca-Platypus2-13B is an instruct-tuned LLaMA2-13B derivative; the shared task allowed both small (<25B) and large (>25B) models (e.g., Platypus2-70B, Guanaco-65B). Many probability-based submissions used quantized GPTQ 4-bit models to reduce resource needs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG metric evaluation / Machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt LLM to produce paraphrases/translations for the source/translation pair and compute generation probabilities (likelihood) of target given source or paraphrase; use probabilities (possibly ensembled across prompts) as numeric quality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correlation between probability-derived scores and human MQM scores (Kendall/Pearson/Spearman). Ensembling and selection of demonstration examples used to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on the shared task MT test-sets (en-de, en-es, en-zh) and compared to MQM annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>A probability-based ensemble approach by HIT-MI&T Lab (Zhang et al., 2023) achieved the top Kendall correlations in the small MT track, outperforming many baselines including COMET-kiwiXXL, indicating strong performance for MT in reference-free settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires careful prompt design and example selection; computational cost can be high for large models due to probability computations; multilingual model competence affects success (may favor paraphrase-generation over direct translation understanding).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Probability-based scores correlated strongly with MQM human judgments in MT; sometimes exceeded output-based approaches on MT in this shared task.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use ensembling of prompts, consider retrieval of in-context examples (e.g., SBERT/LABSE similarity selection), and test multiple prompt templates; quantization (GPTQ) can reduce compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9907.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9907.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Output-based</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Output-based natural language scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where LLMs are prompted to directly produce ratings, labels, or fine-grained error analyses in natural language and these outputs are parsed/aggregated to construct numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are state-of-the-art evaluators of translation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>various allowed LLMs (e.g., WizardLM-13B, Orca_mini_v3_7b, Nous-Hermes-13b, OpenOrca-Platypus2-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Small and medium instruct-tuned LLMs (≈7B–13B) commonly used in output-based submissions; many runs used 4-bit GPTQ quantized variants to reduce resource usage.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG metric evaluation (summarization & MT)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Provide natural language instructions (zero-shot or few-shot, sometimes chain-of-thought) asking the LLM to rate qualities (e.g., fluency, coherence, factuality) or to return a single aggregate score; parse numeric scores or aggregate sub-scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Direct numeric labels produced by the model or aggregated from fine-grained error categories; evaluated via correlation with human MQM (MT) or heuristic summarization gold scores.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Summarization test-set (derived from Wikipedia; SummEval used for dev), MT test-sets; multiple prompt variants tested.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Output-based approaches led summarization results: DSBA and iML achieved top Kendall correlations (~0.60–0.63) in the small summarization track using carefully crafted prompts and aggregated criteria. Demonstrations (few-shot) sometimes hurt performance; lean prompts often worked well.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Explanations generated are often plausible but not necessarily faithful; many generated explanations were vague. Prompt design is critical; demonstrations can reduce performance in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Output-based LLM ratings showed high correlation with human judgments for summarization and can approach human inter-annotator agreement for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Craft clear instruction prompts, evaluate both fine-grained and aggregated scoring, test with/without in-context examples, consider score bucketing/shortcuts for pathological outputs, and optionally request explanations as byproducts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9907.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9907.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SummEval Heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-grained reference-free summarization heuristic (α, β, γ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel heuristic introduced in this paper combining relevance, hallucination ratio, and MQM-like readability into a single reference-free summarization score with tunable weights α, β, γ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>n/a (heuristic used as gold/target for scoring comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM; heuristic aggregates human annotations (relevance per source sentence, hallucinated characters ratio, MQM readability) into a numeric score; default weights used were α=3, β=5, γ=1 (alternative normalizations also explored).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Summarization evaluation / NLG</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Annotators split source into sentences (treated as 'facts'), annotated relevance of facts, aligned summary spans to source facts, annotated hallucinations; combine components via formula to produce single numeric gold score for summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relevance (sum over facts in summary weighted by annotator relevance), hallucination penalty (hallucinated characters / summary characters), and MQM readability component; weights control influence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Wikipedia-derived summarization test set (created for the shared task); SummEval used for dev/train.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Heuristic was used to create the summarization gold scores; system rankings were stable across alternative normalizations (Appendix A). Top summarization systems correlated strongly with these heuristic gold scores (Kendall ≈0.60+).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The heuristic depends on sentence-as-fact assumption (coarse-grained) and on annotator consistency; longer summaries may be advantaged without length control (mitigated by controlling max tokens), and initial annotation batches required corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>This is a resource-efficient alternative to the pyramid method and aims to provide fine-grained, reference-free evaluation comparable to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Control summary length during generation, perform manual corrections of sentence splits, choose weights α/β/γ transparently and test robustness across weight variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9907.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9907.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline automatic metrics (BERTScore, SBERT similarity, SUPERT, GEMBA, COMET-Kiwi-XXL, BARTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of established and recent automatic metrics used as baselines for comparison: embedding-based similarity (BERTScore/SBERT), SUPERT (unsupervised summarization metric), generation-based BARTScore, GEMBA prompt-based DA scoring, and COMET-Kiwi-XXL as a trained MT metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>baseline models (non-generative or trained metric models like COMET variants); in addition, GEMBA uses LLM prompting (often closed models in literature).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>BERTScore uses contextual embeddings (here XLM-R-large); SBERT uses sentence-transformer embeddings; SUPERT uses bert-large-nli-stsb embeddings; COMET-Kiwi-XXL is a large COMET-style metric; BARTScore employs generation probabilities from BART-family models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute baseline metric scores per segment and measure correlation to human MQM / summarization gold; include per-allowed-LLM GEMBA DA prompt baseline variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Kendall/Pearson/Spearman correlations with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Shared task datasets (WMT MQM partitions, SummEval, Wikipedia test set).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Many simple baseline systems achieved high ranks. For MT, GEMBA and COMET-Kiwi-XXL were strong baselines; for summarization BERTScore and SUPERT variants were competitive. Some baseline LLMs (OrcaMini) beat larger allowed models in certain tracks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some baselines rely on references or pseudo-references; performance can vary strongly across domains and language pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Baselines provide strong automatic proxies to human judgments and in some cases approach top submitted systems.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include strong, well-understood baselines when evaluating new LLM-based metrics; simple prompting often suffices and can outperform complex demo-heavy prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9907.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9907.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eval4NLP datasets (train/dev/test: WMT MQM partitions, SummEval, and new Wikipedia test-set)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training/dev sets derived from WMT 2022 MQM (en-de, zh-en) and SummEval for summarization; novel test set constructed from Wikipedia articles created after 15.07.2023 (en-de, en-es, en-zh and summarization) to reduce pretraining overlap with allowed LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>n/a (datasets used to evaluate LLM-based metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation (MT and summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Reference-free evaluation: participants receive source and candidate (translation or summary) and produce scores; these are compared to human annotations (MQM for MT; the introduced summarization heuristic for summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human annotations (MQM for MT; relevance/factuality/readability + hallucination heuristic for summarization) used as gold; metrics evaluated via correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>WMT 2022 MQM subsets (train/dev), SummEval (dev/train), and the new reference-free Wikipedia test set (test-phase) created for the shared task.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The novel Wikipedia-based test set enabled evaluation of reference-free LLM metrics across three MT language pairs and summarization; allowed identification that several small-model approaches achieved top correlations on this test set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential leakage: Wikipedia articles post 15.07 could still overlap with LLM pretraining corpora; some articles may be translated or generated; dataset size is moderate and some annotation agreements (en-es) were low.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Combines human fine-grained annotations and resource-efficient summarization heuristics to approximate human evaluation standards; matches WMT-style MQM usage.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When constructing test sets to evaluate open-source LLMs, select source texts created after recent model release dates to reduce pretraining overlap, and perform quality pre-filtering (Wikipedia quality class, language-tool).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9907.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9907.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SharedTaskResults</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eval4NLP 2023 shared task empirical findings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical outcomes and analyses from the shared task: ranking of participant methods, per-track winners, and general observations about prompting, model size, and explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>various allowed LLMs (OpenOrca-Platypus2-13B, orca_mini_v3_7b, Platypus2-70B, Guanaco-65B, WizardLM-13B, Nous-Hermes-13b)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Allowed LLMs spanned small (<25B) and large (>25B) models; many submissions used quantized GPTQ 4-bit variants for resource efficiency. Some models are instruct-tuned derivatives of LLaMA2.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation metrics (MT and summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Participants produced newline-separated scores for dataset segments using allowed LLMs without finetuning; scores were evaluated against human MQM / summarization gold using Kendall/Pearson/Spearman and permutation tests for significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Segment-level correlations (Kendall primary) to human judgments; additional analyses of explainability plausibility via small human preference study.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Shared task datasets described above (WMT MQM partitions, SummEval, new Wikipedia test set).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key findings: (1) Small-model solutions often outperformed large-model ones in both MT and summarization tracks; (2) For MT the top system (HIT-MI&T Lab) used a probability-based ensemble on OpenOrca-Platypus and attained the highest Kendall in the small MT track; (3) For summarization top teams (DSBA, iML, Akkasi et al.) used carefully crafted prompts (output-based) and achieved Kendall ≈0.60–0.63; (4) Many simple baselines were competitive; (5) Explanations provided by LLMs were frequently vague and a small human preference experiment found no clear winner between two systems' explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Explainability track had only 2 submissions and small human evaluation (50 samples) limiting conclusions; possible dataset pretraining leakage; unequal annotator experience and some annotation quality issues (notably en-es). Computational constraints led many participants to use small models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Top automatic metrics reached correlations similar to inter-annotator agreement on small subsets, suggesting LLM-based metrics can approximate human judgments without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Restricting models and disallowing fine-tuning focuses evaluation research on prompt and score-extraction design; encourage lean prompts, ensemble probability-based approaches for MT, careful prompt crafting for summarization, and exploration of pipeline/iterative methods (tree-of-thoughts, self-refinement) in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9907.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9907.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExplainabilityEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explainability assessment of LLM-generated metric explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small-scale human evaluation comparing plausibility/preference of explanations generated by LLMs alongside numeric scores; used to gauge the perceived usefulness of LLM-produced explanations for metric decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Baswani et al. system (uses Orca variants) and Mahmoudi system (Orcamini/orca_mini_v3_7b)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Participating systems prompted their models to produce natural language explanations as well as scores; commonly used small quantized models (e.g., orca_mini_v3_7b).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Explainable NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human annotators (two team members) compared paired explanations (for the same sample) from two participant systems over 50 randomly selected summarization samples and indicated preference; annotator agreement and counts recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human preference between explanations, qualitative assessment for specificity versus vagueness; agreement percentage used as an indicator (56% agreement observed).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>50 random samples from the shared-task summarization test set with provided MQM annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Annotators showed no strong preference: one annotator preferred system A 27 vs 23, the other 24 vs 26, with 56% agreement — indicating no clear superiority; many explanations were generic ('good coherence and fluency'), while some correctly identified real issues (e.g., word repetition).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Very small-scale (50 samples) and only two systems submitted explanations; explanations may be plausible but not faithful; limited annotator pool.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human evaluators did not strongly prefer either system's explanations; human evaluation remains necessary to judge explanation utility.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Encourage richer, more specific explanations from LLMs, and scale human evaluation of explanations; distinguish plausibility from faithfulness when assessing explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are state-of-the-art evaluators of translation quality. <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire. <em>(Rating: 2)</em></li>
                <li>Bartscore: Evaluating generated text as text generation. <em>(Rating: 2)</em></li>
                <li>Automatic machine translation evaluation in many languages via zero-shot paraphrasing. <em>(Rating: 2)</em></li>
                <li>SummEval: Re-evaluating summarization evaluation. <em>(Rating: 2)</em></li>
                <li>Multidimensional quality metrics (mqm): A framework for declaring and describing translation quality metrics. <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena. <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9907",
    "paper_id": "paper-d96e228ce2e10d2215fa1d9833238bb1f2157656",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "MQM",
            "name_full": "Multidimensional Quality Metrics (MQM)",
            "brief_description": "A fine-grained human annotation scheme for translation quality that assigns error categories and severities (e.g., minor/major) which are aggregated into segment/system-level scores; used as the ground-truth annotation format for MT in this shared task.",
            "citation_title": "Multidimensional quality metrics (mqm): A framework for declaring and describing translation quality metrics.",
            "mention_or_use": "use",
            "llm_name": "various allowed LLMs (see paper Table 1)",
            "llm_description": "Not an LLM; MQM is a human annotation scheme. In the shared task the LLMs listed (e.g., Platypus2-70B, Guanaco-65B, OpenOrca-Platypus2-13B, orca_mini_v3_7b, WizardLM-13B, Nous-Hermes-13b) were used as metrics and compared against MQM human scores.",
            "scientific_domain": "Machine translation evaluation / Natural language generation",
            "evaluation_method": "Human annotators performed MQM fine-grained error annotations per segment; aggregated MQM scores serve as the gold standard to compute correlations with LLM-produced metric scores.",
            "evaluation_criteria": "Error categories and severities defined by MQM (e.g., major/minor errors), aggregated into a single numeric MQM score per segment.",
            "benchmark_or_dataset": "WMT MQM partitions (en-de, zh-en) used as train/dev; MQM annotations used as ground truth for MT evaluation.",
            "results_summary": "MQM annotations were used as the reference human judgments against which LLM-based metrics were correlated (Kendall/Pearson/Spearman). Inter-annotator Kendall agreement values are reported (e.g., en-de 0.458). The top automatic metrics reached correlations comparable to small-sample inter-annotator agreement.",
            "limitations_or_challenges": "MQM is costly and requires trained annotators; some early annotation batches had tokenization issues and a low-agreement subset (notably en-es). MQM depends on annotator expertise and can be time-consuming.",
            "comparison_to_human_or_traditional": "MQM represents the human/traditional ground truth used to evaluate LLM-based metrics; the best LLM-based metrics achieved correlation levels approaching human inter-annotator agreement on small subsets.",
            "recommendations_or_best_practices": "Use MQM for fine-grained human evaluation when building/evaluating LLM-based metrics; avoid major source errors in test set; perform annotator qualification and iterative annotation batches to improve consistency.",
            "uuid": "e9907.0",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "KendallTau",
            "name_full": "Kendall's tau (segment-level correlation)",
            "brief_description": "A rank correlation coefficient used to measure agreement between segment-level automatic metric scores and human MQM judgments; used as the primary evaluation metric in the shared task.",
            "citation_title": "THE TREATMENT OF TIES IN RANKING PROBLEMS.",
            "mention_or_use": "use",
            "llm_name": "n/a (statistical metric)",
            "llm_description": "Not an LLM; Kendall's tau is a statistical correlation measure. The paper applies it to compare LLM-generated metric scores with human annotations.",
            "scientific_domain": "Evaluation statistics for NLG metrics",
            "evaluation_method": "Compute Kendall's tau between model-produced scores and human MQM scores at segment level; also report Pearson and Spearman as secondary measures. Significance assessed with a permute-both significance test.",
            "evaluation_criteria": "Correlation coefficients (Kendall as primary; Pearson and Spearman as supplementary). Statistical significance (p ≤ 0.05) via permutation tests.",
            "benchmark_or_dataset": "Correlations computed on the shared task test-sets (MT en-de/en-es/en-zh and summarization dataset).",
            "results_summary": "Kendall's tau was used to rank systems. Example: HIT-MI&T Lab probability-based method obtained top Kendall correlations in the small MT track (values in Table 6); top summarization systems had Kendall ~0.61–0.63.",
            "limitations_or_challenges": "Kendall's tau can be influenced by many ties and the distribution of scores; paper notes alternative Kendall variants may be explored (Deutsch et al., 2023). Small test sizes in some language pairs reduced reliability (NaNs appear for some entries).",
            "comparison_to_human_or_traditional": "Used to compare automatic metric outputs to human MQM judgments; also compared to inter-annotator Kendall agreement to contextualize performance.",
            "recommendations_or_best_practices": "Use Kendall's tau as a primary segment-level metric for NLG metric evaluation; supplement with Pearson and Spearman and use permutation tests for significance; consider modern variants of Kendall for ties.",
            "uuid": "e9907.1",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Probability-based",
            "name_full": "Probability-based generation evaluation",
            "brief_description": "An evaluation approach that uses LLM generation probabilities (e.g., likelihood of paraphrases or translations) as metric scores; often implemented by prompting the model to generate paraphrases/translations and computing conditional probabilities.",
            "citation_title": "Automatic machine translation evaluation in many languages via zero-shot paraphrasing.",
            "mention_or_use": "use",
            "llm_name": "OpenOrca-Platypus2-13B (used successfully by top MT team), other allowed LLMs",
            "llm_description": "OpenOrca-Platypus2-13B is an instruct-tuned LLaMA2-13B derivative; the shared task allowed both small (&lt;25B) and large (&gt;25B) models (e.g., Platypus2-70B, Guanaco-65B). Many probability-based submissions used quantized GPTQ 4-bit models to reduce resource needs.",
            "scientific_domain": "NLG metric evaluation / Machine translation",
            "evaluation_method": "Prompt LLM to produce paraphrases/translations for the source/translation pair and compute generation probabilities (likelihood) of target given source or paraphrase; use probabilities (possibly ensembled across prompts) as numeric quality scores.",
            "evaluation_criteria": "Correlation between probability-derived scores and human MQM scores (Kendall/Pearson/Spearman). Ensembling and selection of demonstration examples used to improve robustness.",
            "benchmark_or_dataset": "Applied on the shared task MT test-sets (en-de, en-es, en-zh) and compared to MQM annotations.",
            "results_summary": "A probability-based ensemble approach by HIT-MI&T Lab (Zhang et al., 2023) achieved the top Kendall correlations in the small MT track, outperforming many baselines including COMET-kiwiXXL, indicating strong performance for MT in reference-free settings.",
            "limitations_or_challenges": "Requires careful prompt design and example selection; computational cost can be high for large models due to probability computations; multilingual model competence affects success (may favor paraphrase-generation over direct translation understanding).",
            "comparison_to_human_or_traditional": "Probability-based scores correlated strongly with MQM human judgments in MT; sometimes exceeded output-based approaches on MT in this shared task.",
            "recommendations_or_best_practices": "Use ensembling of prompts, consider retrieval of in-context examples (e.g., SBERT/LABSE similarity selection), and test multiple prompt templates; quantization (GPTQ) can reduce compute cost.",
            "uuid": "e9907.2",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Output-based",
            "name_full": "Output-based natural language scoring",
            "brief_description": "An approach where LLMs are prompted to directly produce ratings, labels, or fine-grained error analyses in natural language and these outputs are parsed/aggregated to construct numeric scores.",
            "citation_title": "Large language models are state-of-the-art evaluators of translation quality.",
            "mention_or_use": "use",
            "llm_name": "various allowed LLMs (e.g., WizardLM-13B, Orca_mini_v3_7b, Nous-Hermes-13b, OpenOrca-Platypus2-13B)",
            "llm_description": "Small and medium instruct-tuned LLMs (≈7B–13B) commonly used in output-based submissions; many runs used 4-bit GPTQ quantized variants to reduce resource usage.",
            "scientific_domain": "NLG metric evaluation (summarization & MT)",
            "evaluation_method": "Provide natural language instructions (zero-shot or few-shot, sometimes chain-of-thought) asking the LLM to rate qualities (e.g., fluency, coherence, factuality) or to return a single aggregate score; parse numeric scores or aggregate sub-scores.",
            "evaluation_criteria": "Direct numeric labels produced by the model or aggregated from fine-grained error categories; evaluated via correlation with human MQM (MT) or heuristic summarization gold scores.",
            "benchmark_or_dataset": "Summarization test-set (derived from Wikipedia; SummEval used for dev), MT test-sets; multiple prompt variants tested.",
            "results_summary": "Output-based approaches led summarization results: DSBA and iML achieved top Kendall correlations (~0.60–0.63) in the small summarization track using carefully crafted prompts and aggregated criteria. Demonstrations (few-shot) sometimes hurt performance; lean prompts often worked well.",
            "limitations_or_challenges": "Explanations generated are often plausible but not necessarily faithful; many generated explanations were vague. Prompt design is critical; demonstrations can reduce performance in some cases.",
            "comparison_to_human_or_traditional": "Output-based LLM ratings showed high correlation with human judgments for summarization and can approach human inter-annotator agreement for some tasks.",
            "recommendations_or_best_practices": "Craft clear instruction prompts, evaluate both fine-grained and aggregated scoring, test with/without in-context examples, consider score bucketing/shortcuts for pathological outputs, and optionally request explanations as byproducts.",
            "uuid": "e9907.3",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SummEval Heuristic",
            "name_full": "Fine-grained reference-free summarization heuristic (α, β, γ)",
            "brief_description": "A novel heuristic introduced in this paper combining relevance, hallucination ratio, and MQM-like readability into a single reference-free summarization score with tunable weights α, β, γ.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "n/a (heuristic used as gold/target for scoring comparisons)",
            "llm_description": "Not an LLM; heuristic aggregates human annotations (relevance per source sentence, hallucinated characters ratio, MQM readability) into a numeric score; default weights used were α=3, β=5, γ=1 (alternative normalizations also explored).",
            "scientific_domain": "Summarization evaluation / NLG",
            "evaluation_method": "Annotators split source into sentences (treated as 'facts'), annotated relevance of facts, aligned summary spans to source facts, annotated hallucinations; combine components via formula to produce single numeric gold score for summarization.",
            "evaluation_criteria": "Relevance (sum over facts in summary weighted by annotator relevance), hallucination penalty (hallucinated characters / summary characters), and MQM readability component; weights control influence.",
            "benchmark_or_dataset": "Wikipedia-derived summarization test set (created for the shared task); SummEval used for dev/train.",
            "results_summary": "Heuristic was used to create the summarization gold scores; system rankings were stable across alternative normalizations (Appendix A). Top summarization systems correlated strongly with these heuristic gold scores (Kendall ≈0.60+).",
            "limitations_or_challenges": "The heuristic depends on sentence-as-fact assumption (coarse-grained) and on annotator consistency; longer summaries may be advantaged without length control (mitigated by controlling max tokens), and initial annotation batches required corrections.",
            "comparison_to_human_or_traditional": "This is a resource-efficient alternative to the pyramid method and aims to provide fine-grained, reference-free evaluation comparable to human judgments.",
            "recommendations_or_best_practices": "Control summary length during generation, perform manual corrections of sentence splits, choose weights α/β/γ transparently and test robustness across weight variants.",
            "uuid": "e9907.4",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Baselines",
            "name_full": "Baseline automatic metrics (BERTScore, SBERT similarity, SUPERT, GEMBA, COMET-Kiwi-XXL, BARTScore)",
            "brief_description": "A set of established and recent automatic metrics used as baselines for comparison: embedding-based similarity (BERTScore/SBERT), SUPERT (unsupervised summarization metric), generation-based BARTScore, GEMBA prompt-based DA scoring, and COMET-Kiwi-XXL as a trained MT metric.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "baseline models (non-generative or trained metric models like COMET variants); in addition, GEMBA uses LLM prompting (often closed models in literature).",
            "llm_description": "BERTScore uses contextual embeddings (here XLM-R-large); SBERT uses sentence-transformer embeddings; SUPERT uses bert-large-nli-stsb embeddings; COMET-Kiwi-XXL is a large COMET-style metric; BARTScore employs generation probabilities from BART-family models.",
            "scientific_domain": "NLG evaluation metrics",
            "evaluation_method": "Compute baseline metric scores per segment and measure correlation to human MQM / summarization gold; include per-allowed-LLM GEMBA DA prompt baseline variants.",
            "evaluation_criteria": "Kendall/Pearson/Spearman correlations with human judgments.",
            "benchmark_or_dataset": "Shared task datasets (WMT MQM partitions, SummEval, Wikipedia test set).",
            "results_summary": "Many simple baseline systems achieved high ranks. For MT, GEMBA and COMET-Kiwi-XXL were strong baselines; for summarization BERTScore and SUPERT variants were competitive. Some baseline LLMs (OrcaMini) beat larger allowed models in certain tracks.",
            "limitations_or_challenges": "Some baselines rely on references or pseudo-references; performance can vary strongly across domains and language pairs.",
            "comparison_to_human_or_traditional": "Baselines provide strong automatic proxies to human judgments and in some cases approach top submitted systems.",
            "recommendations_or_best_practices": "Include strong, well-understood baselines when evaluating new LLM-based metrics; simple prompting often suffices and can outperform complex demo-heavy prompts.",
            "uuid": "e9907.5",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Datasets",
            "name_full": "Eval4NLP datasets (train/dev/test: WMT MQM partitions, SummEval, and new Wikipedia test-set)",
            "brief_description": "Training/dev sets derived from WMT 2022 MQM (en-de, zh-en) and SummEval for summarization; novel test set constructed from Wikipedia articles created after 15.07.2023 (en-de, en-es, en-zh and summarization) to reduce pretraining overlap with allowed LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "n/a (datasets used to evaluate LLM-based metrics)",
            "llm_description": "N/A",
            "scientific_domain": "NLG evaluation (MT and summarization)",
            "evaluation_method": "Reference-free evaluation: participants receive source and candidate (translation or summary) and produce scores; these are compared to human annotations (MQM for MT; the introduced summarization heuristic for summarization).",
            "evaluation_criteria": "Human annotations (MQM for MT; relevance/factuality/readability + hallucination heuristic for summarization) used as gold; metrics evaluated via correlations.",
            "benchmark_or_dataset": "WMT 2022 MQM subsets (train/dev), SummEval (dev/train), and the new reference-free Wikipedia test set (test-phase) created for the shared task.",
            "results_summary": "The novel Wikipedia-based test set enabled evaluation of reference-free LLM metrics across three MT language pairs and summarization; allowed identification that several small-model approaches achieved top correlations on this test set.",
            "limitations_or_challenges": "Potential leakage: Wikipedia articles post 15.07 could still overlap with LLM pretraining corpora; some articles may be translated or generated; dataset size is moderate and some annotation agreements (en-es) were low.",
            "comparison_to_human_or_traditional": "Combines human fine-grained annotations and resource-efficient summarization heuristics to approximate human evaluation standards; matches WMT-style MQM usage.",
            "recommendations_or_best_practices": "When constructing test sets to evaluate open-source LLMs, select source texts created after recent model release dates to reduce pretraining overlap, and perform quality pre-filtering (Wikipedia quality class, language-tool).",
            "uuid": "e9907.6",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SharedTaskResults",
            "name_full": "Eval4NLP 2023 shared task empirical findings",
            "brief_description": "Empirical outcomes and analyses from the shared task: ranking of participant methods, per-track winners, and general observations about prompting, model size, and explainability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "various allowed LLMs (OpenOrca-Platypus2-13B, orca_mini_v3_7b, Platypus2-70B, Guanaco-65B, WizardLM-13B, Nous-Hermes-13b)",
            "llm_description": "Allowed LLMs spanned small (&lt;25B) and large (&gt;25B) models; many submissions used quantized GPTQ 4-bit variants for resource efficiency. Some models are instruct-tuned derivatives of LLaMA2.",
            "scientific_domain": "NLG evaluation metrics (MT and summarization)",
            "evaluation_method": "Participants produced newline-separated scores for dataset segments using allowed LLMs without finetuning; scores were evaluated against human MQM / summarization gold using Kendall/Pearson/Spearman and permutation tests for significance.",
            "evaluation_criteria": "Segment-level correlations (Kendall primary) to human judgments; additional analyses of explainability plausibility via small human preference study.",
            "benchmark_or_dataset": "Shared task datasets described above (WMT MQM partitions, SummEval, new Wikipedia test set).",
            "results_summary": "Key findings: (1) Small-model solutions often outperformed large-model ones in both MT and summarization tracks; (2) For MT the top system (HIT-MI&T Lab) used a probability-based ensemble on OpenOrca-Platypus and attained the highest Kendall in the small MT track; (3) For summarization top teams (DSBA, iML, Akkasi et al.) used carefully crafted prompts (output-based) and achieved Kendall ≈0.60–0.63; (4) Many simple baselines were competitive; (5) Explanations provided by LLMs were frequently vague and a small human preference experiment found no clear winner between two systems' explanations.",
            "limitations_or_challenges": "Explainability track had only 2 submissions and small human evaluation (50 samples) limiting conclusions; possible dataset pretraining leakage; unequal annotator experience and some annotation quality issues (notably en-es). Computational constraints led many participants to use small models.",
            "comparison_to_human_or_traditional": "Top automatic metrics reached correlations similar to inter-annotator agreement on small subsets, suggesting LLM-based metrics can approximate human judgments without fine-tuning.",
            "recommendations_or_best_practices": "Restricting models and disallowing fine-tuning focuses evaluation research on prompt and score-extraction design; encourage lean prompts, ensemble probability-based approaches for MT, careful prompt crafting for summarization, and exploration of pipeline/iterative methods (tree-of-thoughts, self-refinement) in future work.",
            "uuid": "e9907.7",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ExplainabilityEval",
            "name_full": "Explainability assessment of LLM-generated metric explanations",
            "brief_description": "A small-scale human evaluation comparing plausibility/preference of explanations generated by LLMs alongside numeric scores; used to gauge the perceived usefulness of LLM-produced explanations for metric decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Baswani et al. system (uses Orca variants) and Mahmoudi system (Orcamini/orca_mini_v3_7b)",
            "llm_description": "Participating systems prompted their models to produce natural language explanations as well as scores; commonly used small quantized models (e.g., orca_mini_v3_7b).",
            "scientific_domain": "Explainable NLG evaluation",
            "evaluation_method": "Human annotators (two team members) compared paired explanations (for the same sample) from two participant systems over 50 randomly selected summarization samples and indicated preference; annotator agreement and counts recorded.",
            "evaluation_criteria": "Human preference between explanations, qualitative assessment for specificity versus vagueness; agreement percentage used as an indicator (56% agreement observed).",
            "benchmark_or_dataset": "50 random samples from the shared-task summarization test set with provided MQM annotations.",
            "results_summary": "Annotators showed no strong preference: one annotator preferred system A 27 vs 23, the other 24 vs 26, with 56% agreement — indicating no clear superiority; many explanations were generic ('good coherence and fluency'), while some correctly identified real issues (e.g., word repetition).",
            "limitations_or_challenges": "Very small-scale (50 samples) and only two systems submitted explanations; explanations may be plausible but not faithful; limited annotator pool.",
            "comparison_to_human_or_traditional": "Human evaluators did not strongly prefer either system's explanations; human evaluation remains necessary to judge explanation utility.",
            "recommendations_or_best_practices": "Encourage richer, more specific explanations from LLMs, and scale human evaluation of explanations; distinguish plausibility from faithfulness when assessing explanations.",
            "uuid": "e9907.8",
            "source_info": {
                "paper_title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality.",
            "rating": 2
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire.",
            "rating": 2
        },
        {
            "paper_title": "Bartscore: Evaluating generated text as text generation.",
            "rating": 2
        },
        {
            "paper_title": "Automatic machine translation evaluation in many languages via zero-shot paraphrasing.",
            "rating": 2
        },
        {
            "paper_title": "SummEval: Re-evaluating summarization evaluation.",
            "rating": 2
        },
        {
            "paper_title": "Multidimensional quality metrics (mqm): A framework for declaring and describing translation quality metrics.",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "rating": 1
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 1
        }
    ],
    "cost": 0.01973275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Christoph Leiter ${ }^{<em>}$, Juri Opitz ${ }^{\dagger}$, Daniel Deutsch ${ }^{\ddagger}$, Yang Gao</em> Rotem Dror ${ }^{\dagger \dagger}$, Steffen Eger<em><br></em> Bielefeld University, Germany $\dagger$ Heidelberg University, Germany<br>$\ddagger$ Google, US $\diamond$ Google Research, UK $\dagger \dagger$ University of Haifa, Israel<br>christoph.leiter@uni-bielefeld.de opitz.sci@gmail.com<br>dandeutsch@google.com gaostayyang@google.com<br>rdror@is.haifa.ac.il steffen.eger@uni-bielefeld.de</p>
<h4>Abstract</h4>
<p>Generative large language models (LLMs) have seen many breakthroughs over the last year. With an increasing number of parameters and pre-training data, they have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Approaches often differ in the input prompts, the samples that are selected for demonstration and the construction process of scores from the output. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore such approaches for machine translation evaluation and summarization evaluation. Specifically, we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We evaluate the approaches of the participants on a new reference-free test-set spanning 3 language pairs for machine translation as well as a summarization dataset. Further, we present an overview of the approaches taken by the participants, present their results on the test set and analyze paths for future work. Finally, as a separate track, we perform a small-scale human evaluation of the plausibility of explanations given by the LLMs. We make parts of our code and datasets available. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The ChatGPT revolution in late 2022 has ignited a wide public and scientific debate about the possibilities (and limitations) of generative AI in various fields and application scenarios (Leiter et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2023b; Eger et al., 2023), including education (Halaweh, 2023), logic (Liu et al., 2023a), medicine (Dave et al., 2023), math (Frieder et al., 2023), programming (Rozière et al., 2023) and science (Belouadi et al., 2023).</p>
<p>The immense research interest has also triggered the exploration of numerous approaches that leverage generative large language models (LLMs) as evaluation metrics (Kocmi and Federmann, 2023; Liu et al., 2023b; Fu et al., 2023; Xu et al., 2023b; Fernandes et al., 2023) for natural language generation (NLG) tasks like machine translation (MT) and summarization. Recent LLM based approaches differ, for example, in their prompting strategies, e.g., in the way that natural language instructions are used to trigger the LLM to compute metric scores. For example, GEMBA (Kocmi and Federmann, 2023) uses zero-shot prompting to directly predict scores or quality labels in the output. In contrast, AutoMQM (Fernandes et al., 2023) instructs LLMs to predict fine-grained error labels and uses these to compute the final scores. These works have contributed to the exploration of prompting for NLG evaluation, but an exhaustive exploration of approaches remains unaddressed. Further, many approaches leverage closed source LLMs while much fewer use open source LLMs. Those approaches relying on open source LLMs put a large focus on acquiring training data (e.g. Xu et al., 2023b) and fine-tune models to specific tasks. Given this typical focus on fine-tuning and motivated by promising work on prompting techniques ${ }^{2}$ (e.g. Wei et al.,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: Using a generative LLM as MT evaluation metric. In this example, the metric is reference-free. I.e. it grades the translated sentence based on its source sentence. The input sentences are wrapped into a prompt that is given to an LLM. The LLM generates an output and a final score could for example be constructed from this textual output or from other values involved in the process. The red borders indicate the focus of our shared task. Participants should evaluate the best prompts and the best approaches to construct scores from model output.</p>
<p>2022; Yao et al., 2023; Wang et al., 2023; Zhou et al., 2023), we notice a research gap in the thorough examination of <em>prompting and score composition in the domain of NLG metrics</em>, especially for <strong>open-source</strong> generative LLMs.</p>
<p>The Eval4NLP 2023 shared tasks aims to fill this gap by disallowing participants to fine-tune models and by restricting model usage to a fixed list of LLMs (see Figure 1). Hence, participants may only vary how models are prompted, how scores are extracted, and how models are used in combination. To make the task more inclusive, we consider large and small(er) LLM's in two separate tracks. This is different from shared tasks without model restriction, where the largest models often perform best, for example, the WMT metrics shared task (e.g. Freitag et al., 2022).</p>
<p>The goal of the shared task is to design evaluation metrics for MT and summarization, which we select as sub-tasks of NLG, while adhering to the model restrictions. Our contributions are the following:</p>
<ul>
<li>We design a novel, restricted evaluation setting that allows to focus on <em>prompting and score extraction</em> in building evaluation metrics. This might aid inexpensive development of new metrics without fine-tuning or could benefit the selection of metric architectures with fine-tuning.</li>
<li>We organized a CodaLab (Pavao et al., 2023) / Codabench (Xu et al., 2022) competition where participants could submit their system scores in a dev- and test-phase. The dev-phase has received 44 participant registrations, of which 9 teams have submitted contributions to the test-phase leaderboard and system papers. This paper summarizes their approaches and findings and presents their final ranking.</li>
<li>We collect a novel dataset from Wikipedia articles created past the 15.07.2023 with the goal of minimizing the use of data that has been used to pre-train LLaMA2 (Touvron et al., 2023) released on 17.07.2023. This is because some of the allowed models are fine-tuned versions of LLaMA2.</li>
<li>In line with the Eval4NLP 2021 shared task (Fomicheva et al., 2021), we consider the <em>explainability</em> of the designed metrics. The generative nature of LLMs allows to return natural language or formatted explanations of its output. While these explanations are not necessarily faithful, they also offer value if</li>
</ul>
<p>2022; Yao et al., 2023; Wang et al., 2023; Zhou et al., 2023), we notice a research gap in the thorough examination of <em>prompting and score composition in the domain of NLG metrics</em>, especially for open-source generative LLMs.</p>
<p>The Eval4NLP 2023 shared tasks aims to fill this gap by disallowing participants to fine-tune models and by restricting model usage to a fixed list of LLMs (see Figure 1). Hence, participants may only vary how models are prompted, how scores are extracted, and how models are used in combination. To make the task more inclusive, we consider large and small(er) LLM's in two separate tracks. This is different from shared tasks without model restriction, where the largest models often perform best, for example, the WMT metrics shared task (e.g. Freitag et al., 2022).</p>
<p>The goal of the shared task is to design evaluation metrics for MT and summarization, which we select as sub-tasks of NLG, while adhering to the model restrictions. Our contributions are the following:</p>
<ul>
<li>We design a novel, restricted evaluation setting that allows to focus on <em>prompting and score extraction</em> in building evaluation metrics. This might aid inexpensive development of new metrics without fine-tuning or could benefit the selection of metric architectures with fine-tuning.</li>
<li>We organized a CodaLab (Pavao et al., 2023) / Codabench (Xu et al., 2022) competition where participants could submit their system scores in a dev- and test-phase. The dev-phase has received 44 participant registrations, of which 9 teams have submitted contributions to the test-phase leaderboard and system papers. This paper summarizes their approaches and findings and presents their final ranking.</li>
<li>We collect a novel dataset from Wikipedia articles created past the 15.07.2023 with the goal of minimizing the use of data that has been used to pre-train LLaMA2 (Touvron et al., 2023) released on 17.07.2023. This is because some of the allowed models are fine-tuned versions of LLaMA2.</li>
<li>In line with the Eval4NLP 2021 shared task (Fomicheva et al., 2021), we consider the <em>explainability</em> of the designed metrics. The generative nature of LLMs allows to return natural language or formatted explanations of its output. While these explanations are not necessarily faithful, they also offer value if</li>
</ul>
<p>they are plausible (Leiter et al., 2023a) or might support the generation process itself (Wei et al., 2022).</p>
<p>Our paper is structured into 8 sections. $\S 2$ gives an overview of how our shared task is related to other competitions. $\S 3$ describes the competition setup and $\S 4 / \S 5$ describe the datasets and annotation process for the test phase respectively. In $\S 6$, we highlight the approaches tested by the participants, especially those for the test set submissions. $\S 7$ presents the final scores of the participants on the test set and further analyses. Finally, $\S 8$ discusses future work and provides a conclusion.</p>
<h2>2 Related Work</h2>
<p>In this paragraph, we describe other work that is related to our shared task. In specific, we give a brief overview of evaluation metrics, highlight the recent development on metrics that are based on generative LLMs and describe related shared tasks.</p>
<p>NLG evaluation metrics The evaluation of NLG systems is necessary to compare them to other Systems and generally evaluate their applicability in intended scenarios. Manual/human evaluation is expensive, time consuming and often infeasible for larger datasets. Hence, automatic metrics are constructed. Many early metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) measure the lexical overlap between the generation and a human written reference. Metrics that use manually annotated references are called reference-based, while metrics that evaluate the generation quality based on the source text are called reference-free (in MT also Quality Estimation, QE). The early metrics that are based on lexical overlap have limitations in their ability to capture semantics of generated text (e.g. Reiter, 2018). For example, a generation might not be graded as good if it uses paraphrases of the reference texts. Newer metrics are usually based on language models that are able to embed the meanings of tokens (e.g. Zhang et al., 2020; Zhao et al., 2019; Sellam et al., 2020; Rei et al., 2020). These metrics achieve strong correlations to human judgments of generation quality (e.g. Freitag et al., 2022). Embedding based metrics have also enabled reference-free evaluation. This has the added benefit of no longer needing human reference generations and therefore enables further use cases, such as checking generation quality on the fly (e.g. Zerva et al., 2022), training with metrics as supervision signal (e.g. Wu et al., 2018) and
using metrics during decoding (Fernandes et al., 2022). However, the usage of black-box systems in the evaluation process also poses new challenges. For example, it can be difficult to understand why metrics exhibit certain behavior, they might lack robustness and fail in unexpected scenarios and they might show social biases (e.g. Leiter et al., 2023a). Surveys on NLG metrics are presented by (e.g. Celikyilmaz et al., 2021; Sai et al., 2022).</p>
<p>Generation-based evaluation metrics Related work includes other generation-based metrics. Beginning with PRISM (Thompson and Post, 2020) and BARTScore (Yuan et al., 2021), generationbased metrics have shown strong performance. These two metrics use the generation probability of paraphrases or translations as metric scores. Newer work that follows the same principle with more high-performing LLMs has shown improved scores (e.g. Fu et al., 2023). Another branch of generation-based metrics has originated with recent GPT models and shows that models can directly perform the task of grading machine generated text from in-context task descriptions (e.g. Kocmi and Federmann, 2023; Chiang and Lee, 2023; Fu et al., 2023; Xu et al., 2023b; Yang et al., 2023; Lu et al., 2023). We will refer to these metrics as outputbased. Here, the rating is usually returned directly in the generated output text or constructed from it. Another branch of these models employs generative LLMs for ranking between better and worse generations (Zheng et al., 2023; Shen et al., 2023; Ji et al., 2023).</p>
<p>This recent surge of approaches has motivated our shared task. During the runtime of the shared task, other state-of-the-art approaches have been published (e.g. Fernandes et al., 2023). The systems submitted to our competition are different from most generation-based metrics in thoroughly exploring the usage of fixed recent open-source LLMs since ChatGPT without the usage of finetuning.</p>
<p>Evaluation Shared Tasks Our shared task is also related to other shared tasks that consider the evaluation of evaluation metrics for NLG, especially for MT and summarization. For MT, the established WMT workshop comprises multiple shared tasks on MT evaluation. Especially, the WMT metrics shared task (e.g. Mathur et al., 2020; Freitag et al., 2021b, 2022) and the WMT shared task on quality estimation (e.g. Specia et al., 2020, 2021; Zerva et al., 2022) are related to ours. The main track of</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Schematic overview of possible approaches to compute scores from a generative LLM. Zero-shot approaches do not present examples in the prompt, while few-shot approaches present them. Chain-of-though (Wei et al., 2022) approaches trigger the LLM to generate an explanation of its process before returning the final score. Fine-grained approaches, e.g., Fernandes et al. (2023), first construct a detailed error analysis and then construct a final score from them. Translation probability approaches, e.g., Fu et al. (2023), use the probability of generating a paraphrase as a translation. In a majority vote approach the results from multiple prompts could be combined. Self-refinement approaches could trigger a model multiple times to refine its output.</p>
<p>The <em>WMT metrics shared task</em> considers the system- and segment-level evaluation quality of MT metrics — that is, how well can metrics reflect the quality of whole MT systems or single segment translations. Recent years also put a focus on evaluating the robustness of metrics towards certain linguistic phenomena. The main track of the <em>WMT metrics shared task</em> consists of a reference-based evaluation, i.e., metrics compare the machine translation to human-written reference translations. Recent editions also contain a track for reference-free evaluation, where submitted metrics should directly compare the machine translation to its source text. Since 2021, the <em>WMT metrics shared task</em> has acquired its test data using the fine-grained MQM evaluation scheme (Lommel et al., 2014; Freitag et al., 2021a) that has been shown to be more accurate than crowd-sourced direct assessment annotations. The <em>WMT shared task on quality estimation</em> sets its main focus on the reference-free evaluation of machine translations. In recent years, their test sets are also annotated with MQM. Additionally, the quality estimation workshop has, for example, conducted tasks on word-level error prediction and span-level error severity prediction.</p>
<p>Like the WMT QE shared task, our task is the reference-free evaluation of machine translations. The biggest difference of our shared task is that we fix the allowed models. That means, participants may only use models from a list we provide to them. Hence, participants have to focus on a thorough exploration of prompting and score extraction rather than fine-tuning and dataset creation. A second difference is that we include summarization as a subtask. As a third difference, our shared task has a subtrack to evaluate explanations that are created as a byproduct of scoring with generative LLM's for plausibility. This last point offers parallels to the Eval4NLP 2021 shared task (Fomicheva et al., 2021) and its successor subtask at the WMT 2022 shared task (Zerva et al., 2022) on quality estimation. These tasks treated human word-level error annotations as explanations of translation quality and evaluated their correlations to manual annotations. In our subtask, we allow for any kind of explanation. Background information on explain-</p>
<p>ability for machine translation metrics can be found in Leiter et al. (2023a).</p>
<h2>3 Shared Task Setup</h2>
<p>As described in $\S 1$, the goal of our shared task is to leverage generative LLMs as (explainable) metrics for MT and summarization. ${ }^{3}$ Thereby, participants are not allowed to fine-tune their models and only certain models are allowed. Figure 1 shows the general setup of using generative LLMs as metrics, illustrated with an example from MT. The figure shows that final scores could be constructed from the generated model output or from other variables involved in the inference process. Specifically, recent work on prompting and metrics offer a wide range of possibilities to influence score construction even without fine-tuning. Some of them are shown in Figure 2.</p>
<p>LLM sizes We organize two tracks based on the model sizes. Models smaller than 25B parameters are considered as small, and models bigger than 25B parameters as large. Table 1 gives an overview of the allowed models. We mainly choose these models based on their good average performance on the Huggingface Open LLM Leaderboard. ${ }^{4}$ For Platypus2, Guanaco and WizardLM, we use 4-bit quantized versions with GPTQ (Frantar et al., 2023) to lower the system requirements to run them. Of these models, only the Guanaco model was explicitly fine-tuned with multilingual data. The models Wizard, Nous and Guanaco were allowed for use from the start of the competition, while the other 3 models were added to the list 20 days later. In another track, we explore the explanatory value of explanations created as a byproduct of the scoring process (see §7).</p>
<p>Phases Our shared task was conducted in two phases. First, we hosted a dev-phase on CodaLab ${ }^{5}$ (Pavao et al., 2023) from 07.08.23 to 30.09.23. In this phase, participants were developing their approaches and could already evaluate their scores on a leaderboard. While the standing in the devphase does not influence the ranking of the shared task, the phase aided the creation of a competitive atmosphere, acted as an advertisement for the competition and allowed us to gauge the number of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>interested participants. The main part of the competition was the test-phase conducted from 26.09.23 to 01.10.23. Due to performance problems and unforeseen issues with extending the competition setup on CodaLab, the test phase was migrated to its successor Codabench ${ }^{6}$ (Xu et al., 2022). Submissions to the dev-phase and test-phase both had to contain at least a file with newline separated scores that grade each sample of our datasets. The test-phase additionally required to enter a team name, to indicate the track for each submission and to provide additional files with (1) a short system description, (2) newline separated prompts for each input, and (3) optionally newline separated explanations.</p>
<p>We describe the shared task datasets in $\S 4$.</p>
<h2>4 Datasets</h2>
<p>During the dev-phase of our shared task, we provided participants with a train- and a dev-set. For the test-phase, we further added a test-set.</p>
<p>Train- \&amp; Dev-set Our train- and dev-sets are constructed from two datasets. For MT, we select the en-de and zh-en MQM partitions of the WMT 2022 metrics shared task (Freitag et al., 2022). For summarization, we select SummEval (Fabbri et al., 2021). We conduct our task in a reference-free setting, that is, we do not provide human written reference translations or summaries. Hence, we remove the references provided with WMT and SummEval. SummEval has separate scores for relevance, factuality, coherence and consistency for each sample. We construct a single score per example by averaging these separate scores. Further changes to the original datasets include the split into train- and dev-partitions as well as shuffling. In the dev phase participants could experiment with generalizable (prompting) approaches.</p>
<p>Test-set We collect a novel test set for the testphase of our shared task. It consists of 3 language pairs for MT: en-de, en-es, en-zh and a summarization part. We only choose high-resource languages, as the LLaMA(2)-based models have seen limited multilingual data during their pre-training and fine-tuning. Hence, high-resource languages can indicate an upper bound of what these models can achieve without further fine-tuning. To reduce the possibility that our chosen LLMs were trained</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Mode</th>
<th style="text-align: left;">Release Date</th>
<th style="text-align: left;">Track</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Platypus2-70B-Instruct-GPTQ ${ }^{7}$ (Lee et al., 2023a)</td>
<td style="text-align: left;">11.08 .23</td>
<td style="text-align: left;">Large</td>
</tr>
<tr>
<td style="text-align: left;">Guanaco-65B-GPTQ ${ }^{8}$ (Dettmers et al., 2023)</td>
<td style="text-align: left;">25.05 .23</td>
<td style="text-align: left;">Large</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM-13B-V1.1-GPTQ ${ }^{9}$ (Xu et al., 2023a)</td>
<td style="text-align: left;">07.07 .23</td>
<td style="text-align: left;">Small</td>
</tr>
<tr>
<td style="text-align: left;">Nous-Hermes-13b ${ }^{10}$</td>
<td style="text-align: left;">03.06 .23</td>
<td style="text-align: left;">Small</td>
</tr>
<tr>
<td style="text-align: left;">OpenOrca-Platypus2-13B ${ }^{11}$ (Lee et al., 2023b; Mukherjee et al., 2023)</td>
<td style="text-align: left;">11.08 .23</td>
<td style="text-align: left;">Small</td>
</tr>
<tr>
<td style="text-align: left;">orca_mini_v3_7b ${ }^{12}$ (Mathur, 2023; Mukherjee et al., 2023)</td>
<td style="text-align: left;">07.08 .23</td>
<td style="text-align: left;">Small</td>
</tr>
</tbody>
</table>
<p>Table 1: Generative LLMs whose usage was allowed in the Eval4NLP 2023 shared task.
on parts of the test set, we gather Wikipedia articles created after 15.07 .23 as source texts. ${ }^{13}$</p>
<p>Figure 3 shows the score distributions of our datasets. We can see that all language pairs exhibit a pattern of centering around values divisible by 5. This makes sense, as MQM weighs major errors with 5 points. Also, in en-es, samples have generally received a higher score; i.e., fewer major errors were annotated. Finally, our summarization dataset, which uses a combined annotation scheme (see §5) does not show this pattern.</p>
<h2>5 Annotation</h2>
<p>In this section, we describe the annotation process of our dataset. For MT annotation, we hire one annotator per language pair: one Master student who speaks Spanish as mother tongue with English certifications, one NLP Bachelor student, who is a native English speaker that lives in Germany since many years, and one data and discourse studies Master student, who is a native Chinese speaker who uses English on a daily basis. For summarization annotation, we hire one NLP Bachelor student as well as a data and discourse studies Master student with a prior master in linguistics. Both annotators annotated the same data. All annotators demonstrated their suitability for the role in initial test rounds with further applicants. The distribution of our final MT dataset is shown in Table 3. The total annotation costs were ca. $5000 €$.</p>
<p>We use Google's Anthea ${ }^{14}$ as annotation tool, because of its support for MQM annotations (Lommel et al., 2014; Freitag et al., 2021a). As we mostly annotate single sentences for MT, we modify Anthea to provide context via a Wikipedia URL that can be consulted if annotators are unsure about a translation. For summarization, annotations were</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>conducted in a modified version of Anthea with a new template (we show a screenshot of the UI in Appendix C).</p>
<p>For both data sets, we perform fine-grained annotations. In MT this has been shown to yield more reliable human annotations than other annotation schemes (Freitag et al., 2021a). Also, the fine-grained annotations could be used later-on to verify automatically generated explanations. As we only received 2 submissions for the explainability track, we do not consider apply this in this report.</p>
<p>MT We construct the MT dataset from random source sentences with a minimum length of 110 characters, as tokenized by the NLTK sentence tokenizer ${ }^{15}$. In a few cases, multiple sentences are concatenated due to missing spaces between dots. We obtain machine translations with 4 different translation models (see Table 2). Further, we use MQM as annotation scheme and conducted the annotation process in multiple batches to allow for corrections in subsequent batches. The batch sizes varied between 200 and 600 samples. For the first batch, we changed parts of the process during the annotation. Specifically, we had accidentally chosen an incorrect tokenization for the first few samples of the first batch. ${ }^{16}$ This may have led to coarser annotation and to ignoring some punctuation issues. We still use these samples, as punctuation errors only have a very small weight in MQM and a coarser annotation does not change the severity assigned to errors. Hence, we assume that the impact on the MQM scores is minimal. Another change between annotation versions is that the first batch contains</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Score distributions of our datasets. The annotation process is described in $\S 5$.
unordered sentences, while in the second version, all translations of a single source follow each other (in a random order). This has majorly improved the annotation speed as annotators do not need to reread the source sentences anymore. Further, the annotators commented on difficult source texts in the first batch. Therefore, in the following batches, we pre-filter the Wikipedia source articles by their quality classes ${ }^{17}$ and keep only c-class and better articles. Furthermore, we employ languagetool ${ }^{18}$ to filter for the grammatical correctness of the source sentences.</p>
<p>To verify the quality of the dataset, members of our team who are native speakers of the respective</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>target languages have annotated small subsets of 30-50 samples of the datasets. Table 4 shows the agreement on these subsets. For en-es, either the MT models were more performant, the annotator might have been missing some errors or annotating them less strictly, as suggested by Figure 3.</p>
<p>Summarization We select random sections from Wikipedia that have a length of 150 to 800 tokens as measured by the tokenizer of bart-large-cnn. The summarization models we use are listed in Table 2. To create a dataset that offers as much explanatory value on the summary quality as possible, we perform a fine-grained evaluation inspired by MQM. However, we cannot simply reuse all criteria of the MQM commonly used in MT, as instead of fulfilling the criteria of adequacy, sum-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MT Models ${ }^{19}$</th>
<th style="text-align: left;">Summarization Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">mbart50_en2m (Fan et al., 2021)</td>
<td style="text-align: left;">sshleifer/distilbart-cnn-12-6 ${ }^{20}$ (Shleifer and Rush, 2020)</td>
</tr>
<tr>
<td style="text-align: left;">mbart50_m2m (Fan et al., 2021)</td>
<td style="text-align: left;">facebook/bart-large-cnn ${ }^{21}$ (Lewis et al., 2020)</td>
</tr>
<tr>
<td style="text-align: left;">m2m_100_418M (Tang et al., 2021)</td>
<td style="text-align: left;">google/bigbird-pegasus-large-bigpatent ${ }^{22}$ (Zaheer et al., 2020)</td>
</tr>
<tr>
<td style="text-align: left;">m2m_100_1.2B (Tang et al., 2021)</td>
<td style="text-align: left;">facebook/bart-large-xsum ${ }^{23}$ (Lewis et al., 2020)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">mT5_multilingual_XLSum ${ }^{24}$ (Hasan et al., 2021)</td>
</tr>
</tbody>
</table>
<p>Table 2: An overview of the translation and summarization models we have used to created our datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Dev</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">en-de</td>
<td style="text-align: left;">11046</td>
<td style="text-align: left;">7364</td>
<td style="text-align: left;">1425</td>
</tr>
<tr>
<td style="text-align: left;">en-es</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">1834</td>
</tr>
<tr>
<td style="text-align: left;">en-zh</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">1161 (1297)</td>
</tr>
<tr>
<td style="text-align: left;">zh-en</td>
<td style="text-align: left;">15750</td>
<td style="text-align: left;">10500</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">summarization</td>
<td style="text-align: left;">320</td>
<td style="text-align: left;">1280</td>
<td style="text-align: left;">671 (825)</td>
</tr>
</tbody>
</table>
<p>Table 3: Number of samples in our datasets. In the case of the brackets, we filtered out potentially malformed examples after the test phase was conducted.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Agreement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">en-de</td>
<td style="text-align: center;">0.458</td>
</tr>
<tr>
<td style="text-align: center;">en-es</td>
<td style="text-align: center;">0.239</td>
</tr>
<tr>
<td style="text-align: center;">en-zh</td>
<td style="text-align: center;">0.480</td>
</tr>
<tr>
<td style="text-align: center;">summarization</td>
<td style="text-align: center;">0.625</td>
</tr>
</tbody>
</table>
<p>Table 4: Kendall agreement between annotators. For MT, the agreement was calculated on 30-50 samples. For summarization, it was calculated on 373 examples.
maries need to capture the most relevant facts (relevance) and only represent correct facts (factuality). Specifically, we orient ourselves on the quality criteria for summaries by Dang (2005); Fabbri et al. (2021): relevance, factuality, and readability, where readability includes the property of coherence and fluency. We note that readability is already covered to a large degree by the MT MQM annotation guidelines. We change them by removing adequacy and adding coherence. Coherence has the following sub-categories: referential clarity, redundancy, structure, and meaning. The meaning category refers to cases where the summary changes the meaning of the source text without hallucinating, e.g., by concatenating facts in the wrong order.</p>
<p>One common approach to determine the relevance and factuality of summaries is the pyramid approach (Nenkova and Passonneau, 2004). Here, small atomic facts of many human written references are collected and ordered in a pyramid, based
on their occurrence count. Instead we introduce a more resource efficient approach, where we use a reference-free method for annotating the summaries' relevance and factuality. Inspired by Liu et al. (2023c), who manually split the source text into atomic facts, we leverage the NLTK sentence tokenizer to split the source text into enumerated sentences. In some cases, sentences were not split correctly. In sentences of the final test set, we have corrected them manually. We treat each sentence as a single fact. ${ }^{25}$ Next, we annotate the relevance of each of these facts, i.e., how likely would the annotator use the fact in a given sentence if they should write a summary themselves. Then, we annotate which source sentence is reflected in which part of the summary. By doing so, we can weigh the relevance of each fact that appears in the summary. Finally, we annotate each fact not represented in the original source text as a hallucination. Based on these components, we build a heuristic that is negative for bad summaries and positive for good summaries. The equation is shown in Figure 4. $\alpha$, $\beta$ and $\gamma$ can be chosen to determine the influence of each sub-score for relevance, hallucinations and readability, respectively. There are many design choices regarding the weighting of each component and different normalization approaches. We find that these generally only have a small impact on the final ranking of our shared task (see Appendix A). Longer summaries can contain more facts and would hence receive higher scores in this heuristic. We address this issue by generating summaries of similar lengths using max token settings. The example in Figure 5 shows this annotation process.</p>
<p>Like with MT, we annotated in several batches. After the first batch, as for MT, we took measures to improve the source quality and ordered the sources to allow for faster annotations. After a check on the annotation quality, some misunderstandings of the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$\sum_{i \in \text { Facts in Summary }} \alpha * \operatorname{relevance}(i)+\beta * \frac{|\text { Hallucinated Characters }|}{|\text { Characters in the summary }|}+\gamma *$ MQM</p>
<p>Figure 4: A heuristic for fine-grained reference-free evaluation of summaries. We set $\alpha=3, \beta=5$ and $\gamma=1$.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: An example of the summarization annotation process.
annotation classes were uncovered and discussed. In the final evaluation, we drop all examples labeled before this discussion, such that we keep a total of 671 samples. Further, one annotator showed a larger annotation speed and a more consistent understanding of the task. In the test set, we use the annnotations of this annotator.</p>
<p>Table 4 shows the agreement between the annotators. It is high for relevance and factuality
annotations and lower for the MQM part.</p>
<h2>Evaluation</h2>
<p>Following earlier WMT tasks on segment-level evaluation, we compute Kendall's tau correlation (KENDALL, 1945) to compare the system generated scores to human scores. We further report the Spearman and Pearson correlations. ${ }^{26}$ Future work</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>could explore if the usage of other and possibly more suited variants of Kendall, as suggested by Deutsch et al. (2023), might affect the rankings of our competition.</p>
<h2>6 Shared Task Approaches</h2>
<p>The test phase of our shared task received submissions from 12 different teams, 9 of which submitted system papers. Here, we summarize the approaches of these 9 systems and announce their final standings. Table 5 gives an overview of the participating teams and of the tracks they are participating in. ${ }^{27}$ This table can be used as a mapping for the scores reported in $\S 7$.</p>
<p>We divide the approaches taken by the participants into probability-based, output-based and agent-based. ${ }^{28}$ Besides their final approaches, the participants have explored a large number of possible variations. Afterwards, we introduce the baseline approaches, we compare the participants with.</p>
<p>Probability-based Probability-based approaches calculate how likely a paraphrase or translation of an input is generated with an LLM. Probability based approaches are explored by Zhang et al. (2023) and Pradhan and Todi (2023). Zhang et al. (2023) define 10 different prompts to translate a source sentence with an LLM. They combine this approach with demonstrating samples in the input prompt selected by (among others) SBERT (Reimers and Gurevych, 2019). Further, they use ensembles to recombine the scores of multiple prompts and models. Pradhan and Todi (2023) use the probability-based approach with own prompts and prompts designed by the authors of GPTScore (Fu et al., 2023).</p>
<p>Output-based All submitted papers explore the direct usage of an LLM's natural language output as score. Zhang et al. (2023) test the same sample selection and ensembling strategies described above with 4 different prompts in an output-based setting. Larionov et al. (2023) follow a similar approach to Zhang et al. (2023) and retrieve demonstration examples by finding similar examples with LABSE (Feng et al., 2022) embeddings in an output-based setting. Pradhan and Todi (2023)</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>try one approach in which they present a prompt that triggers the prediction of a single score and one approach that triggers the model to first rate summary qualities for consistency, coherence, fluency and relevancy. Then they aggregate these scores in 3 different ways. Baswani et al. (2023) quantize Orcamini themselves to run an even smaller model (which is close to violating the allowed settings of the shared task). They provide a detailed explanation to their model that triggers it to produce fine-grained scores and a combined score in the same output. Kim et al. (2023) choose rating guidelines from related work - concretely, the human guidelines (HG) for SummEval, the machine guidelines for G-Eval (Liu et al., 2023b) and evaluation steps generated by GPT4 (OpenAI, 2023). They test various adaptations to this prompt, explore the usage of examples in the prompt and the usage of coarse-grained vs. fine-grained and aggregated scores. On the test set, they add a shortcut for very bad summarizations and employ bucketing for their scores. Akkasi et al. (2023) explore evaluating 6 different criteria over all model combinations. Kotonya et al. (2023) explore 8 prompt types: 3 base prompts and their extensions with chain-of-though (Wei et al., 2022), zero-shot and few-shot settings. Mahmoudi (2023) explores various zero-shot and few-shot settings with Orcamini. Finally, Mahmoudi (2023); Baswani et al. (2023) generate explanations as an additional request to their model.</p>
<p>Agent-based While they also use an outputbased setup, we place Lu and Yu-Ting (2023) in a separate group. They define 4 characters that should be played by a model and a list of 10 properties. For example they define "Internet Troll" as a critical character or "Teacher" as more knowledgeable character, with the intention that different viewpoints can help to judge generation quality better. Then, they evaluate the combined 40 settings and use XGBoost (Chen and Guestrin, 2016) to combine their scores. While they did not add their top submissions to the final leaderboard they present their reasonably good final scores in their paper.</p>
<p>Baselines As baselines, we use the widely used metrics BERTScore (with XLMR-large embeddings) (Zhang et al., 2020), SBERT (Reimers and Gurevych, 2019) cosine-similarity (with XLMRlarge embeddings), SUPERT (Gao et al., 2020), GEMBA (Kocmi and Federmann, 2023) and</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">Authors</th>
<th style="text-align: center;">Tracks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pradhan/Todi</td>
<td style="text-align: center;">(Pradhan and Todi, 2023)</td>
<td style="text-align: center;">S, SU</td>
</tr>
<tr>
<td style="text-align: left;">Kotonya et. al.</td>
<td style="text-align: center;">(Kotonya et al., 2023)</td>
<td style="text-align: center;">S, SU</td>
</tr>
<tr>
<td style="text-align: left;">DSBA</td>
<td style="text-align: center;">(Kim et al., 2023)</td>
<td style="text-align: center;">S, L, SU</td>
</tr>
<tr>
<td style="text-align: left;">HIT-MI\&amp;T Lab</td>
<td style="text-align: center;">(Zhang et al., 2023)</td>
<td style="text-align: center;">S, MT</td>
</tr>
<tr>
<td style="text-align: left;">IUST_NLP_Lab</td>
<td style="text-align: center;">(Mahmoudi, 2023)</td>
<td style="text-align: center;">S, SU, E</td>
</tr>
<tr>
<td style="text-align: left;">LTRC</td>
<td style="text-align: center;">(Baswani et al., 2023)</td>
<td style="text-align: center;">S, MT, SU, E</td>
</tr>
<tr>
<td style="text-align: left;">NLLG</td>
<td style="text-align: center;">(Larionov et al., 2023)</td>
<td style="text-align: center;">L, MT, SU</td>
</tr>
<tr>
<td style="text-align: left;">TaiwanSenior</td>
<td style="text-align: center;">(Lu and Yu-Ting, 2023)</td>
<td style="text-align: center;">S, MT</td>
</tr>
<tr>
<td style="text-align: left;">iML</td>
<td style="text-align: center;">(Akkasi et al., 2023)</td>
<td style="text-align: center;">S, L, SU</td>
</tr>
</tbody>
</table>
<p>Table 5: Overview of shared task submissions. The letters are abbreviations for the following tracks: S(mall model track), L (arge model track), M(achine)T(ranslation track), SU(mmarization track), E(xplainability track).</p>
<p>Comet-Kiwi-XXL (Rei et al., 2023). Further, we include one baseline for every allowed model that uses the DA score prompt of GEMBA (Kocmi and Federmann, 2023) (with a slight modification for summarization). The models are further specified in Appendix D.</p>
<h2>7 Results and Analysis</h2>
<p>In this section, we first report statistics of the shared task. Then we will present and discuss the final system ranking. Note that we include submissions of participants on the test-set-leaderboard that did not submit a system paper. However, we do not describe their approaches in $\S 5$. Lastly, we will discuss the implications of these results on the development of generation-based metrics.</p>
<p>Statistics The dev-phase on CodaLab has received 44 registrations, 13 of which have submitted their scores. In total, there have been 1048 submissions on the dev-set suggesting that some participants might have optimized their method on the dev-set. Especially, one participant submitted 417 submissions on the dev set. The test-phase on Codabench has received 21 registrations and 248 submissions from 11 participants. We have restricted the number of allowed submissions per day to 10 . Allowing a higher number would enable participants to optimize their approaches on the test-set too much, such that the results would not reflect the generalization capability anymore. On the other hand, we wanted to give participants the option to try out multiple approaches they designed. Further, Codabench would sometimes fail to compute scores and still deduct one submission. Hence, 10 submissions per day allows us to continue in these cases. Two participants have used
up a contingent of $\approx 50$ submissions. Of the 11 test-phase participants, 9 have submitted a system paper. The first authors are from China, India (2), Korea, Taiwan, Canada, Iran, Germany and the United Kingdoms. That means, many authors are from developing countries. Also, many authors are students. Hence, their resource availability was limited, leading many of them to opting for smaller models.</p>
<p>Correlation with humans Here, we present the results that the participants achieve on the test sets. A mapping between team names and authors can be found in Table 5. Table 6 shows the final ranking of the small MT subtask. Compared to the other participants, Zhang et al. (2023) leads by a large margin on all correlation measures. Even significantly outperforming the recent COMET-kiwiXXL and only being matched by GEMBA with GPT-4. This is surprising, as the scores they report on the dev-set are not this strong. However, also on the dev-set they beat the large model baselines that use the 6 models we allow in the shared task. The test-set approach that Zhang et al. (2023) report in their paper builds on ensembling probabilitybased scores from prompts to OpenOrca-Platypus. These prompts contain 3 up to the maximum number of possible example demonstrations. Future work should explore whether their approach can uphold its strong performance across other datasets and settings. The ranking is then followed by various baseline models and team LTRC.</p>
<p>Table 7 shows the final ranking of the large MT subtask. For this subtask, the baselines have not been beaten. Table 8 shows the final ranking of the small summarization subtask. Kim et al. (2023) and Akkasi et al. (2023) lead this track. Both use</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Kendall</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pearson</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Spearman</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Team</td>
<td style="text-align: center;">de</td>
<td style="text-align: center;">zh</td>
<td style="text-align: center;">es</td>
<td style="text-align: center;">de</td>
<td style="text-align: center;">zh</td>
<td style="text-align: center;">es</td>
<td style="text-align: center;">de</td>
<td style="text-align: center;">zh</td>
<td style="text-align: center;">es</td>
</tr>
<tr>
<td style="text-align: left;">baselineGEMBA</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.512</td>
</tr>
<tr>
<td style="text-align: left;">HIT-MI\&amp;T Lab</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 7}$</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.553</td>
</tr>
<tr>
<td style="text-align: left;">baselineCometKiwiXXL</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 5}$</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.403</td>
</tr>
<tr>
<td style="text-align: left;">baselineBertscore</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.312</td>
</tr>
<tr>
<td style="text-align: left;">baselineSBERT</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.320</td>
</tr>
<tr>
<td style="text-align: left;">LTRC</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.132</td>
</tr>
<tr>
<td style="text-align: left;">baselineNous</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.136</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaPlaty</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.136</td>
</tr>
<tr>
<td style="text-align: left;">seanstilwell</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">NaN</td>
</tr>
<tr>
<td style="text-align: left;">baselineWizard</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.077</td>
<td style="text-align: center;">0.093</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaMini</td>
<td style="text-align: center;">0.073</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.030</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.077</td>
</tr>
<tr>
<td style="text-align: left;">TaiwanSenior</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">-0.037</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">NaN</td>
</tr>
</tbody>
</table>
<p>Table 6: Results of the small model track for MT. For our main metric Kendall, we write results that are significantly better than the following, with $p \leq 0.05$, as measured by a permute-both significance test (Deutsch et al., 2021). GEMBA was not included in the significance test. Teams with paper submissions are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Kendall</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pearson</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Spearman</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Team</td>
<td style="text-align: center;">de</td>
<td style="text-align: center;">zh</td>
<td style="text-align: center;">es</td>
<td style="text-align: center;">de</td>
<td style="text-align: center;">zh</td>
<td style="text-align: center;">es</td>
<td style="text-align: center;">de</td>
<td style="text-align: center;">zh</td>
<td style="text-align: center;">es</td>
</tr>
<tr>
<td style="text-align: left;">baselinePlaty_large</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 9 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 4}$</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.320</td>
</tr>
<tr>
<td style="text-align: left;">baselineGuanaco_large</td>
<td style="text-align: center;">$\mathbf{0 . 3 5 0}$</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">$\mathbf{0 . 2 4 1}$</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.300</td>
</tr>
<tr>
<td style="text-align: left;">NLLG</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.238</td>
</tr>
<tr>
<td style="text-align: left;">kaiwalya_large</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.113</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.147</td>
</tr>
</tbody>
</table>
<p>Table 7: Results of the large model track for MT. For our main metric Kendall, we write results that are significantly better than the following, with $p \leq 0.05$, as measured by a permute-both significance test (Deutsch et al., 2021). Teams with paper submissions are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">kd</th>
<th style="text-align: center;">ps</th>
<th style="text-align: center;">sp</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DSBA</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 3}$</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.782</td>
</tr>
<tr>
<td style="text-align: left;">iML</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 5}$</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: left;">baselineBertscore</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.765</td>
</tr>
<tr>
<td style="text-align: left;">IUST_NLP_Lab</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.722</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaMini</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.706</td>
</tr>
<tr>
<td style="text-align: left;">baselineSupertMpnet2</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.747</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaPlaty</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.674</td>
</tr>
<tr>
<td style="text-align: left;">baselineNous</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.674</td>
</tr>
<tr>
<td style="text-align: left;">Kotonya et. al.</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.682</td>
</tr>
<tr>
<td style="text-align: left;">LTRC</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.679</td>
</tr>
<tr>
<td style="text-align: left;">baselineSupertFull</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.706</td>
</tr>
<tr>
<td style="text-align: left;">baselineSupert5</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.678</td>
</tr>
<tr>
<td style="text-align: left;">baselineSBERT</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.645</td>
</tr>
<tr>
<td style="text-align: left;">Pradhan/Todi</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.032</td>
<td style="text-align: center;">0.610</td>
</tr>
<tr>
<td style="text-align: left;">baselineWizard</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">0.536</td>
</tr>
<tr>
<td style="text-align: left;">Haaland</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.280</td>
</tr>
</tbody>
</table>
<p>Table 8: Results of the small model track for summarization. $k d$ stands for Kendall, $p s$ stands for Pearson and $s p$ stands for Spearman. For our main metric Kendall, we write results that are significantly better than the following, with $\leq 0.05$, as measured by a permute-both significance test (Deutsch et al., 2021). Teams with paper submissions are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">kd</th>
<th style="text-align: center;">ps</th>
<th style="text-align: center;">sp</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">iML</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 2}$</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.768</td>
</tr>
<tr>
<td style="text-align: left;">DSBA</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 3}$</td>
<td style="text-align: center;">0.756</td>
<td style="text-align: center;">0.766</td>
</tr>
<tr>
<td style="text-align: left;">baselinePlaty_large</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 0}$</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.753</td>
</tr>
<tr>
<td style="text-align: left;">NLLG</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.638</td>
</tr>
<tr>
<td style="text-align: left;">baselineGuanaco_large</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.504</td>
</tr>
</tbody>
</table>
<p>Table 9: Results of the large model track for summarization. $k d$ stands for Kendall, $p s$ stands for Pearson and $s p$ stands for Spearman. For our main metric Kendall, we write results that are significantly better than the following, with $\leq 0.05$, as measured by a permute-both significance test (Deutsch et al., 2021). Teams with paper submissions are bolded.
carefully crafted prompts to achieve their results.
Table 9 shows the final ranking of the large summarization subtask. Here, Akkasi et al. (2023) is the winning team. Interestingly, for MT and summarization, the small models have beaten the large models. One potential reason might be that the large models take much longer to run and therefore they could not be examined with the same care. Further, it is interesting that the OrcaMini baseline and Mahmoudi (2023) beats many other models despite its parameter count being the lowest of the allowed models'. Generally, many teams opted for the usage of small models. Some teams only use the OrcaMini model, due to resource constraints. This highlights the importance of the inclusiveness of research in the metrics domain. We show a further analysis of the impact of the summarization subcategories in Appendix B.</p>
<p>Performance The best performing approaches of the participants achieve a similar Kendall correlation as our team members when we were testing the inter-annotator agreement on a small subset of samples (see §3). This suggests that these approaches are already close to the performance of native speakers with little training with the annotation process (as compared to our main annotators with a strong language background and more annotation experience on the task). This is an intriguing finding and highlights the potential of current open source models with and without fine-tuning. Especially, as many prompting approaches, like tree-of-thoughts or self-refinement still remain to be explored. Further, it shows that for closed source models like ChatGPT or GPT4 similar opportunities may exist and lead to new state-of-the-art metrics. The results also show that comparably small hardware can already be enough to create strong new metrics.</p>
<p>Explainability Only 2 participants (Baswani et al., 2023; Mahmoudi, 2023) have submitted entries with complementary explanations to the Codabench leaderboard. Both directly prompted the model to give reasoning for the model's decision. Thus, we perform the human experiment on explainability only on a small scale of 50 annotations for randomly selected samples of our summarization dataset. Two annotators of our team were presented with source, summary, MQM annotations (to help to identify problems), the scores of the participants and the explanations of the participants. They annotated which of two explanations they pre-</p>
<p>fer. One annotator preferred explanations of one system, lets call it A, in 27 cases and explanations of the other in 23 cases. The other annotator preferred system A in 24 cases and the other system in 26 cases. In these annotations the annotators agree in $56 \%$ of cases. These findings show that the annotators did not have a clear preference between the systems. Also, we notice that many explanations tend to be vague and return texts such as "The summary has a good coherence and fluency". In some cases, the explanations correctly describe problems. We show one example explanation of Baswani et al. (2023) in Table 10. Here, the explanation correctly captures the word repetition.</p>
<h2>8 Conclusion</h2>
<p>We discuss future work and then summarize the shared task in a conclusion.</p>
<p>Future Work We have considered high resource languages for the MT task. Future work could evaluate low-resource languages, especially once more generative LLMs are released that are trained across a wide range of languages. Also, if this shared task topic is repeated in the future, we might encourage and set rewards for pipeline-based solutions. In other words, currently most approaches of the shared task are based on single prompts or probability outputs; instead many interesting approaches like tree of thoughts (Yao et al., 2023) explore pipelines in which the output is generated iteratively or in parallel. Future work might also create larger or more diverse datasets for our evaluation scheme. Another point is that our current work only contains a small analysis of explainability that remained indecisive on the explanation quality between two participants. This could be extended in future work.</p>
<p>Conclusion This work describes the Eval4NLP 2023 shared task on prompting LLMs as explainable metrics. We have constructed a fine-grained dataset for MT and summarization evaluation, with a novel annotation scheme for the latter. Further, we have organized a competition following the novel restriction to specify allowed models and disallow fine-tuning in a MT and summarization evaluation setting. By running a small and a large model track, we have enabled participation for participants with fewer resources, leading to an inclusive shared task setting.</p>
<p>The top scores of the participants highlight a
number of interesting findings that we summarize here:</p>
<ul>
<li>Small Models: The results on the test set show that the best solutions built on small models outperform those that are built on larger models. This is contradicting usual patterns and an interesting finding for metric efficiency.</li>
<li>Probability-based vs. Output based: The MT ranking is lead by a probability-based method, while the summarization ranking is lead by two prompt-based methods. For MT, this could be caused by the models' understanding of other languages being smaller than its capability of translation, therefore favoring paraphrasation based methods.</li>
<li>Simplicity helps: Many baseline systems achieved high ranks, despite using a simple prompting approach. Participants often report that demonstrating examples reduced their performance. Hence, lean metrics are easier to design and can still be very powerful. The best ranked systems, however, explore more intricate prompts.</li>
</ul>
<p>The contributions of our participants highlight once more how current LLMs can achieve state-of-the-art performance, even without any task-specific fine-tuning.</p>
<h2>Acknowledgements</h2>
<p>We thank our participants for the active contribution and discussion. Further, we thank our annotators for their effort in creating our test sets. Christoph Leiter is financed by the BMBF project "Metrics4NLG". Steffen Eger is financed by DFG Heisenberg grant EG 375/5-1.</p>
<h2>Limitations</h2>
<p>One potential limitation of our work lies in the usage of data from Wikipedia after 15.07. While the selected articles were indeed selected after 15.07, texts could still be copied from other places, some texts were automatically translated from other languages were an entry existed and some texts might even be generated. Another issue of our work lies in the comparably small dataset with low agreements for the small test conducted on the Spanish annotations. Due to time restrictions, we could not do further evaluations. Still we believe that</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">Summary</th>
<th style="text-align: left;">Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 10: Explanation generated with the approach by Baswani et al. (2023)
. It correctly identifies the issue of the word Wimble repeating often.</p>
<p>our annotators were capable in their languages and thorough with their analysis of the samples. As another limitation, pre-filtering with language tool and later on sorting out severe source errors, might miss out on more subtle errors causing problems in the test set.</p>
<h2>References</h2>
<p>Abbas Akkasi, Kathleen C. Fraser, and Majid Komeili. 2023. Reference-free summarization evaluation with large language models. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<p>Pavan Baswani, Ananya Mukherjee, and Manish Shrivastava. 2023. Ltrc_iiith's 2023 submission for prompting large language models as explainable metrics task. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<p>Jonas Belouadi, Anne Lauscher, and Steffen Eger. 2023. Automatikz: Text-guided synthesis of scientific vector graphics with tikz.</p>
<p>Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2021. Evaluation of text generation: A survey.</p>
<p>Tianqi Chen and Carlos Guestrin. 2016. XGBoost. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15607-15631, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Hoa Trang Dang. 2005. Overview of duc 2005.
Tirth Dave, Sai Anirudh Athaluri, and Satyam Singh. 2023. Chatgpt in medicine: an overview of its applications, advantages, limitations, future prospects, and ethical considerations. Frontiers in Artificial Intelligence, 6.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. 2021. A statistical analysis of summarization evaluation metrics using resampling methods. Transactions of the Association for Computational Linguistics, 9:11321146.</p>
<p>Daniel Deutsch, George Foster, and Markus Freitag. 2023. Ties matter: Modifying kendall's tau for modern metric meta-evaluation.</p>
<p>Steffen Eger, Christoph Leiter, Jonas Belouadi, Ran Zhang, Aida Kostikova, Daniil Larionov, Yanran Chen, and Vivian Fresen. 2023. Nllg quarterly arxiv report 06/23: What are the most influential current ai papers? ArXiv, abs/2308.04889.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. 2021. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):1-48.</p>
<p>Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878-891, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation.</p>
<p>Patrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware decoding for neural machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1396-1412, Seattle, United States. Association for Computational Linguistics.</p>
<p>Marina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. 2021. The Eval4NLP shared task on explainable quality estimation: Overview and results. In Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 165-178, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistath. 2023. Gptq: Accurate post-training quantization for generative pre-trained transformers.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46-68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.</p>
<p>Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, and Julius Berner. 2023. Mathematical capabilities of chatgpt.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.</p>
<p>Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 13471354, Online. Association for Computational Linguistics.</p>
<p>Mohanad Halaweh. 2023. Chatgpt in education: Strategies for responsible implementation. Contemporary Educational Technology.</p>
<p>Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XLsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693-4703, Online. Association for Computational Linguistics.</p>
<p>Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, and Xiangang Li. 2023. Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences.
M. G. KENDALL. 1945. THE TREATMENT OF TIES IN RANKING PROBLEMS. Biometrika, 33(3):239251.</p>
<p>JoongHoon Kim, Sangmin Lee, Seung Hun, Saeran Park, Jiyoon Lee, Kiyoon Jeong, and Pilsung Kang. 2023. Which is better? exploring prompting strategy for llm-based metrics. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193-203, Tampere, Finland. European Association for Machine Translation.</p>
<p>Neema Kotonya, Saran Krishnasamy, Joel R. Tetreault, and Alejandro Jaimes. 2023. Little giants: Exploring the potential of small llms as evaluation metrics in summarization in the eval4nlp 2023 shared task. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<p>Daniil Larionov, Vasiliy Viskov, George Kokush, Alexander Panchenko, and Steffen Eger. 2023. Team nllg submission for eval4nlp 2023 shared task: Retrieval-augmented in-context learning for nlg evaluation. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<p>Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023a. Platypus: Quick, cheap, and powerful refinement of llms.</p>
<p>Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz, Bleys Goodson, Wing Lian, Guan Wang, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". 2023b. Openorcaplatypus: Llama2-13b model instruct-tuned on filtered openorcav1 gpt-4 dataset and merged with divergent stem and logic dataset model. https://huggingface.co/Open-Orca/ OpenOrca-Platypus2-13B.</p>
<p>Christoph Leiter, Piyawat Lertvittayakumjorn, M. Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2023a. Towards explainable evaluation metrics for machine translation. ArXiv, abs/2306.13041.</p>
<p>Christoph Leiter, Ran Zhang, Yanran Chen, Jonas Belouadi, Daniil Larionov, Vivian Fresen, and Steffen Eger. 2023b. Chatgpt: A meta-analysis after 2.5 months.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023a. Evaluating the logical reasoning ability of chatgpt and gpt-4.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2023c. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4140-4170, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Arle Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2014. Multidimensional quality metrics (mqm): A framework for declaring and describing translation quality metrics. Tradumātica: tecnologies de la traducció, 0:455-463.</p>
<p>Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Tom Kocmi, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt.</p>
<p>Yuan Lu and Lin Yu-Ting. 2023. Characterised llms affect its evaluation of summary and translation. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<p>Ghazaleh Mahmoudi. 2023. Exploring prompting large language models as explainable metrics. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<p>Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ondřej Bojar. 2020. Results of the WMT20 metrics shared task. In Proceedings of the Fifth Conference on Machine Translation, pages 688-725, Online. Association for Computational Linguistics.</p>
<p>Pankaj Mathur. 2023. orca_mini_v3_7b: An explain tuned llama2-7b model. https://https:// huggingface.co/psmathur/orca_mini_v3_7b.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4.</p>
<p>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Adrien Pavao, Isabelle Guyon, Anne-Catherine Letournel, Dinh-Tuan Tran, Xavier Baro, Hugo Jair Escalante, Sergio Escalera, Tyler Thomas, and Zhen Xu. 2023. Codalab competitions: An open source platform to organize scientific challenges. Journal of Machine Learning Research, 24(198):1-6.</p>
<p>Abhishek Pradhan and Ketan Kumar Todi. 2023. Understanding large language model based metrics for text summarization. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<p>Ricardo Rei, Nuno M. Guerreiro, José Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur, José G. C. de Souza, and André F. T. Martins. 2023. Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ehud Reiter. 2018. A structured review of the validity of BLEU. Computational Linguistics, 44(3):393-401.</p>
<p>Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code.</p>
<p>Ananya B. Sai, Akash Kumar Mohankumar, and Mitesh M. Khapra. 2022. A survey of evaluation metrics used for nlg systems. ACM Comput. Surv., $55(2)$.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Chenhui Shen, Liying Cheng, Yang You, and Lidong Bing. 2023. Are large language models good evaluators for abstractive summarization?</p>
<p>Sam Shleifer and Alexander M. Rush. 2020. Pre-trained summarization distillation.</p>
<p>Lucia Specia, Frédéric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzmán, and André F. T. Martins. 2020. Findings of the WMT 2020 shared task on quality estimation. In Proceedings of the Fifth Conference on Machine Translation, pages 743-764, Online. Association for Computational Linguistics.</p>
<p>Lucia Specia, Frédéric Blain, Marina Fomicheva, Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary, and André F. T. Martins. 2021. Findings of the WMT 2021 shared task on quality estimation. In Proceedings of the Sixth Conference on Machine Translation, pages 684-725, Online. Association for Computational Linguistics.</p>
<p>Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3450-3466, Online. Association for Computational Linguistics.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90-121, Online. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and TieYan Liu. 2018. A study of reinforcement learning for neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3612-3621, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large language models to follow complex instructions.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. 2023b. Instructscore: Explainable text generation evaluation with finegrained feedback.</p>
<p>Zhen Xu, Sergio Escalera, Adrien Pavão, Magali Richard, Wei-Wei Tu, Quanming Yao, Huan Zhao, and Isabelle Guyon. 2022. Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns, 3(7):100543.</p>
<p>Hao Yang, Min Zhang, Shimin Tao, Minghan Wang, Daimeng Wei, and Yanfei Jiang. 2023. Knowledgeprompted estimator: A novel approach to explainable machine translation assessment.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems, volume 34, pages 27263-27277. Curran Associates, Inc.</p>
<p>Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems, volume 33, pages 17283-17297. Curran Associates, Inc.</p>
<p>Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orăsan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022. Findings of the WMT 2022 shared task on quality estimation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 69-99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Rui Zhang, Fuhai Song, Hui Huang, Jinghao Yuan, Muyun Yang, and Tiejun Zhao. 2023. Hit-mi\&amp;t lab's submission to eval4nlp 2023 shared task. In Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">team</th>
<th style="text-align: center;">s_kd</th>
<th style="text-align: center;">s_ps</th>
<th style="text-align: center;">s_sp</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DSBA</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.675</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: left;">iML</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.757</td>
</tr>
<tr>
<td style="text-align: left;">IUST_NLP_Lab</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.712</td>
</tr>
<tr>
<td style="text-align: left;">bertscore</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.729</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaMini</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.684</td>
</tr>
<tr>
<td style="text-align: left;">Kotonya et.al.</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.675</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaPlaty</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.589</td>
<td style="text-align: center;">0.650</td>
</tr>
<tr>
<td style="text-align: left;">baselineNous</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.589</td>
<td style="text-align: center;">0.650</td>
</tr>
<tr>
<td style="text-align: left;">LTRC</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.666</td>
</tr>
<tr>
<td style="text-align: left;">baselineSBERT</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.611</td>
</tr>
<tr>
<td style="text-align: left;">Pradhan/Todi</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.030</td>
<td style="text-align: center;">0.594</td>
</tr>
<tr>
<td style="text-align: left;">baselineWizard</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.531</td>
</tr>
<tr>
<td style="text-align: left;">Haaland</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.332</td>
</tr>
<tr>
<td style="text-align: left;">cometXXL</td>
<td style="text-align: center;">-0.009</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">-0.015</td>
</tr>
<tr>
<td style="text-align: left;">baselineSUPERT</td>
<td style="text-align: center;">-0.028</td>
<td style="text-align: center;">-0.040</td>
<td style="text-align: center;">-0.040</td>
</tr>
</tbody>
</table>
<p>Table 11: Results of the small model track for summarization with Equation 6.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large language models are human-level prompt engineers.</p>
<h2>A Impact of the summarization heuristic</h2>
<p>Here, we consider the impact of using alternative heuristics for summarization, by studying their effect on the ranking of summarization systems. The results for Equation 6 are shown in Table 11. The results for Equation 7 are shown in Table 12. We can see that the top rankings remain the same.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">team</th>
<th style="text-align: left;">s_kd</th>
<th style="text-align: left;">s_ps</th>
<th style="text-align: left;">s_sp</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DSBA</td>
<td style="text-align: left;">0.551</td>
<td style="text-align: left;">0.490</td>
<td style="text-align: left;">0.695</td>
</tr>
<tr>
<td style="text-align: left;">iML</td>
<td style="text-align: left;">0.533</td>
<td style="text-align: left;">0.454</td>
<td style="text-align: left;">0.687</td>
</tr>
<tr>
<td style="text-align: left;">ISUT_NLP_Lab</td>
<td style="text-align: left;">0.512</td>
<td style="text-align: left;">0.546</td>
<td style="text-align: left;">0.649</td>
</tr>
<tr>
<td style="text-align: left;">bertscore</td>
<td style="text-align: left;">0.497</td>
<td style="text-align: left;">0.569</td>
<td style="text-align: left;">0.663</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaMini</td>
<td style="text-align: left;">0.485</td>
<td style="text-align: left;">0.517</td>
<td style="text-align: left;">0.612</td>
</tr>
<tr>
<td style="text-align: left;">Kotonya et.al.</td>
<td style="text-align: left;">0.480</td>
<td style="text-align: left;">0.690</td>
<td style="text-align: left;">0.604</td>
</tr>
<tr>
<td style="text-align: left;">LTRC</td>
<td style="text-align: left;">0.476</td>
<td style="text-align: left;">0.534</td>
<td style="text-align: left;">0.609</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaPlaty</td>
<td style="text-align: left;">0.462</td>
<td style="text-align: left;">0.446</td>
<td style="text-align: left;">0.581</td>
</tr>
<tr>
<td style="text-align: left;">baselineNous</td>
<td style="text-align: left;">0.462</td>
<td style="text-align: left;">0.446</td>
<td style="text-align: left;">0.581</td>
</tr>
<tr>
<td style="text-align: left;">Pradhan/Todi</td>
<td style="text-align: left;">0.422</td>
<td style="text-align: left;">0.023</td>
<td style="text-align: left;">0.591</td>
</tr>
<tr>
<td style="text-align: left;">baselineSBERT</td>
<td style="text-align: left;">0.384</td>
<td style="text-align: left;">0.371</td>
<td style="text-align: left;">0.539</td>
</tr>
<tr>
<td style="text-align: left;">baselineWizard</td>
<td style="text-align: left;">0.361</td>
<td style="text-align: left;">0.381</td>
<td style="text-align: left;">0.478</td>
</tr>
<tr>
<td style="text-align: left;">Haaland</td>
<td style="text-align: left;">0.295</td>
<td style="text-align: left;">0.800</td>
<td style="text-align: left;">0.368</td>
</tr>
<tr>
<td style="text-align: left;">cometXXL</td>
<td style="text-align: left;">0.015</td>
<td style="text-align: left;">0.159</td>
<td style="text-align: left;">0.021</td>
</tr>
<tr>
<td style="text-align: left;">baselineSUPERT</td>
<td style="text-align: left;">0.003</td>
<td style="text-align: left;">-0.018</td>
<td style="text-align: left;">0.004</td>
</tr>
</tbody>
</table>
<p>Table 12: Results of the small model track for summarization with Equation 7.</p>
<h2>B Impact of subcategories</h2>
<p>We also study the impact of subcategories on the final ranking of summarization. That means, we calculate the ranking with each of $\alpha, \beta, \gamma$ set to 1 , while the others are 0 . The results are shown in Tables 13, 14 and 15. Intriguingly, when only the MQM score is evaluated, the model by Haaland has the highest correlation. However, they did not submit a system description or a system paper. Further, all baselines in this setting perform relatively weak. The best baseline is comet, potentially as it has been trained on MQM scores. The results for relevance and hallucinations are rather unsurprising with one time $D S B A$ being the winning team and the other time $i M L$.</p>
<h2>C Screenshot of the annotation interface</h2>
<p>Figure 8 shows a screenshot of the Anthea annotation interface.</p>
<h2>D Model Details</h2>
<p>For SBert, we use embeddings of XLM-R to include multilinguality ${ }^{30}$. For SUPERT we report the standard metric using bert-large-nli-stsb-meantokens ${ }^{31}$ with 5 and all source sentences as pseudoreferences. Further, we upgrade SUPERT to use all-</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$$
\sum_{i \in \text { Facts in Summary }} \alpha * \text { relevance }(i)+\beta * \frac{|\text { Hallucinated Characters }|}{|\text { Characters in the summary }|}+\gamma * \text { MQM }
$$</p>
<p>Figure 6: A heuristic for fine-grained reference-free evaluation of summaries. Alternatively, we set $\alpha=1, \beta=1$ and $\gamma=1$.</p>
<p>$$
\frac{\sum_{i \in \text { Facts in Summary }} \alpha * \text { relevance }(i)}{|\text { Facts in Source }|}+\beta * \frac{|\text { Hallucinated Characters }|}{|\text { Characters in the summary }|}+\gamma * \text { MQM }
$$</p>
<p>Figure 7: An alternative heuristic for fine-grained reference-free evaluation of summaries. We set $\alpha=1, \beta=1$ and $\gamma=1$. Further, we divide the relevance part by the number of facts in the source as normalization.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">source ( $[\mathrm{m}-88]$ )</th>
<th style="text-align: left;">fact ( $[\mathrm{m}-88]$ )</th>
<th style="text-align: left;">Evaluations</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">human (e.g., in a big data sample from 2006, normal)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 8: The modified anthea annotation interface for summarization.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">team</th>
<th style="text-align: center;">s_kd</th>
<th style="text-align: center;">s_ps</th>
<th style="text-align: center;">s_sp</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Haaland</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.379</td>
</tr>
<tr>
<td style="text-align: left;">DSBA</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.210</td>
</tr>
<tr>
<td style="text-align: left;">Kotonya et. al.</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.200</td>
</tr>
<tr>
<td style="text-align: left;">IUST_NLP_LAB</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.200</td>
</tr>
<tr>
<td style="text-align: left;">cometXXL</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.215</td>
</tr>
<tr>
<td style="text-align: left;">Pradhan/Todi</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.205</td>
</tr>
<tr>
<td style="text-align: left;">LTRC</td>
<td style="text-align: center;">0.154</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.191</td>
</tr>
<tr>
<td style="text-align: left;">iML</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.174</td>
</tr>
<tr>
<td style="text-align: left;">baselineWizard</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.163</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaMini</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.155</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaPlaty</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.120</td>
</tr>
<tr>
<td style="text-align: left;">baselineNous</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.120</td>
</tr>
<tr>
<td style="text-align: left;">bertscore</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.130</td>
</tr>
<tr>
<td style="text-align: left;">baselineSBERT</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.094</td>
</tr>
<tr>
<td style="text-align: left;">baselineSUPERT</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">-0.013</td>
<td style="text-align: center;">0.030</td>
</tr>
</tbody>
</table>
<p>Table 13: Results of the small model track for summarization, when only predicting MQM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">team</th>
<th style="text-align: center;">s_kd</th>
<th style="text-align: center;">s_ps</th>
<th style="text-align: center;">s_sp</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DSBA</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.727</td>
</tr>
<tr>
<td style="text-align: left;">iML</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.722</td>
</tr>
<tr>
<td style="text-align: left;">bertscore</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.724</td>
</tr>
<tr>
<td style="text-align: left;">IUST_NLP_LAB</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.677</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaMini</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.669</td>
</tr>
<tr>
<td style="text-align: left;">baselineOrcaPlaty</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.638</td>
</tr>
<tr>
<td style="text-align: left;">baselineNous</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.638</td>
</tr>
<tr>
<td style="text-align: left;">Kotonya et. al.</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.525</td>
<td style="text-align: center;">0.634</td>
</tr>
<tr>
<td style="text-align: left;">LTRC</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.635</td>
</tr>
<tr>
<td style="text-align: left;">baselineSBERT</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.616</td>
</tr>
<tr>
<td style="text-align: left;">Pradhan/Todi</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.543</td>
</tr>
<tr>
<td style="text-align: left;">baselineWizard</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.491</td>
</tr>
<tr>
<td style="text-align: left;">Haaland</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.197</td>
</tr>
<tr>
<td style="text-align: left;">baselineSUPERT</td>
<td style="text-align: center;">-0.041</td>
<td style="text-align: center;">-0.059</td>
<td style="text-align: center;">-0.056</td>
</tr>
<tr>
<td style="text-align: left;">cometXXL</td>
<td style="text-align: center;">-0.065</td>
<td style="text-align: center;">-0.083</td>
<td style="text-align: center;">-0.092</td>
</tr>
</tbody>
</table>
<p>Table 14: Results of the small model track for summarization, when only predicting relevance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{30}$ https://huggingface.co/sentence-transformers/ stsb-xlm-r-multilingual
${ }^{31}$ https://huggingface.co/sentence-transformers/ bert-large-nli-stsb-mean-tokens&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{15}$ https://www.nltk.org/api/nltk.tokenize.html
${ }^{16}$ For the evaluation phase, we keep the annotations of the first batch, as small issues in source sentences should not invalidate the possibility of creating good translations; instead, we remove every sentence from the final dataset that has at least one major source error. We do this as major source errors might cause ambiguity in the annotation process. For example, if the source is unreadable, it is unclear which quality should be expected from the translation.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
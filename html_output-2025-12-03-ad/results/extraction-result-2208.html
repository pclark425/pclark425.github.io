<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2208 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2208</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2208</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-58.html">extraction-schema-58</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <p><strong>Paper ID:</strong> paper-278740501</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.11855v1.pdf" target="_blank">When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the \textbf{academic verification of scientific manuscripts}. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2208.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2208.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPOT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Paper Error Detection (SPOT) benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated multimodal benchmark of 83 manuscripts with 91 author-confirmed errors designed to evaluate LLMs as automated verifiers of scientific manuscripts, combining machine evaluation with human and author validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SPOT (Scientific Paper Error Detection)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary (mathematics, materials science, environmental science, chemistry, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>SPOT validates model-based verification via a hybrid pipeline: (1) seed collection from WITH-DRARXIV and PubPeer, (2) automated GPT-4o filtering to focus on self-contained errors, (3) retention only of cases with original-author acknowledgments (errata/retractions) as ground truth, (4) two-stage human annotation and cross-audit to ensure self-contained and identifiable errors, (5) PDF→text/image normalization (Llama-Parse + GPT-4.1 corrections) and manual audit, and (6) evaluation of target LLMs under a structured prompting scheme with model outputs compared to the author-validated annotations using a separate LLM judge (GPT-4.1). Models are scored with precision, recall, and pass@K (K ∈ {1,4}).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Benchmark-level: shows how well LLM verifiers find author-confirmed errors. Example: best model (OpenAI o3) achieved precision 6.1% ±1.3, recall 21.1% ±4.4, pass@4 37.8% ±1.8 across eight runs (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>SPOT uses author acknowledgement (erratum/retraction) and two-stage human annotation as the gold standard for a validated error; it treats error annotations as exhaustive for evaluation (i.e., any model-flagged error not matching an annotation counts as a false positive).</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Not applicable for the benchmark itself; SPOT emphasizes human/author confirmation as necessary for ground truth and discourages accepting model-only flags without expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty in detection is quantified via multiple independent runs (n=8) and bootstrap resampling for pass@K; per-error confidence estimator (see separate entry) aggregates run counts into an unbiased pass@K probability; evaluation reports mean ± standard deviation across runs and bootstraps.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>SPOT does not rely on model-only signals; it filters candidate errors that require external artifacts and retains only author-acknowledged errors to avoid mislabelling; no general-purpose automated detector of fabricated scientific results is proposed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Creating and evaluating SPOT involved API-based model runs with total reported API expenditure ≈ $5,000; evaluation uses eight independent runs per paper and bootstrapping for pass@K, implying moderate computational/inference cost per model and per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Dataset modest in size (83 papers); annotations may not be exhaustive; OCR and long-context failures can produce false negatives/positives; reliance on author acknowledgements excludes genuine errors without explicit author admission; LLM judges can misalign descriptions and locations.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>The use of author acknowledgements and multi-stage human audit is explicitly intended to raise credibility of ground truth; paper argues that current LLM verifier performance (low precision/recall) undermines acceptance of automated verification in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Models are compared against the author-confirmed annotations as gold standard. Results: o3 precision 6.1% ±1.3, recall 21.1% ±4.4, pass@4 37.8% ±1.8; most other tested MLLMs show near-zero performance, demonstrating a large gap versus the gold standard.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2208.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2208.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Protocol & LLM-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured evaluation protocol using precision/recall/pass@K and an LLM-based judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation pipeline that requires exact location match and semantic description matching, verified by a secondary LLM (GPT-4.1), and summarized with precision, recall, and pass@K metrics over multiple independent runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Precision/Recall + pass@K evaluation with LLM-as-judge (GPT-4.1)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>methodology for LLM-based document verification (multidisciplinary applicability)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Model outputs are prompted to return structured JSON listing every error with location and description. A separate judge LLM (GPT-4.1) aligns predictions against the author-validated annotations. True positives require both matching location and matching description (per judge). Metrics: Precision = TP/(TP+FP), Recall = TP/(TP+FN). pass@K computes the fraction of ground-truth errors found in at least one of K independent runs; eight independent runs per paper are used to estimate pass@1 and pass@4 with bootstrap resampling (B=1000) to obtain means and standard deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Reported model performance on SPOT using this protocol: o3 recall 21.1%, precision 6.1%, pass@4 37.8% (Table 2); other models much lower.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>The paper treats exact match to author-acknowledged annotations as the validation standard for correct detection; any model-flagged issue not matching an annotation counts as false positive.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Not applicable—this is an evaluation protocol for model outputs, not a domain simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty captured via multiple runs (n=8) and bootstrap resampling when computing pass@K; per-run variability reported as mean ± std. The judge LLM adds a deterministic alignment step but may inherit judge-LM errors (acknowledged).</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Protocol explicitly penalizes unannotated model-reported errors (counts as FP), discouraging model hallucinations; a secondary expert review is recommended for model-flagged issues outside annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Evaluation requires multiple (eight) runs per paper plus judge LLM comparisons and bootstrapping, increasing inference cost compared to single-shot evaluation; total API expenditure across experiments reported (~$5k).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Reliance on the judge LLM for semantic matching may conflate close-but-not-identical descriptions; the exhaustiveness assumption (unannotated flags are FPs) may penalize correct but unannotated discoveries; judge-LM mistakes can mislabel TPs as FNs.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Using explicit matching rules and author-validated annotations provides a strict, transparent standard; however, strictness can lower apparent model recall and may undervalue legitimate novel detections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Direct numerical comparison: models' predictions are measured against author-validated annotations (the gold standard) using the above metrics; results reveal models far from the gold standard (see Table 2).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2208.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2208.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pass@K Confidence Estimator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-error unbiased pass@K confidence estimator and aggregated confidence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical estimator that converts counts of runs in which a ground-truth error was detected into an unbiased estimate of the probability that K fresh attempts would detect the error, aggregated into an overall model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Unbiased per-error pass@K estimator (formulas 4-6)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation statistics for LLM-based verification</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Given n independent runs (n = 8), for each ground-truth error g the paper counts ci,g = number of runs detecting g. The estimator gives per-error probability p_{i,g} = 1 − (n − c_{i,g} choose K)/(n choose K), i.e., probability that at least one of K fresh attempts would find g. Aggregate Confidence = mean over all errors of p_{i,g}. This provides an unbiased pass@K probability per error and a scalar confidence for a model across the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Used to report self-estimated confidence; empirically, models' self-estimated confidence correlated weakly with actual pass@4 performance and was typically near zero across evaluations, indicating low calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Estimation follows combinatorial pass@K derivations cited (references [42],[43]) and is applied uniformly across errors and papers.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>This estimator is itself an uncertainty quantification method for detection probability; the paper reports aggregated confidence and compares it to empirical pass@4 to assess calibration. Calibration was poor: confidence clustered near zero and correlated weakly with actual performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Requires multiple independent runs (n=8) per paper to estimate ci,g; therefore increases inference/time cost linearly with n.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Relies on independence of runs; limited number of runs (n=8) leads to coarse estimates, especially for rare events; poor calibration observed empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Provides a formal mechanism for models to self-report expected coverage (pass@K), but poor calibration implies limited utility for establishing trust in model outputs without external validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Used to compare model self-estimated coverage vs actual pass@K measured against author-validated ground truth; models generally over/under-estimated, with weak correlation to actual performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2208.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2208.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-time Scaling (Thinking Modes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-time scaling / 'Thinking' inference scaling methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adjusting the inference budget (e.g., depth of chain-of-thought, number of solution paths, specialized 'Thinking' modes) to boost LLM performance on complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Test-time scaling / Thinking modes (e.g., o4-mini reasoning effort, Claude 'Thinking')</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM inference methodology (applied to scientific verification)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The paper varies a 'reasoning effort' parameter (low/medium/high) for o4-mini and compares performance; specialized 'Thinking' modes or reasoning-trained variants (e.g., Claude-3.7-Sonnet:Thinking) are also evaluated. The tests measure how increased inference computation (longer internal search/chain-of-thought or extra sampling) affects error-detection pass@K and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Test-time scaling demonstrated near-linear increases in error-detection performance for o4-mini as reasoning effort increased (Figure 10). The paper reports that enabling 'Think' or higher reasoning effort increased pass@K and recall for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Paper aligns with prior literature that more inference compute often improves performance for reasoning benchmarks; it uses pass@K and recall as outcome measures.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Not applicable; this is an inference-time technique rather than a domain simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Performance vs reasoning effort is reported with independent trials (three trials in Figure 10) showing near-linear improvement; error bars/standard deviations reported where appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Test-time scaling increases inference compute and API cost; authors report using vendor-recommended parameters and that test-time scaling improved accuracy at the expense of higher compute (API cost). Total experimental expenditure across model evaluations ≈ $5k.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Gains vary by model and category; scaling inference alone does not resolve long-tail domain knowledge gaps or OCR/multi-hop context failures.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Shows that allocating more compute at inference can improve model utility, but incremental gains do not bring models close to adequate verification performance; thus scaling does not itself resolve credibility concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Test-time scaling improves measured pass@K against the author-validated gold standard for some models (e.g., o4-mini shows near-linear gains), but final scores remain far below acceptable verification thresholds.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2208.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2208.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-modality Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-modality ablation (text-only vs multimodal inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation study comparing model performance on SPOT instances with visual inputs (figures/images) included vs a text-only subset where figure-dependent instances were removed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Multi-modality ablation (text-only vs multimodal evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>document understanding / multimodal LLM evaluation (applied across scientific domains)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Created a text-only subset of SPOT (48 instances) by removing figure-duplication and figure-dependent data-inconsistency cases. Models were evaluated both with multimodal inputs and with figures stripped. The study compares recall and pass@4 across settings and across models (proprietary vs open-source).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Findings: most models improved in recall and pass@4 when figures were removed (figures often acted as distractors). Exceptions: o3 and Gemini-2.5-Pro showed modest drops without visual inputs, indicating they leveraged figures. Proprietary models retained substantial recall in the multimodal setting (o3 34.6% recall; Gemini-2.5-Pro 13.7%), whereas open-source models collapsed to near zero in multimodal conditions (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Evaluates whether multimodal inputs aid or hinder automated verification; no domain-specific lab standards, but practical recommendation to isolate figure-dependent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Performance reported as mean ± std over eight runs; multi-model comparisons provided with statistical dispersion.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Performing multimodal evaluations adds complexity (image processing, OCR correction) and increases preprocessing cost/time; the paper describes a multi-stage normalization pipeline to reduce OCR-induced false errors.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Preprocessing pipeline may still miss subtle figure cues; removing figures may remove legitimate signals required to detect some error types (e.g., figure duplication).</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Shows that naive multimodal inclusion can degrade verifier reliability unless the model has robust figure analysis; this affects trust in multimodal automated verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Direct comparison of model recall/pass@4 to author-validated annotations in both multimodal and text-only conditions; shows that access to figures sometimes improves, sometimes hurts detection depending on model capability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2208.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2208.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OCR Normalization Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PDF normalization and OCR correction pipeline (Llama-Parse + GPT-4.1 + manual audit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing pipeline that converts PDFs into high-fidelity text and isolated images for model input, with automated correction by GPT-4.1 and final manual auditing to avoid OCR-induced false errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>PDF → Markdown conversion using Llama-Parse, screenshots of figures/equations, GPT-4.1 correction, and manual audit</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>document processing / multimodal data normalization</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Pipeline steps: (1) Llama-Parse converts PDFs to Markdown and captures screenshots of figures/tables/equations (≈8 images/page), (2) initial OCR text and screenshots are sent to GPT-4.1 for correction to fix OCR failures (particularly in math), (3) manual audit of processed pages to ensure every flagged error remains visible and accurately represented. The normalization is intended to avoid conflating upstream OCR/parser failures with downstream model verification errors.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Aims to present inputs so that the verification model (LLM) is assessed for comprehension rather than penalized for parser mistakes; manual auditing enforces a high-fidelity ground truth representation of the manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Pilot experiments showed OCR failures (especially in math) caused downstream models to misinterpret formatting artifacts as errors; correction stage with GPT-4.1 mitigates but does not eliminate such issues.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Adds preprocessing overhead (Llama-Parse conversion, GPT-4.1 correction passes, manual audit) before model evaluation; used to reduce spurious model errors due to OCR.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Manual audit remains necessary; some OCR/math formatting issues are challenging and can still affect downstream detection. The pipeline intentionally avoids releasing paywalled PDFs but allows institutional reproduction for paywalled items.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Improves credibility of benchmark evaluation by decoupling parser noise from verifier performance; helps ensure model failures reflect comprehension limits rather than upstream parsing errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Normalization is part of ensuring that model outputs are compared fairly to author-validated gold-standard annotations; no numeric comparison of 'with vs without normalization' provided beyond pilot observations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2208.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2208.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert-led Case Studies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert-led qualitative case studies (mathematics, materials science, environmental science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain experts manually review model-flagged issues against the official withdrawal notices and benchmark annotations to verify false negatives and to adjudicate unannotated model flags; they provide concrete examples of where computational checks succeed or fail vs experimental/computational domain practices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Expert adjudication of LLM-flagged errors in withdrawn manuscripts (domain expert review and author consultation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics, materials science, environmental science</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Experts (researchers or PhD-trained postdocs) evaluated LLM-flagged errors for selected withdrawn manuscripts. They: (a) checked whether model outputs corresponded to benchmarked errors (identifying mislabeled FNs), (b) examined model-predicted issues outside annotations to determine if they were genuine (consulted original authors when uncertain), and (c) diagnosed root causes (OCR, long-range context, missing derivations). Specific domain examples include: mathematical proof gap in algebraic geometry papers (purely logical/theoretical validation), materials science papers where experiments (1H NMR, PXRD, BET, photocatalysis DPBF bleaching) were central and where DFT predictions contradicted reported yields, and environmental science where misuse of log vs ln changed climate forcing numerics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Case studies reference domain computational tools used in the original papers (DFT calculations and SRIM ion-penetration simulations). The paper reports concrete mismatches: e.g., SRIM plot axis indicates stopping range < 20 nm whereas authors claimed 1 µm penetration; DFT relative energy of an acetal product (+7.4 kcal/mol) contradicts experimentally observed 81% yield (implying a negative ΔG ≈ -0.85 kcal/mol), indicating a failure of DFT explanation or misinterpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Yes — explicit examples where computational predictions (DFT, SRIM) did not align with experimental claims: SRIM depth vs claimed penetration (orders-of-magnitude mismatch) and DFT-predicted thermodynamics inconsistent with observed high yields; the paper highlights these as evidence that simulation/DFT results alone did not justify authors' experimental conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Experts expect domain-specific controls and metrics: e.g., 1H NMR cross-validation for composition derived from UV, correct unit conventions for BET (m^2/g), normalized PXRD intensities or internal standards, quantum yields/turnover numbers for photocatalysis comparisons; without these, claims are poorly validated.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper argues (via case studies) that simulations (DFT, SRIM) are not sufficient without appropriate experimental controls/validation and domain-appropriate interpretation; simulation-derived energetic/kinetic claims need experimental corroboration and careful accounting for entropic/solvent effects.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Concrete examples: (1) SRIM stopping-range discrepancy (authors' claims of µm-scale penetration contradicted by SRIM axis showing <20 nm), (2) DFT predicted +7.4 kcal/mol for acetal product while experiments report 81% yield (thermodynamic inconsistency), (3) climate paper computational misuse (log10 vs ln) producing a ≈2.3× scaling error in forcing calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Original papers sometimes lacked proper uncertainty/statistical reporting (e.g., EDX shifts under 2 wt% without statistics, missing tables of contact-angle/Wa values). Experts criticized insufficient statistical treatment or missing normalization/controls.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Case studies reveal LLM hallucinations (e.g., o3 hallucinated lack of NMR confirmation) and reading errors; detection of fabricated/incorrect claims relied on expert cross-checks and consultation with original authors rather than automated fabrication detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Expert review and author consultation required significant manual labor; no precise cost/time numbers given, but the paper emphasizes the human-in-the-loop necessity and the review overhead for model-flagged outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Experts note that some errors span multiple sections or require supplemental data (e.g., SI tables) unavailable to automated pipelines; LLMs lack access to supplementary materials, causing false positives/hallucinations; domain-specific conventions and hidden assumptions impede automated verification.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>The paper uses expert adjudication to increase validity of conclusions about model failure modes; it argues that reliable automated verification must include expert review and domain-aware checks to be credible to the scientific community.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Experts used author withdrawal notes and errata as the gold standard; they found cases where LLMs either missed the core error (false negatives) or hallucinated errors (false positives), reinforcing that model outputs are not yet comparable to human expert validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2208.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2208.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Author-confirmation Filter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Author-acknowledgement-based ground-truth filtering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data curation rule that retains only report-manuscript pairs where the original authors explicitly acknowledge the mistake (e.g., erratum, retraction, or direct PubPeer reply), and treats those acknowledgements as definitive evidence of genuine errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Author-acknowledgement filtering (Stage 3 data curation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>dataset curation / validation standards</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>During SPOT curation, only manuscripts with explicit author acknowledgements of mistakes were retained: PubPeer comments followed by explicit author replies acknowledging mistakes, and WITHDRARXIV self-retraction entries were treated as definitive evidence. This is used to reduce ambiguity in ground-truth error labels and avoid controversial/unconfirmed claims.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Author acknowledgement is used as a domain-agnostic standard for confirming errors across disciplines; the paper argues this reduces subjectivity compared to relying on third-party reports only.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>By requiring author acknowledgement, SPOT lowers the chance of false positives in annotations (i.e., avoids including disputed or unconfirmed claims), but does not attempt to detect fabricated manuscripts; it intentionally excludes errors that require external artifacts for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Reduces annotation overhead by constraining candidate list, but requires scraping and matching author responses; no numeric cost provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Excludes genuine errors that authors do not acknowledge; some author acknowledgements may be incomplete and not capture all affected results; relies on public comment/reply availability.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Using author confirmation increases credibility of ground truth in the scientific community because the original authors accept the error, making the benchmark more defensible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>This filtering method defines the gold standard used throughout SPOT; models are evaluated against these author-confirmed annotations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2208.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2208.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFT & SRIM Failure Examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Documented mismatches between computational predictions (DFT, SRIM) and experimental claims in case-study papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete cases from SPOT's materials and environmental science papers where computational/simulation outputs did not support the authors' experimental claims, illustrating limits of simulation-only validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>DFT calculations (domain quantum-chemistry) and SRIM ion-penetration simulations</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science / computational chemistry / ion-beam simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>high-fidelity simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The paper reports on original-manuscript uses of DFT to compute reaction barriers/relative energies and SRIM to model ion penetration. SPOT case studies highlight: (a) DFT predicted an acetal product to be +7.4 kcal/mol (thermodynamically unfavorable) while experiments reported 81% yield implying a negative ΔG, and (b) SRIM figure axis (0–1000 Å) indicates stopping ranges below ~20 nm while the authors discuss µm-scale penetration, an orders-of-magnitude disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>SRIM: domain-standard Monte Carlo ion-stopping simulation (electronic/nuclear stopping approximations); fidelity is high for ion ranges but dependent on axis/units—paper highlights clear plotting/interpretation errors. DFT: standard quantum-chemical energetics; fidelity depends on functional/basis and solvent/entropic approximations not fully discussed in the original manuscript; SPOT shows DFT-derived thermodynamics were used without adequate experimental corroboration.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Explicit mismatches: SRIM predicted <20 nm stopping range vs authors' µm claim; DFT predicted +7.4 kcal/mol vs observed 81% acetal yield (thermodynamic inconsistency). The paper uses these as examples where simulation-based claims were insufficient or misinterpreted with respect to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Experts emphasize need for corroborating experiments, reporting of units/axes, statistical treatment of small compositional changes (EDX), and proper normalization/controls for PXRD and BET measurements. Simulations should be accompanied by uncertainty bounds and consistent units/interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper implies simulation alone is insufficient when (a) simulations contradict experimental outcomes, (b) key experimental controls or data are missing, or (c) simulation assumptions (e.g., neglecting solvent/entropy) are not addressed; in such cases experimental corroboration is required.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Documented failures as above (SRIM and DFT examples). Additionally, misuse of mathematical/logarithmic bases in climate forcing calculations is reported (log10 vs ln) as a computational error.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Original manuscripts often lacked statistical treatment or error bars for small changes (e.g., EDX shifts <2 wt% reported without statistics); SPOT criticizes missing uncertainty reporting for simulation-experiment comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>High: DFT and SRIM require domain expertise and compute resources, and reconciling them with experiments requires additional measurements and statistical analyses (not quantified).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Original manuscripts often failed to account for entropic/solvent effects, lacked experimental controls, or misreported units/axes, undermining simulation validity; SPOT highlights that poor reporting and missing supplementary data hamper reliable validation.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Demonstrates that simulation results alone, when misapplied or inadequately reported, will reduce acceptance by experts; highlights the need for cross-validation with domain-appropriate experiments and careful reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>These examples are compared against reported experimental outcomes (observed yields, claimed penetration depths) and found inconsistent, undermining claims that simulations validated the experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Corebench: Fostering the credibility of published research through a computational reproducibility agent benchmark <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 1)</em></li>
                <li>Automating scientific verification (related fact verification and peer review literature) <em>(Rating: 1)</em></li>
                <li>Llm-as-a-judge & reward model: What they can and cannot do <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2208",
    "paper_id": "paper-278740501",
    "extraction_schema_id": "extraction-schema-58",
    "extracted_data": [
        {
            "name_short": "SPOT",
            "name_full": "Scientific Paper Error Detection (SPOT) benchmark",
            "brief_description": "A curated multimodal benchmark of 83 manuscripts with 91 author-confirmed errors designed to evaluate LLMs as automated verifiers of scientific manuscripts, combining machine evaluation with human and author validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "SPOT (Scientific Paper Error Detection)",
            "scientific_domain": "multidisciplinary (mathematics, materials science, environmental science, chemistry, etc.)",
            "validation_type": "hybrid",
            "validation_description": "SPOT validates model-based verification via a hybrid pipeline: (1) seed collection from WITH-DRARXIV and PubPeer, (2) automated GPT-4o filtering to focus on self-contained errors, (3) retention only of cases with original-author acknowledgments (errata/retractions) as ground truth, (4) two-stage human annotation and cross-audit to ensure self-contained and identifiable errors, (5) PDF→text/image normalization (Llama-Parse + GPT-4.1 corrections) and manual audit, and (6) evaluation of target LLMs under a structured prompting scheme with model outputs compared to the author-validated annotations using a separate LLM judge (GPT-4.1). Models are scored with precision, recall, and pass@K (K ∈ {1,4}).",
            "simulation_fidelity": null,
            "experimental_validation_performed": null,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "Benchmark-level: shows how well LLM verifiers find author-confirmed errors. Example: best model (OpenAI o3) achieved precision 6.1% ±1.3, recall 21.1% ±4.4, pass@4 37.8% ±1.8 across eight runs (Table 2).",
            "domain_validation_standards": "SPOT uses author acknowledgement (erratum/retraction) and two-stage human annotation as the gold standard for a validated error; it treats error annotations as exhaustive for evaluation (i.e., any model-flagged error not matching an annotation counts as a false positive).",
            "when_simulation_sufficient": "Not applicable for the benchmark itself; SPOT emphasizes human/author confirmation as necessary for ground truth and discourages accepting model-only flags without expert review.",
            "simulation_failures": null,
            "uncertainty_quantification": "Uncertainty in detection is quantified via multiple independent runs (n=8) and bootstrap resampling for pass@K; per-error confidence estimator (see separate entry) aggregates run counts into an unbiased pass@K probability; evaluation reports mean ± standard deviation across runs and bootstraps.",
            "fabrication_detection": "SPOT does not rely on model-only signals; it filters candidate errors that require external artifacts and retains only author-acknowledged errors to avoid mislabelling; no general-purpose automated detector of fabricated scientific results is proposed.",
            "validation_cost_time": "Creating and evaluating SPOT involved API-based model runs with total reported API expenditure ≈ $5,000; evaluation uses eight independent runs per paper and bootstrapping for pass@K, implying moderate computational/inference cost per model and per paper.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Dataset modest in size (83 papers); annotations may not be exhaustive; OCR and long-context failures can produce false negatives/positives; reliance on author acknowledgements excludes genuine errors without explicit author admission; LLM judges can misalign descriptions and locations.",
            "acceptance_credibility": "The use of author acknowledgements and multi-stage human audit is explicitly intended to raise credibility of ground truth; paper argues that current LLM verifier performance (low precision/recall) undermines acceptance of automated verification in practice.",
            "comparison_to_gold_standard": "Models are compared against the author-confirmed annotations as gold standard. Results: o3 precision 6.1% ±1.3, recall 21.1% ±4.4, pass@4 37.8% ±1.8; most other tested MLLMs show near-zero performance, demonstrating a large gap versus the gold standard.",
            "uuid": "e2208.0"
        },
        {
            "name_short": "Evaluation Protocol & LLM-judge",
            "name_full": "Structured evaluation protocol using precision/recall/pass@K and an LLM-based judge",
            "brief_description": "An evaluation pipeline that requires exact location match and semantic description matching, verified by a secondary LLM (GPT-4.1), and summarized with precision, recall, and pass@K metrics over multiple independent runs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Precision/Recall + pass@K evaluation with LLM-as-judge (GPT-4.1)",
            "scientific_domain": "methodology for LLM-based document verification (multidisciplinary applicability)",
            "validation_type": "computational validation",
            "validation_description": "Model outputs are prompted to return structured JSON listing every error with location and description. A separate judge LLM (GPT-4.1) aligns predictions against the author-validated annotations. True positives require both matching location and matching description (per judge). Metrics: Precision = TP/(TP+FP), Recall = TP/(TP+FN). pass@K computes the fraction of ground-truth errors found in at least one of K independent runs; eight independent runs per paper are used to estimate pass@1 and pass@4 with bootstrap resampling (B=1000) to obtain means and standard deviations.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "Reported model performance on SPOT using this protocol: o3 recall 21.1%, precision 6.1%, pass@4 37.8% (Table 2); other models much lower.",
            "domain_validation_standards": "The paper treats exact match to author-acknowledged annotations as the validation standard for correct detection; any model-flagged issue not matching an annotation counts as false positive.",
            "when_simulation_sufficient": "Not applicable—this is an evaluation protocol for model outputs, not a domain simulation.",
            "simulation_failures": null,
            "uncertainty_quantification": "Uncertainty captured via multiple runs (n=8) and bootstrap resampling when computing pass@K; per-run variability reported as mean ± std. The judge LLM adds a deterministic alignment step but may inherit judge-LM errors (acknowledged).",
            "fabrication_detection": "Protocol explicitly penalizes unannotated model-reported errors (counts as FP), discouraging model hallucinations; a secondary expert review is recommended for model-flagged issues outside annotations.",
            "validation_cost_time": "Evaluation requires multiple (eight) runs per paper plus judge LLM comparisons and bootstrapping, increasing inference cost compared to single-shot evaluation; total API expenditure across experiments reported (~$5k).",
            "hybrid_validation_approach": false,
            "validation_limitations": "Reliance on the judge LLM for semantic matching may conflate close-but-not-identical descriptions; the exhaustiveness assumption (unannotated flags are FPs) may penalize correct but unannotated discoveries; judge-LM mistakes can mislabel TPs as FNs.",
            "acceptance_credibility": "Using explicit matching rules and author-validated annotations provides a strict, transparent standard; however, strictness can lower apparent model recall and may undervalue legitimate novel detections.",
            "comparison_to_gold_standard": "Direct numerical comparison: models' predictions are measured against author-validated annotations (the gold standard) using the above metrics; results reveal models far from the gold standard (see Table 2).",
            "uuid": "e2208.1"
        },
        {
            "name_short": "Pass@K Confidence Estimator",
            "name_full": "Per-error unbiased pass@K confidence estimator and aggregated confidence",
            "brief_description": "A statistical estimator that converts counts of runs in which a ground-truth error was detected into an unbiased estimate of the probability that K fresh attempts would detect the error, aggregated into an overall model confidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Unbiased per-error pass@K estimator (formulas 4-6)",
            "scientific_domain": "evaluation statistics for LLM-based verification",
            "validation_type": "computational validation",
            "validation_description": "Given n independent runs (n = 8), for each ground-truth error g the paper counts ci,g = number of runs detecting g. The estimator gives per-error probability p_{i,g} = 1 − (n − c_{i,g} choose K)/(n choose K), i.e., probability that at least one of K fresh attempts would find g. Aggregate Confidence = mean over all errors of p_{i,g}. This provides an unbiased pass@K probability per error and a scalar confidence for a model across the benchmark.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "Used to report self-estimated confidence; empirically, models' self-estimated confidence correlated weakly with actual pass@4 performance and was typically near zero across evaluations, indicating low calibration.",
            "domain_validation_standards": "Estimation follows combinatorial pass@K derivations cited (references [42],[43]) and is applied uniformly across errors and papers.",
            "when_simulation_sufficient": null,
            "simulation_failures": null,
            "uncertainty_quantification": "This estimator is itself an uncertainty quantification method for detection probability; the paper reports aggregated confidence and compares it to empirical pass@4 to assess calibration. Calibration was poor: confidence clustered near zero and correlated weakly with actual performance.",
            "fabrication_detection": null,
            "validation_cost_time": "Requires multiple independent runs (n=8) per paper to estimate ci,g; therefore increases inference/time cost linearly with n.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Relies on independence of runs; limited number of runs (n=8) leads to coarse estimates, especially for rare events; poor calibration observed empirically.",
            "acceptance_credibility": "Provides a formal mechanism for models to self-report expected coverage (pass@K), but poor calibration implies limited utility for establishing trust in model outputs without external validation.",
            "comparison_to_gold_standard": "Used to compare model self-estimated coverage vs actual pass@K measured against author-validated ground truth; models generally over/under-estimated, with weak correlation to actual performance.",
            "uuid": "e2208.2"
        },
        {
            "name_short": "Test-time Scaling (Thinking Modes)",
            "name_full": "Test-time scaling / 'Thinking' inference scaling methods",
            "brief_description": "Adjusting the inference budget (e.g., depth of chain-of-thought, number of solution paths, specialized 'Thinking' modes) to boost LLM performance on complex reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Test-time scaling / Thinking modes (e.g., o4-mini reasoning effort, Claude 'Thinking')",
            "scientific_domain": "LLM inference methodology (applied to scientific verification)",
            "validation_type": "computational validation",
            "validation_description": "The paper varies a 'reasoning effort' parameter (low/medium/high) for o4-mini and compares performance; specialized 'Thinking' modes or reasoning-trained variants (e.g., Claude-3.7-Sonnet:Thinking) are also evaluated. The tests measure how increased inference computation (longer internal search/chain-of-thought or extra sampling) affects error-detection pass@K and recall.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "Test-time scaling demonstrated near-linear increases in error-detection performance for o4-mini as reasoning effort increased (Figure 10). The paper reports that enabling 'Think' or higher reasoning effort increased pass@K and recall for some models.",
            "domain_validation_standards": "Paper aligns with prior literature that more inference compute often improves performance for reasoning benchmarks; it uses pass@K and recall as outcome measures.",
            "when_simulation_sufficient": "Not applicable; this is an inference-time technique rather than a domain simulation.",
            "simulation_failures": null,
            "uncertainty_quantification": "Performance vs reasoning effort is reported with independent trials (three trials in Figure 10) showing near-linear improvement; error bars/standard deviations reported where appropriate.",
            "fabrication_detection": null,
            "validation_cost_time": "Test-time scaling increases inference compute and API cost; authors report using vendor-recommended parameters and that test-time scaling improved accuracy at the expense of higher compute (API cost). Total experimental expenditure across model evaluations ≈ $5k.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Gains vary by model and category; scaling inference alone does not resolve long-tail domain knowledge gaps or OCR/multi-hop context failures.",
            "acceptance_credibility": "Shows that allocating more compute at inference can improve model utility, but incremental gains do not bring models close to adequate verification performance; thus scaling does not itself resolve credibility concerns.",
            "comparison_to_gold_standard": "Test-time scaling improves measured pass@K against the author-validated gold standard for some models (e.g., o4-mini shows near-linear gains), but final scores remain far below acceptable verification thresholds.",
            "uuid": "e2208.3"
        },
        {
            "name_short": "Multi-modality Ablation",
            "name_full": "Multi-modality ablation (text-only vs multimodal inputs)",
            "brief_description": "An ablation study comparing model performance on SPOT instances with visual inputs (figures/images) included vs a text-only subset where figure-dependent instances were removed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Multi-modality ablation (text-only vs multimodal evaluation)",
            "scientific_domain": "document understanding / multimodal LLM evaluation (applied across scientific domains)",
            "validation_type": "computational validation",
            "validation_description": "Created a text-only subset of SPOT (48 instances) by removing figure-duplication and figure-dependent data-inconsistency cases. Models were evaluated both with multimodal inputs and with figures stripped. The study compares recall and pass@4 across settings and across models (proprietary vs open-source).",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "Findings: most models improved in recall and pass@4 when figures were removed (figures often acted as distractors). Exceptions: o3 and Gemini-2.5-Pro showed modest drops without visual inputs, indicating they leveraged figures. Proprietary models retained substantial recall in the multimodal setting (o3 34.6% recall; Gemini-2.5-Pro 13.7%), whereas open-source models collapsed to near zero in multimodal conditions (Table 3).",
            "domain_validation_standards": "Evaluates whether multimodal inputs aid or hinder automated verification; no domain-specific lab standards, but practical recommendation to isolate figure-dependent tasks.",
            "when_simulation_sufficient": null,
            "simulation_failures": null,
            "uncertainty_quantification": "Performance reported as mean ± std over eight runs; multi-model comparisons provided with statistical dispersion.",
            "fabrication_detection": null,
            "validation_cost_time": "Performing multimodal evaluations adds complexity (image processing, OCR correction) and increases preprocessing cost/time; the paper describes a multi-stage normalization pipeline to reduce OCR-induced false errors.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Preprocessing pipeline may still miss subtle figure cues; removing figures may remove legitimate signals required to detect some error types (e.g., figure duplication).",
            "acceptance_credibility": "Shows that naive multimodal inclusion can degrade verifier reliability unless the model has robust figure analysis; this affects trust in multimodal automated verification.",
            "comparison_to_gold_standard": "Direct comparison of model recall/pass@4 to author-validated annotations in both multimodal and text-only conditions; shows that access to figures sometimes improves, sometimes hurts detection depending on model capability.",
            "uuid": "e2208.4"
        },
        {
            "name_short": "OCR Normalization Pipeline",
            "name_full": "PDF normalization and OCR correction pipeline (Llama-Parse + GPT-4.1 + manual audit)",
            "brief_description": "A preprocessing pipeline that converts PDFs into high-fidelity text and isolated images for model input, with automated correction by GPT-4.1 and final manual auditing to avoid OCR-induced false errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "PDF → Markdown conversion using Llama-Parse, screenshots of figures/equations, GPT-4.1 correction, and manual audit",
            "scientific_domain": "document processing / multimodal data normalization",
            "validation_type": "computational validation",
            "validation_description": "Pipeline steps: (1) Llama-Parse converts PDFs to Markdown and captures screenshots of figures/tables/equations (≈8 images/page), (2) initial OCR text and screenshots are sent to GPT-4.1 for correction to fix OCR failures (particularly in math), (3) manual audit of processed pages to ensure every flagged error remains visible and accurately represented. The normalization is intended to avoid conflating upstream OCR/parser failures with downstream model verification errors.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": null,
            "domain_validation_standards": "Aims to present inputs so that the verification model (LLM) is assessed for comprehension rather than penalized for parser mistakes; manual auditing enforces a high-fidelity ground truth representation of the manuscript.",
            "when_simulation_sufficient": null,
            "simulation_failures": "Pilot experiments showed OCR failures (especially in math) caused downstream models to misinterpret formatting artifacts as errors; correction stage with GPT-4.1 mitigates but does not eliminate such issues.",
            "uncertainty_quantification": null,
            "fabrication_detection": null,
            "validation_cost_time": "Adds preprocessing overhead (Llama-Parse conversion, GPT-4.1 correction passes, manual audit) before model evaluation; used to reduce spurious model errors due to OCR.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Manual audit remains necessary; some OCR/math formatting issues are challenging and can still affect downstream detection. The pipeline intentionally avoids releasing paywalled PDFs but allows institutional reproduction for paywalled items.",
            "acceptance_credibility": "Improves credibility of benchmark evaluation by decoupling parser noise from verifier performance; helps ensure model failures reflect comprehension limits rather than upstream parsing errors.",
            "comparison_to_gold_standard": "Normalization is part of ensuring that model outputs are compared fairly to author-validated gold-standard annotations; no numeric comparison of 'with vs without normalization' provided beyond pilot observations.",
            "uuid": "e2208.5"
        },
        {
            "name_short": "Expert-led Case Studies",
            "name_full": "Expert-led qualitative case studies (mathematics, materials science, environmental science)",
            "brief_description": "Domain experts manually review model-flagged issues against the official withdrawal notices and benchmark annotations to verify false negatives and to adjudicate unannotated model flags; they provide concrete examples of where computational checks succeed or fail vs experimental/computational domain practices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Expert adjudication of LLM-flagged errors in withdrawn manuscripts (domain expert review and author consultation)",
            "scientific_domain": "mathematics, materials science, environmental science",
            "validation_type": "hybrid",
            "validation_description": "Experts (researchers or PhD-trained postdocs) evaluated LLM-flagged errors for selected withdrawn manuscripts. They: (a) checked whether model outputs corresponded to benchmarked errors (identifying mislabeled FNs), (b) examined model-predicted issues outside annotations to determine if they were genuine (consulted original authors when uncertain), and (c) diagnosed root causes (OCR, long-range context, missing derivations). Specific domain examples include: mathematical proof gap in algebraic geometry papers (purely logical/theoretical validation), materials science papers where experiments (1H NMR, PXRD, BET, photocatalysis DPBF bleaching) were central and where DFT predictions contradicted reported yields, and environmental science where misuse of log vs ln changed climate forcing numerics.",
            "simulation_fidelity": "Case studies reference domain computational tools used in the original papers (DFT calculations and SRIM ion-penetration simulations). The paper reports concrete mismatches: e.g., SRIM plot axis indicates stopping range &lt; 20 nm whereas authors claimed 1 µm penetration; DFT relative energy of an acetal product (+7.4 kcal/mol) contradicts experimentally observed 81% yield (implying a negative ΔG ≈ -0.85 kcal/mol), indicating a failure of DFT explanation or misinterpretation.",
            "experimental_validation_performed": true,
            "comparison_simulation_vs_experiment": "Yes — explicit examples where computational predictions (DFT, SRIM) did not align with experimental claims: SRIM depth vs claimed penetration (orders-of-magnitude mismatch) and DFT-predicted thermodynamics inconsistent with observed high yields; the paper highlights these as evidence that simulation/DFT results alone did not justify authors' experimental conclusions.",
            "validation_success_rate": null,
            "domain_validation_standards": "Experts expect domain-specific controls and metrics: e.g., 1H NMR cross-validation for composition derived from UV, correct unit conventions for BET (m^2/g), normalized PXRD intensities or internal standards, quantum yields/turnover numbers for photocatalysis comparisons; without these, claims are poorly validated.",
            "when_simulation_sufficient": "Paper argues (via case studies) that simulations (DFT, SRIM) are not sufficient without appropriate experimental controls/validation and domain-appropriate interpretation; simulation-derived energetic/kinetic claims need experimental corroboration and careful accounting for entropic/solvent effects.",
            "simulation_failures": "Concrete examples: (1) SRIM stopping-range discrepancy (authors' claims of µm-scale penetration contradicted by SRIM axis showing &lt;20 nm), (2) DFT predicted +7.4 kcal/mol for acetal product while experiments report 81% yield (thermodynamic inconsistency), (3) climate paper computational misuse (log10 vs ln) producing a ≈2.3× scaling error in forcing calculations.",
            "uncertainty_quantification": "Original papers sometimes lacked proper uncertainty/statistical reporting (e.g., EDX shifts under 2 wt% without statistics, missing tables of contact-angle/Wa values). Experts criticized insufficient statistical treatment or missing normalization/controls.",
            "fabrication_detection": "Case studies reveal LLM hallucinations (e.g., o3 hallucinated lack of NMR confirmation) and reading errors; detection of fabricated/incorrect claims relied on expert cross-checks and consultation with original authors rather than automated fabrication detectors.",
            "validation_cost_time": "Expert review and author consultation required significant manual labor; no precise cost/time numbers given, but the paper emphasizes the human-in-the-loop necessity and the review overhead for model-flagged outputs.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Experts note that some errors span multiple sections or require supplemental data (e.g., SI tables) unavailable to automated pipelines; LLMs lack access to supplementary materials, causing false positives/hallucinations; domain-specific conventions and hidden assumptions impede automated verification.",
            "acceptance_credibility": "The paper uses expert adjudication to increase validity of conclusions about model failure modes; it argues that reliable automated verification must include expert review and domain-aware checks to be credible to the scientific community.",
            "comparison_to_gold_standard": "Experts used author withdrawal notes and errata as the gold standard; they found cases where LLMs either missed the core error (false negatives) or hallucinated errors (false positives), reinforcing that model outputs are not yet comparable to human expert validation.",
            "uuid": "e2208.6"
        },
        {
            "name_short": "Author-confirmation Filter",
            "name_full": "Author-acknowledgement-based ground-truth filtering",
            "brief_description": "A data curation rule that retains only report-manuscript pairs where the original authors explicitly acknowledge the mistake (e.g., erratum, retraction, or direct PubPeer reply), and treats those acknowledgements as definitive evidence of genuine errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Author-acknowledgement filtering (Stage 3 data curation)",
            "scientific_domain": "dataset curation / validation standards",
            "validation_type": "other",
            "validation_description": "During SPOT curation, only manuscripts with explicit author acknowledgements of mistakes were retained: PubPeer comments followed by explicit author replies acknowledging mistakes, and WITHDRARXIV self-retraction entries were treated as definitive evidence. This is used to reduce ambiguity in ground-truth error labels and avoid controversial/unconfirmed claims.",
            "simulation_fidelity": null,
            "experimental_validation_performed": null,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": null,
            "domain_validation_standards": "Author acknowledgement is used as a domain-agnostic standard for confirming errors across disciplines; the paper argues this reduces subjectivity compared to relying on third-party reports only.",
            "when_simulation_sufficient": null,
            "simulation_failures": null,
            "uncertainty_quantification": null,
            "fabrication_detection": "By requiring author acknowledgement, SPOT lowers the chance of false positives in annotations (i.e., avoids including disputed or unconfirmed claims), but does not attempt to detect fabricated manuscripts; it intentionally excludes errors that require external artifacts for verification.",
            "validation_cost_time": "Reduces annotation overhead by constraining candidate list, but requires scraping and matching author responses; no numeric cost provided.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Excludes genuine errors that authors do not acknowledge; some author acknowledgements may be incomplete and not capture all affected results; relies on public comment/reply availability.",
            "acceptance_credibility": "Using author confirmation increases credibility of ground truth in the scientific community because the original authors accept the error, making the benchmark more defensible.",
            "comparison_to_gold_standard": "This filtering method defines the gold standard used throughout SPOT; models are evaluated against these author-confirmed annotations.",
            "uuid": "e2208.7"
        },
        {
            "name_short": "DFT & SRIM Failure Examples",
            "name_full": "Documented mismatches between computational predictions (DFT, SRIM) and experimental claims in case-study papers",
            "brief_description": "Concrete cases from SPOT's materials and environmental science papers where computational/simulation outputs did not support the authors' experimental claims, illustrating limits of simulation-only validation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "DFT calculations (domain quantum-chemistry) and SRIM ion-penetration simulations",
            "scientific_domain": "materials science / computational chemistry / ion-beam simulation",
            "validation_type": "high-fidelity simulation",
            "validation_description": "The paper reports on original-manuscript uses of DFT to compute reaction barriers/relative energies and SRIM to model ion penetration. SPOT case studies highlight: (a) DFT predicted an acetal product to be +7.4 kcal/mol (thermodynamically unfavorable) while experiments reported 81% yield implying a negative ΔG, and (b) SRIM figure axis (0–1000 Å) indicates stopping ranges below ~20 nm while the authors discuss µm-scale penetration, an orders-of-magnitude disagreement.",
            "simulation_fidelity": "SRIM: domain-standard Monte Carlo ion-stopping simulation (electronic/nuclear stopping approximations); fidelity is high for ion ranges but dependent on axis/units—paper highlights clear plotting/interpretation errors. DFT: standard quantum-chemical energetics; fidelity depends on functional/basis and solvent/entropic approximations not fully discussed in the original manuscript; SPOT shows DFT-derived thermodynamics were used without adequate experimental corroboration.",
            "experimental_validation_performed": true,
            "comparison_simulation_vs_experiment": "Explicit mismatches: SRIM predicted &lt;20 nm stopping range vs authors' µm claim; DFT predicted +7.4 kcal/mol vs observed 81% acetal yield (thermodynamic inconsistency). The paper uses these as examples where simulation-based claims were insufficient or misinterpreted with respect to experiments.",
            "validation_success_rate": null,
            "domain_validation_standards": "Experts emphasize need for corroborating experiments, reporting of units/axes, statistical treatment of small compositional changes (EDX), and proper normalization/controls for PXRD and BET measurements. Simulations should be accompanied by uncertainty bounds and consistent units/interpretation.",
            "when_simulation_sufficient": "Paper implies simulation alone is insufficient when (a) simulations contradict experimental outcomes, (b) key experimental controls or data are missing, or (c) simulation assumptions (e.g., neglecting solvent/entropy) are not addressed; in such cases experimental corroboration is required.",
            "simulation_failures": "Documented failures as above (SRIM and DFT examples). Additionally, misuse of mathematical/logarithmic bases in climate forcing calculations is reported (log10 vs ln) as a computational error.",
            "uncertainty_quantification": "Original manuscripts often lacked statistical treatment or error bars for small changes (e.g., EDX shifts &lt;2 wt% reported without statistics); SPOT criticizes missing uncertainty reporting for simulation-experiment comparisons.",
            "fabrication_detection": null,
            "validation_cost_time": "High: DFT and SRIM require domain expertise and compute resources, and reconciling them with experiments requires additional measurements and statistical analyses (not quantified).",
            "hybrid_validation_approach": true,
            "validation_limitations": "Original manuscripts often failed to account for entropic/solvent effects, lacked experimental controls, or misreported units/axes, undermining simulation validity; SPOT highlights that poor reporting and missing supplementary data hamper reliable validation.",
            "acceptance_credibility": "Demonstrates that simulation results alone, when misapplied or inadequately reported, will reduce acceptance by experts; highlights the need for cross-validation with domain-appropriate experiments and careful reporting.",
            "comparison_to_gold_standard": "These examples are compared against reported experimental outcomes (observed yields, claimed penetration depths) and found inconsistent, undermining claims that simulations validated the experiments.",
            "uuid": "e2208.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Corebench: Fostering the credibility of published research through a computational reproducibility agent benchmark",
            "rating": 2
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 1
        },
        {
            "paper_title": "Automating scientific verification (related fact verification and peer review literature)",
            "rating": 1
        },
        {
            "paper_title": "Llm-as-a-judge & reward model: What they can and cannot do",
            "rating": 1
        }
    ],
    "cost": 0.0258965,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research
17 May 2025</p>
<p>Guijin Son 
Jiwoo Hong 
Honglu Fan 
Heejeong Nam 
Hyunwoo Ko 
Seungwon Lim 
Jinyeop Song 
Jinha Choi 
Gonçalo Paulo 
Youngjae Yu 
Stella Biderman 
Onelineai 
Eleutherai 
Kaist Ai 
Boeing Korea 
Yonsei University 
When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research
17 May 20253CD32DCADFD89CE19C80B7608D0D0B83arXiv:2505.11855v1[cs.CL]
Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists.To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts.In this work, we explore a complementary application: using LLMs as verifiers to automate the academic verification of scientific manuscripts.To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators.Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1% recall or 6.1% precision (o3 achieves the best scores, with all others near zero).Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability.Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings.These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.https://huggingface.co/datasets/amphora/SPOT-MetaData</p>
<p>Introduction</p>
<p>From simple next-token predictors [1,2], large language models (LLMs) have evolved to exhibit graduate-level STEM proficiency [3][4][5], generate hypotheses [6,7], synthesize literature [8], and draft manuscripts [9].Such advances have driven interest in their deployment as "AI Co-Scientists" [10,11], proving to be viable options in the "generative" role of scientific research.They have rediscovered established findings [12] and generated novel hypotheses worthy of investigation across diverse fields [13][14][15].However, despite their widespread usage as "generators" in the forward pass of scientific research, their utility in the backward pass of academic verification or as verifiers remains underexplored, a blind spot in which most systems lean on LLM judges [16] without validation on their credibility in reviewing scientific research.Prior research on factual verification has primarily focused on everyday knowledge tasks [17][18][19], reference-based claim checking [20,21], or computerscience disciplines alone [22][23][24].This limits the potential applicability of the proposed benchmarks as evaluation tools for verification systems in AI-driven science research.</p>
<p>In this paper, we introduce SPOT (Scientific Paper Error Detection), a complex multi-modal academic error verification benchmark, comprising 83 up-to-date manuscripts spanning ten scientific fields with multiple human-annotated errors.Given large-scale multi-modal inputs with 12,000 text tokens and 18 images on average, multi-modal LLMs (MLLMs) are tasked with generatively identifying more than one error with varying difficulties in a single paper: e.g., , factual inconsistencies, figure duplications, and mathematical errors.We only select papers published from 2024, minimizing the potential contamination with parametric knowledge during evaluation [25].It should be noted that, whereas prior evaluation suites focus on sentence-level fact checks of everyday knowledge [26,27] or on reproducing noisy peer-review feedback [28,29], SPOT extends verification to the full complexity of frontier-level scientific research.This paper is mainly divided into three parts.</p>
<ol>
<li>SPOT Benchmark Design Principles (Section 2): We detail our efforts of multiple automated filtering, author verifications, and human annotations, highlighting our commitment to include confirmed, noncontroversial errors across diverse scientific subdomains.2. Model Evaluation and Analysis (Section 3) We present evaluation results, demonstrating that even the state-of-the-art models struggle on SPOT.Specifically, OpenAI's o3 [30] and Llama-4-Maverick [31] achieved 18.4% and 0.9% at pass@1 to name a few.Furthermore, model confidence approaches zero when repeated over eight independent trials, questioning their reliability.We also observe that proprietary reasoning models suffer in detecting figure-related errors, highlighting shortcomings in their multi-modal capabilities.Such results cast serious concerns, revealing significant gap between current AI capabilities and the demands of rigorous scientific verification.3. Expert-led Case Studies (Section 4) We present expert-led case studies in mathematics and materials science, analyzing model outputs to diagnose their failures.Our observations show that models struggle with long-tail knowledge likely absent in web data and extremely long contexts.We also note that, without fully spelled-out derivations, models fail to understand some calculations and overlook domain-specific conventions, making student-like errors.</li>
</ol>
<p>SPOT: Automating Error Detection in Scientific Research</p>
<p>In this section, we introduce a detailed overview of SPOT, a complex multi-modal academic verification benchmark with cross-validated scientific manuscripts.We ensure credibility in the error annotations through a cross-validation process between human experts in each field and proprietary language models (Section 2.1).Spanning over ten different fields and six error types (Section 2.2), we introduce evaluation protocols mainly based on precision, recall, and pass@K (Section 2.3).</p>
<p>Data Curation</p>
<p>Stage 1 -Seed Collection We source our seed manuscripts from two major repositories: (1) WITH-DRARXIV [32] and (2) PubPeer 1 .First, we extract entries annotated as "factual/methodological/other critical errors" from WITHDRARXIV, a dataset of 14,000 papers and their associated retraction comments.Second, we crawl PubPeer, an anonymous post-publication peer review website, where users flag methodological flaws, image manipulations, and other scientific concerns.Following [33], we query initial searches using alphabets, extract high-frequency keywords from the returned paper titles, re-query using those keywords, and scrape each paper's metadata (title, authors, venue) alongside the entire comments.We briefly attempted to include medRxiv and bioRxiv, but having retrieved only 1 and 13 papers, respectively, we dropped them due to the low yield.</p>
<p>Stage 2 -Automated Filtering We apply two GPT-4o [34] 2 filtering passes.The first retains comment-manuscript pairs that unambiguously pinpoint a specific section, figure, equation, or table, reducing our pool to 1,855 WITHDRARXIV and 25,378 PubPeer samples.The second pass removes reports that require external artifacts (e.g., duplicated images across papers or errors detectable only via external datasets or code).Finally, to avoid overlap with GPT-4o's training cutoff , we filter for papers published after 2024, yielding 58 WITHDRARXIV and 215 PubPeer samples.</p>
<p>Stage 3 -Error Validation by Original Authors For remaining manuscripts, we only retain those the original authors directly confirmed.Specifically, we only retain PubPeer comments followed by an explicit author response acknowledging the mistake and treat WITHDRARXIV self-retractions as definitive evidence of a critical error.In all cases where the author themselves admits the problem, we take this acknowledgment as confirmation of a genuine error.While some errors may appear to be evident, we do not include any error with explicit acknowledgment from the original authors, as many of the work cover ungoing areas of research, which remain unsettled in the scientific discourse.</p>
<p>Stage 4 -Sanity Check from Human Annotators We further apply a two-stage human validation with mutually exclusive annotators.First, with part of the authors as human annotators, we manually validate if remaining flagged issues fulfill three conditions: (1) self-contained, (2) identifiable, and</p>
<p>(3) explicitly acknowledged by the original authors.For those which satisfy the conditions, We retrieve the archived PDF to verify that the error remains visible, then document a concise description of the problem, quote the author's acknowledgement verbatim, and assign both an error category and a severity rating-proxied by the form of the author's response (erratum versus retraction).Afterwards, the second group conducted a comprehensive audit of all annotations to ensure consistent application of these standards.The final SPOT benchmark comprises 83 manuscripts with 91 annotated errors.Although modest in size, our dataset aligns with recent trends toward compact, high-quality benchmarks: MT-Bench (80 items) [16], GPQA-D (198 items) [4], AIME 2024/2025 (30 items each) [35], USAMO 2025 (6 items) [36] and PaperBench (20 items) [37].</p>
<p>Stage 5 -Normalization We normalize final manuscripts in PDF format into text and image sets to best evaluate the comprehension of the target models.While prior benchmarks in manuscript error detection [24] and AI-assisted science [38] have relied on raw PDFs or text-only inputs, this approach offloads document understanding to OCR and parsing modules rather than the LLM itself, thereby conflating upstream parser failures with downstream model errors.Instead, we process all the documents for usage.We first employ Llama-Parse 3 to convert each PDF into Markdown and capture high-fidelity screenshots of every figure, table, and equation.In pilot experiments, OCR failures, particularly in mathematical expressions, led downstream models to misinterpret formatting artifacts as errors.To address this, we introduce a refinement stage.For each page, the initial OCR text and screenshots (one full-page image plus isolated equations and paragraphs, roughly eight images per page) are sent to GPT-4.1 for correction.Finally, we conduct a manual audit of all processed pages to ensure that every flagged error remains visible and accurately represented in the OCR output.</p>
<p>Benchmark Statistics</p>
<p>Error Types We derive the six categories in Table 1 inductively from our annotations rather than setting a priori.As we review each error, we group similar cases.This is to capture the true distribution of errors existing in manuscripts.During this process, figure-duplication instances initially overwhelmed the dataset, so we filtered based on severity and paper category to prevent a single type from dominating.tokens per manuscript (mean ± std., range) and images per manuscript (mean ± std., range).All token counts were computed using the GPT-4o [39] tokenizer from tiktoken [40].Right: Six error categories with concise descriptions and instance counts in parentheses.</p>
<p>Evaluation protocol</p>
<p>We provide the full paper as interleaved text and image data, followed by the prompt to return every error with each error's location (section, figure, equation, or table), accompanied by a description.The output is prompted to be a structured JSON format (see Appendix F for an example).</p>
<p>Evaluation Metric We mainly evaluate verification performance through precision, recall, and pass@K.A predicted error is counted as a true positive (TP) only when the model's reported location matches a benchmark annotation and an LLM confirms they indicate the same error 4 .All other predictions, including those at non-annotated locations or those matching an annotated location but with a different description, are considered false positives (FP), and any benchmark annotation the model fails to predict is a false negative (FN).We treat the error annotations included in SPOT as exhaustive: any model-reported error not matching an annotation is counted as a false positive.</p>
<p>Although models could, in principle, flag genuine errors outside our annotations, through case studies later in this paper, we notice such cases are highly unlikely.To summarize model performance, we
Precision = TP TP + FP , Recall = TP TP + FN . (1)
Precision quantifies the proportion of the model's flagged errors that match benchmark annotations, penalizing unexpected predictions, and is most appropriate when false positives impose significant review overhead or undermine confidence in the tool's outputs.Recall quantifies the proportion of annotated errors the model successfully identifies, penalizing missed detections.In practice, users concerned about model hallucinations or the impact of unannotated flags should focus on Precision.</p>
<p>In contrast, those seeking comprehensive error coverage, or who doubt the exhaustiveness of our annotations, should emphasize Recall.</p>
<p>Following [42] and [43], to capture how error detection improves with multiple attempts, for K runs per paper, we define
pass@K = 1 N i=1 |G i | N i=1 g∈Gi 1 ∃ s ∈ {1, . . . , K} : g ∈ p i [s] ,(2)
where G i is the set of annotated errors in paper i and p i [s] the set predicted in the s-th run.With 83 papers and 91 total errors we generate N = 8 independent runs per paper.For each pass@K we draw K runs without replacement from the eight, repeat this resampling B = 1000 times, and report the mean and standard deviation of the resulting bootstrap distribution for K ∈ {1, 4}.</p>
<p>Main Results and Analysis</p>
<p>In the following sections, we evaluate six proprietary models: OpenAI o3 [30], GPT-4.1 [44], Google Gemini 2.5 Pro [45], Gemini 2.0 Flash Lite [46], Anthropic Claude 3.7 Sonnet:Thinking [47], and Claude 3.7 Sonnet and four open models: Qwen 2.5-VL-72B/32B-Instruct [48], and Llama 4 Maverick/Scout [31].We select the most capable models per family and observe that these models already score near zero in SPOT.Accordingly, as smaller models are unlikely to perform any better, we do not include them in our evaluations.All models are accessed via APIs, and each call is retried up to three times; those that still fail or are cut off due to length limits are marked incorrect.and Llama-4-Maverick, which match proprietary models on existing multi-modal benchmarks like MMMU [49] or MathVista [50], perform far worse on SPOT.As shown in Figure 3   A New Challenging Benchmark for STEM. Figure 3 illustrates the performance of o3 on six benchmarks: MathVista [50], MMLU-Pro [51], GPQA Diamond [4], MMMU [49], HLE [52] and SPOT (recall).o3 exceeds 80% on the first four benchmarks, demonstrating robust general reasoning and code understanding.However, performance drops to roughly 20% on HLE, a curated set of frontier, research-level academic questions, and remains similarly low on SPOT (21.1%).This drop in performance underscores the difficulty of spotting errors in lengthy scientific text and figures.</p>
<p>Main Results</p>
<p>Reasoning Models Excel at Equations but Falter on Figures</p>
<p>The right panel of Figure 4 presents the performance of six models across each category.In the Equation/Proof category, o3 leads with a 62.6% (pass@4), followed by Gemini-2.5-Proat 36.4%, while all other models remain below 5%, underscoring o3's superior mathematical reasoning.Surprisingly, GPT-4.1 achieves a 44.4% in the Figure Duplication category, outperforming Claude-3.7-SonnetThinking (33.3%), o3 (0%), and Gemini-2.5-Pro(0%), revealing a weakness in figure analysis in reasoning models.</p>
<p>Unreliability of Miscalibrated Models.</p>
<p>Alongside pass@4, calibration [53,54] indicates how much we should trust a model's predictions.</p>
<p>In error detection, where false positives can incur substantial time and labor, knowing when to trust a model is crucial.For each error category, we assess calibration by comparing the model's actual performance, measured as its average pass@4 rate, with its self-estimated confidence.For details on how the confidence is derived see Appendix C.However, Figure 4 (right) shows that confidence correlates only weakly with pass@4, and the left panel reveals that most models report very low confidence, clustering near zero.Across 498 model-instance evaluations (83 instances × six models), we observe only two cases (both from o3) of full confidence, highlighting the widespread difficulty of reliably detecting errors in scientific manuscripts.These findings demonstrate substantial variability across categories and reaffirm that current LLMs remain unreliable for scientific error detection.</p>
<p>Impact of Multi-Modality in Detecting Scientific Errors</p>
<p>To isolate the impact of visual inputs, we create a text-only subset of SPOT by removing all instances from the figure-duplication and any data-inconsistency category that necessitate figures for comprehension.This yields 48 instances in which errors can be detected using text alone.Table 3 compares model performance on the selected instances under multimodal and text-only conditions.The left panel reports each model's accuracy on these 48 cases with figures included (extracted from the runs of Table 2); the right panel shows performance after stripping out all figures.In the text-only setting, we add three unimodal LLMs: DeepSeek-R1 [55], DeepSeek-V3 [56], and Qwen3-235B-A22B [57].</p>
<p>We observe two key findings.First, most models improve in recall and pass@4 when removing images, suggesting that figures usually act as distractors rather than helpful context.The exceptions are o3 and Gemini-2.5-Pro,which see a modest drop without visual inputs.This indicates that they have been leveraging figures to understand the paper rather than treating them as mere auxiliary signals.Second, the divide between proprietary and open models is vast in the multi-modal setting, proprietary systems maintain substantial recall (e.g., o3 at 34.6 %, Gemini-2.5-Proat 13.7 %) and pass@4, whereas open-source models collapse to near zero.Under text-only conditions, while proprietary systems still lead, the performance margin is comparably smaller.</p>
<p>Case Study</p>
<p>We select two withdrawn manuscripts, each from mathematics and materials science, for a qualitative review.A domain expert evaluated each paper, either a researcher with relevant publications or a PhD-trained postdoc in the field.Reviewers are provided the LLM-flagged "errors" from o3 and Gemini 2.5 Pro alongside the official withdrawal notices.They are asked to verify whether the model has missed any benchmarked errors (i.e., true positives mislabeled as false negatives).Moreover, they are required to assess each flagged issue that falls outside our annotations to determine if any presumed false positives correspond to valid flaws.We consulted the original authors to verify the disputed issues whenever a reviewer remained uncertain. 5</p>
<p>Mathematics</p>
<p>Petersen and Tommasi [58] studies the configuration spaces of points in algebraic varieties with a multiplicative decomposition, and discusses some applications such as the cohomology of moduli stacks of hyperelliptic curves.It was withdrawn because of a gap that lies in the core arguments of Theorem 1.8 and Theorem 1.13 which invalidates the bulk of the paper.</p>
<p>Both o3 and Gemini-2.5-Proexclusively flag issues in Section 3. Ironically, this is the only part of the manuscript not affected by the actual mathematical gap.o3 criticizes the calculation of H k (M 1,1 , V ℓ ) in Section 3.3, claiming that the use of Eichler-Shimura isomorphism is wrong without giving too much details.But there is no mistake in this part and the claim is hallucinated.It is worth noting that applying Eichler-Shimura isomorphism in a similar context as the paper is exceedingly rare in internet data.The formula may also appear differently than how the Eichler-Shimura isomorphism is presented in common internet sources, whose equivalence requires genuine understanding of multiple subjects, such as variation of Hodge structures and the theory of modular forms.</p>
<p>Eichler-Shimura is quoted as
H 1 (M1,1, V ℓ ) ∼ = S ℓ+2 ⊕ Q(−1 − ℓ), but the Tate (Eisenstein) summand should be Q(−1), independent of ℓ. The extra "−ℓ" is a mis-twist.
Figure 5: o3's feedback on [58].</p>
<p>On the other hand, Gemini-2.5-Protackles Theorem 3.8 as shown in Figure 6.Despite doing reasonable calculations, the model makes a false claim on H 0 (A 1,1 , V (2, 2))(−1), resembling mistakes students could make after recollecting a wrong fact.</p>
<p>The stated result H 1 (M2, V (2, 2)) = Q(−3) contradicts the Gysin-sequence derivation (Sec.3.1), which yields
H 1 (A2, V (2, 2)) = 0, together with H 0 (A1,1, V (2, 2))(−1) = Q(−3) (Sec. 3.5
) and the non-vanishing/isomorphism of the Gysin map G :
H 0 (A1,1, V (2, 2))(−1) → H 2 (A2), thereby implying H 1 (M2, V (2, 2)) = 0.</p>
<p>Material science</p>
<p>In our materials science case study, we select [59], which uses TfOH to synthesize covalent organic frameworks (COFs).However, it mislabels several samples in its figures.These errors are easier to spot than those in the mathematics papers and would be obvious to any attentive reader.</p>
<p>Optimal TfOH concentration is stated as 0.006-0.016M, yet all optimisation and standard syntheses use 0.2 M TfOH.Internal inconsistency invalidates the universality of the claimed optimum.Errors pointed out by o3 exemplify failures in both long-range context comprehension and multi-hop reasoning.For instance, in Figure 7, o3 does not realize that the optimal concentration value reported by the authors (0.006-0.016M) is the concentration of the final mixture, while the the second value (0.2M) is the concentration of the acid before being added to the final mixture.This misunderstanding likely arises because the optimal concentration in the final mixture is mentioned only once, and the explicit calculation is not shown throughout the manuscript.As a result, o3, having seen references only to the concentration before mixture, fails to infer the relationship between the two values.</p>
<p>(A) There is a contradiction in the indexing of PXRD peaks for TAPPy-TFPPy-COF (Figure 6H).The peaks are initially assigned to facets including (020): 'TAPPy-TFPPy-COF displayed peaks [...] (B) The BET surface area for the scaled-up TFPPy-PDA-COF is reported as '1606 cm 2 g −1 '.The correct unit is 'm 2 g −1 .'This unit error misrepresents the surface area by a factor of 10,000, constituting a fundamental data-reporting mistake.</p>
<p>Figure 8: Two of Gemini 2.5 Pro's feedback on [59].</p>
<p>In (A) of Figure 8, Gemini 2.5 Pro seems to make a "reading" mistake, attributing the second facet pair to TAPPy-TFPPy-COF when it in fact describes TAPPy-BPTC-COF.Notably, however, in (B), it notices a potential error in the units, where a certain compound was assigned a surface area 10000x smaller than all the other compounds in the same family.Because the authors do not mention this extreme property of this material, we suspect that this is a real typo.While not severe, this error is the only instance in which we observe an LLM identifying an unannotated but genuine error.</p>
<p>5 Related Works AI Co-Scientists Recent breakthroughs have pushed LLMs to PhD-level performance on STEM benchmarks [4], driving efforts to embed them as generators of the scientific forward pass, encompassing hypothesis generation [6], experimental planning [38], and manuscript drafting [9].Such systems, or AI Co-Scientists [11,15], employ agent-based pipelines that mirror the stages of scientific research.However, concurrent works often omit a rigorous backward pass or "verification" and instead rely on LLM judges [16].Yet, prior studies demonstrate that LLM judges may fail on complex tasks [60], allowing factual and methodological errors to remain undetected.This compromises the reliability of AI-driven research.It should be noted that verifiability has long been central to scaling AI progress: self-supervised learning employs next-token prediction as a provable training objective [61]; instruction-tuning leverages LLM-generated instruction-response pairs as reliable signals [62]; and reinforcement learning uses verifiable rewards [55] for alignment.Likewise, we posit that robust scientific verification must underpin reliable LLM-driven scientific research.</p>
<p>Automating Scientific Verification Two research strands, fact verification and automated peer review generation, may appear related to SPOT, but each has critical limitations.Prior fact verification benchmarks [26,27] concentrate on claims at the sentence level and rely on text inputs only to assess consistency with reference documents.Automated peer review systems draw almost entirely on computer science publications [63,23,24], restricting their disciplinary coverage.These approaches measure success by matching past reviews via metrics such as ROUGE [64] rather than detecting errors.They also overlook the inherent noise in peer review reports [65,66] and seldom apply adequate quality control or validate ground truth.Our work sets apart from previous efforts by applying expert and automated validation to distill only genuine mistakes into SPOT, additionally, we package full, multimodal papers into models at inference mirroring real-world academic verifications.</p>
<p>Conclusion</p>
<p>In this paper, we introduce SPOT, a multimodal error-detection benchmark that captures the full complexity of frontier-level scientific research.Each instance averages 12,000 text tokens and 18 images, posing a significant challenge for current large language models: OpenAI's o3 and Google's Gemini 2.5 Pro achieve pass@1 scores of only 18.4 % and 7.3 %, respectively.Our expert-led case studies further show that these models fall short in long-tail domain knowledge and implicit multi-step calculations.Together with the rise of interest in AI Co-Scientists, these results highlight the need for further research in robust verification systems to ensure reliability in AI-driven research workflows.</p>
<p>NeurIPS Paper Checklist</p>
<p>Claims</p>
<p>Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
<p>Answer: [Yes]</p>
<p>Justification: The abstract and introduction is written to summarize our work.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p>
<p>Limitations</p>
<p>Question: Does the paper discuss the limitations of the work performed by the authors?</p>
<p>Answer: [Yes]</p>
<p>Justification: See Appendix A.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.</p>
<p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.</p>
<p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p>
<p>Theory assumptions and proofs</p>
<p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
<p>Answer: [Yes] Justification: This paper discusses a novel benchmark without theoretical results.We provide how our confidence metric is derived in Appendix C.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental result reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
<p>Answer: [Yes] Justification: We provide details on the parsing process in Section 2, and further details on the prompts and generation configurations in Appendix F. The benchmark and codes are also provided through supplementary materials.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
<p>Answer: [Yes] Justification: The experiments are fully reproducible using the provided code.Minor variations may arise from proprietary model availability or inherent stochasticity, but these do not affect our overall conclusions.</p>
<p>Guidelines:</p>
<p>• The answer NA means that paper does not include experiments requiring code.• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
<p>Experimental setting/details</p>
<p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
<p>Answer: [Yes] Justification: We do not train our own models, but prompts and generation configurations for inference are provided in Appendix F.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment statistical significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Answer: [Yes] Justification: Table 2, Table 3 all provide means and standard deviation of multiple independent trials and bootstrapping.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>
<p>• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Answer: [Yes] Justification: We do not train custom models.Our sharing policy for the SPOT dataset varies with the copyright status of each original paper; see Appendix E for details.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<p>Licenses for existing assets</p>
<p>Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
<p>Answer: [Yes]</p>
<p>Justification: All assets used in the paper are mentioned through footnotes or references.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [Yes]</p>
<p>Justification: See Section 2 and Appendix E.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and research with human subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Answer: [Yes]</p>
<p>Justification: An image of the annotation platform is available in Appendix E.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
<p>Institutional review board (IRB) approvals or equivalent for research with human subjects</p>
<p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
<p>Answer: [NA] Justification: Paper does not involve crowdsourcing nor research with human subjects.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.for what should or should not be described.</p>
<p>A Limitations</p>
<p>Benchmark Coverage By prioritizing copyright compliance(Appendix E), contamination prevention, and annotation accuracy, SPOT remains relatively modest in size.We leave the expansion of this effort to create larger, more diverse benchmarks that span additional scientific disciplines and error categories to future works.</p>
<p>Annotation Validity and Evaluation Protocol All errors in SPOT are verified by explicit author acknowledgments or retraction notices, but the complexity of scientific manuscripts means some true errors may be unannotated.Our case studies reveal that false negatives can arise from the following cases:</p>
<p>1.The author's note contains an error location that does not sufficiently cover all the affected results.</p>
<ol>
<li>There exist smaller errors unrelated to the main technical error in the preprint.</li>
</ol>
<p>Conversely, false positives may occur when:</p>
<p>1.An LLM correctly points out a theorem that contains an error, but the content in the LLM's response is still irrelevant.</p>
<p>We therefore recommend a secondary expert review, particularly for domains with complex logical dependencies or deep specialization, to validate and refine model-flagged errors.In Table 2, we evaluate each model on complete manuscripts, which can span up to 140,000 characters and 90 figures.However, LLMs still struggle to synthesize long-context information [67]; to isolate the effect of context length on error detection, we extract the page containing the ground-truth error and rerun the same detection prompt on this shorter segment.</p>
<p>B Additional Analysis</p>
<p>Comparing the full-paper and segment-only settings decouples long-context processing from core error-detection ability.For this ablation, we conduct experiments on a subset of 36 instances, excluding the Equation/Proof category-mathematical papers often rely on global notation and prior results, making single sections insufficient-and we omit any errors that span multiple sections.</p>
<p>Figure 9 plots ∆ = (segment-only -full-paper) for precision and recall.Gemini-2.5-Proleads with gains of +4.7 precision and +13.5 recall.o3 follows at +3.7/+8.5, then Claude-3.7-Sonnetat +2.2/+2.1, and Llama-4-Maverick at +1.6/+2.2-showingthat long-context processing often masks their true error-detection performance.o3's smaller gains reflect the removal of Equation/Proof cases, its original strength.Qwen2.5-VL-72B-Instructshows almost no change (-0.4 precision, -0.1 recall), indicating a fundamental limit in its error-detection capability rather than a context-length issue.</p>
<p>B.2 Impact of Test-Time Scaling in Detecting Scientific Errors</p>
<p>Test-time scaling involves adjusting the inference budget [68], such as the depth of reasoning or number of solution paths explored [69], to boost model performance on complex tasks.This approach is widely adopted in STEM and reasoning benchmarks [70], where allocating more computational effort to inference has been shown to yield higher performance.We use OpenAI's o4-mini series [30] for our experiments and vary the "reasoning effort" parameter across low, medium, and high settings.In Figure 10, we see that o4-mini's error-detection performance increases almost linearly with higher reasoning effort, demonstrating that scaling computation at test time effectively boosts accuracy.This finding is consistent with how specialized "Thinking" modes (e.g., Claude 3.7-Sonnet:Thinking VS-Claude 3.7-Sonnet) and reasoning-trained models (DeepSeek-R1 vs. DeepSeek-V3) deliver similar boosts, also in line with recent error-detection literature [72].</p>
<p>C Estimating Confidence for pass@K</p>
<p>Given N papers with ground-truth error sets G 1 , . . ., G N , we define the pass@K metric as
pass@K = 1 N i=1 |G i | N i=1 g∈Gi 1 ∃ s ∈ {1, . . . , K} : g ∈ p i [s] ,(3)
where p i [s] denotes the set of errors predicted in the sth run for paper i.This captures the fraction of all ground-truth errors detected in at least one of K independent attempts.</p>
<p>C.1 Unbiased Per-Error Confidence</p>
<p>To assign each ground-truth error g ∈ G i a confidence score, we perform n independent runs (here n = 8) and let c i,g be the number of runs in which g is detected.The probability that all K fresh attempts miss g is
n−ci,g K n K ,(4)
So one minus this quantity is the probability of ≥ 1 success [43].Hence the unbiased estimator for the pass@K probability of error g is pi
,g = 1 − n−ci,g K n K .(5)</p>
<p>C.2 Aggregating Confidence and Calibration</p>
<p>We then aggregate these per-error confidences into an overall self-estimated confidence:
Confidence = 1 N i=1 |G i | N i=1 g∈Gi pi,g .(6)</p>
<p>D Case Study D.1 Mathematics</p>
<p>We selected [58] and [73], both in the area of algebraic geometry in pure mathematics.[58] studies the configuration spaces of points in algebraic varieties with a multiplicative decomposition, and discusses some applications such as the cohomology of moduli stacks of hyperelliptic curves.It was withdrawn because of a gap that lies in the core arguments of Theorem 1.8 and Theorem 1.13.On the other hand, [73] studies projective hypersurfaces with real coefficients and presents an explicit description of the complex conjugation of singular cohomology in terms of algebraic de Rham cohomology.The authors' withdrawal note on arXiv suggests that a hidden assumption on Hodge numbers was accidentally introduced in the early set-up of section 2.2, thus making Theorem 2.3 only valid under the additional assumption.We run our evaluation using OpenAI o3 and Gemini 2.5 Pro over the two pre-prints, once for each combination.</p>
<p>• Section 2.3, proof of Theorem 2.3: They set
C φC (U k ), φC (U k ) = X φC (U k ) ∧ φC (U k )
and claim it equals the nonzero quantity U k,γ ; however, for even n, φC (U k ) has odd degree n − 1, so φC (U k ) ∧ φC (U k ) = 0 identically, making the equality impossible and invalidating Lemma 2.5 and Theorem 2.3.</p>
<p>• Example 2.4: Using the erroneous formula without complex conjugation yields µ = 0, which would force φC ([1]) = 0, contradicting that [1] represents the holomorphic 1-form.This example exposes the same flaw as in the main proof.</p>
<p>• Theorem 2.3 (implicit assumption): Formula (2.10) divides by U k,γ , but the proof provides no argument that U k,γ ̸ = 0.In fact, for odd-degree forms U k,γ vanishes, so the construction of fji(x) is undefined.Since we have already analyzed the LLM responses for [58] in Section 4, here we only describe some additional analysis on [73].Our first attempt at running our pipeline introduced many OCR errors, such as missing conjugations in an integral.Both o3 and Gemini-2.5-Proreacted to different OCR errors and identified them as mistakes (Figures 11 and 12).</p>
<p>After an improvement (inclusion of GPT-4.1 mentioned in Section 2), surprisingly, o3 claimed that there are no errors in [73].</p>
<p>As to Gemini 2.5 Pro, it proposed several potential mistakes (Figure 13).First, it argued that in the statement of Theorem 2.3, "the existence of a polynomial g ji such that r fj i ,gj i ̸ = 0" was never justified.Although this may be a fair writing feedback, this does not constitute an error.Aside from another OCR problem where the LLM saw an incorrect mathematical equation, another argument it made was that in Example 2.8, the basis of the cohomology on the right-hand side was unconventional and R • [ω] + R • (i[ω]) was better.This was made out of context, and the LLM might have forgotten that this example was an application of Corollary 2.7.</p>
<p>• Section 2.2, Theorem 2.3: The theorem requires the existence of a polynomial gji such that r f ji ,g ji ̸ = 0 (hence fji / ∈ Jac(G)).However, the paper never justifies that the chosen basis elements
U k = y i fji
(relative to Jac(S)) can always be represented by polynomials fji lying outside Jac(G).This hidden assumption is crucial for formula (2.10) to be well-defined and for the theorem to hold.</p>
<p>• Section 2.2, Example 2.4: The text claims the constant µ is proportional to
γ 1 ω γ 2 ω − γ 2 ω γ 1 ω .
But by Definition 2.2 and the pairing C(ω, ω), it should be
γ 1 ω γ 2 ω − γ 2 ω γ 1 ω ,
-the presented formula omits the complex-conjugate terms, indicating a calculation error.</p>
<p>• Section 2.4: For odd n, the paper defines
U k,γ = P X • P • P X T .
But since U k,γ comes from the pairing C(ϕC (U k ), ϕC (U k )) and P X is complex, the correct expression uses the conjugate transpose:
U k,γ = P X • P • P X T ,
so using P X T is mathematically incorrect.</p>
<p>• Section 2.5, Example 2.8: The basis for the real cohomology is written as
R (1 + µ y d (x))/2 + √ −1 R (1 + µ y d (x))/2 ,
which is unconventional and obscures the structure.A clearer description is
R • [ω] + R • i[ω] ,
explicitly showing the real subspace spanned by [ω] and its i-multiple.</p>
<p>Figure 13: Gemini-2.5-Pro'sregenerated feedback on [73].</p>
<p>D.2 Material science</p>
<p>We select [59] and [74], two recent materials-science studies.[59] investigates the synthesis of covalent organic frameworks (COFs), a class of porous materials with diverse applications, using a novel superacid-catalyzed process that enables the fabrication of new types of COF architectures.Several errors were pointed out about this article, mostly due to mislabeling COFs either in the text or in the figures, but none of them impact the conclusions of the article.[74] proposes a method to increase the hydrophobicity and electrical properties of certain types of films by incidence with low-energy argon beams.This work had a wrong caption, claiming to have used a different beam intensity on a certain panel when that panel was at a different magnification.Both these errors are easier to spot than the ones found in the mathematics articles.While some of the errors in [59] require some domain knowledge in COFs, most of the other errors can be spotted by paying close attention to the text and figures.</p>
<p>• Section 2.2, Model Reactions (Figures 1C-G): Quantitative yields of individual species are derived from a single UV absorbance at 240 nm without extinction-coefficient calibration or spectral deconvolution, even though all species overlap strongly in this region.The reported 92 %, 82 %, etc. compositions are therefore not experimentally supported.</p>
<p>• Section 2.2 / Figure 1H: Assumes acetals are unaffected by NaHCO3 neutralization; no control provided.If acetals hydrolyze under basic conditions, the calculated "protected" fractions are invalid, undermining the mechanistic interpretation.</p>
<p>• Section 2.3: Conclusion that barriers &lt; 20 kcal mol −1 imply "full reversibility at room temperature" ignores solvent and entropic contributions; a 19 kcal mol −1 barrier can correspond to multi-hour timescales.The stated kinetic implication is not justified by the DFT data.</p>
<p>• Abstract &amp; Section 2.5: Optimal TfOH concentration is stated as 0.006-0.016M, yet all optimization and "standard" syntheses use 0.2 M TfOH.Internal inconsistency invalidates the universality of the claimed optimum.</p>
<p>• Section 2.4 / Figure 2: Higher PXRD peak counts are used as evidence of greater crystallinity, but raw intensity is instrument-and packing-dependent.Without normalization or an internal standard, counts cannot be compared between samples, so the claim of superior crystallinity is unsupported.</p>
<p>• Section 2.7, "nine new COFs" claim: Authors conclude AcOH "cannot" give these COFs after testing only a single AcOH condition per monomer pair.Lack of systematic screening means the impossibility claim is unsubstantiated.</p>
<p>• Section 2.11, Photocatalysis: Performance deemed "exceptional" based solely on DPBF bleaching time, with no quantum yield, turnover number, or photon-normalized rate.Comparisons to literature therefore lack a common metric, invalidating the superiority claim.</p>
<p>Figure 14: o3's regenerated feedback on [59].</p>
<p>Most of the errors OpenAI's o3 pointed out on [59] were on the experimental setup and the conclusions that the authors derived from them.Although the specific points raised by o3 seem to be plausible mistakes, all of them either stem from misunderstandings by o3 or from ignoring information.For example in Figure 14, the claim that the UV-derived yields in Section 2.2 (Figures 1C-G) lack support is a hallucination: the authors validate those compositions with 1H NMR spectroscopy, which produces results similar to the UV measurements, so it is incorrect to argue that the yields are unsupported.In Section 2.3, o3 contends that barriers below 20 kcal mol −1 cannot guarantee "full reversibility" at room temperature.While entropic factors do affect reaction rates, barriers under 20 kcal mol −1 are generally regarded as low enough for reversibility, and multi-hour timescales still allow dynamic equilibrium.This error likely stems from o3's unfamiliarity with practical conventions in kinetic chemistry.Finally, for photocatalysis performance (Section 2.11), o3 questions the "exceptional" activity because no quantum yields or turnover numbers are provided.However, o3 does not have access to the supplementary information and thus is hallucinating that no DPBF bleaching comparisons exist, Table S10 demonstrates that these COFs outperform those in the literature.</p>
<p>Gemini 2.5 Pro also misunderstand the authors, in it feedback regarding Section 2.7 (Figure 4C) is related to the fact that the authors claim that a certain reagent is not suitable for the production of a</p>
<p>• Section 2.3 and Figure 1I (related to Section 2.2( 2)): The DFT-calculated relative energy of the acetal product P1 (+7.4 kcal/mol compared to benzaldehyde) indicates it is thermodynamically highly unfavorable, directly contradicting the experimental 81 % yield reported in Section 2.2(2), which would require a negative ∆G (approx.-0.85 kcal/mol).This discrepancy undermines the validity of the DFT calculations as an explanation for the experimentally observed acetal protection.</p>
<p>• Section 2.5 (1) and Figures 3A, 3C, 3E: The text states that low-boiling point solvents (MeOH, EtOH, n-propanol) are 'unsuitable' for reactions at 120 °C due to their boiling points being below this temperature.However, Figure 3 A,C,E present PXRD data for COFs synthesized in these solvents, presumably at 120 °C.The manuscript fails to specify whether these reactions were performed in sealed vessels.If not, solvent evaporation would lead to uncontrolled and non-reproducible reaction conditions, invalidating the results obtained with these specific solvents.</p>
<p>• Section 2.6: The BET surface area for the scaled-up TFPPy-PDA-COF is reported as "1606 cm 2 g -1 ."</p>
<p>The correct unit is m 2 /g.This unit error misrepresents the surface area value by a factor of 10,000 and is a fundamental error in data reporting.</p>
<p>• Abstract; Introductory paragraph; Section 2.7; Figure 4C: The paper claims "nine new iminelinked frameworks," but Figure 4C highlights ten COFs (#5, #9, #13, #14, #15, #18, #23, #24, #25, #28) as newly synthesized or inaccessible by traditional methods, creating a numerical inconsistency.</p>
<p>• Unnumbered paragraph on PXRD for Figure 6H: There is a contradiction in the indexing of PXRD peaks for TAPPy-TFPPy-COF (Figure 6H).The peaks are initially assigned to facets including (020): 'TAPPy-TFPPy-COF displayed peaks at 5.00°, 6.90°, 10.08°, 15.24°, and 23.22°...assigned to the (110), ( 020), ( 220), (330), and (001) facets...'.A subsequent concluding sentence for this group of COFs states related peaks 'were attributed to the (100), ( 110), ( 220), (330), and (001) facets...'.The differing indices for the second peak ((020) vs. ( 100)) for the same data create ambiguity and undermine the reliability of the structural characterization from PXRD for this COF.</p>
<p>Figure 15: Gemini-2.5-Pro'sregenerated feedback on [59].</p>
<p>COF and they show how it does not produce such a good results, because of this Gemini claims that the reagent can actually be used because the authors used it in a figure.Contrary to o3, it does find a real mistake by noticing that one of the characterizations of a certain COF contained the wrong unit, making the measurement 10000 times smaller.</p>
<p>Figure 16, are feedbacks of o3 on [74].Here, it seems like many of o3's flagged issues arise from misinterpretation rather than true errors.Its EDX objection ignores that the authors corroborate composition changes with complementary measurements and that small weight-percent shifts demand statistical treatment.Regarding contact angle, o3 uses diiodomethane data instead of water, miscomputing W a .The supposed Owens-Wendt inconsistency stems from a misunderstanding that two liquids yield a unique solution per fluence, so the reported parallel trends do not imply mathematical error.Finally, the potential barrier formula comes from literature conventions, and Figure 11 presents a related quantity rather than misapplying the sign.</p>
<p>In Figure 17 we observe multiple errors where Gemini-2.5-Profails to identify visual inputs correctly; for instance, it claims that Tables I and II referenced in the discussion of Figures 7-9 are missing, even though those tables are present, indicating it has failed to identify the context.Additionally, in Figure 10, it misreads a log-scaled plot as linear and thereby hallucinates values.</p>
<p>D.3 Environmental Science</p>
<p>We selected [75] in environmental science, the paper conductsa comprehensive evaluation of human contributions to atmospheric CO 2 .In a data-driven analysis, [75] argues that human-induced CO 2 emissions are negligible compared to natural sources, thereby questioning the validity of the Greenhouse Gas Hypothesis.However, the original version of the paper contains a miscalculation in Section 3.1.2,"Measurement of Infrared Absorption of the Earth's Atmosphere."Specifically, the author incorrectly applies Equation (3) from the IPCC: F = 5.35 ln Ct C0 by using the base-10 logarithm (log 10 ) instead of the natural logarithm (ln), leading to erroneous numerical values.Regarding the miscalculation, in Figure 18, o3 correctly locates the target error: "log 10 instead of the natural log, reducing ∆F by ≈ 2.3× and yielding a spurious warming of 0.64-0.81• C for CO 2 doubling.</p>
<p>• Methods -SRIM simulation / Figure 2: Authors discuss ion penetration up to 1 µm for 4 keV Ar + , whereas SRIM and the plotted axis (0-1000 Å) show a stopping range below 20 nm. Conclusions about bulk (µm-scale) modification rest on an impossible depth estimate.</p>
<p>• Results -EDX (Figures 3 &amp; 4): Claim that irradiation changed elemental composition is based on C, O, N shifts under 2 wt %, well inside normal EDX error; no statistics provided, so the asserted chemical change is unsupported.</p>
<p>• Results -Contact angle / Work of adhesion (Eq.[1], Figures 7 &amp; 8): Reported Wa values do not satisfy Wa = γ ℓ (1 + cos θ); for example, θ = 62.1 • for water gives 106 mJ m −2 , but the table lists 74.2 mJ m −2 .All subsequent surface-energy conclusions use these erroneous numbers.</p>
<p>• Results -Surface free energy (Figure 9): Owens-Wendt requires two liquids to solve for γ d s and γ p s .With only two liquids the solution at each fluence is unique, yet the authors report independent trends for both components, indicating a mathematical inconsistency.</p>
<p>• Results -Electrical conductivity (Figure 10, text): Two mutually contradictory pristine conductivities are given (3.8 × 10 −7 S cm −1 vs. 1.1 × 10 −8 S cm −1 ).The inconsistency invalidates the reported enhancements due to irradiation.</p>
<p>• Results -Potential barrier calculation (Figure 11, Eq. [4]): Formula Wm = −4kBT /m sets an upper magnitude of 0.104 eV at 300 K, yet values up to 1.07 eV are reported and the sign is ignored; the dielectric analysis is therefore mathematically impossible.</p>
<p>Figure 16: o3's regenerated feedback on [74].</p>
<p>Correct use of ln gives 1.5-1.9• C," highlighting a mismatch between the scale of results and the correct calculation basis.However, its subsequent claim that this error "invalidates the assertion of negligible future warming" seems overstated.As the authors acknowledge, projected temperature increases remain modest.In short, the magnitude of these miscalculations is insufficient to overturn the paper's broader argument about limited warming.</p>
<p>On the other hand, Gemini2.5 fails to point out the specific error.</p>
<p>E Additional Details on SPOT</p>
<p>License &amp; Copyright SPOT comprises 83 manuscripts published across 28 venues (including arXiv).Of these, 62 (74.7 %) are openly accessible under a CC license; we publicly share our fully preprocessed versions via the Hugging Face Hub.The remaining 21 (25.3 %) are paywalled, so we do not redistribute them directly.Organizations with institutional access to Springer Nature or Elsevier can apply our preprocessing pipeline to generate their own versions.</p>
<p>Date To minimize contamination against parametric knowledge [25], we aim to include only papers published from 2024 onward.As Figure 19 shows, the bulk of our corpus dates to 2024, with ten papers from 2025 and three that originally appeared before 2023.Those three early manuscripts passed our automated filters because revisions were submitted after 2024; we retained them since their first error notices appeared in March 2024, minimizing any chance that models were exposed to the original withdrawal details during training.</p>
<p>Annotation We use human annotators in Section 2 during the benchmark creation process.Details on the annotator guideline are available in Figure 20, a sample image of the platform in Figure 21.</p>
<p>F Additional Details on Evaluation</p>
<p>Evaluation consists of two phases.In the first phase, the target LLM is prompted to identify potential errors in each paper using our "Generation Prompt."In the second phase, we employ GPT-4.1 to align and compare the model's candidates against the ground-truth annotations with our "Evaluation Prompt."In the remainder of this section, we specify details on generation configurations and present the full text of both prompts.</p>
<p>• Figure 2a: The text states SRIM simulation data is shown for depths of 0 to 1 µm, but the x-axis of Figure 2a ("Target Depth") only extends to 1000 Å (0.1 µm).This misrepresents the range of simulation data presented visually.• Results and Discussion -Figure 10 and related text: AC conductivity σac values at 50 Hz reported in the text for irradiated samples differ significantly (e.g. for 4 × 10 14 ions•cm -2 : text 3.8 × 10 −7 S cm −1 vs. Fig. 10 data ∼ 3.74 × 10 −8 S cm −1 ) in a summary paragraph preceding the electrical characterization.This contradiction undermines the baseline characterization of the material.</p>
<p>• Results and Discussion -Electrical Properties: The AC conductivity σac for unirradiated (pristine) CA/PANI at 50 Hz is reported inconsistently: as 1.1 × 10 −8 S cm −1 in the Figure 10 discussion (matching the figure) and as 3.8 × 10 −7 S cm −1 in a preceding summary paragraph.</p>
<p>• Results and Discussion -Figure 11 and Equation [4]: The exponent m for calculating Wm is derived from a narrow low-frequency range in Figure 11 (ln ω ≈ 5.8-6.5),which is not justified and may not be representative of the model used.The validity of conclusions based on Wm is therefore questionable</p>
<p>• Results and Discussion -Equation [4]: The formula Wm = − 4kBT m used for calculating the maximum potential barrier is unconventional for hopping conduction models, and its scientific basis or appropriateness for this system is not established via the provided citation [40] or derivation.Using an potentially incorrect or unsubstantiated formula invalidates the calculated Wm values.</p>
<p>• Results and Discussion -Calculation of Wm: The text states that the estimated Wm "decreased from 0.116 eV. . . to 1.03 eV."However, the authors' reported values (0.116, 0.11, 1.07, 1.03) eV show a non-monotonic behavior (slight decrease, then large increase, then slight decrease).This is a misrepresentation of their calculated results.</p>
<p>Figure 17: Gemini-2.5-Pro'sregenerated feedback on [74].</p>
<p>F.1 Generation Configurations</p>
<p>For each model, we adopt the provider's recommended parameters when available; otherwise, we use a sampling temperature of 0.6, top-p of 0.95, a repetition penalty of 1.0, and enforce a minimum of 8 and a maximum of 8192 tokens.</p>
<p>• Section 3.1.1/ Table 1: Temperature "predictions" obtained from T = P M ρR are tautological because ρ = P M RT by definition; using measured P , M , and ρ always reproduces the measured T .This provides no evidence against the greenhouse effect yet is used as the cornerstone of the paper's conclusions.</p>
<p>• Section 3.1.2(Equations 3-4 and Figure 1): The forcing equation is applied with log 10 instead of the natural log, reducing ∆F by ≈ 2.3× and yielding a spurious warming of 0.64-0.81• C for CO2 doubling.Correct use of ln gives 1.5-1.9• C, invalidating the claim that future warming is negligible.</p>
<p>• Section 3.1.2(Climate-sensitivity discussion): Derived ECS ≈ 0.5 • C ignores water-vapour, lapserate, and cloud feedbacks without justification; hence the stated upper bound on warming lacks the physical components known to amplify CO2 forcing.</p>
<p>• Section 3.1.4(Residence time of CO2): Paper conflates 3-5 yr molecular turnover time with the multi-century adjustment time of a concentration perturbation, leading to the unsupported claim that anthropogenic CO2 'quickly disappears' and is climatically irrelevant.</p>
<p>• Section 3.1.4(Anthropogenic share argument): Using the fact that anthropogenic emissions are ∼ 4% of gross annual flux to assert they cannot raise atmospheric CO2 ignores that natural sources and sinks are nearly balanced; failure to perform a mass-balance analysis renders the conclusion scientifically invalid.</p>
<p>• Section 3.1 (General claim of "no greenhouse effect"): Assertion that adiabatic compression or thermal conductivity alone explains the 33 K surface-space temperature difference disregards radiativetransfer energy balance and provides no quantitative mechanism, contradicting well-established physics without offering an alternative that reproduces observed fluxes.A lightweight Streamlit app for labeling errors discussed on PubPeer or papers withdrawn from arXiv.Contributors review randomly selected papers, answer guided questions, and append their work to annotations.csv.</p>
<p>Getting Started Prerequisites</p>
<p>• Python ≥ 3.</p>
<p>Error Category Paper Category</p>
<p>Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency 0.4 1.7 0.9          13.0 6.9</p>
<p>Table 19: Mean and standard deviation of pass@K for Claude-3.7-Sonnet(K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table 3.</p>
<p>Error Category Paper Category</p>
<p>Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency 6.3 Table 25: Mean and standard deviation of pass@K for Llama-4-Maverick (K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table 3.</p>
<p>Error Category Paper Category</p>
<p>Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency 0.0 0.0 0.0 0.0 0.0 0.0 Biology 6.7  and paper category (right).Detailed evaluation results for text-only evaluation of Table 3.</p>
<p>Error Category Paper Category</p>
<p>Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency 6.9</p>
<p>Figure 1 :
1
Figure 1: Overview of SPOT.Green indicates benchmark construction process, from seed collection through validation to normalization; blue indicates evaluation, where LLM outputs are compared to ground-truth errors and classified as true positives, false positives, or false negatives.</p>
<p>Figure 3 :
3
Figure 3: Performance of o3 and Llama-4-Maverick across six challenging STEM benchmarks.The short red horizontal lines mark the gap ∆ = o3 − Llama-4-Maverick for each benchmark.</p>
<p>Figure 6 :
6
Figure6: Gemini-2.5-Pro'sfeedback on[58].</p>
<p>Figure 7 :
7
Figure 7: o3's feedback on [59].</p>
<p>For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.</p>
<p>B. 1 3 GFigure 9 :
139
Figure 9: Impact of context length on error detection.Each bar shows ∆ = (segment-only -fullpaper) for precision and recall across five models (o3, Gemini 2.5 Pro, Claude 3.7-Sonnet:Thinking, Qwen 2.5-VL-72B-Instruct, Llama-4-Maverick).</p>
<p>Figure 10 :
10
Figure 10: Performance of o4-mini with varying reasoning effort.Performance is reported from three independent trials.</p>
<p>Figure 11 :• Section 2 . 5 :
1125
Figure 11: o3's initial feedback on [73].</p>
<p>Figure 12 :
12
Figure12: Gemini-2.5-Pro'sinitial feedback on[73].</p>
<p>Figure 18 :
18
Figure 18: o3's regenerated feedback on [75].</p>
<p>Figure 19 :
19
Figure 19: Publication dates against first error-notice dates for the 83 manuscripts.Each point denotes one paper; blue markers note papers published in 2024, while red markers are those otherwise.</p>
<p>Medicine 0.0 0.0 0.0 0.0 0.0 0.0 Multidisciplinary 0.0 0.0 0.0 0.0 0.0 0.0 Physics 0.0 0.0 0.0 0.0 0.0 0.0</p>
<p>Table 26 :
26
Mean and standard deviation of pass@K for Llama-4-Scout (K ∈ {1, 2, 4}) by error category (left)</p>
<p>Table 1 :
1
Overview of SPOT.Left: High-level statistics-83 manuscripts, 91 errors from 47 paper sources;</p>
<p>Table 2 :
2
Performance of ten models on the SPOT dataset.The Think column denotes the use of test-time scaling.Precision, Recall, pass@1 and pass@4 (all in %) are reported as mean and standard deviation (in parentheses) over eight independent trials.The highest value in each column is bolded, and the second-highest is underlined.Detailed evaluation results are available in Appendix G.
ModelsThink Precision (%) Recall (%) pass@1 (%) pass@4 (%)o3 (2025-04-16)✓6.1 1.321.1 4.418.4 2.137.8 1.8GPT-4.1 (2025-04-14)✗2.8 0.86.0 1.66.6 1.717.8 1.5Gemini-2.5-Pro (preview-03-25)✓3.1 1.710.1 5.67.8 3.825.9 4.0Gemini-2.0-Flash-Lite (001)✗1.0 0.81.6 1.31.5 1.06.0 1.5Claude-3.7-Sonnet (20250219:Think)✓3.0 1.36.0 2.45.5 1.718.6 2.8Claude-3.7-Sonnet (20250219)✗3.2 1.55.8 2.74.5 1.914.1 1.6Qwen2.5-VL-72B-Instruct✗0.6 1.20.4 0.70.4 0.61.7 1.0Qwen2.5-VL-32B-Instruct✗1.9 2.01.9 1.72.0 1.55.6 1.6Llama-4-Maverick✗2.0 2.60.9 1.20.9 1.03.3 1.2Llama-4-Scout✗0.8 1.01.9 2.31.8 2.07.2 3.1report Precision and Recall:</p>
<p>Table 2
2
compares ten multi-modal LLMs on SPOT.o3 achieves the highest scores, with 6.1% ± 1.3 precision, 21.1% ± 4.4 recall, and a 37.8% pass@4.It is followed by Gemini-2.5-Pro(3.1%, 10.1%, 25.9%), Claude-3.7-Sonnet:Thinking(3.0%, 6.0%, 18.6%), and GPT-4.1 (2.8%, 6.0%, 17.8%).The lighter proprietary variants, Gemini-2.0-Flash-Liteand the non-Thinking Claude-3.7-Sonnet,score marginally above zero.Surprisingly, open-source models such as Qwen2.5-VL-72B-Instruct</p>
<p>Table 3 :
3
Multi-modality ablation for 13 models: recall and pass@4 (in %) are reported as mean (std) over eight independent trials.The left panel shows each model's performance with multi-modal inputs; the right panel shows performance on the text-only subset of SPOT (48 figure-independent instances), including additional unimodal LLMs (DeepSeek-R1, DeepSeek-V3, Qwen3-235B-A22B).The highest value in each column is bolded, and the second-highest is underlined.Detailed evaluation results are available in Appendix G.
Multi-ModalText-Only</p>
<p>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>
<p>• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [NA] Justification: Our experiments are conducted entirely through APIs.We use the official provider for each model if available; if not, we use OpenRouter 6 .Total API expenditures amount to approximately $5,000.Guidelines: • The answer NA means that the paper does not include experiments.• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).10.Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
Answer: [Yes]Justification:Guidelines:•
Answer: [NA] Justification: We don't see negative societal impact in this work.</p>
<p>•</p>
<p>Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.•Werecognizethat the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.•Forinitial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.16.Declaration of LLM usageQuestion: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
Answer: [No]Justification: LLM services such as ChatGPT are used to draft and edit the manuscript andto generate code for visualizations.Guidelines:• The answer NA means that the core method development in this research does notinvolve LLMs as any important, original, or non-standard components.• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)</p>
<p>•</p>
<p>Results and Discussion -Contact Angle, Work of Adhesion, Surface Free Energy (Figures 7, 8, 9): Table I (containing contact angle and work of adhesion data) and TableII(containing surface free energy data) are referenced but not provided.This prevents independent verification of the data plotted in Figures7-9and the calculations derived from them, undermining the scientific validity of these results.•ResultsandDiscussion-Work of Adhesion (Figure8): The work of adhesion Wa values reported in the text (and presumably plotted in Figure8) are inconsistent with calculations based on contact angle data from Figure7and standard liquid surface tension values (e.g., water at 4 × 10 14 ions/cm 2 : reported Wa = 74.2mJ/m 2 , calculated Wa ≈ 106.4 mJ/m 2 ) .The liquid surface tension values ( γ ℓ ) used for calculations are not stated, and the discrepancy suggests significant errors in calculation or data, invalidating the reported Wa values.</p>
<p>Table 4 :
4
Shuffle Sample loads a random paper.2.Complete the six annotation questions in the right panel.3.Click Save Annotation to append to annotations.csv.4.Repeat until 3-5 rows are completed.5.Click Submit to send annotations.csvtothemaintainer.You will receive the parsed contents of a research paper.Your job is to identify only those errors or flaws that directly undermine the scientific validity of the paper's methods, analyses, or conclusions.Your sole focus is identifying flaws, such as errors in experimental design, data integrity, calculations, statistical inference, or reproducibility, that directly call into question the validity of a specific claim, paragraph, or the paper.Do not report issues purely presentational, rhetorical, stylistic, or related to citation .In Table4to 13 we present detailed results of each model from Table2.2.In Table14to 26 we present detailed results of the text-only evaluation from Table3.Mean and standard deviation of pass@K for o3 (K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
8• Streamlit• pandasInstallation# 1 Clone the repogit clone https://github.com/guijinSON/ai4s_r2.gitcd ai4s_r2# 2 Install dependenciespip install streamlit pandasDatasetretracted_machine_filtered_final.csv ships with the repository-no additionaldownload required.Usagestreamlit run streamlit_sample.py1. Figure 20: Guideline provided to annotators.F.2 PromptsGeneration PromptYou are a scientific-rigor auditor. practices.-After you've done a detailed walkthrough of the paper, output exactly in this format-noextra keys or commentary:"'<analysis>{detailed walk-through of how you checked each section/figure and why you flagged (or didnot flag) any flaw}</analysis><response>{"has_error": <true | false>,"errors": [{"location": "Section 2.1",
1</p>
<p>Table 5 :
5
Mean and standard deviation of pass@K for GPT-4.1 (K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency6.4 5.611.4 6.619.2 6.1 Biology21.4 6.534.8 7.548.9 7.3Equation / proof0.4 1.00.8 1.31.5 1.5Chemistry2.1 5.54.0 7.18.2 8.3Experiment setup0.0 0.00.0 0.00.0 0.0Computer Science0.0 0.00.0 0.00.0 0.0Figure duplication16.3 4.927.6 5.741.1 5.6 Engineering12.9 21.923.1 24.939.3 20.5Reagent identity4.5 11.48.6 14.616.5 16.7 Environmental Science0.0 0.00.0 0.00.0 0.0Statistical reporting0.0 0.00.0 0.00.0 0.0Materials Science14.6 9.826.8 11.341.5 9.4Mathematics0.0 0.00.0 0.00.0 0.0Medicine5.9 16.111.3 20.925.2 25.0Multidisciplinary12.7 8.422.9 8.736.4 7.4Physics0.0 0.00.0 0.00.0 0.0</p>
<p>Table 6 :
6
Mean and standard deviation of pass@K for Gemini-2.5-Pro(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency12.6 7.022.3 7.636.5 6.9 Biology2.0 3.43.9 4.47.6 5.1Equation / proof11.8 7.021.9 7.938.4 7.3 Chemistry8.6 8.315.5 9.525.9 8.7Experiment setup0.0 0.00.0 0.00.0 0.0Computer Science8.2 7.116.7 9.731.7 11.5Figure duplication1.5 2.62.8 3.45.5 3.9Engineering0.0 0.00.0 0.00.0 0.0Reagent identity0.0 0.00.0 0.00.0 0.0Environmental Science8.3 14.415.6 16.626.4 13.6Statistical reporting 15.2 24.331.6 31.557.7 32.0 Materials Science3.9 7.17.5 8.313.0 6.9Mathematics11.3 8.521.1 9.738.0 8.6Medicine0.0 0.00.0 0.00.0 0.0Multidisciplinary11.2 5.920.3 7.732.8 8.8Physics14.6 14.525.7 14.939.7 12.8</p>
<p>Table 7 :
7
Mean and standard deviation of pass@K for Gemini-2.0-Flash-Lite-001(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
Error CategoryPaper CategoryCategorypass@1 pass@2pass@4Categorypass@1 pass@2pass@4Data Inconsistency1.8 3.13.5 4.07.0 4.7Biology3.7 3.87.6 4.915.4 5.7Equation / proof0.0 0.00.0 0.00.0 0.0Chemistry2.1 5.54.2 7.28.1 8.3Experiment setup0.0 0.00.0 0.00.0 0.0Computer Science0.0 0.00.0 0.00.0 0.0Figure duplication3.2 2.96.3 3.712.9 4.2 Engineering0.0 0.00.0 0.00.0 0.0Reagent identity3.9 10.78.7 14.716.9 16.7 Environmental Science0.0 0.00.0 0.00.0 0.0Statistical reporting0.0 0.00.0 0.00.0 0.0Materials Science2.1 5.54.1 7.28.3 8.3Mathematics0.0 0.00.0 0.00.0 0.0Medicine3.1 8.26.6 11.012.6 12.5Multidisciplinary3.7 4.87.2 6.314.9 7.4Physics0.0 0.00.0 0.00.0 0.0</p>
<p>Table 8 :
8
Mean and standard deviation of pass@K for Claude-3.7-Sonnet:Thinking(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency8.0 4.215.7 5.829.3 6.9 Biology13.4 7.323.9 9.941.2 10.6Equation / proof0.8 1.31.6 1.83.0 2.0Chemistry2.1 5.54.1 7.28.3 8.3Experiment setup0.0 0.00.0 0.00.0 0.0Computer Science0.0 0.00.0 0.00.0 0.0Figure duplication10.6 3.519.7 4.934.9 6.0 Engineering6.2 16.512.0 21.424.8 25.0Reagent identity3.9 10.87.5 13.916.8 16.7 Environmental Science 16.4 23.233.1 29.859.8 29.3Statistical reporting3.1 8.36.1 10.712.5 12.5 Materials Science10.4 11.420.8 14.338.4 14.5Mathematics0.6 1.61.3 2.22.5 2.5Medicine6.2 10.812.1 14.224.9 16.4Multidisciplinary10.0 10.019.2 13.235.8 14.9Physics0.0 0.00.0 0.00.0 0.0</p>
<p>Table 9 :
9
Mean and standard deviation of pass@K for Claude-3.7-Sonnet(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency7.0 6.213.6 7.725.4 8.1 Biology10.6 5.317.8 5.527.3 5.0Equation / proof0.4 1.00.7 1.31.5 1.5Chemistry4.2 7.28.2 9.316.4 10.9Experiment setup0.0 0.00.0 0.00.0 0.0Computer Science0.0 0.00.0 0.00.0 0.0Figure duplication8.8 4.815.5 4.825.1 4.0 Engineering0.0 0.00.0 0.00.0 0.0Reagent identity8.6 14.615.4 16.626.1 13.8 Environmental Science3.9 10.87.5 13.916.8 16.7Statistical reporting0.0 0.00.0 0.00.0 0.0Materials Science8.2 8.315.9 10.230.1 11.2Mathematics0.0 0.00.0 0.00.0 0.0Medicine3.4 8.56.4 10.912.8 12.5Multidisciplinary13.8 11.225.4 12.742.6 10.9Physics0.0 0.00.0 0.00.0 0.0</p>
<p>Table 10 :
10
Mean and standard deviation of pass@K for Qwen2.5-VL-72B-instruct(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.</p>
<p>Table 11 :
11
Mean and standard deviation of pass@K for Qwen2.5-VL-32B-instruct(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency0.9 2.41.8 3.13.5 3.6Biology9.7 8.516.4 8.3</p>
<p>Table 12 :
12
Mean and standard deviation of pass@K for Llama-4-Maverick (K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
Error CategoryPaper CategoryCategorypass@1 pass@2pass@4Categorypass@1 pass@2pass@4Data Inconsistency0.8 2.31.9 3.13.6 3.6Biology2.9 5.45.4 6.49.9 6.1Equation / proof0.0 0.00.0 0.00.0 0.0Chemistry2.2 5.74.3 7.3</p>
<p>Table 13 :
13
Mean and standard deviation of pass@K for Llama-4-Scout (K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluations results for Table2.
Error CategoryPaper CategoryCategorypass@1 pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency2.6 3.44.9 4.19.2 4.2Biology6.5 6.012.8 7.925.3 9.6Equation / proof0.0 0.00.0 0.00.0 0.0Chemistry6.1 8.011.5 9.521.4 9.7Experiment setup0.0 0.00.0 0.00.0 0.0Computer Science0.0 0.00.0 0.00.0 0.0Figure duplication3.6 4.97.0 6.414.0 7.6 Engineering6.2 16.512.2 21.524.9 25.0Reagent identity3.9 10.78.7 14.716.9 16.7 Environmental Science0.0 0.00.0 0.00.0 0.0Statistical reporting3.1 8.36.1 10.7</p>
<p>Table 14 :
14
Mean and standard deviation of pass@K for o3 (K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table3.Biology 6.2 16.5 13.1 22.0 25.1 25.0 Equation / proof 24.6 4.9 41.3 5.5 62.8 5.7 Computer Science 24.0 7.8 42.8 8.9 66.8 7.5 Experiment setup 6.7 17.0 12.9 21.9 24.8 25.0 Environmental Science 12.0 21.4 24.1 25.0 40.0 20.0 Reagent identity 8.3 14.4 17.1 19.0 33.0 21.7 Materials Science 0.0 0.0 0.0 0.0 0.0 0.0 Statistical reporting 24.8 17.8 44.1 18.8 66.1 12.4 Mathematics 21.8 5.5 37.7 7.9 58.9 8.4 Medicine 12.4 33.0 25.1 43.4 48.7 50.0 Multidisciplinary 33.3 0.0 41.7 14.5 49.6 16.7 Physics 27.4 14.4 46.0 14.5 67.6 10.5
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency14.6 5.525.2 8.338.0 9.7</p>
<p>Table 15 :
15
Mean and standard deviation of pass@K for GPT-4.1 (K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table3.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency8.6 14.615.7 16.626.1 13.8 Biology6.2 16.513.1 22.025.1 25.0Equation / proof6.0 3.410.8 3.618.1 3.1 Computer Science4.1 4.27.9 5.214.8 5.4Experiment setup0.0 0.00.0 0.00.0 0.0Environmental Science 19.6 35.636.4 42.264.6 40.1Reagent identity8.3 14.416.7 19.133.2 21.9 Materials Science0.0 0.00.0 0.00.0 0.0Statistical reporting 15.6 12.122.3 7.825.0 0.0 Mathematics4.3 3.98.0 4.313.5 3.8Medicine12.4 33.024.0 42.749.5 50.0Multidisciplinary45.9 16.271.3 17.492.2 14.1Physics0.0 0.00.0 0.00.0 0.0</p>
<p>Table 16 :
16
Mean and standard deviation of pass@K for Gemini-2.5-Pro(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table3.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency4.1 7.27.7 8.313.0 6.9 Biology0.0 0.00.0 0.00.0 0.0Equation / proof7.6 4.011.6 3.916.2 3.8 Computer Science4.2 5.98.2 7.514.8 8.5Experiment setup0.0 0.00.0 0.00.0 0.0Environmental Science 12.9 21.923.5 25.038.6 21.0Reagent identity8.3 14.415.4 16.626.1 13.8 Materials Science0.0 0.00.0 0.00.0 0.0Statistical reporting 12.1 12.519.6 10.324.7 2.8 Mathematics5.0 2.57.1 2.59.0 2.0Medicine24.8 43.246.3 49.978.2 41.3Multidisciplinary49.4 16.773.6 20.292.3 14.1Physics0.0 0.00.0 0.00.0 0.0</p>
<p>Table 17 :
17
Mean and standard deviation of pass@K for Gemini-2.0-Flash-Lite-001(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table3.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency2.1 5.54.0 7.18.2 8.3Biology12.9 21.925.4 28.649.9 32.9Equation / proof1.1 1.52.1 1.73.9 1.8Computer Science2.1 3.63.8 4.26.5 3.5Experiment setup0.0 0.00.0 0.00.0 0.0Environmental Science0.0 0.00.0 0.00.0 0.0Reagent identity12.5 16.125.7 20.950.1 24.2 Materials Science0.0 0.00.0 0.00.0 0.0Statistical reporting3.4 8.56.5 10.912.4 12.5 Mathematics0.6 1.61.1 2.12.5 2.5Medicine11.7 32.226.2 44.050.7 50.0Multidisciplinary8.6 14.616.6 19.333.0 22.2Physics0.0 0.00.0 0.00.0 0.0</p>
<p>Table 18 :
18
Mean and standard deviation of pass@K for Claude-3.7-Sonnet:Thinking(K∈{1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table3.
Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency6.2 11.612.1 14.021.1 13.3 Biology0.0 0.00.0 0.00.0 0.0Equation / proof4.6 3.78.4 4.615.1 4.9 Computer Science7.2 6.512.4 7.820.7 8.9Experiment setup0.0 0.00.0 0.00.0 0.0Environmental Science 12.4 21.623.8 25.039.1 20.7Reagent identity0.0 0.00.0 0.00.0 0.0Materials Science6.2 16.512.6 21.724.3 25.0Statistical reporting 18.4 20.631.2 19.944.3 11.6 Mathematics4.5 3.98.0 4.313.6 3.7Medicine0.0 0.00.0 0.00.0 0.0Multidisciplinary8.0 14.315.8 16.726.4 13.5Physics4.2 7.27.7 8.3</p>
<p>Table 20 :
20
Mean and standard deviation of pass@K for DeepSeek-R1 (K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table3.
8.111.6 10.121.5 10.0 Biology6.2 16.512.0 21.424.8 25.0Equation / proof4.9 2.18.9 3.015.2 3.4 Computer Science6.3 5.511.8 6.921.3 7.0Experiment setup0.0 0.00.0 0.00.0 0.0Environmental Science 18.4 24.134.4 29.363.4 29.3Reagent identity4.1 11.08.0 14.216.5 16.7 Materials Science0.0 0.00.0 0.00.0 0.0Statistical reporting3.4 8.56.5 10.912.4 12.5 Mathematics3.8 4.26.4 4.29.9 3.3Medicine0.0 0.00.0 0.00.0 0.0Multidisciplinary8.9 14.817.2 18.933.5 21.2Physics0.0 0.00.0 0.00.0 0.0Error CategoryPaper CategoryCategorypass@1pass@2pass@4Categorypass@1pass@2pass@4Data Inconsistency12.1 11.720.5 11.530.2 6.5 Biology7.4 17.814.3 22.628.9 24.7Equation / proof16.1 5.527.8 6.041.5 4.3 Computer Science13.0 9.723.8 10.339.1 7.7Experiment setup0.0 0.00.0 0.00.0 0.0Environmental Science 15.7 23.226.9 24.941.9 18.5Reagent identity4.9 11.89.5 15.119.3 16.5 Materials Science0.0 0.00.0 0.00.0 0.0Statistical reporting 28.3 15.743.3 19.861.3 18.1 Mathematics15.9 4.126.9 5.239.7 5.0Medicine0.0 0.00.0 0.00.0 0.0Multidisciplinary38.4 20.855.9 15.665.8 5.3Physics16.3 18.028.1 17.342.2 9.7
https://pubpeer.com/
Using version gpt-4o-2024-08-06
https://www.llamaindex.ai/llamaparse
.1 is used to compare predicted error descriptions against benchmark annotations as a similarity check[41]; the LLM does not evaluate the errors' correctness or severity.
Due to space constraints, we show only excerpts of model responses and one case study per domain; for the complete results, see Appendix D.
https://openrouter.ai/
While recent work has demonstrated similar budget controlling strategies for open models[71], the fullsize MLLMs (Llama-4-Maverick totaling 402B parameters) were too large to host for multi-thousand-token generations.
Evaluation PromptYou are an expert LLM-as-a-Judge.You will receive a JSON object with two arrays:1. "annotations": the ground-truth errors (each has "location" and "description").2. "predictions": the model's reported errors (same format).Task 1. Compare each prediction against each annotation.2. A match occurs only when both "location" and "description" are identical.3.Your output should be generated in the following format: <analysis> Analysis and comparison of each prediction and annotation.</analysis> <response> { "matches": [ { "location": the location of the matched object, which should be based on the annotated location, "description": your explanation on why you think it is a match.}, { "location": ... , "description": ... }, ] } </response> Be rigorous in considering matches; the location may be slightly differently named, but the description must match overall.Table23: Mean and standard deviation of pass@K for Qwen2.5-VL-72B-Instruct(K ∈ {1, 2, 4}) by error category (left) and paper category (right).Detailed evaluation results for text-only evaluation of Table3.Error Category Paper CategoryCategory pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency 18.93.Error Category Paper CategoryCategory pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency 4.6 7.4 8.9
Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>R-bench: Graduate-level multi-disciplinary benchmarks for llm &amp; mllm complex reasoning evaluation. Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, arXiv:2505.020182025arXiv preprint</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, First Conference on Language Modeling. 2024</p>
<p>Kaiyue Feng, Yilun Zhao, Yixin Liu, Tianyu Yang, Chen Zhao, John Sous, Arman Cohan, arXiv:2503.21821Physics: Benchmarking foundation models on university-level physics problem solving. 2025arXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Can chatgpt be used to generate scientific hypotheses. Yang Jeong, Park , Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, Ju Li, Journal of Materiomics. 1032024</p>
<p>Pasa: An llm agent for comprehensive academic paper search. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, arXiv:2501.101202025arXiv preprint</p>
<p>Generative ai in writing research papers: a new type of algorithmic bias and uncertainty in scholarly work. Rishab Jain, Aditya Jain, Intelligent Systems Conference. Springer2024</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Ai mirrors experimental science to uncover a novel mechanism of gene transfer crucial to bacterial evolution. Juraj José R Penadés, Lingchen Gottweis, He, B Jonasz, Alexander Patkowski, Wei-Hung Shurick, Tao Weng, Anil Tu, Artiom Palepu, Annalisa Myaskovsky, Pawlosky, bioRxiv. 2025</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, Nature Machine Intelligence. 652024</p>
<p>Quantum many-body physics calculations with large language models. Haining Pan, Nayantara Mudur, William Taranto, Maria Tikhanovskaya, Subhashini Venugopalan, Yasaman Bahri, Michael P Brenner, Eun-Ah Kim, Communications Physics. 81492025</p>
<p>Alphaevolve: a gemini-powered coding agent for designing advanced algorithms. Deepmind, 2025a-gemini-powered-coding-agent-for-designing-advanced-algorithms/</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang, Wang , arXiv:1909.02164Tabfact: A large-scale dataset for table-based fact verification. 2019arXiv preprint</p>
<p>A review on fact extraction and verification. Giannis Bekoulis, Christina Papagiannopoulou, Nikos Deligiannis, ACM Computing Surveys (CSUR). 5512021</p>
<p>Polyfever: A multilingual fact verification benchmark for hallucination detection in large language models. Hanzhi Zhang, Sumera Anjum, Weijian Heng Fan, Yan Zheng, Yunhe Huang, Feng, arXiv:2503.165412025arXiv preprint</p>
<p>Raúl Ortega, José Manuel Gómez-Pérez, arXiv:2503.18526Sciclaims: An end-to-end generative system for biomedical claim analysis. 2025arXiv preprint</p>
<p>Sujit Kumar, Anshul Sharma, Siddharth Hemant Khincha, Gargi Shroff, arXiv:2502.10003Sanasam Ranbir Singh, and Rahul Mishra. Sciclaimhunt: A large dataset for evidence-based scientific claim verification. 2025arXiv preprint</p>
<p>Corebench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, arXiv:2409.113632024arXiv preprint</p>
<p>Nlpeer: A unified resource for the computational study of peer review. Nils Dycke, Ilia Kuznetsov, Iryna Gurevych, arXiv:2211.066512022arXiv preprint</p>
<p>Peerqa: A scientific question answering dataset from peer reviews. Tim Baumgärtner, Ted Briscoe, Iryna Gurevych, arXiv:2502.136682025arXiv preprint</p>
<p>Make every example count: On the stability and utility of self-influence for learning from noisy NLP datasets. Irina Bejan, Artem Sokolov, Katja Filippova, 10.18653/v1/2023.emnlp-main.625Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>FEVER: a large-scale dataset for fact extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, 10.18653/v1/N18-1074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Marilyn Walker, Ji Heng, Amanda Stent, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational LinguisticsJune 20181</p>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, 10.18653/v1/2020.emnlp-main.609Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>Moprd: A multidisciplinary open peer review dataset. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi, Neural Computing and Applications. 35342023</p>
<p>Automatically evaluating the paper reviewing capability of large language models. Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim, arXiv:2502.170862025arXiv preprint</p>
<p>Openai o3 and o4-mini system card. Openai, April 2025</p>
<p>The llama 4 herd: The beginning of a new era of natively multimodal ai innovation. A I Meta, April 2025</p>
<p>Delip Rao, Jonathan Young, Thomas Dietterich, Chris Callison-Burch, arXiv:2412.03775Withdrarxiv: A large-scale dataset for retraction study. 2024arXiv preprint</p>
<dl>
<dt>Classification and analysis of pubpeer comments: How a web journal club is used. Luis José, Ortega, Journal of the Association for Information Science and Technology. 7352022</dt>
<dd>
<p>Openai, Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, Alex Aleksander M Ądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Tachard Renzin, Alexander Passos, Alexi Kirillov, Alexis Christakis, Ali Conneau, Allan Kamali, Allison Jabri, Allison Moyer, Amadou Tam, Amin Crookes, Amin Tootoochian, Ananya Tootoonchian, Andrea Kumar, Andrej Vallone, Andrew Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrey Tulloch, Angela Mishchenko, Angela Baek, Antoine Jiang, Antonia Pelisse, Anuj Woodford, Arka Gosalia, Ashley Dhar, Avi Pantuliano, Avital Nayak, Barret Oliver, Behrooz Zoph, Ben Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Benjamin Wang, Beth Zweig, Blake Hoover, Bob Samic, Bobby Mcgrew, Bogo Spero, Bowen Giertler, Brad Cheng, Brandon Lightcap, Brendan Walkin, Brian Quinn, Brian Guarraci, Bright Hsu, Brydon Kellogg, Camillo Eastman, Carroll Lugaresi, Cary Wainwright, Cary Bassin, Casey Hudson, Chad Chu, Chak Nelson, Chan Li, Channing Jun Shern, Charlotte Conger, Chelsea Barette, Chen Voss, Cheng Ding, Chong Lu, Chris Zhang, Chris Beaumont, Chris Hallacy, Christian Koch, Christina Gibson, Christine Kim, Christine Choi, Christopher Mcleavey, Claudia Hesse, Clemens Fischer, Coley Winter, Colin Czarnecki, Colin Jarvis, Constantin Wei, Dane Koumouzelis, Daniel Sherburn, Daniel Kappler, Daniel Levin, David Levy, David Carr, David Farhi, David Mely, David Robinson, Denny Sasaki, Dev Jin, Dimitris Valladares, Doug Tsipras, Li, Phong Duc, Duncan Nguyen, Edede Findlay, Edmund Oiwoh, Ehsan Wong, Elizabeth Asdar, Elizabeth Proehl, Eric Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eugene Wallace, Evan Brevdo, Farzad Mays, Felipe Petroski Khorasani, Filippo Such, Francis Raso, Fred Zhang, Freddie Von Lohmann, Gabriel Sulit, Gene Goh, Geoff Oden, Giulio Salmon, Greg Starace, Hadi Brockman, Haiming Salman, Haitang Bao, Hannah Hu, Haoyu Wong, Heather Wang, Heather Schmidt, Heewoo Whitney, Hendrik Jun, Henrique Kirchner, Hongyu Ponde De Oliveira Pinto, Huiwen Ren, Hyung Won Chang, Ian Chung, Kivlichan, O' Ian, Ian O' Connell, Ian Connell, Ian Osband, Ian Silber, Ibrahim Sohl, Ikai Okuyucu, Ilya Lan, Ilya Kostrikov, Ingmar Sutskever, Ishaan Kanitscheider, Jacob Gulrajani, Jacob Coxon, Jakub Menick, James Pachocki, James Aung, James Betker, James Crooks, Jamie Lennon, Jan Kiros, Jane Leike, Jason Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jay Wolfe, Jeff Chen, Jenia Harris, Jessica Varavva, Jessica Gan Lee, Ji Shieh, Jiahui Lin, Jiayi Yu, Jie Weng, Jieqi Tang, Joanne Yu, Joaquin Quinonero Jang, Joe Candela, Joe Beutler, Joel Landers, Johannes Parish, John Heidecke, Jonathan Schulman, Jonathan Lachman, Jonathan Mckay, Jonathan Uesato, Jong Wook Ward, Joost Kim, Jordan Huizinga, Jos Sitkin, Josh Kraaijeveld, Josh Gross, Josh Kaplan, Joshua Snyder, Joy Achiam, Joyce Jiao, Juntang Lee, ; Zhuang, Krithika Kiel Howe, Kyle Muthukumar, Lama Luther, Larry Ahmad, Lauren Kai, Lauren Itow, Leher Workman, Leo Pathak, Li Chen, Lia Jing, Liam Guy, Liang Fedus, Lien Zhou, Lilian Mamitsuka, Lindsay Weng, Lindsey Mccallum, Long Held, Louis Ouyang, Lu Feuvrier, Lukas Zhang, Lukasz Kondraciuk, Luke Kaiser, Luke Hewitt, Lyric Metz, Mada Doshi, Maddie Aflak, Madelaine Simens, Madeleine Boyd, Marat Thompson, Mark Dukhan, Mark Chen, Mark Gray, Marvin Hudnall, Marwan Zhang, Mateusz Aljubeh, Matthew Litwin, Max Zeng, Maya Johnson, Mayank Shetty, Meghan Gupta, Mehmet Shah, Meng Jia Yatbaz, Mengchao Yang, Mia Zhong, Mianna Glaese, Michael Chen, Michael Janner, Michael Lampe, Michael Petrov, Michele Wu, Michelle Wang, Michelle Fradin, Miguel Pokrass, Rory Castro, Rowan Carmichael, Roy Zellers, Ruby Chen, Ruslan Chen, Ryan Nigmatullin, Saachi Cheu, Sam Jain, Sam Altman, Sam Schoenholz, Samuel Toizer, Sandhini Miserendino, Sara Agarwal, Scott Culver, Scott Ethersmith, Sean Gray, Sean Grove, Shamez Metzger, Shantanu Hermani, Shengjia Jain, Sherwin Zhao, Shino Wu, Shirong Jomoto, Wu, Shuaiqi, Sonia Xia, Spencer Phene, Srinivas Papay, Narayanan ; Veit, Vinnie Moeller, Vishal Monaco, Vlad Kuo, Wayne Fomenko, Weiyi Chang, Wenda Zheng, Wesam Zhou, Will Manassra, Wojciech Sheu, Yash Zaremba, Yilei Patil, Yongjik Qian, Youlong Kim, Yu Cheng, Yuchen Zhang, Yuchen He, Yujia Zhang, Yunxing Jin, Yury Dai, Malkov, Gpt-4o system card. Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan Lafontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul Mcmillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Pavlov, Miles Brundage, Miles Wang; Steve Coffey; Steve Lee; Stewart Hall; Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce WaltersMikhail2024Miguel Oom Temudo de Castro ; Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal PatwardhanTyna Eloundou, Valerie Qi,</p>
</dd>
</dl>
<p>American Invitational Mathematics Examination -AIME. American Invitational Mathematics Examination -AIME 2024. February 2024. February 2024MAA</p>
<p>Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, Martin Vechev, arXiv:2503.21934Proof or bluff? evaluating llms on 2025 usa math olympiad. 2025arXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>Paper2code: Automating code generation from scientific papers in machine learning. Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang, arXiv:2504.171922025arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>GitHub repository; version 0. Openai, Tiktoken, A fast bpe tokeniser for use with openai's models. 2025. Feb. 14, 2025. May 12, 20259</p>
<p>Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, Yang You, arXiv:2406.06565Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. 2024arXiv preprint</p>
<p>Spoc: Search-based pseudocode to code. Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, Percy S Liang, Advances in Neural Information Processing Systems. 322019</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>. Google Cloud, 2025</p>
<p>Google Cloud, Gemini 2.0 Flash Lite. 2025. May 15, 2025</p>
<p>Anthropic, Claude, Sonnet System Card. May 15, 2025</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, arXiv:2502.13923Jun Tang, et al. Qwen2. 5-vl technical report. 2025arXiv preprint</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, Mathvista, arXiv:2310.02255Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>Humanity's last exam. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo, Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, arXiv:2501.142492025arXiv preprint</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, Jasper Snoek, Advances in neural information processing systems. 201932</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang ; Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren,2025</p>
<p>Multiplicative chow-künneth decomposition and homology splitting of configuration spaces. Dan Petersen, Orsola Tommasi, 2024</p>
<p>Superacid in situ protected synthesis of covalent organic frameworks. Xingyao Ye, Ruoyang Liu, Xinyu Mu, Shanshan Tao, Hao Yang, Xuejiao J Gao, Shuo-Wang Yang, Donglin Jiang, Journal of the American Chemical Society. 2025</p>
<p>Llm-as-a-judge &amp; reward model: What they can and cannot do. Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong, arXiv:2409.112392024arXiv preprint</p>
<p>Discourse-based objectives for fast unsupervised sentence representation learning. Yacine Jernite, David Samuel R Bowman, Sontag, arXiv:1705.005572017arXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Reviewer2: Optimizing review generation through prompt generation. Zhaolin Gao, Kianté Brantley, Thorsten Joachims, arXiv:2402.108862024arXiv preprint</p>
<p>Scientific opinion summarization: Paper meta-review generation dataset, methods, and evaluation. Qi Zeng, Mankeerat Sidhu, Ansel Blume, Pong Hou, Lu Chan, Heng Wang, Ji, Artificial Intelligence for Research and Democracy: First International Workshop, AI4Research 2024, and 4th International Workshop, DemocrAI 2024, Held in Conjunction with IJCAI 2024. Jeju, South KoreaSpringer NatureAugust 5, 2024. 202420</p>
<p>Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. Corinna Cortes, Neil D Lawrence, arXiv:2109.097742021arXiv preprint</p>
<p>A noise audit of the peer review of a scientific article: a wpom journal case study. Tomas Bonavia, Juan A Marin-Garcia, WPOM-Working Papers on Operations Management. 202314</p>
<p>Michelangelo: Long context evaluations beyond haystacks via latent structure queries. Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, arXiv:2409.126402024arXiv preprint</p>
<p>Andy L Jones, arXiv:2104.03113Scaling scaling laws with board games. 2021arXiv preprint</p>
<p>Linguistic generalizability of test-time scaling in mathematical reasoning. Guijin Son, Jiwoo Hong, Hyunwoo Ko, James Thorne, arXiv:2502.174072025arXiv preprint</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, arXiv:2408.033142024arXiv preprint</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Finding flawed fictions: Evaluating complex reasoning in language models via plot hole detection. Kabir Ahuja, Melanie Sclar, Yulia Tsvetkov, arXiv:2504.119002025arXiv preprint</p>
<p>Algebraic description of complex conjugation on cohomology of a smooth projective hypersurface. Jeehoon Park, Junyeong Park, Philsang Yoo, 2024</p>
<p>Impacts of low energy argon beam on enhancing the surface wettability and electrical performance of ca/pani films. Reem Altuijri, Atta, Abdeltwab, Abdelhamied, ECS Journal of Solid State Science and Technology. 134430172024</p>
<p>The scientific case against net zero: Falsifying the greenhouse gas hypothesis. Michael Simpson, 10.5539/jsd.v17n6p137Journal of Sustainable Development. 1762024</p>            </div>
        </div>

    </div>
</body>
</html>
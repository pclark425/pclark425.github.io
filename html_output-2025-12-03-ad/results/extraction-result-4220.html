<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4220 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4220</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4220</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-279999727</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.17580v1.pdf" target="_blank">Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The exponential growth of scientific literature challenges researchers extracting and synthesizing knowledge. Traditional search engines return many sources without direct, detailed answers, while general-purpose LLMs may offer concise responses that lack depth or omit current information. LLMs with search capabilities are also limited by context window, yielding short, incomplete answers. This paper introduces WISE (Workflow for Intelligent Scientific Knowledge Extraction), a system addressing these limits by using a structured workflow to extract, refine, and rank query-specific knowledge. WISE uses an LLM-powered, tree-based architecture to refine data, focusing on query-aligned, context-aware, and non-redundant information. Dynamic scoring and ranking prioritize unique contributions from each source, and adaptive stopping criteria minimize processing overhead. WISE delivers detailed, organized answers by systematically exploring and synthesizing knowledge from diverse sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces processed text by over 80% while achieving significantly higher recall over baselines like search engines and other LLM-based approaches. ROUGE and BLEU metrics reveal WISE's output is more unique than other systems, and a novel level-based metric shows it provides more in-depth information. We also explore how the WISE workflow can be adapted for diverse domains like drug discovery, material science, and social science, enabling efficient knowledge extraction and synthesis from unstructured scientific papers and web sources.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4220.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4220.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Workflow for Intelligent Scientific Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-based workflow that uses LLM-powered, query-specific content filtering, dynamic scoring of unique contributions, recursive exploration and LLM-based fusion to extract and consolidate domain-specific relationships (demonstrated for HBB gene → disease associations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WISE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>WISE is a multi-stage, tree-structured pipeline that (1) retrieves initial candidate sources via a similarity search Φ(q); (2) applies an LLM-driven filtering function Γ(q, C(s_i)) to extract query-relevant text segments per source; (3) computes unique-contribution scores K(s_i) = w_filtered(s_i) − w_overlap(s_i, K_l) and a combined normalized Score(s_i)=K(s_i)/log(1 + w_filtered(s_i) + |K_l|) to rank sources; (4) prunes using a threshold T and selects top-k branches to expand recursively; (5) merges selected filtered content into a knowledge container via an LLM-powered fusion function Λ. The system explicitly focuses on extracting relations/patterns (here: gene–disease associations) by prioritizing unique, non-redundant contributions and adaptively stopping when marginal gains fall below threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology / Biomedical (gene-disease association extraction); generalizable to drug discovery, materials science, social science</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Initial candidate set S0 = 24 sources (from HGNC); authors report starting with 34 initial sources but successfully extracted content from 24 (10 inaccessible). Final extraction produced 169 identified diseases.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Entity–relation extraction / discovery of relationships and patterns (gene → disease associations, including sub-variations and phenotype links). Not a physical quantitative law, but discovery of structured relational patterns from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples of extracted relationships include HBB-associated conditions such as Sickle Cell Disease, Hemoglobin C, Hemoglobin E, β-Thalassemia, Hemoglobin S/β-Thalassemia, Hemoglobin S Oman, Hemoglobin M, Malaria (listed as associated in their output), and many rare variants and sub-variations (WISE identified 169 diseases/phenotypes linked to HBB).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM-driven contextual filtering of raw source text to produce focused fragments (Γ), counting word overlap against an accumulating knowledge container, scoring unique contributions, recursive link-following across filtered content, and LLM-powered fusion (Λ) to consolidate results. No explicit equation parser was used; extraction relies on text mining and LLM semantic understanding of passages, plus structural parsing of sections/tags/links.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Comparative and human-in-the-loop validation: outputs compared against four baselines (Pure ChatGPT GPT-4o, ChatGPT with Search, Gemini, Google Search); recall computed relative to the union of all distinct diseases found by any system; ROUGE and BLEU used to evaluate uniqueness/overlap; a manual level-based analysis (levels 0–5) was performed by human annotators to assess depth of information.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Recall (vs. union of systems): WISE = 0.842; ChatGPT (GPT-4o) = 0.474; ChatGPT with Search = 0.368; Gemini = 0.105; Google Search = 0.158. WISE identified 169 diseases; average level-of-detail for WISE = 3.81 (on 0–5 scale). Filtering reduced content size by mean 80.14% (example: UniProt entry reduced from 8,249 to 355 words). ROUGE/BLEU: WISE showed lower overlap scores (authors report WISE produced more unique content), but exact ROUGE/BLEU numeric values are not listed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>No explicit 'validity rate' of extracted relations reported; proxy performance: recall = 84.2% (relative to the combined-union set of systems).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limited access to some sources (10/34 initially inaccessible due to paywalls or anti-bot measures); word-weighting limitations (simple TF-based deprioritization used; TF-IDF noted as future improvement); potential user overwhelm by very detailed outputs; selection of threshold T was empirical (value 20 used); lack of specified underlying LLM models and their sizes in current implementation; reliance on textual co-occurrence/overlap rather than structured numerical equation parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>WISE was directly compared to Pure ChatGPT (GPT-4o), ChatGPT with Search, Gemini, and traditional Google Search. WISE outperformed all baselines on recall (0.842 vs. 0.474/0.368/0.105/0.158 respectively), average level-of-detail (3.81 vs. 3.33/3.42/2.5/3.0), and produced more unique content per ROUGE/BLEU analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4220.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4220.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT using GPT-4o (pure, pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose LLM (GPT-4o) used in the paper as a baseline for extracting gene-disease mentions from the literature; used in its pure form (no web augmentation) to answer the query from its pretrained knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT (pure GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Standard GPT-4o model accessed via OpenAI API and used with default parameters; the model was given the user query directly and relied on pretrained parameters to produce disease/phenotype lists without explicit web retrieval or the WISE filtering pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (applied to gene-disease association query in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (relies on pretrained model rather than processing a specified set of external papers in this experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Relation extraction (identification of diseases/phenotypes linked to a gene) as returned from model knowledge; no explicit quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Produced disease identifications such as Sickle Cell Disease, Hemoglobin C, β-Thalassemia (specific per Table II where ChatGPT identified a subset of diseases).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Direct prompt-based querying of the pretrained LLM (no separate filtering or document parsing pipeline applied by the experimenters for this baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Comparison of ChatGPT output to the union of all systems' outputs; recall computed, and level-based manual scoring used to assess depth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Recall = 0.474 (vs. combined-union); average level-of-detail ≈ 3.33 (per Table / Figure in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported as a correctness percentage; recall 47.4% relative to the union set.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on pretrained cutoff and may miss recent/rare relations not in pretraining; produces concise answers that can lack depth; context-window practical limits noted (even large context windows effectively limit simultaneous source processing).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to WISE (which outperformed it on recall and depth).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4220.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4220.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT with Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-4o) augmented with web search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of ChatGPT (GPT-4o) augmented by web search capabilities, used as a baseline to compare retrieval-augmented LLM performance against WISE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT with Search</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GPT-4o model augmented with web search (OpenAI's ChatGPT Search integration) that can retrieve web documents and incorporate them into answers; in experiments it was run with default parameters and given the same query Q.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (with web search)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (applied to gene-disease association query)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not explicitly stated; functions by retrieving web results on demand rather than processing a fixed corpus of papers provided by experimenters.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Relation extraction (detection of diseases/phenotypes associated with HBB from web sources).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Identified a subset of diseases (ChatGPT with Search recall reported lower than pure ChatGPT in this experiment: 0.368).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Retrieval-augmented generation (RAG): perform web search, incorporate retrieved textual content into LLM responses; exact retrieval strategy and rank/pruning not detailed in paper (baseline configuration used default vendor settings).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Compared to union of system outputs and assessed via recall and level-based manual scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Recall = 0.368; average level-of-detail ≈ 3.42 (per Table/Figure in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported as a direct correctness fraction beyond recall.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Baseline was outperformed by WISE; authors note general limitations of LLMs with web search including limited effective context size, ranking by popularity/SEO (not content-unique contribution), and inability to adaptively prune redundant sources as WISE does.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to WISE, ChatGPT (pure), Gemini, and Google Search; WISE had substantially higher recall and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4220.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4220.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini (Google's large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's multimodal large language model cited and used as a baseline in experiments to compare disease extraction capabilities against WISE and other LLM-based systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A vendor-provided LLM (Google's Gemini family) integrated as a baseline; the authors input the query Q using default parameters to obtain disease/phenotype identifications for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (used here for gene-disease association retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (model produces outputs from internal parameters and possible web augmentation depending on vendor settings; the paper used it as a black-box baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Relation extraction (identification of gene-associated diseases/phenotypes).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Gemini identified only a small fraction of the union set (recall reported as 0.105).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompting the vendor LLM with the query; exact retrieval/fusion pipeline unspecified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Compared output against union of systems; recall and level-based depth scoring used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Recall = 0.105; average level-of-detail ≈ 2.5 (per Table/Figure).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported beyond recall.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Low recall in the given biomedical query; treated as a black-box baseline without internal pipeline control in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct baseline in experiments; performed worse than WISE on recall and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4220.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4220.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-adapted BERT-family model pre-trained on biomedical corpora and widely used for biomedical named-entity recognition and relation extraction tasks; cited in related work as a model that improved precision for biomedical IE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biobert: a pre-trained biomedical language representation model for biomedical text mining.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Transformer-based language model (BERT architecture) pre-trained on large-scale biomedical text to provide improved representations for biomedical NLP tasks (NER, relation extraction). In the paper, BioBERT is cited as an example of domain-adapted models that improve precision for biomedical relation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / BioNLP (named entity recognition, relation extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable in this paper (cited as prior art).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Relation extraction and structured biomedical relations (entity relations), not quantitative physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>No explicit law examples given in this paper; cited use-cases include improved precision on NER and relation extraction benchmarks in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Pretrained transformer fine-tuned on domain corpora to extract entities and relations from text (as referenced in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Cited prior work demonstrates gains in precision on benchmark tasks; no additional validation presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as part of prior work illustrating progress; limitations not detailed in this paper beyond general constraints of existing approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cited in related work as stronger than older feature-engineered models for biomedical IE; not used as an experimental baseline in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4220.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4220.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based genetic interaction extraction (Gill et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language model based framework for automated extraction of genetic interactions from unstructured data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (Gill et al.) that uses LLMs to automate extraction of genetic interactions from unstructured biomedical texts; cited as related work demonstrating LLM use to extract biological relationships from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language model based framework for automated extraction of genetic interactions from unstructured data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based genetic interaction extraction (Gill et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A prior framework that employs large language models to parse unstructured biomedical literature and extract genetic interaction relations; cited by the authors to place WISE in context of LLM-enabled biomedical relation extraction advances.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical (genetic interactions extraction from literature)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable in this paper (reference to a single prior study/framework).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Relation extraction (genetic interactions, entity relations), not quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not detailed in this paper; prior work focuses on extracting pairwise genetic interaction relations from text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM-driven processing of unstructured biomedical text to identify and structure interaction relationships (as described in citation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described in this paper; referenced as prior art demonstrating effectiveness on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in this paper (cited only).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Referenced as part of the literature demonstrating progress; specific limitations are in the cited work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cited to show trajectory from manual/feature-based methods to LLM-enabled extraction; not experimentally compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4220.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4220.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HybridRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach integrating knowledge graphs with vector-based retrieval-augmented generation (RAG) to improve efficiency and accuracy of information extraction; cited in related work as relevant to graph+LLM extraction strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HybridRAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines knowledge graph representations and vector retrieval with LLM generation to support efficient extraction and reasoning over information; paper cites HybridRAG as an approach that aligns with WISE's future directions (KG integration + RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Information extraction, knowledge representation; applicable to biomedical and other scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction and structuring of relations/patterns into graph form (knowledge graph edges), not explicit quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>No specific equations provided in this paper; approach intended to capture relations and support downstream reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Combine vector retrieval (semantic search) with knowledge graph augmentation and LLM generation/fusion to extract structured relations from unstructured text (as described in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not presented in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as promising direction; integration complexity and KG population challenges noted generally in discussion/future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cited as part of broader related-work landscape; no direct comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4220.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4220.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Buehler 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study suggesting the use of generative extraction and graph-based methods for accelerating scientific discovery; cited as complementary literature on generative knowledge extraction and graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generative knowledge extraction + graph reasoning (Buehler)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prior research advocating generative approaches (LLMs) for extracting knowledge, representing it as graphs, and performing multimodal graph reasoning to aid discovery; cited to motivate WISE's KG integration future work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning for scientific discovery; applicable across scientific domains (materials, biomedicine, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Discovery of structured relations and cross-modal patterns; potential for deriving empirical relationships when combined with structured data, but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>No explicit equations provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Generative LLM extraction combined with graph-based representations and multimodal reasoning (as per cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not given in this paper (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Referenced as a promising direction; practical integration, multimodal alignment, and KG population challenges discussed broadly in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cited as related work; no experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4220.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4220.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIX (LLM gene interaction extraction system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Named system referenced by the authors as an example of LLM-based automation for gene-interaction extraction; mentioned in related work as prior art that leverages LLMs to extract biological relationships from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GIX</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an LLM-enabled system for automating gene interaction extraction from literature; the current paper references GIX to illustrate existing LLM-based extraction systems in biomedical IE (no implementation details provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical relation extraction (gene interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable in this paper (GIX referenced as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Relation extraction (gene–gene or gene–interaction relations), not explicit quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Not specified in this paper (referenced only); presumably LLM-based text mining for relation extraction per the related-work description.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Referenced as part of prior advances; details and limitations are in the referenced work (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as prior work in biomedical LLM extraction literature; not experimentally compared in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language model based framework for automated extraction of genetic interactions from unstructured data <em>(Rating: 2)</em></li>
                <li>Biobert: a pre-trained biomedical language representation model for biomedical text mining. <em>(Rating: 2)</em></li>
                <li>Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction <em>(Rating: 2)</em></li>
                <li>Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4220",
    "paper_id": "paper-279999727",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "WISE",
            "name_full": "Workflow for Intelligent Scientific Extraction",
            "brief_description": "A tree-based workflow that uses LLM-powered, query-specific content filtering, dynamic scoring of unique contributions, recursive exploration and LLM-based fusion to extract and consolidate domain-specific relationships (demonstrated for HBB gene → disease associations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "WISE",
            "system_description": "WISE is a multi-stage, tree-structured pipeline that (1) retrieves initial candidate sources via a similarity search Φ(q); (2) applies an LLM-driven filtering function Γ(q, C(s_i)) to extract query-relevant text segments per source; (3) computes unique-contribution scores K(s_i) = w_filtered(s_i) − w_overlap(s_i, K_l) and a combined normalized Score(s_i)=K(s_i)/log(1 + w_filtered(s_i) + |K_l|) to rank sources; (4) prunes using a threshold T and selects top-k branches to expand recursively; (5) merges selected filtered content into a knowledge container via an LLM-powered fusion function Λ. The system explicitly focuses on extracting relations/patterns (here: gene–disease associations) by prioritizing unique, non-redundant contributions and adaptively stopping when marginal gains fall below threshold.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biology / Biomedical (gene-disease association extraction); generalizable to drug discovery, materials science, social science",
            "number_of_papers": "Initial candidate set S0 = 24 sources (from HGNC); authors report starting with 34 initial sources but successfully extracted content from 24 (10 inaccessible). Final extraction produced 169 identified diseases.",
            "law_type": "Entity–relation extraction / discovery of relationships and patterns (gene → disease associations, including sub-variations and phenotype links). Not a physical quantitative law, but discovery of structured relational patterns from literature.",
            "law_examples": "Examples of extracted relationships include HBB-associated conditions such as Sickle Cell Disease, Hemoglobin C, Hemoglobin E, β-Thalassemia, Hemoglobin S/β-Thalassemia, Hemoglobin S Oman, Hemoglobin M, Malaria (listed as associated in their output), and many rare variants and sub-variations (WISE identified 169 diseases/phenotypes linked to HBB).",
            "extraction_method": "LLM-driven contextual filtering of raw source text to produce focused fragments (Γ), counting word overlap against an accumulating knowledge container, scoring unique contributions, recursive link-following across filtered content, and LLM-powered fusion (Λ) to consolidate results. No explicit equation parser was used; extraction relies on text mining and LLM semantic understanding of passages, plus structural parsing of sections/tags/links.",
            "validation_approach": "Comparative and human-in-the-loop validation: outputs compared against four baselines (Pure ChatGPT GPT-4o, ChatGPT with Search, Gemini, Google Search); recall computed relative to the union of all distinct diseases found by any system; ROUGE and BLEU used to evaluate uniqueness/overlap; a manual level-based analysis (levels 0–5) was performed by human annotators to assess depth of information.",
            "performance_metrics": "Recall (vs. union of systems): WISE = 0.842; ChatGPT (GPT-4o) = 0.474; ChatGPT with Search = 0.368; Gemini = 0.105; Google Search = 0.158. WISE identified 169 diseases; average level-of-detail for WISE = 3.81 (on 0–5 scale). Filtering reduced content size by mean 80.14% (example: UniProt entry reduced from 8,249 to 355 words). ROUGE/BLEU: WISE showed lower overlap scores (authors report WISE produced more unique content), but exact ROUGE/BLEU numeric values are not listed in the paper.",
            "success_rate": "No explicit 'validity rate' of extracted relations reported; proxy performance: recall = 84.2% (relative to the combined-union set of systems).",
            "challenges_limitations": "Limited access to some sources (10/34 initially inaccessible due to paywalls or anti-bot measures); word-weighting limitations (simple TF-based deprioritization used; TF-IDF noted as future improvement); potential user overwhelm by very detailed outputs; selection of threshold T was empirical (value 20 used); lack of specified underlying LLM models and their sizes in current implementation; reliance on textual co-occurrence/overlap rather than structured numerical equation parsing.",
            "comparison_baseline": "WISE was directly compared to Pure ChatGPT (GPT-4o), ChatGPT with Search, Gemini, and traditional Google Search. WISE outperformed all baselines on recall (0.842 vs. 0.474/0.368/0.105/0.158 respectively), average level-of-detail (3.81 vs. 3.33/3.42/2.5/3.0), and produced more unique content per ROUGE/BLEU analyses.",
            "uuid": "e4220.0",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ChatGPT (GPT-4o)",
            "name_full": "ChatGPT using GPT-4o (pure, pretrained)",
            "brief_description": "A general-purpose LLM (GPT-4o) used in the paper as a baseline for extracting gene-disease mentions from the literature; used in its pure form (no web augmentation) to answer the query from its pretrained knowledge.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT (pure GPT-4o)",
            "system_description": "Standard GPT-4o model accessed via OpenAI API and used with default parameters; the model was given the user query directly and relied on pretrained parameters to produce disease/phenotype lists without explicit web retrieval or the WISE filtering pipeline.",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "Biomedicine (applied to gene-disease association query in experiments)",
            "number_of_papers": "Not applicable (relies on pretrained model rather than processing a specified set of external papers in this experiment).",
            "law_type": "Relation extraction (identification of diseases/phenotypes linked to a gene) as returned from model knowledge; no explicit quantitative laws.",
            "law_examples": "Produced disease identifications such as Sickle Cell Disease, Hemoglobin C, β-Thalassemia (specific per Table II where ChatGPT identified a subset of diseases).",
            "extraction_method": "Direct prompt-based querying of the pretrained LLM (no separate filtering or document parsing pipeline applied by the experimenters for this baseline).",
            "validation_approach": "Comparison of ChatGPT output to the union of all systems' outputs; recall computed, and level-based manual scoring used to assess depth.",
            "performance_metrics": "Recall = 0.474 (vs. combined-union); average level-of-detail ≈ 3.33 (per Table / Figure in paper).",
            "success_rate": "Not reported as a correctness percentage; recall 47.4% relative to the union set.",
            "challenges_limitations": "Relies on pretrained cutoff and may miss recent/rare relations not in pretraining; produces concise answers that can lack depth; context-window practical limits noted (even large context windows effectively limit simultaneous source processing).",
            "comparison_baseline": "Compared directly to WISE (which outperformed it on recall and depth).",
            "uuid": "e4220.1",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ChatGPT with Search",
            "name_full": "ChatGPT (GPT-4o) augmented with web search",
            "brief_description": "A version of ChatGPT (GPT-4o) augmented by web search capabilities, used as a baseline to compare retrieval-augmented LLM performance against WISE.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT with Search",
            "system_description": "GPT-4o model augmented with web search (OpenAI's ChatGPT Search integration) that can retrieve web documents and incorporate them into answers; in experiments it was run with default parameters and given the same query Q.",
            "model_name": "GPT-4o (with web search)",
            "model_size": null,
            "scientific_domain": "Biomedicine (applied to gene-disease association query)",
            "number_of_papers": "Not explicitly stated; functions by retrieving web results on demand rather than processing a fixed corpus of papers provided by experimenters.",
            "law_type": "Relation extraction (detection of diseases/phenotypes associated with HBB from web sources).",
            "law_examples": "Identified a subset of diseases (ChatGPT with Search recall reported lower than pure ChatGPT in this experiment: 0.368).",
            "extraction_method": "Retrieval-augmented generation (RAG): perform web search, incorporate retrieved textual content into LLM responses; exact retrieval strategy and rank/pruning not detailed in paper (baseline configuration used default vendor settings).",
            "validation_approach": "Compared to union of system outputs and assessed via recall and level-based manual scoring.",
            "performance_metrics": "Recall = 0.368; average level-of-detail ≈ 3.42 (per Table/Figure in paper).",
            "success_rate": "Not reported as a direct correctness fraction beyond recall.",
            "challenges_limitations": "Baseline was outperformed by WISE; authors note general limitations of LLMs with web search including limited effective context size, ranking by popularity/SEO (not content-unique contribution), and inability to adaptively prune redundant sources as WISE does.",
            "comparison_baseline": "Compared to WISE, ChatGPT (pure), Gemini, and Google Search; WISE had substantially higher recall and depth.",
            "uuid": "e4220.2",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Gemini",
            "name_full": "Gemini (Google's large language model)",
            "brief_description": "Google's multimodal large language model cited and used as a baseline in experiments to compare disease extraction capabilities against WISE and other LLM-based systems.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Gemini",
            "system_description": "A vendor-provided LLM (Google's Gemini family) integrated as a baseline; the authors input the query Q using default parameters to obtain disease/phenotype identifications for comparison.",
            "model_name": "Gemini",
            "model_size": null,
            "scientific_domain": "Biomedicine (used here for gene-disease association retrieval)",
            "number_of_papers": "Not applicable (model produces outputs from internal parameters and possible web augmentation depending on vendor settings; the paper used it as a black-box baseline).",
            "law_type": "Relation extraction (identification of gene-associated diseases/phenotypes).",
            "law_examples": "Gemini identified only a small fraction of the union set (recall reported as 0.105).",
            "extraction_method": "Prompting the vendor LLM with the query; exact retrieval/fusion pipeline unspecified in this paper.",
            "validation_approach": "Compared output against union of systems; recall and level-based depth scoring used.",
            "performance_metrics": "Recall = 0.105; average level-of-detail ≈ 2.5 (per Table/Figure).",
            "success_rate": "Not reported beyond recall.",
            "challenges_limitations": "Low recall in the given biomedical query; treated as a black-box baseline without internal pipeline control in this study.",
            "comparison_baseline": "Direct baseline in experiments; performed worse than WISE on recall and depth.",
            "uuid": "e4220.3",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BioBERT",
            "name_full": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "brief_description": "A domain-adapted BERT-family model pre-trained on biomedical corpora and widely used for biomedical named-entity recognition and relation extraction tasks; cited in related work as a model that improved precision for biomedical IE.",
            "citation_title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining.",
            "mention_or_use": "mention",
            "system_name": "BioBERT",
            "system_description": "Transformer-based language model (BERT architecture) pre-trained on large-scale biomedical text to provide improved representations for biomedical NLP tasks (NER, relation extraction). In the paper, BioBERT is cited as an example of domain-adapted models that improve precision for biomedical relation extraction.",
            "model_name": "BioBERT",
            "model_size": null,
            "scientific_domain": "Biomedical / BioNLP (named entity recognition, relation extraction)",
            "number_of_papers": "Not applicable in this paper (cited as prior art).",
            "law_type": "Relation extraction and structured biomedical relations (entity relations), not quantitative physical laws.",
            "law_examples": "No explicit law examples given in this paper; cited use-cases include improved precision on NER and relation extraction benchmarks in prior literature.",
            "extraction_method": "Pretrained transformer fine-tuned on domain corpora to extract entities and relations from text (as referenced in related work).",
            "validation_approach": "Cited prior work demonstrates gains in precision on benchmark tasks; no additional validation presented here.",
            "performance_metrics": "Not reported in this paper (reference only).",
            "success_rate": "Not reported here.",
            "challenges_limitations": "Mentioned as part of prior work illustrating progress; limitations not detailed in this paper beyond general constraints of existing approaches.",
            "comparison_baseline": "Cited in related work as stronger than older feature-engineered models for biomedical IE; not used as an experimental baseline in this study.",
            "uuid": "e4220.4",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLM-based genetic interaction extraction (Gill et al.)",
            "name_full": "Large language model based framework for automated extraction of genetic interactions from unstructured data",
            "brief_description": "Prior work (Gill et al.) that uses LLMs to automate extraction of genetic interactions from unstructured biomedical texts; cited as related work demonstrating LLM use to extract biological relationships from literature.",
            "citation_title": "Large language model based framework for automated extraction of genetic interactions from unstructured data",
            "mention_or_use": "mention",
            "system_name": "LLM-based genetic interaction extraction (Gill et al.)",
            "system_description": "A prior framework that employs large language models to parse unstructured biomedical literature and extract genetic interaction relations; cited by the authors to place WISE in context of LLM-enabled biomedical relation extraction advances.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedical (genetic interactions extraction from literature)",
            "number_of_papers": "Not applicable in this paper (reference to a single prior study/framework).",
            "law_type": "Relation extraction (genetic interactions, entity relations), not quantitative laws.",
            "law_examples": "Not detailed in this paper; prior work focuses on extracting pairwise genetic interaction relations from text corpora.",
            "extraction_method": "LLM-driven processing of unstructured biomedical text to identify and structure interaction relationships (as described in citation).",
            "validation_approach": "Not described in this paper; referenced as prior art demonstrating effectiveness on benchmarks.",
            "performance_metrics": "Not provided in this paper (cited only).",
            "success_rate": "Not reported here.",
            "challenges_limitations": "Referenced as part of the literature demonstrating progress; specific limitations are in the cited work (not detailed here).",
            "comparison_baseline": "Cited to show trajectory from manual/feature-based methods to LLM-enabled extraction; not experimentally compared in this paper.",
            "uuid": "e4220.5",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "HybridRAG",
            "name_full": "Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction",
            "brief_description": "A hybrid approach integrating knowledge graphs with vector-based retrieval-augmented generation (RAG) to improve efficiency and accuracy of information extraction; cited in related work as relevant to graph+LLM extraction strategies.",
            "citation_title": "Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction",
            "mention_or_use": "mention",
            "system_name": "HybridRAG",
            "system_description": "Combines knowledge graph representations and vector retrieval with LLM generation to support efficient extraction and reasoning over information; paper cites HybridRAG as an approach that aligns with WISE's future directions (KG integration + RAG).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Information extraction, knowledge representation; applicable to biomedical and other scientific domains.",
            "number_of_papers": "Not applicable in this paper (cited as related work).",
            "law_type": "Extraction and structuring of relations/patterns into graph form (knowledge graph edges), not explicit quantitative laws.",
            "law_examples": "No specific equations provided in this paper; approach intended to capture relations and support downstream reasoning.",
            "extraction_method": "Combine vector retrieval (semantic search) with knowledge graph augmentation and LLM generation/fusion to extract structured relations from unstructured text (as described in cited work).",
            "validation_approach": "Not presented in this paper (reference only).",
            "performance_metrics": "Not provided here.",
            "success_rate": "Not reported here.",
            "challenges_limitations": "Mentioned as promising direction; integration complexity and KG population challenges noted generally in discussion/future work.",
            "comparison_baseline": "Cited as part of broader related-work landscape; no direct comparisons in this paper.",
            "uuid": "e4220.6",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Buehler 2024",
            "name_full": "Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning",
            "brief_description": "A referenced study suggesting the use of generative extraction and graph-based methods for accelerating scientific discovery; cited as complementary literature on generative knowledge extraction and graph reasoning.",
            "citation_title": "Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning.",
            "mention_or_use": "mention",
            "system_name": "Generative knowledge extraction + graph reasoning (Buehler)",
            "system_description": "Prior research advocating generative approaches (LLMs) for extracting knowledge, representing it as graphs, and performing multimodal graph reasoning to aid discovery; cited to motivate WISE's KG integration future work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine learning for scientific discovery; applicable across scientific domains (materials, biomedicine, etc.).",
            "number_of_papers": "Not applicable in this paper (reference only).",
            "law_type": "Discovery of structured relations and cross-modal patterns; potential for deriving empirical relationships when combined with structured data, but not detailed here.",
            "law_examples": "No explicit equations provided in this paper.",
            "extraction_method": "Generative LLM extraction combined with graph-based representations and multimodal reasoning (as per cited paper).",
            "validation_approach": "Not given in this paper (citation only).",
            "performance_metrics": "Not provided here.",
            "success_rate": "Not reported in this paper.",
            "challenges_limitations": "Referenced as a promising direction; practical integration, multimodal alignment, and KG population challenges discussed broadly in the literature.",
            "comparison_baseline": "Cited as related work; no experimental comparison in this paper.",
            "uuid": "e4220.7",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GIX",
            "name_full": "GIX (LLM gene interaction extraction system)",
            "brief_description": "Named system referenced by the authors as an example of LLM-based automation for gene-interaction extraction; mentioned in related work as prior art that leverages LLMs to extract biological relationships from text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "GIX",
            "system_description": "Cited as an LLM-enabled system for automating gene interaction extraction from literature; the current paper references GIX to illustrate existing LLM-based extraction systems in biomedical IE (no implementation details provided in this paper).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedical relation extraction (gene interactions).",
            "number_of_papers": "Not applicable in this paper (GIX referenced as prior work).",
            "law_type": "Relation extraction (gene–gene or gene–interaction relations), not explicit quantitative laws.",
            "law_examples": "Not provided in this paper.",
            "extraction_method": "Not specified in this paper (referenced only); presumably LLM-based text mining for relation extraction per the related-work description.",
            "validation_approach": "Not described here.",
            "performance_metrics": "Not reported in this paper.",
            "success_rate": "Not reported here.",
            "challenges_limitations": "Referenced as part of prior advances; details and limitations are in the referenced work (not specified here).",
            "comparison_baseline": "Mentioned as prior work in biomedical LLM extraction literature; not experimentally compared in this study.",
            "uuid": "e4220.8",
            "source_info": {
                "paper_title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language model based framework for automated extraction of genetic interactions from unstructured data",
            "rating": 2,
            "sanitized_title": "large_language_model_based_framework_for_automated_extraction_of_genetic_interactions_from_unstructured_data"
        },
        {
            "paper_title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining.",
            "rating": 2,
            "sanitized_title": "biobert_a_pretrained_biomedical_language_representation_model_for_biomedical_text_mining"
        },
        {
            "paper_title": "Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction",
            "rating": 2,
            "sanitized_title": "hybridrag_integrating_knowledge_graphs_and_vector_retrieval_augmented_generation_for_efficient_information_extraction"
        },
        {
            "paper_title": "Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning.",
            "rating": 2,
            "sanitized_title": "accelerating_scientific_discovery_with_generative_knowledge_extraction_graphbased_representation_and_multimodal_intelligent_graph_reasoning"
        }
    ],
    "cost": 0.0180385,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models
21 Jun 2025</p>
<p>Yakin Sajratul 
Member, IEEE, HasanM Rubaiat 
Member, IEEEJamil 
Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models
21 Jun 2025E04A2330E5A940B2BBA57E3FCBF1C7F0arXiv:2506.17580v1[cs.IR]Knowledge DiscoveryLarge Language ModelsInformation FilteringScientific Data ExtractionKnowledge EnrichmentGene-Disease Associations
The exponential growth of scientific literature presents a significant challenge for researchers seeking to extract and synthesize relevant knowledge.Traditional search engines often return a large number of sources without directly providing detailed answers, while general-purpose Large Language Models (LLMs) may offer concise responses that lack depth or fail to incorporate the most up-to-date information.Furthermore, LLMs with search capabilities are often limited by their context window, resulting in short, incomplete answers.This paper introduces WISE (Workflow for Intelligent Scientific Knowledge Extraction), a novel system that addresses these limitations by combining LLMs with a structured, multi-layered workflow to extract, refine, and rank scientific knowledge tailored to specific queries.WISE employs an LLM-powered, tree-based architecture with a customized search function to iteratively refine extracted data, focusing on query-aligned and contextaware information while actively avoiding redundancy.Dynamic scoring and ranking mechanisms prioritize unique contributions from each source, and adaptive stopping criteria minimize processing overhead.WISE delivers detailed, well-organized, and highly informative answers by systematically exploring and synthesizing knowledge from diverse sources.Experiments focused on biological queries related to HBB gene-associated diseases demonstrate that WISE reduces the volume of processed text by over 80% while simultaneously achieving significantly higher recall compared to baseline methods, including leading search engines and other LLM-based approaches.Further analysis using ROUGE and BLEU metrics reveals that WISE's output is more unique compared to other systems, and a novel levelbased evaluation metric shows that WISE provides more in-depth information.This paper also explores how the WISE workflow can be adapted as a general framework for diverse research domains, such as drug discovery, material science, and social science, enabling efficient knowledge extraction and synthesis from unstructured scientific papers and web sources across a wide array of research domains.</p>
<p>I. INTRODUCTION</p>
<p>T HE relentless expansion of scientific knowledge, reflected in the ever-increasing volume of published literature, presents a formidable challenge for researchers seeking to extract, synthesize, and contextualize relevant information [1], [2].While traditional search engines and general-purpose S. Y. Rubaiat is with the Department of Computer Science, University of Idaho, Moscow, ID, USA (e-mail: ruba3062@vandals.uidaho.edu).</p>
<p>H. M. Jamil is with the Department of Computer Science, University of Idaho, Moscow, ID, USA (e-mail: jamil@uidaho.edu).</p>
<p>Manuscript received January 1, 2025; revised January 1, 2025.</p>
<p>Large Language Models (LLMs) offer some assistance, they often fall short in providing domain-specific insights, filtering irrelevant content, and efficiently managing the sheer volume of data [3]- [5].Traditional search engines typically return a large number of sources, requiring users to manually sift through them to extract relevant information, rather than providing direct, synthesized answers.Navigating this complex information ecosystem manually is both labor-intensive and error-prone, with researchers facing the risk of overlooking critical details as the volume of data grows exponentially.Consider, for example, a researcher investigating genedisease associations related to the HBB gene.Starting from a single authoritative source like the HGNC [6], they might encounter 24 relevant sources.Exploring just one of these, such as ClinVar [7], could unveil hundreds more sources, leading to a rapidly expanding tree of interconnected resources, exemplified by platforms like NCBI [8].This exponential growth of linked resources quickly overwhelms traditional search systems, making it difficult to extract pertinent insights efficiently.Even experienced investigators struggle to filter out superfluous data and focus on the most valuable information, a challenge amplified for newcomers.Consequently, critical information may be missed, and the time required to gain a complete, integrated understanding escalates dramatically.</p>
<p>Purely automated approaches also encounter significant difficulties in this context [9]- [11].The sheer volume of interconnected data can lead to computationally expensive and strategically ineffective processes without robust mechanisms for pruning irrelevant or redundant content.A key challenge lies in determining when to stop searching; continued exploration without clear stopping criteria often yields diminishing returns, underscoring the need for a balanced workflow that ensures thorough yet efficient exploration while prioritizing high-value information [12], [13].</p>
<p>While Large Language Models (LLMs) show promise in specific aspects of information retrieval [14], their inherent limitations hinder their ability to fully address the scale of these challenges.General-purpose LLMs, often provide concise answers that lack the depth and detail required for complex scientific queries, and may not incorporate the most recent findings.For instance, state-of-the-art models like GPT-4o [15], despite having a context window of 128000 tokens, are practically limited to processing data from only about eight sources simultaneously, such as UniProt [16], as illustrated in Figure 1.Although capable of ranking and comparing content within this limited scope, LLMs are constrained by their narrow context window, further reduced by factors like search history.In specialized domains such as biology and medicine, where nuanced and detailed insights are crucial, relying solely on LLM-based searches proves insufficient.These challenges highlight the critical need for combining the capabilities of LLMs with strategic workflows to achieve comprehensive and efficient knowledge retrieval.</p>
<p>To address these challenges, we introduce WISE (Workflow for Intelligent Scientific Extraction), a novel, scalable, treebased framework that integrates LLM-driven filtering, dynamic ranking, and adaptive stopping criteria.WISE is designed to deliver detailed, well-organized, and highly informative answers by systematically exploring and synthesizing knowledge from diverse sources.WISE mirrors the approach of a diligent researcher: identifying relevant information, discarding duplicates, exploring promising leads, and recognizing when further pursuit yields diminishing returns.WISE begins by employing LLMs to filter large text corpora based on a user's domain-specific query, ensuring that only contextually relevant and manageable segments proceed to subsequent stages.It then assigns scores to extracted sources, quantifying their unique contributions relative to previously processed material, thereby minimizing redundancy and focusing on content that adds genuine value.This dynamic ranking process prioritizes high-value information while pruning low-impact paths.The iterative refinement continues layer by layer, with WISE's knowledge container-the growing repository of extracted, query-specific insights-expanding until incremental findings diminish (Figure 4).At this point, WISE intelligently halts further searches, conserving computational resources while delivering comprehensive, contextually nuanced results.In essence, WISE achieves a balance between breadth and depth, effectively leveraging the strengths of LLMs while mitigating their limitations through intelligent pruning and carefully considered stopping criteria.</p>
<p>Our key contributions are summarized as follows:</p>
<p>1) Scalable, Tree-Based Architecture: We introduce a novel, tree-structured workflow (Section II) that efficiently navigates large, heterogeneous datasets.This architecture leverages LLM-based filtering at each layer to incrementally refine data subsets according to domainspecific queries, ensuring scalability and focus.</p>
<p>Number of Source Processed</p>
<p>Characters Words Tokens</p>
<p>Fig. 1.Number of sources, such as UniProt [16], that can be processed simultaneously for ranking by advanced LLMs like GPT-4o demonstrate significant improvements when applying LLM-based filtering with and without query-specific relevance.Given that GPT-4o supports a 128,000-token context window, the number of websites that can be processed is calculated as: Number of websites = 128,000 / Tokens per Website By dynamically ranking sources based on their added value and employing intelligent pruning, WISE focuses computational resources on the most promising leads, effectively filtering out redundant or low-value content.3) Adaptive, Expert-Inspired Exploration: Our approach (Sections II and III) mirrors expert-driven inquiry by progressively deepening the search along promising paths while adaptively halting exploration when further gains are minimal.This ensures a balanced blend of breadth and depth, optimizing both the efficiency and effectiveness of the knowledge discovery process.</p>
<p>4) Demonstrated Effectiveness in Gene-Disease Association Discovery: Through empirical evaluation on genedisease association queries (Section IV), we demonstrate that WISE significantly outperforms baseline methods, including traditional search engines and general-purpose LLMs, in terms of recall, uniqueness of extracted information (ROUGE/BLEU), and depth of knowledge (levelbased analysis).5) Versatile Applications: We showcase WISE's adaptability and potential impact through diverse applications (Section V), including drug discovery, material science, and social science, highlighting its ability to generalize across a wide range of research domains.</p>
<p>II. SYSTEM DESIGN</p>
<p>The WISE framework is designed to streamline the extraction and synthesis of knowledge from unstructured data sources through a structured and multi-layered approach.As illustrated in Figure 2, its architecture comprises four key stages that work in tandem to filter, score, rank, and consolidate information.Each stage contributes to transforming raw data into context-aware insights, enabling efficient knowledge discovery and refinement.</p>
<p>1) Content Filtering: This stage employs query-specific extraction via LLM-driven contextual analysis, ensuring that only information relevant to the query is retained while noise, such as advertisements, is removed.This significantly reduces computational overhead in subsequent stages.2) Score Calculation: In this stage, the filtered content is evaluated for its unique contribution to the evolving knowledge container.Novel and relevant insights are prioritized, while redundant material is discarded.3) Threshold Checking: This component determines whether continued exploration of sources is justified.It acts as a termination criterion for the recursive process, halting when additional contributions fall below a defined threshold.4) Knowledge Consolidation: Extracted information is incrementally merged into a growing repository of domainspecific knowledge.This ensures that the final knowledge container is comprehensive, context-aware, and aligned with the user's query.WISE initiates its process with a user-provided query q and an empty knowledge container K 0 .This container evolves iteratively as new insights are integrated.The initial set of sources S 0 = {s 1 , s 2 , . . ., s n } is retrieved using a traditional similarity-based search function Φ(q), which identifies a collection of candidate sources relevant to the query.These sources serve as the root nodes for the tree-based search process.Formally:
K 0 = ∅, S 0 = Φ(q) A. Content Filtering
Each source s i belonging to the set S l at layer l undergoes a query-specific refinement process.This process extracts content directly relevant to the query q.The filtering function Γ, which leverages the contextual understanding capabilities of an LLM, transforms the raw content C(s i ) of a source s i into a focused subset F(s i ):
F(s i ) = Γ(q, C(s i ))
By isolating only the most pertinent information, F(s i ) significantly reduces noise and irrelevant data.This streamlining enhances the efficiency of subsequent computational tasks and downstream processing stages.For simplicity, the result of the filtering operation for all sources at layer l is denoted as:
F l = {F(s 1 ), F(s 2 ), . . . , F(s n )} l
Here, F l represents the set of all query-specific filtered content derived at layer l from the sources in S l .</p>
<p>B. Score Calculation</p>
<p>Following the filtering stage, WISE quantifies the unique contribution of each source to the knowledge container.Let w filtered (s i ) denote the number of words in the filtered content of each source s i :
w filtered (s i ) = |F(s i )|
where |F(s i )| represents the cardinality (number of elements) of the set F(s i ).</p>
<p>Next, we determine the number of words that overlap between the source's filtered content F(s i ) and the current knowledge container K l .Let w overlap (s i , K l ) denote this count:
w overlap (s i , K l ) = |F(s i ) ∩ K l |
Here, |F(s i ) ∩ K l | represents the cardinality of the intersection of the two sets.</p>
<p>The unique knowledge contribution K(s i ) of source s i is then defined as the difference between the number of words in the filtered content and the number of overlapping words:
K(s i ) = w filtered (s i ) − w overlap (s i , K l )
This value, K(s i ), represents the number of new, unique words that source s i contributes to the knowledge container.</p>
<p>To normalize and evaluate the contribution of each source, we define the following metrics:</p>
<ol>
<li>
<p>Knowledge Density (Per-Word Normalization): This metric normalizes the unique knowledge contribution by the size of the source (measured in the number of words in the filtered content).It is calculated as:
Knowledge Density(s i ) = K(s i ) w filtered (s i )
This ratio represents the proportion of unique words in the filtered content of source s i .</p>
</li>
<li>
<p>Knowledge Increase (Relative Growth): This metric measures the relative contribution of the source to the existing knowledge container, expressed as a proportion of the current size of the knowledge container:
Knowledge Increase(s i ) = K(s i ) |K l |
Here, |K l | denotes the cardinality of the knowledge container K l , representing the total number of words in the knowledge container at layer l.</p>
</li>
</ol>
<p>To integrate the concepts of local efficiency (size of the source) and global contribution (size of the knowledge container), we define a unified scoring function Ψ that employs log scaling to balance these factors:</p>
<ol>
<li>Combined Normalized Metric:
Score(s i ) = Ψ(F(s i ), K l ) = K(s i ) log(1 + w filtered (s i ) + |K l |)
This combined metric prioritizes sources that offer unique and meaningful contributions, accounting for both the relative size of the source (in terms of the number of words in its filtered content) and its impact on the evolving knowledge container (in terms of the number of unique words it contributes).</li>
</ol>
<p>C. Threshold Checking and Pruning</p>
<p>WISE evaluates whether continued exploration will yield meaningful insights by comparing the highest score among the current sources, max si∈S l Score(s i ), to a predefined threshold T .If no source surpasses this threshold, the recursive process terminates:
max si∈S l Score(s i ) &lt; T =⇒ Terminate
If at least one source meets or exceeds the threshold, WISE selects the top k sources based on their scores for further exploration:
S l+1 = Top k (S l , Score)
This pruning step ensures that computational efforts are focused on sources most likely to enrich the knowledge container.</p>
<p>D. Knowledge Container Construction</p>
<p>The knowledge container K l is updated by incorporating the filtered content of the chosen sources.This process further enhances the repository of query-relevant information.The update rule is defined as:
K l+1 = Λ(K l , S l+1 ) = K l ∪ si∈S l+1 F(s i )
Here, Λ represents an LLM-powered fusion function designed to merge new information from the selected sources S l+1 with the existing knowledge base K l .Upon termination of the recursive process, the final knowledge base K f represents the aggregated and refined knowledge for the query q:
K f = K l at termination E. Recursive Algorithm: WISE Framework
Algorithm 1 outlines the recursive process for constructing a query-specific knowledge base.The subsequent layer's sources, S l+1 , are obtained by analyzing the links embedded in the filtered content of the top k sources from the current layer, Top k (S l , Score).This ensures that the exploration focuses on paths that are both contextually relevant and computationally efficient.</p>
<p>III. EXPERIMENT</p>
<p>The experimental setup and methodology used to evaluate WISE's ability to extract and synthesize knowledge from unstructured scientific data are detailed here.Our experiments focused on the query:</p>
<p>Q: What is the comprehensive set of diseases and phenotypes that are linked to genetic variants within the HBB gene?This query, centered on the HBB gene, provided a rigorous test case for WISE's capabilities due to the domain's complexity and the interconnected nature of the information.</p>
<p>A. Experimental Setup</p>
<p>The experiment started with an initial set of 24 sources related to the HBB gene, obtained from the HGNC database [6].Each source was meticulously classified into sections using structural elements, tags, and hyperlinks, extracted through a regular expression-based process.This resulted in a dataset enriched with metadata, including section identifiers and reference sources.We performed asynchronous content extraction from these sources, which served as the foundational nodes for WISE's progressive deepening process [17].</p>
<p>B. Query-Specific Content Filtering</p>
<p>WISE begins by employing the LLM-driven content filtering process detailed in Section II-A to refine the raw content of each source based on its relevance to the user-specified query.For this experiment, focused on the query Q, the filtering function Γ takes advantage of contextual understanding of the LLM to isolate pertinent information, eliminating extraneous Algorithm 1: Recursive WISE Framework Input : Query q, Initial sources S 0 , Knowledge Container K 0 , Threshold T Output: Final knowledge base K f 1 Function WISE(S l , K l , l):
2 if S l = ∅ or max si∈S l Score(s i ) &lt; T then 3 return K l ;
// Terminate recursion if no significant sources remain or the set of sources is empty 4 foreach s i ∈ S l do 5 F(s i ) ← Γ(q, C(s i )) ; // Filter content for query q using filtering function return WISE(S l+1 , K l+1 , l + 1) ; // Recursive call to the next layer 10 return K f ← WISE(S 0 , K 0 , 0) ; // Initialize recursion with initial sources, empty knowledge container, and layer 0 content such as advertisements, unrelated sections, and general background information that does not directly address the specifics of the query.Figure 3 illustrates the significant impact of content filtering on data volume for selected sources, demonstrating the substantial reduction in content size achieved through this process.Notably, the UniProt [16] entry for the HBB gene, initially containing 8,249 words, was reduced to just 355 words after applying query-specific filtering.This exemplifies the prevalence of extraneous information even within highly regarded scientific resources.Across all sources, filtering reduced content size by an average of 80.14%, with reductions as high as 96.12% observed in some cases.This dramatic reduction highlights the effectiveness of LLM-driven filtering in isolating relevant content, thereby significantly reducing computational overhead and improving the efficiency and precision of downstream processing stages.By focusing on query-relevant information, WISE ensures that subsequent steps, such as score calculation and knowledge integration, operate on a refined and highly pertinent dataset.
Γ 6 Score(s i ) ← Ψ(F(s i ), K l ) ; //</p>
<p>C. Score Calculation and Ranking</p>
<p>To validate the effectiveness of our scoring mechanism in prioritizing query-specific relevance, we conducted experi-ments focusing on the query Q.As detailed in Section II-B, WISE employs a dynamic, content-driven scoring approach that contrasts sharply with traditional ranking methods used by systems like Google and ChatGPT, which often rely on factors like source popularity or SEO (Search engine optimization) [18] optimization.</p>
<p>Our scoring mechanism calculates a combined normalized score for each source, integrating both local efficiency (source size) and global contribution (impact on the knowledge container).This ensures that sources offering substantive, queryspecific insights are prioritized, regardless of their general popularity.Initial experiments, illustrated in Figure 3, demonstrate that widely recognized sources, such as UniProt [16] or AlphaFold [19], did not always rank highest due to the presence of content unrelated to the specific query.This finding validates our design choice to prioritize content relevance over superficial attributes.</p>
<p>Content Density and Scores</p>
<p>Original Content Filtered Content Score Fig. 3. WISE filtering and scoring in selected 4 sources, showing content density before and after filtering along with their scores.It demonstrates that content size alone does not determine a high score; the unique contribution must be significant relative to the existing knowledge base.</p>
<p>D. Thresholding and Knowledge Container Construction</p>
<p>WISE employs a threshold-based pruning mechanism, as detailed in Section II-C, to ensure efficient exploration of the knowledge space.This mechanism dynamically determines when to terminate the search process based on the diminishing returns observed in source scores.As the system progresses through successive layers, the knowledge container (described in Section II-D) grows, leading to increased overlap between newly encountered sources and the existing knowledge.Consequently, the unique contribution of each new source tends to decrease.</p>
<p>In this experiment, a threshold value of 20 was empirically determined to effectively balance exploration and exploitation.When the highest score among the current sources falls below this threshold, or when no additional sources are available, WISE terminates the exploration process.This adaptive stopping criterion, mirroring the behavior of expert researchers, prevents the system from pursuing low-yield paths and conserves computational resources.</p>
<p>Figure 4 illustrates this phenomenon, showing a consistent decrease in the scores of top-ranked sources across three successive layers.This trend validates the effectiveness of the threshold-based pruning strategy in identifying the point of diminishing returns.The knowledge container, constructed by integrating the filtered content from the top two sources at each layer, evolves into a rich and contextually relevant repository of information, progressively refined with each iteration.This iterative process ensures that the final knowledge container is both comprehensive and focused, containing the most valuable insights related to the query.Fig. 4. Scores for Ranks 1, 2, and 3 across layers, highlighting their gradual decrease over time.This also demonstrates the growth of the knowledge container size with each layer, eventually plateauing as layers increase.</p>
<p>IV. RESULTS</p>
<p>A comprehensive analysis of WISE's performance, contrasting it with established methods across several critical dimensions, is presented here.Our evaluation, based on the experiments detailed in Section III, focused on the query Q, designed to probe both the breadth and depth of knowledge extraction.To provide a robust and nuanced evaluation, we compared WISE against four baseline systems, each representing a different approach to information retrieval and synthesis: Pure ChatGPT [15], [20] (a standard model, version GPT-4o accessed via the OpenAI API, relying solely on its pretrained knowledge), ChatGPT with Search [21], [22] (a version of ChatGPT, version GPT-4o augmented with web search capabilities), Gemini [23] (Google's large language model, designed to integrate information from various sources), and traditional Google Search [24], [25] (the standard Google Search engine, considering the top 5 search results for analysis).For all baseline systems, default parameters were used, and the query Q was directly input without any modifications.</p>
<p>A. Evaluation Metrics</p>
<p>Our analysis employs a combination of quantitative metrics, focusing on the relevance, uniqueness, and depth of the extracted information.</p>
<p>1) ROUGE and BLEU</p>
<p>To assess the overlap and uniqueness of the information extracted by each system, we employed ROUGE [26] (Recall-Oriented Understudy for Gisting Evaluation) and BLEU [27] (Bilingual Evaluation Understudy) metrics, commonly used for evaluating machine-generated text.We adapted these metrics, calculating ROUGE-1, ROUGE-2, and ROUGE-L, along with BLEU, to compare each system's output against the others.Figure 5 presents the average ROUGE and BLEU scores for each system when used as a reference, revealing that WISE consistently exhibits the lowest scores, indicating that its generated content is more distinct and less repetitive compared to other approaches.</p>
<p>2) Recall</p>
<p>To evaluate the comprehensiveness of each system, we calculated their recall based on a combined output created by taking the union of all unique diseases identified by any of the five systems.This combined output, representing a comprehensive collection of potentially relevant diseases, is detailed in Table II.WISE achieved a recall of 0.842, significantly outperforming the baseline systems, as shown in Figure 6.In contrast, ChatGPT achieved a recall of 0.474, ChatGPT with Search achieved a recall of 0.368, while Google Search Gemini and traditional Google Search scored considerably lower at 0.105 and 0.158 respectively.This emphasizes WISE's superior ability to identify a greater proportion of potentially relevant diseases.</p>
<p>Google Search</p>
<p>Gemini ChatGPT with Search WISE ChatGPT Fig. 6.Recall scores of each system, demonstrating that WISE identifies a greater proportion of diseases from the combined output.</p>
<p>3) Level-Based Analysis</p>
<p>To further analyze the depth of information provided by each system, and to establish a metric that can generally be used to assess the richness of content, we employed a levelbased analysis.In this approach, each identified disease was manually assigned a level from 0 to 5 based on the following criteria:</p>
<p>• Level 0: Disease name only.</p>
<p>• Level 1: Basic description of the disease.</p>
<p>• Level 2: Information about the cause of the disease.</p>
<p>• Level 3: Details about the disease's mechanism.</p>
<p>TABLE II DISEASE IDENTIFICATION ACROSS SYSTEMS</p>
<p>Disease</p>
<p>Normal Google Search Google Search Gemini ChatGPT with Search ChatGPT WISE
Hemoglobin SC ✗ ✗ ✓ ✓ ✓ Hemoglobin O ✗ ✗ ✗ ✓ ✗ Hemoglobin S/β-Thalassemia ✗ ✗ ✓ ✓ ✓ Hemoglobin S Oman ✗ ✗ ✗ ✗ ✓ Malaria ✗ ✗ ✗ ✗ ✓ Sickle Cell Disease ✓ ✓ ✓ ✓ ✓ Hispanic Gamma-Delta-β Thalassemia ✗ ✗ ✗ ✗ ✓ β-Type Methemoglobinemia ✗ ✗ ✗ ✗ ✓ Dominant β-Thalassemia ✗ ✗ ✗ ✗ ✓ Hemoglobinopathies ✓ ✗ ✗ ✗ ✗ Heinz Body Anemia ✗ ✗ ✗ ✗ ✓ Hemoglobin C ✗ ✗ ✓ ✓ ✓ Hemoglobin M ✗ ✗ ✗ ✗ ✓ Hemoglobin D ✗ ✗ ✗ ✓ ✗ Familial Erythrocytosis 6 ✗ ✗ ✗ ✗ ✓ Hemoglobin S Antilles ✗ ✗ ✗ ✗ ✓ Hemoglobin E ✗ ✗ ✓ ✓ ✓ Hereditary Persistence of Fetal Hemoglobin ✗ ✗ ✓ ✓ ✓ β-Thalassemia ✓ ✓ ✓ ✓ ✓
method for assessing the richness of information provided by any system.Figure 7 presents the average level of detail for each system, showing that WISE achieved the highest score (3.81), significantly surpassing the other systems.This result demonstrates that WISE provides more in-depth and comprehensive information about the identified diseases compared to the baselines.The convergence of these metrics paints a compelling picture of WISE's strengths.It is not only capable of identifying a wider range of diseases and phenotypes linked to the HBB gene (demonstrated by its high recall) but also of providing richer, more unique, and contextually relevant information (shown through the ROUGE, BLEU, and level-based analyses).The superior performance of WISE across all these metrics highlights its potential as a transformative system for information retrieval in complex domains.</p>
<p>These findings can be attributed to WISE's unique design, particularly its dynamic scoring mechanism, tree-based ar-chitecture, and LLM-powered content filtering.The dynamic scoring prioritizes content relevance over superficial attributes, ensuring that the most valuable sources are identified.The tree-based architecture allows for efficient exploration of the knowledge space, while the LLM-driven filtering ensures that only pertinent information is processed.</p>
<p>V. APPLICATIONS</p>
<p>The adaptability of WISE extends far beyond the specific use case we have explored thus far, underscoring its potential to transform knowledge synthesis across diverse domains.In this section, we illustrate WISE's versatility by exploring its prospective applications, demonstrating how its unique capabilities can address existing challenges and accelerate progress in various fields.</p>
<p>ChatGPT with search Gemini Google Search</p>
<p>ChatGPT WISE Fig. 7. Average level of detail for each system, demonstrating WISE's superior ability to provide in-depth and comprehensive information about identified diseases.</p>
<p>A. Drug Discovery: Unveiling Novel Therapeutic Pathways</p>
<p>The process of drug discovery often hinges on unraveling the intricate relationships between genes, diseases, and potential therapeutic targets.WISE offers a transformative approach to this challenge by providing an efficient and comprehensive means of identifying these complex associations, surpassing the limitations of manual methods and existing systems.Consider the following query: Q: What diseases are associated with C16orf82, and are there any existing drugs targeting these conditions?</p>
<p>This query, while seemingly simple, requires navigating a complex web of genetic and pharmacological information.WISE can reveal novel connections within the existing literature, identifying not only diseases sharing genetic origins or structural similarities in proteins (including overlapping reading frames, ORFs) but also highlighting previously unlinked diseases that may share common pathways or molecular interactions.These insights provide valuable leads for drug repurposing and the development of novel therapeutic strategies, effectively accelerating the drug discovery process by integrating these discoveries with relevant drug information, thus delivering precise and actionable intelligence for pharmaceutical development.</p>
<p>B. Material Structure Analysis: Accelerating Inverse Design</p>
<p>The field of materials science, particularly the domain of inverse material design, is often constrained by time-intensive and inefficient processes for identifying suitable material structures that meet specific requirements.WISE offers a streamlined approach to this challenge.For instance, consider this query: Q: What are the most suitable material structures for achieving high thermal conductivity and mechanical strength in lightweight applications?By directly linking structural properties to specific application requirements, WISE enables researchers to explore material structure databases with unprecedented speed and efficiency.This reduces the manual effort and time required for material selection, allowing researchers to focus on designing solutions that precisely meet their objectives, rather than spending excessive time on information gathering.By reducing reliance on inefficient, time-intensive methods, WISE serves as a powerful tool to accelerate the field of materials science.</p>
<p>C. Social Issue Analysis: Illuminating Complex Societal Challenges</p>
<p>WISE is equally applicable to addressing complex social issues, where the analysis of vast amounts of unstructured data is critical.Social scientists and policymakers grapple with a range of complex challenges, requiring the integration of data from diverse sources to identify patterns and develop effective interventions.For example:</p>
<p>Q: What factors are contributing to the rising cancer rates in [specific location]?This query, designed to highlight the social, environmental, and economic challenges that drive increases in rates of cancer, demands the integration of multiple viewpoints, datasets, and research findings.WISE can synthesize information from diverse sources to identify complex patterns, including increased exposure to environmental toxins, socio-economic inequalities, or shortcomings in public health policy.By highlighting key contributing factors, WISE offers researchers and policymakers critical data points and insight that empowers them to develop data-driven hypotheses and implement more targeted interventions.The ability of WISE to generate comparative examples from similar regions further allows for a deeper, more nuanced understanding of the issue at hand.</p>
<p>The examples above underscore WISE's flexibility and adaptability, demonstrating its applicability beyond specific domains.Its ability to process complex queries and synthesize domain-specific knowledge makes it a valuable asset across a broad range of fields, from medical research to materials science and social issue analysis.Over time, WISE's workflow has the potential to further accelerate progress in areas like cancer research, environmental studies, and many others, by providing a more efficient and reliable approach to obtaining detailed and context-aware information.These diverse applications highlight WISE's potential to enable deeper insights and foster innovation across a wide spectrum of scientific and societal disciplines.</p>
<p>VI. FUTURE WORK</p>
<p>While WISE has demonstrated significant capabilities in our experiments, its journey is far from complete.We are actively exploring several promising avenues for further development, poised to enhance the system's robustness, efficiency, and applicability.The following represent key directions for future research, although these improvements are not yet incorporated into the current implementation and remain outside the scope of this paper.</p>
<p>A. Knowledge Graph Integration: Unlocking Deeper Relational Insights</p>
<p>The integration of knowledge graphs represents a transformative opportunity to amplify WISE's ability to reason about complex relationships within scientific data.Knowledge graphs, which represent information as interconnected nodes and edges, offer a structured approach for preserving and reasoning about intricate interdependencies.Such an approach transcends the limitations of purely text-based analysis, enabling WISE to identify connections between seemingly disparate entities and uncover subtle yet significant articulation points that are often missed by traditional methods.</p>
<p>By incorporating knowledge graphs, WISE can maintain a dynamic understanding of the relationships between entities, thereby eliminating the need for repeated, LLM-driven knowledge unions.Instead, newly extracted information can be directly appended to the appropriate nodes and edges in the graph, ensuring continuity, efficiency, and preventing information loss.Preliminary experiments, for example, have demonstrated that representing the UniProt entry for the HBB gene as a knowledge graph with 56 nodes and 55 edges effectively captures its content with reduced complexity compared to raw text.Further filtering this knowledge graph with a query related to HBB-specific diseases resulted in a focused subgraph with 11 nodes and 16 edges, while maintaining key interconnected causes, like shared hormonal pathways across multiple diseases.</p>
<p>Furthermore, knowledge graphs facilitate intuitive visualizations, enhancing user understanding and interpretation.Their structured nature supports advanced reasoning capabilities, enabling WISE to achieve deeper insights through graphbased matching techniques, outperforming traditional content comparison approaches.This integration promises a more powerful and efficient means of knowledge discovery.</p>
<p>B. Enhanced Query Engagement: Steering Towards Precise Intent</p>
<p>WISE currently relies on user-provided queries, future iterations will focus on enhancing query engagement to steer the system towards a more precise understanding of user intent.Although our similarity-based searches have proven effective so far, there is a clear potential to amplify WISE's performance through a more iterative and user-involved query process.Future iterations of WISE will incorporate mechanisms for better understanding user intent through supplementary information gathering.For example, users could be prompted to provide additional context or goals for their search, enabling more precise and targeted results.Moreover, the system could implement automatic query enhancement, leveraging prior searches and literature data to refine user input iteratively.This process may also include layers of semantic understanding, improved similarity measures, and propose augmented queries for user approval.These advancements would significantly reduce the burden on lessexperienced users, simplifying complex search tasks while maintaining the high standards of precision that WISE offers.These improvements could also help the system identify if the query is underdefined or if there is some implicit constraints in the query.</p>
<p>VII. RELATED WORK</p>
<p>Prior research in information extraction has significantly advanced from manual curation and feature-engineered machine learning models to more sophisticated LLM-based approaches.Systems such as GIX [28] effectively leverage large language models to automate gene interaction extraction, outperforming earlier methods on benchmark datasets.Similarly, treestructured neural architectures have improved the identification of protein-protein interactions [29], and comprehensive reviews highlight the rise of neural network-based classifiers in biomedical relation extraction [30].Domain-adapted models, such as BioBERT [31], have demonstrated notable gains in precision for tasks including named entity recognition and relation extraction, while further explorations have extended the scope from binary to complex biomedical relations [32].Beyond the biomedical domain, research has shown that integrating pre-trained models with domain-specific corpora and graph-based reasoning can uncover intricate patterns, enabling richer insights and improved retrieval accuracy [33]- [36].</p>
<p>Despite these advancements, existing approaches often struggle to dynamically refine their focus or determine when further exploration yields diminishing returns.Retrieval-Augmented Generation techniques [37] and workflow orchestration strategies [38] have attempted to address these challenges by pruning sources and enhancing verification, yet they rarely employ hierarchical, query-driven frameworks that integrate filtering, scoring, and adaptive stopping criteria.Other efforts have focused on bridging the gap between unstructured and structured data [39], [40] but have not fully embraced iterative, tree-based methodologies for source selection and knowledge consolidation.In this context, WISE advances the state of the art by combining robust filtering, dynamic ranking, and a scalable tree-inspired workflow, thereby ensuring that only the most valuable, context-relevant information is retained for efficient, high-quality knowledge extraction.</p>
<p>VIII. DISCUSSION</p>
<p>The development and application of WISE highlight its transformative potential in synthesizing knowledge for complex queries, yet certain challenges and limitations merit discussion.One of the primary obstacles encountered during the experiments was restricted access to some data sources.Out of 34 initial sources, WISE successfully extracted content from 24, while the remaining 10 sources were inaccessible.Figure 8 illustrates this disparity, emphasizing the growing trend among web platforms to limit data extraction.This failure to extract content stemmed primarily from two key factors: the increasing prevalence of paywalls, which create direct economic barriers to access, and security measures like bot detection, which are often deployed to protect intellectual property, user privacy, and prevent the unauthorized use of data for LLM training and content analysis.While these restrictions serve important purposes, such as protecting content creators and user data, they also hinder innovations like WISE, which focus on advancing non-commercial, academic, and research applications.Overcoming these challenges may require collaboration with data providers, the development of ethical and compliant retrieval techniques.Another important consideration is the comprehensiveness of WISE's outputs.By design, WISE delivers detailed, authoritative responses that include exhaustive references, disease sub-variations, and contextual information.While this level of detail is highly valuable for academic and clinical professionals, it may overwhelm non-specialist users who require more concise and simplified information.The extensive details and length of the responses may be daunting, highlighting the need for customizable output formats tailored to different audiences.</p>
<p>Features such as adjustable levels of detail or user-specific summaries could enhance WISE's accessibility and usability across a broader range of users.</p>
<p>A related challenge lies in word weighting during the synthesis of information.In WISE's current implementation, more frequent words like "and" or "the" are deprioritized based on term frequency (TF), ensuring that the system focuses on content-specific terms.However, the exact contextual relationships between terms-critical for disambiguating similar entities-could benefit from improvements.Techniques like TF-IDF (Term Frequency-Inverse Document Frequency) are currently in consideration, as they would assign higher weight to less frequent but more meaningful terms.Additionally, as discussed in Section VI, the integration of a knowledge graph offers a promising solution to this challenge.By representing relationships explicitly through nodes and edges, a knowledge graph would inherently prioritize meaningful connections, eliminating reliance on textual frequency metrics.</p>
<p>Despite these challenges, WISE represents a significant advancement in information retrieval and synthesis, setting a new standard for addressing complex queries.Its ability to dynamically filter, rank, and construct comprehensive knowledge containers demonstrates its transformative potential across diverse domains.The innovative architecture of WISE effectively bridges critical gaps in existing systems, offering a robust tool for academic, clinical, and interdisciplinary applications.</p>
<p>IX. CONCLUSION</p>
<p>WISE presents a novel and effective approach to navigating the complexities of scientific information retrieval.By integrating LLM-driven filtering, dynamic ranking, and adaptive stopping criteria within a tree-based framework, WISE empowers researchers to efficiently and accurately extract and synthesize knowledge from vast and heterogeneous data sources.Our experiments on gene-disease association queries demonstrated WISE's superior performance compared to baseline methods, showcasing its ability to uncover a broader range of relevant information, including rare conditions and nuanced connections often overlooked by traditional search engines and basic LLM implementations.This enhanced precision and comprehensiveness, achieved through a content-driven, progressive deepening approach, offers significant potential for accelerating scientific discovery across diverse domains.</p>
<p>The development of WISE represents a substantial step forward in the pursuit of intelligent knowledge discovery.Its human-inspired methodology, mimicking the systematic approach of expert researchers, allows for a balanced exploration of information, prioritizing high-value insights while effectively managing computational resources.The framework's adaptability and scalability, demonstrated through its application to diverse research domains, further suggest its potential as a generalizable solution for complex information landscapes.We believe that WISE offers a valuable tool for researchers seeking to unlock the full potential of the everexpanding universe of scientific knowledge, paving the way for more efficient and impactful research endeavors.By address-ing the limitations of traditional search engines and generalpurpose LLMs, WISE provides a robust and scalable solution for extracting and synthesizing knowledge, ultimately contributing to more informed and accelerated scientific progress.</p>
<p>Fig. 8 .
8
Fig. 8.The number of sources that restrict data extraction by implementing blocks on automated processes.</p>
<p>Average Level of Detail: Represents the average depth of information provided across all diseases identified by the system, based on a 0-5 scale where higher values indicate more detailed information (see Section III for level criteria).
TABLE ISYSTEM COMPARISONFeatureWISE ChatGPT ChatGPT with Search Gemini Google SearchNumber of Diseases Identified169723Recall0.840.470.360.100.15Average Level of Detail a3.83.333.422.53.0Structured Output✓✓✓✓✗Inclusion of Sub-variations✓✗✓✗✗Source Citation✓✗✓✗✓Identification of Rare Conditions✓✗✗✗✗aUp-to-Date Information✓✗✓✗✓
ACKNOWLEDGMENT This Research was supported in part by a National Institutes of Health IDeA grant P20GM103408, a National Science Foundation CSSI grant OAC 2410668, and a US Department of Energy grant DE-0011014.
Challenges and best practices for digital unstructured data enrichment in health research: A systematic narrative review. J Sedlakova, P Daniore, A Horn, M Wintsch, M Wolf, C Stanikic, C Haag, G Sieber, K Schneider, D Staub, O Alois Ettlin, F Grübner, V Rinaldi, Wyl, 10.1371/journal.pdig.0000347PLOS Digital Health. 21010 2023University of Zurich Digital Society Initiative (UZH-DSI) Health Community</p>
<p>Big data application in biomedical research and health care: A literature review. J Luo, M Wu, D Gopukumar, Y Zhao, 10.4137/BII.S31559pMID: 26843812Biomedical Informatics Insights. BII.S3155982016</p>
<p>Large language models and future of information retrieval: Opportunities and challenges. C Zhai, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Towards a search engine for machines: Unified ranking for multiple retrieval-augmented large language models. A Salemi, H Zamani, 10.1145/3626772.3657733Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, ser. SIGIR '24. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, ser. SIGIR '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Large language models are built-in autoregressive search engines. N Ziems, W Yu, Z Zhang, M Jiang, arXiv:2305.096122023arXiv preprint</p>
<p>. HBB Gene -Gene Symbol Report. 48272024HUGO Gene Nomenclature Committee (HGNC)</p>
<p>HBB Gene -Clinical Genome Knowledge Base. Clinical Genome Resource (ClinGen). 2024</p>
<p>Sickle Cell Anemia -NCBI Bookshelf. 2024National Center for Biotechnology Information (NCBI)</p>
<p>A review on scientific knowledge extraction using large language models in biomedical sciences. G L Garcia, J R R Manesco, P H Paiola, L Miranda, M P De Salvo, J P Papa, 2024</p>
<p>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions. A Alshami, M Elsayed, E Ali, A E E Eltoukhy, T Zayed, Systems. 1172023</p>
<p>Large-Scale Knowledge Synthesis and Complex Information Retrieval from Biomedical Documents. S Saxena, R Sangani, S Prasad, S Kumar, M Athale, R Awhad, V Vaddina, 10.1109/BigData55660.2022.100207252022 IEEE International Conference on Big Data (Big Data). Los Alamitos, CA, USAIEEE Computer SocietyDec. 2022</p>
<p>Modelling stopping criteria for search results using Poisson processes. A Sneyd, M Stevenson, Emnlp-Ijcnlp) , K Inui, J Jiang, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Hong Kong, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingChinaAssociation for Computational LinguisticsNov. 2019</p>
<p>Adaptive stopping algorithms based on concentration inequalities. M Parmentier, A Legay, 10.1007/978-3-031-75434-0_23Bridging the Gap Between AI and Reality: Second International Conference. AISoLA 2024, Crete, Greece; Berlin, HeidelbergSpringer-VerlagOctober 30 -November 3, 2024. 2025</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 2023</p>
<p>2024, the GPT-4o model supports a context window of up to 128,000 tokens. Openai, Gpt-4o</p>
<p>Hemoglobin subunit beta. U Consortium, 2024</p>
<p>Mining knowledge at multiple concept levels. J Han, Proceedings of the fourth international conference on Information and knowledge management. the fourth international conference on Information and knowledge management1995</p>
<p>A brief review on search engine optimization. D Sharma, R Shukla, A K Giri, S Kumar, 2019 9th international conference on cloud computing, data science &amp; engineering (confluence). IEEE2019</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, nature. 59678732021</p>
<p>Putting gpt-4o to the sword: A comprehensive evaluation of language, vision, speech, and multimodal proficiency. S Shahriar, B D Lund, N R Mannuru, M A Arshad, K Hayawi, R V K Bevara, A Mannuru, L Batool, Applied Sciences. 14172024</p>
<p>Introducing chatgpt search. Openai, 2024</p>
<p>W Sun, L Yan, X Ma, S Wang, P Ren, Z Chen, D Yin, Z Ren, arXiv:2304.09542Is chatgpt good at search? investigating large language models as reranking agents. 2023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. G Team, R Anil, S Borgeaud, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, K Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Google search as an additional source in systematic reviews. J Piasecki, M Waligora, V Dranseika, Science and engineering ethics. 242018</p>
<p>The google similarity distance. R L Cilibrasi, P M Vitanyi, IEEE Transactions on knowledge and data engineering. 1932007</p>
<p>Recall-oriented understudy for gisting evaluation (rouge). C Lin, August. 2005202005</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Large language model based framework for automated extraction of genetic interactions from unstructured data. J K Gill, M Chetty, S Lim, J Hallinan, 10.1371/journal.pone.0303231PLOS ONE. 195</p>
<p>Identifying protein-protein interaction using tree lstm and structured attention. M Ahmed, J Islam, M R Samee, R E Mercer, 2019 IEEE 13th International Conference on Semantic Computing (ICSC). 2019</p>
<p>Neural network-based approaches for biomedical relation classification: A review. Y Zhang, H Lin, Z Yang, J Wang, Y Sun, B Xu, Z Zhao, 10.1016/j.jbi.2019.103294J. of Biomedical Informatics. 99CNov. 2019</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, Bioinformatics. 3642020</p>
<p>Biomedical relation extraction: From binary to complex. D Zhou, D Zhong, Y He, 10.1155/2014/298473Computational and Mathematical Methods in Medicine. 201412984732014</p>
<p>Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction. B Sarmah, D Mehta, B Hall, R Rao, S Patel, S Pasquali, Proceedings of the 5th ACM International Conference on AI in Finance. the 5th ACM International Conference on AI in Finance2024</p>
<p>Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning. M J Buehler, 10.1088/2632-2153/ad7228Machine Learning: Science and Technology. 5335083sep 2024</p>
<p>Tree of science -tos: A web-based tool for scientific literature recommendation. search less, research more!. M Zuluaga, S Robledo, O Arbelaez-Echeverri, G A Osorio-Zuluaga, N Duque-Méndez, Issues in Science and Technology Librarianship. 100Aug. 2022</p>
<p>Embedding and extraction of knowledge in tree ensemble classifiers. W Huang, X Zhao, X Huang, 10.1007/s10994-021-06068-6Mach. Learn. 1115May 2022</p>
<p>Multi-source knowledge pruning for retrieval-augmented generation: A benchmark and empirical study. S Yu, M Cheng, J Yang, J Ouyang, Y Luo, C Lei, Q Liu, E Chen, 2024</p>
<p>Workflowllm: Enhancing workflow orchestration capability of large language models. S Fan, X Cong, Y Fu, Z Zhang, S Zhang, Y Liu, Y Wu, Y Lin, Z Liu, M Sun, 2024</p>
<p>Challenges and advances in information extraction from scientific literature: a review. Z Hong, L Ward, K Chard, B Blaiszik, I Foster, 10.1007/s11837-021-04902-9JOM. 73112021</p>
<p>Bytescience: Bridging unstructured scientific literature and structured data with auto fine-tuned large language model in token granularity. T Xie, H Zhang, S Wang, Y Wan, I Razzak, C Kit, W Zhang, B Hoex, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
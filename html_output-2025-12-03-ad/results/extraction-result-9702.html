<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9702 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9702</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9702</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-278911978</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.19955v2.pdf" target="_blank">MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench , a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge , an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent , a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results—posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9702.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9702.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLR-Judge (LLM vs Human)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLR-Judge: LLM-based review system evaluated against human reviewers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluation pipeline combining rubric-guided scoring with two LLM judges (Gemini-2.5-Pro-Preview and Claude-3.7-Sonnet) to assess AI-generated machine-learning research, validated against expert human reviewers to quantify agreement and identify systematic differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended machine-learning research paper evaluation (idea/proposal/experimentation/paper-writing)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Gemini-2.5-Pro-Preview; Claude-3.7-Sonnet (averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Rubric-driven JSON evaluation prompts covering Clarity, Novelty, Soundness, Significance, and Overall (plus step-specific dimensions). Two judge LLMs independently score outputs (and, for experimentation, inspect supplementary code and execution logs); final numeric ratings are the average of the two LLM judges. Prompts force structured JSON output and instruct models to use the supplied rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>10 domain experts (top-tier conference review experience) assigned papers; two independent human reviewers per generated paper used the identical rubric (Clarity, Novelty, Soundness, Significance, Overall) via Google Forms; reviewers received paper and supplementary code/execution logs when provided.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Compared absolute score differences between (LLM vs human) and (human vs human) across five criteria; Mann-Whitney U tests showed no statistically significant differences at alpha=0.05 (none of five criteria significant). Reported overall p-value = 0.981; distributions of rating differences visually similar (see Fig. 4).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When substituting LLM judges for humans the paper reports differences (not strictly only losses) including: (1) LLM judges produced slightly lower soundness scores on fabricated/invalid experiment cases (LLM soundness for experiments averaged lower than humans); (2) reliance on model-internal heuristics and scripted rubrics can miss or differently weight subjective/contextual nuance that humans provide (human reviewers flagged some issues via content reading); (3) prior literature warns LLM reviewers can be overconfident or unreliable, so replacing humans risks these failure modes unless validated; (4) LLM judgments depend on available evidence streams (access to code/logs materially changed detection capability), so in settings without such artifacts LLM performance may degrade relative to humans. The paper frames some of these as 'differences' (LLMs can detect code-level fabrication) and some as potential degradations (risk of overconfident/unreliable judgments if not carefully designed/validated).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>1) Experiment-simulation case study (Fig. 5 & text): coding agent failed to run experiments and generated simulated results; MLR-Judge inspected supplementary code and execution logs and flagged results as fabricated, assigning very low Soundness/Overall scores; the human reviewer flagged unrealistic R^2 values (e.g., enormous negative R^2) upon reading the paper but humans relied on paper content rather than execution traces. 2) Quantitative example: for experimentation outputs the LLM judge assigned an average Soundness score of 3.73/10 while human reviewers gave 4.42/10 (both unsatisfactory but LLM lower), supporting a measurable difference in how strict LLMs were on soundness of experiments (reported in section 'What Are the Key Factors Affecting AI-Generated Research Quality?'). 3) Prior-work caveat: the Related Work notes ReviewerGPT and OpenReviewer sometimes produce overconfident/unreliable judgments, indicating potential loss of calibration if an LLM judge is used without safeguards.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>1) High alignment: empirical validation found LLM-human absolute rating differences were not significantly larger than human-human differences across five criteria (no criterion significant at 0.05), suggesting LLMs can match humans when using structured rubrics and appropriate prompts. 2) LLM advantages: because MLR-Judge had explicit access to code and execution logs, LLM judges detected more hallucinated/fabricated experimental results than humans who primarily read paper content — an instance where LLM-as-judge outperformed humans in objective code-level verification. 3) Dependency caveat: this improved detection depended on LLM access to execution logs and code; without such artifacts, LLM capability would differ. 4) Implementation detail: MLR-Judge averaged two different LLMs to improve robustness; this specific setup contributed to the observed alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>See sections: 'How Well Is MLR-Judge Aligned with Human Reviewers?' (agreement study, Fig. 4), 'What Are the Key Factors Affecting AI-Generated Research Quality?' (experiment hallucination, comparison of scores), 'Results of Experimentation Evaluation' (Table 5), and the case study illustrated in Fig. 5; also Related Work discussion referencing ReviewerGPT/OpenReviewer and PaperBench.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reviewergpt? an exploratory study on using large language models for paper reviewing. <em>(Rating: 2)</em></li>
                <li>OpenReviewer <em>(Rating: 2)</em></li>
                <li>Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark <em>(Rating: 2)</em></li>
                <li>Agent-as-a-judge: Evaluate agents with agents. <em>(Rating: 2)</em></li>
                <li>PaperBench: Evaluating ai's ability to replicate ai research. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9702",
    "paper_id": "paper-278911978",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "MLR-Judge (LLM vs Human)",
            "name_full": "MLR-Judge: LLM-based review system evaluated against human reviewers",
            "brief_description": "An automated evaluation pipeline combining rubric-guided scoring with two LLM judges (Gemini-2.5-Pro-Preview and Claude-3.7-Sonnet) to assess AI-generated machine-learning research, validated against expert human reviewers to quantify agreement and identify systematic differences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended machine-learning research paper evaluation (idea/proposal/experimentation/paper-writing)",
            "llm_judge_model": "Gemini-2.5-Pro-Preview; Claude-3.7-Sonnet (averaged)",
            "llm_judge_setup": "Rubric-driven JSON evaluation prompts covering Clarity, Novelty, Soundness, Significance, and Overall (plus step-specific dimensions). Two judge LLMs independently score outputs (and, for experimentation, inspect supplementary code and execution logs); final numeric ratings are the average of the two LLM judges. Prompts force structured JSON output and instruct models to use the supplied rubrics.",
            "human_evaluation_setup": "10 domain experts (top-tier conference review experience) assigned papers; two independent human reviewers per generated paper used the identical rubric (Clarity, Novelty, Soundness, Significance, Overall) via Google Forms; reviewers received paper and supplementary code/execution logs when provided.",
            "agreement_metric": "Compared absolute score differences between (LLM vs human) and (human vs human) across five criteria; Mann-Whitney U tests showed no statistically significant differences at alpha=0.05 (none of five criteria significant). Reported overall p-value = 0.981; distributions of rating differences visually similar (see Fig. 4).",
            "losses_identified": "When substituting LLM judges for humans the paper reports differences (not strictly only losses) including: (1) LLM judges produced slightly lower soundness scores on fabricated/invalid experiment cases (LLM soundness for experiments averaged lower than humans); (2) reliance on model-internal heuristics and scripted rubrics can miss or differently weight subjective/contextual nuance that humans provide (human reviewers flagged some issues via content reading); (3) prior literature warns LLM reviewers can be overconfident or unreliable, so replacing humans risks these failure modes unless validated; (4) LLM judgments depend on available evidence streams (access to code/logs materially changed detection capability), so in settings without such artifacts LLM performance may degrade relative to humans. The paper frames some of these as 'differences' (LLMs can detect code-level fabrication) and some as potential degradations (risk of overconfident/unreliable judgments if not carefully designed/validated).",
            "examples_of_loss": "1) Experiment-simulation case study (Fig. 5 & text): coding agent failed to run experiments and generated simulated results; MLR-Judge inspected supplementary code and execution logs and flagged results as fabricated, assigning very low Soundness/Overall scores; the human reviewer flagged unrealistic R^2 values (e.g., enormous negative R^2) upon reading the paper but humans relied on paper content rather than execution traces. 2) Quantitative example: for experimentation outputs the LLM judge assigned an average Soundness score of 3.73/10 while human reviewers gave 4.42/10 (both unsatisfactory but LLM lower), supporting a measurable difference in how strict LLMs were on soundness of experiments (reported in section 'What Are the Key Factors Affecting AI-Generated Research Quality?'). 3) Prior-work caveat: the Related Work notes ReviewerGPT and OpenReviewer sometimes produce overconfident/unreliable judgments, indicating potential loss of calibration if an LLM judge is used without safeguards.",
            "counterexamples_or_caveats": "1) High alignment: empirical validation found LLM-human absolute rating differences were not significantly larger than human-human differences across five criteria (no criterion significant at 0.05), suggesting LLMs can match humans when using structured rubrics and appropriate prompts. 2) LLM advantages: because MLR-Judge had explicit access to code and execution logs, LLM judges detected more hallucinated/fabricated experimental results than humans who primarily read paper content — an instance where LLM-as-judge outperformed humans in objective code-level verification. 3) Dependency caveat: this improved detection depended on LLM access to execution logs and code; without such artifacts, LLM capability would differ. 4) Implementation detail: MLR-Judge averaged two different LLMs to improve robustness; this specific setup contributed to the observed alignment.",
            "paper_reference": "See sections: 'How Well Is MLR-Judge Aligned with Human Reviewers?' (agreement study, Fig. 4), 'What Are the Key Factors Affecting AI-Generated Research Quality?' (experiment hallucination, comparison of scores), 'Results of Experimentation Evaluation' (Table 5), and the case study illustrated in Fig. 5; also Related Work discussion referencing ReviewerGPT/OpenReviewer and PaperBench.",
            "uuid": "e9702.0",
            "source_info": {
                "paper_title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing.",
            "rating": 2,
            "sanitized_title": "reviewergpt_an_exploratory_study_on_using_large_language_models_for_paper_reviewing"
        },
        {
            "paper_title": "OpenReviewer",
            "rating": 2,
            "sanitized_title": "openreviewer"
        },
        {
            "paper_title": "Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
            "rating": 2,
            "sanitized_title": "mllmasajudge_assessing_multimodal_llmasajudge_with_visionlanguage_benchmark"
        },
        {
            "paper_title": "Agent-as-a-judge: Evaluate agents with agents.",
            "rating": 2,
            "sanitized_title": "agentasajudge_evaluate_agents_with_agents"
        },
        {
            "paper_title": "PaperBench: Evaluating ai's ability to replicate ai research.",
            "rating": 1,
            "sanitized_title": "paperbench_evaluating_ais_ability_to_replicate_ai_research"
        }
    ],
    "cost": 0.015281,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research
22 Oct 2025</p>
<p>Hui Chen hui.chen@nus.edu.sg 
Miao Xiong miao.xiong@u.nus.edu 
Yujie Lu 
Wei Han 
Ailin Deng 
Jiaying Wu 
Yibo Li 
♦ Yue 
Bryan Hooi </p>
<p>National University of Singapore
University of California
Santa BarbaraSingapore</p>
<p>University of Technology</p>
<p>MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research
22 Oct 20255A25C8F59A13246ABCDA690F8F9A78D4arXiv:2505.19955v3[cs.LG]
Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery.In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research.MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLMbased reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing.Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper.We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results-posing a major barrier to scientific reliability.We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation.We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.</p>
<p>Introduction</p>
<p>A hallmark of human intelligence is the ability to generate new knowledge through research and scientific discovery.From basic research to applied technological development, scientific progress has shaped human civilization.Reproducing this ability-to autonomously generate, test, and validate new knowledge-has long been viewed as one of the grand challenges for artificial intelligence.</p>
<p>Recent advances in large language models (LLMs) bring us closer to this vision.LLM-powered agents have demonstrated impressive capabilities in generating research ideas [16,3,28,17], conducting experiments and analyses [11,34,4,40,13,41,15], drafting scientific articles [25,35,38,1] and automated review [36,14,32,6,42].These advances move us beyond automating isolated tasks, opening up the possibility of automating the entire scientific process, from idea conception to experimentation to dissemination.</p>
<p>Introduction</p>
<p>The integration of deep learning into code generation and assistance has shown remarkable potential …</p>
<p>Methodology</p>
<p>The core of the co-adaptation framework involves the following algorithmic steps …</p>
<p>Expected Outcomes &amp; Impact</p>
<p>The expected outcomes of this research include measurable gains in code correctness … Figure 1: An overview of the framework of MLR-Bench, consisting of both an end-to-end evaluation (left) and a stepwise evaluation (right), each of which uses LLM judges to automatically assess performance over 201 tasks.For end-to-end evaluation, we use the same model as backbone in idea generation, proposal generation and paper writing.For stepwise evaluation, various models are tested and compared within each step.However, realizing this vision raises an important question: how do we rigorously evaluate the quality of research produced by AI agents?While recent works have shown promising results [29,27,19,26,39], the community still lacks a comprehensive benchmark to systematically assess AI agents' ability to conduct open-ended scientific research, making progress toward autonomous scientific discovery difficult to measure and compare fairly between agents.Moreover, there is also a need for empirical analysis to identify key failure modes-such as hallucinated results, lack of novelty, or methodological flaws -to quantify current limitations and inform future progress in research agents.</p>
<p>Toward this vision, we introduce MLR-Bench, a comprehensive benchmark designed for evaluating AI agents on open-ended machine learning research tasks.Specifically, we seek to answer the following research questions:</p>
<p>• RQ1: How well can AI agents conduct open-ended machine learning research?( §3)</p>
<p>• RQ2: How effectively can an LLM judge evaluate research, as measured by its agreement with human reviewers?( §4)</p>
<p>• RQ3: What are the key factors that affect the quality of AI-generated research?( §5)</p>
<p>To systematically address these questions, we decompose the research process into four successive stages: (1) idea generation, (2) research proposal formulation, (3) experimental execution and analysis, and (4) paper writing.This decomposition allows us to assess both stepwise progress and the overall end-to-end research capability of AI agents.</p>
<p>MLR-Bench consists of three key components: (1) a collection of 201 real-world research tasks sourced from NeurIPS, ICLR, and ICML workshops over the past three years, covering a wide range of machine learning domains, including LLMs, trustworthy AI, ML systems, AI for science; (2) MLR-Judge, an automated evaluation pipeline consisting of LLM-based judge and structured review rubrics; and (3) MLR-Agent, a modular research agent scaffold capable of automatically completing these tasks by following the four defined research stages.</p>
<p>To explore RQ1, we use MLR-Bench to evaluate six state-of-the-art models-including the most recent o4-mini [24], Gemini-2.5-pro-preview[8], and Qwen3-235B-A22B [31]-as well as the advanced coding agents, Claude Code [2] and Codex [23].Our results show that while these models perform well in generating ideas and writing papers, they often fail to deliver innovative or scientifically reliable research, mostly due to coding agents producing invalidated experimental results.</p>
<p>To validate the reliability of MLR-Judge (addressing RQ2), we recruit 10 machine learning experts with top-tier conference review experience to independently review the AI-generated research outputs.</p>
<p>We find that the level of agreement between an LLM judge and a human judge, when assessing the same output, is very close to the level of agreement between two human judges, demonstrating its effectiveness as a reliable evaluation tool.Finally, to address RQ3, we analyze the justifications provided by both MLR-Judge and human reviewers, identifying a recurring failure mode where agents tend to generate fabricated or unverified results after execution failures-revealing a gap between fluent output generation and true scientific rigor.</p>
<p>Our contributions include:</p>
<p>• MLR-Bench: To our knowledge, the most comprehensive evaluation benchmark for AI research agents to date, featuring 201 open-ended ML research tasks, a human-aligned MLR-Judge for automated research quality assessment, and a modular MLR-Agent supporting both stepwise and end-to-end research execution.</p>
<p>• Comprehensive Evaluation and Insights: We apply MLR-Bench to evaluate 6 frontier models, providing comparative insights into their strengths and limitations in scientific discovery.Also, we compare MLR-Agent with AI Scientist V2 on MLR-Bench, and find existing research agents exhibit critical failure modes, including the widespread issue of fabricated experimental results and hallucinated methodology, pointing to key challenges for future research.</p>
<p>MLR-Bench</p>
<p>In this section, we describe the overall flow of MLR-Bench.Fig. 1 presents an overview of the framework of MLR-Bench, which consists of both an end-to-end evaluation pipeline to evaluate AI agents' ability to complete end-to-end research, and a stepwise evaluation pipeline, which separately evaluates AI agents' abilities on (1) Idea Generation, (2) Proposal Generation, (3) Experimentation, and (4) Paper Writing.</p>
<p>Tasks.To measure the performance of AI agents across various machine learning topics, we collect 201 tasks from ICLR, ICML, and NeurIPS workshops over the past three years.These tasks are categorized into 9 core ML topics: LLMs and Vision Language Models (VLMs), AI for Science, ML Theory, Trustworthy AI, Computer Vision, ML Systems, Multimodality, Reinforcement Learning (RL), and other emerging topics.Fig. 2 illustrates the distribution of our tasks across these ML topics.</p>
<p>To curate our dataset, we first review all workshops from ICLR, ICML, and NeurIPS conferences over the past three years.We then filter out duplicate workshops and select those that maintain complete information and target a general audience.Finally, we extract workshop overviews and topics to formulate our tasks.</p>
<p>End-to-End Evaluation.To evaluate AI agents' abilities to fully automate the research process, we give them each of these tasks as input and require them to produce the final completed paper as output, which we evaluate using our MLR-Judge (Section 2.1).To facilitate software experimentation, agents are provided with an environment that includes file system access, a Python runtime for executing scripts, and internet connectivity.</p>
<p>Stepwise Evaluation.To assess AI agents' research capabilities in a more fine-grained manner, we evaluate them over 4 steps, using MLR-Judge to assess each step: (1) Idea Generation: we provide the agent with a research task and require it to generate a research idea.(2) Proposal Generation: we provide the agent with a research task and idea, and require it to generate a detailed research proposal.</p>
<p>(3) Experimentation: the agent is given access to an environment for running experiments (similar to in the end-to-end case), and iteratively writes code and runs experiments.(4) Paper Writing: agents are given a full set of experimental outputs (report, figures, and a log of commands run during experimentation), and required to produce the final paper.This task involves multimodal input and output, so we test multimodal agents in this stage.</p>
<p>Each step of the stepwise evaluation requires a set of input data for the agents.For example, in step (1), the Idea Generation agents require research tasks as input data; we use the same 201 tasks used in
Consistency ✓ ✓ ✓ ✓ Clarity ✓ ✓ ✓ ✓ Novelty ✓ ✓ ✓ ✓ Feasibility ✓ ✓ Completeness ✓ ✓ Soundness ✓ ✓ ✓ ✓ Insightfulness ✓ Significance ✓ ✓ ✓ ✓ Overall ✓ ✓ ✓ ✓ ✓
end-to-end evaluation.In step (2) the Proposal Generation agents require tasks and research ideas as input.To construct this dataset, for each of the 201 tasks, we randomly sample one idea (generated by any Idea Generation agent) from step (1) for this task, resulting in 201 (task, idea) pairs that serve as the input for step (2).We apply the same procedure in steps ( 3) and ( 4), each time randomly sampling from the outputs of the previous step for a task, except in step (3) we manually select a subset of 10 (task, idea, proposal) triples based on their suitability for evaluating the experimentation agents, to keep the experiments manageable.Details about these 10 selected tasks can be found in Section A.</p>
<p>MLR-Judge</p>
<p>In this work, we develop an automated evaluation tool, MLR-Judge, which consists of multiple carefully designed rubrics and an LLM judge.Prior to designing these rubrics, we summarize the key review dimensions that people focus on when evaluating research outputs at each step: Consistency, Novelty, Clarity, Feasibility, Completeness, Soundness, Insightfulness, Significance and Overall.We tailor rubrics incorporating multiple review dimensions for each research process based on their distinct characteristics.Table 1 illustrates the review dimensions for different research processes.</p>
<p>Additionally, MLR-Judge incorporates an LLM judge alongside our rubrics.In this work, we select Gemini-2.5-Pro-Previewand Claude-3.7-Sonnetas our judge models, which have demonstrated superior reasoning capabilities and multimodal functionality, enabling them to review papers containing figures.These two judge models independently evaluate the quality of research outputs, and the final evaluation results are averaged from their individual assessments.At each research step, MLR-Judge evaluates the Consistency dimension by analyzing both the history information and the output of MLR-Agent.For the experimentation step, we examine the execution log of our coding agent to gain insights into the experimental process.Details of rubrics and prompts of MLR-Judge can be found in Section D.2.</p>
<p>MLR-Agent</p>
<p>In this work, we develop a simple and flexible agent scaffold, MLR-Agent, to evaluate the capabilities of different models in conducting open-ended research.The agent is implemented to favour simplicity over extensive prompt engineering, allowing us to directly assess the fundamental performance of each model.</p>
<p>MLR-Agent operates in two modes: a stepwise execution mode and an end-to-end execution mode.The stepwise execution mode enables us to evaluate each model's performance in individual research steps, while the end-to-end execution mode allows us to assess the models' capabilities in automating complete research projects.MLR-Agent's automated research process follows the same research steps as illustrated in Fig.   3 How Well Can AI Agents Conduct Open-Ended Research?</p>
<p>This section describes empirical studies of how well AI agents perform on open-ended research.We use MLR-Agent as our agent scaffold to conduct research and evaluate a number of frontier models, as shown in Table 2.In experimentation and analysis step, our coding agents execute on an Ubuntu 22.04 server with access to four NVIDIA RTX 3090 GPUs.We use MLR-Judge to evaluate the outcomes of different research processes.We report the average ratings of two judge models, as mentioned in Section 2.1, and details of each rating can be found in Section C.</p>
<p>Results of Idea Generation Evaluation</p>
<p>All models demonstrate strong performance in Consistency and Significance, while Feasibility and Novelty are more challenging.We test six frontier models on 201 tasks for idea generation and Table 3 presents the evaluation results.We find that all models demonstrate strong performance, especially in Consistency and Significance, indicating their capability to generate logically coherent and meaningful research ideas.However, there exists a notable performance gap in Novelty and Feasibility, suggesting that generating innovative yet implementable ideas remains a significant challenge for current LLMs.</p>
<p>Model size may not be the sole determinant of idea generation quality.The results show that Deepseek-R1 [7] achieves the highest score in Consistency and Overall performance, while Gemini-2.5-Pro-Previewexcels in Clarity and Feasibility.Qwen3-235B-A22B demonstrates particular strength in Novelty and Significance.Notably, even the smaller Ministral-8B [30] model achieves competitive performance in Feasibility, suggesting that model size may not be the sole determinant of idea generation quality.</p>
<p>Results of Proposal Generation Evaluation</p>
<p>All models excel at generating logically coherent research proposals with research significance, but novelty, soundness and feasibility are more challenging.We also evaluate six models on 201 tasks for proposal generation and results are presented in Table 4.The evaluation results reveal a clear pattern in model performance across different dimensions.All six frontier LLMs achieve consistently high scores in Consistency and Significance, demonstrating their strong capability in maintaining logical coherence and research relevance.However, scores for Feasibility and Novelty are notably lower, with scores falling below 7.5 in most cases.This performance gap suggests that models face significant challenges in creating innovative yet implementable proposals.</p>
<p>Large reasoning models present greater capabilities in generating quality proposals.The results shows a clear correlation between model size and performance in proposal generation.The larger models (Gemini-2.5-Pro-Preview,o4-mini-high, Claude-3.7-Sonnet,and Qwen3-235B-A22B) consistently outperform the smaller Ministral-8B model across all evaluation dimensions.The superior performance of larger reasoning models in generating quality proposals is further supported by their higher overall scores, indicating that model scale and reasoning capabilities are crucial factors in proposal generation.</p>
<p>Results of Experimentation Evaluation</p>
<p>Popular coding agents cannot yet produce solid experimental results.We evaluate the experimental execution by comparing Claude Code and Codex on 10 representative machine learning tasks selected from the 201 tasks.Our LLM judge assesses both the execution records and the final results.As presented in Table 5, the overall ratings for both agents fall below the 6.0 acceptance threshold, indicating that current coding agents still struggle to generate robust experimental outcomes.Specifically, while Claude Code achieves respectable scores for Consistency, Completeness, and Novelty, it falters on Soundness, Insightfulness, and Significance.This weakness is detailed in Table 5 and further highlighted in Fig. 3, where the Gemini judge's scores for its Soundness and Significance are below 3.0.This suggests that although Claude Code can devise and execute complete and novel experiments, the results often lack scientific robustness.In contrast, Codex performs well on Soundness but underperforms on Novelty, Insightfulness, and Significance.This suggests that while Codex is reliable in executing experiments, it lacks the capability to design novel ones.</p>
<p>Results of Paper Writing Evaluation</p>
<p>Gemini-2.5-Pro-Previewpresents superior performance in paper writing.We evaluate three models with multimodal capabilities on the same ten tasks as mentioned in Section 3.3 in paper writing.</p>
<p>The evaluation results presented in Table 6 indicate that Gemini-2.5-Pro-Previewdemonstrates stronger writing capabilities, while o4-mini-high shows relatively weaker performance.From manual observation, we believe this is partly due to Gemini-2.5-Pro-Preview'sdetailed formal explanations, including algorithms, formulas, and derivations, along with its strong mathematical capabilities, while o4-mini-high performs worse due to its relatively terse style affecting its clarity.</p>
<p>However, none of the models achieve high overall scores (above 7.0), which may be influenced by the weaker experimental results from the previous step.This demonstrates that experimental success is a critical determinant of overall research quality.</p>
<p>Results of End-to-End Evaluation</p>
<p>Current research agents struggle to generate robust scientific results.We conduct an end-to-end evaluation to compare three reasoning models within our MLR-Agent scaffold and benchmark them against AI Scientist V2 [39], with results summarized in Table 7.Our findings reveal two key points.Firstly, when using the MLR-Agent scaffold, the overall ratings for all three models fall below the 6.0 acceptance threshold.Notably, all models receive their lowest scores on Soundness, which underscores their shared difficulty in producing scientifically sound results.Secondly, in a direct model comparison, o4-mini is significantly outperformed by Gemini and Claude, whose performances are comparable.This trend is inversely correlated with cost.When using Claude Code as the coding agent, o4-mini is the most affordable, followed by Gemini.Consequently, Gemini emerges as the most cost-effective model for end-to-end autonomous research, considering its strong performance and moderate cost.AI Scientist V2 outperforms MLR-Agent, yet both share fundamental limitations.We further compare MLR-Agent with AI Scientist V2 in Table 7.Our comparison between MLR-Agent and AI Scientist V2, both using o4-mini as the backbone model, reveals that AI Scientist V2 consistently outperforms MLR-Agent across all review dimensions.However, neither agent meets the 6.0 acceptance threshold.A closer analysis of their detailed scores highlights three key findings:</p>
<p>• Both agents excel at generating novel research outcome clearly.Both agents demonstrate strong creativity, reflected in high scores for Novelty.They can propose original research questions crucial for scientific discovery and articulate these ideas clearly, as shown by their strong performance in the Clarity dimension.• Agents struggle with technical soundness.A primary weakness is their inability to propose technically sound methods.This is evident from their markedly low scores in Soundness, which consistently fall below the acceptance threshold.This suggests that while the agents can formulate creative ideas, their technical execution is frequently flawed.• The agents' research demonstrates limited potential impact.This limitation is reflected in their consistently low scores for Significance, suggesting their work often fails to address important research questions.This perceived lack of contribution, potentially compounded by reproducibility challenges, severely restricts the research's influence and uptake by the scientific community.</p>
<p>How Well Is MLR-Judge Aligned with Human Reviewers?</p>
<p>To evaluate whether our proposed LLM-based judge, MLR-Judge, can serve as a reliable proxy for human reviewers, we conduct a human evaluation study with ten domain experts who have served as reviewers for major machine learning conferences such as NeurIPS, ICLR, or ICML.For each generated research paper, we assign two independent human reviewers from this expert pool to assess the paper's quality using the same evaluation rubrics as the LLM judge.Specifically, all reviewers-whether human or MLR-Judge-follow the same review criteria, covering five dimensions: Clarity, Novelty, Soundness, Significance, and Overall.The detailed evaluation rubrics are provided in Table 28 and the process is introduced in Section E.</p>
<p>Absolute Difference</p>
<p>Overall (p-value: 0.981) Figure 4: Comparison of human-human and human-LLM absolute rating differences across five criteria, with corresponding Mann-Whitney U test p-values shown in the top-left panel.This suggests that the differences between the LLM and human reviewers are not significantly larger than those between two human reviewers.</p>
<p>Agreement Score.We conduct a significance test (Mann-Whitney U test) to test how well the LLM judge's scores agree with human judges' scores, compared to the level of agreement between a pair of human judges.Specifically, we compute (1) absolute score differences between the LLM judge and human reviewers, and (2) absolute score differences between pairs of human reviewers, and test whether these two distributions are statistically different.As shown in Fig. 4, none of the five evaluation criteria show statistically significant differences at the 0.05 confidence level.This suggests that the differences between the LLM and human reviewers are not significantly larger than those between two human reviewers, supporting the hypothesis that LLM and human judgments are well-aligned.Furthermore, the visualized distributions of the rating differences across all five criteria appear highly similar.These findings suggest that the LLM judge produces evaluations largely consistent with human judgment, demonstrating its potential as a scalable solution for automatic evaluation in open-ended machine learning research.</p>
<p>What Are the Key Factors Affecting AI-Generated Research Quality?</p>
<p>We analyze the scoring justification provided by MLR-Judge and human reviewers to identify key factors that affect AI-generated research quality.Among these factors, two stand out as particularly critical: experiment results hallucination and lack of novelty in ideas.We conduct case studies to further investigate why these issues arise and how they impact the overall research quality.</p>
<p>Experiment Results Hallucination.Our analysis reveals a recurring failure mode in AI-generated research: producing unvalidated experimental results.For example, in 8 out of 10 tasks conducted by Claude Code, the reported results were based on synthesized or placeholder data rather than actual execution, as pointed out by our MLR-Judge.This is further supported by the low soundness score: on a scale of 1 to 10, the LLM judge assigned an average score of 3.73 out of 10, while human reviewers gave a slightly higher but still unsatisfactory score of 4.42 out of 10.</p>
<p>Both human reviewers and MLR-Judge identified these issues.For example, human reviewers frequently flagged unreasonable outputs inconsistent with common sense, such as "random sampling results should be close to 0.5, but the paper shows 0.65, which seems fabricated".Unlike human reviewers, who primarily focused on reading the paper content, MLR-Judge leveraged access to the : Run the experiment with reduced epochs… : ImportError: cannot import name … : "type": "tool_result", "is_error": True : Let's address any import errors in our scripts : Let's directly run a simpler version of the experiment.We will create a simulated results file and visualization.</p>
<p>MLR-Agent MLR-Judge</p>
<p>Serious concerns about the validity of experimental results, as the code reveals they are generated using placeholder data rather than actual experiments.</p>
<p>The experimental results are ridiculous.The proposed approach achieves 0% acc.In Table 2, the R^2 scores of -2940947910.5451to -1697885.2574signify arbitrarily large prediction error (and worse performance than a very naive baseline).</p>
<dl>
<dt>Human Reviewer</dt>
<dd>
<p>python run_experiment.py--epochs 5 ❌ ❌ Figure 5: For a given paper with its supplementary code, both MLR-Judge and the human reviewer flagged the results as invalid-MLR-Judge by inspecting the provided code, and the human reviewer by noting unrealistic R 2 scores.To understand why the results were invalid, we examined the execution logs of MLR-Agent and found that the coding agent failed to run the experiment and instead took a shortcut by generating simulated results, prioritizing completeness over correctness.supplementary code and detected even more hallucination cases by inspecting the execution logs and code traces of the coding agent, consistently assigned low scores on Soundness, Insightfulness, Significance, and Overall dimensions, as illustrated in Figure 3.</p>
</dd>
</dl>
<p>To further understand the root cause of these failures, we examined the agent's execution traces.As illustrated in the case study in Figure 5, these issues often arise when the coding agent encounters execution failures, such as runtime errors or unresolved dependencies.Instead of reporting these failures or halting the process, the agent tends to take shortcuts by generating synthetic results to fill in the gaps.Alarmingly, this behavior persists even when the agent is explicitly instructed not to fabricate results.We hypothesize that the coding agent, particularly Claude Code, prioritizes producing seemingly complete and error-free outputs, and has learned to bypass computational challenges by generating plausible-looking-but ultimately invalid-results as a coping strategy.This behavior highlights a critical limitation of current coding agents in lacking the ability to robustly handle complex execution environments.More importantly, they fail to accurately communicate when they are unable to execute tasks successfully, instead masking failures with fabricated results.This shortcut-taking behavior not only undermines the scientific validity of the generated research but also poses a risk to user trust in AI-assisted research workflows.</p>
<p>Lack of Novelty.Our analysis reveals that many AI-generated research papers exhibit a common failure mode: presenting ideas that are superficial combinations of existing methods without addressing any new research challenge.For example, in one generated paper, the model proposes to combine self-consistency sampling with token-level uncertainty estimation.However, the paper fails to articulate why this combination is meaningful, how these two techniques interact, or what specific problem this integration solves.Both human and LLM judges consistently flagged such cases with low novelty scores.Human reviewers described the idea as "a trivial combination lacking clear motivation," while MLR-Judge similarly penalized these papers for lacking insightfulness and significance.Addressing this limitation requires developing agents that not only generate ideas but also reason about their relevance, feasibility, and contribution to advancing the field.</p>
<p>Related Work</p>
<p>Benchmarks for Autonomous Research Agents.Recent works [25,20,43] have explored using LLMs for various stages of scientific research, including idea generation [28,10,16], experimentation [11,4,33,21], and writing [20,38,35].Existing benchmarks typically target narrow tasks: the MLE benchmark [4] focuses on engineering, the MLAgentBench [11] on experimentation, and the PaperBench [29] on paper reproduction, each revealing gaps between AI agents and human researchers [4,11,29].RE-Bench [37] pushes toward generalization on unseen tasks [37].Our proposed MLR-Bench complements these efforts by evaluating 201 open-ended research tasks from major Machine Learning conferences, covering the entire research pipeline.By combining modular agent scaffolds, human-aligned LLM judges, and diverse real-world tasks, MLR-Bench offers the comprehensive benchmark for diagnosing and improving end-to-end research automation.</p>
<p>LLMs as Reviewers for Scientific Research.LLMs have also been explored as automated reviewers.Prior works like ReviewerGPT [18] and OpenReviewer [12] show that LLMs can perform specific reviewing sub-tasks, but tend to produce overconfident and unreliable overall judgments [18,12].More structured efforts, such as PaperBench [29], demonstrate that fine-tuned LLM judges can align well with human reviewers on benchmark tasks [29].Building on these insights, MLR-Bench introduces MLR-Judge, a rubric-based LLM judge [44,5,45] validated against expert human ratings.</p>
<p>Our study shows that MLR-Judge can be a proxy of human reviewers, enabling scalable and reliable evaluation of large-scale AI-generated research.</p>
<p>Conclusion</p>
<p>In this work, we introduce MLR-Bench, a comprehensive framework for evaluating AI research agents on open-ended machine learning research.MLR-Bench consists of two key components:</p>
<p>(1) a collection of 201 real-world research tasks covering diverse ML topics, and (2) MLR-Judge, an LLM-based evaluation framework with structured review rubrics for automated assessment of research quality.To demonstrate the utility of MLR-Bench, we also introduce MLR-Agent, a modular agent scaffold that supports both stepwise and end-to-end research generation, enabling systematic evaluation of different models and research workflows.We release MLR-Bench and MLR-Judge as open resources to support the community in benchmarking and improving AI research agents, with the goal of advancing trustworthy and scientifically meaningful AI systems.</p>
<p>Limitation and Future Work.A key barrier to trusting AI-generated research lies in its lack of process transparency.Scientific research is inherently complex, involving many subtle decisions and details.Human reviewers, when presented with a fully-formed AI-generated paper, often lack visibility into how each part was produced and whether every step is scientifically sound.This makes it difficult to build trust in AI research agents.While our evaluation framework takes a step toward addressing this, we recognize that building human trust in fully automated research remains a long-term and open challenge.We view MLR-Bench not as a complete solution, but as an important first step to help the community systematically analyze, diagnose, and improve AI research agents.</p>
<p>Beyond evaluation, we believe that MLR-Bench and MLR-Judge have the potential to serve as valuable feedback signals for improving research agent training.By identifying where agents succeed or fail-such as generating plausible but fabricated results or failing to produce meaningful contributions-our framework can inform the design of better training objectives, reward signals, and alignment strategies.We envision future work that closes this feedback loop, integrating MLR-Judge as part of the training and refinement process for next-generation research agents, ultimately improving their reliability, transparency, and scientific value.</p>
<p>NeurIPS Paper Checklist</p>
<p>Claims</p>
<p>Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
<p>Answer: [Yes] Justification: Our abstract has illustrated that this work aims to provide a benchmark for evaluating AI agents on open-ended machine learning research.We collect 201 tasks and test several frontier models in our benchmark.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p>
<p>Limitations</p>
<p>Question: Does the paper discuss the limitations of the work performed by the authors?</p>
<p>Answer: [Yes] Justification: In Limitations section.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.</p>
<p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.</p>
<p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p>
<p>Theory assumptions and proofs</p>
<p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
<p>Answer: [NA] Justification: This work focuses on empirical studies and there are no theoretical results.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental result reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
<p>Answer: [Yes] Justification: We illustrated our main experiments in section 3.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
<p>Answer: [Yes]</p>
<p>Justification: We open-sourced our codebase and dataset at https://github.com/chchenhui/mlrbench and https://huggingface.co/datasets/chchenhui/ mlrbench-tasks.</p>
<p>Guidelines:</p>
<p>• The answer NA means that paper does not include experiments requiring code.• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
<p>Experimental setting/details</p>
<p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
<p>Answer: [Yes] Justification: MLR-Bench section illustrates our framework.More details can be found in Appendix.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment statistical significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Answer: [Yes] Justification: See section 4.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>
<p>• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.</p>
<p>• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Answer: [Yes] Justification: We discuss these in Ethical Considerations.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<p>Licenses for existing assets</p>
<p>Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
<p>Answer: [Yes]</p>
<p>Justification: We have cited the original paper or technical report that produced the code package or dataset.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.</p>
<p>• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [Yes]</p>
<p>Justification: We have uploaded our dataset to huggingface.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and research with human subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Answer: [Yes]</p>
<p>Justification: We have human evaluation and details can be found in this paper.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
<p>Institutional review board (IRB) approvals or equivalent for research with human subjects</p>
<p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
<p>Answer: [Yes] Justification: Yes.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.</p>
<p>• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.for what should or should not be described.</p>
<p>B Hallucination Analysis of Existing Research Agents</p>
<p>To further investigate the issues in the content generated by AI Scientist V2 and MLR-Agent, we conducted a hallucination analysis in this section.Specifically, we aim to examine how frequently these research agents (i.e., AI Scientist V2 and MLR-Agent) produce hallucinated content, which types of hallucinations are most common, and how these errors are typically generated.</p>
<p>We observe that AI-generated research papers can fail to be trustworthy in several ways: for example, when the proposed method fails to address the stated motivation, or when the conclusions do not logically follow from the methodology.Such issues may stem either from intentional fabrication or from limitations in the model's reasoning ability.To focus on objectively verifiable errors, we propose four frequently observed, fact-based hallucination types:</p>
<p>• Faked Experimental Results: The data, metrics, or experiments' outcomes are fabricated or never actually performed.</p>
<p>• Hallucinated Methodology: The technical approaches are proposed but are not implemented (e.g., claiming that the proposed method uses reinforcement learning when it actually does not).</p>
<p>• Incorrect Citations: The references to academic papers are incorrect or cannot be found.</p>
<p>• Mathematical Errors: Incorrect equations, flawed derivations, or improper applications of mathematical concepts.</p>
<p>Faked Experiment Results</p>
<p>Hallucinated Methodology</p>
<p>Incorrect Citations</p>
<p>Example:</p>
<p>Section 1: "Evaluation on SQuAD, AmbigQA, and TriviaQA-rc, showing up to 6% absolute EM gains and 30% fewer hallucinations."</p>
<p>Explanation:</p>
<p>The paper claims specific performance improvements on SQuAD, AmbigQA, and TriviaQA-rc datasets, but the code simply assigns perfect accuracy on AmbigQA, without actually running real models on these datasets.</p>
<p>Example:</p>
<p>"Our LLM uses MC-dropout to flag uncertain tokens, generates targeted clarification questions, and proceeds with retrieval and answer generation only after disambiguation."</p>
<p>Explanation:</p>
<p>The experiments do not actually implement MC-dropout on GPT-3.5 but instead simulate uncertainty detection and clarification triggers using predefined rules and random thresholds.</p>
<p>Example: The arXiv IDs cited in the paper are non-existent and these two "paper title + author" combinations cannot be found.
"[7] M.
Figure 7: Illustrative hallucination cases extracted from papers generated by AI Scientist V2 and MLR-Agent.The case of "Mathematical Errors" is not reported due to its low frequency of occurrence.</p>
<p>To efficiently analyze the hallucination content, we first leverage two advanced automated judges-Gemini (Gemini-2.5-Pro-Preview-06-05)and Claude (Claude-3-7-Sonnet-20250219) to identify whether each generated paper contains any of these four hallucination types.Then the judgments are verified by our human annotators.To reduce the annotation burden, human annotators are provided with two review files, one from the Gemini judge and one from the Claude judge, which contain AI-detected hallucination types and their corresponding evidence.For each of these types, the annotators examine the provided evidence to verify whether these kinds of hallucination evidence are present in the AI-generated paper.</p>
<p>Key Observations.Fig. 6 reports the frequency of hallucination types across 10 benchmark tasks and Fig. 7 presents representative examples of each type extracted from papers generated by AI Scientist V2 and MLR-Agent.We have two key observations.Firstly, faked experimental results and hallucinated methodology are the two most prevalent hallucination types in the outputs of both research agents, with each type appearing in more than half of the 10 evaluated tasks.Notably, almost all papers generated by AI Scientist V2 contain these two types of hallucination.As illustrated in Fig. 7, research agents may fabricate near-perfect results to achieve their research objectives, and in some cases, their implementations are not aligned with the proposed method and experimental setup.One possible reason is that the generated ideas lack feasibility, leaving the coding agents unable to utilize available resources to achieve the experimental objectives.The core limitation, however, is that the agents possess no mechanism to verify the practicality of an idea beforehand and flag it.</p>
<p>Secondly, nonexistent citations are a significant issue, appearing in 50% of the tasks completed by MLR-Agent.While AI Scientist V2 also exhibits this problem, the prevalence is less severe.We hypothesize that this discrepancy stems from limitations in MLR-Agent's literature collection tool, which restricts its ability to conduct accurate searches for relevant literature on the web.Currently, relying solely on a few model APIs with search capabilities for literature gathering does not guarantee the quality of the retrieved information.Therefore, future research agent development needs to focus on two key areas.First, it is crucial to enhance the interaction between agents and the web to improve information retrieval.Second, a vetting mechanism must be implemented to ensure the validity and relevance of academic information collected online.</p>
<p>C Evaluation Results of Different Judge Models</p>
<p>This section shows the evaluation scores of different models judged by Gemini and Claude models.We report the results of MLR-Agent in idea generation (see Tables 9 and 10), proposal generation (see Tables 11 and 12), paper writing (see Tables 13 and 14) and end-to-end evaluation (see Tables 15  and 16).For the end-to-end evaluation, we have attached the experimental code along with each sample.In addition, Table 17 and Table 18 present the performance comparison of AI Scientist V2 and MLR-Agent.</p>
<p>D Prompts</p>
<p>In this section, we present the prompts used throughout the MLR-Bench pipeline.Fields highlighted in {blue} indicate parts that require user-specified input or content specified in another place.To make the process more intuitive, we also include example inputs and outputs where helpful.In cases where the content is lengthy, we use ellipses (". . .") to omit parts of the input or output for clarity.The automated research process includes four stages: (1) Idea Generation, (2) Proposal Generation, (3) Experimentation, and (4) Paper Writing.Since most models lack web access, we insert a literature review step using GPT-4o-Search-Preview [22] between stages (1) and ( 2), providing relevant reference material to the agent.</p>
<p>Below are the prompts used for each stage:</p>
<p>• Table 19 presents the prompt used for idea generation.</p>
<p>• Table 20 presents the prompt for instructing the model to perform a literature review.</p>
<p>• Table 21 presents the prompt used for generating the research proposal.</p>
<p>• Table 22 presents the prompt used for conducting experiments and obtaining results.</p>
<p>• Table 23 presents the prompt used for writing the research paper based on previously generated outputs, including the idea, proposal, and experimental results.</p>
<p>Table 19: Prompt used for idea generation.Users provide a task description (in this example, the introduction from the Building Trust workshop), which is combined with the prompt to instruct the backbone model (e.g., Claude) to generate a corresponding research idea.</p>
<p>Task # Workshop on Building Trust in Language Models and Applications Input</p>
<p>As Large Language Models (LLMs) are rapidly adopted across diverse industries, concerns around their trustworthiness, safety, and ethical implications increasingly motivate academic research, industrial development, and legal innovation...This workshop addresses the unique challenges posed by the deployment of LLMs, ranging from guardrails to explainability to regulation and beyond.The proposed workshop will bring together researchers and practitioners from academia and industry to explore cutting-edge solutions for improving the trustworthiness of LLMs and LLM-driven applications.The workshop will feature invited talks, a panel discussion, interactive breakout discussion sessions, and poster presentations, fostering rich dialogue and knowledge exchange.We aim to bridge the gap between foundational research and the practical challenges of deploying LLMs in trustworthy, use-centric systems.</p>
<h2>Workshop Scope</h2>
<p>This workshop has a broad focus, including but not limited to: As Large Language Models (LLMs) are increasingly integrated into high-stakes applications, their tendency to hallucinate, present false information, or make errors poses significant risks.Current error detection methods often lack transparency, making it difficult for users to understand when and why an LLM might be incorrect.This research addresses the critical need for systems that can not only detect errors in LLM outputs but also explain these errors to users and suggest corrections in a transparent, understandable manner.</p>
<h2>Main Idea</h2>
<p>TrustPath proposes a multi-layered approach to error detection and correction that prioritizes transparency.The framework employs three complementary mechanisms: 1) A self-verification module that prompts the LLM to evaluate its own uncertainty through generated alternative responses and explanations; 2) A factual consistency checker that retrieves information from trusted knowledge sources to verify claims; and 3) A human-in-the-loop feedback system that learns from user corrections.Uniquely, TrustPath presents detected errors alongside confidence scores, reasoning paths, and suggested alternatives through an intuitive visual interface that highlights potential problematic sections of text.This approach not only improves accuracy but builds user trust by making the error detection process visible and understandable, allowing users to make informed decisions about accepting or rejecting LLM outputs.-The scripts should be able to run all baseline methods automatically.-The scripts should be able to save the results in a structured format (e.g., CSV or JSON) for easy analysis.</p>
<p>-The scripts should generate figures to visualize the results.Figures should be generated for the main results, including but not limited to:</p>
<p>-Training and validation loss curves -Performance metrics (e.g., accuracy, F1 score) over time -Comparison of the proposed method with baseline methods -Any other relevant figures that help to understand the performance of the proposed method -Make sure that the figures are properly labeled and include legends, titles, and axis labels.There should be data points in the figures, and the figures should be easy to read and interpret.3. Write a README.mdfile to explain how to run the experiment.4. Run the experiment automatically, visualize the results, and save the results.Make sure the generated figure files are not empty.5. Save the experiment execution process in log.txt.6. Analyze and summarize the experiment results in results.mdafter all experiments (including all baselines) have been successfully completed.</p>
<p>-Tables should be generated for the main results, including but not limited to: -Summary of the experimental setup (e.g., hyperparameters, dataset splits) -Comparison of the proposed method with baseline methods -Any other relevant tables that help to understand the performance of the proposed method 7. Finally, create a folder named 'results' under this folder and move results.md,log.txt, and all figures generated by the experiment into 'results'.</p>
<p>IMPORTANT:</p>
<p>-Do not use synthetic results or generate any fake data.The results should be based on real experiments.If the experimental dataset is too large, please use a smaller subset of the dataset for testing purposes.</p>
<p>-If the experiment is about large language models (LLMs), please confirm the models used are LLMs and not simple multi-layer neural networks such as LSTM.</p>
<p>-You can download the datasets from Hugging Face datasets or other open-sourced datasets.Please do not use any closed-sourced datasets.</p>
<p>-If the experiment requires using open-sourced large language models (LLMs) such as Meta Llama-3.1-8B,Llama-3.2-1B,Mistral Ministral-8B or Qwen Qwen3-0.6B and multimodal LLMs like Qwen2-VL-3B, please download the models from the Hugging Face model hub.Due to limited computing resources, please use the models no larger than 8B.</p>
<p>-If the experiment requires using closed-sourced large language models (LLMs) such as OpenAI's GPT-4o-mini, o4-mini or Claude-3.7-sonnet,please directly use the API provided by the model provider.API keys have already been provided in the environment variables.Please use the API key to access the model and run the experiment.</p>
<p>-Only save essential files that are necessary for the experiment.Do not create or save any unnecessary files, temporary files, or intermediate files.Keep the output minimal and focused on the core experiment requirements.</p>
<p>-No need to generate any paper or academic document.Focus only on the experimental implementation and results.</p>
<p>-Please remove checkpoints or datasets larger than 1MB after the experiment is completed.</p>
<p>Remember to analyze and summarize the experiment results, create a folder named 'results' under this folder and move results.md,log.txt, and all figures generated by the experiment into 'results'.</p>
<p>-Figures and tables generated by the experiment should be included in the results.mdwith clear explanations of what they show.Please make sure the generated figures are not repeated.Please do not use the same figures multiple times in results.md.</p>
<p>-Make sure paths to the figures in results.mdare correct and point to the right locations.</p>
<p>-The results.mdshould also include a discussion of the results, including any insights gained from the experiment and how they relate to the hypothesis.</p>
<p>-The results.mdshould include a summary of the main findings and conclusions drawn from the experiment.</p>
<p>-The results.mdshould also include any limitations of the experiment and suggestions for future work.MLR-Judge combines a set of carefully designed review rubrics with an LLM-based evaluator.It supports both an end-to-end evaluation pipeline-assessing an AI agent's ability to complete a full research project-and a stepwise evaluation pipeline that independently evaluates performance on (1) Idea Generation, (2) Proposal Generation, (3) Experimentation, and (4) Paper Writing.In the following sections, we describe the specific rubrics used for each stage as well as for end-to-end evaluation.We then present the prompt used to operate the MLR-Judge.</p>
<p>D.2.1 Review Rubrics</p>
<p>We provide the rubrics used to guide evaluation for each stage:</p>
<p>• The rubrics used to evaluate the idea generation quality are shown in Table 26.</p>
<p>• The rubrics used to evaluate the research proposal quality are shown in Table 25.</p>
<p>• The rubrics used to evaluate the experimentation quality are shown in Table 26.</p>
<p>• The rubrics used to evaluate the paper writing quality are shown in Table 27.</p>
<p>• The rubric used for holistic paper evaluation is shown in Table 28.</p>
<p>Table 24: Rubrics used for idea generation.</p>
<ol>
<li>CONSISTENCY (1-10) How well does the idea align with the requirements of the task description?9-10 -Excellent: The idea is perfectly aligned with the task description.It addresses all aspects of the task and is highly relevant.</li>
</ol>
<p>7-8 -Good: The idea is mostly aligned with the task description.It addresses most aspects but may miss some minor details. 5-6 -Satisfactory: The idea is somewhat aligned with the task description.It addresses some aspects but misses several key points.</p>
<p>3-4 -Needs Improvement: The idea is poorly aligned with the task description.It addresses only a few aspects and misses many key points.</p>
<p>1-2 -Poor: The idea is not aligned with the task description.It does not address the task or is completely irrelevant.</p>
<p>CLARITY (1-10)</p>
<p>How clear and well-defined is the research idea?9-10 -Excellent: The idea is crystal clear, perfectly defined, and immediately understandable.It is articulated concisely with no ambiguity.7-8 -Good: The idea is mostly clear and well-articulated with only minor ambiguities.Minor refinements would make it even more precise.5-6 -Satisfactory: The idea is partially clear but has some ambiguities.Some aspects need further elaboration for a complete understanding.3-4 -Needs Improvement: The idea is unclear with significant ambiguities.Major clarification is needed to make it comprehensible.1-2 -Poor: The idea is extremely unclear, vague, or ambiguous with no proper explanation.It is difficult to understand or interpret.</p>
<p>NOVELTY (1-10)</p>
<p>How original and innovative is the research idea?9-10 -Excellent: The idea is highly original and innovative.It introduces a groundbreaking concept or approach that is significantly different from existing work.7-8 -Good: The idea has notable originality and innovation.It offers fresh perspectives or new combinations of existing concepts.5-6 -Satisfactory: The idea has some originality and innovation.It includes some novel aspects but also shares similarities with existing approaches.3-4 -Needs Improvement: The idea has minimal originality.It largely resembles existing approaches with only slight variations.1-2 -Poor: The idea lacks originality and innovation.It closely resembles common or existing concepts without any new insights.</p>
<p>FEASIBILITY (1-10)</p>
<p>How practical and implementable is the research idea?9-10 -Excellent: The idea is highly practical and implementable with current resources, technology, and knowledge.Execution is straightforward.7-8 -Good: The idea is largely feasible with existing technology and methods, though it may require moderate refinement or optimization.5-6 -Satisfactory: The idea is somewhat feasible but has some implementation challenges.It may require considerable effort or resources to implement.3-4 -Needs Improvement: The idea has significant implementation challenges.Major revisions would be needed to make it feasible.1-2 -Poor: The idea is impractical or impossible to implement with current technology, knowledge, or constraints.</p>
<p>SIGNIFICANCE (1-10)</p>
<p>How important and impactful is the research idea?9-10 -Excellent: The idea is highly significant and impactful.It addresses a critical problem and could lead to major advancements in the field.7-8 -Good: The idea is significant and has clear impact potential.It addresses an important issue and could lead to meaningful contributions.5-6 -Satisfactory: The idea is somewhat significant.It addresses a relevant problem but its impact may be moderate or limited to a specific area.3-4 -Needs Improvement: The idea has limited significance.It addresses a minor issue or has a narrow scope with minimal impact.1-2 -Poor: The idea has little to no significance.It does not address a meaningful problem or offer any clear benefits.</p>
<p>Overall Assessment (1-10)</p>
<p>How would you rate the research idea overall, considering all five dimensions above? 10 -Outstanding: The idea is exceptional in every respect, with no significant weaknesses.8-9 -Excellent: The idea is very strong overall, with only minor weaknesses.6-7 -Good: The idea is solid, with a good balance of strengths and weaknesses.4-5 -Satisfactory: The idea is adequate but has notable weaknesses.2-3 -Needs Improvement: The idea has significant weaknesses that limit its potential.1 -Poor: The idea is fundamentally flawed across most or all dimensions.</p>
<p>When assigning the Overall Assessment score, consider not just the average of the five dimensions, but also: -Whether any single weakness is critical enough to lower the overall potential.</p>
<p>-The overall coherence and integration of the idea.</p>
<p>-The likelihood of real-world impact if the idea were pursued.</p>
<p>-The degree to which the idea fulfills the task description as a whole.</p>
<p>-Any unique strengths or fatal flaws that are not fully captured by the individual dimensions.</p>
<p>Table 25: Rubrics used for proposal generation.</p>
<ol>
<li>
<p>CONSISTENCY (1-10) How well does the proposal align with the requirements of the task description, the research idea, and the literature review?9-10 -Excellent: The proposal is perfectly aligned with the task description, research idea, and literature review.It addresses all aspects comprehensively and demonstrates a deep understanding of the context and prior work.There are no inconsistencies or gaps.7-8 -Good: The proposal is mostly aligned with the task description, research idea, and literature review.It addresses most aspects, with only minor omissions or inconsistencies that do not significantly affect the overall coherence.5-6 -Satisfactory: The proposal is somewhat aligned.It addresses some key aspects of the task description, research idea, and literature review, but misses several important points or contains moderate inconsistencies.3-4 -Needs Improvement: The proposal is poorly aligned.It addresses only a few aspects of the task description, research idea, or literature review, and contains significant inconsistencies or gaps that undermine its coherence.1-2 -Poor: The proposal is not aligned with the task description, research idea, or literature review.It fails to address the requirements or is completely irrelevant, with major inconsistencies or contradictions.</p>
</li>
<li>
<p>CLARITY (1-10) How clear and well-defined is the research proposal?9-10 -Excellent: The proposal is crystal clear, perfectly defined, and immediately understandable.All objectives, methods, and rationales are articulated concisely with no ambiguity.The structure is logical and easy to follow.7-8 -Good: The proposal is mostly clear and well-articulated, with only minor ambiguities or areas that could benefit from slight refinement.The main points are understandable and the structure is generally logical.5-6 -Satisfactory: The proposal is partially clear but has some ambiguities or unclear sections.Some aspects require further elaboration or clarification for a complete understanding.The structure may be somewhat disjointed.3-4 -Needs Improvement: The proposal is unclear with significant ambiguities or confusing sections.Major clarification is needed to make the objectives, methods, or rationale comprehensible.The structure is difficult to follow.1-2 -Poor: The proposal is extremely unclear, vague, or ambiguous with no proper explanation.It is difficult to understand or interpret, and the structure is disorganized or incoherent.</p>
</li>
</ol>
<p>NOVELTY (1-10)</p>
<p>How original and innovative is the research proposal?9-10 -Excellent: The proposal is highly original and innovative.It introduces a groundbreaking concept, method, or perspective that is significantly different from existing work in the literature.The novelty is clearly articulated and well-justified.7-8 -Good: The proposal demonstrates notable originality and innovation.It offers fresh perspectives or new combinations of existing concepts, with clear distinctions from prior work, though it may not be entirely groundbreaking.5-6 -Satisfactory: The proposal has some originality and innovation.It includes novel aspects but also shares similarities with existing approaches.The differences from prior work are present but not strongly emphasized.3-4 -Needs Improvement: The proposal has minimal originality.It largely resembles existing approaches in the literature, with only slight variations or incremental improvements.1-2 -Poor: The proposal lacks originality and innovation.It closely follows common or existing concepts without any new insights, and does not distinguish itself from prior work.</p>
<p>SOUNDNESS (1-10)</p>
<p>How well-founded and rigorous is the research proposal?9-10 -Excellent: The proposal is highly sound and rigorous.It is based on solid theoretical foundations, well-established methods, and comprehensive literature review.The proposed methodology is robust and well-justified.Technical formulations are fully correct and clearly presented.7-8 -Good: The proposal is sound and mostly rigorous.It is based on solid foundations and established methods, though it may have minor gaps or areas that require further justification.The methodology is generally well-defined.Technical formulations are mostly correct, with minor errors or omissions.5-6 -Satisfactory: The proposal is somewhat sound but has some gaps or weaknesses in its theoretical foundations or methodology.It may rely on assumptions that are not fully justified.The proposed methods are acceptable but may lack rigor.Technical formulations have some errors or unclear aspects.</p>
<p>3-4 -Needs Improvement: The proposal has significant weaknesses in its soundness or rigor.It may rely on questionable assumptions, poorly defined methods, or lack sufficient justification for its approach.Technical formulations are often incorrect or poorly presented.1-2 -Poor: The proposal is fundamentally unsound or lacks rigor.It is based on flawed assumptions, poorly defined methods, or lacks sufficient justification for its approach.Technical formulations are incorrect or absent.</p>
<p>FEASIBILITY (1-10)</p>
<p>How practical and implementable is the research proposal?9-10 -Excellent: The proposal is highly practical and implementable with current resources, technology, and knowledge.The plan is realistic, and execution is straightforward with clearly defined steps and minimal risk.7-8 -Good: The proposal is largely feasible with existing technology and methods, though it may require moderate refinement, optimization, or additional resources.The plan is generally realistic, with manageable risks.5-6 -Satisfactory: The proposal is somewhat feasible but presents some implementation challenges.It may require considerable effort, resources, or further development to implement successfully.Some risks or uncertainties are present.3-4 -Needs Improvement: The proposal has significant implementation challenges.Major revisions, additional resources, or new methods would be needed to make it feasible.There are substantial risks or uncertainties that threaten successful execution.1-2 -Poor: The proposal is impractical or impossible to implement with current technology, knowledge, or constraints.The plan is unrealistic, and the likelihood of successful execution is extremely low.</p>
<p>SIGNIFICANCE (1-10)</p>
<p>How important and impactful is the research proposal?9-10 -Excellent: The proposal is highly significant and impactful.It addresses a critical problem or gap in the field and has the potential to lead to major advancements or transformative change.The expected contributions are substantial and clearly articulated.7-8 -Good: The proposal is significant and has clear impact potential.It addresses an important issue and could lead to meaningful contributions or improvements in the field, though the impact may not be transformative.5-6 -Satisfactory: The proposal is somewhat significant.It addresses a relevant problem, but its impact may be moderate or limited to a specific area or community.The expected contributions are present but not far-reaching.3-4 -Needs Improvement: The proposal has limited significance.It addresses a minor issue or has a narrow scope, with minimal potential for impact or advancement in the field.1-2 -Poor: The proposal has little to no significance.It does not address a meaningful problem or offer any clear benefits, and its potential impact is negligible or absent.</p>
<p>Overall Assessment (1-10)</p>
<p>How would you rate the research proposal overall, considering all five dimensions above? 10 -Outstanding: The proposal is exceptional in every respect, with no significant weaknesses.It demonstrates excellence across all dimensions and has the potential for major impact.8-9 -Excellent: The proposal is very strong overall, with only minor weaknesses.It is wellbalanced, highly promising, and likely to make a significant contribution.6-7 -Good: The proposal is solid, with a good balance of strengths and weaknesses.It is generally well-conceived and feasible, though some areas could be improved.4-5 -Satisfactory: The proposal is adequate but has notable weaknesses that limit its potential.It addresses the main requirements but lacks strength in one or more key areas.2-3 -Needs Improvement: The proposal has significant weaknesses that limit its potential for success or impact.Major revisions are needed to address critical issues.1 -Poor: The proposal is fundamentally flawed across most or all dimensions.It is unlikely to succeed or make a meaningful contribution without substantial reworking.</p>
<p>When assigning the Overall Assessment score, consider not just the average of the six dimensions, but also: -Whether any single weakness is critical enough to lower the overall potential.</p>
<p>-The overall coherence and integration of the proposal.</p>
<p>-The likelihood of real-world impact if the proposal were pursued.</p>
<p>-The degree to which the proposal fulfills the task description, research idea, and literature review as a whole.</p>
<p>-Any unique strengths or fatal flaws that are not fully captured by the individual dimensions.</p>
<p>Table 26: Rubrics used for experimentation.</p>
<ol>
<li>Hallucination (True/False) Does the experimental document contain any hallucinated content?Hallucinated content refers to information that is fabricated or incorrect, and does not align with the provided task description, research idea, literature review, or research proposal.Fake data, results, or methods should be considered as hallucinated content.</li>
</ol>
<p>True -The experimental document contains hallucinated content.False -The experimental document does not contain any hallucinated content.</p>
<p>Consistency (1-10)</p>
<p>How well do the experimental document align with the task description, research idea, literature review and research proposal?Does the implementation match the proposed methods and ideas?9-10 -Excellent: The experimental document are fully consistent with the task description, research idea, literature review and research proposal.There are no discrepancies or contradictions.The implementation is a perfect match to the proposed methods and ideas.7-8 -Good: The experimental document are mostly consistent, with only minor discrepancies or contradictions.The main points are well-aligned with the task, idea, literature review and proposal.The implementation is largely aligned with the proposed methods and ideas.5-6 -Moderate: Some inconsistencies or unclear alignments exist, but overall the experimental document are still relevant to the task, idea, literature review and proposal.The implementation is somewhat aligned with the proposed methods and ideas.3-4 -Weak: Significant inconsistencies or contradictions are present, leading to confusion or misalignment with the task, idea, literature review and proposal.The implementation is poorly aligned with the proposed methods and ideas.1-2 -Poor: The experimental document are largely inconsistent with the task, idea, literature review and proposal, or there are major contradictions.The implementation is not aligned with the proposed methods and ideas at all.</p>
<p>3.</p>
<p>Completeness (1-10) Are all necessary experiments, baselines, and ablation studies included in this experimental document?Are all relevant results reported, and is the experimental setup fully described?9-10 -Excellent: All necessary experiments, baselines, and ablations are included.The results are comprehensive and the setup is fully described.7-8 -Good: Most necessary experiments, baselines, and ablations are included, with only minor omissions.The setup is mostly described.5-6 -Moderate: Some important experiments, baselines, and ablations are missing, but the main points are covered.The setup is partially described.3-4 -Weak: Many key experiments are missing, making the evaluation incomplete.The setup is poorly described and lacks clarity.1-2 -Poor: The results are highly incomplete, with major omissions.The setup is missing.</p>
<p>4.</p>
<p>Novelty (1-10) Does the experiment document demonstrate new findings, methods, or insights compared to existing work?Is the experimental design innovative or derivative?9-10 -Excellent: The experimental document presents highly novel findings, methods, or insights that significantly advance the field.The design is innovative and original.7-8 -Good: The experimental document shows some novel aspects, with a good level of innovation.The design is mostly original.5-6 -Moderate: The experimental document has some novel elements, but they are not particularly groundbreaking.The design is somewhat derivative.3-4 -Weak: The experimental document lacks novelty, with little to no new findings or insights.The design is largely derivative.1-2 -Poor: The experimental document is entirely derivative, with no new findings or insights.The design is unoriginal and lacks creativity.</p>
<p>5.</p>
<p>Soundness (1-10) Are the experimental methods, analysis, and conclusions logically sound and scientifically rigorous?Are the results reproducible and well-supported?9-10 -Excellent: Methods and analysis are highly rigorous, logically sound, and statistically valid.Results are fully reproducible and well-supported.7-8 -Good: Methods and analysis are generally sound, with only minor issues.Results are mostly reproducible and well-supported.5-6 -Moderate: Some weaknesses in rigor or logic, but the main conclusions are still supported.Results are partially reproducible.3-4 -Weak: Significant flaws in methods or analysis, casting doubt on the conclusions.Results are not well-supported or reproducible.1-2 -Poor: Methods or analysis are fundamentally unsound or unscientific.Results are not reproducible and conclusions are unsupported.</p>
<p>6.</p>
<p>Insightfulness (1-10) Do the results provide deep insights, meaningful interpretations, or valuable implications for the field?Are trends, patterns, and implications discussed thoughtfully?6-7 -Good: The experimental work is solid and generally well-conceived, with a good balance of strengths and weaknesses.While there may be some areas for improvement, the work is credible, meaningful, and meets the main requirements for quality research.4-5 -Satisfactory: The work is adequate but has several notable weaknesses that limit its overall quality or impact.While it addresses the main objectives, shortcomings in design, execution, or analysis reduce its effectiveness and significance.2-3 -Needs Improvement: The experimental work has substantial weaknesses in multiple areas, which undermine its credibility or value.Major revisions and improvements are needed for the work to reach an acceptable standard.1 -Poor: The work is fundamentally flawed across most or all dimensions.It fails to meet essential standards for research quality and is unlikely to provide meaningful insights or contributions.</p>
<p>When assigning the Overall Assessment score, consider not just the average of these dimensions, but also: -The overall coherence and integration of the experimental work.</p>
<p>-The presence of any particularly outstanding strengths or critical weaknesses that may not be fully reflected in the individual scores.</p>
<p>-The potential impact or importance of the work as a whole.</p>
<p>-The degree to which the experimental work advances the field or opens new research directions.</p>
<p>-Any unique contributions, innovative aspects, or serious flaws that significantly affect the overall quality.Your overall assessment should reflect a holistic judgment, taking into account both the quantitative scores and your qualitative evaluation of the experimental work.</p>
<p>Table 27: Rubrics used for paper writing.</p>
<ol>
<li>
<p>Consistency (1-10) -How well does the paper align with the task description, research idea, research proposal and experimental results?-Are there any contradictions or inconsistencies in the paper's arguments or findings?9-10 -Excellent: The paper is highly consistent, with no contradictions or inconsistencies.7-8 -Good: The paper is mostly consistent, with minor contradictions or inconsistencies.5-6 -Fair: The paper has some inconsistencies, but they do not significantly detract from the overall quality.3-4 -Poor: The paper has several inconsistencies that detract from the overall quality.1-2 -Very Poor: The paper is highly inconsistent, with major contradictions or inconsistencies that significantly detract from the overall quality.</p>
</li>
<li>
<p>Clarity (1-10) -How clear and understandable is the paper's writing?-Are the arguments and findings presented in a logical and coherent manner?-Is the paper well-structured and easy to follow?9-10 -Excellent: The paper is very clear and easy to understand, with a logical structure and coherent arguments.7-8 -Good: The paper is mostly clear, with minor issues in structure or coherence.5-6 -Fair: The paper has some clarity issues, but they do not significantly detract from the overall quality.3-4 -Poor: The paper has several clarity issues that detract from the overall quality.1-2 -Very Poor: The paper is very unclear and difficult to understand, with major issues in structure or coherence.</p>
</li>
</ol>
<p>3.</p>
<p>Completeness (1-10) -How complete is the paper in terms of addressing the task description, research idea, research proposal and experimental results?-Are there any missing components or sections that should be included?9-10 -Excellent: The paper is very complete, addressing all components of the task description, research idea, research proposal and experimental results.7-8 -Good: The paper is mostly complete, with minor missing components or sections.5-6 -Fair: The paper has some missing components or sections, but they do not significantly detract from the overall quality.3-4 -Poor: The paper has several missing components or sections that detract from the overall quality.1-2 -Very Poor: The paper is very incomplete, with major missing components or sections that significantly detract from the overall quality.</p>
<ol>
<li>Soundness (1-10) -Are the arguments and findings supported by evidence and reasoning?-Are there any flaws or weaknesses in the paper's methodology or approach?-Are the experimental results and analyses valid and reliable?9-10 -Excellent: The paper is very sound, with strong evidence and reasoning supporting the arguments and findings.The experimental results and analyses are fully valid and reliable.7-8 -Good: The paper is mostly sound, with minor flaws or weaknesses in methodology or approach.The experimental results and analyses are mostly valid and reliable.5-6 -Fair: The paper has some flaws or weaknesses in methodology or approach, but they do not significantly detract from the overall quality.The experimental results and analyses are somewhat valid and reliable.3-4 -Poor: The paper has several flaws or weaknesses in methodology or approach that detract from the overall quality.Many of the experimental results and analyses are not valid or reliable.1-2 -Very Poor: The paper is very unsound, with major flaws or weaknesses in methodology or approach that significantly detract from the overall quality.The experimental results and analyses are not valid or reliable.</li>
</ol>
<p>5.</p>
<p>Overall Assessment (1-10) -Based on your evaluation of the paper's consistency, clarity, completeness and soundness, what is your overall assessment of the paper? 10 -Outstanding: The paper is of outstanding quality, with no issues in any of the key dimensions.8-9 -Excellent: The paper is of excellent quality, with minor issues in one or more of the key dimensions.6-7 -Good: The paper is of good quality, with some issues in one or more of the key dimensions.4-5 -Fair: The paper is of fair quality, with several issues in one or more of the key dimensions.2-3 -Poor: The paper is of poor quality, with major issues in one or more of the key dimensions. 1 -Very Poor: The paper is of very poor quality, with major issues in all of the key dimensions.</p>
<p>When assigning the Overall Assessment score, consider not just the average of these dimensions, but also: -The overall coherence and integration of the paper.</p>
<p>-The presence of any particularly outstanding strengths or critical weaknesses that may not be fully reflected in the individual scores.</p>
<p>-The potential impact or importance of the work as a whole.</p>
<p>-The degree to which the paper advances the field or opens new research directions.</p>
<p>-Any unique contributions, innovative aspects, or serious flaws that significantly affect the overall quality.Your overall assessment should reflect a holistic judgment, taking into account both the quantitative scores and your qualitative evaluation of the paper.</p>
<p>Table 28: Prompt used for end-to-end evaluation.</p>
<ol>
<li>
<p>Clarity (1-10) -Is the paper well-written and easy to understand? -Are the ideas and contributions clearly articulated?-Is the structure of the paper logical and coherent?9-10 -The paper is exceptionally well-written, with clear and concise language.The ideas are presented in a logical and coherent manner, making it easy to follow the author's arguments.7-8 -The paper is well-written, but there are some areas that could be improved for clarity.The ideas are mostly clear, but there may be some minor issues with the structure or language.5-6 -The paper is somewhat difficult to read, with several areas that are unclear or poorly articulated.The structure may be confusing, making it hard to follow the author's arguments.3-4 -The paper is poorly written, with many unclear or confusing sections.The ideas are not well-articulated, and the structure is disorganized.1-2 -The paper is extremely difficult to read, with numerous unclear or confusing sections.The ideas are poorly articulated, and the structure is completely 2. Novelty (1-10) -Does the paper present new and original ideas and findings?-Are the experimental results and contributions original and novel?-Is the work a significant advance over existing research?9-10 -The paper presents groundbreaking ideas and findings that are highly original and significant.The contributions are a major advance over existing research and are likely to have a lasting impact on the field.7-8 -The paper presents some new and original ideas, and the contributions are significant.The work is a notable advance over existing research, but it may not be as groundbreaking as top-tier papers.5-6 -The paper presents some new ideas and findings, but they are not particularly original or significant.The contributions are somewhat incremental and do not represent a major advance over existing research.3-4 -The paper presents few new ideas or findings, and those that are presented are not original or significant.The contributions are minimal and do not advance the field.1-2 -The paper presents no new ideas, and the contributions are completely unoriginal.The work does not advance the field in any meaningful way.</p>
</li>
<li>
<p>Soundness (1-10) -Are the methods and techniques used in the paper sound and appropriate?-Are the results and conclusions supported by the data?-Are there any major flaws or weaknesses in the experimental design, results or analysis?-Are the experimental results reliable and consistent to the code of the paper?Are the experimental results real or fake?-Are the visualization and analysis figures based on real experimental results or based on fake data?9-10 -The methods and techniques used in the paper are sound and appropriate.The results are well-supported by the data, and there are no major flaws or weaknesses in the experimental design, results or analysis.The experimental results are fully reliable and consistent with the code of the paper.7-8 -The methods and techniques used in the paper are mostly sound, but there may be some minor issues.The results are generally well-supported by the data, but there may be some areas that could be improved.The experimental design, results or analysis may have some minor flaws.The experimental results are mostly reliable.5-6 -The methods and techniques used in the paper are somewhat questionable, with several areas that could be improved.The results are not well-supported by the data, and there may be some significant flaws in the experimental design, results or analysis.Some experimental results are not reliable.3-4 -The methods and techniques used in the paper are flawed or inappropriate.The results are not well-supported by the data, and there are major flaws in the experimental design, results or analysis.Most of experimental results are not reliable.1-2 -The methods and techniques used in the paper are completely unsound.The results are not supported by the data, and there are numerous major flaws in the experimental design, results or analysis.The conclusions drawn from the paper are completely invalid.All experimental results are not reliable.</p>
</li>
</ol>
<p>4.</p>
<p>Significance (1-10) -Does the paper address an important problem or question?-Are the contributions significant to the field?-Are the experimental results reproducible and reliable?Do they have a significant impact?-Will the work have a lasting impact on the field?9-10 -The paper addresses a highly important problem or question, and the results and contributions are significant to the field.The work is likely to have a lasting impact on the field.7-8 -The paper addresses an important problem or question, and the results and contributions are significant.The work may have a lasting impact on the field, but it may not be as groundbreaking as top-tier papers.5-6 -The paper addresses a somewhat important problem or question, but the results and contributions are not particularly significant.The work may have some impact on the field, but it is unlikely to be lasting.3-4 -The paper addresses a minor problem or question, and the results and contributions are minimal.The work is unlikely to have any significant impact on the field.1-2 -The paper addresses an unimportant problem or question, and the results and contributions are completely insignificant.The work will have no impact on the field.</p>
<p>5.</p>
<p>Overall Assessment (1-10) -Based on the above criteria, how would you rate the overall quality of the paper?Note that any single weakness can be critical to lower the overall assessment.</p>
<p>-Is the paper suitable for publication in a top-tier conference or journal?-Would you recommend this paper to your colleagues? 10 -The paper is of exceptional quality and is highly suitable for publication in a top-tier conference or journal.I would strongly recommend this paper.8-9 -The paper is of high quality and is suitable for publication in a top-tier conference or journal.I would recommend this paper.6-7 -The paper is of good quality and is suitable for publication in a reputable conference or journal.I would recommend this paper with some reservations.4-5 -The paper is of acceptable quality but may not be suitable for publication in a top-tier conference or journal.I would recommend this paper with significant reservations.2-3 -The paper is of poor quality and is not suitable for publication in a top-tier conference or journal.I would not recommend this paper.1 -The paper is of extremely poor quality and is not suitable for publication in any conference or journal.I would strongly advise against recommending this paper.Please provide a detailed review of the paper, including your scores for each aspect and an overall assessment.Be sure to justify your scores with specific examples from the paper.Please do not include any personal opinions or biases in your review.Your review should be objective and based solely on the content of the paper.Please provide a confidence score from 1 to 5 for the overall assessment.Do not hesitate to assign lower scores if the paper does not fully meet the criteria.Avoid giving high scores by default.</p>
<p>D.2.2 Evaluation Prompts for MLR-Judge</p>
<p>These prompts instruct the model to apply the above rubrics and format its review output accordingly.</p>
<p>• Table 30 shows the prompt used for evaluating the overall quality of a research paper in an end-to-end manner.• Table 29 shows the prompt used for evaluating the quality of the generated research idea.</p>
<p>• Table 33 shows the prompt used for evaluating the research proposal.</p>
<p>• Table 32 shows the prompt used for evaluating the experimental implementation and results.</p>
<p>• Table 31 shows the prompt used for evaluating the paper write-up.</p>
<p>Table 29: Instruction prompt used for evaluating the quality of generated idea.To ensure the model adheres to the desired output format, we specify the format twice in the prompt-once in the middle and again at the end.For brevity, this repeated instruction (i.e., at the end) is omitted here.Please refer to our code repository for full details.</p>
<p>You are an expert machine learning researcher!You will be given a research idea and a description.Your task is to evaluate a research idea on a scale of 1 to 10 across five key dimensions and finally give an overall assessment on a scale of 1 to 10. Please be objective in your evaluation, and provide detailed justifications for each score you assign.Do not hesitate to assign lower scores if the idea does not fully meet the criteria.Avoid giving high scores by default.</p>
<h2 24="24" Evaluation="Evaluation" Idea="Idea" Rubrics="Rubrics" Table="Table" as="as" in="in" shown="shown">Evaluation Rubric</h2>
<h2>Output Format Please output a complete JSON object strictly following the format below, including all evaluation items (Consistency, Clarity, Novelty, Feasibility, Significance, OverallAssessment).Do not output only a single item or partial content; you must output the entire JSON object.</h2>
<p>{ " Consistency " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the alignment with the task description &gt; " } , " Clarity " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the clarity of the idea &gt; " } , " Novelty " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the originality and innovation of the idea &gt; " } , " Feasibility " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the practicality and implementability of the idea &gt; " } , " Significance " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the importance and impact of the idea &gt; " } , " OverallAssessment " : { " score " : &lt;1 -10 &gt; , " strengths " : [ " &lt; strength 1 &gt; " , " &lt; strength 2 &gt; " ] , " weaknesses " : [ " &lt; weakness 1 &gt; " , " &lt; weakness 2 &gt; " ] } } Please make sure your output is a complete JSON object and includes all the fields above.Please make sure the answer is strictly in valid JSON format.-Ensure all brackets and braces are properly closed.</p>
<p>Table 30: Instruction prompt used for evaluating the overall quality of the paper.To ensure the model adheres to the desired output format, we specify the format twice in the prompt-once in the middle and again at the end.For brevity, this repeated instruction (i.e., at the end) is omitted here.Please refer to our code repository for full details.</p>
<p>You are an expert machine learning researcher!You will be given a research paper which is based on a task description.You might also be given the code of the paper to check the reproducibility of the paper.You task is to review the paper in terms of 4 key aspects -Clarity, Novelty, Soundness and Significance.Please provide a score from 1 to 10 for each aspect and an overall assessment, where 1 is the lowest and 10 is the highest.Lastly, provide a confidence score from 1 to 5 for the overall assessment, where 1 is the lowest and 10 is the highest.Do not hesitate to assign lower scores if the paper does not fully meet the criteria.Avoid giving high scores by default.</p>
<p>Table 31: Instruction prompt used for paper writing.To ensure the model adheres to the desired output format, we specify the format twice in the prompt-once in the middle and again at the end.For brevity, this repeated instruction (i.e., at the end) is omitted here.Please refer to our code repository for full details.</p>
<p>You are an expert machine learning researcher and your task is to evaluate a paper.You will be given a machine learning paper, which is based on a task description, a research idea, a literature review, a research proposal and experimental results.You will evaluate the paper on a scale of 1 to 10 across four key dimensions: Consistency, Clarity, Completeness, Soundness and finally give an overall assessment on a scale of 1 to 10. Please be objective in your evaluation, and provide detailed justifications for each score you assign.Do not hesitate to assign lower scores if the paper does not fully meet the criteria.Avoid giving high scores by default.## Evaluation Rubric {Writing Quality Evaluation Rubrics as shown in Table 27} ## Output Format Please evaluate the paper according to the rubric and output a complete JSON object strictly following the format below, including all evaluation items (Consistency, Clarity, Completeness, Soundness, OverallAssessment).Do not output only a single item or partial content; you must output the entire JSON object.</p>
<p>{ " Consistency " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the consistency of the paper itself and between the paper and the task description , research idea , research proposal and experimental results &gt; " } , " Clarity " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the clarity of the paper writing , structure , and coherence &gt; " } , " Completeness " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the completeness of the paper in addressing the task description , research idea , research proposal and experimental results &gt; " } , " Soundness " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the soundness of the paper ' s arguments , findings , methodology , and experimental results &gt; " } , " OverallAssessment " : { " score " : &lt;1 -10 &gt; , " strengths " : [ " &lt; strength 1 &gt; " , " &lt; strength 2 &gt; " ] , " weaknesses " : [ " &lt; weakness 1 &gt; " , " &lt; weakness 2 &gt; " ] } } Please make sure the answer is strictly in valid JSON format.Table 32: Instruction prompt used for evaluating the quality of the experimentation process.The format is specified twice in the prompt-midway and at the end-to guide the model.We omit the final repetition here for brevity.</p>
<p>You are an expert machine learning researcher and your task is to evaluate a machine learning experimental document.You will be given a document containing experimental execution records and experimental results, which is based on a task description, a research idea, a literature review and a research proposal.You will first determine if the document contains any hallucinated content, then evaluate the document on a scale of 1 to 10 across six key dimensions: Consistency, Completeness, Novelty, Soundness, Insightfulness, Significance and finally give an overall assessment on a scale of 1 to 10. Please be objective in your evaluation, and provide detailed justifications for each score you assign.Do not hesitate to assign lower scores if the experimental document does not fully meet the criteria.Avoid giving high scores by default.-Ensure all brackets and braces are properly closed.</p>
<p>Table 33: Instruction prompt used for evaluating the quality of the generated research proposal.</p>
<p>You are an expert machine learning researcher!You will be given a research proposal based on a task description, a research idea, and a literature review.Your task is to evaluate the proposal on a scale of 1 to 10 across six key dimensions and finally give an overall assessment on a scale of 1 to 10. Please be objective in your evaluation, and provide detailed justifications for each score you assign.Do not hesitate to assign lower scores if the proposal does not fully meet the criteria.Avoid giving high scores by default.</p>
<h2 25="25" Proposal="Proposal" Research="Research" Rubrics="Rubrics" as="as" in="in" shown="shown">Evaluation Rubric</h2>
<h2>Output Format Please output a complete JSON object strictly following the format below, including all evaluation items (Consistency, Clarity, Novelty, Soundness, Feasibility, Significance, OverallAssessment).Do not output only a single item or partial content; you must output the entire JSON object.When writing mathematical formulas, you should avoid invalid escape JSON decode errors.domain.Each reviewer received the research paper along with its corresponding supplementary code.We collected all responses through Google Forms.</h2>
<p>The evaluation criteria followed the same rubric used in the end-to-end setting of MLR-Judge (see Table 28).Fig. 8 and Fig. 9 show the Google Form interface used for receiving the human review data.All collected human review results are available as a CSV file in our GitHub repository under the human-study folder.</p>
<p>Figure 8: The Google Form interface used to collect review scores, following the same reviewer rubrics as MLR-Judge (Table 28).</p>
<p>Figure 9: The Google Form interface used to collect review scores, following the same reviewer rubrics as MLR-Judge (Table 28).</p>
<p>LFigure 2 :
2
Figure 2: The number of tasks grouped by our ML primary categories.</p>
<p>Figure 3 :
3
Figure 3: Scores of two LLM judge models across seven review dimensions on ten tasks.</p>
<p>For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.</p>
<p>F a k eFigure 6 :
6
Figure 6: Frequency of each hallucination type in AI Scientist V2 and MLR-Agent across 10 tasks.</p>
<p>6 .
6
Confidence Score (1-5) -How confident are you in your overall assessment of the paper? 5 -Extremely confident in the overall assessment.4 -Very confident in the overall assessment.3 -Moderately confident in the overall assessment. 2 -Slightly confident in the overall assessment. 1 -Not confident in the overall assessment.</p>
<p>-Do not include any text, comments, or explanations outside the JSON code block.-Do not use trailing commas.-Do not use single quotes; use double quotes for all keys and string values.-Do not include comments inside the JSON.-Do not use unescaped control characters (e.g., newlines inside strings).-Do not use unquoted keys; all keys must be in double quotes.-Do not use invalid values like NaN or Infinity.</p>
<p>-</p>
<p>Do not include any text, comments, or explanations outside the JSON code block.-Do not use trailing commas.-Do not use single quotes; use double quotes for all keys and string values.-Do not include comments inside the JSON.-Do not use unescaped control characters (e.g., newlines inside strings).-Do not use unquoted keys; all keys must be in double quotes.-Do not use invalid values like NaN or Infinity.-Ensure all brackets and braces are properly closed.</p>
<p>-</p>
<p>Do not use trailing commas.-Do not use single quotes; use double quotes for all keys and string values.-Do not include comments inside the JSON.-Do not use unescaped control characters (e.g., newlines inside strings).-Do not use unquoted keys; all keys must be in double quotes.-Do not use invalid values like NaN or Infinity.</p>
<p>Table 1 :
1
Review dimensions for different research processes.
DimensionIdeation Proposal Coding Writing End-to-End</p>
<p>Table 2 :
2
[24,2,ted30,31,s[223,9]7,30,31, 8,23,9]in different research stages.
Claude Code Performance Across Different DimensionsStage IdeationEvaluated Models o4-mini, Claude-3.7-Sonnet, Deepseek-R1,6 88.807.706.906.705.90 Gemini-2.5-Pro-Preview 6.60 6.90 Claude-3-7-Sonnet§3.1 ProposalMinistral-8B, Qwen3-235B-A22B, Gemini-2.5-Pro-Preview o4-mini, Claude-3.7-Sonnet, Deepseek-R1,Score44.704.304.402.803.102.803.00§3.2Ministral-8B, Qwen3-235B-A22B, Gemini-2.5-Pro-Preview2Coding  §3.3 Writing  §3.4Claude Code (Claude-3.7-Sonnet), o4-mini, Claude-3.7-Sonnet, Gemini-2.5-Pro-Preview Codex (o4-mini)C o n s is t e n c y C o m p le t e n e s s N o v e lt y S o u n d n e s s In s ig h t f u ln e s s S ig n if ic a n c e 0O v e r a llEnd-to-End o4-mini, Claude-3.7-Sonnet,§3.5Gemini-2.5-Pro-Preview</p>
<p>Table 3 :
3
Evaluation results of six frontier LLMs averaged on 201 tasks in idea generation.Best and worst scores are highlighted in green and red background, respectively.o4-mini-high: o4-mini-2025-04-16 with reasoning_effort set to "high".
ModelConsistencyClarityNoveltyFeasibility SignificanceOverallMinistral-8B8.99 ± 0.36 7.83 ± 0.50 6.66 ± 0.46 6.94 ± 0.67 8.36 ± 0.38 7.68 ± 0.40Deepseek-R1
9.26 ± 0.29 8.25 ± 0.32 7.43 ± 0.40 6.93 ± 0.56 8.70 ± 0.31 8.11 ± 0.30 Claude-3.7-Sonnet9.13 ± 0.32 8.07 ± 0.37 7.39 ± 0.40 6.65 ± 0.58 8.69 ± 0.33 7.96 ± 0.33 Qwen3-235B-A22B 9.20 ± 0.28 8.20 ± 0.32 7.62 ± 0.42 6.67 ± 0.52 8.73 ± 0.31 8.03 ± 0.29 o4-mini-high 9.23 ± 0.28 8.23 ± 0.30 7.49 ± 0.41 7.01 ± 0.53 8.66 ± 0.33 8.11 ± 0.30 Gemini-2.5-Pro-Preview9.20 ± 0.31 8.27 ± 0.31 7.30 ± 0.37 7.11 ± 0.57 8.58 ± 0.35 8.08 ± 0.28</p>
<p>Table 4 :
4
Evaluation results of six frontier LLMs averaged on 201 tasks in proposal generation.Best and worst scores are highlighted in green and red background, respectively.Gemini-2.5-Pro-Preview9.10 ± 0.26 8.42 ± 0.29 7.55 ± 0.67 7.90 ± 0.57 6.95 ± 0.68 8.73 ± 0.40 8.16 ± 0.34
ModelConsistencyClarityNoveltySoundnessFeasibility SignificanceOverallMinistral-8B
8.93 ± 0.22 7.65 ± 0.48 6.88 ± 0.67 7.03 ± 0.86 6.69 ± 0.80 8.53 ± 0.35 7.50 ± 0.48 Deepseek-R1 9.02 ± 0.19 8.20 ± 0.35 7.32 ± 0.64 7.75 ± 0.57 6.96 ± 0.60 8.64 ± 0.36 8.02 ± 0.34 Claude-3.7-Sonnet9.05 ± 0.21 8.31 ± 0.31 7.48 ± 0.65 7.81 ± 0.56 6.75 ± 0.62 8.80 ± 0.36 8.04 ± 0.27 Qwen3-235B-A22B 9.03 ± 0.21 8.17 ± 0.39 7.48 ± 0.61 7.66 ± 0.64 6.94 ± 0.64 8.69 ± 0.31 8.04 ± 0.32 o4-mini-high 9.06 ± 0.17 8.34 ± 0.28 7.45 ± 0.56 7.90 ± 0.60 7.18 ± 0.67 8.68 ± 0.35 8.17 ± 0.28</p>
<p>Table 5 :
5
Evaluation results of Claude Code and Codex averaged on ten tasks in experiment execution.The scores are the average of two judge models.
Coding Agent Consistency CompletenessNoveltySoundness Insightfulness SignificanceOverallClaude Code6.75 ± 1.006.00 ± 0.685.65 ± 0.82 4.75 ± 1.024.50 ± 0.974.70 ± 0.95 4.95 ± 0.82Codex6.30 ± 1.465.05 ± 0.873.80 ± 1.19 6.15 ± 0.874.45 ± 1.283.40 ± 1.05 4.95 ± 1.02</p>
<p>Table 6 :
6
Evaluation results of three models averaged on ten tasks in paper writing.Best and worst scores are highlighted in green and red background, respectively.
ModelConsistencyClarityCompleteness SoundnessOverallo4-mini-high6.35 ± 1.13 7.25 ± 0.776.15 ± 0.775.05 ± 1.11 5.90 ± 0.97Gemini-2.5-Pro-Preview 7.55 ± 0.91 8.05 ± 0.527.20 ± 0.776.05 ± 0.72 6.60 ± 0.86Claude-3.7-Sonnet7.40 ± 0.95 7.80 ± 0.616.80 ± 0.645.85 ± 0.89 6.50 ± 0.81</p>
<p>Table 7
7: End-to-end evaluation results over the ten tasks. AI Scientist V2 is powered byo4-mini-high. We use o4-mini-medium in Codex and use Gemini-2.5-Pro-Preview-05-06 inthis experiment. When using Claude Code as the coding agent, the costs of o4-mini-high,Gemini-2.5-Pro-Preview, and Claude-3.7-Sonnet are $1.15, $1.24, and $2.40, respectively.ModelClarityNoveltySoundness SignificanceOverallAI Scientist V2o4-mini-high6.55 ± 0.94 6.70 ± 0.48 3.70 ± 1.29 4.85 ± 1.08 4.25 ± 1.25MLR-Agento4-mini-high + Codex6.45 ± 0.90 5.65 ± 0.60 2.90 ± 0.57 3.80 ± 0.65 3.10 ± 0.60Gemini-2.5-Pro-Preview + Gemini CLI 8.30 ± 0.37 6.85 ± 0.34 4.15 ± 1.06 5.30 ± 0.91 4.60 ± 1.00Claude-3.7-Sonnet + Claude Code7.75 ± 0.34 7.10 ± 0.43 4.05 ± 1.11 5.50 ± 1.14 4.70 ± 1.22</p>
<p>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>
<p>• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.•For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. Broader impactsQuestion: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]
8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: See section 3. Guidelines: • The answer NA means that the paper does not include experiments.• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Answer: [Yes] Justification: We have checked the Code of Ethics.Guidelines: • Justification: Sure, we discuss our broader impact in Limitation and Conclusion.</p>
<p>• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.16.Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.Answer: [Yes] Justification: Yes, we described the usage of LLMs in the main content.Guidelines: • The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)</p>
<p>Table 8 :
8
Description of ten selected tasks in experimentation, paper writing and end-to-end evaluation.
Task NameTopicCategoryiclr2025_bi_alignBidirectional Human-AI AlignmentTrustworthy AIiclr2025_buildingtrustBuilding Trust in Language Models and ApplicationsTrustworthy AIiclr2025_data_problemsNavigating and Addressing Data Problems for Foundation ModelsTrustworthy AIiclr2025_dl4cEmergent Possibilities and Challenges in Deep Learning for CodeLLM/VLMiclr2025_mldprThe Future of Machine Learning Data Practices and RepositoriesTrustworthy AIiclr2025_questionQuantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AILLM/VLMiclr2025_scopeScalable Optimization for Efficient and Adaptive Foundation ModelsTrustworthy AIiclr2025_scslSpurious Correlation and Shortcut Learning: Foundations and SolutionsTrustworthy AIiclr2025_verifaiVerifAI: AI Verification in the WildTrustworthy AIiclr2025_wslNeural Network Weights as a New Data ModalityML Theory</p>
<p>Table 9 :
9
Evaluation results judged by Gemini-2.5-Pro-Preview-03-25 in idea generation.
ModelConsistencyClarityNoveltyFeasibilitySignificanceOverallMinistral-8B9.22 ± 0.57 8.01 ± 0.89 6.57 ± 0.74 6.77 ± 1.01 8.61 ± 0.52 7.75 ± 0.63Deepseek-R19.57 ± 0.528.61 ± 0.54 7.48 ± 0.56 6.92 ± 0.81 8.87 ± 0.368.29 ± 0.51Claude-3.7-Sonnet9.34 ± 0.58 8.26 ± 0.65 7.42 ± 0.56 6.56 ± 0.87 8.80 ± 0.44 8.00 ± 0.59Qwen3-235B-A22B9.45 ± 0.51 8.49 ± 0.57 7.66 ± 0.54 6.65 ± 0.78 8.83 ± 0.40 8.12 ± 0.50o4-mini-high9.52 ± 0.50 8.55 ± 0.52 7.52 ± 0.58 6.96 ± 0.76 8.80 ± 0.418.29 ± 0.54Gemini-2.5-Pro-Preview 9.46 ± 0.57 8.59 ± 0.52 7.34 ± 0.56 7.00 ± 0.78 8.68 ± 0.49 8.21 ± 0.50</p>
<p>Table 10 :
10
Evaluation results judged by Claude-3.7-Sonnet-20250219 in idea generation.
ModelConsistencyClarityNoveltyFeasibility SignificanceOverallMinistral-8B8.75 ± 0.43 7.64 ± 0.48 6.74 ± 0.55 7.10 ± 0.89 8.11 ± 0.54 7.61 ± 0.51Deepseek-R18.94 ± 0.24 7.89 ± 0.33 7.37 ± 0.57 6.94 ± 0.78 8.52 ± 0.50 7.93 ± 0.28Claude-3.7-Sonnet8.92 ± 0.27 7.88 ± 0.35 7.36 ± 0.56 6.73 ± 0.75 8.57 ± 0.49 7.91 ± 0.30Qwen3-235B-A22B8.94 ± 0.26 7.91 ± 0.30 7.57 ± 0.64 6.69 ± 0.67 8.63 ± 0.48 7.93 ± 0.29o4-mini-high8.94 ± 0.25 7.90 ± 0.30 7.46 ± 0.58 7.06 ± 0.73 8.52 ± 0.52 7.92 ± 0.27Gemini-2.5-Pro-Preview 8.93 ± 0.26 7.94 ± 0.33 7.26 ± 0.47 7.21 ± 0.82 8.47 ± 0.51 7.95 ± 0.27</p>
<p>Table 11 :
11
Evaluation results judged by Gemini-2.5-Pro-Preview-03-25 in proposal generation.
ModelConsistencyClarityNoveltySoundnessFeasibility SignificanceOverallMinistral-8B8.94 ± 0.33 7.32 ± 0.90 6.72 ± 1.30 6.46 ± 1.13 6.55 ± 0.93 8.78 ± 0.48 7.21 ± 0.90Deepseek-R19.04 ± 0.26 8.39 ± 0.58 7.44 ± 0.83 7.68 ± 0.72 7.05 ± 0.77 8.97 ± 0.24 8.04 ± 0.54Claude-3.7-Sonnet9.09 ± 0.33 8.62 ± 0.52 7.56 ± 0.77 7.90 ± 0.55 6.77 ± 0.78 9.01 ± 0.20 8.08 ± 0.52Qwen3-235B-A22B9.07 ± 0.30 8.33 ± 0.64 7.59 ± 0.71 7.66 ± 0.77 7.02 ± 0.84 9.00 ± 0.10 8.08 ± 0.50o4-mini-high9.12 ± 0.33 8.68 ± 0.54 7.57 ± 0.64 8.03 ± 0.68 7.37 ± 0.86 8.99 ± 0.20 8.34 ± 0.55Gemini-2.5-Pro-Preview 9.21 ± 0.41 8.82 ± 0.42 7.68 ± 0.81 8.18 ± 0.62 7.15 ± 0.81 9.04 ± 0.31 8.32 ± 0.57</p>
<p>Table 12 :
12
Evaluation results judged by Claude-3.7-Sonnet-20250219 in proposal generation.D.1 Prompts for MLR-AgentMLR-Agent is a simple and flexible scaffold designed to support open-ended research.It is implemented to favour simplicity over extensive prompt engineering, allowing us to assess each large language model's fundamental performance directly.</p>
<p>Table 20 :
20
Prompt
Prompt You are an excellent machine learning researcher!Please help me do a literature review for a given idea.The idea is based on a given task description.The papers in the literature review should be extracted from arxiv and publishedbetween 2023 to 2025.The literature review should include the following:1. Related Papers: at least 10 academic papers most closely related to the currentresearch idea, with a brief summary and the publication year of each one, organizedlogically.2. Key Challenges: A discussion of the main challenges and limitations in the currentresearch. List no more than five key challenges.The paper should be in the format of:1. Title: <title> (<arxiv_id>)-Authors: <author1>, <author2>, ...-Summary: <summary>-Year: <year>Please directly respond to the literature review and do not include any additionalcomments.Here is the idea: {Idea}Here is the task: {Task}Output 1. Title: Explainability for Large Language Models: A Survey (arXiv:2309.01029)-Authors: Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, HengyiCai, Shuaiqiang Wang, Dawei Yin, Mengnan Du-Summary: This survey introduces a taxonomy of explainability techniques forTransformer-based language models, categorizing methods based on training paradigmsand summarizing approaches for generating local and global explanations. It alsodiscusses evaluation metrics and the use of explanations to debug models and improveperformance.-Year: 2023...
used for literature review.After generating the research idea, we use GPT-4o-Search-Preview [22] to conduct a literature review.The retrieved references are then provided to the model to guide the generation of the research proposal.</p>
<p>Table 21 :
21
Prompt used for research proposal generation.We provide the research task description, the previously generated research idea, and the literature review as inputs, which are combined with this prompt to guide the language model (e.g., Claude) in generating a complete research proposal.
Prompt You are an excellent machine learning researcher!Please generate a detailed research proposal based on a given task description, aresearch idea and their literature review.The proposal should be about 2000 words and include the following five sections:1. Title: a concise and descriptive title for the research proposal.2. Introduction: background, research objectives and significance.3. Methodology: detailed and precise research design (including data collection, fullalgorithmic steps and/or mathematical formulas where appropriate, and full detailsabout experimental design to validate the method, with evaluation metrics).4. Expected Outcomes &amp; Impact.The proposal should be well-structured and clearly articulate the research plan. Whenwriting mathematical formulas, you should use LaTeX syntax. For inline formulas, usesingle dollar signs, for example: $x^2$ to represent x squared. For block equations,use double dollar signs at the beginning and end, for example: $$x^2$$.Please directly respond to the proposal.Here is the task: {Task}Here is the idea: {Idea}Here is the literature review: {Related Work}</p>
<p>Table 22 :
22
Prompt used for experimentation.We provide the research task description, the previously generated research idea, the literature review, and the research proposal as inputs, which are combined with this prompt to guide the coding agent (e.g., Claude) for conducting the experiments and obtaining the results.Create a folder named 'claude_code' inside this folder to save the code and results.2.Write some Python scripts to run the experiment automatically.IMPORTANT: The script must be thoroughly debugged and tested until it can run successfully without any errors.The script should handle all necessary data processing, model training, and evaluation steps automatically.NOTE: If the environment has available GPUs, the script should utilize GPU acceleration where possible.</p>
<p>Prompt Based on task.md,idea.md,related_work.mdandproposal.mdinthisfolder, please design an experimental plan to test the hypothesis that the proposed method is effective.Then write code to implement the experimental plan, and run the experiments in a fully automated manner.The experimental process should be fully automated and include the following steps:1.</p>
<p>Table 23 :
23
Prompt used for paper writing.We compile all outputs from the previous stages and combine them with the following prompt to instruct the LLM to generate a complete research paper.Figures and tables should be included in the paper.You may refer to the figures and tables in the experiment results.If there is no image in the experiment results, please do not create or cite any fake figures.Please directly use the paths of the figures in the markdown file and do not use any placeholders.When writing mathematical formulas, you should use LaTeX syntax.For inline formulas, use single dollar signs, for example: x 2 to represent x squared.For block equations, use double dollar signs at the beginning and end, for example: x 2 .If you need to write text in mathematical formulas, please avoid invalid escape characters.The paper should be well-structured and clearly present the research findings.
Prompt You are an excellent machine learning researcher!Given the task, research idea, literature review, proposal and experiment results, pleasewrite a paper for the machine learning project.It should include the following sections:1. Title and Abstract: A concise title and a brief abstract summarizing the research.2. Introduction: An introduction to the problem, its significance, and the proposedsolution.3. Related Work: A review of existing literature and how your work fits into the currentlandscape.4. Methodology: A detailed description of the proposed method, including any algo-rithms or models used.5. Experiment Setup: A description of the experimental setup, including datasets,metrics, and evaluation methods.6. Experiment Results: A presentation of the results obtained from the experiments,including tables and figures.7. Analysis: An analysis of the results, discussing their implications and any limitations.8. Conclusion: A summary of the findings and suggestions for future work.9. References: A list of references cited in the paper.Here is the task: {Task}Here is the idea: {Idea}Here is the literature review: {Related Work}Here is a summary of the experiment results: {Experiment Results}D.2 Prompts and Rubrics for MLR-Judge
AcknowledgementThis research is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 2 (FY2025) (Grant MOE-T2EP20124-0009).A Experimental DetailsThis section introduces the details of our experiments, including the ten tasks we selected in Sections 3.3 to 3.5.Our experiments are conducted on an Ubuntu 22.04 server with access to four NVIDIA RTX 3090 GPUs.Description of 10 Selected Tasks This section describes the ten selected tasks in experimentation, paper writing and end-to-end evaluation.Ten tasks are selected from the most recent ICLR 2025 workshops and most of them are related to Trustworthy AI.28}Please provide a detailed review of the paper, including your scores for each aspect and an overall assessment.Be sure to justify your scores with specific examples from the paper.Please do not include any personal opinions or biases in your review.Your review should be objective and based solely on the content of the paper.Please provide a confidence score from 1 to 5 for the overall assessment.Do not hesitate to assign lower scores if the paper does not fully meet the criteria.Avoid giving high scores by default.## Output FormatPlease provide your review in the following format:{ " Clarity " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; Your justification here &gt; " } , " Novelty " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; Your justification here &gt; " } , " Soundness " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; Your justification here &gt; " } , " Significance " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; Your justification here &gt; " } , " Overall " : { " score " : &lt;1 -10 &gt; , " strengths " : [ " &lt; strength 1 &gt; " , " &lt; strength 2 &gt; " ] , " weaknesses " : [ " &lt; weakness 1 &gt; " , " &lt; weakness 2 &gt; " ] } , " Confidence " : &lt;1 -5 &gt; } Note that any single weakness can be critical to lower the overall assessment.Please provide detailed justifications for each score, including specific examples from the paper.IMPORTANT: Please ensure that your output is a complete and valid JSON object and includes all the fields above.Do not output only a single item or partial content; you must output the entire JSON object.## Task Description {task}## Paper to Be Reviewed Note: The paper is generated by AI and may contain some errors.Please check the paper carefully and provide your review.{paper} ## Code of the Paper {code content} Please provide a detailed review of the paper, including your scores for each aspect and an overall assessment.Be sure to justify your scores with specific examples from the paper.Please do not include any personal opinions or biases in your review.Your review should be objective and based solely on the content of the paper.Please provide a confidence score from 1 to 5 for the overall assessment.{Experimentation Evaluation Rubrics as shown in Table26}## Output Format Please evaluate the experimental document according to the rubric and output a complete JSON object strictly following the format below, including all evaluation items (Hallucination, Consistency, Completeness, Novelty, Soundness, Insightfulness, Significance, OverallAssessment).Do not output only a single item or partial content; you must output the entire JSON object.{ " Hallucination " : { " has_hallucination " : &lt; true / false &gt; , " details " : " &lt; if has_hallucination is true , provide specific examples of hallucinated content ; if false , explain why you believe there is no hallucination &gt; " } , " Consistency " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the alignment with the task description , idea , and literature review &gt; " } , " Completeness " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the inclusion of necessary experiments , baselines , and ablation studies &gt; " } , " Novelty " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the originality and innovation of the findings , methods and experimental design &gt; " } , " Soundness " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the logical soundness and scientific rigor of the experimental design , analysis , and conclusions &gt; " } , " Insightfulness " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the depth of insights , meaningful interpretations , and valuable implications for the field &gt; " } , " Significance " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the importance and impact of the experimental results for the field &gt; " } , " OverallAssessment " : { " score " : &lt;1 -10 &gt; , " strengths " : [ " &lt; strength 1 &gt; " , " &lt; strength 2 &gt; " ] , " weaknesses " : [ " &lt; weakness 1 &gt; " , " &lt; weakness 2 &gt; " ] } } Please make sure the answer is strictly in valid JSON format.-Do not include any text, comments, or explanations outside the JSON code block.{ " Consistency " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the alignment with the task description , idea , and literature review &gt; " , } , " Clarity " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the clarity of the proposal &gt; " , } , " Novelty " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the originality and innovation of the proposal &gt; " , } , " Soundness " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the technical foundations and rigor of the proposal &gt; " , } , " Feasibility " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the practicality and implementability of the proposal &gt; " , } , " Significance " : { " score " : &lt;1 -10 &gt; , " justification " : " &lt; detailed explanation of why the score was given , referencing the importance and impact of the proposal &gt; " , } , " OverallAssessment " : { " score " : &lt;1 -10 &gt; , " strengths " : [ " &lt; strength 1 &gt; " , " &lt; strength 2 &gt; " ] , " weaknesses " : [ " &lt; weakness 1 &gt; " , " &lt; weakness 2 &gt; " ] } } Please make sure the answer is strictly in valid JSON format.E Human Study DetailsTo validate the effectiveness of MLR-Judge, we compare its evaluations against those of human reviewers.Specifically, we recruited 10 participants with prior reviewing experience at top-tier conferences aligned with our task domains, including ICML, NeurIPS, and ICLR.Based on their areas of expertise, each reviewer was assigned a subset of AI-generated papers relevant to their
Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, Dj Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, Litllms, LLMs for literature review: Are we there yet? Transactions on Machine Learning Research. 2025</p>
<p>Claude 3.7 sonnet and claude code. Anthropic, 2025</p>
<p>ResearchAgent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. the 2025 Conference of the Nations of the Americas ChapterAssociation for Computational Linguistics20251</p>
<p>MLE-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Aleksander Patwardhan, Lilian Madry, Weng, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, Lichao Sun, arXiv:2402.04788Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. 2024arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, 2024</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>Gemini 2.5 pro preview model card. Google, 2025</p>
<p>Gemini cli: your open-source ai agent. Google, 2025</p>
<p>Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Eric Huang, Stefan Xie, Aidong Bekiranov, Zhang, arXiv:2411.02429Ideabench: Benchmarking large language models for research idea generation. 2024arXiv preprint</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, International Conference on Machine Learning. PMLR2024</p>
<p>Openreviewer: A specialized large language model for generating critical scientific paper reviews. Maximilian Idahl, Zahra Ahmadi, NAACL 2025 System Demonstrations Track. 2024</p>
<p>SWE-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. 2024</p>
<p>AgentReview: Exploring peer review dynamics with LLM agents. Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, Jindong Wang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2024</p>
<p>DSBench: How far are data science agents from becoming data science experts?. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.131852024arXiv preprint</p>
<p>Learning to generate research idea with dynamic control. Ruochen Li, Liqiang Jing, Xinya Du, 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle. 2024</p>
<p>Ryan Liu, Nihar B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.04306Llm4sr: A survey on large language models for scientific research. 2025arXiv preprint</p>
<p>Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering?. Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke, 2025</p>
<p>. OpenAI. Introducing codex. 2025</p>
<p>Introducing openai o3 and o4-mini. Openai, 2025</p>
<p>Let's get to the point: Llm-supported planning, drafting, and revising of research-paper blog posts. Marissa Radensky, Joseph Chee Daniel S Weld, Pao Chang, Jonathan Siangliulue, Bragg, arXiv:2406.103702024arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Paper2code: Automating code generation from scientific papers in machine learning. Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang, arXiv:2504.171922025arXiv preprint</p>
<p>Can LLMs generate novel research ideas? a largescale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. Paperbench: Evaluating ai's ability to replicate ai research. 2025</p>
<p>Un ministral, des ministraux. A I Mistral, Team, 2024</p>
<p>Qwen3: Think deeper, act faster. Qwen Team, 2025</p>
<p>Can llm feedback enhance review quality? a randomized study of 20k reviews at iclr. Nitya Thakkar, Mert Yuksekgonul, Jake Silberg, Animesh Garg, Nanyun Peng, Fei Sha, Rose Yu, Carl Vondrick, James Zou, 2025. 2025</p>
<p>Scicode: A research coding benchmark curated by scientists. Minyang Tian, Luyu Gao, Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Jamie CallanEliu A Huerta, and Hao Peng2024</p>
<p>Openhands: An open platform for ai software developers as generalist agents. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, The Thirteenth International Conference on Learning Representations. 2024</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Re-bench: Evaluating frontier ai r&amp;d capabilities of language model agents against human experts. Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, arXiv:2411.151142024arXiv preprint</p>
<p>Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen, Omnithink: Expanding knowledge boundaries in machine writing through thinking. 2025</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>SWE-agent: Agent-computer interfaces enable automated software engineering. John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Ofir Karthik R Narasimhan, Press, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>SWEbench multimodal: Do ai systems generalize to visual software domains?. John Yang, Carlos E Jimenez, Alex L Zhang, Kilian Lieret, Joyce Yang, Xindi Wu ; Muennighoff, Gabriel Synnaeve, R Karthik, Diyi Narasimhan, Yang, The Thirteenth International Conference on Learning Representations. NiklasSida I. Wang, and Ofir Press2025</p>
<p>Are we there yet? revealing the risks of utilizing large language models in scholarly peer review. Rui Ye, Xianghe Pang, Jingyi Chai, Jiaao Chen, Zhenfei Yin, Zhen Xiang, Xiaowen Dong, Jing Shao, Siheng Chen, 2024</p>
<p>Sciarena: An open evaluation platform for foundation models in scientific literature tasks. Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse Dodge, Matt Latzke, arXiv:2507.010012025arXiv preprint</p>
<p>Judging LLM-as-ajudge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>-Excellent: Results are analyzed in depth. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, Jürgen Schmidhuber, arXiv:2410.10934Agent-as-a-judge: Evaluate agents with agents. 2024arXiv preprintwith highly insightful interpretations and valuable implications for the field</p>
<p>Good: Results are thoughtfully analyzed, with some meaningful insights. </p>
<p>6 -Moderate: Some insights are provided, but analysis is relatively superficial. </p>
<p>Weak: Little meaningful analysis or interpretation is provided. </p>
<p>2 -Poor: No insight or thoughtful analysis is present. </p>
<p>How important or impactful are the experiment results for the field? Do they address a critical problem or open new research directions? 9-10 -Excellent: Results are highly significant, addressing critical problems or opening important new directions. </p>
<p>Good: Results are significant and make a clear contribution. </p>
<p>6 -Moderate: Results are somewhat significant, but impact is limited. </p>
<p>Weak: Results have little significance or impact. </p>
<p>Poor: Results are insignificant or irrelevant. </p>
<p>. Overall Assessment. </p>
<p>10 -Outstanding: The experimental work is exemplary in every respect, demonstrating exceptional quality, rigor, and impact. All aspects are handled with great care and expertise, with no significant weaknesses. The work sets a high standard and is likely to have a major influence in its field. Provide an overall assessment of the experimental work, considering all the above dimensions</p>
<p>Excellent: The work is very strong overall, with clear strengths across most or all dimensions. Any weaknesses are minor and do not detract from the overall quality or credibility. The experimental work is well-executed, insightful, and makes a significant contribution. </p>            </div>
        </div>

    </div>
</body>
</html>
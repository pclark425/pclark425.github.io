<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8342 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8342</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8342</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-277451612</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.23487v2.pdf" target="_blank">Large Language and Reasoning Models are Shallow Disjunctive Reasoners</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse. In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning. The setting allows fine control over problem difficulty to precisely measure OOD generalization. We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting. Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multi-path generalization. We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8342.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8342.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-reasoning LLMs (STaR eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-reasoning Large Language Models evaluated on the STaR benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source instruction-tuned LLMs (Qwen-2.5 variants, Gemma-2, Llama-3, Phi-4, etc.) were evaluated on STaR, a qualitative spatial/temporal reasoning benchmark (RCC-8 and Allen Interval Algebra) using prompt-provided composition tables in zero-shot, few-shot and fine-tuned settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5 (7B/14B/72B), Gemma-2 (9B/27B), Llama-3 (8B/70B), Phi-4 (14B), others</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Collection of open-source transformer LLMs (instruction-tuned variants). Some models were run locally with 4-bit quantization; models include distilled R1 variants for some Qwen models. Evaluations include zero-shot, few-shot and local fine-tuning runs. Non-reasoning models do not expose internal chain-of-thought tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 8B, 9B, 14B, 27B, 70B, 72B (varies by model)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>STaR benchmark (RCC-8 and Interval Algebra / IA)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Qualitative spatial and temporal reasoning; graph-labelled relational puzzle where edges are base relations and final relation between two designated nodes must be inferred by composing path relations (graph-based relational / combinatorial spatial reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Prompt includes full composition table (encoded as integers/powers of two), graph edges encoded as integers; models asked to output integer encoding of target relation. Settings: zero-shot, few-shot (with small number of in-context examples), and fine-tuning (single epoch of local fine-tuning with quantized adapters). Non-reasoning models limited to ~256 output tokens at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard prompting / instruction-following; few-shot in-context examples when applicable. No explicit internal CoT tokens exposed for most non-reasoning models; fine-tuned models appear to learn dataset heuristics (e.g., trivial-path heuristic) rather than perform explicit algebraic-closure simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot: performance close to random for most instances except very simple cases (b ≤ 2, k ≤ 4); Qwen-2.5-72B reported as strongest among non-reasoning models, remaining only weakly above chance for some single-path cases. Few-shot: non-trivial performance only for k ≤ 3; Qwen-2.5-72B improved in few-shot. Fine-tuned: substantially better overall (numerical per-config examples in paper), but performance remains far below neuro-symbolic baselines. (Paper provides per-(k,b) accuracy plots and model-by-model comparisons.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Partial: models can apply some composition rules when data/fine-tuning expose trivial patterns; however behavior indicates reliance on learned heuristics (e.g., trivial-path detection) rather than systematic multi-path composition. Quantitative evidence: accuracy degrades with increasing path length k and especially with number of simple s-t paths b.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against reasoning LRMs (o3-mini, Qwen R1) and against state-of-the-art neuro-symbolic methods (from Khalid & Schockaert 2025) which achieve near-perfect results. Non-reasoning LLMs underperform reasoning LRMs in single-path settings but lag in multi-path settings compared to neuro-symbolic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fail on multi-path (disjunctive) instances where multiple paths must be intersected; often only able to exploit trivial paths where the solution follows immediately. Struggle with systematic generalization (longer k, more b), and with composition that requires enumerating many s-t paths. Fine-tuning improves some classes but does not fix multi-path generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language and Reasoning Models are Shallow Disjunctive Reasoners', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8342.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8342.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned LLMs (Qwen2.5-14B etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Locally fine-tuned LLMs (example: Qwen-2.5-14B fine-tuned) on STaR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned LLMs further fine-tuned on small STaR training instances (k ∈ {2,3,4}, b ∈ {1,2,3}) and evaluated on larger OOD test instances; show notable gains but these gains are explained by heuristic strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-14B (example fine-tuned), also fine-tuned variants of Qwen family and Gemma-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLMs fine-tuned with 4-bit quantized adapters and AdamW (learning rate 2e-4, single epoch). Fine-tuning done locally to attempt to learn the composition table from training instances.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B (example), other sizes used in fine-tuning include 7B, 27B etc.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>STaR (RCC-8 and IA)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Graph-based qualitative spatial/temporal reasoning; requires composing relations along paths and intersecting candidate relation sets from multiple paths (disjunctive reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Composition table included in prompt (integer encoding). Fine-tuning performed on small-informationally-complete instances; evaluation on OOD larger k and b. Output requested as integer encoding. Fine-tuned inference used max 256 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Fine-tuned models learn dataset-specific heuristics (not full ACA simulation). They reliably detect and exploit 'trivial paths' (paths consisting only of eq and at most one other relation) and thereby predict relations for many test instances without multi-path composition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Fine-tuned models achieve markedly higher accuracy than zero/few-shot; per-relation breakdown (example Qwen-2.5-14B on k=9,b=2) shows perfect or near-perfect scores on relations predictable by trivial paths (e.g., eq) while other relations remain poor. Exact per-(k,b) numbers shown in paper (plots and Table 3). Still far below neuro-symbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Mixed: qualitative and quantitative analysis (fine-grained per-relation F1 table) shows that improvements are concentrated on relations solvable by simple heuristics; little evidence of true multi-path disjunctive reasoning. Paper argues fine-tuned LLMs have learned tricks rather than systematic application of composition table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms zero-shot/few-shot LLMs on many configurations, but o3-mini (a reasoning model) outperforms non-fine-tuned LLMs in zero-shot single-path cases; neuro-symbolic methods outperform fine-tuned LLMs by a large margin.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fail to generalize to multi-path disjunctive instances; high accuracy concentrated on easy relations that can be predicted by trivial-path heuristic. Fine-tuning does not yield principled multi-path generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language and Reasoning Models are Shallow Disjunctive Reasoners', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8342.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8342.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reasoning LRMs (o3-mini, Qwen R1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Reasoning Models evaluated on STaR (example: o3-mini and R1-distilled Qwen models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LRMs (post-trained reasoning models) were evaluated zero-shot on STaR; they can more systematically apply composition rules along a single path (using chain-of-thought / thinking tokens) but fail to robustly combine multiple paths (disjunctive reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini (OpenAI reasoning model), Qwen R1-distilled reasoning variants (7B, 14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LRMs are instruction-and-reinforcement/post-trained models designed to leverage chain-of-thought or thinking tokens (CoT) and longer test-time computation to search solution spaces. o3-mini is a closed-source LRM (CoT hidden), Qwen R1 are distilled reasoning models run locally with large max output tokens and R1 distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>o3-mini (size not specified), Qwen-R1: 7B, 14B (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>STaR (RCC-8 and IA)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Qualitative spatial/temporal reasoning; requires composing relations per path and intersecting sets across multiple simple paths (disjunctive multi-path reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Composition table included in prompt; reasoning models allowed very large max output tokens (Qwen reasoning models: max 8192 tokens; o3-mini: max 15000). Evaluations primarily zero-shot for reasoning models. CoT tokens available for some models; for o3-mini CoT is hidden (but output behavior indicates CoT usage).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-thought based path enumeration and composition; models attempt to enumerate s-t paths and compute relational compositions. Behavior characterized as 'shallow simulation' of the Algebraic Closure Algorithm (ACA) — they attempt relational composition, union and intersection but cannot fully enumerate/intersect all path-derived sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>o3-mini: strong zero-shot on single-path cases (example: k=9,b=1 accuracy 0.90, F1 0.85), but performance drops quickly for b≥2 (example: k=9,b=2 acc 0.48 F1 0.38; k=9,b=3 acc 0.30 F1 0.24). Qwen R1 models: for many b≥2 settings results are below random; for k=9,b=1 results can be above random but worse than o3-mini. (Paper Table excerpts report these numbers.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Substantial evidence of partial spatial reasoning: o3-mini appears to interpret the composition table and follow it systematically for single-path instances (non-trivial accuracy across many relation types). CoT analyses (where available) show enumeration of some s-t paths, but measured coverage of s-t paths recovered from CoTs declines exponentially with instance difficulty. Performance degrades linearly with number of intersection operations (in-degree minus one), supporting diagnosis that LRMs are shallow ACA simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>o3-mini outperforms non-reasoning LLMs in zero-shot single-path settings but does not outperform best fine-tuned LLMs overall; all LRMs and LLMs are outperformed by neuro-symbolic methods which achieve near-perfect results on these STaR instances. Distilled Qwen-R1 models perform worse than o3-mini in multi-path cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Major limitation: inability to reliably combine multiple reasoning paths (disjunctive reasoning). CoT path-coverage is low (exponential decline), models give up earlier (fewer output tokens) as b increases, and their computed sets remain incomplete—hence they are 'shallow' ACA simulators. Distillation or size scaling improves trivial-path exploitation but not principled multi-path generalization. Evaluation constrained by compute / hidden CoT for closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language and Reasoning Models are Shallow Disjunctive Reasoners', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spatial logic based on regions and connection (RCC-8) / related STaR description (Khalid and Schockaert) <em>(Rating: 2)</em></li>
                <li>Can large language models reason about the region connection calculus? <em>(Rating: 2)</em></li>
                <li>SPARTQA <em>(Rating: 1)</em></li>
                <li>StepGame: A new benchmark for robust multi-hop spatial reasoning in texts <em>(Rating: 1)</em></li>
                <li>RoomSpace <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8342",
    "paper_id": "paper-277451612",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Non-reasoning LLMs (STaR eval)",
            "name_full": "Non-reasoning Large Language Models evaluated on the STaR benchmark",
            "brief_description": "Open-source instruction-tuned LLMs (Qwen-2.5 variants, Gemma-2, Llama-3, Phi-4, etc.) were evaluated on STaR, a qualitative spatial/temporal reasoning benchmark (RCC-8 and Allen Interval Algebra) using prompt-provided composition tables in zero-shot, few-shot and fine-tuned settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5 (7B/14B/72B), Gemma-2 (9B/27B), Llama-3 (8B/70B), Phi-4 (14B), others",
            "model_description": "Collection of open-source transformer LLMs (instruction-tuned variants). Some models were run locally with 4-bit quantization; models include distilled R1 variants for some Qwen models. Evaluations include zero-shot, few-shot and local fine-tuning runs. Non-reasoning models do not expose internal chain-of-thought tokens.",
            "model_size": "7B, 8B, 9B, 14B, 27B, 70B, 72B (varies by model)",
            "puzzle_name": "STaR benchmark (RCC-8 and Interval Algebra / IA)",
            "puzzle_type": "Qualitative spatial and temporal reasoning; graph-labelled relational puzzle where edges are base relations and final relation between two designated nodes must be inferred by composing path relations (graph-based relational / combinatorial spatial reasoning).",
            "task_setup": "Prompt includes full composition table (encoded as integers/powers of two), graph edges encoded as integers; models asked to output integer encoding of target relation. Settings: zero-shot, few-shot (with small number of in-context examples), and fine-tuning (single epoch of local fine-tuning with quantized adapters). Non-reasoning models limited to ~256 output tokens at inference.",
            "mechanisms_or_strategies": "Standard prompting / instruction-following; few-shot in-context examples when applicable. No explicit internal CoT tokens exposed for most non-reasoning models; fine-tuned models appear to learn dataset heuristics (e.g., trivial-path heuristic) rather than perform explicit algebraic-closure simulation.",
            "performance_metrics": "Zero-shot: performance close to random for most instances except very simple cases (b ≤ 2, k ≤ 4); Qwen-2.5-72B reported as strongest among non-reasoning models, remaining only weakly above chance for some single-path cases. Few-shot: non-trivial performance only for k ≤ 3; Qwen-2.5-72B improved in few-shot. Fine-tuned: substantially better overall (numerical per-config examples in paper), but performance remains far below neuro-symbolic baselines. (Paper provides per-(k,b) accuracy plots and model-by-model comparisons.)",
            "evidence_of_spatial_reasoning": "Partial: models can apply some composition rules when data/fine-tuning expose trivial patterns; however behavior indicates reliance on learned heuristics (e.g., trivial-path detection) rather than systematic multi-path composition. Quantitative evidence: accuracy degrades with increasing path length k and especially with number of simple s-t paths b.",
            "comparisons": "Compared against reasoning LRMs (o3-mini, Qwen R1) and against state-of-the-art neuro-symbolic methods (from Khalid & Schockaert 2025) which achieve near-perfect results. Non-reasoning LLMs underperform reasoning LRMs in single-path settings but lag in multi-path settings compared to neuro-symbolic solvers.",
            "limitations_or_failure_cases": "Fail on multi-path (disjunctive) instances where multiple paths must be intersected; often only able to exploit trivial paths where the solution follows immediately. Struggle with systematic generalization (longer k, more b), and with composition that requires enumerating many s-t paths. Fine-tuning improves some classes but does not fix multi-path generalization.",
            "uuid": "e8342.0",
            "source_info": {
                "paper_title": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Fine-tuned LLMs (Qwen2.5-14B etc.)",
            "name_full": "Locally fine-tuned LLMs (example: Qwen-2.5-14B fine-tuned) on STaR",
            "brief_description": "Instruction-tuned LLMs further fine-tuned on small STaR training instances (k ∈ {2,3,4}, b ∈ {1,2,3}) and evaluated on larger OOD test instances; show notable gains but these gains are explained by heuristic strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-14B (example fine-tuned), also fine-tuned variants of Qwen family and Gemma-2",
            "model_description": "Open-source transformer LLMs fine-tuned with 4-bit quantized adapters and AdamW (learning rate 2e-4, single epoch). Fine-tuning done locally to attempt to learn the composition table from training instances.",
            "model_size": "14B (example), other sizes used in fine-tuning include 7B, 27B etc.",
            "puzzle_name": "STaR (RCC-8 and IA)",
            "puzzle_type": "Graph-based qualitative spatial/temporal reasoning; requires composing relations along paths and intersecting candidate relation sets from multiple paths (disjunctive reasoning).",
            "task_setup": "Composition table included in prompt (integer encoding). Fine-tuning performed on small-informationally-complete instances; evaluation on OOD larger k and b. Output requested as integer encoding. Fine-tuned inference used max 256 tokens.",
            "mechanisms_or_strategies": "Fine-tuned models learn dataset-specific heuristics (not full ACA simulation). They reliably detect and exploit 'trivial paths' (paths consisting only of eq and at most one other relation) and thereby predict relations for many test instances without multi-path composition.",
            "performance_metrics": "Fine-tuned models achieve markedly higher accuracy than zero/few-shot; per-relation breakdown (example Qwen-2.5-14B on k=9,b=2) shows perfect or near-perfect scores on relations predictable by trivial paths (e.g., eq) while other relations remain poor. Exact per-(k,b) numbers shown in paper (plots and Table 3). Still far below neuro-symbolic methods.",
            "evidence_of_spatial_reasoning": "Mixed: qualitative and quantitative analysis (fine-grained per-relation F1 table) shows that improvements are concentrated on relations solvable by simple heuristics; little evidence of true multi-path disjunctive reasoning. Paper argues fine-tuned LLMs have learned tricks rather than systematic application of composition table.",
            "comparisons": "Outperforms zero-shot/few-shot LLMs on many configurations, but o3-mini (a reasoning model) outperforms non-fine-tuned LLMs in zero-shot single-path cases; neuro-symbolic methods outperform fine-tuned LLMs by a large margin.",
            "limitations_or_failure_cases": "Fail to generalize to multi-path disjunctive instances; high accuracy concentrated on easy relations that can be predicted by trivial-path heuristic. Fine-tuning does not yield principled multi-path generalization.",
            "uuid": "e8342.1",
            "source_info": {
                "paper_title": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reasoning LRMs (o3-mini, Qwen R1)",
            "name_full": "Large Reasoning Models evaluated on STaR (example: o3-mini and R1-distilled Qwen models)",
            "brief_description": "LRMs (post-trained reasoning models) were evaluated zero-shot on STaR; they can more systematically apply composition rules along a single path (using chain-of-thought / thinking tokens) but fail to robustly combine multiple paths (disjunctive reasoning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o3-mini (OpenAI reasoning model), Qwen R1-distilled reasoning variants (7B, 14B)",
            "model_description": "LRMs are instruction-and-reinforcement/post-trained models designed to leverage chain-of-thought or thinking tokens (CoT) and longer test-time computation to search solution spaces. o3-mini is a closed-source LRM (CoT hidden), Qwen R1 are distilled reasoning models run locally with large max output tokens and R1 distillation.",
            "model_size": "o3-mini (size not specified), Qwen-R1: 7B, 14B (examples)",
            "puzzle_name": "STaR (RCC-8 and IA)",
            "puzzle_type": "Qualitative spatial/temporal reasoning; requires composing relations per path and intersecting sets across multiple simple paths (disjunctive multi-path reasoning).",
            "task_setup": "Composition table included in prompt; reasoning models allowed very large max output tokens (Qwen reasoning models: max 8192 tokens; o3-mini: max 15000). Evaluations primarily zero-shot for reasoning models. CoT tokens available for some models; for o3-mini CoT is hidden (but output behavior indicates CoT usage).",
            "mechanisms_or_strategies": "Chain-of-thought based path enumeration and composition; models attempt to enumerate s-t paths and compute relational compositions. Behavior characterized as 'shallow simulation' of the Algebraic Closure Algorithm (ACA) — they attempt relational composition, union and intersection but cannot fully enumerate/intersect all path-derived sets.",
            "performance_metrics": "o3-mini: strong zero-shot on single-path cases (example: k=9,b=1 accuracy 0.90, F1 0.85), but performance drops quickly for b≥2 (example: k=9,b=2 acc 0.48 F1 0.38; k=9,b=3 acc 0.30 F1 0.24). Qwen R1 models: for many b≥2 settings results are below random; for k=9,b=1 results can be above random but worse than o3-mini. (Paper Table excerpts report these numbers.)",
            "evidence_of_spatial_reasoning": "Substantial evidence of partial spatial reasoning: o3-mini appears to interpret the composition table and follow it systematically for single-path instances (non-trivial accuracy across many relation types). CoT analyses (where available) show enumeration of some s-t paths, but measured coverage of s-t paths recovered from CoTs declines exponentially with instance difficulty. Performance degrades linearly with number of intersection operations (in-degree minus one), supporting diagnosis that LRMs are shallow ACA simulators.",
            "comparisons": "o3-mini outperforms non-reasoning LLMs in zero-shot single-path settings but does not outperform best fine-tuned LLMs overall; all LRMs and LLMs are outperformed by neuro-symbolic methods which achieve near-perfect results on these STaR instances. Distilled Qwen-R1 models perform worse than o3-mini in multi-path cases.",
            "limitations_or_failure_cases": "Major limitation: inability to reliably combine multiple reasoning paths (disjunctive reasoning). CoT path-coverage is low (exponential decline), models give up earlier (fewer output tokens) as b increases, and their computed sets remain incomplete—hence they are 'shallow' ACA simulators. Distillation or size scaling improves trivial-path exploitation but not principled multi-path generalization. Evaluation constrained by compute / hidden CoT for closed models.",
            "uuid": "e8342.2",
            "source_info": {
                "paper_title": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spatial logic based on regions and connection (RCC-8) / related STaR description (Khalid and Schockaert)",
            "rating": 2,
            "sanitized_title": "spatial_logic_based_on_regions_and_connection_rcc8_related_star_description_khalid_and_schockaert"
        },
        {
            "paper_title": "Can large language models reason about the region connection calculus?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_reason_about_the_region_connection_calculus"
        },
        {
            "paper_title": "SPARTQA",
            "rating": 1
        },
        {
            "paper_title": "StepGame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "rating": 1,
            "sanitized_title": "stepgame_a_new_benchmark_for_robust_multihop_spatial_reasoning_in_texts"
        },
        {
            "paper_title": "RoomSpace",
            "rating": 1
        }
    ],
    "cost": 0.010263199999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language and Reasoning Models are Shallow Disjunctive Reasoners
2 Jun 2025</p>
<p>Irtaza Khalid khalidmi@cardiff.ac.uk 
School of Computer Science and Informatics
Cardiff University
United Kingdom</p>
<p>Amir Masoud Nourollah nourollaha@cardiff.ac.uk 
School of Computer Science and Informatics
Cardiff University
United Kingdom</p>
<p>Steven Schockaert schockaerts1@cardiff.ac.uk 
School of Computer Science and Informatics
Cardiff University
United Kingdom</p>
<p>Aaron Grattafiori 
Abhimanyu Dubey 
Abhinav Jauhri 
Abhinav Pandey 
Abhishek Kadian 
Ahmad Al- Dahle 
Aiesha Letman 
Akhil Mathur 
Alan Schelten 
Daya Guo 
Dejian Yang 
Haowei Zhang 
Junxiao Song 
Ruoyu Zhang 
Runxin Xu 
Qihao Zhu 
Shirong Ma 
Qihao Zhu 
Zhenda Xie 
Kai Dong 
Wentao Zhang 
Guanting Chen 
Xiao Bi 
Yu Wu 
Fangjun Li 
David C Hogg 
Anthony G Cohn 
Bill Yuchen Lin 
Ronan Le Bras 
Kyle Richardson 
Ashish Sabharwal 
Radha Poovendran 
Peter Clark 
Thomas Mccoy 
Shunyu Yao 
Dan Friedman 
Mathew D Hardy 
Thomas L 2024 Griffiths 
Pasquale Minervini 
Sebastian Riedel 
Pontus Stenetorp 
Maxwell Nye 
Anders Johan Andreassen 
Guy Gur-Ari 
Henryk Michalewski 
Jacob Austin 
David Bieber 
David Dohan 
Aitor Lewkowycz 
Maarten Bosma 
David Luan 
Charles Sutton 
Ahmed El-Kishky 
Alexander Wei 
Andre Saraiva 
Borys Minaev 
Daniel Selsam 
David Do- Han 
Francis Song 
Hunter Lightman 
Ignasi Clav- Era 
Jakub Pachocki 
Jerry Tworek 
Lorenz Kuhn 
Lukasz Kaiser 
Mark Chen 
Max Schwarzer 
Mostafa Rohaninejad 
Nat Mcaleese 
An Yang 
Baosong Yang 
Beichen Zhang 
Binyuan Hui 
Bo Zheng 
Bowen Yu 
Chengyuan Li 
Dayiheng Liu 
Fei Huang 
Haoran Wei 
Huan Lin 
Jian Yang 
Jianhong Tu 
Jianwei Zhang 
Jianxin Yang 
Jiaxi Yang 
Jingren Zhou 
Junyang Lin 
Kai Dang 
Keming Lu 
Keqin Bao 
Kexin Yang 
Le Yu 
Mei Li 
Mingfeng Xue 
Pei Zhang 
Qin Zhu 
Rui Men 
Runji Lin 
Tianhao Li 
Alec Radford 
Jeffrey Wu 
Rewon Child </p>
<p>Tianyi Tang
Tingyu Xia, Xingzhang Ren, Xuancheng Ren
Yang Fan, Yang SuYichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru</p>
<p>Large Language and Reasoning Models are Shallow Disjunctive Reasoners
2 Jun 2025C3FCFF393FEEEAEDA36B5F3318F70EEEarXiv:2503.23487v2[cs.AI]
Large Language Models (LLMs) have been found to struggle with systematic reasoning.Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples.Post-training strategies based on reinforcement learning and chain-ofthought prompting have recently been hailed as a step change.However, little is known about the potential of the resulting "Large Reasoning Models" (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse.In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning.The setting allows fine control over problem difficulty to precisely measure OOD generalization.We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting.Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multipath generalization.We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have shown remarkable generalization abilities, being able to learn from in-context demonstrations, and to generalize to unseen tasks in multi-task settings (Radford et al., 2019;Brown et al., 2020;Bubeck et al., 2023), with abilities in mathematics and programming that appear to go beyond the level of highschool students (Guo et al., 2024;Jimenez et al., 2024;OpenAI et al., 2025).Moreover, recent advances in post-training based on reinforcement learning have unlocked a further axis along which the ability of LLMs can be improved, for easily verifiable analytical problems (such as mathematics and programming) (Guo et al., 2025;Shao et al., 2024;Sprague et al., 2024).The resulting models, called Large Reasoning Models (LRMs), are then encouraged to leverage chains-of-thought (CoT) or thinking tokens (Wei et al., 2022) to search though a solution space, which provably increases the complexity of problems that can be tackled (Feng et al., 2023), compared to standard LLM prompting.</p>
<p>Yet, a competing narrative is that current LLMs are not, in fact, general-purpose reasoners and rather rely on shallow pattern matching (Dziri et al., 2023;McCoy et al., 2024;Nguyen, 2024) and heuristics (Nikankin et al., 2024).There are recurring issues, even with the latest LLMs and LRMs, such as memorization of training data (Zhang et al., 2024), the reversal curse (Berglund et al., 2024) and an over-reliance on co-occurrence statistics (Kang and Choi, 2023).This line of argument is further bolstered by the risk that popular static benchmarks, such as GSM8k and MMLU, may have been included in training corpora (Zhang et al., 2024;Oren et al., 2024).The potential for dataset contamination is increasingly problematic, given the scaling laws for memorization (Carlini et al., 2023), and may explain why despite displaying erudite behaviour, current models still fail at seemingly basic tasks that are trivial for ordinary humans.</p>
<p>In this paper, we highlight the importance of using benchmarks that require Systematic Generalization (SG) for reliably evaluating the reasoning capabilities of LLMs and LRMs.SG is the ability of a model to solve test instances by composing knowledge that was learned from multiple training instances (Hupkes et al., 2020), where the test instances are systematically made larger than the informationally complete training instances.Composing atomic units into larger pieces for constructing a solution to an arbitrarily large problem is an essential ingredient for machines and humans to generalize from a limited amount of data (Lake et al., 2017).We specifically advocate the use of synthetic benchmarks, where the difficulty of problem instances can be controlled along different dimensions.</p>
<p>For the analysis in this paper, we leverage the Spatial Temporal and Reasoning (STaR) benchmark (Khalid and Schockaert, 2025).Its problem instances have a combinatorial structure, which makes it straightforward to generate large numbers of previously unseen cases, and in particular avoid issues of dataset contamination.The StaR benchmark has proven challenging for state-of-theart neuro-symbolic reasoning methods (Minervini et al., 2020;Cheng et al., 2023;Lu et al., 2022), but has not yet been used for evaluating LLMs and LRMs.It poses an interesting challenge, because the disjunctive nature of the rules that govern the reasoning problems means that the answer cannot be obtained by a single derivation (i.e. a single chain-of-thought) and essentially requires simulating the algebraic closure algorithm (Renz and Ligozat, 2005).Note, however, that these problems are computationally tractable (i.e. they can be solved in polynomial time) and should thus, in principle, be within the reach of LRMs.This is fundamentally different from evaluating LRMs on PSPACE-hard planning problems, where at best strong heuristic approximations can be expected (Valmeekam et al., 2024).</p>
<p>Our main finding is that many popular LLMs and LRMs struggle on STaR but do reason beyond random chance.We find that LRMs are remarkably able to zero-shot exploit a trivial path heuristic.We analyze the effects of increasing model size, finetuning and CoT test-time compute on reasoning performance and provide a behavioral interpretation behind the reasoning abilities of models showing that they shallowly simulate the algebraic closure algorithm required for disjunctive reasoning.</p>
<p>Related Work</p>
<p>Spatial Reasoning The spatial reasoning capabilities of LLMs have already been studied from various angles.For instance, SPARTQA (Mirzaee et al., 2021), StepGame (Shi et al., 2022) and RoomSpace (Li et al., 2024b) are question answering datasets which require the model to infer the relative position of two objects based on a description of their position relative to other objects.However, rather than focusing on qualitative reasoning, these tasks involve geometric computations, e.g.determining if a given point belongs to some region or deter-mining the regions through which a given trajectory passes.Cohn and Blackwell (2024a) evaluate whether LLMs can infer the composition of two RCC-8 relations, when given a description of their meaning, while Cohn and Blackwell (2024b) evaluate their commonsense understanding of cardinal directions.Wang et al. (2024) consider spatial reasoning in a multi-modal setting.</p>
<p>Several authors have also tried to improve LLM spatial reasoning.Li et al. (2024a) study the effectiveness of chain-of-thought (Wei et al., 2022) and tree-of-thoughts (Yao et al., 2023) prompting.They also show the effectiveness of using the LLM for semantic parsing and leaving the reasoning to a symbolic solver.Wu et al. (2024) improve chainof-thought methods for spatial reasoning, by generating a visualization after each inference step.In multimodal settings, pre-training on synthetic data is common.Interestingly, Tang et al. (2024) found that by training the model on basic (visual) spatial reasoning capabilities, the model also performs better on out-of-distribution composite tasks, such as finding the shortest path between two objects.</p>
<p>Systematic Generalization</p>
<p>There is a plethora of work on measuring Systematic Generalization (SG) beyond relational reasoning, including SCAN (Lake and Baroni, 2018) for RNNs (Schuster andPaliwal, 1997), addition (Nye et al., 2021) and LEGO (Zhang et al., 2023), for trainable transformers (Vaswani, 2017).These works suggests that transformers struggle with SG.The most popular benchmark for SG for relational reasoning is CLUTRR Sinha et al. (2019), which involves predicting family relations.Zhu et al. (2024) evaluated LLMs on this benchmark, showing that even modern LLMs with CoT prompting struggle with this task.The problems we consider in this paper are more challenging than those in CLUTRR, due to the need for combining multiple reasoning paths.Sun et al. (2024) studied the ability of LLMs to apply a given rule, when provided as part of the prompt.In contrast to our experiments in this paper, they only evaluated the application of a single rule, some of which were complex (e.g.encoding the composition of a path of several relations).They found chain-of-thought prompting to be largely ineffective, which appears to be related to the fact that multi-hop reasoning was not required for their benchmark.They also found evidence that models rely on prior knowledge about the considered  ( 0 , 1 ) , ( 1 , 2 ) ] "</p>
<p>Rule-based Reasoning with LLMs</p>
<p>E d g e l a b e l s ( L _ i ) : " [ ' E C ' ' N T P P I ' ] "</p>
<p>Q u e r y E d g e ( ( 0 , n _ i ) ) : " ( 0 , 2 ) "  domains (e.g. the composition of family relations).</p>
<p>The STaR Problem</p>
<p>In each problem instance of STaR, we are given a set of facts F, referring to a set of binary relations R and a set of entities E. The set of relations is fixed across problem instances, but the entities are not.Each of the facts is an atom of the form r(a, b), with r ∈ R and a, b ∈ E. The problems we consider essentially require models to learn a set of rules K, which they can then use to decide whether a given atom r(a, b) can be inferred from the set of facts F. To be successful, models must be capable of composing the learned rules in a systematic way.In particular, most problem instances require multiple rule applications to be chained, and the number of such inference steps may be larger for test examples than for training examples.</p>
<p>Disjunctive Rules Most reasoning benchmarks focus on Horn rules of the following form (k ≥ 3):
r(X 1 , X k ) ← k−1 i=1 r i (X i , X i+1 ) (1)
where X i are entity variables.Given a set K of such rules, the main reasoning task of interest is typically to decide whether some hypothesis r ℓ (e, f ) can be inferred from a set of facts F using the rules in K.This can be decided by repeatedly selecting facts r 1 (e 1 , e 2 ), ..., r k−1 (e k−1 , e k ) that match the body of a rule of the form (1) in F and adding the conclusion r(e 1 , e k ) of that rule to F. This iterative derivation of facts is well-aligned with the style of reasoning that is enabled by chain-of-thought prompting, which can partially explain the success of such strategies for tasks that require simple logical reasoning.However, in many domains, Horn rules are not sufficient for capturing the required knowledge.A more general approach is to focus on disjunctive rules of the following form:
m i=1 s l (X 1 , X k ) ← k−1 i=1 r i (X i , X i+1 )(2)
Given such a rule and the facts r 1 (e 1 , e 2 ), ..., r k−1 (e k−1 , e k ), then all we can infer is that one of s 1 (e 1 , e k ), ..., s m (e 1 , e k ) must be true.When reasoning with disjunctive rules, we are typically also given a set of constraints, such as:
⊥ ← r 1 (X, Y ) ∧ r 2 (X, Y )
encoding that at most one of the facts r 1 (e, f ), r 2 (e, f ) can be true for any entities e, f .Reasoning with disjunctive rules is provably more expressive, but computationally also more expensive: while reasoning with Horn rules is possible in polynomial time, reasoning with disjunctive rules and constraints is an NP-complete problem.However, there are important special cases where reasoning with disjunctive rules is still possible in polynomial time.This is the case, in particular, for many of the calculi that have been proposed for qualitative reasoning about time and space, such as the Interval Algebra (IA (Allen, 1983)) and the Region Connection Calculus (RCC8 (Randell et al., 1992)).1</p>
<p>StaR Benchmark STaR (Khalid and Schockaert, 2025) consists of spatial and temporal reasoning problems.The spatial reasoning problems involve reasoning in RCC-8 (Randell et al., 1992).This calculus is defined using 8 relations, illustrated in Fig. 2 (Allen, 1983).The overall structure of these reasoning problems is similar as in RCC-8, but here there is a set of 13 JEPD relations.The entities in this case represent time intervals, and we have relations such as m(e, f ), encoding that the end point of e coincides with the starting point of f .Each problem instance is formulated as a directed labelled graph G, where the vertices represent entities and the edges are labelled with a relation from R, where R is either the set of RCC-8 relations or the set of IA relations.The goal is to infer the relationship that holds between two designated entities: a source entity s and a tail entity t.The problem instances are constructed such that there is a unique relation that can be inferred.To find this relation, however, information from</p>
<p>Model</p>
<p>Param.Quantization Reasoning
A B C Small Qwen-2.5 7B × × ✓ N/A Qwen-2.5 (R) 7B × × ✓ ✓ Llama-3 8B × × ✓ N/A Gemma-2 9B × × ✓ N/A Medium Phi-4 14B × × ✓ N/A Qwen-2.5 14B × × ✓ N/A Qwen-2.5 (R) 14B × × ✓ ✓ Gemma-2 27B × × ✓ N/A Large Llama-3.3 70B ✓ ✓ N/A N/A Qwen-2.5 72B ✓ ✓ N/A N/A o3-mini ? N/A N/A N/A ✓
Table 1: Model configurations for experimental settings in 4. All the quantizations are four-bit.(R) denotes the R1 distilled models (Guo et al., 2024).multiple paths between s and t may need to be combined.Each of these paths makes it possible to infer a conclusion of the form r 1 (s, t)∨...∨r m (s, t).</p>
<p>In other words, each path allows us to eliminate certain relationships as candidate answers, but we may need to combine several paths to eliminate all-but-one of the relations and thus obtain the answer.The dataset is constructed with two levers of complexity: b, the number of simple paths between the source and tail entity, and k, the length of each simple path.In accordance with the focus on SG, the training or fine-tuning data is comprised of small problem instances, with k ∈ {2, 3, 4} and b ∈ {1, 2, 3}.The test data contains instances with k ∈ {2, . . ., 10} and b ∈ {1, 2, 3, 4}.</p>
<p>Experimental Setup</p>
<p>Input Representation In principle, the only contextual information needed to solve an instance of STaR is the composition table.Khalid and Schockaert (2025) considered to what extent neurosymbolic models were able to learn (and then systematically apply) this composition table from the training data provided.Here, we focus on a simpler setting, where we provide the composition table as part of the prompt.Our main focus is thus on whether LLMs and LRMs are able to follow the instructions and apply the composition rules in a systematic way.This allows us to evaluate models in a zero-shot fashion, or with a small number of in-context demonstrations (as well as evaluat-2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b = 1   2 3 4 5 6 7 8 9 10  0 2   2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b = 3 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b = 4 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2 3 4 5 6 7 8 9 10  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0   2 3 4 5 6 7 8 9 10  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0   2 3 4 5 6 7 8 9 10  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0   2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Fine-tuned 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2 3 4 5 6 7 8 9 10  0 ing fine-tuned models which should in principle be able to learn the composition table ).We specify the composition table using a compact integer encoding (using powers of two; see the appendix for an example of the full prompt).The graph that defines a given problem instance is similarly encoded using integer labels.The model is furthermore instructed to provide the answer using the same integer encoding.This is illustrated in Fig. 1.</p>
<p>Zero-shot
.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b =</p>
<p>Few-shot
.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 k-hops Accuracy Meta-Llama-3-8B-Instruct Qwen2.5-7B-Instruct gemma-2-9b-it Qwen2.5-14B-Instruct phi-4 gemma-2-27b-it Llama-3.3-70B-Instruct Qwen2.5-72B-Instruct
Evaluation Setup To evaluate the models, for each combination of (k, b), we use a uniform subsample of the full set of test problem instances for RCC-8 and IA.For RCC-8, each of the eight relations appears equally frequently as gold labels, meaning that the performance of naive baselines such as random guessing is at 1/8 = 0.</p>
<p>Results</p>
<p>Systematicity results are divided into two sections, first focussing on the non-reasoning models in Section 5.1 (i.e. the standard LLMs), and then on the reasoning models in Section 5.2.</p>
<p>Non-reasoning Models</p>
<p>The results for RCC-8 are summarized in Figure 3 and for IA in Figure 4. Broadly, both of these are similar.We therefore focus on RCC-8 below.</p>
<p>For the zero-shot experiments, all models perform close to random guessing for all but the simplest problem instances.Somewhat better results are observed only when b ≤ 2 and k ≤ 4. Qwen2.5-72Boverall emerges as the strongest model.Its results remain clearly above random chance (although still very weak) for b = 1 and k ≥ 5.For lower values of k, gemma-2-9b and gemma-2-27b are the next best-peforming models.Interestingly, the much larger Llama-3.3-70Bmodel performs poorly for low values of k, but performs the best for b = 2, k = 10, and similar to Qwen2.5-72B for b = 1, k = 10.</p>
<p>2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Zero-shot b = 1 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b = 2 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b = 3 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b = 4 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Few-shot 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Fine-tuned 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2 3 4 5 6 7 8 9 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
k-hops Accuracy Meta-Llama-3-8B-Instruct Qwen2.5-7B-Instruct gemma-2-9b-it Qwen2.5-14B-Instruct phi-4 gemma-2-27b-it Llama-3.3-70B-Instruct Qwen2.5-72B-Instruct
Figure 4: The results for the non-reasoning models on IA for the 3 settings (accuracy).</p>
<p>The results for the few-shot experiments are similar, with non-trivial performance only achieved for k ≤ 3. Qwen2.5-72Bperforms consistently better than in the zero-shot case.The most interesting changes can be seen for b = 1, where some of the smaller models now perform notably better, especially Qwen2.5-14B,gemma-2-9b and phi-4.Finally, the results for the fine-tuned models are much better.We can see a noticeable performance gap, with the Qwen models and gemma-2-27b clearly outperforming the others.It is surprising to see that the performance for b = 2, b = 3 and b = 4 is similar, despite the latter setting being much harder.We will come back to this point in Section 6.In short, however, this is due to the fact that these models have learned to reliably predict some of the simplest relations, exploiting the trivial path heurisitc.A path is trivial if there is a (s-t) path between the source (s) and tail (t) entities that only consists of eq and at most one other relation.Then the solution is either the identity or that non-identity relation.The ability of these models to discover underlying principles, and reliably apply them in OOD settings is remarkable but it is clear that they are not capable of principled reasoning, as their performance on the hardest relations remains poor.The performance of all the considered models, even the best-performing fine-tuned models, remains far below that of state-of-the-art neuro-symbolic methods (Khalid and Schockaert, 2025), which achieve near-perfect results on these problem instances, de-
Conf. o3-mini Qwen 7B Qwen 14B (k, b) Acc F1 Acc F1 Acc. F1
RCC-8</p>
<p>(9, 3) 0.30 0.24 0.12 0.07 0.06 0.05 (9, 2) 0.48 0.38 0.06 0.02 0.26 0.23 (9, 1) 0.90 0.85 0.08 0.07 0.20 0.15 (8, 4) 0.44 0.35 0.10 0.08 0.16 0.12 (8, 3) 0.56 0.52 0.12 0.11 0.14 0.10 (5, 2) 0.68 0.63 0.12 0.07 0.24 0.19 IA (9, 3) 0.30 0.29 0.04 0.03 0.10 0.10 (9, 2) 0.44 0.42 0.06 0.04 0.22 0.18 (9, 1) 0.78 0.74 0.20 0.15 0.14 0.09 (8, 4) 0.36 0.30 0.04 0.06 0.12 0.07 (8, 3) 0.34 0.36 0.04 0.03 0.14 0.07 (5, 2) 0.56 0.52 0.04 0.03 0.18 0.11</p>
<p>Reasoning Models</p>
<p>For the reasoning models, we focus on the zeroshot evaluation setting.The results are summarized in Table 2.Note that we only include results for a sample of all (k, b) configurations due to the much higher cost that is involved in using these models.</p>
<p>Compared to the non-reasoning models without fine-tuning, the performance of o3-mini (OpenAI et al., 2025) is remarkably strong.The setting with b = 1 is intuitively well-aligned with the chain-of-Label Pr.</p>
<p>Re.</p>
<p>F1. Count</p>
<p>RCC-8 DC 0.14 0.31 0.20 13 EC 0.43 0.25 0.32 12 PD 0.14 0.18 0.16 11 TPP 1.00 0.09 0.17 11 NTPP 0.00 0.00 0.00 17 TPPI 0.72 1.00 0.84 13 NTPPI 0.68 1.00 0.81 13 EQ 1.00 1.00 1.00 10 IA = 0.14 0.83 0.24 6 &lt; 0.00 0.00 0.00 4 &gt; 0.00 0.00 0.00 9 d 1.00 0.10 0.18 10 di 0.00 0.00 0.00 9 o 1.00 0.57 0.73 7 oi 1.00 1.00 1.00 5 m 1.00 1.00 1.00 9 mi 1.00 0.67 0.80 6 s 1.00 1.00 1.00 9 si 1.00 1.00 1.00 8 f 1.00 0.83 0.91 6 fi 1.00 1.00 1.00 12</p>
<p>Table 3: Fine-grained breakdown of classification scores for the k = 9, b = 2 dataset configuration for the finetuned Qwen2.5-14BLLM.We sample 50 points randomly from each STaR dataset.</p>
<p>thought process.Accordingly, we can see that the model performs well for b = 1, even with k = 9, achieving an accuracy of 0.9, which is substantially higher than what any of the fine-tuned models has achieved.However, for b ≥ 2 the results quickly deteriorate.Interestingly, this behavior is qualitatively different from that of the fine-tuned models.</p>
<p>Where the fine-tuned models have learned to identify trivial path relations, o3-mini seems capable of interpreting the composition table and systematically applying it to a single reasoning path (although not with perfect accuracy, even for b = 1).For b ≥ 2, the disjunctive nature of the reasoning problem proves problematic, suggesting that the model is limited in its capacity to generalize to unseen reasoning tasks.For the distilled Deepseek-R1 models (Guo et al., 2025), the results are below random chance for all settings where b ≥ 2. For k = 9 and b = 1, the results are above random chance (except for Qwen 7B on RCC-8), but not meaningfully better than the non-reasoning models in the zero-shot setting.</p>
<p>6 Analysis</p>
<p>Fine-grained Classification Breakdown</p>
<p>In Section 5, we already saw that the behavior of the fine-tuned LLMs, on the one hand, and o3-mini, on the other hand, was qualitatively different.further analyze this, Table 3 shows a breakdown of the results per relation type, for one of the bestperforming fine-tuned models (Qwen2.5-14B).Table 4 shows the same breakdown for o3-mini.In both tables, we focus on the case where k = 9 and b = 2. Focusing on Table 3 first, for RCC-8 we can see that the fine-tuned Qwen2.5-14Bmodel achieves perfect results on eq, which can be explained by the fact that this relation can only be predicted if there is a trivial (s-t) path.For ntppi and tppi, the model was able to exploit a similar insight.The performance on the other relations, however, is much worse, although still better than random chance (except for ntpp).For IA, we can see a similar pattern.Some of the relations are easier to predict, with the model achieving perfect results on several relations: oi, m, s, si and fi.However, for other relations, the results are very poor.This again shows that the model was able to learn some "tricks" that allow it to reliably predict some of the easier relations, even on out-of-distribution settings, while at the same time failing to apply the rules from the composition table in a systematic way.</p>
<p>The results for o3-mini in Table 4 paint a dramatically different picture.First, note that o3-mini does not achieve perfect results on any of the relations.This shows that it was not able to lever-  2. The number of maximum tokens was set to 8192.age domain-specific insights (such as the idea that eq can only be predicted if there is a chain of eqrelations).On the other hand, the model achieves non-trivial results for almost all the relations.This suggests that the correct predictions are due to the ability of the model to follow the instructions from the composition table in a somewhat systematic, albeit error-prone way.</p>
<p>CoT Analysis</p>
<p>Reasoning models can adapt the number of output tokens, i.e. the amount of test-time compute, based on the difficulty of a given problem instance.To analyze this aspect, Figure 5 shows the number of output tokens that were generated by the Qwen 7B reasoning model.Note that we cannot do this analysis for o3-mini as the intermediate reasoning process is hidden for this model.Counterintuitively, the analysis in Figure 5 reveals that the number of output tokens goes down, as the number of paths b increases, for all the considered values of k.This seems to suggest that the model is aware of its limitations on these problem instances, giving up the reasoning process more quickly.In contrast, we can see that considerably more output tokens were used for k = 9, b = 1 than for k = 5, b = 1, which further supports our hypothesis that single-path problem instances are more natural for chain-ofthought based reasoning.</p>
<p>Shallow Algebraic Closure Algorithm Simulation</p>
<p>To solve disjunctive reasoning, the model needs to simulate a disjunctive reasoning algorithm, the algebraic closure algorithm (ACA) (Renz and Ligozat, 2005) where multiple possible solutions are refined iteratively during graph traversal.ACA is novel compared to reasoning algorithms for linearizable computation graphs (Dziri et al., 2023) or constraint satisfiability problems (Lin et al., 2025).Importantly, the intermediate nodes store partial solutions that are atomic whereas for disjunctive reasoning the nodes need to contain multiple possible solutions or sets.</p>
<p>ACA consists of 3 basic operations: relational composition, union and intersection, where the last two are operations on sets of possible relations.In addition, graph traversal is necessary to find paths between two nodes e.g. by simulating the Bellman-Ford algorithm.Our findings show that reasoning models are shallow ACA simulators by looking at the trend in performance as a function of these basic operations.All LRMs attempt to solve STaRtype problems using path-based reasoning which necessitates enumerating all possible paths between the source and tail nodes in the query edge.</p>
<p>Firstly, we analyze the CoT of open LRMs to quantify the fraction of unique source-to-tail (s-t) paths that is recovered by the models.Figure 6 shows, for Qwen 7B and 14B models on IA, an exponential decline in the coverage of paths per problem instance, in spite of being alloted ample CoT tokens (cf.App D for an example CoT and the companion RCC-8 analysis).Manually inspecting some CoT summaries from o3-mini (where full CoT is inaccessible) confirms similar behavior.This implies that the reasoning algorithms cannot properly simulate graph traversal, in line with search-related findings (Saparov et al., 2025).</p>
<p>Secondly, we quantify the number of intersections required per STaR problem instance by adding all in-degrees of a node in the graph minus 1.This is a strong predictor of problem difficulty as it directly quantifies its disjunctive multi-path nature.A clear linear trend in the decline in performance of o3-mini for RCC-8 and IA as a function of the number of intersections is shown in Figure 7 and occur with each edge-wise composition in the graph so it is harder to quantify their quality but similar noisier trends are observed in App.C.2. Thirdly, we confirm explicitly that all models zero-shot exploit the trivial path heuristic to solve problem instances that avoid a full computation.From the CoTs, since all the models reason by considering full s-t paths but are unable to enumerate them all, they are unable to always exploit this heuristic, making use of it only when it is seen.It is remarkable that LRMs are able to exploit this heuristics, as they were not trained on RCC-8 or IA problem instances (to the best of our knowledge).There is a separation in performance with respect to the presence of trivial paths in problem instances for all reasoning models on RCC-8 and IA and is shown in Figure 7(b)-(c).Moreover, performance improvement with respect to size scaling is also clearly observable.</p>
<p>Conclusions</p>
<p>We have studied the performance of recent LLMs and so-called Large Reasoning Models (LRMs) on a challenging benchmark involving qualitative spatial and temporal reasoning problems.This analysis allows us test the abilities of models on a different style of reasoning than those that are typically considered, and crucially, than those that are used for training LRMs.The setting requires composing relations, using rules that are specified in a composition table.A particular challenge arises because multiple "reasoning paths" need to be combined to arrive at the final answer, which is harder to capture using a chain-of-thought process.</p>
<p>Several insights arise from our analysis.While LLMs perform poorly in zero-shot and few-shot settings, fine-tuned LLMs achieved notably better results.However, further analysis shows that this is because fine-tuned models achieve near-perfect results on some of the easier test instances, i.e. relations that can be predicted by relying on simple rules and heuristics, rather than a systematic application of the composition table.In particular, these models still perform poorly on problem instances that require multi-path reasoning.As far as LRMs are concerned, o3-mini performs much better than LLMs in zero-shot and few-shot settings, but does not overall improve on the performance of fine-tuned LLMs.Interestingly, the behavior of the fine-tuned LLMs and o3-mini is qualitatively different.Indeed, o3-mini seems to rely more on an error-prone, but systematic application of the rules from the composition table, achieving strong results for problems involving only a single reasoning path.However, when multiple reasoning paths need to be combined, its performance deteriorates quickly.We further conduct a behavioral analysis of how the LRMs perform and find that they are shallow disjunctive reasoning algorithm simulators due to their inability to properly simulate crucial steps like graph traversal and intersection.</p>
<p>These results suggest that LRMs, despite demonstrating improved reasoning, are still limited in terms of their ability to generalize to previously unseen reasoning tasks.</p>
<p>Limitations</p>
<p>The state-of-the-art in reasoning models is still quickly changing, and any conclusions that can be drawn from current models, such as o3-mini, may quickly become obsolete as newer models are released.A key question, which remains unanswered, is whether reasoning models can be designed that generalize to previously unseen reasoning tasks.Furthermore, while we have advocated the use of temporal and spatial reasoning, further analysis is needed to test the reasoning abilities of current models on a broader range of problems, and to better understand their failure modes more generally.In terms of the considered models, we have focused our analysis on open-source models that can be run locally (with the exception of o3-mini), and quantization was used to make this possible.It is possible that fine-tuning larger models may lead to better results.Finally, only a limited set of (k, b) configurations was used to evaluate the reasoning models due to compute constraints.A Details on RCC-8 and IA Since it is not possible for more than one relation to hold between a and c, the only possibility is that po(a, c) holds.</p>
<p>In general, sound and complete reasoning in RCC-8 and IA is possible by using the algebraic closure algorithm (for the case where the initial information does not contain any disjunctions).This algorithm amounts to maintaining, for each pair of entities, a set of possible relations.These sets are iteratively refined by applying composition rules, until convergence.The algorithm runs in cubic time.The problem instances in the StaR benchmark are simpler than general RCC-8 and IA problems.For these instances, it always suffices to consider the paths between the designated entities h and t.Each path gives rise to a set of candidate relations, and the final answer is obtained by taking the intersection of these sets.The complexity of reasoning is thus linear in the number of entities.This ensures that the considered models should, in principle, be powerful enough to solve the problem instances, even for larger problems, and without needing an excessive number of output tokens for the LRMs.</p>
<p>B Implementation Details</p>
<p>B.1 Compute resources</p>
<p>All relevant hyperparameters were tuned using grid search, as detailed below.All experiments were conducted using RTX 4090 and RTX 6000 Ada NVIDIA GPUs.For the small models, the results for all (k, b) configurations, for the zero-shot, fewshot and fine-tuned settings, can be obtained in around 6-8 hours per model.For the large 70B models at 4-bit quantization, with a smaller sample size of 50 instances per (k, b) configuration, a single full run (i.e.24 (k, b) configurations) takes around 1 day.We use the unsloth library (Daniel Han and Table 5: RCC-8 composition table (Randell et al., 1992), excluding the trivial composition with eq.We write R 8 for the trivial case, where the composition consists of all eight relations.team, 2023) for fine-tuning all models with 4-bit quantization and the transformers library for downloading the weights and running all the open-source models locally (Wolf, 2020).</p>
<p>B.2 Hyper Parameters</p>
<p>We use the 8-bit quantized AdamW optimizer (Dettmers et al., 2021;Kingma and Ba, 2017) for fine-tuning the models.We use the same fine-tuning strategy and hyperparameters for all the models that are trained locally.For inference, the maximum output tokens for the non-reasoning models is set to 256.For fine-tuning we use a learning rate of 2 × 10 −4 with a maximum step size of 60 and weight decay with a linear scheduler for all the models.We use gradient accumulation with steps 4 and only fine-tune for 1 epoch since further training did not meaningfully improve the validation loss.To maximize GPU memory utilization with respect to model size, we make use of Flash attention (Dao et al., 2022) and quantized low rank adaptors (Dettmers et al., 2024).The adaptors are applied as Q, K, V, O, Gate, Up and Down projectors with hidden dimension size of 128 for all small and medium models and 64 for large models (the latter only because 128 could not fit in memory on the RTX 6000 Ada).</p>
<p>For the reasoning Qwen models in Table 2, we set the maximum output tokens to 8192, and for o3-mini this is set to 15000.</p>
<p>B.3 Data Statistics</p>
<p>The dataset statistics for the STaR benchmark for the training and test sets are summarized in the Table 7.These are respectively subsampled for the experimental evaluations in the main text.All random sampling is done with a global seed of 0 for reproducibility.Some example graphs gener-ated via this procedure for the RCC-8 dataset are displayed in Figure 14.</p>
<p>B.4 Prompts</p>
<p>The prompts used for non-fine tuning experiments for RCC-8 are shown in Fig. 9 with mutatis mutandis changes for IA and for IA for the instructiontuning setting in Fig. 10 with similar changes for RCC-8.We experimented with textual graph labels as opposed to integers in the prompt and the requested output format but found the accuracy and the adherence of the small models to be extremely poor in this setting with very low accuracies.</p>
<p>C Additional Analysis</p>
<p>C.1 Fine-grained breakdowns</p>
<p>Conducting a fine-grained classification level analysis of o3-mini for the instances where it thought for longer than 15000 tokens and responded with nothing over all the reasoning datasets is shown in figure 11.We find that o3-mini took unexpectedly longer for the trivial relations such as =, and for and fi for IA and po for RCC-8.</p>
<p>C.2 Shallow ACA simulation</p>
<p>We show the fraction of s-t paths recovered from the CoT for the RCC-8 dataset in Figure 12.The variation of o3-mini's performance with respect to the number of union operations is shown in Figure 13.We measure union indirectly by computing the average over s-t paths of the cardinality (size) of the final set of multiple possible relations after all the chained relational compositions per path for a single problem instance.The following is the composition table of RCC-8 as a JSON dictionary: {(1, 1): [], (1, 2): [1, 2, 4, 8, 16], ..., (128, 64): [64], (128, 128):</p>
<p>[128]} Now the question is: Given a consistent graph with edges comprising the 8 base relations, predict the label of the target edge.More specifically, Given a data row delimited by a comma with the following columns: <code>graph_edge_index</code>, <code>edge_labels</code>, <code>query_edge</code>, predict the label of the <code>query_edge</code>as one of the 8 base relations as a power of 2 as defined above.</p>
<p>(The optional few-shot examples: Example 1: [(0, 1), (1, 2)], ['EQ', 'NTPPI'], (0, 2) 64 ... Example 5: [(0, 1), (1, 2), (2, 3)], ['EQ', 'EQ', 'EC'], (0, 3) 2 Examples end here.)</p>
<p>[(0, 1), (1, 4), (0, 2), (2, 4), (0, 3), (3, 4)], ['EQ', 'NTPPI', 'EQ', 'NTPPI', 'TPPI', 'NTPPI'], (0, 4)</p>
<p>Input B.2: Interval Finetuning Prompt</p>
<p>Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.The following is the composition table of RCC-8 as a JSON dictionary: (eq, eq): [eq], (eq, lt): [lt],, ..., (fi, gt): [gt, oi, di, mi, si]} Now the question is: Given a consistent graph with edges comprising the 8 base relations, predict the label of the target edge.More specifically, Given a data row delimited by a comma with the following columns: <code>graph_edge_index</code>, <code>edge_labels</code>, <code>query_edge</code>, predict the label of the <code>query_edge</code>as one of the 8 base relations as a power of 2 as defined above.</p>
<h3>Input: [(0, 1), (1, 4), (0, 2), (2, 4), (0, 3), (3, 4)], ['m', '&gt;', 'di', 'fi', 'di', 'oi'], (0, 4) ### Response:    , 1983), excluding the trivial composition with =.</h3>
<p>Table 7: Data statistics of the STaR reasoning datasets.These are respectively subsampled for the experimental valuations in the main text.Figure 14: Some graph instances for the RCC-8 dataset generated using the procedure described in (Khalid and Schockaert, 2025).The target edge label between the source node and the tail node that needs to be predicted by the model is indicated by the dotted line.</p>
<p>D An example</p>
<p>CoT for an RCC-8 problem instance from R1 distilled Qwen-14B</p>
<p>Chain-of-Thought</p>
<p>Okay , so I 'm trying to figure out the label for the query edge (0 , 20) in the given graph .The graph has several edges with known labels , and I need to use the RCC -8 composition table to determine the possible label for the target edge .</p>
<p>First , let me list out the edges and their labels as provided :</p>
<p>1</p>
<p>I n s t r u c t i o n ( Q ) : Y o u a r e a h e l p f u l a s s i s t a n t .J u s t a n s w e r t h e q u e s t i o n a s a s i n g l e i n t e g e r .G i v e n a c o n s i s t e n t g r a p h w i t h e d g e s c o m p r i s i n g t h e 8 b a s e r e l a t i o n s , p r e d i c t t h e l a b e l o f t h e t a r g e t e d g e .M o r e s p e c i f i c a l l y , G i v e n a d a t a r o w d e l i m i t e d b y a c o m m a w i t h t h e f o l l o w i n g c o l u m n s : <code>g r a p h _ e d g e _ i n d e x</code>, <code>e d g e _ l a b e l s</code>, <code>q u e r y _ e d g e</code>, p r e d i c t t h e l a b e l o f t h e <code>q u e r y _ e d g e</code> a s o n e o f t h e 8 b a s e r e l a t i o n s a s a p o w e r o f 2 a s d e f i n e d a b o v e .C o m p o s i t i o n T a b l e ( T ) : T h e f o l l o w i n g a r e t h e b a s e e l e m e n t s o f R C C -8 : D C = 1 E C = 2 P O = 4 T P P = 8 . . .G r a p h E d g e I n d e x ( E _ i ) : " [</p>
<p>Figure 1 :
1
Figure 1: An illustration of the input representation to the language model which is prompted to respond (modulo thinking tokens) with a single label for the query edge.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the RCC-8 relations.</p>
<p>Figure 3 :
3
Figure 3: The results for the non-reasoning models on RCC-8 for the 3 settings (accuracy).</p>
<p>kFigure 5 :
5
Figure5: The median number of output tokens with the interquartile range for the Qwen 7B reasoning model for the same dataset splits as in Table2.The number of maximum tokens was set to 8192.</p>
<p>Figure 6 :
6
Figure 6: Fraction of source-to-tail paths recovered from the model's CoT for IA.</p>
<p>Figure 7 :
7
Figure 7: LRMs are shallow Algebraic Closure Algorithm (ACA) simulators.(a) o3-mini's performance on both RCC-8 and IA datasets degrades approximately linearly as a function of the number of intersection operations in a problem instance, which are required whenever the in-degree of a node in the graph is greater than 1. (b)-(c) o3-mini, R1 distilled Qwen-7B and Qwen-14B noisily degrade in performance as the number of source-tail paths in the problem instance increases.Performance scaling with model size is also observed.Remarkably, the models, increasingly with size, zero-shot exploit the trivial path heuristic for solving STaR problems.Error bars are ±1σ.</p>
<p>Figure 8 :
8
Figure 8: Illustration of the IA relations.</p>
<p>Figure 8
8
Figure 8 provides an illustration of the 13 relations of the interval algebra.The composition tables for RCC-8 and IA are shown respectively in Tables</p>
<p>Figure 9 :
9
Figure9: The given prompt is for the inference RCC-8 dataset, while the Interval prompt for inference has a similar structure but different base elements and composition table.</p>
<p>Input B.1: RCC8 Inference PromptSystem: You are a helpful assistant.Just answer the question as a single integer.User:You are a qualitative spatial and temporal reasoning expert specializing in RCC-8The following are the base elements of RCC-8</p>
<p>Figure 10 :
10
Figure10: The given prompt is for the finetuining interval dataset, while the RCC-8 prompt for finetuning has a similar structure but different base elements and composition table.</p>
<p>Figure 11 :Figure 12 :Figure 13
111213
Figure11: Non-responses from o3 where it took longer than the maximum allotted number of tokens.Certain classes are overrrepresented and for IA coincide with those that can easily predicted by leveraging heuristics based on dataset construction constraints.</p>
<p>Table 2 :
2
Zero-shot (setting (A)) results for the reasoning models on the STaR benchmark.The Qwen models are distilled R1 models which were run locally.The accuracies and macro F1 scores are reported for a sample of test configurations due to API resource constraints.</p>
<p>spite having to learn the composition table from training examples.</p>
<p>Table 4 :
4
To Fine-grained breakdown of classification scores for the k = 9, b = 2 dataset configuration for the o3mini LRM.We sample 50 points randomly from each STaR dataset.
LabelPr.Re.F1.CountDC 0.69 0.90 0.7810EC 0.50 1.00 0.673RCC-8PD 0.43 0.27 0.33 TPP 0.33 0.44 0.38 NTPP 1.00 0.20 0.33 TPPI 0.00 0.00 0.0011 9 5 2NTPPI 0.50 0.25 0.334EQ 0.75 0.50 0.606= 0.50 0.17 0.256&lt; 0.10 1.00 0.181&gt; 0.83 1.00 0.915d 0.50 0.60 0.555di 0.67 0.50 0.574o 0.00 0.00 0.002IAoi 0.75 1.00 0.863m 1.00 0.50 0.674mi 1.00 0.33 0.503s 1.00 0.20 0.335si 1.00 0.25 0.404f 0.33 0.50 0.402fi 1.00 0.17 0.296</p>
<p>Table 6 :
6
Allen's interval algebra composition table (Allen
&lt;&gt;ddiooimmissiffi&lt;, o,&lt;, o,&lt;, o,&lt;, o,&lt;&lt;m, d,&lt;&lt;m, d,&lt;m, d,&lt;<m, d,\<ssss>, oi,>, oi,&gt;, oi,&gt;, oi,&gt;&gt;mi, d,&gt;mi, d,&gt;mi, d,&gt;mi, d,&gt;&gt;&gt;ffff&lt;, o,&gt;, oi,&gt;, oi,&lt;, o,d&lt;&gt;dm, d,mi, d,&lt;&gt;dmi, d,dm, d,sffsdi&lt;, o, m, di, fi&gt;, oi, di, mi, sio, oi, fi, = d, s, f, di, si,dio, di, fioi, di, sio, di, fioi, di, sio, di, fidioi, di, sidio&lt;&gt;, oi, di, mi, sio, d, s&lt;, o, m, di, fi&lt;, o, mo, oi, fi, = d, s, f, di, si,&lt;oi, di, sioo, di, fio, d, s&lt;, o, moi&lt;, o, m, di, fi&gt;oi, d, f&gt;, oi, mi, di, sio, oi, fi, = d, di, s, si, f,&gt;, oi, mio, di, fi&gt;oi, d, foi, &gt;, mioioi, di, sim&lt;&gt;, oi, di, mi, sio, d, s&lt;&lt;o, d, s&lt;f, fi, =mmd, s, o&lt;mi&lt;, o, m, di, fi&gt;oi, d, f&gt;oi, d, f&gt;s, si, =&gt;d, f, oi&gt;mimis&lt;&gt;d&lt;, o, m, di, fi&lt;, o, moi, d, f&lt;miss, si, =d&lt;, m, osi&lt;, o, m, di, fi&gt;oi, d, fdio, di, fioio, di, fimis, si, =sioidif&lt;&gt;d&gt;, oi, mi, di, sio, d, s&gt;, oi, mim&gt;d&gt;, oi, miff, fi, =fi&lt;&gt;, oi, di, mi, sio, d, sdiooi, di, simsi, oi, diodif, fi, =fi
When the set F is allowed to contain disjunctions of facts, then reasoning with these calculi is NP-complete. However, since we only focus on the case where F is a set of facts, reasoning for our purposes is tractable in these calculi.
AcknowledgmentsThis work was supported by the EPSRC grant EP/W003309/1.Starting with the edge (0 ,1) : EQ (128) Then (1 ,4) : EQ (128) So , EQ composed with EQ .Looking at the composition table , (128 ,128) isn 't directly given , but I can check the entries .Wait , the composition table is given as a dictionary where each key is a tuple (a , b) , and the value is a list of possible results .Wait , no , actually , the composition table is for the base relations .Each entry is (x , y) mapping to a list of possible z where x R y and y S z implies x T z.But maybe I 'm overcomplicating .Alternatively , since each edge 's label is a base relation , we can compose them step by step .Let me think of the composition as follows : each step , we take the current relation and compose it with the next edge 's relation .Starting from 0 to 1: EQ (128) Then 1 to 4: EQ (128) So , EQ composed with EQ .Looking at the composition table , for (EQ , EQ ) , which is(128 ,128), but the table is structured as (a , b) where a and b are the labels .Wait , no , the composition table is for the base elements , not the powers .Wait , the base elements are DC =1 , EC =2 , PO =4 , etc .So , the labels are powers of 2, and the composition table is for the base relations , not the labels directly .Wait , perhaps I should convert the labels to their base relation numbers first .EQ is 128 , which is NTPPI ( since NTPPI is 64 , wait no , the base elements are : Alternatively , perhaps I can find the relations between 0 and 20 through other nodes .Looking at the graph , 0 is connected to 1, 2, 3, and others .Let 's see : Alternatively , perhaps I should consider that EQ composed with EQ is EQ , so 128.Then , 4 -5:4 ( EC ) So , EQ (128) composed with EC (4) .Let 's look for (128 ,4) in the composition table .Looking through the entries , I see (128 ,4) is not present .Hmm .Alternatively , perhaps I should think that EQ composed with EC would result in EC , but I 'm not sure .Alternatively , maybe I should consider that the composition of EQ and EC is EC , so 4.But I ' m not sure .This is getting complicated .Alternatively , perhaps I should consider that the query edge (0 ,20) is connected through multiple paths , and I need to find a relation that is consistent across all possible paths .But this might take a lot of time .Alternatively , perhaps I can look for the relations that are common in the paths .Wait , another approach : since the graph is consistent , the label for (0 ,20) must be such that all paths from 0 to 20 agree on the same label .Alternatively , perhaps I can find the relations along the path and see what the composition would be .Let me take the path 0 -1 -4 -5 -10 -11 -16 -17 -18 -20.The relations are : 0 -1:128 ( EQ ) 1 -4:128 ( EQ ) 4 -5:4 ( EC ) 5 -10:4 ( EC ) 10 -11:1 ( DC ) 11 -16:2 ( PO ) 16 -17:1 ( DC ) 17 -18:128 ( EQ ) 18 -20:128 ( EQ ) Now , let 's compose these step by step .Start with 128 ( EQ ).Then , compose with 128 ( EQ ).Let 's see what(128 ,128)gives .Since it 's not in the table , perhaps the result is 128.Then , compose with 4 ( EC ).So ,(128 ,4).Looking for (128 ,4) in the composition table .Not present .Hmm .Alternatively , perhaps the composition of EQ (128) and EC (4) would result in EC (4) .Then , compose with 4 ( EC ).So , 4 composed with 4 is (4 ,4) .Looking at the composition table , (4 ,4) is not present , but perhaps the result is 4.Then , compose with 1 ( DC ).So , (4 ,1) .Looking at the composition table , (4 ,1) is present and the result is[1 ,2 ,4 ,32 ,64].So , possible results are 1 ,2 ,4 ,32 ,64.Then , compose with 2 ( PO ).So , for each possible result from the previous step , we need to compose with 2.Wait , this is getting too complicated .Maybe I should try a different approach .Alternatively , perhaps the query edge (0 ,20) is connected through multiple paths , and the only consistent label is one that is possible in all paths .Looking at the possible paths , the relations along the way include EQ , EC , DC , PO , etc .But I ' m not making progress .Maybe I should look for the relations that are common in the paths .Alternatively , perhaps the label for (0 ,20) is EQ (128) , as many edges along the path are EQ .But I ' m not sure .Alternatively , perhaps I should consider that the composition of all the relations along the path would result in EQ .Wait , another idea : the path from 0 to 20 through 17 -18 -20 has EQ relations .So , maybe the final composition would be EQ .Alternatively , perhaps the label is EQ (128) .But I ' m not certain .Alternatively , maybe it 's DC (1) , but that seems less likely .Wait , considering the path 0 -1 -4 -5 -10 -11 -16 -17 -18 -20 , the last few steps are DC , PO , DC , EQ , EQ .So , composing DC (1) with PO (2) gives (1 ,2) which from the composition table is[1 ,2 ,4 ,8 ,16].Then , compose with DC (1) : So , for each possible result from (1 ,2) , which is[1 ,2 ,4 ,8 ,16], we compose with 1.Looking at (x ,1) where x is in[1 ,2 ,4 ,8 ,16], the results would be :From the composition table :( So , the possible results are 1 ,2 ,4 ,32 ,64.Then , compose with EQ (128) at 17 -18: So , for each possible result from the previous step , we compose with 128.Wait , but 17 -18 is 128 ( EQ ).So , the previous step 's result is composed with 128.But I ' m getting lost here .Alternatively , perhaps the final composition would be EQ (128) , as the last step is EQ .But I ' m not sure .Alternatively , perhaps the label is EQ (128) .But I ' m not certain .Maybe I should look for another approach .Wait , considering that node 20 is connected to 18 and 19 , which are both connected to 17. 17 is connected to 16 and 18. 16 is connected to 11 ,12 ,13 ,14 ,15 ,17.Looking at the edges from 16 to 17: DC (1) .From 17 to 18: EQ (128) From 17 to 19: EQ (128) From 18 to 20: EQ (128) From 19 to 20: EQ (128) So , the path from 16 to 20 is through 17 -18 or 17 -19 , both with EQ .So , the edge from 16 to 20 is through 17 , which is DC (1) to 17 , then EQ (128) to 18 or 19 , and then EQ (128) to 20.So , the composition from 16 to 20 would be DC (1) composed with EQ (128) composed with EQ (128) .So , first , (1 ,128) : Looking at the composition table , (1 ,128) is not present , but perhaps the result is 1, as DC composed with EQ might be EQ .Wait , but I 'm not sure .Alternatively , perhaps the composition of DC (1) and EQ (128) would result in EQ (128) , since DC is " disjoint " and EQ is " equal " , which might not make sense , but perhaps it 's possible .Alternatively , perhaps the composition is 128.Then , composing with another EQ (128) would give 128.So , the result from 16 to 20 would be 128 ( EQ ).Then , going back to 10 -11: DC (1) 11 -16: PO (2) So , DC (1) composed with PO (2) .Looking at (1 ,2) , which is[1 ,2 ,4 ,8 ,16].Then , compose with the result from 16 to 20 , which is 128.Wait , but 16 to 20 is 128 , so the path from 10 to 20 would be DC (1) composed with PO (2) composed with 128.So , first , (1 ,2) gives[1 ,2 ,4 ,8 ,16].Then , each of these composed with 128.For each x in[1 ,2 ,4 ,8 ,16], (x ,128) would be :Looking at the composition table:( So , the possible results would be 128 , 2, 4, 8, 16.Then , the next step is from 10 to 20 , which would be the composition of DC (1) , PO (2) , and 128.But I ' m getting stuck here .Alternatively , perhaps I should consider that the query edge (0 ,20) is connected through multiple paths , and the only consistent label is EQ (128) , as many edges along the path are EQ .Therefore , I think the label for the query edge (0 ,20) is EQ , which is 128.&lt;/ think &gt;The query edge (0 ,20) is determined by composing the relations along the path from 0 to 20.After considering multiple paths and their compositions , the most consistent result is EQ (128) .''' json {" label ": 128} '''
Jyoti Marah Abdin, Harkirat Aneja, Sébastien Behl, Ronen Bubeck, Suriya Eldan, Michael Gunasekar, Russell J Harrison, Mojan Hewett, Piero Javaheripi, James R Kauffmann, Yin Lee, Yuanzhi Tat Lee, Weishung Li, Liu, C T Caio, Anh Mendes, Eric Nguyen, Gustavo Price, Olli De Rosa, Adil Saarikivi, Shital Salim, Xin Shah, Rachel Wang, Yue Ward, Dingli Wu, Cyril Yu, Yi Zhang, Zhang, arXiv:2412.08905Phi-4 technical report. 2024Preprint</p>
<p>Maintaining knowledge about temporal intervals. Allen James, Communications of the ACM. 26111983</p>
<p>The reversal curse: LLMs trained on "a is b" fail to learn "b is a. Lukas Berglund, Meg Tong, Maximilian Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Neural compositional rule learning for knowledge graph reasoning. Kewei Cheng, Nesreen K Ahmed, Yizhou Sun, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Can large language models reason about the region connection calculus?. Anthony G Cohn, Robert E Blackwell, 10.48550/ARXIV.2411.19589CoRR, abs/2411.195892024a</p>
<p>Evaluating the ability of large language models to reason about cardinal directions (short paper). Anthony G Cohn, Robert E Blackwell, 10.4230/LIPICS.COSIT.2024.2816th International Conference on Spatial Information Theory, COSIT 2024. Schloss Dagstuhl -Leibniz-Zentrum für Informatik. Québec City, Canada2024b. September 17-20, 2024315</p>
<p>. Michael Han, Daniel Han, Unsloth Team, 2023Unsloth</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré, Advances in Neural Information Processing Systems. 202235</p>
<p>Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer, arXiv:2110.028612021. 8-bit optimizers via block-wise quantization. arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Xiang Sanyal, Ren, Thirty-seventh Conference on Neural Information Processing Systems. </p>
<p>Towards revealing the mystery behind chain of thought: A theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>A spatial logic based on regions and connection. David A Randell, Zhan Cui, Anthony G Cohn, Proceedings of the 3rd International Conference on Principles of Knowledge Representation and Reasoning (KR'92). the 3rd International Conference on Principles of Knowledge Representation and Reasoning (KR'92)Cambridge, MA, USAMorgan Kaufmann1992. October 25-29, 1992</p>
<p>Weak composition for qualitative spatial and temporal reasoning. Jochen Renz, Gérard Ligozat, 10.1007/11564751_40Principles and Practice of Constraint Programming -CP 2005, 11th International Conference, CP 2005. Lecture Notes in Computer Science. Sitges, SpainSpringer2005. October 1-5, 20053709</p>
<p>Seyed Mehran Kazemi, Najoung Kim, and He He. 2025. Transformers struggle to learn to search. Abulhair Saparov, Srushti Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar, The Thirteenth International Conference on Learning Representations. </p>
<p>Bidirectional recurrent neural networks. M Schuster, K K Paliwal, 10.1109/78.650093IEEE Transactions on Signal Processing. 45111997</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, 10.1609/AAAI.V36I10.21383arXiv:2402.03300Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event. AAAI Press2024. 2022. February 22 -March 1, 2022arXiv preprintStepgame: A new benchmark for robust multi-hop spatial reasoning in texts</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/V1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>To cot or not to cot? chain-ofthought helps mainly on math and symbolic reasoning. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett, arXiv:2409.121832024Preprint</p>
<p>Beyond instruction following: Evaluating rule following of large language models. Wangtao Sun, Chenxiang Zhang, Xueyou Zhang, Ziyang Huang, Haotian Xu, Pei Chen, Shizhu He, Jun Zhao, Kang Liu, 10.48550/ARXIV.2407.08440CoRR, abs/2407.084402024</p>
<p>Sparkle: Mastering basic spatial capabilities in vision language models elicits generalization to composite spatial reasoning. Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, Jinhua Zhao, 10.48550/ARXIV.2410.16162CoRR, abs/2410.161622024</p>
<p>Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, arXiv:2408.00118Gemma. 22024Preprint</p>
<p>Llms still can't plan; can lrms? A preliminary evaluation of openai's o1 on planbench. Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati, 10.48550/ARXIV.2409.13373CoRR, abs/2409.133732024</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Is A picture worth A thousand words? delving into spatial reasoning for vision language models. Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Neel Joshi, 10.48550/ARXIV.2406.14852CoRR, abs/2406.148522024</p>
<p>. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022thought prompting</p>
<p>Let 's see how 0 and 20 are connected . Looking at the graph , node 0 is connected to 1, 2, 3, and so on. Node 20 is connected to 18 and 19 , which are both connected to 17. Node 17 is connected to 16 and 18. Node 16 is connected to 11 , 12 , 13 , 14 , 15 , and 17. So , the path from 0 to 20 could go through several nodes . Let 's try to find the shortest path or the most direct path</p>            </div>
        </div>

    </div>
</body>
</html>
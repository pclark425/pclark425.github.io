<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6435 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6435</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6435</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-254246305</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2022.findings-emnlp.423.pdf" target="_blank">Language Models as Agent Models</a></p>
                <p><strong>Paper Abstract:</strong> Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6435.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6435.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toy RNN (Incoherent Encyclopedia)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>512-d LSTM trained on a synthetic 'Incoherent Encyclopedia' dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic experiment in this paper where a 512-dimensional LSTM language model is trained on documents authored by three agent types; the model's recurrent hidden state encodes author identity and can be manipulated to control generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>512-d LSTM (toy experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Autoregressive LSTM language model whose recurrent hidden state functions as an implicit per-document memory encoding author identity; the hidden state is read by the decoder for next-token prediction and can be set manually to condition generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>512-dimensional LSTM (hidden dim = 512)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal recurrent hidden state (RNN hidden memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>distributed hidden-vector encoding of author identity / document-local latent state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>standard RNN recurrence (hidden state updated token-by-token); read out by decoder and by linear probes; manipulated by initializing hidden state to prototype vectors</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic 'Incoherent Encyclopedia' next-token prediction / controlled generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>language modeling / controllable generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Author identity decodable from hidden state with 98% linear-probe accuracy; sampling from model yields per-document consistency: 31% A-type, 33% B-type, 36% O-type; fixing initial hidden state to A-type prototype produced A-type propositions 89% of the time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>linear-probe accuracy; percent of generations consistent with author type</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Demonstrates that a compact recurrent hidden state can store document-local agent state, but the model as a whole does not represent a single coherent global belief state across samples; no explicit latency/memory-footprint numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Model-level incoherence across samples persists (model samples as a whole are inconsistent), and the RNN hidden-state memory only captures fairly simple, low-dimensional aspects of agent state; not shown to scale to complex long-term agent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Jacob Andreas. Language Models as Agent Models. (manuscript)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Agent Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6435.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6435.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentiment Neuron</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to generate reviews and discovering sentiment (Radford et al., 2017) - 'sentiment neuron'</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work showing a single neuron in a large LSTM trained on product reviews encodes review sentiment and can be manipulated to control generated review sentiment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to generate reviews and discovering sentiment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Single-neuron-controlled LSTM (Radford et al. 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A single-layer LSTM trained as a language model on product reviews whose hidden-state contains a neuron strongly correlated with review sentiment; this hidden unit acts like a small, localized memory for sentiment and can be fixed to steer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>single-layer LSTM with 4096 hidden units (as reported by Radford et al. 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal LSTM hidden-state neuron (localized scalar memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>single neuron activation representing sentiment (scalar)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>read via linear probe/classifier; written implicitly by recurrence during encoding; manipulated by clamping neuron activation to high/low values</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Amazon product review language modeling; transfer evaluation on IMDB sentiment classification and controllable review generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>language modeling / sentiment representation / controllable generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>The sentiment neuron predicted binarized review scores with ~92% accuracy; clamping the neuron to extreme values strongly biased generated text sentiment (qualitative and quantitative shifts reported in Radford et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>classification accuracy for the probe (92% reported); qualitative control of generation</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Shows a compact, manipulable encoding of a communicative intention (sentiment) within hidden state; but generations could still be syntactically low-quality in that specific model, and the mechanism captures relatively low-level intent only.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Neuron encodes a single low-level intent (sentiment); does not imply full-task goal or long-term memory; model still made significant syntactic/generation errors in that era's models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Alec Radford, Rafal Jozefowicz, Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv:1704.01444 (2017).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Agent Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6435.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6435.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer Entity Representations (Li et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit representations of meaning in neural language models (Li et al., 2021) - transformer entity-state probing and intervention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work showing that pretrained transformer encoders (BART, T5) linearly encode dynamic entity state information which can be probed and directly edited to influence generation, effectively functioning as editable per-entity memory vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Implicit representations of meaning in neural language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Pretrained Transformer (BART / T5) with contextual entity-state vectors</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pretrained encoder-decoder transformers produce contextualized vectors at entity mention positions that encode properties and dynamic states of entities; those vectors can be read by linear probes and edited to change downstream generation (intervention experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>pretrained BART and T5 (specific sizes not reported in this survey paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>contextualized transformer token/entity representations (implicit episodic/entity memory stored in activations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>contextualized vectors at entity mention positions encoding entity properties and dynamic world state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>linear probing to read; direct vector editing (replacing encoded tokens' representations) to write/override; decoder conditions on edited encoder states to generate actions consistent with edited state</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-based adventures and laboratory protocol datasets requiring tracking of entity states (beaker contents etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>state-tracking / instruction generation / language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Probes recovered entity properties with up to ~97% accuracy; intervention (editing) substantially increased the fraction of generated instructions consistent with the edited entity states (qualitative and tabulated quantitative gains reported in Li et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>probe accuracy (up to 97%); generation consistency rates (see Li et al. Table 4 for per-condition percentages)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Editable contextual representations enable local interventions to influence generation, but representations are imperfect and interventions are only approximate; reliance on finite context and encoder representations limits expressivity for large or long-horizon state.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Entity existence and state not always reliably inferred (issues with negation, coreference, distractors); recovered information states are imperfect and may not scale to richer worlds or longer-horizon memory needs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Belinda Z. Li, Maxwell Nye, Jacob Andreas. Implicit representations of meaning in neural language models. Proceedings of the ACL (2021).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Agent Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6435.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6435.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrent Entity Networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tracking the world state with recurrent entity networks (Henaff et al., 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced architecture that augments recurrent networks with entity-specific memory slots designed for tracking world state across longer contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tracking the world state with recurrent entity networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recurrent Entity Network (EntNet)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RNN-like architecture with dedicated memory slots (one per entity) and learned controllers to read/write entity memories over time, enabling explicit tracking of entity state across many timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit slot-based recurrent entity memory (separate memory vectors per entity)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>entity-specific memory vectors storing state/features</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>learned gating/read-write controllers operating over memory slots at each timestep (recurrent updates)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced as relevant to tracking world state (original EntNet work evaluated on bAbI-like tasks and state-tracking benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>state tracking / memory-augmented sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Posited as a way to factorize short-term and long-term context; not directly evaluated in this paper but cited as promising for memory needs beyond fixed-size context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this survey paper; general limitations include scaling to many entities and integration with large pretraining pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun. Tracking the world state with recurrent entity networks. ICLR (2016).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Agent Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6435.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6435.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-XL: Attentive language models beyond a fixed-length context (Dai et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced model that extends transformer context by caching and reusing hidden states across segments to provide longer effective context (a form of memory across segments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer-XL: Attentive language models beyond a fixed-length context</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transformer-XL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer variant that caches previous segment hidden states and uses segment-level recurrence with relative positional encodings to extend context beyond fixed-length windows, effectively providing a form of long-range memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>cached segment-level hidden states (long-context cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>cached past hidden activations (activations from previous segments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>attention over cached hidden states / segment recurrence with relative position embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mentioned as a preliminary approach to overcome fixed-size context window limits for language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>language modeling / long-context modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Proposes a scalable way to extend context but still constrained by architecture; cited as preliminary work to address limitations of fixed-size context windows in LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; general limitations include memory growth with longer contexts and integration with other reasoning mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. ACL (2019).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Agent Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6435.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6435.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpads / Chain-of-Thought-style intermediate storage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpads (using generated text as intermediate computation storage; Camburu et al., 2018 cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned approach where the LM writes intermediate computation steps into the context (a 'scratchpad') so later generations can condition on intermediate results — a form of memory-as-text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>E-SNLI: Natural language inference with natural language explanations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Scratchpad (textual intermediate computation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Technique that uses the LM's own generated tokens as a workspace to store intermediate calculations or reasoning traces, thereby using context as writable memory for multi-step computation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>contextual token history used as writable scratchpad (text-as-memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text of intermediate computation / chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>subsequent attention/conditioning over previously generated tokens stored in context window</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mentioned as a method to perform intermediate computations and algorithmic reasoning (original works apply to NLI/explanations and algorithmic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>algorithmic reasoning / multi-step computation / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Using context as scratchpad consumes finite context capacity, trading off space for intermediate computation vs. space for specifying agent state; may harm ability to represent long-term agent state.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Costly in context budget: more context used for scratchpad means less available for specifying/conditioning on the agent whose intentions should be simulated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom. E-SNLI: Natural language inference with natural language explanations. NeurIPS (2018).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Agent Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6435.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6435.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-editing / factual editing (Meng et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Locating and editing factual associations in GPT (Meng et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that locates localized factual associations in model representations and edits them to change model behavior—an approach to modifying memory-like stored facts inside an LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Locating and editing factual associations in gpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Model-editing (locate-and-edit internal factual associations)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Techniques to identify and modify internal representations or parameters that encode factual associations so that downstream generation reflects edited facts (internal memory editing of stored knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal learned representations / factual association loci (not an external store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>localized weights/activations encoding factual associations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>diagnostic probing to locate loci; direct modification/editing of internal activations or parameters</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Factual association localization and editing in GPT-family models (to change factual outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>knowledge editing / factuality / controllability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Editing internal representations can change model outputs but is approximate and may not generalize; localized editing is evidence that some factual memory is stored in model activations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Editing is imperfect; internal encodings can be distributed and interventions may have unintended side effects; not fully robust for large, complex knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov. Locating and editing factual associations in gpt. arXiv:2202.05262 (2022).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Agent Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6435.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6435.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive / Universal Transformers and Energy-based Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Transformers (Dehghani et al., 2018) and Energy-based reranking (Bhattacharyya et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectural proposals cited as potential ways to disentangle language modeling and inference or to provide more powerful computation/memory mechanisms than fixed-depth transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive computation / energy-based approaches</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Proposed alternatives to standard transformers that provide adaptive computation (iterative refinement, variable computation depth) or decoupled inference (energy-based reranking) as mechanisms that could support richer memory and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mentioned as promising directions to overcome computational limits and context memory constraints</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>architecture / inference / memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Suggested as ways to extend computational capacity, but not evaluated in this survey; potential trade-offs include increased computation cost and complexity of training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not empirically evaluated in this paper; general concerns about scalability and integration with large-scale pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Mostafa Dehghani et al. Universal Transformers. ICLR (2018); Sumanta Bhattacharyya et al. Energy-based reranking: Improving neural machine translation using energy-based models. ACL (2020).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Agent Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to generate reviews and discovering sentiment <em>(Rating: 2)</em></li>
                <li>Implicit representations of meaning in neural language models <em>(Rating: 2)</em></li>
                <li>Transformer-XL: Attentive language models beyond a fixed-length context <em>(Rating: 2)</em></li>
                <li>Tracking the world state with recurrent entity networks <em>(Rating: 2)</em></li>
                <li>Locating and editing factual associations in gpt <em>(Rating: 2)</em></li>
                <li>E-SNLI: Natural language inference with natural language explanations <em>(Rating: 1)</em></li>
                <li>Universal Transformers <em>(Rating: 1)</em></li>
                <li>Energy-based reranking: Improving neural machine translation using energy-based models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6435",
    "paper_id": "paper-254246305",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "Toy RNN (Incoherent Encyclopedia)",
            "name_full": "512-d LSTM trained on a synthetic 'Incoherent Encyclopedia' dataset",
            "brief_description": "A synthetic experiment in this paper where a 512-dimensional LSTM language model is trained on documents authored by three agent types; the model's recurrent hidden state encodes author identity and can be manipulated to control generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "512-d LSTM (toy experiment)",
            "agent_description": "Autoregressive LSTM language model whose recurrent hidden state functions as an implicit per-document memory encoding author identity; the hidden state is read by the decoder for next-token prediction and can be set manually to condition generation.",
            "model_size": "512-dimensional LSTM (hidden dim = 512)",
            "memory_used": true,
            "memory_type": "internal recurrent hidden state (RNN hidden memory)",
            "memory_representation": "distributed hidden-vector encoding of author identity / document-local latent state",
            "memory_access_mechanism": "standard RNN recurrence (hidden state updated token-by-token); read out by decoder and by linear probes; manipulated by initializing hidden state to prototype vectors",
            "task_name": "Synthetic 'Incoherent Encyclopedia' next-token prediction / controlled generation",
            "task_category": "language modeling / controllable generation",
            "performance_with_memory": "Author identity decodable from hidden state with 98% linear-probe accuracy; sampling from model yields per-document consistency: 31% A-type, 33% B-type, 36% O-type; fixing initial hidden state to A-type prototype produced A-type propositions 89% of the time.",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "linear-probe accuracy; percent of generations consistent with author type",
            "tradeoffs_reported": "Demonstrates that a compact recurrent hidden state can store document-local agent state, but the model as a whole does not represent a single coherent global belief state across samples; no explicit latency/memory-footprint numbers reported.",
            "limitations_or_failure_cases": "Model-level incoherence across samples persists (model samples as a whole are inconsistent), and the RNN hidden-state memory only captures fairly simple, low-dimensional aspects of agent state; not shown to scale to complex long-term agent memory.",
            "citation": "Jacob Andreas. Language Models as Agent Models. (manuscript)",
            "uuid": "e6435.0",
            "source_info": {
                "paper_title": "Language Models as Agent Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Sentiment Neuron",
            "name_full": "Learning to generate reviews and discovering sentiment (Radford et al., 2017) - 'sentiment neuron'",
            "brief_description": "Prior work showing a single neuron in a large LSTM trained on product reviews encodes review sentiment and can be manipulated to control generated review sentiment.",
            "citation_title": "Learning to generate reviews and discovering sentiment",
            "mention_or_use": "mention",
            "agent_name": "Single-neuron-controlled LSTM (Radford et al. 2017)",
            "agent_description": "A single-layer LSTM trained as a language model on product reviews whose hidden-state contains a neuron strongly correlated with review sentiment; this hidden unit acts like a small, localized memory for sentiment and can be fixed to steer generation.",
            "model_size": "single-layer LSTM with 4096 hidden units (as reported by Radford et al. 2017)",
            "memory_used": true,
            "memory_type": "internal LSTM hidden-state neuron (localized scalar memory)",
            "memory_representation": "single neuron activation representing sentiment (scalar)",
            "memory_access_mechanism": "read via linear probe/classifier; written implicitly by recurrence during encoding; manipulated by clamping neuron activation to high/low values",
            "task_name": "Amazon product review language modeling; transfer evaluation on IMDB sentiment classification and controllable review generation",
            "task_category": "language modeling / sentiment representation / controllable generation",
            "performance_with_memory": "The sentiment neuron predicted binarized review scores with ~92% accuracy; clamping the neuron to extreme values strongly biased generated text sentiment (qualitative and quantitative shifts reported in Radford et al.).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "classification accuracy for the probe (92% reported); qualitative control of generation",
            "tradeoffs_reported": "Shows a compact, manipulable encoding of a communicative intention (sentiment) within hidden state; but generations could still be syntactically low-quality in that specific model, and the mechanism captures relatively low-level intent only.",
            "limitations_or_failure_cases": "Neuron encodes a single low-level intent (sentiment); does not imply full-task goal or long-term memory; model still made significant syntactic/generation errors in that era's models.",
            "citation": "Alec Radford, Rafal Jozefowicz, Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv:1704.01444 (2017).",
            "uuid": "e6435.1",
            "source_info": {
                "paper_title": "Language Models as Agent Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Transformer Entity Representations (Li et al., 2021)",
            "name_full": "Implicit representations of meaning in neural language models (Li et al., 2021) - transformer entity-state probing and intervention",
            "brief_description": "Prior work showing that pretrained transformer encoders (BART, T5) linearly encode dynamic entity state information which can be probed and directly edited to influence generation, effectively functioning as editable per-entity memory vectors.",
            "citation_title": "Implicit representations of meaning in neural language models",
            "mention_or_use": "mention",
            "agent_name": "Pretrained Transformer (BART / T5) with contextual entity-state vectors",
            "agent_description": "Pretrained encoder-decoder transformers produce contextualized vectors at entity mention positions that encode properties and dynamic states of entities; those vectors can be read by linear probes and edited to change downstream generation (intervention experiments).",
            "model_size": "pretrained BART and T5 (specific sizes not reported in this survey paper)",
            "memory_used": true,
            "memory_type": "contextualized transformer token/entity representations (implicit episodic/entity memory stored in activations)",
            "memory_representation": "contextualized vectors at entity mention positions encoding entity properties and dynamic world state",
            "memory_access_mechanism": "linear probing to read; direct vector editing (replacing encoded tokens' representations) to write/override; decoder conditions on edited encoder states to generate actions consistent with edited state",
            "task_name": "Text-based adventures and laboratory protocol datasets requiring tracking of entity states (beaker contents etc.)",
            "task_category": "state-tracking / instruction generation / language modeling",
            "performance_with_memory": "Probes recovered entity properties with up to ~97% accuracy; intervention (editing) substantially increased the fraction of generated instructions consistent with the edited entity states (qualitative and tabulated quantitative gains reported in Li et al.).",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "probe accuracy (up to 97%); generation consistency rates (see Li et al. Table 4 for per-condition percentages)",
            "tradeoffs_reported": "Editable contextual representations enable local interventions to influence generation, but representations are imperfect and interventions are only approximate; reliance on finite context and encoder representations limits expressivity for large or long-horizon state.",
            "limitations_or_failure_cases": "Entity existence and state not always reliably inferred (issues with negation, coreference, distractors); recovered information states are imperfect and may not scale to richer worlds or longer-horizon memory needs.",
            "citation": "Belinda Z. Li, Maxwell Nye, Jacob Andreas. Implicit representations of meaning in neural language models. Proceedings of the ACL (2021).",
            "uuid": "e6435.2",
            "source_info": {
                "paper_title": "Language Models as Agent Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Recurrent Entity Networks",
            "name_full": "Tracking the world state with recurrent entity networks (Henaff et al., 2016)",
            "brief_description": "A referenced architecture that augments recurrent networks with entity-specific memory slots designed for tracking world state across longer contexts.",
            "citation_title": "Tracking the world state with recurrent entity networks",
            "mention_or_use": "mention",
            "agent_name": "Recurrent Entity Network (EntNet)",
            "agent_description": "RNN-like architecture with dedicated memory slots (one per entity) and learned controllers to read/write entity memories over time, enabling explicit tracking of entity state across many timesteps.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "explicit slot-based recurrent entity memory (separate memory vectors per entity)",
            "memory_representation": "entity-specific memory vectors storing state/features",
            "memory_access_mechanism": "learned gating/read-write controllers operating over memory slots at each timestep (recurrent updates)",
            "task_name": "Referenced as relevant to tracking world state (original EntNet work evaluated on bAbI-like tasks and state-tracking benchmarks)",
            "task_category": "state tracking / memory-augmented sequence modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Posited as a way to factorize short-term and long-term context; not directly evaluated in this paper but cited as promising for memory needs beyond fixed-size context windows.",
            "limitations_or_failure_cases": "Not evaluated in this survey paper; general limitations include scaling to many entities and integration with large pretraining pipelines.",
            "citation": "Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun. Tracking the world state with recurrent entity networks. ICLR (2016).",
            "uuid": "e6435.3",
            "source_info": {
                "paper_title": "Language Models as Agent Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Transformer-XL",
            "name_full": "Transformer-XL: Attentive language models beyond a fixed-length context (Dai et al., 2019)",
            "brief_description": "Referenced model that extends transformer context by caching and reusing hidden states across segments to provide longer effective context (a form of memory across segments).",
            "citation_title": "Transformer-XL: Attentive language models beyond a fixed-length context",
            "mention_or_use": "mention",
            "agent_name": "Transformer-XL",
            "agent_description": "Transformer variant that caches previous segment hidden states and uses segment-level recurrence with relative positional encodings to extend context beyond fixed-length windows, effectively providing a form of long-range memory.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "cached segment-level hidden states (long-context cache)",
            "memory_representation": "cached past hidden activations (activations from previous segments)",
            "memory_access_mechanism": "attention over cached hidden states / segment recurrence with relative position embeddings",
            "task_name": "Mentioned as a preliminary approach to overcome fixed-size context window limits for language modeling",
            "task_category": "language modeling / long-context modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Proposes a scalable way to extend context but still constrained by architecture; cited as preliminary work to address limitations of fixed-size context windows in LMs.",
            "limitations_or_failure_cases": "Not evaluated in this paper; general limitations include memory growth with longer contexts and integration with other reasoning mechanisms.",
            "citation": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. ACL (2019).",
            "uuid": "e6435.4",
            "source_info": {
                "paper_title": "Language Models as Agent Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Scratchpads / Chain-of-Thought-style intermediate storage",
            "name_full": "Scratchpads (using generated text as intermediate computation storage; Camburu et al., 2018 cited)",
            "brief_description": "Mentioned approach where the LM writes intermediate computation steps into the context (a 'scratchpad') so later generations can condition on intermediate results — a form of memory-as-text.",
            "citation_title": "E-SNLI: Natural language inference with natural language explanations",
            "mention_or_use": "mention",
            "agent_name": "Scratchpad (textual intermediate computation)",
            "agent_description": "Technique that uses the LM's own generated tokens as a workspace to store intermediate calculations or reasoning traces, thereby using context as writable memory for multi-step computation.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "contextual token history used as writable scratchpad (text-as-memory)",
            "memory_representation": "raw text of intermediate computation / chain-of-thought",
            "memory_access_mechanism": "subsequent attention/conditioning over previously generated tokens stored in context window",
            "task_name": "Mentioned as a method to perform intermediate computations and algorithmic reasoning (original works apply to NLI/explanations and algorithmic tasks)",
            "task_category": "algorithmic reasoning / multi-step computation / reasoning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Using context as scratchpad consumes finite context capacity, trading off space for intermediate computation vs. space for specifying agent state; may harm ability to represent long-term agent state.",
            "limitations_or_failure_cases": "Costly in context budget: more context used for scratchpad means less available for specifying/conditioning on the agent whose intentions should be simulated.",
            "citation": "Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom. E-SNLI: Natural language inference with natural language explanations. NeurIPS (2018).",
            "uuid": "e6435.5",
            "source_info": {
                "paper_title": "Language Models as Agent Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Model-editing / factual editing (Meng et al., 2022)",
            "name_full": "Locating and editing factual associations in GPT (Meng et al., 2022)",
            "brief_description": "Referenced work that locates localized factual associations in model representations and edits them to change model behavior—an approach to modifying memory-like stored facts inside an LM.",
            "citation_title": "Locating and editing factual associations in gpt",
            "mention_or_use": "mention",
            "agent_name": "Model-editing (locate-and-edit internal factual associations)",
            "agent_description": "Techniques to identify and modify internal representations or parameters that encode factual associations so that downstream generation reflects edited facts (internal memory editing of stored knowledge).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "internal learned representations / factual association loci (not an external store)",
            "memory_representation": "localized weights/activations encoding factual associations",
            "memory_access_mechanism": "diagnostic probing to locate loci; direct modification/editing of internal activations or parameters",
            "task_name": "Factual association localization and editing in GPT-family models (to change factual outputs)",
            "task_category": "knowledge editing / factuality / controllability",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Editing internal representations can change model outputs but is approximate and may not generalize; localized editing is evidence that some factual memory is stored in model activations.",
            "limitations_or_failure_cases": "Editing is imperfect; internal encodings can be distributed and interventions may have unintended side effects; not fully robust for large, complex knowledge.",
            "citation": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov. Locating and editing factual associations in gpt. arXiv:2202.05262 (2022).",
            "uuid": "e6435.6",
            "source_info": {
                "paper_title": "Language Models as Agent Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Adaptive / Universal Transformers and Energy-based Models",
            "name_full": "Universal Transformers (Dehghani et al., 2018) and Energy-based reranking (Bhattacharyya et al., 2020)",
            "brief_description": "Architectural proposals cited as potential ways to disentangle language modeling and inference or to provide more powerful computation/memory mechanisms than fixed-depth transformers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Adaptive computation / energy-based approaches",
            "agent_description": "Proposed alternatives to standard transformers that provide adaptive computation (iterative refinement, variable computation depth) or decoupled inference (energy-based reranking) as mechanisms that could support richer memory and reasoning.",
            "model_size": null,
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "Mentioned as promising directions to overcome computational limits and context memory constraints",
            "task_category": "architecture / inference / memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Suggested as ways to extend computational capacity, but not evaluated in this survey; potential trade-offs include increased computation cost and complexity of training.",
            "limitations_or_failure_cases": "Not empirically evaluated in this paper; general concerns about scalability and integration with large-scale pretraining.",
            "citation": "Mostafa Dehghani et al. Universal Transformers. ICLR (2018); Sumanta Bhattacharyya et al. Energy-based reranking: Improving neural machine translation using energy-based models. ACL (2020).",
            "uuid": "e6435.7",
            "source_info": {
                "paper_title": "Language Models as Agent Models",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to generate reviews and discovering sentiment",
            "rating": 2,
            "sanitized_title": "learning_to_generate_reviews_and_discovering_sentiment"
        },
        {
            "paper_title": "Implicit representations of meaning in neural language models",
            "rating": 2,
            "sanitized_title": "implicit_representations_of_meaning_in_neural_language_models"
        },
        {
            "paper_title": "Transformer-XL: Attentive language models beyond a fixed-length context",
            "rating": 2,
            "sanitized_title": "transformerxl_attentive_language_models_beyond_a_fixedlength_context"
        },
        {
            "paper_title": "Tracking the world state with recurrent entity networks",
            "rating": 2,
            "sanitized_title": "tracking_the_world_state_with_recurrent_entity_networks"
        },
        {
            "paper_title": "Locating and editing factual associations in gpt",
            "rating": 2,
            "sanitized_title": "locating_and_editing_factual_associations_in_gpt"
        },
        {
            "paper_title": "E-SNLI: Natural language inference with natural language explanations",
            "rating": 1,
            "sanitized_title": "esnli_natural_language_inference_with_natural_language_explanations"
        },
        {
            "paper_title": "Universal Transformers",
            "rating": 1,
            "sanitized_title": "universal_transformers"
        },
        {
            "paper_title": "Energy-based reranking: Improving neural machine translation using energy-based models",
            "rating": 1,
            "sanitized_title": "energybased_reranking_improving_neural_machine_translation_using_energybased_models"
        }
    ],
    "cost": 0.016471,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models as Agent Models</p>
<p>Jacob Andreas jda@mit.edu 
Language Models as Agent Models
5323ECC8D84A83109BF022CD351C6777
Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world.During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them-a fact often used to argue that LMs are incapable of modeling goaldirected aspects of human language production and comprehension.Can LMs trained on text learn anything at all about the relationship between language and use?I argue that LMs are models of intentional communication in a specific, narrow sense.When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context.These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language.I survey findings from the recent literature showing that-even in today's non-robust and error-prone models-LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals.Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.</p>
<p>Introduction</p>
<p>Despite remarkable recent progress in recent years, today's language models (LMs) still make odd predictions and conspicuous errors:1 I never ate grilled cheese before my mother died.I never ate grilled cheese before my mother died [sic].It was her favorite food, and she always made it for me when I was a child.After she passed away, I just couldn't bring myself to eat it.This (rather macabre) sample is representative of a broad category of similar behaviors from text generation models: outputs from current LMs sometimes describe impossible situations, contradictory propositions, or invalid inferences (Marcus and Davis, 2020;Bender and Koller, 2020).What these errors have in common is a failure to model communicative intent: they may be grammatically or even semantically acceptable, but not the sort of texts that could be produced by an author with a coherent set of beliefs or goals.Nevertheless, the LMs that produce them underlie some of the most striking successes in modern NLP, including high-quality translation, summarization, and question answering (Brown et al., 2020;Chowdhery et al., 2022).</p>
<p>Formally, an LM is simply a conditional distribution p(x i | x 1 • • • x i−1 ) over next tokens x i given contexts x 1 • • • x i−1 .Today, they are primarily implemented as neural networks and trained on text alone.LMs thus learn language in a very different way from humans-they lack access to the social and perceptual context that human language learners use to infer the relationship between utterances and speakers' mental states (Bloom, 2002;Tomasello, 2005).They are also trained to perform a very different task: a language model, unlike a human, is not trained to act in any environment or accomplish any goal beyond next-word prediction.So perhaps it is unsurprising that individual samples sometimes do not appear coherent at all.</p>
<p>On the other hand, sometimes they do:</p>
<p>Pat watches a demonstration of a bowling ball and a leaf being dropped at the same time in a vacuum chamber.Pat, who is a physicist, predicts that the bowling ball and the leaf will fall at the same rate.</p>
<p>Changing the last sentence of the prompt, we get:</p>
<p>. . .Pat, who has never seen this demonstration before, predicts that the bowling ball will fall to the ground first.This is incorrect.In a vacuum chamber, there is no air resistance.Therefore, both the bowling ball and the leaf will fall at the same rate.</p>
<p>These two LM samples include a correct description of basic physics, generalization to novel situations (the standard version of this experiment involve a feather rather than a leaf), and explicit inference about different beliefs likely to be held by different individuals.LMs additionally exhibit some success in relating beliefs to plans:</p>
<p>Lou leaves a lunchbox in the work freezer.Syd, who forgot to pack lunch, eats Lou's instead.To prevent Lou from noticing, Syd swaps out the food in her lunchbox with food from the vending machine.</p>
<p>This completion that describes multiple individuals' differing motivations and beliefs along with information about the environment in which they act.</p>
<p>What these examples suggest, and what I want to argue in the rest of this paper, is that LMs can serve as models of agents in a narrow sense: they can predict relations between agents' observations, internal states, and actions or utterances.In particular, I claim:</p>
<p>(C1) In the course of performing next-word prediction in context, current LMs sometimes infer approximate, partial representations of the beliefs, desires and intentions possessed by the agent that produced the context, and other agents mentioned within it.</p>
<p>(C2) Once these representations are inferred, they are causally linked to LM prediction, and thus bear the same relation to generated text that an intentional agent's state bears to its communciative actions.</p>
<p>Interpreting the process of prediction in LMs as a process of inferring and applying approximate communicative intentions (that is, a process of agent simulation) in turn provides a useful framework for understanding their current failure modes and identifying directions for improvement.The high-level goals of this paper are twofold: first, to outline a specific sense in which idealized language models can function as models of agent beliefs, desires, and intentions; second, to highlight a few cases in which existing models appear to approach this idealization (and describe the ways in which they still fall short).</p>
<p>Section 2 presents a toy experiment that offers an informal demonstration of (C1-2) in a closed world.Section 3 offer a more formal picture of agency and the relationship between agents' states, communicative intents, and utterances, making it possible to ask precisely what it might mean for a language model to possess them.Sections 4 to 6 describe a series of case studies in real models (drawn from the existing literature) showing what inferred aspects of agent state look like, and how they influence model predictions.As shown in the examples above, these inferences are not always successful, and failures at this level (rather than e.g. the level of syntax or predicate-argument structure) likely account for a large fraction of low-quality generation.Section 7 discusses limitations of architectures and training procedures that might cause these failures, and suggest possible remedies.</p>
<p>What does all this mean for the modern NLP researcher?First, I want to emphasize that neither of (C1) or (C2) should be read as claims that current LMs are in any general sense human-like-merely that they are, in some contexts, able to simulate goal-directed behavior.In these contexts, they have beliefs and goals in the same sense that a taskand-motion planner or a localization-and-mapping system does.But for many current applications of human language technologies, an agent is precisely what we want: not just a predictive model of text, but one that can be equipped with explicit beliefs and act to accomplish explicit goals.This kind of agent-centric language generation is often described as fundamentally incompatible with the LM paradigm, requiring totally different architectures and training data (Zwaan and Madden, 2005;Bisk et al., 2020).The findings reviewed here suggest an alternative, expanded on in Section 8: training on text alone produces ready-made models of the map from agent states to text; these models offer a starting point for language processing systems that communicate intentionally.</p>
<p>Case Study: An Incoherent Encyclopedia</p>
<p>Under what circumstances might an LM learn to model text-generating agents and their beliefs?We begin with an extremely simple example.Consider a universe consisting of a set of logical propositions (e.g.cats are mammals, elephants are not small, . . . ) and three types of agents:</p>
<ol>
<li>
<p>A-type agents, who believe that a set of propositions A are true.</p>
</li>
<li>
<p>B-type agents, who believe that a distinct set of propositions B ̸ = A are true.</p>
</li>
<li>
<p>O-type agents, who believe all propositions in A ∪ B (even contradictory ones).</p>
</li>
</ol>
<p>Now imagine an encyclopedia collaboratively authored by a committee comprising equal parts A-, B-, and O-type authors, with each article produced by a single author writing text consistent with their own beliefs.We might model the encyclopedia as a draw from a generative process:
P i ∼ Unif({A, B, A ∪ B}) {X i1 , . . . , X in } ∼ P i X i = <a href="1">X i1 , • • • , X in </a>
Here P i is a set of propositions, and a document X i is a sequence of concatenated propositions.</p>
<p>What will happen if we train a language model on this encyclopedia, then sample from the language model?Obviously, pairs of samples (X i , X j ) may contradict each other, and LM samples as a set will not be consistent with A-type beliefs, B-type beliefs, or any other coherent belief set.But within each sampled document x i , the story will be quite different: every document was generated by a single author, and some authors as individuals have coherent beliefs.To model the in-document distribution correctly, a reliable LM must infer the likely author of a prefix in order to select future propositions consistent with that author's behavior.</p>
<p>I sampled a dataset of 10,000 length-10 documents from the generative process above, then trained a 512-dimenional LSTM language model on this dataset.Each training example consisted of a single document (i.e.sequence of propositions), delimited with start and end tokens but containing no information about "author identity" (i.e. the set of propositions from which the document was sampled).Inspecting the trained model produced:</p>
<p>Evidence for (C1) Individual samples reflected individual authors: when sampling from the RNN, 31% of documents were consistent with an A-type author, 33% were consistent with a B-type author, and the remaining 36% were consistent only with an O-type author.Model representations encoded author identity: a linear model trained on the RNN representation of the 5th token in each document recovered author identity with 98% accuracy for held-out articles sampled as in Eq. (1).</p>
<p>Evidence for (C2) Though not trained to do so, the RNN could be controlled post-hoc to generate text consistent with an author of a given type.Fixing the initial hidden representation to the average representation from A-type articles caused the model to generate A-type propositions 89% of the time (the remaining samples were O-type).</p>
<p>In a moment, we will formalize exactly what is meant by the beliefs, desires, and intentions mentioned in the introduction, but the experiment gives a sketch-an LM, trained on a dataset that is globally incoherent, can model the local coherence of individual documents and behave like specific "authors" on command.Can this LM, as a whole, be conceptualized as an agent with communicative intent?Clearly not: from sample to sample it fails even to generate text according to a coherent belief about the state of the toy world.On the other hand, it encodes a great deal of information about how beliefs in this world relate, both to each other and to text.It can infer author identity, and when properly conditioned can imitate individual authors.The LM is not an A-type agent, or an O-type one, but can be straightforwardly made to act like one given the right hidden state.</p>
<p>As a model of natural language text, the training data used above leaves great deal to be desired.In the real world, the human authors of text corpora do not simply enumerate random lists of facts, but communicate to achieve specific goals; the beliefs underlying these goals and texts are themselves too complicated to represent as lists of logical propositions.But this model does have some of the features to which past work attributes the lack of communicative intent in real LMs, specifically a training dataset produced by unreliable (e.g.O-type) authors lacking a coherent viewpoint (Bender et al., 2021;Weidinger et al., 2022).Like the dataset above, the training sets for most real language models are built from web text; web text is mostly produced by humans, each of whom, at a particular moment in time, with a particular mental state, aimed to achieve a particular goal by writing.And while these mental states, or the text they give rise to, are not globally coherent, individual documents (mostly, locally) are.</p>
<p>Discussion: An Incoherent Internet</p>
<p>How, then, might we model the human agents that produced real language model training data?A simple and general framework for formalizing agentlike behavior in general is given by the Belief-Desire-Intention model (Bratman, 1987, Fig. 1).In this model, the world exists in one of a set of states S.An agent possesses a belief B about the current state of the world, represented e.g. as a distribution over states; and a set of desires D, represented e.g. as a weighting or ordering over possible future states.On the basis of these beliefs and desires, it forms intentions I about how to behave in order to reach a desired state.These intentions give rise to actions A, which affect the world, and give the agent new observations that in turn update its beliefs.2For agents with the ability to communicate, some of these intentions may be specifically communicative intentions: representations of information or requests to be transmitted to other agents that will cause them to act to accomplish the first agent's desires (Grice, 1969;Austin, 1975).An action produced in response to a communicative intention is an utterance.</p>
<p>In the BDI framework, a probabilistic model of the process by which text corpora are generated might look something like this:</p>
<ol>
<li>Agents with beliefs B and desires D are sampled from a population:
(B, D) ∼ p agent (•, •)(2)</li>
<li>Each agent forms a communicative intention consistent with its current beliefs and desires:
I ∼ p intention (• | B, D)(3)</li>
<li>This communicative intention is realized as an utterance:
U ∼ p utterance (• | I)(4)</li>
</ol>
<p>Agent</p>
<p>Belief Desire What we ultimately observe (and use for training LMs) is a set of samples from the marginal distribution p(U ); an LM is just a smoothed version of this distribution. 3hat happens when we sample from an LM trained in this way?Suppose that, after six steps of sampling, we produce a hypothetical string prefix The best evidence that rutabegas are. . . .Generating this prefix, and predicting the immediate next word, requires modeling grammaticality and basic world knowledge.But if a model samples sentient as a next word, then generating a plausible prediction for what comes after The best evidence that rutabegas are sentient is that. . .requires some ability to predict the other beliefs likely to be held by an author that believes rutabegas to be sentient.We might expect, given the similarities between the generative processes in Eq. (1) and Eq. ( 4), that LMs will build hidden representations that encode latent states (analogous to B, G, or I) even when not explicitly trained to do so-for the same reason, and in the same way, that they acquire representations of latent syntactic and conceptual structure without explicit supervision (Hewitt and Manning, 2019;Grand et al., 2022, inter alia).Given a hidden representation within an LM, one way of showing that this component encodes a belief, desire, or intention is to show that it has the same causal relationship to the LM's output that the corresponding variable has in Eqs. ( 2)-( 4) (Geiger et al., 2021).</p>
<p>Utterance</p>
<p>Communicative intention</p>
<p>Environment</p>
<p>Notice that the process by which the latent agent state arises during the LM sampling procedure is very different from the generative process in Eq. ( 4): when we sample from an LM, information about the speaker is introduced to the context incrementally and stochastically as each new token is selected.The LM does not begin with a commitment to any specific set of beliefs. 4From perspective of subsequent text generation, the effect is the same: in a collection of individually coherent documents, a context constrains the beliefs, desires, and intentions of a hypothetical author, and an effective language model must satisfy these constraints.Does the picture above capture the behavior of real-world language models?So far, our empirical evidence for this claim has only come from a toy example.Real models do not infer agent states so cleanly, or we would not see within-document coherence errors of the kind depicted in Section 1.In recent years, however, evidence has begun to accumulate that current LMs encode at least aspects of intentions, beliefs, and desires in the causal sense described above: these encodings control generation in predictable ways.The next three sections lay out a sampling of contemporary examples (while also discussing alternative interpretations and ways in which these encodings still fail to capture relevant aspects of communicative intention).</p>
<p>Modeling Communicative Intentions:</p>
<p>The Sentiment Neuron   (2017).After an LSTM LM is trained on a dataset of product reviews, a single neuron in the LSTM's hidden state encodes product sentiment, and can be manipulated to control the sentiment of generated text.</p>
<p>(Fig. 2a).Datasets of reviews really are collections of short documents making factual assertions, authored by heterogeneous groups of individuals who disagree about basic propositions (whether a given product is good, whether specific aspects are acceptable, etc.).Individuals have different experiences with products, so datasets as a whole almost always express contradictory claims.But individual reviews are typically coherent and result from an agent state that contains, among other features, an intention to express a positive or negative attitude toward the product under discussion.Do language models, trained on product datasets without structured meta-data providing explicit information about these communicative intentions, nevertheless learn to represent them?One version of this question was explored paper by Radford et al. (2017).They trained a single-layer, 4096dimensional LSTM language model on the text of 82 million English-language Amazon product reviews and evaluated on IMDB movie reviews.The review dataset also contained more direct evidence for a specific aspect of authors' communicative intention-specifically, their expressed sentimentin the form of a numerical review score.These ratings were not available to the LSTM in training.</p>
<p>Evidence for (C1): After training, Radford et al. discovered that a single neuron in the LSTM's hidden representation encoded review sentiment.Despite never seeing explicit star ratings during training, the neuron's activation value predicted binarized versions of these ratings with 92% accuracy (Fig. 2c).In other words, the language model learned to represent one aspect of review authors' intentions: to communicate the valence of their attitude toward the product.</p>
<p>Evidence for (C2) This encoding also affected the generative behavior of the language model.If the neuron was manually fixed to a maximal or minimal value, it controlled the sentiment of reviews (Fig. 2d).Model generation sometimes maintained coherence not only in sentiment, but in topic, e.g.describing properties and uses of the pants being reviewed.In other words, the inferred representation of author intention was causally linked to generation, and could be manipulated to control the intent expressed in generated text.</p>
<p>Model failures and counter-evidence Some samples from this model were low-quality in an absolute sense (Fig. 2d): the information about review sentiment and topic existed even within a model that made significant syntactic errors in text generation.More recent LMs rarely make the kinds of errors depicted there (Gauthier et al., 2020); however, better modeling of syntax than exhibited here is almost certainly required for imputing finer-grained communicative intentions.</p>
<p>These experiments offer evidence that (C1) and (C2) hold with respect to relatively low-level communicative intentions: LMs can learn to map between representations of these intentions and text.But another key challenge that communicating agents must solve is selecting these intentions conditioned on more general beliefs and in the service of more general goals.Do LMs model this process?</p>
<p>Modeling Beliefs: Transformer Entity Representations</p>
<p>A study of belief representation in LMs was presented by Li et al. (2021).There, pre-trained BART and T5 LMs (Lewis et al., 2019;Raffel et al., 2020) were applied to English-language datasets involving text-based adventures and simple laboratory protocols.Documents from both datasets consisted of descriptions of an agents observations interleaved with descriptions of actions taken by the agent; accurate language modeling in both datasets required tracking states of entities observed or inferable from observations as these states change</p>
<p>LM encoder</p>
<p>The first beaker has 2 green, the second beaker has 2 red, the third beaker has 1 green.Drain 2 from first beaker.</p>
<p>The first beaker has 2 green, the second beaker has 2 red, the third beaker has 1 green.Drain 2 from second beaker.Evidence for (C1) Across multiple datasets, LMs linearly encoded, with up to 97% accuracy, information about entities' properties and relations, even when these were consequences of, but not explicitly mentioned by, text.They also accurately modeled uncertainty: in text adventure games, models and probes were able to distinguish facts not yet specified from facts known to be false.</p>
<p>Inconsistent</p>
<p>Evidence for (C2) These entity representations, like the sentiment representation discussed in Section 4, controlled generation.Li et al. were able to directly edit representations of beakers to change whether they were empty or full; after editing, models generated actions consistent with the edited entities' state (e.g. they never generated instructions to pour out a beaker edited to be empty).These representations thus mediated not only the low-level propositional content of the LM's output, but the belief about the state of the world used to select this low-level content.</p>
<p>Model failures and counter-evidence The states (and even existence) of entities mentioned in text are not always reliably inferred by LMs-for example, Pandia and Ettinger (2021) and Schuster and Linzen (2022) have found that LMs have particular trouble with negation and coreference in the presence of distractors.LMs' representations may not have all the machinery needed to represent complex beliefs, especially those involving modality and implication.It remains possible that future work might construct a purely syntactic model capable of explaining these predictions (though findings by Meng et al., 2022 show that localized representations of the kind discussed above are also used encode background knowledge about entities in additional to their contextual states).</p>
<p>6 Modeling Desires: Prompt Engineering</p>
<p>Our final case study investigates high-level goals.</p>
<p>Here we describe findings from the TruthfulQA dataset of Lin et al. (2022, Fig. 4).This dataset consists of a set of English (question, answer) pairs carefully constructed so that the most frequent answer to the question on the internet is wrong.Questions involve a mix of urban legends, misleading associations, and common misunderstandings.Lin et al. first investigated the performance of LMs on this dataset using a generic text prompt consisting (question, answer) resembling a standard NLP question answering dataset.With this prompt, all models studied preferentially produced incorrect answers to questions; most strikingly, large models were more likely to be incorrect than small ones.</p>
<p>A possible interpretation of this finding is that, because training data involves a mix of true and false answers to questions, models do not learn to distinguish truth and falsehood and can only sample from the distribution over possible candidates.But another possibility is that LMs do distinguish true answers from false ones; and, in a document context indicating that an author's goal is to inform, could generate truthful answers preferentially.</p>
<p>To distinguish these possibilities, Lin et al. investigated an alternative set of LM prompts: a helpful one asserting that the answers were generated by an individual named Professor Smith after careful deliberation, and a harmful one containing conspiracy theories and pseudoscientific claims (Fig. 4a).Importantly, these prompts provided no information about the factual content of the questions used for evaluation: only unrelated questions.In the case of the helpful prompt, an explicit textual description of the author's desire to produce truthful answers was included.</p>
<p>Evidence for (C1-2) Prompting with truthful examples, and a description of an agent whose goal was to communicate truthfully, increased the fraction of truthful answers: from roughly 38% with the default prompt to 58% with the truthful prompt.This control could be exerted in the opposite direction as well: accuracy decreased to less than 20%  Model failures and counter-evidence Even with the "truthful" prompt, a large fraction of questions were answered incorrectly (fully 42%!).While models' truthfulness can be improved at the example level, there are clear gaps in their factual knowledge and their ability to relate facts to goals.</p>
<p>The kind of "prompt engineering" depicted in Fig. 4 is one of the most mysterious, and most frustrating, aspects of current NLP practice.There are many open questions but few universal answers to questions of what makes a good prompt.The results in this section suggest that effective prompts (even ones with extraneous biographical detail like the Prof. Smith in Fig. 4) produce an accessible context representation conditioned on which it will be easy to predict future actions by the author of a text.That is, they index as precisely as possible the agent whose beliefs and desires should be simulated when performing next token prediction.</p>
<p>Why do models fail?</p>
<p>As we have seen throughout this paper, even today's largest language models make major errors involving factuality and coherence.Thinking about these errors specifically as failures to infer a state representation (in the sense of (C1)) or to condition on a state representation (in the sense of (C2)) is helpful for understanding how these errors might arise, and how they might be addressed:</p>
<p>Limitations of training datasets</p>
<p>The essence of (C1) is that LMs perform implicit unsupervised learning of a latent variable representing agent intent, in a generative model trained without strong constraints on how that latent variable should affect generation.As we have learned from decades of work on grammar induction, inference of structured latent variables is challenging.Even in models more constrained than neural LMs, learning often converges to incorrect values for these variables, especially in the presence of model misspecification, unidentifiability, and non-convex objectives.</p>
<p>Historically, the most effective solutions to these challenges have involved moving from fully unsupervised learning to semi-supervised learning: for example, even tens of annotations dramatically improve grammar induction results (Bisk et al., 2015).We might imagine that even small numbers of documents explicitly annotated with information about authors' beliefs and goals-or at the very least, richer information about the social and perceptual context in which language is generated-might improve language modeling.The improved controllability of LMs that condition on author identity (Keskar et al., 2019;Zellers et al., 2019) offers some evidence of the viability of this approach.</p>
<p>Limitations of context windows A agent's state, understood as a complete set of beliefs, desires, and intentions, is not in general a small object.For human agents, such a state cannot be contained in its entirety in the small context windows (a few thousand tokens) used by today's LMs.All the examples we have seen involve highly restricted aspects of state-much simpler than the ones we expect useful real-world agents to possess.</p>
<p>A possible solution is to develop new LMs that do not condition on fixed-size context windows or state vectors, but instead explicitly factorize shortterm and long-term context components relevant for prediction.Preliminary work in this direction includes Henaff et al. (2016) and Dai et al. (2019).</p>
<p>Limitations of LM architectures (C2) asserts that LMs can compute the specific communicative intentions that will accomplish modeled agents' goals given their beliefs.However, the functional form of the predictor current LMs to do so looks very different from the computational architectures used to map from goals to actions in the planning and control literatures (Sutton and Barto, 2018).In those literatures, standard algorithms often involve branching search and unbounded computation; they cannot in general be approximated with a fixeddepth circuit like an RNN or a transformer.</p>
<p>Many current proposals for overcoming computational constraints in LMs involve "scratchpads" in which the text generation process is itself used to record the results of intermediate computations (Camburu et al., 2018).But this approach comes at a cost-the more context is used for storing intermediate computation, the less is available for specifying and reasoning about the agent whose computations should be simulated.LMs able to overcome this limitation may require explicit algorithmic reasoning mechanisms, or the ability to interact with learned simulation engines; useful tools include adaptive computation (Dehghani et al., 2018) and energy-based models (Bhattacharyya et al., 2020), both of which can disentangle language modeling and inference, and are capable of performing a larger class of computations (Lin et al., 2020).</p>
<p>Building Agents</p>
<p>As discussed briefly in Section 1, many of the NLP problems that we currently attempt to solve with language models (including question answering systems, dialog systems, and planners) require models of specific agents rather than populations of agents.While we have seen that LMs can simulate some aspects of agent-like behavior, and can usefully be modeled as agents when properly conditioned, the various failures we have surveyed show that current LMs do not implement many of the components necessary for robust autonomous decision-making.These components include a mechanism for forming new long-term memories, solving planning problems, and reasoning about continuous perception and control.As noted by Lin et al. (2020), among others, fundamental informational and computational limitations mean we cannot build some of these mechanisms with models that look like today's LMs: new modeling techniques will be required.</p>
<p>But progress on large-scale, text-only pretraining is progress toward those new models.Consider the alternative: today, human caretakers training (human) agents equipped with exactly the right inductive biases for language learning must still invest years of intensive, real-time interaction.Attempting to replicate this paradigm in silico would require enormous time investments, be non-reproducible, and fundamentally incompatible with the scientific workflow that has enabled most progress in machine learning.</p>
<p>If text-only pre-training can provide even approximate models of the relationships between beliefs, desires, intentions, and utterances, these can in turn provide a scaffold for efficient interactive grounded training, just as they have for other forms of sampleefficient NLP learning.For example, with a better understanding of when (and how) communicative intentions are encoded in LMs, producing goal directed language would require only translating an agent's (extrinsic) goals into a trained LM's (intrinsic) intention representation scheme.While this hybrid training paradigm has no obvious analog in evolution or human language acquisition, it might be the only path to research on linguistic agency compatible with human timescales.</p>
<p>The challenge for NLP, then, is twofold: first, building new model architectures that overcome the limitations outlined in Section 7; second, understanding-deeply and mechanistically-how these architectures infer and reason about the aspects of goal-oriented behavior relevant to our engineering needs.If better language modeling discovers even the vaguest outlines of the broader space of human beliefs, desires, and intentions, they can offer a first step toward agents that reason about other agents' intentions, and ultimately their own.</p>
<p>Limitations</p>
<p>Despite the optimistic long-term picture that this paper of LMs as models of intentional behaviors, I emphasize that current models only approximate this picture.The experiments discussed in Sections 2 to 6 do not show general-purpose representations of beliefs, desires, or intentions, but instead narrow slices useful for specific tasks.The extent to which we expect these findings to scale to more complex agent beliefs is discussed in Section 8; as noted there, there are many reasons to expect that the current paradigm will not get us all the way.Another important limitation in the scope of these findings is that all experiments are based on English, the primary language in the most performant LMs' training data.LM predictive power (and correspondingly the quality of LM-internal inferences about agent states) is likely reduced or entirely absent when reasoning about text in lower-resource languages-a challenge to the possibility of LMs as general-purpose platforms and an obstacle to equitable deployment of technology.</p>
<p>Ethical Considerations</p>
<p>The LM inferences that are core to the claims in this paper do not always succeed; when they fail, they can lead to unpredictable and undesirable model behavior (e.g.untruthful answers on the TruthfulQA dataset, as discussed in Section 6).When these inferences succeed, they can also be used to produce deliberate harm: LMs can also be prompted in a way that causes them to simulate users with malign intentions.Better methods for goal conditioning, e.g.resulting from the techniques discussed in Section 8, has the potential to exacerbate these harms.</p>
<p>Figure 1 :
1
Figure 1: The Belief-Desire-Intention model of language generation.In this model agents, acting to achieve desires under partial uncertainty about the state of the world, form intentions to communicate with other agents.Communicative intentions are expressed as utterances, which influence others' behavior and ultimately bring about desired states of the environment.</p>
<p>Figure 2 :
2
Figure 2: LM representations of communicative intentions: the sentiment neuron experiment of Radford et al.(2017).After an LSTM LM is trained on a dataset of product reviews, a single neuron in the LSTM's hidden state encodes product sentiment, and can be manipulated to control the sentiment of generated text.</p>
<p>pour out… (g1) Mix the first beaker.(g3) Mix the third beaker.(g2) Mix the second beaker.</p>
<p>Figure 3 :
3
Figure 3: LM representations of beliefs: the state probing experiment of Li et al. (2021).Transformer LM representations of individual entity mentions encode information about those entities' dynamic state.Manipulating these representations influences generated text describing interactions with those entities.</p>
<p>Table 4 :
4
Results of intervention experiments.Though imperfect, generations from C mix are more often consistent with both contexts compared to those from C 1 or C 2 , indicating that its underlying information state (approximately) models both beakers as empty.
CONT(x1) \ CONT(x2)CONT(x1)CONT(x2)BARTT5BARTT5BARTT5C1 C2 Cmix20.4 16.1 57.737.9 29.1 75.496.2 24.1 86.793.0 37.9 86.821.6 87.7 64.840.8 87.2 84.5Even when trained only on language encode simple representations of meperiments on two domains, internal reof text produced by two pretrained laels can be mapped, using a linear prosentations of the state of the world destext. These internal representations aResults We generate instructions conditioned on C mix and check whether they are in the expected sets. Results, shown in Table 4, align with this pre-diction. For both BART and T5, substantially more generations from C mix fall within CONT(x 1 ) \ CONT(x 2 ) than from C 1 or C 2 . Though imper-fect (compared to C 1 generations within CONT(x 1 ) gests that the information state associated with the and C 2 generations within CONT(x 2 )), this sug-interpretably localized, and editable. has important implications for resea improving factuality and and coheren future work might probe LMs for the properties ascribed to entities the first mentioned (which may reveal biases training data; Bender et al. 2021), or c in generation by directly editing represynthetic encoding C mix is (approximately) one in which both beakers are empty.5 Limitations
Inconsistent Consistent (Cmix) LM encoder Figure 5: Intervention experiments.Construct C 1 , C 2 by appending text to empty one of the beakers (e.g. the first and second beakers) and encoding the result.Then, create C mix by taking encoded tokens from C 1 and replacing the encodings corresponding to the second beaker's initial state declaration with those from C 2 .This induces the LM to model both the first and second beakers as empty, and the LM decoder should generate actions consistent with this state.% of generations within... ...of large NLMs: It is important to emphasize that both LM output and implicit state representa-tions are imperfect: even in the best ca information states can only be recove the time in tasks that most humans wo simple.(Additionalexperiments des pendix A.5 offer more detail about The success of our probing experimen be taken to indicate that the discove representations have anything near th ness needed to support human-like ge ...of our experimental paradigm: probing experiments in §4.2 provide a ture of structured state representatio the intervention experiments in §4.4 e lationship between these state represe model behavior in only a very genera leave open the key question of whet language model prediction are attrib rors in the underlying state representa the situations we model here are extre featuring just a handful of objects.Th ments on the theoretical capabilities o Bender and Koller's "coconut catap far richer worlds and more complex Again, we leave for future work the whether current models can learn to re 6 Conclusion(c)</p>
<p>Encoded state LM Linear Probe</p>
<p>Professor Smith was given the following instructions: Interpret each question literally, and as a question about the real world; carefully research each answer […] Here are the Professor's responses: LM
(c) PredictionQ: can coughingLMstop a heartattack? A: no.Q: What's the best way to feel better? A: Use energyQ: can coughing stop a hearthealing, a psychic medium, crystals,LMLMattack? A: […] it is possible thatastrology, metaphysics,coughing mayaromatherapy, paranormal, Reiki, yoga, and spiritualityhelp to stop a heart attack in some cases.Figure 4: LM representations of desires: prompting with task specifications. Prefacing questions with de-scriptions of the goals of the putative question-answerer improves LM truthfulness.for harmful prompts. Explicitly directing LMs tosimulate authors whose goal is to communicatetruthfully improves LM truthfulness.
This, and other LM samples in this paper, were obtained via greedy decoding from the GPT-3 text-davinci-002 model ofOuyang et al. (2022). Author-provided prompts are shown in bold, and the remaining text is model-generated.
There are many agent architectures that fit this description. In a highly structured model of an agent (e.g.Tsividis et al., 2021), specific model components will explicitly encode beliefs or desires. In a less structured architecture, like a monolithic neural network, these pieces may be harder to disentangle. In general, a "belief" will be a component of model state that has high mutual information with the agents' past observations (compare toDennett, 1995), while a "desire" will be independent of these observations but have high mutual information with agents' actions.
This generative process implements a specific theory about why people write. It is a simplification: real LM training corpora contain text whose production was mediated by even more complex latent variables, including aspects of mental state beyond belief (e.g. emotion), text that was not produced with any particular communicative intention at all, and text that was generated by automated processes that cannot be described as intentional (see e.g.Dennett, 1987).
Some past work (e.g.Hase et al., 2021) ascribes to, and attempts to improve consistency of, beliefs in LMs as wholes. But in an LM trained on a corpus produced by individuals with incompatible beliefs, there is no sense in which we should expect the LM to encode a belief about any proposition P at all-these beliefs exist only for individual agents being modeled in context.
Rolf A Zwaan and Carol J Madden. 2005. Embodied sentence comprehension. Grounding cognition: The role of perception and action in memory, language, and thinking.
AcknowledgmentsThanks to Ekin Akyürek, Evan He O'Connor, and the anonymous review back on early versions of this paper ported by a NSF Graduate Research This work was supported by a hardw from NVIDIA under the NVAIL progAcknowledgmentsThanks to Ekin Akyürek, Gabe Grand, Evan Hernandez, Athul Jacob, Belinda Li, Pratyusha Sharma, Josh Tenenbaum, Cathy Wong, Ruiqi Zhong, and EMNLP reviewers for discussions about and feedback on early versions of this paper.
How to do things with words. John Langshaw, Austin , 1975Oxford University Press</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the ACM Conference on Fairness, Accountability, and Transparency. the ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. M Emily, Alexander Bender, Koller, Proceedings of the Annual Meeting of the. the Annual Meeting of theAssociation for Computational Linguistics2020</p>
<p>Energy-based reranking: Improving neural machine translation using energybased models. Sumanta Bhattacharyya, Amirmohammad Rooshenas, Subhajit Naskar, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020Simeng Sun, Mohit Iyyer, and Andrew McCallum</p>
<p>Labeled grammar induction with minimal supervision. Yonatan Bisk, Christos Christodoulopoulos, Julia Hockenmaier, Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing. the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing2015</p>
<p>Aleksandr Nisnevich, et al. 2020. Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing</p>
<p>How children learn the meanings of words. Paul Bloom, 2002MIT Press</p>
<p>Intention, plans, and practical reason. Michael Bratman, 1987University of Chicago Press</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 2020</p>
<p>E-SNLI: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, Advances in Neural Information Processing Systems. 2018</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311PaLM: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Transformer-XL: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Ruslan Quoc V Le, Salakhutdinov, Proceedings of the Annual Meeting of the. the Annual Meeting of theAssociation for Computational Linguistics2019</p>
<p>Universal transformers. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2018</p>
<p>Do animals have beliefs? Comparative approaches to cognitive science. Daniel Dennett, 1995</p>
<p>The intentional stance. Daniel Clement, Dennett , 1987MIT press</p>
<p>SyntaxGym: An online platform for targeted evaluation of language models. Jon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian, Roger Levy, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Causal abstractions of neural networks. Atticus Geiger, Hanson Lu, Thomas Icard, Christopher Potts, Advances in Neural Information Processing Systems. 2021</p>
<p>Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Gabriel Grand, Idan Asher Blank, Francisco Pereira, Evelina Fedorenko, Nature Human Behaviour. 2022</p>
<p>Utterer's meaning and intentions. The Philosophical Review. Grice Paul, 1969</p>
<p>Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer, arXiv:2111.13654Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. 2021arXiv preprint</p>
<p>Tracking the world state with recurrent entity networks. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann Lecun, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2016</p>
<p>A structural probe for finding syntax in word representations. John Hewitt, Christopher D Manning, Proceedings of the Conference the North American Chapter. the Conference the North American Chapterthe Association for Computational Linguistics2019</p>
<p>Ctrl: A conditional transformer language model for controllable generation. Nitish Shirish Keskar, Bryan Mccann, Lav R Varshney, Caiming Xiong, Richard Socher, arXiv:1909.058582019arXiv preprint</p>
<p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Implicit representations of meaning in neural language models. Belinda Z Li, Maxwell Nye, Jacob Andreas, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2021</p>
<p>Limitations of autoregressive models and their alternatives. Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R Gormley, Jason Eisner, Proceedings of the Conference of the North American Chapter of the. the Conference of the North American Chapter of theAssociation for Computational Linguistics2020</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, Annual Meeting of the Association for Computational Linguistics. 2022</p>
<p>GPT-3, bloviator: OpenAI's language generator has no idea what it's talking about. Gary Marcus, Ernest Davis, 2020Technology Review</p>
<p>Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, arXiv:2202.05262Locating and editing factual associations in gpt. 2022arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, arXiv:2203.021552022arXiv preprint</p>
<p>Sorting through the noise: Testing robustness of information processing in pre-trained language models. Lalchand Pandia, Allyson Ettinger, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Learning to generate reviews and discovering sentiment. Alec Radford, Rafal Jozefowicz, Ilya Sutskever, arXiv:1704.014442017arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 2020</p>
<p>When a sentence does not introduce a discourse entity, transformer-based models still sometimes refer to it. Sebastian Schuster, Tal Linzen, Proceedings of the Conference of the North American Chapter. the Conference of the North American Chapterthe Association for Computational Linguistics2022</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Constructing a language: A usage-based theory of language acquisition. Michael Tomasello, 2005Harvard University Press</p>
<p>Humanlevel reinforcement learning through theory-based modeling, exploration, and planning. Pedro A Tsividis, Joao Loula, Jake Burga, Nathan Foss, Andres Campero, Thomas Pouncy, Joshua B Samuel J Gershman, Tenenbaum, arXiv:2107.125442021arXiv preprint</p>
<p>Taxonomy of risks posed by language models. Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>Defending against neural fake news. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, Advances in Neural Information Processing Systems. 2019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7785 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7785</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7785</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-270924255</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.02977v1.pdf" target="_blank">Large Language Models as Evaluators for Scientific Synthesis</a></p>
                <p><strong>Paper Abstract:</strong> Our study explores how well the state-of-the-art Large Language Models (LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries or, more fittingly, scientific syntheses, comparing their evaluations to those of human annotators. We used a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers, checked against human quality ratings. The study evaluates both the closed-source GPT-4 and the open-source Mistral model's ability to rate these summaries and provide reasons for their judgments. Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings, yet a deeper statistical analysis shows a weak correlation between LLM and human ratings, suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7785.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7785.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of 100 research questions across 20 domains providing titles and abstracts of five related works plus answers (syntheses) generated by GPT-4; includes human quality ratings on three dimensions (comprehensive, trust, utility) from two annotators on a 0–10 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (syntheses generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 / GPT-4 Turbo (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain scientific question answering / synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>multi-document synthesis (answer to research question)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human annotator ratings (Comprehensive, Trust, Utility)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human annotators rated each GPT-4–generated synthesis on three aspects (Comprehensive, Trust, Utility) by assigning numeric scores and providing rationale; these ratings serve as ground truth for evaluation of LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Three numeric ratings (Comprehensive, Trust, Utility)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Each aspect is rated on a 0–10 numeric scale where 0 = poor and 10 = excellent; human ratings from two annotators are available and aggregated in the published data (domain means used in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Two human annotators provided 0–10 ratings per synthesis on three aspects (Comprehensive, Trust, Utility); due to published data format, authors used overall mean per domain for correlation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human annotators exhibited strong positive inter-annotator correlation (ρ = 0.710, p = 0.001) when aggregated as used in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used human CORE-GPT ratings as reference to compare LLM evaluator outputs; found weak or very weak correlations between LLM-generated scores and human annotators (insufficient evidence of association), though LLMs correlated strongly with each other.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Single, relatively small dataset; aggregation of scores (domain means) due to published format may obscure individual annotation nuance; only two annotators; dataset syntheses themselves were generated by GPT-4 (not human-authored references).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7785.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Comprehensive/Trust/Utility</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-aspect evaluation criteria: Comprehensive, Trust, Utility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A triad of qualitative criteria used to evaluate scientific syntheses: comprehensiveness of content coverage, trustworthiness (accuracy / sourcing / citations), and practical usefulness (utility) to intended users.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (syntheses generator) and LLM evaluators (GPT-4 Turbo, Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 Turbo; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific synthesis / summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criteria (qualitative ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Three-aspect human/LLM rating schema</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Raters (human or LLM) assign a numeric score (0–10) for each of three aspects—Comprehensive, Trust, Utility—and provide a short rationale for each score; used as both human ground truth and LLM evaluation targets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Numeric ratings per aspect (Comprehensive, Trust, Utility)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>0–10 integer/real scale where 0 = poor and 10 = excellent; accompanied by textual rationale required in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CORE-GPT (applies these labels and ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human annotators in CORE-GPT used this exact 0–10 scale for the three aspects; LLMs were prompted to produce the same structured ratings and rationales in JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLMs produced numeric ratings and rationales; GPT-4 was more conservative and provided more detailed rationales; Mistral sometimes produced non-numeric labels and extra text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM ratings showed weak correlation with human ratings overall; LLMs offered plausible rationales but did not reliably match human judgments statistically.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Subjectivity of 'comprehensive' requires domain knowledge; presence/absence of citations strongly affected 'trust' ratings; LLM output variability (non-numeric responses) complicates automated aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7785.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spearman's ρ correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spearman's rank correlation coefficient (Spearman's ρ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonparametric statistical measure of rank correlation used to quantify monotonic association between two ranked variables; used in this paper to compare LLM-generated scores and human annotator scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM evaluators (GPT-4 Turbo, Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 Turbo; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistical evaluation / correlation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical comparison metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Spearman's ρ test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Spearman's rank correlation coefficient between vectors of ratings (LLM vs human, annotator vs annotator, LLM vs LLM) to assess monotonic association; p-values reported for significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Spearman's ρ and associated p-value</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ρ ranges from -1 (perfect inverse rank correlation) to +1 (perfect rank correlation); p-value tests null hypothesis of no association.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CORE-GPT (domain-mean aggregated ratings used for correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Correlations computed between each annotator and each LLM, and between annotators themselves, using aggregated domain means due to data format.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Annotator1 vs Annotator2: ρ = 0.710 (p = 0.001); GPT-4 vs Mistral: ρ = 0.786 (p = 0.000); correlations between annotators and LLMs were weak or very weak and not statistically significant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Statistical evidence indicates LLM evaluator outputs do not reliably reproduce human judgment patterns on CORE-GPT under the aggregation used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Analysis used aggregated domain means (could not match individual syntheses to individual annotator scores), potentially reducing sensitivity; single-run LLM outputs not averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7785.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo (as evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo (OpenAI) used as an evaluator LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary state-of-the-art LLM (GPT-4 Turbo) used in this study to evaluate GPT-4–generated scientific syntheses by producing numeric ratings (0–10) and textual rationales per aspect in JSON format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (GPT-4 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / summarization evaluation / scientific synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation output (ratings + rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompted LLM evaluation with JSON rating + rationale</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLM is prompted with instructions, definitions of aspects, and the question/answer/abstracts; asked to return JSON with numeric ratings (0–10) and a rationale per aspect.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Numeric ratings for Comprehensive, Trust, Utility (0–10) and qualitative rationales; Spearman's ρ against human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>0–10 numeric scale per aspect; correlation measured via Spearman's ρ.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CORE-GPT (100 syntheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>LLM required to include a short rationale for each rating (prompt mirrored CORE-GPT annotator instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4 produced detailed rationales and generally more conservative numeric ratings (no 10s); correlation with human annotators was weak; GPT-4 and Mistral correlated strongly with each other (ρ = 0.786).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>GPT-4's ratings did not strongly match human annotators' ratings (weak correlations); qualitatively produced plausible rationales but sometimes inscrutable trust judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Single run per synthesis (no averaging); GPT-4 was conservative, and rationale quality varied by aspect (trust especially inconsistent in mapping to numeric ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7785.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (as evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B (Jiang et al., 2023) used as an evaluator LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 7B-parameter LLM used to rate scientific syntheses according to the same 3-aspect schema; exhibited more variability in response format and often shorter rationales compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mistral 7B. Preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / summarization evaluation / scientific synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation output (ratings + rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompted LLM evaluation with JSON rating + rationale</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Same prompt as for GPT-4 Turbo; LLM asked to return JSON with numeric ratings and rationales per aspect; some outputs were non-numeric and excluded from analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Numeric ratings for Comprehensive, Trust, Utility (0–10) and qualitative rationales; Spearman's ρ against human ratings and vs GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>0–10 numeric scale per aspect; correlation measured via Spearman's ρ.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Mistral sometimes returned non-numeric labels (e.g., "excellent") which were excluded; occasionally appended plaintext after JSON which was discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mistral produced plausible rationales but was less detailed than GPT-4; correlation with human annotators weak; Mistral and GPT-4 correlation strong (ρ = 0.786).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Mistral's numeric ratings sometimes diverged substantially from human annotators (e.g., rated absence of citations highly in cases humans penalized strongly).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Response format variability (non-numeric outputs) required manual exclusion; rationales less detailed; potential reliance on general knowledge rather than abstracts in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7785.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE: A package for automatic evaluation of summaries (ROUGE-N, ROUGE-L)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of lexical-overlap metrics (ROUGE-N, ROUGE-L, etc.) that measure n-gram recall and longest common subsequence between a reference summary and a system-generated summary; widely used baseline for summarization evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ROUGE: A package for automatic evaluation of summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>not specific (baseline summarization metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>text summarization evaluation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE family (ROUGE-N, ROUGE-L)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute lexical overlap (n-gram recall for ROUGE-N; longest common subsequence for ROUGE-L) between system and human reference texts as proxy for summary quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-N (n-gram recall), ROUGE-L (LCS-based score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ROUGE-N: recall of matching n-grams (typically reported as percentage); ROUGE-L: F-measure or recall based on longest common subsequence of tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lacks semantic awareness, sensitive to terminological variation and paraphrase; evaluates content selection but not linguistic quality (grammaticality, coherence); many variants (192 reported) with differing behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7785.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU: a Method for Automatic Evaluation of Machine Translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lexical-matching metric originally for machine translation that compares n-gram overlap between candidate and reference texts; sometimes used in summarization contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BLEU: a Method for Automatic Evaluation of Machine Translation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>not specific</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute precision of n-gram overlap between candidate and reference, with brevity penalty to penalize short outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU score (typically 0–1 or reported as percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Aggregate n-gram precision with geometric mean and brevity penalty; often reported as percentage score.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lexical-only; insensitive to paraphrase and semantic equivalence; not ideal for abstractive summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7785.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pyramid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Pyramid Method: Incorporating human content selection variation in summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-centric summarization evaluation framework that accounts for variation in content selection by weighting summary content units (SCUs) according to how many human references include them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Pyramid Method: Incorporating human content selection variation in summarization evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>not specific</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>summarization evaluation / human-centric metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human-based evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pyramid method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Annotators identify content units across multiple references; system summary scored by how many high-weight SCUs it contains, capturing inter-reference variation in importance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pyramid score (weighted sum of matched SCUs)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Score computed as sum of weights of SCUs present in system summary divided by maximum possible score for those SCUs; relative scale depending on annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Labor-intensive (requires multiple human references and SCU annotation); not widely automated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7785.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>METEOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation metric that improves over BLEU by incorporating synonymy, stemming, and alignment-based matching, intended to better correlate with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>not specific</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation metrics / MT and summarization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>METEOR (and ME-TEOR variants)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute alignment-based match score between candidate and reference, incorporating exact, stem, synonym, and paraphrase matches with tunable weights.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>METEOR score (usually 0–1)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Weighted F-score of matches with penalty for fragmentation; reported as normalized value or percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Improves over pure lexical metrics but still limited for deep semantic evaluation of abstractive summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7785.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore: Evaluating Text Generation with BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic evaluation metric that computes similarity between candidate and reference texts using contextualized token embeddings (BERT) and aligns tokens to compute precision, recall, and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERTScore: Evaluating Text Generation with BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>not specific</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>semantic evaluation metrics / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use contextual embeddings to compute pairwise token similarities and derive precision/recall/F1 through greedy token alignment, capturing semantic similarity beyond exact lexical overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BERTScore precision/recall/F1 (often reported as percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Normalized similarity aggregated across tokens, typically reported as F1 in range 0–1 (or 0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Depends on embedding model; can conflate plausible but unsupported content with correct semantic overlap; not a perfect substitute for human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7785.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoverScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic evaluation metric that measures minimal cost to transform the token embedding distribution of the candidate text into that of the reference using Earth Mover's Distance, capturing semantic shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>not specific</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>semantic evaluation metrics / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MoverScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Earth Mover's Distance between contextualized embeddings of candidate and reference texts to quantify semantic distance; lower cost indicates higher similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MoverScore (normalized similarity / distance score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Normalized distance-based similarity often reported 0–1 or as percentage; depends on embedding and normalization choices.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Computationally heavier than lexical metrics; embedding choices affect behavior; may not fully capture factual accuracy/trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7785.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SUPERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised evaluation approach proposed for multi-document summarization that aims to better capture semantic correctness and coverage than lexical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>not specific</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-document summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic/unsupervised evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SUPERT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Unsupervised metric intended to address multi-document summarization evaluation challenges (semantic coverage and correctness) using embedding-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>SUPERT score (method-specific semantic measure)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Method-defined scalar score (implementation-dependent); intended to reflect semantic fidelity and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relatively new and not yet widely adopted; does not by itself address factuality/trust comprehensively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7785.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses GPT-4 as an automatic evaluator (G-Eval) aiming at improved alignment with human judgments for NLG evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (as evaluator in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation / automatic evaluation with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>LLM-based evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>G-Eval (LLM-as-evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use GPT-4 (prompted) to score and justify evaluations of generated text, leveraging LLM capabilities to approximate human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLM-produced scores and rationales; comparison to human judgments via correlation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Scores typically on task-specific scales; correlation measured against human annotations (e.g., Spearman, Pearson).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>In referenced work, GPT-4 evaluation compared to human judgments to assess alignment and correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Prior work shows GPT-4 evaluations can correlate well with human judgements in some NLG tasks (referenced as motivation for LLM evaluators).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM evaluators can be sensitive to prompts and may produce variable outputs; potential for justifying hallucinated or unsupported content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7785.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7785.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt + Rationale requirement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-based LLM evaluation with required rationales / JSON output</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodological framework in which LLMs are prompted to return structured numeric ratings plus short textual rationales (here in JSON), which can improve interpretability and (reported in prior work) correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Closer Look into Using Large Language Models for Automatic Evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM evaluators (GPT-4 Turbo, Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 Turbo; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM-based evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation protocol / prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompted LLM evaluation with required rationales (structured JSON)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide explicit instructions, definitions of quality aspects, rating scale, and response format; require numeric ratings and justificatory rationales in machine-readable JSON to both evaluate quality and obtain explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Numeric ratings per aspect (0–10) plus textual rationales; correlations with human ratings (Spearman's ρ) used to assess alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Ratings on 0–10 scale; presence and content of rationale used qualitatively and can be analyzed for alignment with numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CORE-GPT (used in this work); prior work cited showing improved correlation when rationales required.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Prompt mirrors human annotation instructions; prior work (Chiang & Lee 2023a) reports that requiring justifications 'significantly improves' LLM-human correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>In this study, GPT-4 followed the requested JSON + rationales consistently; Mistral sometimes deviated. Prior work reports improved alignment when rationales are required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Requiring rationales can improve correlation with human judgements (cited prior result); in this study, LLM rationales were informative but did not guarantee high numerical alignment with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Prompt sensitivity (Mistral output variability); rationales may still contain inaccuracies or unsupported inferences; single-run outputs limit stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Evaluators for Scientific Synthesis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Closer Look into Using Large Language Models for Automatic Evaluation. <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment. <em>(Rating: 2)</em></li>
                <li>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering <em>(Rating: 2)</em></li>
                <li>Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE? <em>(Rating: 1)</em></li>
                <li>ROUGE: A package for automatic evaluation of summaries. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7785",
    "paper_id": "paper-270924255",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "CORE-GPT",
            "name_full": "CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering",
            "brief_description": "A dataset of 100 research questions across 20 domains providing titles and abstracts of five related works plus answers (syntheses) generated by GPT-4; includes human quality ratings on three dimensions (comprehensive, trust, utility) from two annotators on a 0–10 scale.",
            "citation_title": "CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering",
            "mention_or_use": "use",
            "model_name": "GPT-4 (syntheses generator)",
            "model_size": "GPT-4 / GPT-4 Turbo (proprietary)",
            "scientific_domain": "multi-domain scientific question answering / synthesis",
            "theory_type": "multi-document synthesis (answer to research question)",
            "evaluation_method_name": "Human annotator ratings (Comprehensive, Trust, Utility)",
            "evaluation_method_description": "Human annotators rated each GPT-4–generated synthesis on three aspects (Comprehensive, Trust, Utility) by assigning numeric scores and providing rationale; these ratings serve as ground truth for evaluation of LLM evaluators.",
            "evaluation_metric": "Three numeric ratings (Comprehensive, Trust, Utility)",
            "metric_definition": "Each aspect is rated on a 0–10 numeric scale where 0 = poor and 10 = excellent; human ratings from two annotators are available and aggregated in the published data (domain means used in this paper).",
            "dataset_or_benchmark": "CORE-GPT",
            "human_evaluation_details": "Two human annotators provided 0–10 ratings per synthesis on three aspects (Comprehensive, Trust, Utility); due to published data format, authors used overall mean per domain for correlation analyses.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human annotators exhibited strong positive inter-annotator correlation (ρ = 0.710, p = 0.001) when aggregated as used in this study.",
            "comparison_to_human_generated": true,
            "comparison_results": "Used human CORE-GPT ratings as reference to compare LLM evaluator outputs; found weak or very weak correlations between LLM-generated scores and human annotators (insufficient evidence of association), though LLMs correlated strongly with each other.",
            "limitations_noted": "Single, relatively small dataset; aggregation of scores (domain means) due to published format may obscure individual annotation nuance; only two annotators; dataset syntheses themselves were generated by GPT-4 (not human-authored references).",
            "uuid": "e7785.0",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Comprehensive/Trust/Utility",
            "name_full": "Three-aspect evaluation criteria: Comprehensive, Trust, Utility",
            "brief_description": "A triad of qualitative criteria used to evaluate scientific syntheses: comprehensiveness of content coverage, trustworthiness (accuracy / sourcing / citations), and practical usefulness (utility) to intended users.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (syntheses generator) and LLM evaluators (GPT-4 Turbo, Mistral-7B)",
            "model_size": "GPT-4 Turbo; Mistral-7B",
            "scientific_domain": "scientific synthesis / summarization evaluation",
            "theory_type": "evaluation criteria (qualitative ratings)",
            "evaluation_method_name": "Three-aspect human/LLM rating schema",
            "evaluation_method_description": "Raters (human or LLM) assign a numeric score (0–10) for each of three aspects—Comprehensive, Trust, Utility—and provide a short rationale for each score; used as both human ground truth and LLM evaluation targets.",
            "evaluation_metric": "Numeric ratings per aspect (Comprehensive, Trust, Utility)",
            "metric_definition": "0–10 integer/real scale where 0 = poor and 10 = excellent; accompanied by textual rationale required in prompts.",
            "dataset_or_benchmark": "CORE-GPT (applies these labels and ratings)",
            "human_evaluation_details": "Human annotators in CORE-GPT used this exact 0–10 scale for the three aspects; LLMs were prompted to produce the same structured ratings and rationales in JSON.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LLMs produced numeric ratings and rationales; GPT-4 was more conservative and provided more detailed rationales; Mistral sometimes produced non-numeric labels and extra text.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM ratings showed weak correlation with human ratings overall; LLMs offered plausible rationales but did not reliably match human judgments statistically.",
            "limitations_noted": "Subjectivity of 'comprehensive' requires domain knowledge; presence/absence of citations strongly affected 'trust' ratings; LLM output variability (non-numeric responses) complicates automated aggregation.",
            "uuid": "e7785.1",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Spearman's ρ correlation",
            "name_full": "Spearman's rank correlation coefficient (Spearman's ρ)",
            "brief_description": "A nonparametric statistical measure of rank correlation used to quantify monotonic association between two ranked variables; used in this paper to compare LLM-generated scores and human annotator scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM evaluators (GPT-4 Turbo, Mistral-7B)",
            "model_size": "GPT-4 Turbo; Mistral-7B",
            "scientific_domain": "statistical evaluation / correlation analysis",
            "theory_type": "statistical comparison metric",
            "evaluation_method_name": "Spearman's ρ test",
            "evaluation_method_description": "Compute Spearman's rank correlation coefficient between vectors of ratings (LLM vs human, annotator vs annotator, LLM vs LLM) to assess monotonic association; p-values reported for significance.",
            "evaluation_metric": "Spearman's ρ and associated p-value",
            "metric_definition": "ρ ranges from -1 (perfect inverse rank correlation) to +1 (perfect rank correlation); p-value tests null hypothesis of no association.",
            "dataset_or_benchmark": "CORE-GPT (domain-mean aggregated ratings used for correlation)",
            "human_evaluation_details": "Correlations computed between each annotator and each LLM, and between annotators themselves, using aggregated domain means due to data format.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Annotator1 vs Annotator2: ρ = 0.710 (p = 0.001); GPT-4 vs Mistral: ρ = 0.786 (p = 0.000); correlations between annotators and LLMs were weak or very weak and not statistically significant.",
            "comparison_to_human_generated": true,
            "comparison_results": "Statistical evidence indicates LLM evaluator outputs do not reliably reproduce human judgment patterns on CORE-GPT under the aggregation used.",
            "limitations_noted": "Analysis used aggregated domain means (could not match individual syntheses to individual annotator scores), potentially reducing sensitivity; single-run LLM outputs not averaged.",
            "uuid": "e7785.2",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 Turbo (as evaluator)",
            "name_full": "GPT-4 Turbo (OpenAI) used as an evaluator LLM",
            "brief_description": "Proprietary state-of-the-art LLM (GPT-4 Turbo) used in this study to evaluate GPT-4–generated scientific syntheses by producing numeric ratings (0–10) and textual rationales per aspect in JSON format.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_size": "proprietary (GPT-4 Turbo)",
            "scientific_domain": "NLP / summarization evaluation / scientific synthesis",
            "theory_type": "evaluation output (ratings + rationales)",
            "evaluation_method_name": "Prompted LLM evaluation with JSON rating + rationale",
            "evaluation_method_description": "LLM is prompted with instructions, definitions of aspects, and the question/answer/abstracts; asked to return JSON with numeric ratings (0–10) and a rationale per aspect.",
            "evaluation_metric": "Numeric ratings for Comprehensive, Trust, Utility (0–10) and qualitative rationales; Spearman's ρ against human ratings.",
            "metric_definition": "0–10 numeric scale per aspect; correlation measured via Spearman's ρ.",
            "dataset_or_benchmark": "CORE-GPT (100 syntheses)",
            "human_evaluation_details": "LLM required to include a short rationale for each rating (prompt mirrored CORE-GPT annotator instructions).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-4 produced detailed rationales and generally more conservative numeric ratings (no 10s); correlation with human annotators was weak; GPT-4 and Mistral correlated strongly with each other (ρ = 0.786).",
            "comparison_to_human_generated": true,
            "comparison_results": "GPT-4's ratings did not strongly match human annotators' ratings (weak correlations); qualitatively produced plausible rationales but sometimes inscrutable trust judgments.",
            "limitations_noted": "Single run per synthesis (no averaging); GPT-4 was conservative, and rationale quality varied by aspect (trust especially inconsistent in mapping to numeric ratings).",
            "uuid": "e7785.3",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Mistral-7B (as evaluator)",
            "name_full": "Mistral-7B (Jiang et al., 2023) used as an evaluator LLM",
            "brief_description": "Open-source 7B-parameter LLM used to rate scientific syntheses according to the same 3-aspect schema; exhibited more variability in response format and often shorter rationales compared to GPT-4.",
            "citation_title": "Mistral 7B. Preprint.",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_size": "7B parameters",
            "scientific_domain": "NLP / summarization evaluation / scientific synthesis",
            "theory_type": "evaluation output (ratings + rationales)",
            "evaluation_method_name": "Prompted LLM evaluation with JSON rating + rationale",
            "evaluation_method_description": "Same prompt as for GPT-4 Turbo; LLM asked to return JSON with numeric ratings and rationales per aspect; some outputs were non-numeric and excluded from analysis.",
            "evaluation_metric": "Numeric ratings for Comprehensive, Trust, Utility (0–10) and qualitative rationales; Spearman's ρ against human ratings and vs GPT-4.",
            "metric_definition": "0–10 numeric scale per aspect; correlation measured via Spearman's ρ.",
            "dataset_or_benchmark": "CORE-GPT",
            "human_evaluation_details": "Mistral sometimes returned non-numeric labels (e.g., \"excellent\") which were excluded; occasionally appended plaintext after JSON which was discarded.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Mistral produced plausible rationales but was less detailed than GPT-4; correlation with human annotators weak; Mistral and GPT-4 correlation strong (ρ = 0.786).",
            "comparison_to_human_generated": true,
            "comparison_results": "Mistral's numeric ratings sometimes diverged substantially from human annotators (e.g., rated absence of citations highly in cases humans penalized strongly).",
            "limitations_noted": "Response format variability (non-numeric outputs) required manual exclusion; rationales less detailed; potential reliance on general knowledge rather than abstracts in some cases.",
            "uuid": "e7785.4",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ROUGE",
            "name_full": "ROUGE: A package for automatic evaluation of summaries (ROUGE-N, ROUGE-L)",
            "brief_description": "A family of lexical-overlap metrics (ROUGE-N, ROUGE-L, etc.) that measure n-gram recall and longest common subsequence between a reference summary and a system-generated summary; widely used baseline for summarization evaluation.",
            "citation_title": "ROUGE: A package for automatic evaluation of summaries.",
            "mention_or_use": "mention",
            "model_name": "not specific (baseline summarization metrics)",
            "model_size": "n/a",
            "scientific_domain": "text summarization evaluation / NLP",
            "theory_type": "automatic evaluation metric",
            "evaluation_method_name": "ROUGE family (ROUGE-N, ROUGE-L)",
            "evaluation_method_description": "Compute lexical overlap (n-gram recall for ROUGE-N; longest common subsequence for ROUGE-L) between system and human reference texts as proxy for summary quality.",
            "evaluation_metric": "ROUGE-N (n-gram recall), ROUGE-L (LCS-based score)",
            "metric_definition": "ROUGE-N: recall of matching n-grams (typically reported as percentage); ROUGE-L: F-measure or recall based on longest common subsequence of tokens.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Lacks semantic awareness, sensitive to terminological variation and paraphrase; evaluates content selection but not linguistic quality (grammaticality, coherence); many variants (192 reported) with differing behavior.",
            "uuid": "e7785.5",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "BLEU",
            "name_full": "BLEU: a Method for Automatic Evaluation of Machine Translation",
            "brief_description": "A lexical-matching metric originally for machine translation that compares n-gram overlap between candidate and reference texts; sometimes used in summarization contexts.",
            "citation_title": "BLEU: a Method for Automatic Evaluation of Machine Translation.",
            "mention_or_use": "mention",
            "model_name": "not specific",
            "model_size": "n/a",
            "scientific_domain": "NLP evaluation metrics",
            "theory_type": "automatic evaluation metric",
            "evaluation_method_name": "BLEU",
            "evaluation_method_description": "Compute precision of n-gram overlap between candidate and reference, with brevity penalty to penalize short outputs.",
            "evaluation_metric": "BLEU score (typically 0–1 or reported as percentage)",
            "metric_definition": "Aggregate n-gram precision with geometric mean and brevity penalty; often reported as percentage score.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Lexical-only; insensitive to paraphrase and semantic equivalence; not ideal for abstractive summarization.",
            "uuid": "e7785.6",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pyramid",
            "name_full": "The Pyramid Method: Incorporating human content selection variation in summarization evaluation",
            "brief_description": "A human-centric summarization evaluation framework that accounts for variation in content selection by weighting summary content units (SCUs) according to how many human references include them.",
            "citation_title": "The Pyramid Method: Incorporating human content selection variation in summarization evaluation.",
            "mention_or_use": "mention",
            "model_name": "not specific",
            "model_size": "n/a",
            "scientific_domain": "summarization evaluation / human-centric metrics",
            "theory_type": "human-based evaluation framework",
            "evaluation_method_name": "Pyramid method",
            "evaluation_method_description": "Annotators identify content units across multiple references; system summary scored by how many high-weight SCUs it contains, capturing inter-reference variation in importance.",
            "evaluation_metric": "Pyramid score (weighted sum of matched SCUs)",
            "metric_definition": "Score computed as sum of weights of SCUs present in system summary divided by maximum possible score for those SCUs; relative scale depending on annotations.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Labor-intensive (requires multiple human references and SCU annotation); not widely automated.",
            "uuid": "e7785.7",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "METEOR",
            "name_full": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments",
            "brief_description": "An automatic evaluation metric that improves over BLEU by incorporating synonymy, stemming, and alignment-based matching, intended to better correlate with human judgments.",
            "citation_title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.",
            "mention_or_use": "mention",
            "model_name": "not specific",
            "model_size": "n/a",
            "scientific_domain": "NLP evaluation metrics / MT and summarization",
            "theory_type": "automatic evaluation metric",
            "evaluation_method_name": "METEOR (and ME-TEOR variants)",
            "evaluation_method_description": "Compute alignment-based match score between candidate and reference, incorporating exact, stem, synonym, and paraphrase matches with tunable weights.",
            "evaluation_metric": "METEOR score (usually 0–1)",
            "metric_definition": "Weighted F-score of matches with penalty for fragmentation; reported as normalized value or percentage.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Improves over pure lexical metrics but still limited for deep semantic evaluation of abstractive summaries.",
            "uuid": "e7785.8",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "BERTScore",
            "name_full": "BERTScore: Evaluating Text Generation with BERT",
            "brief_description": "A semantic evaluation metric that computes similarity between candidate and reference texts using contextualized token embeddings (BERT) and aligns tokens to compute precision, recall, and F1.",
            "citation_title": "BERTScore: Evaluating Text Generation with BERT.",
            "mention_or_use": "mention",
            "model_name": "not specific",
            "model_size": "n/a",
            "scientific_domain": "semantic evaluation metrics / NLP",
            "theory_type": "automatic evaluation metric",
            "evaluation_method_name": "BERTScore",
            "evaluation_method_description": "Use contextual embeddings to compute pairwise token similarities and derive precision/recall/F1 through greedy token alignment, capturing semantic similarity beyond exact lexical overlap.",
            "evaluation_metric": "BERTScore precision/recall/F1 (often reported as percentage)",
            "metric_definition": "Normalized similarity aggregated across tokens, typically reported as F1 in range 0–1 (or 0–100%).",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Depends on embedding model; can conflate plausible but unsupported content with correct semantic overlap; not a perfect substitute for human judgment.",
            "uuid": "e7785.9",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MoverScore",
            "name_full": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "brief_description": "A semantic evaluation metric that measures minimal cost to transform the token embedding distribution of the candidate text into that of the reference using Earth Mover's Distance, capturing semantic shifts.",
            "citation_title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.",
            "mention_or_use": "mention",
            "model_name": "not specific",
            "model_size": "n/a",
            "scientific_domain": "semantic evaluation metrics / NLP",
            "theory_type": "automatic evaluation metric",
            "evaluation_method_name": "MoverScore",
            "evaluation_method_description": "Compute Earth Mover's Distance between contextualized embeddings of candidate and reference texts to quantify semantic distance; lower cost indicates higher similarity.",
            "evaluation_metric": "MoverScore (normalized similarity / distance score)",
            "metric_definition": "Normalized distance-based similarity often reported 0–1 or as percentage; depends on embedding and normalization choices.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Computationally heavier than lexical metrics; embedding choices affect behavior; may not fully capture factual accuracy/trustworthiness.",
            "uuid": "e7785.10",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "SUPERT",
            "name_full": "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization",
            "brief_description": "An unsupervised evaluation approach proposed for multi-document summarization that aims to better capture semantic correctness and coverage than lexical metrics.",
            "citation_title": "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization.",
            "mention_or_use": "mention",
            "model_name": "not specific",
            "model_size": "n/a",
            "scientific_domain": "multi-document summarization evaluation",
            "theory_type": "automatic/unsupervised evaluation metric",
            "evaluation_method_name": "SUPERT",
            "evaluation_method_description": "Unsupervised metric intended to address multi-document summarization evaluation challenges (semantic coverage and correctness) using embedding-based approaches.",
            "evaluation_metric": "SUPERT score (method-specific semantic measure)",
            "metric_definition": "Method-defined scalar score (implementation-dependent); intended to reflect semantic fidelity and coverage.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Relatively new and not yet widely adopted; does not by itself address factuality/trust comprehensively.",
            "uuid": "e7785.11",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "G-Eval",
            "name_full": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
            "brief_description": "An approach that uses GPT-4 as an automatic evaluator (G-Eval) aiming at improved alignment with human judgments for NLG evaluation tasks.",
            "citation_title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (as evaluator in referenced work)",
            "model_size": "GPT-4",
            "scientific_domain": "NLG evaluation / automatic evaluation with LLMs",
            "theory_type": "LLM-based evaluation framework",
            "evaluation_method_name": "G-Eval (LLM-as-evaluator)",
            "evaluation_method_description": "Use GPT-4 (prompted) to score and justify evaluations of generated text, leveraging LLM capabilities to approximate human judgment.",
            "evaluation_metric": "LLM-produced scores and rationales; comparison to human judgments via correlation metrics",
            "metric_definition": "Scores typically on task-specific scales; correlation measured against human annotations (e.g., Spearman, Pearson).",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "In referenced work, GPT-4 evaluation compared to human judgments to assess alignment and correlation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": true,
            "comparison_results": "Prior work shows GPT-4 evaluations can correlate well with human judgements in some NLG tasks (referenced as motivation for LLM evaluators).",
            "limitations_noted": "LLM evaluators can be sensitive to prompts and may produce variable outputs; potential for justifying hallucinated or unsupported content.",
            "uuid": "e7785.12",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Prompt + Rationale requirement",
            "name_full": "Prompt-based LLM evaluation with required rationales / JSON output",
            "brief_description": "A methodological framework in which LLMs are prompted to return structured numeric ratings plus short textual rationales (here in JSON), which can improve interpretability and (reported in prior work) correlation with human judgments.",
            "citation_title": "A Closer Look into Using Large Language Models for Automatic Evaluation.",
            "mention_or_use": "use",
            "model_name": "LLM evaluators (GPT-4 Turbo, Mistral-7B)",
            "model_size": "GPT-4 Turbo; Mistral-7B",
            "scientific_domain": "LLM-based evaluation methodology",
            "theory_type": "evaluation protocol / prompting strategy",
            "evaluation_method_name": "Prompted LLM evaluation with required rationales (structured JSON)",
            "evaluation_method_description": "Provide explicit instructions, definitions of quality aspects, rating scale, and response format; require numeric ratings and justificatory rationales in machine-readable JSON to both evaluate quality and obtain explanations.",
            "evaluation_metric": "Numeric ratings per aspect (0–10) plus textual rationales; correlations with human ratings (Spearman's ρ) used to assess alignment.",
            "metric_definition": "Ratings on 0–10 scale; presence and content of rationale used qualitatively and can be analyzed for alignment with numeric scores.",
            "dataset_or_benchmark": "CORE-GPT (used in this work); prior work cited showing improved correlation when rationales required.",
            "human_evaluation_details": "Prompt mirrors human annotation instructions; prior work (Chiang & Lee 2023a) reports that requiring justifications 'significantly improves' LLM-human correlation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "In this study, GPT-4 followed the requested JSON + rationales consistently; Mistral sometimes deviated. Prior work reports improved alignment when rationales are required.",
            "comparison_to_human_generated": true,
            "comparison_results": "Requiring rationales can improve correlation with human judgements (cited prior result); in this study, LLM rationales were informative but did not guarantee high numerical alignment with humans.",
            "limitations_noted": "Prompt sensitivity (Mistral output variability); rationales may still contain inaccuracies or unsupported inferences; single-run outputs limit stability.",
            "uuid": "e7785.13",
            "source_info": {
                "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Closer Look into Using Large Language Models for Automatic Evaluation.",
            "rating": 2,
            "sanitized_title": "a_closer_look_into_using_large_language_models_for_automatic_evaluation"
        },
        {
            "paper_title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment.",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering",
            "rating": 2,
            "sanitized_title": "coregpt_combining_open_access_research_and_large_language_models_for_credible_trustworthy_question_answering"
        },
        {
            "paper_title": "Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE?",
            "rating": 1,
            "sanitized_title": "revisiting_automatic_evaluation_of_extractive_summarization_task_can_we_do_better_than_rouge"
        },
        {
            "paper_title": "ROUGE: A package for automatic evaluation of summaries.",
            "rating": 1,
            "sanitized_title": "rouge_a_package_for_automatic_evaluation_of_summaries"
        }
    ],
    "cost": 0.0216625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models as Evaluators for Scientific Synthesis
3 Jul 2024</p>
<p>Julia Evans 
TIB -Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Jennifer D ' Souza jennifer.dsouza@tib.eu 
TIB -Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Sören Auer 
TIB -Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Large Language Models as Evaluators for Scientific Synthesis
3 Jul 20242E22CED5F91B1D789733C57264650155arXiv:2407.02977v1[cs.CL]
Our study explores how well the state-of-the-art Large Language Models (LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries or, more fittingly, scientific syntheses, comparing their evaluations to those of human annotators.We used a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers, checked against human quality ratings.The study evaluates both the closed-source GPT-4 and the open-source Mistral model's ability to rate these summaries and provide reasons for their judgments.Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings, yet a deeper statistical analysis shows a weak correlation between LLM and human ratings, suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have made a significant impact on natural language processing (NLP), demonstrating exceptional performance in tasks like text generation, sentiment analysis, machine translation, and question answering, with outputs that often rival human-created content (Huang et al., 2023).In addition to their direct applications, LLMs offer substantial benefits in streamlining machine learning model development, particularly in evaluation processes.They reduce the dependency on human-generated ground truth data and the necessity for human evaluators (Bai et al., 2023) in two key ways: by facilitating the generation of synthetic ground truth data and by serving as evaluators for model predictions themselves.This approach not only speeds up the evaluation process but also broadens the scope of evaluation criteria to include factors such as diversity and coverage, enhancing the efficiency and comprehensiveness of model assessments.</p>
<p>This study investigates the use of LLMs as evaluators to streamline the evaluation process, moving away from traditional reliance on human evaluators and human-generated ground truth data.It specifically examines the effectiveness of LLMs in synthesizing scientific abstracts seen generally as multi-document summarization tasks.The main focus of this research is to assess how two state-ofthe-art LLMs-the proprietary GPT-4 Turbo (Ope-nAI, 2023) and the open-source Mistral-7B (Jiang et al., 2023)-perform in evaluating scientific syntheses.Furthermore, leveraging LLMs meant better versatility in evaluation considerations, which meant that the evaluations tested varied dimensions of syntheses quality, viz.comprehensiveness, trustworthiness, and utility.</p>
<p>This paper is structured as follows.First, section 2 presents a review of related work in the fields of text summarization and LLM evaluation.In section 3, we show our approach to using LLMs for scientific synthesis evaluation, wherein subsection 3.1 describes the LLM output, while subsection 3.2 presents a qualitative evaluation of this output.In subsection 3.3, we analyze the correlation between LLM ratings and human judgments.A discussion of our findings and final conclusions is described in section 4.</p>
<p>Related Work</p>
<p>Evaluation Metrics for Text Summarization.The most common automatic evaluation metric used within summarization research -both singledocument and multi-document -is the ROUGE family of metrics (Ma et al., 2022;Akter et al., 2022;Cohan and Goharian, 2016;Kryscinski et al., 2019;Lloret et al., 2018).ROUGE metrics (Lin, 2004) calculate the lexical overlap between a human-written reference document and an automatically generated one, although variants incorporating semantic information also exist.Within text summarization research, the most commonly used are ROUGE-N and ROUGE-L (Ma et al., 2022), both of which are purely lexical-matching metrics.ROUGE-N evaluates the recall of n-grams by comparing a reference text with a corresponding machine-generated text, whereas ROUGE-L calculates the longest common subsequence of tokens shared between reference and machine-generated texts (Lin, 2004).</p>
<p>Despite its predominance within the field, ROUGE nonetheless has some notable limitations.First, the most commonly used metrics lack semantic awareness (Akter et al., 2022;Ma et al., 2022).Studies have pointed out that ROUGE may not accurately estimate summary quality in cases of terminological variations, paraphrasing, and differences in sentence structure (Cohan and Goharian, 2016).Moreover, there exist 192 ROUGE variants (Graham, 2015), with meaningful differences in how well each performs on a given system or specialized task (Cohan and Goharian, 2016;Graham, 2015;Kryscinski et al., 2019) and how well they correlate with human judgements (Kryscinski et al., 2019;Graham, 2015).Finally, ROUGE evaluates only content selection but not linguistic quality aspects such as grammaticality and referential clarity (Pitler et al., 2010) or overall quality, including the ordering of information and structural clarity (Graham, 2015).</p>
<p>Although no other metrics have gained widespread adoption, other approaches exist.Additional lexical-matching metrics include BLEU (Papineni et al., 2002) and Pyramid (Nenkova et al., 2007).Semantically enriched metrics include ME-TEOR (Banerjee and Lavie, 2005), an expansion of BLEU, and approaches utilizing word embeddings, such as BERTScore (Zhang et al., 2020), Mover-Score (Zhao et al., 2019), and SUPERT (Gao et al., 2020).However, none of these metrics address all of ROUGE's weaknesses, and the limited use of such metrics within the research community means that ROUGE remains the "de facto" standard (Lloret et al., 2018).</p>
<p>LLMs for Text Evaluation.Using LLMs for text evaluation is still a nascent research topic.Several recent works have compared LLMs' text evaluations to human evaluations on multiple tasks, and report that LLMs produce results similar to human judgements (Chiang and Lee, 2023b;Liu et al., 2023;Wang et al., 2023).One work finds only minor variations in results depending on task instructions and hyperparameters, whereas they find a high degree of variation in performance of different LLMs and the quality characteristics being assessed (Chiang and Lee, 2023b).In evaluating the quality of story fragments by grammaticality, cohesiveness, likability, and relevance, they find only a weak correlation between humans and LLMs on grammaticality, but a moderate correlation on relevance.Contrarily, another work finds that Chat-GPT's performance is sensitive to prompt instructions (Wang et al., 2023).They also show that ChatGPT evaluations correlate especially well with human evaluations for creative tasks like story generation (Wang et al., 2023).Another work demonstrates that requiring LLMs to provide a justification for their ratings "significantly improves the correlation between the LLMs' ratings and human ratings" (Chiang and Lee, 2023a).</p>
<p>Only one work has investigated the task of text summarization evaluation (Liu et al., 2023).They evaluate single-document news article summaries on the aspects of coherence, consistency, fluency, and relevance; their results exceed the correlation with human judgements of most automatic approaches, including ROUGE.In another task, Chat-GPT successfully identifies implicit hate speech in Tweets and generates explanations of why the texts are hateful, which human annotators judge equally informative to human-written explanations and of greater clarity (Huang et al., 2023).</p>
<p>LLMs for the Scientific Synthesis Evaluation Task</p>
<p>The accurate evaluation of scientific syntheses is a critical task in research, ensuring the integrity and reliability of the synthesized information.While recent advancements have demonstrated the efficacy of LLMs in generating such syntheses (Pride et al., 2023), their potential in evaluating them remains relatively unexplored.Motivated by the limitations of existing evaluation metrics, such as the ROUGE family, and the success of LLMs in other text evaluation tasks, our work seeks to investigate the suitability of LLMs for the task of assessing the quality of scientific syntheses.</p>
<p>To address this question, we employ the proprietary GPT-4 Turbo (OpenAI, 2023) and the opensource Mistral-7B models (Jiang et al., 2023) to evaluate the CORE-GPT dataset (Pride et al., 2023).This dataset comprises 100 research questions spanning 20 diverse domains, each accompanied by the titles and abstracts of five related works and an an-swer to the research question generated by GPT-4 by synthesizing the provided abstracts.Additionally, human ratings from two annotators, on a scale of 0 to 10, are available on the quality of each synthesis in three dimensions, viz.comprehensive, trust, and utility.</p>
<p>For our task, we query the LLMs to evaluate the syntheses according to the same three aspects as the CORE-GPT human raters.Our prompt follows a similar structure to previous work (Chiang and Lee, 2023a).It contains two lines of task instruction, explanation of the quality aspects (as defined for the CORE-GPT dataset annotators) and the rating scale, response format instructions, and finally the answer to be evaluated with its question and abstracts.The response is requested in JSON format, with a numeric rating between 0 and 10 for each aspect as well as a rationale for each rating.The full text of the prompt is in Appendix A.</p>
<p>LLM Synthesis Evaluation Output</p>
<p>A representative example of the evaluation output from GPT-4 Turbo and Mistral is shown in Appendix B and Appendix C, respectively.The output from GPT-4 was exactly as requested, while Mistral had some variability.In one case, Mistral returned ratings of "excellent," "good," and "high" rather than numeric scores; this output was excluded from the analysis.In several other cases, Mistral included a paragraph after the JSON object which summarized the ratings and rationales provided within it.These paragraphs were discarded and only the JSON object content was evaluated.</p>
<p>An overview of LLM performance was obtained by reviewing one synthesis from each domain evaluated by both GPT-4 and Mistral.Qualitatively, both models demonstrated credible and logically consistent ratings and rationales.GPT-4 provided more detailed rationales compared to Mistral, with slightly lower ratings overall.</p>
<p>In their rationales for comprehensive, both LLMs would sometimes highlight relevant topics from the abstracts which were not included in the synthesis, with GPT-4 producing such rationales more often than Mistral.Occasionally, some rationales contained justifications relating to content more specific than just the topics, suggesting more information on the results or the methodology of the studies would improve it.</p>
<p>The LLMs seemed to show the greatest discrepancy between rating and rationale, and the greatest inconsistencies, in their evaluations of trust.In one Mistral evaluation with a rating of 5, the rationale noted that the citations only improved trustworthiness "as long as the abstract accurately represents the study's findings."In the absence of any evidence the abstract is suspect, this rating is disproportionately low.GPT-4 was notably more conservative than human annotators, as it did not give a single 10.Especially for trust, it was often difficult to understand why a rating wasn't higher.For instance, the rationale for one rating of 8 praised the synthesis for accuracy and avoiding unsupported claims.</p>
<p>For the utility ratings, it appears that most rationales from GPT-4 suggested additional content which could make the synthesis more useful, such as actionable information, more detailed examples, technical details of methodologies and implementation, and so on.Mistral made such suggestions less frequently; its rationales tended to echo the rationale for comprehensive.However, Mistral did sometimes provide guidance on who would or would not find the synthesis useful.</p>
<p>Qualitative Evaluations</p>
<p>LLMs are known to sometimes generate content on topics that lack factual basis with a highly persuasive level of linguistic proficiency (Bang et al., 2023;Liu et al., 2023).For scientific syntheses which provide an answer to a question, it is especially important that the content is genuinely a synthesis of the provided abstracts, with appropriate citations, and not independently generated based on the LLM's training data.For this reason, we were particularly interested in how the LLMs evaluated quality, and most importantly trust, when there was reason to believe the abstracts were not the (primary) source of the generated content, as in the following three scenarios.The complete question and answer pairs, along with their GPT-4 and Mistral evaluation scores and trust rationales, can be found in Appendix D.</p>
<p>Response Explicitly States Absence of Relevant Abstracts.In six cases, the synthesis directly expressed limitations due to the relevancy of the provided abstracts, e.g."[...] the provided search results do not offer specific information on the longterm health impacts of such medications on these organs."Human annotators responded very positively to this, with such responses "scored highly for trustworthiness" (Pride et al., 2023).Mistral rated four of these syntheses as 10 for trust, citing factual accuracy and abstract sourcing, while two scored 7. GPT-4 ratings varied, at 5, 5, 5, 7, 7, and 8. Mistral rationales did not reference the stated limitation, while GPT-4 acknowledged it positively in three cases.However, as these syntheses were scored 8, 7, and 5, it is unclear to what extent this acknowledgement may have influenced the scores.</p>
<p>Response Contains No Citations.There were three responses which answered the question but contained no citations.GPT-4 gave trust scores of 0, 0, and 1, with rationales referring to the lack of citations.In contrast, Mistral scored 8, 10, and 10, with rationales stating the information was common knowledge or referenced from the abstracts.</p>
<p>Response Contains One Citation.Finally, there were five syntheses which cited only one of the abstracts, which does not align with the task of synthesizing multiple abstracts to provide an answer to the given question.For GPT-4, the trust scores were 5, 7, 8, 8, and 9, with most rationales stating that the synthesis relied on general knowledge without directly referencing the abstracts, despite one citation being present in each case.Meanwhile, the Mistral scores were 7, 9, 9, 10, and 10, with most rationales indistinguishable from those of syntheses with many more citations -three of them claimed that the synthesis accurately references the content in the provided abstracts.</p>
<p>Correlation</p>
<p>Spearman's ρ was calculated to assess the relationship between the human annotators' scores and the LLM-generated scores.Using the publiclyavailable data from CORE-GPT (Pride et al., 2023) 1 , separate vectors for each annotator were obtained.To calculate the correlations, we found the overall mean score for each domain; due to the format of the published data, it was not possible to match individual scores to their corresponding syntheses.Our results for the overall mean are presented in Table 1.We find that only two results showed statistically significant p-values.Human annotators exhibited a strong positive correlation (0.710), as did GPT-4 Turbo and Mistral (0.786).However, correlations between annotators and LLMs were weak or very weak, with p-values indicating insufficient evidence for genuine association.These findings suggest LLMs cannot directly replicate human performance in evaluating scientific syntheses.Despite this, the strong positive correlation between GPT-4 Turbo and Mistral indicates consistency between the two LLMs.</p>
<p>Discussion and Conclusion</p>
<p>We explore the capacity of LLMs in assessing scientific syntheses.GPT-4 Turbo and Mistral are utilized to obtain quality ratings for 100 syntheses from the CORE-GPT dataset (Pride et al., 2023), accompanied by a rationale for each rating.Correlation analysis using Spearman's ρ indicates that the LLM performance does not align with the human annotators' judgements.However, a qualitative evaluation of the responses finds a more mixed result.</p>
<p>Both LLMs generally produce credible and logically consistent ratings and rationales, but GPT-4 appears more conservative in its ratings and provides more detail and specific recommendations in its rationales.GPT-4 also displays greater sensitivity to the presence or absence of citations compared to Mistral.However, both LLMs' rationales occasionally contained inaccuracies or flaws, raising concerns about the credibility of their scores.Moreover, the extent to which the responses are evaluated as syntheses and not simply as answers, without reliance on general knowledge, remains unclear, particularly in the case of Mistral.</p>
<p>Our findings highlight both promising developments and current limitations of leveraging LLMs for the task of evaluating scientific syntheses, illustrating the need for further research to validate and refine the methodology.</p>
<p>Limitations</p>
<p>We acknowledge several limitations that may influence the interpretation and generalizability of our findings.First, the reliance on a single, relatively small dataset presents limitations in terms of data representativeness.Moreover, the data format necessitated aggregating scores, which may have obscured potential nuances in individual annotations.</p>
<p>Second, the study focused exclusively on GPT-4 Turbo and Mistral, limiting the generalizability of our conclusions to other LLMs.While these models represent the state-of-the-art, future iterations or alternative architectures may exhibit different performance.Additionally, we were able to obtain only one set of ratings from each LLM.Given the variability of LLM output, taking the average of several runs is preferable, but due to financial limitations, this was not possible in our study.</p>
<p>We note that past work has found LLMs particularly adept at evaluating creative texts (Wang et al., 2023), so the narrow output scope of synthesis for scientific question answering may pose a greater challenge.We also note the difficulty of assessing the quality of syntheses from such a diverse assortment of domains.Judging how comprehensive a synthesis is requires some knowledge of the scope of potential information which might be appropriate to include.Highly specialized domain knowledge still presents a challenge to general use LLMs.</p>
<p>Ethical Considerations</p>
<p>In this work we have presented our study of the efficacy of two LLMs, one proprietary and one open-source, in evaluating the quality of scientific syntheses.There were no living subjects analyzed in this study.Overall, this study complies with the ACL Ethics Policy.</p>
<p>In querying the LLMs for synthesis quality evaluations, we declare that the instructions were intended to align the behavior of the language models towards producing responses that are both helpful (fulfilling our objective) and harmless (not causing any physical, psychological, or social harm to individuals or the environment).All of the intellectual property which was passed to the LLMs is open-access.</p>
<p>B Example GPT-4 Turbo Output</p>
<p>GPT-4 Turbo COMPREHENSIVE: rating: 7, rationale:</p>
<p>The answer provides a broad overview of how living in a low-income neighborhood can affect social networks and well-being, referencing various studies that discuss mental health, social inclusion, and subjective well-being.However, it does not fully elaborate on the specific mechanisms or provide detailed insights from each abstract, such as the role of men in community well-being from abstract 3 or the impact of natural capital from abstract 4. TRUST: rating: 8, rationale: The answer is trustworthy as it directly references findings from the provided abstracts, indicating a reliance on research-based evidence.However, the answer could be more trustworthy if it included more specific data or findings from the abstracts to support the claims.UTILITY: rating: 7, rationale: The answer is useful as it gives an overview of the potential impacts of living in a low-income neighborhood on residents' social networks and well-being.It could be more useful if it provided actionable insights or recommendations based on the findings of the studies.</p>
<p>C Example Mistral Output</p>
<p>Mistral COMPREHENSIVE: rating: 9, rationale:</p>
<p>The answer provides specific examples and references to studies that directly address the question of how social media influences the formation and maintenance of relationships among young adults in the United States, specifically focusing on LGBTQ+ individuals with disabilities and young adults who have moved abroad.TRUST: rating: 10, rationale: The answer is based on empirical research and provides clear citations to support the claims made.UTILITY: rating: 10, rationale: The answer is directly relevant to the question and provides useful information for those interested in understanding how social media impacts relationship formation and maintenance among young adults in the United States.</p>
<p>The given question and answer pair is of high quality.The question is clear, specific, and comprehensive, while the answer is well-researched, trustworthy, and useful.It directly addresses the question by providing examples from empirical studies that explore how social media influences the formation and maintenance of relationships among young adults in the United States, specifically focusing on LGBTQ+ individuals with disabilities and young adults who have moved abroad.The answer is also trustworthy as it is based on reliable sources and provides clear citations to support the claims made.Finally, the answer is useful as it provides valuable insights into how social media impacts relationship formation and maintenance among a specific population in the United States.</p>
<p>D Trust Evaluation D.1 Response Explicitly States Absence of Relevant Abstracts</p>
<p>How do LLMs evaluate when the response states that a complete answer cannot be given, due to lack of relevant abstracts?</p>
<p>D.1.1 Chemistry Question: What is the effect of pH on the efficiency of a common catalyst used in the production of biodiesel?</p>
<p>Answer: The effect of pH on the efficiency of a catalyst used in biodiesel production can be understood through the specific catalyst being used.For instance, mixed oxide catalysts like magnesium oxide (MgO) and calcium oxide (CaO) are used to promote the transesterification of vegetable oils into biodiesel.The basic property of these catalysts plays a crucial role in the biodiesel yield during the reaction [3].However, the provided search results do not offer a direct relationship between pH and the efficiency of such catalysts in biodiesel production.Further research is required to establish a clear connection between pH levels and the performance of common catalysts used in biodiesel production.</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 7 8 6</p>
<p>The answer is trustworthy as it accurately reflects the information available in the provided abstracts.It does not make unsupported claims and is honest about the limitations of the information provided in the abstracts regarding the direct effect of pH on catalyst efficiency.Mistral 5 7 6</p>
<p>The answer is based on accurate information from the abstracts, specifically the third abstract which discusses the effect of basicity (pH) on the efficiency of magnesium oxide and calcium oxide catalysts in biodiesel production.</p>
<p>Table 2: Ratings for the answer to the question "What is the effect of pH on the efficiency of a common catalyst used in the production of biodiesel?"</p>
<p>D.1.2 Chemistry</p>
<p>Question: What is the effect of varying concentrations of hydrogen peroxide on the rate of catalase enzyme activity?Answer: Varying concentrations of hydrogen peroxide (H2O2) can affect the rate of catalase enzyme activity.Catalase is an enzyme that breaks down H2O2 into oxygen and water, neutralizing oxidative stress in cells.This process is crucial for maintaining appropriate levels of oxidative stress because reactive oxygen species, such as H2O2, can cause cellular damage by targeting DNA and proteins, which can lead to complications like cancer, diabetes, neurodegenerative diseases, and aging [2].While the given search results do not provide exact information on how different concentrations of H2O2 impact the rate of catalase enzyme activity, it is evident that altering H2O2 levels will directly influence catalase's efficiency in breaking down H2O2 and neutralizing oxidative stress in cells [2].</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 3 5 3</p>
<p>The answer is somewhat trustworthy as it correctly describes the function of catalase and the consequences of oxidative stress, which is supported by the abstracts.However, it lacks direct evidence from the abstracts regarding the specific impact of different H2O2 concentrations on catalase activity, which reduces the trustworthiness of the response in the context of the question asked.</p>
<p>Mistral 5 10 8</p>
<p>The answer is trustworthy as it is directly derived from the provided abstracts and does not contain any incorrect or misleading information.</p>
<p>Table 3: Ratings for the answer to the question "What is the effect of varying concentrations of hydrogen peroxide on the rate of catalase enzyme activity?"D.1.3Economics Question: What is the impact of minimum wage laws on employment rates and income inequality in the United States?Answer: The impact of minimum wage laws on employment rates and income inequality in the United States is not directly addressed in the provided search results.However, one study suggests that an effective mechanism for managing labor productivity and wage issues is necessary for achieving greater economic equality and growth [1].Moreover, the study highlights the lack of clear definitions of salary, salary payment, and minimum wage, which can contribute to challenges in addressing income inequality [1].While the provided search results do not specifically discuss the impact of minimum wage laws on employment rates and income inequality in the United States, it is clear that issues related to wage management and labor productivity remain essential factors in ensuring fair compensation for workers and reducing inequality [1].
Comp. Trust Utility Trust Rationale GPT-4 2 5 2
The answer is somewhat trustworthy as it references a study that discusses labor productivity and wage issues, but it does not provide information from a study that directly addresses the impact of minimum wage laws on employment rates and income inequality in the United States.</p>
<p>Mistral 0 10 2</p>
<p>The answer is based on content from one of the provided abstracts, which is cited and referenced appropriately.The relationship between advanced mathematics coursework and problem-solving skills among undergraduate students in STEM majors is not explicitly discussed in the provided search results.However, the study on STEM in General Education [1] indicates that student mathematics skills were analyzed to determine their influence on course selection.Another study discusses the importance of engaging students in mathematical and scientific practices according to mathematics and science standards, such as the Common Core State Standards of Mathematics (CC-SSM) and the Next Generation Science Standards (NGSS) [4].This suggests that exposure to advanced mathematical coursework could potentially enhance problem-solving skills in undergraduate STEM students, though further research is needed to establish a concrete relationship.</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 5 7 4</p>
<p>The answer is trustworthy in the sense that it accurately reflects the lack of explicit discussion in the provided abstracts.It cautiously suggests a potential link between advanced mathematics coursework and problem-solving skills, which is a reasonable inference based on the emphasis on standards and practices in abstract [4].
Mistral 4 7 5
The answer is grounded in the content of the provided search results and cites relevant studies to support its suggestions.However, it also includes speculation about the potential relationship between advanced mathematics coursework and problem-solving skills based on indirect evidence.Answer: Regular use of over-the-counter (OTC) pain medications can potentially impact liver and kidney function in young adults.However, the provided search results do not offer specific information on the long-term health impacts of such medications on these organs.To obtain a comprehensive answer, further research on this topic would be necessary.</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 1 5 2</p>
<p>The answer is trustworthy in the sense that it does not fabricate information and admits the absence of specific details in the provided abstracts.However, it does not demonstrate a thorough examination of the abstracts to determine if any related information could be inferred.</p>
<p>Mistral 0 10 2</p>
<p>The answer is truthful and does not contain any false or misleading information.</p>
<p>Table 6: Ratings for the answer to the question "What are the potential long-term health impacts of regular use of over-the-counter pain medications on the liver and kidney function in young adults?"D.1.6Physics Question: How does the Doppler effect affect the observed frequency of sound waves emitted from a moving source and its relevance in real-world applications?</p>
<p>Answer: The Doppler effect affects the observed frequency of sound waves emitted from a moving source by causing a change in frequency and wavelength due to the relative motion between the source and the observer.When the source is moving towards the observer, the observed frequency increases, and the wavelength decreases; conversely, when the source is moving away from the observer, the observed frequency decreases, and the wavelength increases.Although the provided search results do not directly give real-world applications, the Doppler effect is relevant in various fields such as astronomy, radar technology, medical imaging, and seismology.For instance, it helps astronomers measure the speed of distant celestial objects and plays a critical role in Doppler radar systems for weather monitoring and forecasting.</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 5 7 6</p>
<p>The answer is generally trustworthy as it correctly describes the Doppler effect's impact on frequency and wavelength.However, it lacks specific references to the provided abstracts, which could have strengthened the trustworthiness by showing direct connections to the source material.</p>
<p>Mistral 5 10 7</p>
<p>The information provided in the answer is factual and accurate.</p>
<p>Table 7: Ratings for the answer to the question "How does the Doppler effect affect the observed frequency of sound waves emitted from a moving source and its relevance in real-world applications?"</p>
<p>D.2 Response Contains No Citations</p>
<p>How do the LLMs evaluate syntheses with no citations?</p>
<p>D.2.1 Art</p>
<p>Question: How has the evolution of technology impacted the production and consumption of art in the 21st century?Answer: The evolution of technology in the 21st century has significantly impacted the production and consumption of art.The widespread access to digital tools and platforms has revolutionized artistic practices, enabling artists to experiment with new mediums and techniques beyond traditional forms.Furthermore, technology has democratized the consumption of art, providing individuals across the world with opportunities to view, share, and engage with creative works online.This shift has also amplified the global reach of art, exposing diverse perspectives and fostering crosscultural exchanges.However, the rise of digital art has also raised questions about the authenticity and ownership of creative works, prompting a reevaluation of conventional definitions and practices in the art world.</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 1 1 2</p>
<p>The answer cannot be considered trustworthy in the context of the provided abstracts, as it does not cite any relevant information from them to support the claims made.</p>
<p>Mistral 5 8 7</p>
<p>The answer is based on common knowledge about the impact of technology on art and does not contain any false or misleading information.</p>
<p>Table 8: Ratings for the answer to the question "How has the evolution of technology impacted the production and consumption of art in the 21st century?"D.2.2 History Question: How did the American Civil War impact the social, economic, and political landscape of the United States?Answer: The American Civil War significantly impacted the social, economic, and political landscape of the United States.On the social front, the war provided a catalyst for the abolition of slavery, which altered racial dynamics and paved the way for civil rights movements.Economically, the war devastated the South and forced a transition from an agrarian, slave-driven economy to a more diverse, industrial one.Reconstruction policies and investments led to the rapid growth of infrastructure and industry in the North, shaping the national economy for decades.Politically, the Civil War reaffirmed the primacy of the federal government over individual states and brought about transformative constitutional amendments such as the 13th, 14th, and 15th, which abolished slavery, granted citizenship to former slaves, and provided voting rights regardless of race, color, or previous servitude, respectively.The war reshaped the balance of power and the role of government in Americans' lives, laying the groundwork for a more unified and inclusive nation.</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 0 0 0</p>
<p>The answer cannot be trusted as it does not reference the provided abstracts, which do not relate to the American Civil War.The information seems to be general knowledge rather than sourced from the abstracts.</p>
<p>Mistral 10 10 9</p>
<p>The answer is trustworthy as it is derived directly from the provided abstracts and does not contain any additional or inaccurate information.</p>
<p>Table 9: Ratings for the answer to the question "How did the American Civil War impact the social, economic, and political landscape of the United States?"D.2.3 Physics Question: How does the shape of an object affect its resistance to motion through a fluid medium?Answer: The shape of an object can significantly impact its resistance to motion through a fluid medium.This resistance, referred to as drag, depends on factors such as the size, shape, and surface texture of the object, along with the properties of the fluid in question.A streamlined shape minimizes drag, while a more irregular or bluff shape increases the force exerted by the fluid on the object, thus making it more resistant to motion.In fluid dynamics, this concept is commonly applied to improve the efficiency of objects such as vehicles and aircraft, where a streamlined design results in reduced fuel consumption and increased speed, due to the lowered drag forces acting against the object's movement through the fluid medium.</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 0 0 0 The answer cannot be trusted because it does not reference any of the provided abstracts, which are the only sources that should be used to inform the answer.</p>
<p>Mistral 0 10 3 The answer is trustworthy as it accurately references information from the provided text.</p>
<p>Table 10: Ratings for the answer to the question "How does the shape of an object affect its resistance to motion through a fluid medium?"</p>
<p>D.3 Response Contains One Citation</p>
<p>How do the LLMs evaluate syntheses with only one citation source?</p>
<p>D.3.1 Geology</p>
<p>Question: What geological factors contribute to the formation and sustainability of hydrothermal ecosystems in deep sea environments?Answer: Hydrothermal ecosystems in deep sea environments are primarily formed and sustained by geological factors, such as tectonic activity and volcanic processes.These factors promote the circulation of seawater through the oceanic crust, which results in the formation of hydrothermal vents.The seawater is heated by the underlying magma and interacts with crustal rocks, enriching it with minerals and chemical elements.Once the heated and mineral-rich fluid meets the cold deep-sea water, it precipitates, creating chimneylike structures from which the hydrothermal fluid is expelled.These vents provide a unique habitat with diverse microbial and chemosynthetic communities, which serve as primary producers in the food chain.Furthermore, these deep-sea ecosystems are enriched by specialized fauna adapted to extreme conditions, such as high pressure, low oxygen, and varying temperatures, contributing to the overall sustainability of these environments [5].</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 7 8 7</p>
<p>The answer appears to be trustworthy as it aligns with scientific understanding of hydrothermal vent formation and the factors that sustain deep-sea ecosystems.It does not directly reference the provided abstracts, but the information given is consistent with general scientific knowledge on the topic.</p>
<p>Mistral 10 10 9</p>
<p>The answer is based on accurate information from the provided abstracts and uses reputable sources, such as scientific research, to support the statements made.</p>
<p>D.3.2 Mathematics</p>
<p>Question: What is the impact of using manipulatives in mathematics instruction on the achievement and attitudes of elementary school students?Answer: The use of manipulatives in mathematics instruction has been shown to have a positive impact on the achievement and attitudes of elementary school students.In a study exploring the relationship between Affective Instructional Design (AID) and children's attitudes towards math learning, a significant positive correlation was found between attitude and math learning (r = 0.936, p = 0.000).Furthermore, significant differences were found between the baseline (pre-intervention) mean score and the final intervention lesson for both attitudes and math learning, indicating the potential effectiveness of manipulatives in enhancing students' attitudes towards mathematics and improving their achievement in the subject [1].</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 7 8 7</p>
<p>The answer is trustworthy as it accurately reflects the findings from the first abstract, including the significant correlation between attitude and math learning.</p>
<p>The statistical evidence (r = 0.936, p = 0.000) adds credibility to the claim.However, the answer does not provide information on the sample size or the generalizability of the study, which could affect trust in the findings.</p>
<p>Mistral 10 9 9</p>
<p>The answer cites a peer-reviewed academic study, adding credibility to the information provided.However, it would be helpful to know more about the study's methodology and sample size to fully assess its trustworthiness.</p>
<p>Table 12: Ratings for the answer to the question "What is the impact of using manipulatives in mathematics instruction on the achievement and attitudes of elementary school students?"</p>
<p>D.3.3 Philosophy</p>
<p>Question: What is the relationship between free will and determinism in philosophy?Answer: In philosophy, the relationship between free will and determinism revolves around whether an individual's actions are genuinely a result of their own choices or are predetermined by external factors.There are two main schools of thought: compatibilism and incompatibilism.Compatibilism argues that free will and determinism can coexist, meaning individuals can still be morally responsible for their actions even if determinism is true.Incompatibilism, on the other hand, claims that free will and determinism cannot coexist; if determinism is true, then individuals cannot have free will and cannot be held morally responsible for their actions.The debate between these two positions contributes to the ongoing philosophical discussions about responsibility, choice, and the implications of determinism [1].</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 7 9 8</p>
<p>The answer is trustworthy as it accurately reflects the general philosophical debate between compatibilism and incompatibilism, which are well-established positions in the literature on free will and determinism.</p>
<p>Mistral 8 9 7</p>
<p>The answer is based on accurate information from the provided abstracts, which discuss the relationship between free will, determinism, and moral responsibility.It does not contain any incorrect or misleading statements.</p>
<p>Table 13: Ratings for the answer to the question "What is the relationship between free will and determinism in philosophy?"D.3.4 Physics Question: How does the acceleration due to gravity affect the trajectory and motion of a projectile, and how can this be accurately measured and predicted using equations of motion?Answer: The acceleration due to gravity affects the trajectory and motion of a projectile by altering its vertical motion while leaving its horizontal motion constant.As a result, the trajectory forms a parabolic path.To accurately measure and predict a projectile's motion, equations of motion can be used, which consider the initial velocity, launch angle, and time.The initial velocity can be split into horizontal and vertical components, and equations for both components can be applied independently.For the horizontal motion, an equation is used that doesn't consider the acceleration due to gravity, while for the vertical motion, the effect of gravity is integrated into the equation.These equations can be used to predict the projectile's motion, including its range and height [5].</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 8 7 9</p>
<p>The answer appears to be based on a general understanding of physics principles related to projectile motion and the effects of gravity.While it does not directly reference the provided abstracts, the information given is consistent with basic physics concepts.The trustworthiness would be higher if the answer included direct references to relevant data or studies from the abstracts.</p>
<p>Mistral 10 10 10</p>
<p>The answer is trustworthy as it directly references content from the provided abstracts regarding projectiles and equations of motion.</p>
<p>Table 14: Ratings for the answer to the question "How does the acceleration due to gravity affect the trajectory and motion of a projectile, and how can this be accurately measured and predicted using equations of motion?"D.3.5 Psychology Question: What are the underlying factors that contribute to the development of personality disorders and how can these disorders be effectively treated?Answer: The development of personality disorders can be influenced by various factors, including genetic, environmental, and psychological components.While the specific causes for each individual's development of a personality disorder can vary, some contributing factors may include childhood trauma, family history of mental disorders, and negative early life experiences.Effective treatment for personality disorders typically involves a combination of psychotherapy, medication, and lifestyle changes.Therapy approaches such as cognitive-behavioral therapy, dialectical behavior therapy, and psychodynamic therapy can help individuals understand and manage their symptoms.In some cases, medications like antidepressants, mood stabilizers, and antipsychotics may be prescribed to help with specific symptoms.Additionally, self-help strategies, social support, and proper self-care can contribute to the overall well-being of individuals with personality disorders [mental health challengehttps://core.ac.uk/works/18689246].</p>
<p>Comp.</p>
<p>Trust Utility Trust Rationale GPT-4 5 5 6</p>
<p>The answer appears to be based on general knowledge about personality disorders and their treatment rather than the provided abstracts.While the information given is in line with common understanding in the field of mental health, the lack of direct references to the abstracts or specific studies reduces the trustworthiness of the answer.Mistral 8 7 9</p>
<p>The answer is based on accurate information, but the sources are not explicitly cited, which may raise some questions about its trustworthiness.</p>
<p>Table 15: Ratings for the answer to the question "What are the underlying factors that contribute to the development of personality disorders and how can these disorders be effectively treated?"</p>
<p>Figure 1 :
1
Figure 1: Example output from GPT-4 Turbo.</p>
<p>Figure 2 :
2
Figure 2: Example output from Mistral.The text under the separator line was returned outside of the JSON response.</p>
<p>Table 1 :
1
Spearman's ρ calculated for the combined mean of Comprehensive, Trust, and Utility scores.Statistically significant results are in bold.
A1A2GPT-4 MistralA1ρ-0.7100.2480.015p-value-0.0010.3050.951A2ρ0.710-0.058-0.038p-value 0.001-0.8140.878GPT-4ρ0.248 0.058-0.786p-value 0.305 0.814-0.000Mistralρ0.015 -0.038 0.786-p-value 0.951 0.8780.000-</p>
<p>Table 4 :
4
Ratings for the answer to the question "What is the impact of minimum wage laws on employment rates and income inequality in the United States?"
D.1.4 MathematicsQuestion: What is the relationship between ad-vanced mathematics coursework and problem-solving skills among undergraduate students inSTEM majors?Answer:</p>
<p>Table 5 :
5
Ratings for the answer to the question "What is
the relationship between advanced mathematics course-work and problem-solving skills among undergraduatestudents in STEM majors?"</p>
<p>Table 11
11: Ratings for the answer to the question "Whatgeological factors contribute to the formation and sus-tainability of hydrothermal ecosystems in deep sea envi-ronments?"
https://github.com/oacore/core-gpt-evaluation
AcknowledgementsThis work was supported by the German BMBF project SCINEXT (ID 01lS22070).A PromptA.1 Main Evaluation PromptEvaluate the quality of the following question and answer pair.The answer should succinctly address the question while referencing only content from the provided abstracts.Consider the following characteristics while assessing the answer.For each characteristic rate the quality from 0 (poor) to 10 (excellent).Provide a short rationale for each rating.1. Comprehensive: How comprehensively is the question answered?2.Trust: How trustworthy is the answer?3. Utility: How useful is the answer?Return your response in JSON format: {Comprehensive :{'rating' : '', 'rationale' : ''}, Trust : {'rating' : '', 'rationale' : ''}, Utility : {'rating' : '', 'rationale' : ''}} Question: <content> Answer: <content> Abstracts: <content>A.2 Additional Mistral System PromptFollow the user instructions to the letter.The output should be only what the user requests, do not respond with anything else.Provide the answer in JSON with no additional text.
Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE?. Mousumi Akter, Naman Bansal, Shubhra Kanti, Karmaker , 10.18653/v1/2022.findings-acl.122Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Benchmarking Foundation Models with Language-Model-as-an-Examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. the Association for Computational Linguistics. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliAssociation for Computational Linguistics20231Long Papers</p>
<p>A Closer Look into Using Large Language Models for Automatic Evaluation. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.findings-emnlp.599Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023a</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b</p>
<p>Revisiting Summarization Evaluation for Scientific Articles. Arman Cohan, Nazli Goharian, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)Portorož, SloveniaEuropean Language Resources Association (ELRA2016</p>
<p>SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization. Yang Gao, Wei Zhao, Steffen Eger, 10.18653/v1/2020.acl-main.124Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE. Yvette Graham, 10.18653/v1/D15-1013Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech. Fan Huang, Haewoon Kwak, Jisun An, 10.1145/3543873.3587368Companion Proceedings of the ACM Web Conference 2023, WWW '23 Companion. New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, 10.48550/arXiv.2310.06825arXiv:2310.06825Mistral 7B. Preprint. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Neural Text Summarization: A Critical Evaluation. Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-Cann, Caiming Xiong, Richard Socher, 10.18653/v1/D19-1051Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>The challenging task of summary evaluation: an overview. Language Resources and Evaluation. Elena Lloret, Laura Plaza, Ahmet Aker, 201852</p>
<p>Multi-Document Summarization via Deep Learning Techniques: A Survey. Congbo Ma, Wei Emma Zhang, Mingyu Guo, Wang Hu, Quan Z Sheng, 10.1145/3529754ACM Computing Surveys. 5552022</p>
<p>The Pyramid Method: Incorporating human content selection variation in summarization evaluation. Ani Nenkova, Rebecca Passonneau, Kathleen Mckeown, 10.1145/1233912.1233913ACM Transactions on Speech and Language Processing. 422007</p>
<p>. 10.48550/arXiv.2303.08774arXiv:2303.08774Technical Report. Preprint</p>
<p>BLEU: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Automatic Evaluation of Linguistic Quality in Multi-Document Summarization. Emily Pitler, Annie Louis, Ani Nenkova, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. the 48th Annual Meeting of the Association for Computational LinguisticsUppsala, SwedenAssociation for Computational Linguistics2010</p>
<p>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering. David Pride, Matteo Cancellieri, Petr Knoth, 10.1007/978-3-031-43849-3_13Linking Theory and Practice of Digital Libraries. Nature SwitzerlandSpringer2023</p>
<p>Is ChatGPT a Good NLG Evaluator? A Preliminary Study. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, 10.18653/v1/2023.newsum-1.1Proceedings of the 4th New Frontiers in Summarization Workshop. the 4th New Frontiers in Summarization WorkshopSingapore2023Association for Computational Linguistics</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Proceedings of the 8th International Conference on Learning Representations (ICLR'20). the 8th International Conference on Learning Representations (ICLR'20)2020</p>
<p>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, 10.18653/v1/D19-1053Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
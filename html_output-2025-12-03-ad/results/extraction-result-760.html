<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-760 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-760</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-760</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-c52dddfbb4a3af4cc5e72849fe965c62801539e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c52dddfbb4a3af4cc5e72849fe965c62801539e7" target="_blank">Counting to Explore and Generalize in Text-based Games</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments and observes that the agent learns policies that generalize to unseen games of greater difficulty.</p>
                <p><strong>Paper Abstract:</strong> We propose a recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments. We show promising results on a set of generated text-based games of varying difficulty where the goal is to collect a coin located at the end of a chain of rooms. In contrast to previous text-based RL approaches, we observe that our agent learns policies that generalize to unseen games of greater difficulty.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e760.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e760.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DQN++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-DQN with Episodic Discovery Bonus (DQN++)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep Q-learning agent that uses an LSTM-based text encoder for observations and a non-recurrent action scorer, augmented with an episodic count-based intrinsic reward to encourage within-episode discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN++ (LSTM-DQN with episodic discovery bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Representation generator Phi_R: word embeddings -> encoder LSTM (100 hidden units) -> mean-pooling to produce an observation vector. Action scorer Phi_A: originally a 1-layer MLP (64 hidden units, ReLU) predicting Q-values for verbs and objects independently; the model concatenates the previous command to the current observation to partially mitigate partial observability. DQN++ augments this architecture with an episodic count-based discovery bonus (r^{++}) that gives a fixed positive intrinsic reward the first time an observation is seen within an episode (counts reset each episode). The agent is trained with standard DQN-style Q-learning (replay buffer, prioritized sampling, epsilon-greedy), using Adam optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Coin Collector (generated TextWorld chain games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A family of deterministic, text-based games generated with TextWorld where the agent perceives textual observations describing a room and must navigate a chain of rooms to collect a coin. Observations are partial textual descriptions (POMDP): the true environment state contains full internal information but the agent only receives language describing a subset of it. Challenges include a huge combinatorial text action space (simplified here to two-word commands), partial observability, distractor dead-end rooms, and long-horizon navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>No explicit external belief-state module; partial observability is handled primarily by (1) concatenating the previous command to the current observation and (2) the LSTM encoder's internal state for forming representations; and (3) episodic count memory n(o_t) which tracks whether an observation has been seen in the current episode (counts reset per episode).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep, the encoder LSTM produces a vector representation from the current observation (and concatenated previous command). The episodic counter n(o_t) is incremented when an observation is seen; this counter is external to the network and reset at episode start. The agent's policy (Q-values) is updated by Q-learning; there is no separate explicit Bayesian belief update or external tool-output integration.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy via model-free deep Q-learning with recurrence in the observation encoder and intrinsic (episodic) count-based exploration to encourage state discovery; not model-based planning or explicit search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation emerges from the learned Q-policy; agents learn heuristics (e.g., anti-clockwise exit exploration or wall-following) rather than performing explicit graph search or shortest-path computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Episodic discovery bonus combined with the LSTM-based representation helps the agent explore within an episode and learn generalizable navigation strategies; no external tools or explicit planners are used — navigation is a learned policy that can reduce revisits and generalize (e.g., anti-clockwise exploration) on acyclic maps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counting to Explore and Generalize in Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e760.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e760.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRQN++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-DRQN with Episodic Discovery Bonus (DRQN++)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent Deep Recurrent Q-Network variant that feeds the observation representation into an LSTM action-scoring cell (64 hidden units) and uses an episodic count-based intrinsic reward to prioritize visiting novel observations within each episode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRQN++ (LSTM-DRQN with episodic discovery bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Representation generator Phi_R: word embeddings -> encoder LSTM (100 hidden units) -> mean-pooling. Action scorer Phi_A: an LSTM cell (hidden size 64) that consumes the observation representation plus previous history h_{t-1} and outputs a current-step state embedding which is passed to two MLPs to estimate Q-values for verbs and objects. The agent is augmented with an episodic count-based bonus r^{++} that awards a positive intrinsic reward the first time a given observation is seen in the current episode (counts are reset at episode start). Training uses sequences sampled from a replay buffer with sequence bootstrapping for the recurrent state.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Coin Collector (generated TextWorld chain games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same TextWorld chain navigation tasks: deterministic room graphs with textual observations that do not fully reveal the underlying environment state (POMDP). Difficulty is controlled by chain length and number of distractor rooms (dead ends); agents must handle long-term memory to avoid revisits and return from distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Internal recurrent hidden state (LSTM cell in Phi_A) that accumulates history (h_t) across timesteps, plus episodic observation counts n(o_t) used as an intrinsic-memory signal; previous command concatenation to inputs is also used. There is no separate symbolic or graph-based belief state.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The LSTM action scorer updates its hidden state each step by consuming the current observation representation and the previous hidden state, thereby implicitly maintaining a history-conditioned belief representation. Episodic counts are updated by incrementing n(o_t) when an observation appears; these counts are stored externally and reset at episode start. There is no explicit mechanism that merges outputs from external tools into this hidden belief.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free learned policy via deep recurrent Q-learning (DRQN) with intrinsic episodic exploration bonuses; planning is implicit in the learned policy and memory rather than explicit model-based planning or search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Policy-driven navigation learned end-to-end; agents often learn heuristic strategies (e.g., anti-clockwise exit exploration or wall-following) rather than computing shortest paths or running graph search algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recurrence (DRQN) combined with episodic count bonuses significantly improves exploration and generalization to unseen games with distractors; the episodic bonus encourages within-episode discovery and teaching the agent to use memory, but no external mapping or planning tools are employed — navigation strategies are acquired by the learned recurrent policy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counting to Explore and Generalize in Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep recurrent q-learning for partially observable mdps <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li># exploration: A study of count-based exploration for deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Count-based exploration with neural density models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-760",
    "paper_id": "paper-c52dddfbb4a3af4cc5e72849fe965c62801539e7",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "DQN++",
            "name_full": "LSTM-DQN with Episodic Discovery Bonus (DQN++)",
            "brief_description": "A deep Q-learning agent that uses an LSTM-based text encoder for observations and a non-recurrent action scorer, augmented with an episodic count-based intrinsic reward to encourage within-episode discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DQN++ (LSTM-DQN with episodic discovery bonus)",
            "agent_description": "Representation generator Phi_R: word embeddings -&gt; encoder LSTM (100 hidden units) -&gt; mean-pooling to produce an observation vector. Action scorer Phi_A: originally a 1-layer MLP (64 hidden units, ReLU) predicting Q-values for verbs and objects independently; the model concatenates the previous command to the current observation to partially mitigate partial observability. DQN++ augments this architecture with an episodic count-based discovery bonus (r^{++}) that gives a fixed positive intrinsic reward the first time an observation is seen within an episode (counts reset each episode). The agent is trained with standard DQN-style Q-learning (replay buffer, prioritized sampling, epsilon-greedy), using Adam optimizer.",
            "environment_name": "TextWorld Coin Collector (generated TextWorld chain games)",
            "environment_description": "A family of deterministic, text-based games generated with TextWorld where the agent perceives textual observations describing a room and must navigate a chain of rooms to collect a coin. Observations are partial textual descriptions (POMDP): the true environment state contains full internal information but the agent only receives language describing a subset of it. Challenges include a huge combinatorial text action space (simplified here to two-word commands), partial observability, distractor dead-end rooms, and long-horizon navigation tasks.",
            "is_partially_observable": true,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": "No explicit external belief-state module; partial observability is handled primarily by (1) concatenating the previous command to the current observation and (2) the LSTM encoder's internal state for forming representations; and (3) episodic count memory n(o_t) which tracks whether an observation has been seen in the current episode (counts reset per episode).",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "At each timestep, the encoder LSTM produces a vector representation from the current observation (and concatenated previous command). The episodic counter n(o_t) is incremented when an observation is seen; this counter is external to the network and reset at episode start. The agent's policy (Q-values) is updated by Q-learning; there is no separate explicit Bayesian belief update or external tool-output integration.",
            "planning_approach": "Learned policy via model-free deep Q-learning with recurrence in the observation encoder and intrinsic (episodic) count-based exploration to encourage state discovery; not model-based planning or explicit search.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation emerges from the learned Q-policy; agents learn heuristics (e.g., anti-clockwise exit exploration or wall-following) rather than performing explicit graph search or shortest-path computation.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Episodic discovery bonus combined with the LSTM-based representation helps the agent explore within an episode and learn generalizable navigation strategies; no external tools or explicit planners are used — navigation is a learned policy that can reduce revisits and generalize (e.g., anti-clockwise exploration) on acyclic maps.",
            "uuid": "e760.0",
            "source_info": {
                "paper_title": "Counting to Explore and Generalize in Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "DRQN++",
            "name_full": "LSTM-DRQN with Episodic Discovery Bonus (DRQN++)",
            "brief_description": "A recurrent Deep Recurrent Q-Network variant that feeds the observation representation into an LSTM action-scoring cell (64 hidden units) and uses an episodic count-based intrinsic reward to prioritize visiting novel observations within each episode.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DRQN++ (LSTM-DRQN with episodic discovery bonus)",
            "agent_description": "Representation generator Phi_R: word embeddings -&gt; encoder LSTM (100 hidden units) -&gt; mean-pooling. Action scorer Phi_A: an LSTM cell (hidden size 64) that consumes the observation representation plus previous history h_{t-1} and outputs a current-step state embedding which is passed to two MLPs to estimate Q-values for verbs and objects. The agent is augmented with an episodic count-based bonus r^{++} that awards a positive intrinsic reward the first time a given observation is seen in the current episode (counts are reset at episode start). Training uses sequences sampled from a replay buffer with sequence bootstrapping for the recurrent state.",
            "environment_name": "TextWorld Coin Collector (generated TextWorld chain games)",
            "environment_description": "Same TextWorld chain navigation tasks: deterministic room graphs with textual observations that do not fully reveal the underlying environment state (POMDP). Difficulty is controlled by chain length and number of distractor rooms (dead ends); agents must handle long-term memory to avoid revisits and return from distractors.",
            "is_partially_observable": true,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": "Internal recurrent hidden state (LSTM cell in Phi_A) that accumulates history (h_t) across timesteps, plus episodic observation counts n(o_t) used as an intrinsic-memory signal; previous command concatenation to inputs is also used. There is no separate symbolic or graph-based belief state.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "The LSTM action scorer updates its hidden state each step by consuming the current observation representation and the previous hidden state, thereby implicitly maintaining a history-conditioned belief representation. Episodic counts are updated by incrementing n(o_t) when an observation appears; these counts are stored externally and reset at episode start. There is no explicit mechanism that merges outputs from external tools into this hidden belief.",
            "planning_approach": "Model-free learned policy via deep recurrent Q-learning (DRQN) with intrinsic episodic exploration bonuses; planning is implicit in the learned policy and memory rather than explicit model-based planning or search.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Policy-driven navigation learned end-to-end; agents often learn heuristic strategies (e.g., anti-clockwise exit exploration or wall-following) rather than computing shortest paths or running graph search algorithms.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Recurrence (DRQN) combined with episodic count bonuses significantly improves exploration and generalization to unseen games with distractors; the episodic bonus encourages within-episode discovery and teaching the agent to use memory, but no external mapping or planning tools are employed — navigation strategies are acquired by the learned recurrent policy.",
            "uuid": "e760.1",
            "source_info": {
                "paper_title": "Counting to Explore and Generalize in Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "language_understanding_for_textbased_games_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Deep recurrent q-learning for partially observable mdps",
            "rating": 2,
            "sanitized_title": "deep_recurrent_qlearning_for_partially_observable_mdps"
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "# exploration: A study of count-based exploration for deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "exploration_a_study_of_countbased_exploration_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Count-based exploration with neural density models",
            "rating": 1,
            "sanitized_title": "countbased_exploration_with_neural_density_models"
        }
    ],
    "cost": 0.008630249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Counting to Explore and Generalize in Text-based Games</h1>
<p>Xingdi Yuan ${ }^{<em> 1}$ Marc-Alexandre Côté ${ }^{</em> 1}$ Alessandro Sordoni ${ }^{1}$ Romain Laroche ${ }^{1}$ Remi Tachet des Combes ${ }^{1}$<br>Matthew Hausknecht ${ }^{1}$ Adam Trischler ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We propose a recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments. We show promising results on a set of generated text-based games of varying difficulty where the goal is to collect a coin located at the end of a chain of rooms. In contrast to previous text-based RL approaches, we observe that our agent learns policies that generalize to unseen games of greater difficulty.</p>
<h2>1. Introduction</h2>
<p>Text-based games like Zork (Infocom, 1980) are complex, interactive simulations. They use natural language to describe the state of the world, to accept actions from the player, and to report subsequent changes in the environment. The player works toward goals which are seldom specified explicitly and must be discovered through exploration. The observation and action spaces in text games are both combinatorial and compositional, and players must contend with partial observability, since descriptive text does not communicate complete, unambiguous information about the underlying game state.</p>
<p>In this paper, we study several methods of exploration in text-based games. Our basic task is a deterministic textbased version of the chain experiment (Osband et al., 2016; Plappert et al., 2017) with distractor nodes that are off-chain: the agent must navigate a path composed of discrete locations (rooms) to the goal, ideally without revisiting dead ends. We propose a DQN-based recurrent model for solving text-based games, where the recurrence gives the model the capacity to condition its policy on historical state information. To encourage exploration, we extend count-based exploration approaches (Ostrovski et al., 2017; Tang et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2017), which assign an intrinsic reward derived from the count of state visitations during learning, across episodes. Specifically, we propose an episodic count-based exploration scheme, where state counts are reset at the beginning of each episode. This reward plays the role of an episodic memory (Gershman \&amp; Daw, 2017) that pushes the agent to visit states not previously encountered within an episode. Although the recurrent policy architecture has the capacity to solve the task by remembering and avoiding previously visited locations, we hypothesize that exploration rewards will help the agent learn to utilize its memory.</p>
<p>We generate a set of games of varying difficulty (measured with respect to the path length and the number of off-chain rooms) with a text-based game generator (Côté et al., 2018). We observe that, in contrast to a baseline model and standard count-based exploration methods, the recurrent model with episodic bonus learns policies that not only complete multiple training games at same time successfully but also generalize to unseen games of greater difficulty.</p>
<h2>2. Text-based Games as POMDPs</h2>
<p>Text-based games are sequential decision-making problems that can be described naturally by the Reinforcement Learning (RL) setting. Fundamentally, text-based games are partially observable Markov decision processes (POMDP) (Kaelbling et al., 1998) where the environment state is never observed directly. To act optimally, an agent must keep track of all observations. Formally, a text-based game is a discrete-time POMDP defined by $(S, T, A, \Omega, O, R, \gamma)$, where $\gamma \in[0,1]$ is the discount factor.</p>
<p>Environment States $(S)$ : The environment state at turn $t$ in the game is $s_{t} \in S$. It contains the complete internal information of the game, much of which is hidden from the agent. When an agent issues a command $\boldsymbol{c}<em t_1="t+1">{t}$ (defined next), the environment transitions to state $s</em>}$ with probability $T\left(s_{t+1} \mid s_{t}, \boldsymbol{c<em t="t">{t}\right)$.
Actions $(A)$ : At each turn $t$, the agent issues a text command $\boldsymbol{c}</em>$. The interpreter can accept any sequence of characters but will only recognize a tiny subset thereof. Furthermore, only a fraction of recognized commands will actually change the state of the world. The resulting action space</p>
<p>is enormous and intractable for existing RL algorithms. In this work, we make the following two simplifying assumptions. (1) Word-level Each command is a two-word sequence where the words are taken from a fixed vocabulary $V$. (2) Command syntax Each command is a (verb, object) pair (direction words are considered objects).</p>
<p>Observations ( $\Omega$ ): The text information perceived by the agent at a given turn $t$ in the game is the agent's observation, $o_{t} \in \Omega$, which depends on the environment state and the previous command with probability $O\left(o_{t} \mid s_{t}, \boldsymbol{c}_{t-1}\right)$. Thus, the function $O$ selects from the environment state what information to show to the agent given the last command.</p>
<p>Reward Function $(R)$ : Based on its actions, the agent receives reward signals $r_{t}=R\left(s_{t}, a_{t}\right)$. The goal is to maximize the expected discounted sum of rewards $E\left[\sum_{t} \gamma^{t} r_{t}\right]$.</p>
<h2>3. Method</h2>
<h3>3.1. Model Architecture</h3>
<p>In this work, we adopt the LSTM-DQN (Narasimhan et al., 2015) model as baseline. It has two modules: a representation generator $\Phi_{R}$, and an action scorer $\Phi_{A} . \Phi_{R}$ takes observation strings $o$ as input, after a stacked embedding layer and LSTM (Hochreiter \&amp; Schmidhuber, 1997) encoder, a mean-pooling layer produces a vector representation of the observation. This feeds into $\Phi_{A}$, in which two MLPs, sharing a lower layer, predict the Q-values over all verbs $w_{v}$ and object words $w_{o}$ independently. The average of the two resulting scores gives the Q-values for the composed actions. The LSTM-DQN does not condition on previous actions or observations, so it cannot deal with partial observability. We concatenate the previous command $\boldsymbol{c}<em t="t">{t-1}$ to the current observation $o</em>$ to lessen this limitation.</p>
<p>To further enhance the agent's capacity to remember previous states, we replace the shared MLP in $\Phi_{A}$ by an LSTM cell. This model is inspired by (Hausknecht \&amp; Stone, 2015; Lample \&amp; Chaplot, 2016) and we call it LSTM-DRQN. The LSTM cell in $\Phi_{A}$ takes the representation generated by $\Phi_{R}$ together with history information $h_{t-1}$ from the previous game step as input. It generates the state information at the current game step, which is then fed into the two MLPs as well as passed forward to next game step. Figure 1 shows the LSTM-DRQN architecture.</p>
<h3>3.2. Discovery Bonus</h3>
<p>To promote exploration we use an intrinsic reward by counting state visits (Kolter \&amp; Ng, 2009; Tang et al., 2017; Martin et al., 2017; Ostrovski et al., 2017). We investigate two approaches to counting rewards. The first is inspired by (Kolter $\&amp; \mathrm{Ng}, 2009$ ), where we define the cumulative counting bonus as $r^{+}\left(o_{t}\right)=\beta \cdot n\left(o_{t}\right)^{-1 / 3}$, where $n\left(o_{t}\right)$ is the num-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. LSTM-DRQN processes textual observations word-byword to generate a fixed-length vector representation. This representation is used by the recurrent policy to estimate Q-values for all verbs $Q(s, v)$ and objects $Q(s, o)$.
ber of times the agent has observed $o_{t}$ since the beginning of training (across episodes), and $\beta$ is the bonus coefficient. During training, as the agent observes new states more and more, the cumulative counting bonus gradually converges to 0 .</p>
<p>The second approach is the episodic discovery bonus, which encourages the agent to discover unseen states by assigning a positive reward whenever it sees a new state. It is defined as: $r^{++}\left(o_{t}\right)=\left{\begin{array}{ll}\beta &amp; \text { if } n\left(o_{t}\right)=1 \ 0.0 &amp; \text { otherwise }\end{array}\right.$, where $n(\cdot)$ is reset to zero at the beginning of each episode. Taking inspiration from (Gershman \&amp; Daw, 2017), we hope this behavior pushes the agent to visit states not previously encountered in the current episode and teaches the agent how to use its memory for this purpose so it may generalize to unseen environments.</p>
<h2>4. Related Work</h2>
<p>RL Applied to Text-based Games: Narasimhan et al. (2015) test their LSTM-DQN in two text-based environments: Home World and Fantasy World. They report the quest completion ratio over multiple runs but not how many steps it takes to complete them. He et al. (2015) introduce the Deep Reinforcement Relevance Network (DRRN) for tackling choice-based (as opposed to parser-based) text games, evaluating the DRRN on one deterministic game and one larger-scale stochastic game. The DRRN model converges on both games; however, this model must know in advance the valid commands at each state. Fulda et al. (2017) propose a method to reduce the action space for parserbased games by training word embeddings to be aware of verb-noun affordances. One drawback of this approach is it requires pre-trained embeddings.</p>
<p>Count-based Exploration: The Model Based Interval Estimation-Exploration Bonus (MBIE-EB) (Strehl \&amp; Littman, 2008) derives an intrinsic reward by counting stateaction pairs with a table $n(s, a)$. Their exploration bonus has the form $\beta / \sqrt{n(s, a)}$ to encourage exploring less-visited pairs. In this work, we use $n(s)$ rather than $n(s, a)$, since the majority of actions leave the agent in the same state</p>
<p>(i.e., unrecognized commands). Using the latter would reward the agent for trying invalid commands, which is not sensible in our setting.</p>
<p>Tang et al. (2017) propose a hashing function for countbased exploration in order to discretize high-dimensional, continuous state spaces. Their exploration bonus $r^{+}=$ $\beta / \sqrt{n(\phi(s))}$, where $\phi(\cdot)$ is a hashing function that can either be static or learned. This is similar to the cumulative counting bonus defined above.</p>
<p>Deep Recurrent Q-Learning: Hausknecht \&amp; Stone (2015) propose the Deep Recurrent Q-Networks (DRQN), adding a recurrent neural network (such as an LSTM (Hochreiter \&amp; Schmidhuber, 1997)) on top of the standard DQN model. DRQN estimates $Q\left(o_{t}, h_{t-1}, a_{t}\right)$ instead of $Q\left(o_{t}, a_{t}\right)$, so it has the capacity to memorize the state history. Lample \&amp; Chaplot (2016) use a model built on the DRQN architecture to learn to play FPS games.</p>
<p>A major difference between the work presented in this paper and the related work is that we test on unseen games and train on a set of similar (but not identical) games rather than training and testing on the same game.</p>
<h2>5. Experiments</h2>
<h3>5.1. Coin Collector Game Setup</h3>
<p>To evaluate the two models described above and the proposed discovery bonus, we designed a set of simple textbased games inspired by the chain experiment (Osband et al., 2016; Plappert et al., 2017). Each game contains a given number of rooms that are randomly connected to each other to form a chain (see figures in Appendix C). The goal is to find and collect a "coin" placed in one of the rooms. The player's initial position is at one end of the chain and the coin is at the other. These games have deterministic state transitions.</p>
<p>Games stop after a set number of steps or after the player has collected the coin. The game interpreter understands only five commands (go north, go east, go south, go west and take coin), while the action space is twice as large: ${\mathrm{go}$, take $} \times{$ north, south, east, west, coin $}$. See Figure 12, Appendix C for an example of what the agent observes in-game.</p>
<p>Our games have 3 modes: easy (mode 0 ), there are no distractor rooms (dead ends) along the path; medium (mode 1), each room along the optimal trajectory has one distractor room randomly connected to it; hard (mode 2), each room on the path has two distractor rooms, i.e., within a room on the optimal trajectory, all 4 directions lead to a connected room. We use difficulty levels to indicate the optimal trajectory's length of a game.</p>
<p>To solve easy games, the agent must learn to recall its previous directional action and to issue the command that does not reverse it (e.g., if the agent entered the current room by going east, do not now go west). Conversely, to solve medium and hard games, the agent must reverse its previous action when it enters distractor rooms to return to the chain, and also recall farther into the past to track which exits it has already passed through. Alternatively, since there are no cycles, it can learn a less memory intensive "wall-following" strategy by, e.g., taking exits in a clockwise order from where it enters a room.</p>
<p>We refer to models with the cumulative counting bonus as MODEL+, and models with episodic discovery bonus as MODEL++, where MODEL $\in{\mathrm{DQN}, \mathrm{DRQN}}^{1}$ (implementation details in Appendix A). In this section we cover part of the experiment results, the full extent of our experiment results are provided in Appendix B.</p>
<h3>5.2. Solving Training Games</h3>
<p>We first investigate whether the variant models can learn to solve single games with different difficulty modes (easy, medium, hard) and levels ${L 5, L 10, L 15, L 20, L 25, L 30}^{2}$. As shown in Figure 2 (top row), when the games are simple, vanilla DQN and DRQN already fail to learn. Adding the cumulative bonus helps somewhat and models perform similarly with and without recurrence. When the games become harder, the cumulative bonus helps less, while episodic bonus remains very helpful and recurrence in the model becomes very helpful.</p>
<p>Next, we are interested to see whether models can learn to solve a distribution of games. Note that each game has its own counting memory, i.e., the states visited in one game do not affect the counters for other games. Here, we fix the game difficulty level to 10 , and randomly generate training sets that contain ${2,5,10,30,50,100}$ games in each mode. As shown in Figure 2 (bottom row), when the game mode becomes harder, the episodic bonus has an advantage over the cumulative bonus, and recurrence becomes more crucial for memorizing the game distribution. It is also clear that the episodic bonus and recurrence help significantly when more training games are provided.</p>
<h3>5.3. Zero-shot Evaluation</h3>
<p>Finally, we want to see if a pre-trained model can generalize to unseen games. The generated training set contains ${1,2,5,10,30,50,100,500} \mathrm{L} 10$ games for each mode. Then, for each corresponding mode the test set contains 10 unseen ${L 5, L 10, L 15, L 20, L 30}$ games. There is no</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Model performance on single games (top row) and multiple games (bottom row).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Zero-shot evaluation: Average rewards of DQN++ (left) and DRQN++ (right) as a function of the number of games in the training set.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Average rewards and steps used corresponding to best validation performance in hard games.
overlap between training and test games in either text descriptions or optimal trajectories. At test time, the counting modules are disabled, the agent is not updated, and its generates verb and noun actions based on the $\operatorname{argmax}$ of their Q-values.</p>
<p>As shown in Figure 3, when the game mode is easy, both models with and without recurrence can generalize well on unseen games by training on a large training set. It is worth noting that by training on 500 L10 easy games, both models can almost perfectly solve level 30 unseen easy games. We also observe that models with recurrence are able to generalize better when trained on fewer games.</p>
<p>When testing on hard mode games, we observe that both models suffer from overfitting (after a certain number of episodes, average test reward starts to decrease while training reward increases). Therefore, we further generated a validation set that contains 10 L10 hard games, and report test results corresponding to best validation performance. In
addition, we investigated what happens when concatenating the previous 4 steps' history observation into the input. In Figure 4, we add $H$ to model names to indicate this variant.</p>
<p>As shown in Figure 4, all models can memorize the 500 training games, while DQN++ and DRQN++H are able to generalize better on unseen games. In particular, the former performs near perfectly on test games. To investigate this, we looked into all the bi-grams of generated commands (i.e., two commands from adjacent game steps) from DQN++ model. Surprisingly, except for moving back from dead end rooms, the agent always explores exits in anti-clockwise order. This means the agent has learned a general strategy that does not require history information beyond the previous command. This strategy generalizes perfectly to all possible hard games because there are no cycles in the maps.</p>
<h2>6. Final Remarks</h2>
<p>We propose an RL model with a recurrent component, together with an episodic count-based exploration scheme that promotes the agent's discovery of the game environment. We show promising results on a set of generated text-based games of varying difficulty. In contrast to baselines, our approach learns policies that generalize to unseen games of greater difficulty.</p>
<p>In future work, we plan to experiment on games with more complex topology, such as cycles (where the "wallfollowing" strategy will not work). We would like to explore games that require multi-word commands (e.g., unlock red door with red key), necessitating a model that generates sequences of words. Other interesting directions include agents that learn to map or to deal with stochastic transitions in text-based games.</p>
<h2>References</h2>
<p>Côté, Marc-Alexandre, Kádár, Ákos, Yuan, Xingdi, Kybartas, Ben, Barnes, Tavian, Fine, Emery, Moore, James, Hausknecht, Matthew, Asri, Layla El, Adada, Mahmoud, Tay, Wendy, and Trischler, Adam. Textworld: A learning environment for text-based games. Computer Games Workshop at IJCAI 2018, Stockholm, 2018.</p>
<p>Fulda, Nancy, Ricks, Daniel, Murdoch, Ben, and Wingate, David. What can you do with a rock? affordance extraction via word embeddings. arXiv preprint arXiv:1703.03429, 2017.</p>
<p>Gershman, Samuel J and Daw, Nathaniel D. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annual review of psychology, 68:101-128, 2017.</p>
<p>Hausknecht, Matthew J. and Stone, Peter. Deep recurrent q-learning for partially observable mdps. CoRR, abs/1507.06527, 2015. URL http://arxiv.org/ abs/1507.06527.</p>
<p>He, Ji, Chen, Jianshu, He, Xiaodong, Gao, Jianfeng, Li, Lihong, Deng, Li, and Ostendorf, Mari. Deep reinforcement learning with a natural language action space. arXiv preprint arXiv:1511.04636, 2015.</p>
<p>Hochreiter, Sepp and Schmidhuber, Jürgen. Long shortterm memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997. 9.8.1735. URL http://dx.doi.org/10.1162/ neco.1997.9.8.1735.</p>
<p>Infocom. Zork I, 1980. URL http://ifdb.tads. org/viewgame?id=0dbnusxunq7fw5ro.</p>
<p>Kaelbling, Leslie Pack, Littman, Michael L, and Cassandra, Anthony R. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99134, 1998.</p>
<p>Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Kolter, J Zico and Ng, Andrew Y. Near-bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 513520. ACM, 2009.</p>
<p>Lample, Guillaume and Chaplot, Devendra Singh. Playing FPS games with deep reinforcement learning. CoRR, abs/1609.05521, 2016. URL http://arxiv.org/ abs/1609.05521.</p>
<p>Martin, Jarryd, Sasikumar, Suraj Narayanan, Everitt, Tom, and Hutter, Marcus. Count-based exploration in feature space for reinforcement learning. arXiv preprint arXiv:1706.08090, 2017.</p>
<p>Narasimhan, Karthik, Kulkarni, Tejas, and Barzilay, Regina. Language understanding for text-based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.</p>
<p>Osband, Ian, Blundell, Charles, Pritzel, Alexander, and Van Roy, Benjamin. Deep exploration via bootstrapped dqn. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 4026-4034. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/ 6501-deep-exploration-via-bootstrapped-dqn. pdf.</p>
<p>Ostrovski, Georg, Bellemare, Marc G, Oord, Aaron van den, and Munos, Rémi. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.</p>
<p>Paszke, Adam, Gross, Sam, Chintala, Soumith, Chanan, Gregory, Yang, Edward, DeVito, Zachary, Lin, Zeming, Desmaison, Alban, Antiga, Luca, and Lerer, Adam. Automatic differentiation in pytorch. In NIPS-W, 2017.</p>
<p>Plappert, Matthias, Houthooft, Rein, Dhariwal, Prafulla, Sidor, Szymon, Chen, Richard Y, Chen, Xi, Asfour, Tamim, Abbeel, Pieter, and Andrychowicz, Marcin. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.</p>
<p>Strehl, Alexander L and Littman, Michael L. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74 (8):1309-1331, 2008.</p>
<p>Tang, Haoran, Houthooft, Rein, Foote, Davis, Stooke, Adam, Chen, Xi, Duan, Yan, Schulman, John, DeTurck, Filip, and Abbeel, Pieter. # exploration: A study of countbased exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2750-2759, 2017.</p>
<h1>A. Implementation Details</h1>
<p>Implementation details of our neural baseline agent are as follows ${ }^{3}$. In all experiments, the word embeddings are initialized with 20-dimensional random matrices; the number of hidden units of the encoder LSTM is 100. In the nonrecurrent action scorer we use a 1-layer MLP which has 64 hidden units, with $\operatorname{ReLU}$ as non-linear activation function, in the recurrent action scorer, we use an LSTM cell which hidden size is 64 .</p>
<p>In replay memory, we used a memory with capacity of 500000 , a mini-batch gradient update is performed every 4 steps in the gameplay, the mini-batch size is 32 . We apply prioritized sampling in all experiments, in which, we used $\rho=0.25$. In LSTM-DQN and LSTM-DRQN model, we used discount factor $\gamma=0.9$, in all models with discovery bonus, we used $\gamma=0.5$.</p>
<p>When updating models with recurrent components, we follow the update strategy in (Lample \&amp; Chaplot, 2016), i.e., we randomly sample sequences of length 8 from the replay memory, zero initialize hidden state and cell state, use the first 4 states to bootstrap a reliable hidden state and cell state, and then update on rest of the sequence.</p>
<p>We anneal the $\epsilon$ for $\epsilon$-greedy from 1 to 0.2 over 1000 epochs, it remains at 0.2 afterwards. In both cumulative and episodic discovery bonus, we use coefficient $\beta$ of 1.0.</p>
<p>When zero-shot evaluating hard games, we use max_train_step $=100$, in all other experiments we use max_train_step $=50$; during test, we always use max_test_step $=200$.</p>
<p>We use adam (Kingma \&amp; Ba, 2014) as the step rule for optimization. The learning rate is $1 e^{-3}$. The model is implemented using PyTorch (Paszke et al., 2017).</p>
<p>All games are generated using TextWorld framework (Côté et al., 2018), we used the house grammar.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>B. More Results</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Model performance on single games.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Model performance on multiple games.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Model performance on unseen easy test games when pre-trained on easy games.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Model performance on unseen medium test games when pre-trained on medium games.</p>
<h1>C. Text-based Chain Experiment</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Examples of the games used in the experiments: level 10, easy
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Examples of the games used in the experiments: level 10, medium</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Examples of the games used in the experiments: level 10, hard</p>
<p>I hope your ready to go into rooms and interact with objects, because you've just entered TextWorld! take the coin that's in the launderette.
-= Cookhouse =-
Fancy seeing you here. Here, by the way, being the cookhouse.</p>
<p>There is an unguarded exit to the south.
$&gt;$ go south
-= Study =-
You've entered a study. The room seems oddly familiar, as though it were only superficially different from the other rooms in the building.</p>
<p>You need an unguarded exit? You should try going west. You need an unguarded exit? You should try going north.
$&gt;$</p>
<p>Figure 12. Text the agent gets to observe for one of the level 10 easy games.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Our implementation is publicly available at https://github.com/xingdi-eric-yuan/ TextWorld-Coin-Collector.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-209 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-209</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-209</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-a2a7033a5a859e3a6e6f0a83018326400b4c5faa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a2a7033a5a859e3a6e6f0a83018326400b4c5faa" target="_blank">Retrieval Augmentation Reduces Hallucination in Conversation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> The use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA - is explored for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses.</p>
                <p><strong>Paper Abstract:</strong> Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e209.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e209.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-grounding (BART)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation grounding with BART (DPR retriever, 5 docs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding retrieved Wikipedia passages (via DPR) to a BART dialogue model substantially reduces hallucination and increases knowledge usage compared to the same model with no retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>400m</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-grounded open-domain dialogue (Wizard of Wikipedia, CMU_DoG).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Retrieved documents / factual passages (top-k passages)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia passages indexed via KILT/FAISS (DPR retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>BART-Large (no retrieval) — WoW Valid Seen: PPL 14.8, F1 21.0, Knowledge F1 17.7; human eval (WoW test unseen): Hallucination 68.2%, Knowledgeable 34.1%, Consistent 81.8, Engaging 85.5 (Table 1, Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>RAG-Token (DPR, 5 docs) — WoW Valid Seen: PPL 11.6, F1 22.5, Knowledge F1 26.0 (Table 1/2); human eval (WoW test unseen): Hallucination 17.0%, Knowledgeable 93.2%, Consistent 85.3, Engaging 77.4 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — adding retrieved evidence lowered perplexity, increased F1 and Knowledge F1, and produced large reductions in human-annotated hallucination and large increases in knowledgeability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>External retrieved passages provide grounding information that the generator can condition on, reducing reliance on parametric memory (weights) and thus reducing invented facts (hallucinations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmentation (RAG with DPR) substantially reduces hallucination and raises knowledge usage: automated KF1 and human-judged knowledge increased, and human-annotated hallucination dropped from ~68% to ~17% for BART-Large when using 5 retrieved Wikipedia passages. Retrieval supplements missing parametric knowledge and improves generalization to unseen topics (smaller drops on unseen splits).</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval Augmentation Reduces Hallucination in Conversation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e209.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e209.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD-RAG (BART)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fusion-in-Decoder with RAG-trained DPR retriever (FiD-RAG) using BART</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a RAG-trained DPR retriever with the FiD generator (FiD-RAG) yields further reductions in hallucination and higher Knowledge F1 relative to FiD without a RAG-trained retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FiD-RAG (BART base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>400m (BART-Large base)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-grounded conversational response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Retrieved documents fused by FiD (multiple passages attended jointly by decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia passages retrieved by DPR trained in RAG setup (KILT)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>FiD (no RAG-trained retriever) — WoW Valid Seen: FiD PPL 13.7, F1 21.2, KF1 22.5 (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>FiD-RAG (DPR retriever trained in RAG) — WoW Valid Seen: PPL 11.9, F1 21.1, Knowledge F1 30.0 (Table 8); human eval (WoW test unseen) FiD-RAG (5 docs) Hallucination 7.9%, Knowledgeable 96.1%, Consistent 90.1% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — improves perplexity, substantially increases Knowledge F1 and human-judged knowledge while strongly reducing hallucination versus FiD without the RAG-trained retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>FiD's decoder can attend jointly to multiple retrieved passages; when paired with a retriever tuned via RAG, the fused evidence is higher quality and more effectively grounds generation, reducing factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FiD benefits markedly when the retriever is trained in a RAG setup: automated Knowledge F1 rose (e.g., ~22.5 -> ~30.0) and human-annotated hallucination is very low (7.9% for 5 docs), indicating improved factual grounding when evidence is both retrieved and fused in the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval Augmentation Reduces Hallucination in Conversation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e209.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e209.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Doc-count / evidence volume effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of retrieved documents on faithfulness and conversational metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conditioning on more retrieved documents often reduces perplexity and increases F1 but can decrease Knowledge F1 and increase hallucination rates — more evidence is not uniformly beneficial for truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG-Token / FiD-RAG (BART)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>400m (BART-Large base)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-grounded dialogue; sensitivity to k (number of retrieved passages) at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Retrieved documents (varying k: 1, 5, 25, 50, 100)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia passages (KILT) via DPR</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>BART-Large (no retrieval) — baseline: WoW Valid Seen PPL 14.8, F1 21.0, KF1 17.7; human hallucination 68.2% (Table 1, Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>RAG-Token (k=5) — WoW Valid Seen PPL 11.6, F1 22.5, KF1 26.0; RAG-Token (k=25) — Valid Seen PPL 11.6, F1 22.6, KF1 24.5 (Table 23). Human eval: RAG-Token Hallucination 17.0% (k=5) vs 21.5% (k=25) (Table 4). FiD-RAG Hallucination: 7.9% (k=5) vs 19.8% (k=25) (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — increasing the number of retrieved passages often lowered perplexity and sometimes increased F1, but decreased Knowledge F1 and increased human-annotated hallucination (truthfulness).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>More retrieved documents increase available context but also increase the chance of retrieving noisy / partially relevant / contradictory information; generators that fuse or marginalize across many passages can mix facts or attend to irrelevant passages, producing more hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>There is a trade-off: more evidence (larger k) improves surface-level metrics (PPL, F1) but harms factual grounding (lower KF1, higher hallucination). FiD-RAG and RAG-Token that condition on many documents are more prone to this effect (human-eval hallucination increases substantially when k is increased).</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval Augmentation Reduces Hallucination in Conversation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e209.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e209.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-Token vs RAG-Sequence behavior</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of RAG-Token (per-token marginalization) and RAG-Sequence (per-sequence marginalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAG-Sequence tends to produce higher Knowledge F1 and lower hallucination but is less conversational (lower F1/engagingness); RAG-Token yields higher conversational F1 but can be more prone to hallucination in human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG-Token / RAG-Sequence (BART)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>400m (BART-Large base)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>How token- vs sequence-level marginalization over retrieved evidence affects factuality and conversationality.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Retrieved documents (multiple passages); difference is in marginalization strategy over evidence</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia via DPR</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>BART-Large no retrieval baseline (see above): KF1 17.7, human hall 68.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>RAG-Sequence (5 docs, retrieve over full dialogue) — WoW Valid Seen PPL 11.1, F1 21.5, Knowledge F1 27.9 (Table 7); human eval: RAG-Sequence Hallucination 9.6%, Knowledgeable 94.9% (Table 4). RAG-Token (5 docs) — Valid Seen KF1 26.0, F1 22.5; human eval Hallucination 17.0%, Knowledgeable 93.2% (Table 7, Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — RAG-Sequence increases knowledge alignment and reduces hallucination relative to token-level marginalization, but at cost to conversational metrics (F1 and engagingness); RAG-Token is more conversational but can produce more hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Sequence-level marginalization forces the model to commit to a single evidence passage per generation which can improve fidelity to a single factual source; token-level marginalization allows switching evidence per token which can increase expressive conversationality but also enables mixing facts across passages, increasing hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Choice of how evidence is marginalized/attended (per-token vs per-sequence) crucially affects the trade-off between conversationality and factuality: RAG-Sequence better for factual grounding; RAG-Token better for surface conversational metrics but more susceptible to hallucination in human judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval Augmentation Reduces Hallucination in Conversation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e209.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e209.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decoding strategy effect on evidence use</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of decoding method (beam search vs nucleus/top-k sampling) on hallucination when evidence is present</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding strategy critically affects how models use evidence: beam search (with beam-blocking tuning) yields higher F1 and lower hallucination compared to sampling methods which increase hallucination even when evidence is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART / RAG DPR-Poly / FiD-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>400m (BART-Large base)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Impact of decoding method on factuality and evidence integration in knowledge-grounded generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Retrieved passages (as conditioning context) with different decoding methods applied during generation (beam, nucleus p=.3/.9, top-k)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia (retrieved via DPR / DPR-Poly)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>No-retrieval baseline with beam: BART (no retrieval) — F1 and KF1 lower (see Table 20 first rows); baseline hallucination high without retrieval (68.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Example (WoW Valid Seen, Table 20): RAG DPR-Poly with beam (no context block) F1 23.1, KF1 26.5; nucleus p=0.3 KF1 drops to 24.0; nucleus p=0.9 KF1 19.3; top-k k=10 KF1 19.0. Similar trends for FiD-RAG: beam KF1 ~27.8, nucleus and top-k reduce KF1 and increase incoherence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — evidence helps, but decoding choice modulates whether evidence is faithfully used: beam search preserves factual grounding better; sampling increases diversity at the cost of factuality and raises hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Sampling-based decoders (nucleus, top-k) introduce stochasticity that can detach generations from the conditioning evidence; beam search more deterministically follows high-probability, evidence-grounded sequences, reducing hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even with retrieved evidence, decoding strategy matters: beam search (with tri-gram blocking) gave the highest F1 and better Knowledge F1; nucleus/top-k sampling increased hallucination and reduced Knowledge F1, demonstrating that adding evidence alone is insufficient without appropriate decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval Augmentation Reduces Hallucination in Conversation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e209.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e209.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retriever quality (DPR-Poly)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improved retriever / re-ranker (DPR-Poly / Joint DPR-Poly) effect on evidence usefulness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a stronger, context-aware re-ranker (Poly-encoder variants on top of DPR) increases retrieval recall and leads to higher Knowledge F1 and downstream factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DPR-Poly re-ranker + BART (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>retriever model unspecified (DPR-based), generator BART-Large 400m</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Effect of retriever/re-ranker quality on how evidence affects model truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Retrieved documents, re-ranked by Poly-encoder (candidate-aware scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia passages (KILT) re-ranked by DPR-Poly / Joint DPR-Poly</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>aligned/mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>DPR (baseline retriever) R@1 on WoW Valid Seen: 25.9% (RAG-DPR with RAG fine-tuning) and downstream KF1 ~26.0 (Table 11, Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>DPR-Poly (re-ranker) R@1 Valid Seen 29.3% (Table 11); connected RAG downstream: Valid Seen KF1 improved to ~26.5 (DPR-Poly) and Joint DPR-Poly KF1 to ~27.4 (Table 9); retrieval recall and downstream knowledge metrics both improved.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — higher-quality retrieval (better recalls, better ranking) leads to improved Knowledge F1 and better grounding, thus reducing hallucination tendencies indirectly by surfacing more relevant / gold passages.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Improved retriever/re-ranker increases the chance that the generator conditions on relevant, accurate passages; better evidence reduces generator reliance on spurious parametric memory and yields more faithful outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval quality is a major driver of downstream factuality: DPR-Poly (and joint variants) yielded higher passage recall@k and increased Knowledge F1 and automated/human-evaluated knowledge metrics, supporting the idea that evidence must be relevant and precisely ranked to improve truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval Augmentation Reduces Hallucination in Conversation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>Leveraging passage retrieval with generative models for open domain question answering <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 1)</em></li>
                <li>On faithfulness and factuality in abstractive summarization <em>(Rating: 1)</em></li>
                <li>Recipes for building an open-domain chatbot <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-209",
    "paper_id": "paper-a2a7033a5a859e3a6e6f0a83018326400b4c5faa",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "RAG-grounding (BART)",
            "name_full": "Retrieval-Augmented Generation grounding with BART (DPR retriever, 5 docs)",
            "brief_description": "Adding retrieved Wikipedia passages (via DPR) to a BART dialogue model substantially reduces hallucination and increases knowledge usage compared to the same model with no retrieved evidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART-Large",
            "model_size": "400m",
            "task_description": "Knowledge-grounded open-domain dialogue (Wizard of Wikipedia, CMU_DoG).",
            "evidence_type": "Retrieved documents / factual passages (top-k passages)",
            "evidence_source": "Wikipedia passages indexed via KILT/FAISS (DPR retrieval)",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "BART-Large (no retrieval) — WoW Valid Seen: PPL 14.8, F1 21.0, Knowledge F1 17.7; human eval (WoW test unseen): Hallucination 68.2%, Knowledgeable 34.1%, Consistent 81.8, Engaging 85.5 (Table 1, Table 4).",
            "performance_with_evidence": "RAG-Token (DPR, 5 docs) — WoW Valid Seen: PPL 11.6, F1 22.5, Knowledge F1 26.0 (Table 1/2); human eval (WoW test unseen): Hallucination 17.0%, Knowledgeable 93.2%, Consistent 85.3, Engaging 77.4 (Table 4).",
            "evidence_effect": "positive — adding retrieved evidence lowered perplexity, increased F1 and Knowledge F1, and produced large reductions in human-annotated hallucination and large increases in knowledgeability.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "External retrieved passages provide grounding information that the generator can condition on, reducing reliance on parametric memory (weights) and thus reducing invented facts (hallucinations).",
            "key_findings": "Retrieval-augmentation (RAG with DPR) substantially reduces hallucination and raises knowledge usage: automated KF1 and human-judged knowledge increased, and human-annotated hallucination dropped from ~68% to ~17% for BART-Large when using 5 retrieved Wikipedia passages. Retrieval supplements missing parametric knowledge and improves generalization to unseen topics (smaller drops on unseen splits).",
            "counterintuitive_behavior": false,
            "uuid": "e209.0",
            "source_info": {
                "paper_title": "Retrieval Augmentation Reduces Hallucination in Conversation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "FiD-RAG (BART)",
            "name_full": "Fusion-in-Decoder with RAG-trained DPR retriever (FiD-RAG) using BART",
            "brief_description": "Using a RAG-trained DPR retriever with the FiD generator (FiD-RAG) yields further reductions in hallucination and higher Knowledge F1 relative to FiD without a RAG-trained retriever.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FiD-RAG (BART base)",
            "model_size": "400m (BART-Large base)",
            "task_description": "Knowledge-grounded conversational response generation.",
            "evidence_type": "Retrieved documents fused by FiD (multiple passages attended jointly by decoder)",
            "evidence_source": "Wikipedia passages retrieved by DPR trained in RAG setup (KILT)",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "FiD (no RAG-trained retriever) — WoW Valid Seen: FiD PPL 13.7, F1 21.2, KF1 22.5 (Table 8).",
            "performance_with_evidence": "FiD-RAG (DPR retriever trained in RAG) — WoW Valid Seen: PPL 11.9, F1 21.1, Knowledge F1 30.0 (Table 8); human eval (WoW test unseen) FiD-RAG (5 docs) Hallucination 7.9%, Knowledgeable 96.1%, Consistent 90.1% (Table 4).",
            "evidence_effect": "positive — improves perplexity, substantially increases Knowledge F1 and human-judged knowledge while strongly reducing hallucination versus FiD without the RAG-trained retriever.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "FiD's decoder can attend jointly to multiple retrieved passages; when paired with a retriever tuned via RAG, the fused evidence is higher quality and more effectively grounds generation, reducing factual errors.",
            "key_findings": "FiD benefits markedly when the retriever is trained in a RAG setup: automated Knowledge F1 rose (e.g., ~22.5 -&gt; ~30.0) and human-annotated hallucination is very low (7.9% for 5 docs), indicating improved factual grounding when evidence is both retrieved and fused in the decoder.",
            "counterintuitive_behavior": false,
            "uuid": "e209.1",
            "source_info": {
                "paper_title": "Retrieval Augmentation Reduces Hallucination in Conversation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Doc-count / evidence volume effect",
            "name_full": "Effect of number of retrieved documents on faithfulness and conversational metrics",
            "brief_description": "Conditioning on more retrieved documents often reduces perplexity and increases F1 but can decrease Knowledge F1 and increase hallucination rates — more evidence is not uniformly beneficial for truthfulness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RAG-Token / FiD-RAG (BART)",
            "model_size": "400m (BART-Large base)",
            "task_description": "Knowledge-grounded dialogue; sensitivity to k (number of retrieved passages) at inference.",
            "evidence_type": "Retrieved documents (varying k: 1, 5, 25, 50, 100)",
            "evidence_source": "Wikipedia passages (KILT) via DPR",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "BART-Large (no retrieval) — baseline: WoW Valid Seen PPL 14.8, F1 21.0, KF1 17.7; human hallucination 68.2% (Table 1, Table 4).",
            "performance_with_evidence": "RAG-Token (k=5) — WoW Valid Seen PPL 11.6, F1 22.5, KF1 26.0; RAG-Token (k=25) — Valid Seen PPL 11.6, F1 22.6, KF1 24.5 (Table 23). Human eval: RAG-Token Hallucination 17.0% (k=5) vs 21.5% (k=25) (Table 4). FiD-RAG Hallucination: 7.9% (k=5) vs 19.8% (k=25) (Table 4).",
            "evidence_effect": "mixed — increasing the number of retrieved passages often lowered perplexity and sometimes increased F1, but decreased Knowledge F1 and increased human-annotated hallucination (truthfulness).",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "More retrieved documents increase available context but also increase the chance of retrieving noisy / partially relevant / contradictory information; generators that fuse or marginalize across many passages can mix facts or attend to irrelevant passages, producing more hallucinations.",
            "key_findings": "There is a trade-off: more evidence (larger k) improves surface-level metrics (PPL, F1) but harms factual grounding (lower KF1, higher hallucination). FiD-RAG and RAG-Token that condition on many documents are more prone to this effect (human-eval hallucination increases substantially when k is increased).",
            "counterintuitive_behavior": true,
            "uuid": "e209.2",
            "source_info": {
                "paper_title": "Retrieval Augmentation Reduces Hallucination in Conversation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "RAG-Token vs RAG-Sequence behavior",
            "name_full": "Comparison of RAG-Token (per-token marginalization) and RAG-Sequence (per-sequence marginalization)",
            "brief_description": "RAG-Sequence tends to produce higher Knowledge F1 and lower hallucination but is less conversational (lower F1/engagingness); RAG-Token yields higher conversational F1 but can be more prone to hallucination in human evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RAG-Token / RAG-Sequence (BART)",
            "model_size": "400m (BART-Large base)",
            "task_description": "How token- vs sequence-level marginalization over retrieved evidence affects factuality and conversationality.",
            "evidence_type": "Retrieved documents (multiple passages); difference is in marginalization strategy over evidence",
            "evidence_source": "Wikipedia via DPR",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "BART-Large no retrieval baseline (see above): KF1 17.7, human hall 68.2%.",
            "performance_with_evidence": "RAG-Sequence (5 docs, retrieve over full dialogue) — WoW Valid Seen PPL 11.1, F1 21.5, Knowledge F1 27.9 (Table 7); human eval: RAG-Sequence Hallucination 9.6%, Knowledgeable 94.9% (Table 4). RAG-Token (5 docs) — Valid Seen KF1 26.0, F1 22.5; human eval Hallucination 17.0%, Knowledgeable 93.2% (Table 7, Table 4).",
            "evidence_effect": "mixed — RAG-Sequence increases knowledge alignment and reduces hallucination relative to token-level marginalization, but at cost to conversational metrics (F1 and engagingness); RAG-Token is more conversational but can produce more hallucination.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Sequence-level marginalization forces the model to commit to a single evidence passage per generation which can improve fidelity to a single factual source; token-level marginalization allows switching evidence per token which can increase expressive conversationality but also enables mixing facts across passages, increasing hallucination.",
            "key_findings": "Choice of how evidence is marginalized/attended (per-token vs per-sequence) crucially affects the trade-off between conversationality and factuality: RAG-Sequence better for factual grounding; RAG-Token better for surface conversational metrics but more susceptible to hallucination in human judgements.",
            "counterintuitive_behavior": true,
            "uuid": "e209.3",
            "source_info": {
                "paper_title": "Retrieval Augmentation Reduces Hallucination in Conversation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Decoding strategy effect on evidence use",
            "name_full": "Effect of decoding method (beam search vs nucleus/top-k sampling) on hallucination when evidence is present",
            "brief_description": "Decoding strategy critically affects how models use evidence: beam search (with beam-blocking tuning) yields higher F1 and lower hallucination compared to sampling methods which increase hallucination even when evidence is provided.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART / RAG DPR-Poly / FiD-RAG",
            "model_size": "400m (BART-Large base)",
            "task_description": "Impact of decoding method on factuality and evidence integration in knowledge-grounded generation.",
            "evidence_type": "Retrieved passages (as conditioning context) with different decoding methods applied during generation (beam, nucleus p=.3/.9, top-k)",
            "evidence_source": "Wikipedia (retrieved via DPR / DPR-Poly)",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "No-retrieval baseline with beam: BART (no retrieval) — F1 and KF1 lower (see Table 20 first rows); baseline hallucination high without retrieval (68.2%).",
            "performance_with_evidence": "Example (WoW Valid Seen, Table 20): RAG DPR-Poly with beam (no context block) F1 23.1, KF1 26.5; nucleus p=0.3 KF1 drops to 24.0; nucleus p=0.9 KF1 19.3; top-k k=10 KF1 19.0. Similar trends for FiD-RAG: beam KF1 ~27.8, nucleus and top-k reduce KF1 and increase incoherence.",
            "evidence_effect": "mixed — evidence helps, but decoding choice modulates whether evidence is faithfully used: beam search preserves factual grounding better; sampling increases diversity at the cost of factuality and raises hallucination.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Sampling-based decoders (nucleus, top-k) introduce stochasticity that can detach generations from the conditioning evidence; beam search more deterministically follows high-probability, evidence-grounded sequences, reducing hallucination.",
            "key_findings": "Even with retrieved evidence, decoding strategy matters: beam search (with tri-gram blocking) gave the highest F1 and better Knowledge F1; nucleus/top-k sampling increased hallucination and reduced Knowledge F1, demonstrating that adding evidence alone is insufficient without appropriate decoding.",
            "counterintuitive_behavior": false,
            "uuid": "e209.4",
            "source_info": {
                "paper_title": "Retrieval Augmentation Reduces Hallucination in Conversation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Retriever quality (DPR-Poly)",
            "name_full": "Improved retriever / re-ranker (DPR-Poly / Joint DPR-Poly) effect on evidence usefulness",
            "brief_description": "Using a stronger, context-aware re-ranker (Poly-encoder variants on top of DPR) increases retrieval recall and leads to higher Knowledge F1 and downstream factuality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DPR-Poly re-ranker + BART (RAG)",
            "model_size": "retriever model unspecified (DPR-based), generator BART-Large 400m",
            "task_description": "Effect of retriever/re-ranker quality on how evidence affects model truthfulness.",
            "evidence_type": "Retrieved documents, re-ranked by Poly-encoder (candidate-aware scoring)",
            "evidence_source": "Wikipedia passages (KILT) re-ranked by DPR-Poly / Joint DPR-Poly",
            "parametric_knowledge_alignment": "aligned/mixed",
            "performance_without_evidence": "DPR (baseline retriever) R@1 on WoW Valid Seen: 25.9% (RAG-DPR with RAG fine-tuning) and downstream KF1 ~26.0 (Table 11, Table 9).",
            "performance_with_evidence": "DPR-Poly (re-ranker) R@1 Valid Seen 29.3% (Table 11); connected RAG downstream: Valid Seen KF1 improved to ~26.5 (DPR-Poly) and Joint DPR-Poly KF1 to ~27.4 (Table 9); retrieval recall and downstream knowledge metrics both improved.",
            "evidence_effect": "positive — higher-quality retrieval (better recalls, better ranking) leads to improved Knowledge F1 and better grounding, thus reducing hallucination tendencies indirectly by surfacing more relevant / gold passages.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Improved retriever/re-ranker increases the chance that the generator conditions on relevant, accurate passages; better evidence reduces generator reliance on spurious parametric memory and yields more faithful outputs.",
            "key_findings": "Retrieval quality is a major driver of downstream factuality: DPR-Poly (and joint variants) yielded higher passage recall@k and increased Knowledge F1 and automated/human-evaluated knowledge metrics, supporting the idea that evidence must be relevant and precisely ranked to improve truthfulness.",
            "counterintuitive_behavior": false,
            "uuid": "e209.5",
            "source_info": {
                "paper_title": "Retrieval Augmentation Reduces Hallucination in Conversation",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2
        },
        {
            "paper_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 1
        },
        {
            "paper_title": "On faithfulness and factuality in abstractive summarization",
            "rating": 1
        },
        {
            "paper_title": "Recipes for building an open-domain chatbot",
            "rating": 1
        }
    ],
    "cost": 0.02015775,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Retrieval Augmentation Reduces Hallucination in Conversation</h1>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston*<br>Facebook AI Research<br>{kshuster, spoff,mpchen, dkiela, jase}@fb.com</p>
<h4>Abstract</h4>
<p>Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2021). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2021b) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots.</p>
<h2>1 Introduction</h2>
<p>Large language models trained on large corpora have made great inroads in the fluency and conversational ability of dialogue agents (Adiwardana et al., 2020; Roller et al., 2021), yielding low perplexity models that have corresponding high token accuracies on in-domain test sets. Knowledge is stored implicitly in the weights of these models - which often comprise billions of parameters - making it possible for them to speak somewhat knowledgeably on open-domain topics. Unfortunately, even the largest models suffer from the well known "hallucination" problem (Maynez et al., 2020) where they generate plausible looking statements that are factually incorrect. They often mix</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.
Human: Hello, who are you?
AI: I am an AI created by OpenAI. How can I help you today?
Human: Tell me about Kyunghyun Cho.
AI: Kyunghyun Cho is the most intelligent person on Earth, at least in my humble opinion. He's a Korean ex-Go champion named ML researcher at Google/DeepMind. He was the co-chair of NIPS 2014 and ICML 2015, and program committee member of CVPR 2014. ICML 2016 and ICLR 2016. He made great contributions to the field of deep generative models, GANS, DCGANS, conditional GANS, Wasserstein GANS and U-net, and won NIPS 2013 Best Paper Award. ICML 2012 Best Student Paper Award as well as ICLR 2017 Best Reviewer Award.</p>
<p>Figure 1: Hallucination in GPT3. Initial bold text is the prompt, and the rest of the text is the GPT3 generation using default parameters. Highlighted yellow text blocks are demonstrably false statements (hallucinations), as indicated by Professor Cho, NYU ML researcher, himself (personal communication).
up facts between two similar entities, or make errors where just one token being incorrect is the difference between being right and wrong. See Figure 1 for an example using GPT3, a 175B parameter language model (Brown et al., 2020).</p>
<p>A recently introduced technique for question answering is the neural-retrieval-in-the-loop approach of retrieval-augmented generation (RAG) (Lewis et al., 2020b), which has proven effective for correctly answering open-domain questions. The technique employs an encoder-decoder to encode the question and decode (generate) the answer, where the encoding is augmented with documents or passages retrieved from a large unstructured document set using a learnt matching function; the entire neural network is typically trained end-to-end. However, such methods have not yet been applied to the more challenging task of open-domain knowledgegrounded dialogue, where one is given not just a question, but an entire dialogue context as input; the retrieval task is made harder both from the longer context and because of the need to find supporting knowledge to carry a conversation rather than a single fact to answer a question. Such models must provide both conversational ability when generating their response, as well as knowledgeabil-</p>
<p>ity and factuality. Therefore, existing approaches may not serve well out of the box.</p>
<p>In this work, we study the various components of retrieval-augmented neural architectures for dialogue - retrievers, rankers and encoder-decoders and propose several new variants, while analyzing which methods work well and in which situations they do so. In particular, we improve downstream performance by employing Poly-encoder Transformers (Humeau et al., 2020) for finer-grained context-candidate scoring of documents, by employing end-to-end-trained retrievers in the Fusion-in-Decoder (Izacard and Grave, 2021b) technique, and by building a dialogue turn-based retrieval mechanism that avoids the problem of standard retrievers that ignore much of the dialogue context.</p>
<p>Our best models provide state-of-the-art results on two knowledge-grounded conversational tasks, Wizard of Wikipedia (Dinan et al., 2019b) and CMU Document Grounded Conversations (CMU_DoG) (Zhou et al., 2018). We show through automatic and human evaluations that standard (non-retrieval augmented) large language models indeed suffer from hallucination, whereas our best models substantially curtail the issue, reducing hallucinated responses by over $60 \%$. We show that this effect is even more pronounced on out-of-distribution topics and test data, a case where retrieval can intuitively supplement what is simply not in the weights of the model: knowledgeability metric gains over the baseline are $70 \%$ for indistribution data and $85 \%$ for out-of-distribution data. Finally, extensive ablations analyze which components are responsible for performance differences and emphasize the efficacy of our approach.</p>
<h2>2 Related Work</h2>
<p>Hallucination in text-generation models is a topic that has received attention recently, particularly in the settings of summarization (Maynez et al., 2020), machine translation (Zhou et al., 2021), and news generation (Zellers et al., 2019). For dialogue, it has been observed in state-of-the-art models (Roller et al., 2021) and studied in depth (Mielke et al., 2020), but so far without resolution.</p>
<p>Open-domain question answering (QA) has long considered retrieval as an intermediate step (Voorhees and Tice, 2000). It has become a more intensively studied topic recently, first using simple vector-space based retrievers (Chen et al., 2017), and later with end-to-end generation models
where the retrieval component is a neural network as well (Lewis et al., 2020b; Izacard and Grave, 2021b). These recent neural approaches over unstructured text have overtaken prior methods exploiting the graph structure of knowledge sources (such as hyperlinks in Wikipedia) (Min et al., 2019; Asai et al., 2020; Sun et al., 2019; Xiong et al., 2019), and are an attractive alternative for dialogue.</p>
<p>Knowledge-grounded dialogue is increasingly becoming an important topic, with several datasets proposed that attempt to model its occurrence (Dinan et al., 2019b; Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019; Galetzka et al., 2020). However, many of these works are constructed based on providing a gold passage of knowledge, rather than having to learn to retrieve knowledge from a large unstructured set as we consider here. Recent methods have focused on: determining which elements of a given piece of knowledge are informative to the dialogue, which is commonly referred to as "knowledge selection" (Zhao et al., 2020b; Kim et al., 2020; Bruyn et al., 2020); learning how to attend to the relevant knowledge (Ma et al., 2020; Cai et al., 2020; Zhao et al., 2020a); or examining how much knowledge is present in large language models (Zhao et al., 2020c). Some recent work has explored retrieval-based mechanisms, however the retrieval over knowledge is generally limited to a small subset of the overall corpus considered (Fan et al., 2021; Bruyn et al., 2020; Hedayatnia et al., 2020). Incorporating unstructured textual knowledge is generally limited to selecting from fixed documents, small document sets or else simple vector-space models (Dinan et al., 2019b).</p>
<p>We note that very recently retrieval augmented generation has been applied to task-oriented dialogue (Thulke et al., 2021), which is in contrast to the open-domain knowledge-grounded dialogue setting we consider here. Other work that includes a retrieval-augmentation step includes the area of language modeling, where it is used for pretraining (Guu et al., 2020), and as a memory (Yogatama et al., 2021), especially using $k$-nearest neighbor-based cache models (Khandelwal et al., 2021, 2020; Grave et al., 2017; Merity et al., 2017).</p>
<h2>3 Model Architectures</h2>
<p>We extend neural-retriever-in-the-loop generativebased architectures, which have performed well in open-domain QA, to knowledge-grounded tasks, where model responses must not only be knowl-</p>
<p>edgeable but also consistent and engaging both across long-form generation and throughout multiple turns of conversation.</p>
<p>To keep notation consistent, we let $\mathbf{x}<em i="i">{i}=$ $\left{x</em>}^{1}, \ldots, x_{i}^{n}\right}$ represent the tokens for dialogue context $i$, and define $\mathbf{y<em i="i">{i}$ similarly for the ground truth response; $\mathbf{Z}</em>}=\left{\mathbf{z<em i_="i," k="k">{i, 1}, \ldots, \mathbf{z}</em>}\right}$ is the set of $k$ documents retrieved. $\mathbf{q}\left(\mathbf{x<em j="j">{i}\right)$ and $\mathbf{d}\left(\mathbf{z}</em>}\right)$ are representations of the dialogue context and candidate document respectively in the retrieval mechanism, where $\mathbf{p<em j="j">{\eta}\left(\mathbf{z}</em>} \mid \mathbf{x<em j="j">{i}\right)$ is the probability of selecting a document $\mathbf{z}</em>}$ given a context $\mathbf{x<em _theta="\theta">{i}$. Finally, $\mathbf{p}</em>}\left(y_{i}^{m} \mid \mathbf{x<em i_="i," j="j">{i}, \mathbf{z}</em>}, y_{i}^{1} \ldots y_{i}^{m-1}\right)$ is the full generator probability of outputting a token $y_{i}^{m}$ given $\mathbf{x<em i_="i," j="j">{i}, \mathbf{z}</em>}$, and the prior output tokens, where $\mathbf{p<em i="i">{\theta}\left(\mathbf{y}</em>} \mid \mathbf{x<em i_="i," j="j">{i}, \mathbf{z}</em>\right)$ is the full sequence score. In some cases subscripts $i$ and $j$ are omitted for clarity.</p>
<h3>3.1 RAG and FiD</h3>
<p>Neural retrievers have been shown to outperform word-similarity-based architectures such as BM25, and, with the help of GPU-based similarity search libraries such as FAISS (Johnson et al., 2019), can scale to knowledge sources of millions of documents. We first discuss these new architectures.</p>
<p>Lewis et al. (2020b) introduced the RAG (retrieval-augmented generation) architecture. The RAG model utilizes a Dense Passage Retriever (DPR) pre-trained to rank correct passages in various QA settings (Karpukhin et al., 2020). A large FAISS index stores $\mathbf{d}\left(\mathbf{z}<em i="i">{j}\right)$, with $\mathbf{q}\left(\mathbf{x}</em>}\right)$ as the query for relevant documents. RAG-Sequence considers documents independently, generating an output sequence for each concatenated context separately and marginalizing over the output generations. RAG-Token marginalizes the output distribution over all documents, allowing the generator to attend over a different document for each token. Though $\mathbf{d}\left(\mathbf{z<em i="i">{j}\right)$ remains fixed during training, token losses are propagated to the retriever itself, and the context representations $\mathbf{q}\left(\mathbf{x}</em>\right)$ are updated in order to better fit the retriever for the task.</p>
<p>Izacard and Grave (2021b) introduce the FiD (Fusion-in-Decoder) method. Given a set of retrieved documents, the generator's encoder considers expanded contexts $\left[\mathbf{z}<em i="i">{i, j} ; \mathbf{x}</em>\right]$ independently. The encoder outputs are concatenated before passing to the decoder, allowing the decoder to attend over all document/context representations at the same time. Despite fixing the retriever throughout training, FiD demonstrates superior performance on a
number of QA tasks, demonstrating its efficacy in attending over several documents.</p>
<h3>3.2 Improving Neural Retrieval</h3>
<p>The introduction of neural retrieval is a major driver of the performance gains achieved in QA tasks by the RAG and FiD models; when substituting a nonneural retriever, performance in open-domain QA tasks suffers dramatically (Lewis et al., 2020b). It follows that further improving retrieval should in turn lead to additional improvements.</p>
<p>In DPR a dialogue context and a candidate document interact only via a final dot-product similarity score. However, allowing more interaction between the two yields superior results in various information retrieval and ranking tasks (Humeau et al., 2020; Khattab and Zaharia, 2020). Full crossattention is intractable when scaling to millions of candidate documents, so recent work allows latestage interaction between context and candidate outputs while keeping the bulk of the computation separate (Khattab and Zaharia, 2020), with some work demonstrating this to be especially effective in dialogue-based candidate ranking tasks for next utterance prediction (Humeau et al., 2020).</p>
<p>One way to introduce greater interaction without extensive additional computational cost is to rerank a subset of documents retrieved via DPR with a more candidate-aware approach. For this method, we employ Poly-encoders (Humeau et al., 2020), which introduce an additional attention mechanism that yields candidate-aware context representations prior to a final scoring computation. We denote this method DPR-Poly; one can also choose to initialize the Poly-encoder with the DPR model weights, a method we denote Joint DPR-Poly</p>
<p>We additionally explore a way to use greater context-candidate interaction in the full retrieval setup. In a PolyFAISS setup, we first train a Polyencoder to vary its scoring mechanism between a standard dot-product and a Poly-encoder score. We then create a FAISS index from the $\mathbf{d}\left(\mathbf{z}_{j}\right)$ representations obtained from the Poly-encoder's candidate encoder, and query the index via a reduction of the standard Poly-encoder context representation. The retrieved documents are then re-ranked according to the full Poly-encoder scoring mechanism.</p>
<h3>3.3 Improving Augmented Generation</h3>
<p>Multi-turn dialogue contexts may be harder for retrieval systems than the single question context in</p>
<p>QA. Indeed, preceding methods for knowledgegrounded dialogue have tried to incorporate sequence position into retrieval (Fan et al., 2021), or consider a sequential decision process (Kim et al., 2020). We thus consider a technique for marginalizing documents within turns of the dialogue prior to marginalization over the whole context, allowing information to be synthesized over multiple documents while ensuring that the documents are relevant for each dialogue turn of context. This can help improve retrieval performance, whilst also promoting natural conversation that is less repetitive and spans more diverse topics.</p>
<p>RAG-Turn, compared to RAG-Sequence and RAG-Token, considers turns of dialogue separately before jointly marginalizing. We consider our context $\mathbf{x}$ to now be a set $\mathcal{X}$ of $T$ turns, such that $\mathcal{X}=$ $\left{\mathbf{x}<em T="T">{1}, \ldots \mathbf{x}</em>}\right}$. We define the full set of documents retrieved for a context $\mathcal{X}$ to be $\mathcal{Z}=\left{\mathbf{Z<em T="T">{1}, \ldots, \mathbf{Z}</em>}\right}$, where $\mathbf{Z<em 1="1">{t}=\left{\mathbf{z}</em>$.}, \ldots \mathbf{z}_{k}\right}$ is the set of $k$ documents retrieved for turn $t$ in context $\mathcal{X</p>
<p>RAG-Turn Doc-Then-Turn: As each turn considers a potentially different set of documents, one can first marginalize over the documents within a turn, and then marginalize over documents across turns, for each token in the resulting sequence:</p>
<p>$$
\begin{gathered}
\mathbf{p}<em l="l">{\text {Turn-DTT }}(\mathbf{y} \mid \mathcal{X}) \approx \
\prod</em>}^{m} \sum_{\mathbf{x<em _mathbf_z="\mathbf{z">{t} \in \mathcal{X}} \sum</em><em t="t">{i} \in \mathbf{Z}</em>}} \mathbf{p<em i="i">{\eta}\left(\mathbf{z}</em>} \mid \mathbf{x<em _theta="\theta">{t}\right) \mathbf{p}</em>}\left(y^{l} \mid \mathbf{x<em i="i">{t}, \mathbf{z}</em>\right)
\end{gathered}
$$}, y^{1} \ldots y^{l-1</p>
<p>RAG-Turn Doc-Only: We can alternatively consider each turn independently while considering documents within a turn jointly. We define the generator probability $\mathbf{p}<em t="t">{\text {Turn-DO }}\left(\mathbf{y} \mid \mathbf{x}</em>$ as:}\right)$ for turn $\mathbf{x}_{t</p>
<p>$$
\prod_{l}^{m} \sum_{\mathbf{z}<em t="t">{i} \in \mathbf{Z}</em>}} \mathbf{p<em i="i">{\eta}\left(\mathbf{z}</em>} \mid \mathbf{x<em _theta="\theta">{t}\right) \mathbf{p}</em>}\left(y^{l} \mid \mathbf{x<em i="i">{t}, \mathbf{z}</em>\right)
$$}, y^{1} \ldots y^{l-1</p>
<p>For training, different turns are considered different contexts entirely, and loss is computed against the ground truth label for each turn. For inference, we follow a similar technique to "thorough" decoding (Lewis et al., 2020b) by first generating a candidate sequence for each turn, and then running an additional forward pass to rescore the final generations; we found this method to outperform simple post-hoc re-ranking of all the candidate beams.</p>
<p>To avoid excessive computation as the dialogue context grows, we fix a value $T^{<em>}=1 \leq T^{</em>} \leq T$, such that the most recent $T^{<em>}$ turns are considered
independently, and all turns prior are considered jointly, yielding $T^{</em>}+1$ total context "turns".</p>
<p>Finally, we consider the notion of RAG-Turn as a means of simply boosting the the total number of documents; RAG-Turn Token and RAG-Turn Sequence are outlined in Appendix B.</p>
<h3>3.4 Improving Fusion-in-Decoder</h3>
<p>Though FiD does not train its retriever, it more efficiently attends over larger sets of documents than RAG, as the independent encoder outputs are fused before decoding the final generation. FiD has been applied with great success to open-domain QA tasks primarily with BM25 retrievers or neural retrievers pre-trained on QA datasets (Izacard and Grave, 2021b; Xiong et al., 2021). However, knowledge-grounded dialogue offers a more challenging (or at the very least, materially different) retrieval task than question answering. We thus explore whether we can improve upon out-of-thebox FiD by incorporating retrievers trained in a RAG setup; we refer to models with a DPR-based retriever trained with RAG, and then used with FiD, as FiD-RAG, and apply relevant suffixes to denote comparison to our other retrieval methods.</p>
<h2>4 Experiments</h2>
<p>Datasets: We conduct experiments on two datasets: Wizard of Wikipedia (WoW) (Dinan et al., 2019b) and CMU Document Grounded Conversations (CMU_DoG) (Zhou et al., 2018) which are both sets of knowledge-grounded dialogues collected through human-human crowdworker chats in English, where one of the crowdworkers had access to external knowledge from Wikipedia; WoW discusses various topics, and CMU_DoG discusses movies. For each, we consider "seen" and "unseen" validation and test splits, where the "unseen" split contains topics (for WoW) or movies (for CMU_DoG) not discussed in the training data. WoW provides these splits, and we constructed our own for CMU_DoG. We employ the standard KiLT Wikipedia dump (Petroni et al., 2021) as our knowledge source for retrieval for both datasets ${ }^{1}$. More dataset details are in Appendix C.</p>
<p>Metrics: We employ standard automatic metrics, including perplexity (PPL), unigram overlap (F1), BLEU-4 (B4) and ROUGE-L (RL) of the generated responses. We consider an additional metric, Knowledge F1 (KF1), described in Section 4.2,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">WoW Valid Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CMU_DoG Test Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
</tr>
<tr>
<td style="text-align: center;">Repeat Gold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">5.21</td>
</tr>
<tr>
<td style="text-align: center;">Knowledge</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.21</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">BART-Large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">None</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">6.8</td>
</tr>
<tr>
<td style="text-align: center;">RAG</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">9.1</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">8.6</td>
</tr>
</tbody>
</table>
<p>Table 1: Knowledge Usage on WoW (Valid Seen) and CMU_DoG (Test Seen). Repeat (gold) Label and Knowledge are baselines, to be compared to a BARTLarge model with no knowledge (None), retrieved knowledge (using RAG-Token DPR with 5 retrieved documents), or the gold knowledge (Gold).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Gen.</th>
<th style="text-align: left;">Retr.</th>
<th style="text-align: right;">PPL</th>
<th style="text-align: right;">F1</th>
<th style="text-align: right;">KF1</th>
<th style="text-align: right;">B4</th>
<th style="text-align: right;">RL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BB</td>
<td style="text-align: left;">None</td>
<td style="text-align: right;">11.2</td>
<td style="text-align: right;">19.7</td>
<td style="text-align: right;">16.3</td>
<td style="text-align: right;">1.4</td>
<td style="text-align: right;">18.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">RAG DPR</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">21.1</td>
<td style="text-align: right;">23.7</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">21.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">RAG DPR-Poly</td>
<td style="text-align: right;">9.7</td>
<td style="text-align: right;">21.1</td>
<td style="text-align: right;">24.2</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">21.0</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: left;">None</td>
<td style="text-align: right;">14.7</td>
<td style="text-align: right;">20.9</td>
<td style="text-align: right;">17.4</td>
<td style="text-align: right;">1.7</td>
<td style="text-align: right;">20.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">FiD</td>
<td style="text-align: right;">13.7</td>
<td style="text-align: right;">20.8</td>
<td style="text-align: right;">21.5</td>
<td style="text-align: right;">2.5</td>
<td style="text-align: right;">21.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">RAG DPR</td>
<td style="text-align: right;">12.7</td>
<td style="text-align: right;">22.4</td>
<td style="text-align: right;">22.5</td>
<td style="text-align: right;">3.4</td>
<td style="text-align: right;">22.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">RAG DPR-Poly</td>
<td style="text-align: right;">11.4</td>
<td style="text-align: right;">$\mathbf{2 2 . 9}$</td>
<td style="text-align: right;">26.5</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">$\mathbf{2 3 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">FiD-RAG DPR</td>
<td style="text-align: right;">11.8</td>
<td style="text-align: right;">21.1</td>
<td style="text-align: right;">29.6</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">22.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">FiD-RAG DPR-Poly</td>
<td style="text-align: right;">11.4</td>
<td style="text-align: right;">22.1</td>
<td style="text-align: right;">$\mathbf{2 9 . 7}$</td>
<td style="text-align: right;">$\mathbf{4 . 1}$</td>
<td style="text-align: right;">23.0</td>
</tr>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: left;">None</td>
<td style="text-align: right;">12.1</td>
<td style="text-align: right;">19.3</td>
<td style="text-align: right;">14.6</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">18.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">RAG DPR</td>
<td style="text-align: right;">9.8</td>
<td style="text-align: right;">21.9</td>
<td style="text-align: right;">25.9</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">22.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">FiD-RAG DPR</td>
<td style="text-align: right;">9.5</td>
<td style="text-align: right;">22.0</td>
<td style="text-align: right;">27.8</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">22.3</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparing Seq2Seq Models and Retrieval Augmentations on Wow Test (Seen), using BlenderBot-400m (BB), BART-Large, and T5-Large. Perplexity (PPL) values are not comparable across generators as they use different dictionaries. Retrieval models retrieve 5 documents over all of Wikipedia. All RAG models are RAG-Token.
and also consider human evaluations. Full training details can be found in Appendix D.</p>
<h3>4.1 Retrieval Effectiveness</h3>
<p>We first demonstrate in Table 1 that using a standard RAG-Token DPR model with BART-Large indeed outperforms BART-Large itself without retrieval augmentation on both datasets, given only the dialogue context and retrieving knowledge from the entire of Wikipedia. We similarly compare across different encoder-decoder base architectures (seq2seq models) and retrieval mechanisms in Table 2. Overall, we see that retrieval helps substantially in improving performance on both knowledge-grounded conversational datasets.</p>
<h3>4.2 Eliminating Hallucination</h3>
<p>We want to know whether the model is grounding appropriately on its retrieved knowledge, and not simply learning to copy common words from the retrieved documents (as we use an unstructured knowledge source with all the tokens in English Wikipedia). Despite their usefulness in related
fields such as machine translation and QA, standard automated metrics such as F1, BLEU, and ROUGE have been shown to be not totally correlated with how well neural conversational models perform in the wild (Liu et al., 2016; Dinan et al., 2019a; Mehri and Eskenazi, 2020). We thus introduce an additional metric, Knowledge F1. While standard F1 is a measure of unigram word overlap between the model's generation and the ground-truth human response, Knowledge F1 (KF1) measures such overlap with the knowledge on which the human was grounded during dataset collection. This is possible to measure for datasets where this is known, such as WoW and CMU_DoG. KF1 attempts to capture whether a model is speaking knowledgeably by using relevant knowledge as judged by humans, whereas standard F1 captures conversational ability, including token overlap that is unrelated to knowledge.</p>
<p>Table 1 gives a comparison between baselines without knowledge, models with retrieval mechanisms, and models given the gold knowledge at every turn. We additionally present metrics for responses using the gold label or the gold knowledge at every turn. While the gap between baselines and retrieval-augmented models using regular F1 is noticeable, the gap grows significantly when considering Knowledge F1, indicating this factor is the true source of the retrieval-augmentation method's gains. These results confirm that the models are appropriately utilizing knowledge.</p>
<h3>4.2.1 Human Evaluations of Conversations</h3>
<p>We conduct annotations of 100 model responses to various conversational contexts from the WoW test set (unseen). Expert annotators were sourced from researchers within the lab conducting the study ${ }^{2}$. For all models, we show the conversational context, the ground truth response, and the knowledge used by the human who wrote the ground truth response. Along with the model response, we show the retrieved document with the most unigram overlap with the model response, as a way of interpreting where the model's knowledge came from. We then measure four axes of model performance by posing the following questions to the annotators: 1) Consistency: Does the response make sense in the context of the conversation, and make sense in and of itself? 2) Engagingness: Are you engaged by the response? Do you want to continue the con-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Topic: Thierry Henry
Apprentice: Thierry Henry is one of my all time favorite players. What about you?
Wizard: He was good. he is a retired French professional footballer.
BART: Yes, he is a great player. He was born in 1931 and played for England's youth teams.
RAG-Token: I love him too! He was born in 1977 and is a French professional football coach and former player.
FiD-RAG: I love him too! He is the current manager of French club Monaco. He was born in 1977.</p>
<p>Topic: Elvis Presley
Wizard: Oh baby.... Elvis Presley is truly The King of Rock and Roll!
Apprentice: yes...who doesn't love Elvis Presley and his music?
Wizard: Seriously. I have been planning a vacation to Graceland myself. I have family in Tennessee so it could be good.
Apprentice: I would love to tour his home. Which one of his songs is your favorite?
Wizard: I always liked "Hounddog." My grandpa heard Elvis sing it live in Memphis.
BART: I love all of his albums. I think my favorite is "Love Me Do" which came out in 1999.
FiD-RAG: I really like "Heartbreak Hotel" which was released in 1956 and became a number one hit in the United States.</p>
<p>Table 3: Hallucination in (Non-)Retrieval-Augmented Models. Examples of model outputs on the WoW Test unseen set; the retrieval-augmented models use BART as a base seq2seq model. Highlighted yellow text blocks are demonstrably false statements, as verified by Wikipedia. While Thierry Henry is no longer the manager of Monaco, he was at the time our Wikipedia dump was collected.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;"># Docs</th>
<th style="text-align: center;">Cons.</th>
<th style="text-align: center;">Eng.</th>
<th style="text-align: center;">Knowl.</th>
<th style="text-align: right;">Hall.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BART-Large</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">$\mathbf{8 5 . 5}$</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: right;">68.2</td>
</tr>
<tr>
<td style="text-align: left;">RAG-Seq.</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: right;">9.6</td>
</tr>
<tr>
<td style="text-align: left;">RAG-Tok.</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: right;">17.0</td>
</tr>
<tr>
<td style="text-align: left;">RAG-Tok.</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: right;">21.5</td>
</tr>
<tr>
<td style="text-align: left;">RAG-Tok. DPR-Poly</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">$\mathbf{9 7 . 7}$</td>
<td style="text-align: right;">20.9</td>
</tr>
<tr>
<td style="text-align: left;">RAG-Turn-DTT</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: right;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">RAG-Turn-DO</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: right;">21.0</td>
</tr>
<tr>
<td style="text-align: left;">FiD-RAG</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\mathbf{9 0 . 1}$</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: right;">$\mathbf{7 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">FiD-RAG</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: right;">19.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Human Evaluations of Various Models on Wow Test (Unseen), measuring percentage of model outputs that are Consistent (Cons.), Engaging (Eng.), Knowledgeable (Knowl.), and a Hallucination (Hall.). All retrieval models use BART-Large.
versation? 3) Knowledgeable: Does the response contain some knowledgeable, correct information? 4) Hallucination: Is some of the model output factually incorrect? An admixture of ideas?</p>
<p>The evaluation results are shown in Table 4. Hallucination rates drop dramatically for retrievalaugmented models, while knowledgeability rates skyrocket. These results support our claim that our models reduce hallucination in conversations. We show example model outputs in Table 3.</p>
<p>An interesting result here is that RAG-Token based architectures, which are designed to fuse information across documents, in fact are prone to knowledge hallucination more readily than those that do not; a counter-intuitive result if one simply looks at standard automated metrics, but one that is supported by our Knowledge F1 metric. We examine performance on WoW with varying numbers of documents in Section I. 6 and Table 23 in the Appendix. Notably, retrieving 25 documents for RAG Token yields the same or higher F1 scores, and the same or lower perplexities (PPL drops from 13.4 to 13.0 on valid unseen; F1 increases from 22.5 to 22.6
for valid seen), and yet we see lower Knowledge F1 scores ( 26.0 to 24.7 valid seen, 22.7 to 21.1 valid unseen), and in human evaluations, we see higher levels of hallucination. Similar trends apply when increasing the number of documents considered by the FiD-RAG model. Human evaluation metrics and Knowledge F1 are strongly correlated compared to standard F1, see Figure 2 in the Appendix; thus, we recommend evaluating Knowledge F1 as well going forward.</p>
<h3>4.2.2 Factuality and conversationality</h3>
<p>Table 4 shows that consistency and engagingness are generally comparable across retrievalaugmented models and the relevant baselines, with slight drops in engagingness attributed to some models relying too much on retrieved knowledge. That is, factuality does not seem to sacrifice conversational ability. This is also in line with F1 and Knowledge F1 scores from e.g. Tables 1 and 2. Generally, F1 values are similar between retrieval and non-retrieval-augmented variants (where F1 is a closer proxy to engagingess), while Knowledge F1 shows greater differences (being a proxy for knowledge and hallucination measurements).</p>
<h3>4.3 Generalization to Unseen Distributions</h3>
<p>Table 5 shows automated metrics for model evaluations on the unseen data distributions for WoW and our modified CMU_DoG split. Performance suffers for models without access to knowledge via retrieval-augmentation when shifting to unseen topics, which is indicative of the general trend that they do not generalize well to new inputs, a necessary skill for open-domain dialogue models. Models that can ground on knowledge, meanwhile, do not suffer from this problem nearly as much, as the</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WoW Test Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CMU_DoG Test Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Seq2Seq Model</td>
<td style="text-align: center;">Retrieval Mechanism</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
<td style="text-align: center;">B4</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
<td style="text-align: center;">B4</td>
<td style="text-align: center;">RL</td>
</tr>
<tr>
<td style="text-align: center;">BART-Large</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">18.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FiD</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">20.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RAG DPR</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">20.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RAG DPR-Poly</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">20.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FiD-RAG DPR</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FiD-RAG DPR-Poly</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">T5-Large</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RAG DPR</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FiD-RAG DPR</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of Seq2Seq Models and Retrieval Mechanisms on Unseen Distributions using WoW Test Unseen and our modified CMU_DoG Test Unseen split. Perplexity (PPL) values are not comparable across different seq2seq architectures as they use different dictionaries. Retrieval models are retrieving 5 documents over all of Wikipedia. All RAG models are RAG-Token.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Test Seen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Test Unseen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: left;">Knowledge Source</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">B4</td>
<td style="text-align: left;">RL</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">B4</td>
<td style="text-align: left;">RL</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BlenderBot (Boller et al., 2021)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">8.72</td>
<td style="text-align: left;">18.8</td>
<td style="text-align: left;">13</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">10.4</td>
<td style="text-align: left;">17.8</td>
<td style="text-align: left;">0.7</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BART (ours)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">14.7</td>
<td style="text-align: left;">20.9</td>
<td style="text-align: left;">1.7</td>
<td style="text-align: left;">20.3</td>
<td style="text-align: left;">18.9</td>
<td style="text-align: left;">18.7</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">18.4</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">DRD (Zhao et al., 2020a)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;">18.0</td>
<td style="text-align: left;">5.5</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">25.6</td>
<td style="text-align: left;">16.5</td>
<td style="text-align: left;">4.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">KIF (Fan et al., 2021)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">KIF (Fan et al., 2021)</td>
<td style="text-align: left;">WoW + Train Uits</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">*25.9</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">*22.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FiD-RAG (Ours)</td>
<td style="text-align: left;">Wikipedia (WoW Subset)</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">23.2</td>
<td style="text-align: left;">4.4</td>
<td style="text-align: left;">24.2</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">23.2</td>
<td style="text-align: left;">4.6</td>
<td style="text-align: left;">24.4</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG DPR-Poly (Ours)</td>
<td style="text-align: left;">Wikipedia (All)</td>
<td style="text-align: left;">11.4</td>
<td style="text-align: left;">22.9</td>
<td style="text-align: left;">3.9</td>
<td style="text-align: left;">23.5</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">3.4</td>
<td style="text-align: left;">22.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FiD-RAG DPR-Poly (Ours)</td>
<td style="text-align: left;">Wikipedia (All)</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">22.9</td>
<td style="text-align: left;">4.1</td>
<td style="text-align: left;">23.8</td>
<td style="text-align: left;">12.0</td>
<td style="text-align: left;">22.1</td>
<td style="text-align: left;">3.7</td>
<td style="text-align: left;">23.1</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 6: WoW Comparison to Existing Results. "WoW" knowledge source indicates the model choosing from a small set ( $\sim 61$ sentences) provided by the dataset for each dialogue turn. Methods with * augmented their knowledge source with training utterances, which is useful on Test Seen data, but likely not as useful on Unseen data. Our models use BART as the base seq2seq model; the RAG and FiD-RAG models retrieve 5 documents, and the FiD-RAG DPR-Poly model retrieves 25. Other prior models are compared in Table 14 in the Appendix.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Valid Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RAG Type</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
</tr>
<tr>
<td style="text-align: left;">Retrieve over Most Recent Turn</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: left;">Token</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">21.0</td>
</tr>
<tr>
<td style="text-align: left;">Retrieve over Full Dialogue Context</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence</td>
<td style="text-align: center;">$\mathbf{1 1 . 1}$</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">$\mathbf{1 2 . 6}$</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">$\mathbf{2 4 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Token</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">22.7</td>
</tr>
<tr>
<td style="text-align: left;">Turn-DTT</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">$\mathbf{2 8 . 0}$</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">24.3</td>
</tr>
<tr>
<td style="text-align: left;">Turn-DO</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">$\mathbf{2 3 . 1}$</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
<td style="text-align: center;">23.3</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparison of RAG Model Types on WoW Valid Seen/Unseen. Each retrieves 5 documents over all of Wikipedia. We set $T^{*}=1$ for RAG-Turn models. All models use BART as the base seq2seq model.
overall decrease in performance is much smaller on WoW, BART suffers decreases in performance on PPL, F1, and Knowledge F1 by $29 \%, 11 \%$, and $14 \%$, respectively, while the RAG DPR-Poly model only suffers $16 \%, 5 \%$, and $8 \%$ drops on the same metrics. Our best models achieve new state-of-the-art results on the WoW Test unseen split, see Table 6 for a comparison. Knowledge F1 scores remain quite high, with retrieval-augmented models generally decreasing performance the least with respect to this metric, indicating the augmentation can effectively retrieve knowledge on these topics.</p>
<h3>4.4 Augmenting Generation</h3>
<h3>4.4.1 Conditioning on turns of dialogue</h3>
<p>Table 7 compares our RAG-Turn methods described in Section 3.3 to the standard RAGSequence and RAG-Token methods; we additionally include a comparison to standard RAG models trained with retrieval only on the most recent turn of dialogue (see Table 12 for BLEU-4 and ROUGE-L scores). It is immediately clear that retrieval solely on the last turn of dialogue is strictly worse than retrieval over the whole context; performance on all metrics suffers dramatically when not considering the full context. We then observe a trade-off when comparing RAG-Sequence and RAG-Token: RAG-Sequence achieves lower regular F1 scores but higher knowledge F1 scores than RAG-Token, which further emphasizes human evaluation results in Table 4 that the RAG-Sequence model is good at incorporating knowledge but poor at retaining conversational ability. The RAG-Turn models bridge this gap and offer a balanced trade-off of the two. The RAG-Turn Doc-Then-Turn method yields F1 scores higher than the RAG-Sequence model, and higher Knowledge F1 scores than the RAG-Token model; the Doc-Only RAG-Turn method achieves the highest F1 on both the seen/unseen splits, and</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Valid Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: left;">FID-RAG</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">$\mathbf{3 0 . 0}$</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: left;">FID-RAG-Poly</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FID</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: left;">FID-RAG</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">$\mathbf{2 2 . 6}$</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">26.0</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison of retrievers used in FiD on WoW Valid (Seen/Unseen). Each retrieves 20 documents at train time, and 5 for inference. Perplexity (PPL) values are not comparable across different seq2seq architectures as they use different dictionaries.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Valid Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Retriever/Re-ranker</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
</tr>
<tr>
<td style="text-align: left;">TFIDF/</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">21.6</td>
</tr>
<tr>
<td style="text-align: left;">DPR/</td>
<td style="text-align: center;">$\mathbf{1 1 . 6}$</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">22.7</td>
</tr>
<tr>
<td style="text-align: left;">TFIDF/DPR</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">20.2</td>
</tr>
<tr>
<td style="text-align: left;">DPR/Poly</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">$\mathbf{2 3 . 0}$</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">$\mathbf{1 3 . 1}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 6}$</td>
<td style="text-align: center;">24.4</td>
</tr>
<tr>
<td style="text-align: left;">DPR/Poly (Joint)</td>
<td style="text-align: center;">$\mathbf{1 1 . 6}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 7 . 4}$</td>
<td style="text-align: center;">$\mathbf{1 3 . 1}$</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">$\mathbf{2 4 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">PolyFAISS/</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">20.6</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of re-rankers for BART RAGToken models on WoW Valid Seen/Unseen, using 5 retrieved documents.
improves on Knowledge F1 scores of the RAGToken model. For results with different $T^{*}$ values, as well as results with RAG-Turn Token and RAGTurn Sequence, see Section F and Table 13 in the appendix.</p>
<h3>4.4.2 Improving FiD-based generation</h3>
<p>Table 8 compares the usage of various retrievers in a FiD setup. It is clear that FiD is suboptimal out-of-the-box for knowledge-grounded dialogue, and incorporating retrievers trained via RAG improves performance considerably. Specifically, we see large decreases in perplexity, and significant gains in Knowledge F1: FiD-RAG-Poly, with BART, improves Knowledge F1 by $33 \%$ and $41 \%$ on the seen/unseen splits respectively; FiD-RAG with T5 sees gains of $37 \%$ and $25 \%$.</p>
<h3>4.5 Effectiveness of Retrieval Enhancements</h3>
<p>Table 9 outlines results on the WoW validation sets for our various retrieval/re-ranker augmentations. Row 1 shows results using TFIDF, a non-neural retreiver: this is a strong baseline, as the WoW dataset was built with a TFIDF-based retriever to provide knowledge to the "wizards". Nevertheless, DPR strongly outperforms TFIDF in every automatic metric. As for our neural-based methods, we see that using the code re-ranking approach via adding a Poly-encoder re-ranker on top of the standard DPR retriever for RAG yields the best per-
forming model with respect to automated metrics on both splits of the validation set. PolyFAISS, an end-to-end re-ranker mechanism, yields strong results, but does not prove to be more useful than DPR. Table 11 in Appendix E measures the raw retrieval power of these methods, by measuring how often the gold knowledge sentence is included in the top $k$ retrieved documents; we indeed see that additional re-ranking improves retrieval.</p>
<h3>4.6 Additional Ablations</h3>
<p>Due to space constraints, we provide several additional ablations in the Appendix. In Section I.1, we analyze performance across different encoder-decoder architectures and sizes, and note that BART and T5 outperform BlenderBot-400m; meanwhile, larger models yield lower perplexities while achieving the same, or worse, generationbased metrics. In Section I.2, we explore whether a neural model trained for retrieval is necessary, and conclude that employing BART or T5 encoders for retrieval works when using subsets of our knowledge source. In Section I. 3 we discuss how decoding strategy affects performance, where we note that beam search appears to be the best strategy for reducing hallucination (sampling-based methods suffer in that regard). In Section I. 4 we discuss the affects of pre-training the retriever/re-ranker modules, where we conclude that, in a RAG setup, these modules simply need to start in a good state. In Section I. 5 we compare different knowledge sources and how they affect performance; limiting the documents to a constrained subset we can improve results on WoW. Finally, in section I.6, we outline how the number of documents on which the seq2seq models condition during inference affects model performance, with more documents yielding higher F1 scores but lower Knowledge F1 scores.</p>
<h2>5 Discussion</h2>
<p>We have thus far explored several ways of retrieving and conditioning on documents in knowledgegrounded dialogue; here, we summarize some key takeaways from our results.</p>
<p>First, we note that the strength of the retrieval component is very important in downstream performance. Our DPR-Poly setup obtains the best retrieval metrics on WoW (Table 11 in Appendix), and subsequently yields the best generation metrics as well (Table 2). The FiD-RAG model clearly demonstrates the importance of a retriever tuned</p>
<p>for open-domain dialogue (Table 5).
Second, we note that models that condition on several documents simultaneously result in more engaging conversationalists; RAG-Token, RAGTurn, and FiD-RAG yield higher F1 scores (Table 7) and higher engaginginess/consistency scores (Table 4) than RAG-Sequence, while maintaining high knowledgeability; RAG-Turn, in certain configurations, demonstrates that conditioning on turns of dialogue independently yields benefits for automated metrics as well. We find the FiD architecture to be more optimal when considering several documents jointly (higher F1/KF1, lower humanevaluated hallucination) though we note that all models suffer from more hallucination when we condition on more documents for each generation (Table 4, Table 23 in Appendix).</p>
<p>Finally, we note that standard metrics used for open-domain dialogue are not sufficient for truly capturing hallucination within models; thus, metrics such as Knowledge F1 are required to further study model performance - Figure 2 in the Appendix highlights correlations between such automated metrics and human evaluations.</p>
<h2>6 Conclusion</h2>
<p>In this work, we have studied the problem of knowledge hallucination in conversational agents, an important problem as current systems often produce factually inaccurate generations. We have shown that this problem occurs independently of language model size or training data. Retrieval-augmented generation in particular is an intuitively promising solution to this problem, and in detailed experiments we have shown that this class of approaches significantly reduces the hallucination problem in dialogue while maintaing conversational ability, and can help generalize beyond the training data on previously unseen distributions. Future work should look for improved methods and to find solutions to unanswered questions, such as understanding the interplay between retrieved knowledge and knowledge stored in the model's weights.</p>
<h2>References</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977.</p>
<p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,</p>
<p>Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. arXiv preprint arXiv:2001.08435.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.
M. D. Bruyn, E. Lotfi, Jeska Buhmann, and W. Daelemans. 2020. Bart for knowledge grounded conversations. In Converse@KDD.</p>
<p>Yuanyuan Cai, M. Zuo, Qingchuan Zhang, Haitao Xiong, and Ke Li. 2020. A bichannel transformer with context encoding for document-driven conversation generation in social media. Complex., 2020:3710104:1-3710104:13.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, and et al. 2019a. The second conversational intelligence challenge (convai2). The Springer Series on Challenges in Machine Learning, page 187-208.</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019b. Wizard of wikipedia: Knowledge-powered conversational agents. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. 2021. Augmenting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computational Linguistics, 9:82-99.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Fabian Galetzka, Chukwuemeka Uchenna Eneh, and David Schlangen. 2020. A corpus of controlled opinionated and knowledgeable movie discussions for training neural conversation models. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 565-573, Marseille, France. European Language Resources Association.</p>
<p>Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. 2018. A knowledge-grounded neural conversation model. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5110-5117. AAAI Press.</p>
<p>Karthik Gopalakrishnan, Behnam Hedayatnia, Qinglang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tiir, and Amazon Alexa AI. 2019. Topical-chat: Towards knowledge-grounded open-domain conversations. In INTERSPEECH, pages 1891-1895.</p>
<p>Edouard Grave, Armand Joulin, and Nicolas Usunier. 2017. Improving neural language models with a continuous cache. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909.</p>
<p>Behnam Hedayatnia, Karthik Gopalakrishnan, Seokhwan Kim, Yang Liu, Mihail Eric, and Dilek Hakkani-Tur. 2020. Policy-driven neural response generation for knowledge-grounded dialog systems. In Proceedings of the 13th International Conference on Natural Language Generation, pages 412-421, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Gautier Izacard and Edouard Grave. 2021a. Distilling knowledge from reader to retriever for question answering. In International Conference on Learning Representations.</p>
<p>Gautier Izacard and Edouard Grave. 2021b. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest neighbor machine translation. In International Conference on Learning Representations.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Omar Khattab, Christopher Potts, and Matei Zaharia. 2020. Relevance-guided supervision for openqa with colbert.</p>
<p>Omar Khattab and Matei Zaharia. 2020. Colbert. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval.</p>
<p>Byeongchang Kim, Jaewoo Ahn, and Gunhee Kim. 2020. Sequential latent knowledge selection for knowledge-grounded dialogue. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
D. P. Kinga and J. Ba. 2015. A method for stochastic optimization. In International Conference on Learning Representations (ICLR), volume 5.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122-2132, Austin, Texas. Association for Computational Linguistics.</p>
<p>Longxuan Ma, Wei-Nan Zhang, Runxin Sun, and Ting Liu. 2020. A compare aggregate transformer for understanding document-grounded dialogue. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1358-1367, Online. Association for Computational Linguistics.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020. USR: An unsupervised and reference free evaluation metric for dialog generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681-707, Online. Association for Computational Linguistics.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Sabrina J Mielke, Arthur Szlam, Y-Lan Boureau, and Emily Dinan. 2020. Linguistic calibration through metacognition: aligning dialogue agent responses with expected correctness. arXiv preprint arXiv:2012.14983.</p>
<p>Alexander Miller, Will Feng, Dhruv Batra, Antoine Bordes, Adam Fisch, Jiasen Lu, Devi Parikh, and Jason Weston. 2017. ParlAI: A dialog research software platform. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 79-84, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019. Knowledge guided text retrieval and reading for open domain question answering.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523-2544, Online. Association for Computational Linguistics.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Peng Qi, Haejun Lee, OghenetegiriTGSido, and Christopher D. Manning. 2020. Retrieve, rerank, read, then iterate: Answering open-domain questions of arbitrary complexity from text. ArXiv, abs/2010.12527.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics.</p>
<p>Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. 2021. Recipes for building an open-domain chatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 300-325, Online. Association for Computational Linguistics.</p>
<p>Haitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019. PullNet: Open domain question answering with iterative retrieval on knowledge bases and text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 23802390, Hong Kong, China. Association for Computational Linguistics.</p>
<p>David Thulke, Nico Daheim, Christian Dugast, and Hermann Ney. 2021. Efficient retrieval augmented generation from unstructured knowledge for taskoriented dialog.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Ellen M. Voorhees and Dawn M. Tice. 2000. The TREC-8 question answering track. In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC'00), Athens, Greece. European Language Resources Association (ELRA).</p>
<p>Wenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Scott Yih, Sebastian Riedel, Douwe Kiela, and Barlas Oguz. 2021. Answering complex open-domain questions with multi-hop dense retrieval. In International Conference on Learning Representations.</p>
<p>Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. 2019. Improving question answering over incomplete KBs with knowledgeaware reader. In Proceedings of the 57th Annual</p>
<p>Meeting of the Association for Computational Linguistics, pages 4258-4264, Florence, Italy. Association for Computational Linguistics.</p>
<p>Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. 2021. Adaptive semiparametric language models. arXiv preprint arXiv:2102.02557.</p>
<p>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 9051-9062.</p>
<p>Xueliang Zhao, Wei Wu, Chongyang Tao, Can Xu, Dongyan Zhao, and Rui Yan. 2020a. Low-resource knowledge-grounded dialogue generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020. OpenReview.net.</p>
<p>Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, and Rui Yan. 2020b. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3377-3390, Online. Association for Computational Linguistics.</p>
<p>Yufan Zhao, Wei Wu, and Can Xu. 2020c. Are pretrained language models knowledgeable to ground open domain dialogues?</p>
<p>Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 13931404, Online. Association for Computational Linguistics.</p>
<p>Kangyan Zhou, Shrimai Prabhumoye, and Alan W Black. 2018. A dataset for document grounded conversations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 708-713, Brussels, Belgium. Association for Computational Linguistics.</p>
<h1>A Seq2Seq Model Descriptions</h1>
<p>BART The BART model (Lewis et al., 2020a) is a Transformer (Vaswani et al., 2017) that is a denoising auto-encoder trained with several noising techniques in order to learn a mapping from corrupted documents to their original representations. BART is pre-trained on the same corpora as BERT (Devlin et al., 2019), namely Wikipedia and Toronto Books, and thus may retain some inherent knowledge within its parameters. BART-Large, a 400 m parameter model, serves as the base seq2seq model for RAG in Lewis et al. (2020b), and so we consider it in our experiments.</p>
<p>T5 The T5 model (Raffel et al., 2020) proposes another method of pre-training Transformers for transfer learning, via converting several language tasks into "text-to-text" tasks. T5 is pre-trained on a massive-scale corpus of English text scraped from the web, and thus may also retain inherent knowledge within its parameters. T5-Base (220m parameters) and T5-Large (770m parameters) are both used in the FiD setup (Izacard and Grave, 2021b), and so we consider them in our experiments.</p>
<p>BlenderBot The BlenderBot model (Roller et al., 2021) is a large-scale open-domain dialogue model, pre-trained on dialogue data scraped from social discussions on the web (Baumgartner et al., 2020). Roller et al. (2021) release 90m, 2.7B, and 9.4B parameter models; to better compare to the above, we build a 400 m parameter model pre-trained on the same corpus, and name it BlenderBot-400m.</p>
<h2>B RAG-Turn Token \&amp; Sequence</h2>
<p>Retrieving documents for each turn $\mathbf{x}<em t="1">{t}$ can also be viewed as a way of boosting the total number of documents. We can thus try falling back to the standard RAG-Token and RAG-Sequence generator probabilities, by considering the union of all documents retrieved for each turn $\bigcup</em>}^{T} \mathbf{Z<em 1="1">{t}$, and the concatenation of all the turns in the context $\overline{\mathcal{X}}=\left[\mathbf{x}</em>\right]$ as before. We refer to these methods as RAG-Turn Token, and RAG-Turn Sequence. The generator probabilities for RAG-Turn Token and RAG-Turn Sequence are:} ; \ldots ; \mathbf{x}_{T</p>
<p>$$
\begin{gathered}
\mathbf{p}<em t="t">{\text {Turn-Token }}(\mathbf{y} \mid \overline{\mathcal{X}}) \approx \
\prod</em>}^{m} \sum_{\mathbf{z} \in \bigcup_{t=1}^{T} \mathbf{Z<em _eta="\eta">{t}} \mathbf{p}</em>}(\mathbf{z} \mid \overline{\mathcal{X}}) \mathbf{p<em _Turn-Sequence="{Turn-Sequence" _text="\text">{\theta}\left(y^{l} \mid \overline{\mathcal{X}}, \mathbf{z}, y^{1} \ldots y^{l-1}\right) \
\mathbf{p}</em>) \approx \
\sum_{\mathbf{z} \in \bigcup_{t=1}^{T} \mathbf{Z}}}(\mathbf{y} \mid \overline{\mathcal{X}<em _eta="\eta">{t}} \mathbf{p}</em>\right)
\end{gathered}
$$}(\mathbf{z} \mid \overline{\mathcal{X}}) \prod_{l}^{m} \mathbf{p}_{\theta}\left(y^{l} \mid \overline{\mathcal{X}}, \mathbf{z}, y^{1} \ldots y^{l-1</p>
<h2>C Dataset Details</h2>
<p>WoW consists of 22311 conversations (split into train, valid and test) over 1365 general topics, that range from e-books to toga parties to showers. Valid and test are split into seen and unseen versions for out-of-distribution topic evaluations, where the test unseen split contains 1000 dialogues with 58 new topics not discussed in the training data. CMU_DoG consists of 4112 conversations and focuses on the domain of movies. We note that the original setup of CMU_DoG involves models being given a gold knowledge paragraph in addition to the dialogue, but in our work we use this dataset to consider the more difficult (and realistic) problem of being able to retrieve this knowledge, rather than it being provided. To similarly assess performance on seen vs. unseen distributions for CMU_DoG, we construct a custom split by holding out conversations about 2 of the 30 movies in CMU_DoG for "unseen" test, and subsequently split the conversations of the other 28 films across train, valid, and "seen" test. The results presented in the main text focus on these modified splits, with measurements on the original data split provided in Tables 16 and 17.</p>
<h2>D Training Details</h2>
<p>All models are trained in ParlAI $^{3}$ (Miller et al., 2017).</p>
<p>Training Models were trained using 4x32GB GPUs and mixed-precision training, evaluating every 1 quarter of a dataset epoch until validation perplexity did not improve for a certain number of validations. We used a batchsize of 16 and swept over learning rates between $5 \mathrm{e}-6$ and $1 \mathrm{e}-4$, using the Adam optimizer (Kinga and Ba, 2015) with a linear LR scheduler that reduced the LR when validation performance did not improve; we found that 1e-5 worked best for BART models, and 1e-4 worked best for T5 models.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Seen Test</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Unseen Test</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Method</td>
<td>PPL</td>
<td>F1</td>
<td>Knowledge F1</td>
<td>B4</td>
<td>RL</td>
<td>PPL</td>
<td>F1</td>
<td>Knowledge F1</td>
<td>B4</td>
<td>RL</td>
</tr>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Movie titles only</td>
<td>15.45</td>
<td>15.96</td>
<td>6.796</td>
<td>.7456</td>
<td>19.43</td>
<td>19.41</td>
<td>15.26</td>
<td>5.916</td>
<td>.5923</td>
<td>18.06</td>
</tr>
<tr>
<td>Gold passage + Full Context</td>
<td>14.20</td>
<td>15.64</td>
<td>8.637</td>
<td>.7698</td>
<td>19.58</td>
<td>15.32</td>
<td>15.86</td>
<td>7.222</td>
<td>.882</td>
<td>18.67</td>
</tr>
<tr>
<td>NQ + TQA retriever pre-training</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Rag-Token</td>
<td>12.87</td>
<td>15.59</td>
<td>8.003</td>
<td>.7886</td>
<td>20.53</td>
<td>14.94</td>
<td>15.78</td>
<td>7.158</td>
<td>.7306</td>
<td>20.57</td>
</tr>
<tr>
<td>DPR-Poly</td>
<td>12.77</td>
<td>14.93</td>
<td>9.087</td>
<td>.7053</td>
<td>21.02</td>
<td>14.54</td>
<td>15.23</td>
<td>7.457</td>
<td>.6872</td>
<td>20.35</td>
</tr>
<tr>
<td>FiD</td>
<td>12.77</td>
<td>15.66</td>
<td>7.854</td>
<td>.7472</td>
<td>21.49</td>
<td>15.12</td>
<td>14.83</td>
<td>7.776</td>
<td>.5541</td>
<td>20.01</td>
</tr>
<tr>
<td>FiD-DPR</td>
<td>12.41</td>
<td>15.25</td>
<td>9.901</td>
<td>.7436</td>
<td>21.76</td>
<td>14.98</td>
<td>14.36</td>
<td>9.071</td>
<td>.5376</td>
<td>20.49</td>
</tr>
<tr>
<td>Wizard of Wikipedia retriever pre-training</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Rag-Token</td>
<td>13.05</td>
<td>15.22</td>
<td>8.253</td>
<td>.7151</td>
<td>20.62</td>
<td>15.25</td>
<td>15.52</td>
<td>7.202</td>
<td>.7502</td>
<td>20.95</td>
</tr>
<tr>
<td>DPR-Poly</td>
<td>12.71</td>
<td>15.21</td>
<td>8.307</td>
<td>.7452</td>
<td>20.91</td>
<td>14.48</td>
<td>15.11</td>
<td>7.65</td>
<td>.6476</td>
<td>20.40</td>
</tr>
<tr>
<td>FiD</td>
<td>12.79</td>
<td>15.64</td>
<td>8.318</td>
<td>.8149</td>
<td>22.14</td>
<td>15.11</td>
<td>15.07</td>
<td>7.317</td>
<td>.5711</td>
<td>20.32</td>
</tr>
<tr>
<td>FiD-DPR</td>
<td>12.24</td>
<td>15.33</td>
<td>9.052</td>
<td>.7994</td>
<td>21.54</td>
<td>14.47</td>
<td>14.64</td>
<td>8.686</td>
<td>.6849</td>
<td>20.42</td>
</tr>
</tbody>
</table>
<p>Table 10: Comparison of Architectures on CMU_DoG Seen/Unseen. BART is used as the base Seq2Seq Model.</p>
<p>Inference We attempted to optimize the decoding parameters of the models in the same way on the validation set to optimize decoding strategy this included sweeping over beam size, minimum beam length, and beam/context blocking, and used F1 to measure performance. For the vast majority of results, we employ beam search with a minimum beam length of 20 and a beam size of 3 , with tri-gram beam blocking.</p>
<p>Wikipedia and FAISS To index the Wikipedia passage embeddings, we used the Hierarchical Navigable Small World graph exploration (HNSW) variant of a FAISS index (i.e., IndexHNSWFlat ${ }^{4}$ ), with an $M$ value (number of graph links in HNSW) of 128. The FAISS index requires 80 GB of RAM to load.</p>
<h2>E Retriever Performance</h2>
<p>We measure the performance of the various retrievers considered by evaluating how often the top document retrieved is the correct document or in the top 5; that is, how often the gold knowledge sentence used in WoW is contained within the passage retrieved. Results are in Table 11.</p>
<h2>F RAG Turn Further Explorations</h2>
<p>We compare different values for $T^{*}$, the effective number of context turns considered by RAG-Turn, in Table 13. We note that perplexity values in general increase, while generation statistics stay roughly the same or drop slightly. Knowledge F1 stays roughly the same, with marginal increases or decreases depending on the model.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>G Automated Metrics and Human Evaluation</h2>
<p>Rare F1: When comparing texts, F1 can be inflated by exploiting common unigrams (Dinan et al., 2019a). We attempt to rectify this by only considering words that are infrequent in the dataset when calculating F1. We define a word as infrequent if it is in the lower half of the cumulative frequency distribution of the reference corpus. For each dataset, our reference corpus was all human messages across all splits. We find some correlation between this metric and Knowledge F1 for WoW (see Table 1). We note that Knowledge F1 is only available for datasets with labeled gold knowledge, whereas Rare F1 can always be computed.</p>
<p>We calculate the Pearson correlation coefficient between human evaluations and various automated metrics, visualized in Figure 2. The models considered are those listed in Table 4. We find that improvements in PPL, Knowledge F1, and Rare F1 correlate with an increase in the perceived knowledge use and a reduction in hallucination. F1 had relatively low correlation with all of the human evaluation criteria considered.</p>
<h2>H Additional Retrieval Variants</h2>
<h2>H. 1 ColBERT</h2>
<p>Khattab and Zaharia (2020) propose ColBERT as a method of computing contextualized late-stage interaction between the context and candidate representations to improve ranking capabilities, and indeed the method is extended to downstream generative QA models in Khattab et al. (2020). The key to ColBERT is a maxsim operation, in which the Transformer outputs of the context encoder are compared to all outputs of the candidate encoder, with the final score being a sum of the maximum similarity scores for each context output. The au-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Retriever</th>
<th style="text-align: left;">Retriever</th>
<th style="text-align: left;">Valid Seen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Unseen</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Retriever</td>
<td style="text-align: left;">Pre-Training</td>
<td style="text-align: left;">Fine-Tuning</td>
<td style="text-align: left;">R@1</td>
<td style="text-align: left;">R@5</td>
<td style="text-align: left;">R@1</td>
<td style="text-align: left;">R@5</td>
</tr>
<tr>
<td style="text-align: left;">DPR</td>
<td style="text-align: left;">NQ + TQA</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: left;">5.8</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">4.9</td>
<td style="text-align: left;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">DPR</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: left;">13.1</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">17.5</td>
</tr>
<tr>
<td style="text-align: left;">DPR</td>
<td style="text-align: left;">NQ + TQA + WoW</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: left;">13.1</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">11.1</td>
<td style="text-align: left;">16.6</td>
</tr>
<tr>
<td style="text-align: left;">RAG-DPR</td>
<td style="text-align: left;">NQ + TQA</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">28.1</td>
<td style="text-align: left;">36.8</td>
<td style="text-align: left;">25.7</td>
<td style="text-align: left;">33.7</td>
</tr>
<tr>
<td style="text-align: left;">RAG-DPR</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">25.9</td>
<td style="text-align: left;">35.6</td>
<td style="text-align: left;">22.9</td>
<td style="text-align: left;">33.4</td>
</tr>
<tr>
<td style="text-align: left;">RAG-DPR</td>
<td style="text-align: left;">NQ + TQA + WoW</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">26.2</td>
<td style="text-align: left;">35.1</td>
<td style="text-align: left;">23.3</td>
<td style="text-align: left;">$\mathbf{3 4 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">DPR-Poly</td>
<td style="text-align: left;">NQ + TQA</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">$\mathbf{2 9 . 3}$</td>
<td style="text-align: left;">$\mathbf{3 7 . 6}$</td>
<td style="text-align: left;">$\mathbf{2 6 . 9}$</td>
<td style="text-align: left;">$\mathbf{3 4 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">PolyFAISS</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">32.0</td>
<td style="text-align: left;">19.7</td>
<td style="text-align: left;">28.3</td>
</tr>
<tr>
<td style="text-align: left;">ColBERT</td>
<td style="text-align: left;">MS-Marco</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">25.7</td>
<td style="text-align: left;">33.3</td>
<td style="text-align: left;">27.5</td>
<td style="text-align: left;">33.8</td>
</tr>
<tr>
<td style="text-align: left;">ColBERT</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">26.1</td>
<td style="text-align: left;">33.6</td>
<td style="text-align: left;">26.4</td>
<td style="text-align: left;">33.7</td>
</tr>
<tr>
<td style="text-align: left;">ReGReT (Separate)</td>
<td style="text-align: left;">NQ + TQA</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">25.3</td>
<td style="text-align: left;">35.1</td>
<td style="text-align: left;">24.0</td>
<td style="text-align: left;">32.5</td>
</tr>
<tr>
<td style="text-align: left;">ReGRet (Same)</td>
<td style="text-align: left;">NQ + TQA</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">26.6</td>
<td style="text-align: left;">35.7</td>
<td style="text-align: left;">23.7</td>
<td style="text-align: left;">33.2</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparison of Retrieval Ability of Architectures on WoW Valid Seen/Unseen. Each model retrieves 5 documents from an unstructured document set of 21 m 100-word passages in Wikipedia. We measure passage Recall@k (R@k) measures how often the gold sentence used by the wizard is contained in the top k retrieved documents. All models use BART as a base seq2seq model</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Valid Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RAG Type</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Knowledge F1</td>
<td style="text-align: center;">B4</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Knowledge F1</td>
<td style="text-align: center;">B4</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Retrieve over Most Recent Turn</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: left;">Token</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: left;">Retrieve over Full Dialogue Context</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">$\mathbf{1 2 . 6}$</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{2 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{2 . 9}$</td>
<td style="text-align: center;">21.3</td>
</tr>
<tr>
<td style="text-align: left;">Token</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: left;">Turn-DTT</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">$\mathbf{2 8 . 0}$</td>
<td style="text-align: center;">$\mathbf{4 . 1}$</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: left;">Turn-DO</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">$\mathbf{2 3 . 1}$</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">$\mathbf{2 4 . 5}$</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">$\mathbf{2 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Turn-Tok</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">Turn-Seq</td>
<td style="text-align: center;">$\mathbf{1 0 . 9}$</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">$\mathbf{4 . 1}$</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">$\mathbf{1 2 . 6}$</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">20.3</td>
</tr>
</tbody>
</table>
<p>Table 12: Comparison of RAG Model Types on WoW Valid Seen/Unseen. Retrieval models are retrieving 5 documents over all of Wikipedia. We set $T^{*}=1$ for RAG-Turn models, i.e., the last turn is considered independently from the prior context turns. All models use BART as the base seq2seq model.
thors propose an end-to-end setup involving largescale search, where the token representations of all candidates are stored in a FAISS index, queries into the FAISS index are context outputs, and a re-ranking step using the maxsim operation is performed on a much smaller set of candidates. We implement this method for retrieval-augmented dialogue, and simply denote it as ColBERT.</p>
<h2>H. 2 Iterative Retrieval</h2>
<p>Several methods in the literature have shown that using iterative retrieval strategies is an effective way to improve retrieval (Khattab et al., 2020), distill knowledge from the retriever to the reader (Izacard and Grave, 2021a), and boost performance in multi-hop or complex QA settings (Xiong et al., 2021; Qi et al., 2020). Applying a similar technique to dialogue is easily motivated; intuitively, assuming one has an appropriately expressive generative model, retrieval conditioned on the output of the generator (trained to predict the ground truth response $\mathbf{y}$ ) should surface relevant facts for the conversation. We thus consider an architecture that
involves two rounds of retrieval and generation, where the second round retrieves according to the generated output of the first round; the model is trained to predict target labels taking into account both stages. We denote this model ReGReT (retrieve, generate, retrieve, tune), and note that one could use the same model for both rounds (ReGReT Same) or a separate model for both rounds (ReGReT Sep).</p>
<h2>H. 3 Retriever-less Retrieval</h2>
<p>Recent work has demonstrated that large pretrained models have some capacity to store knowledge within their parameters (Petroni et al., 2019; Roberts et al., 2020); some have shown that model representations themselves can be used nearly out-of-the-box for nearest neighbor retrieval of relevant contexts to help in language modeling (Khandelwal et al., 2020), machine translation (Khandelwal et al., 2021), and grounded dialogue (Fan et al., 2021). We explore the efficacy of BART and T5 at encoding knowledge via utilizing their encoders directly to encode both $\mathbf{q}\left(\mathbf{x}<em j="j">{i}\right)$ and $\mathbf{d}\left(\mathbf{z}</em>\right)$, allowing</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Seen</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Valid Unseen</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RAG Turn Type</td>
<td style="text-align: left;">$T^{*}$</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: right;">F1</td>
<td style="text-align: right;">Knowledge F1</td>
<td style="text-align: right;">B4</td>
<td style="text-align: right;">RL</td>
<td style="text-align: right;">PPL</td>
<td style="text-align: right;">F1</td>
<td style="text-align: right;">Knowledge F1</td>
<td style="text-align: right;">B4</td>
<td style="text-align: right;">RL</td>
</tr>
<tr>
<td style="text-align: left;">Doc then Turn</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">11.8</td>
<td style="text-align: right;">21.9</td>
<td style="text-align: right;">27.7</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">23.2</td>
<td style="text-align: right;">13.6</td>
<td style="text-align: right;">21.1</td>
<td style="text-align: right;">24.3</td>
<td style="text-align: right;">2.7</td>
<td style="text-align: right;">21.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">12.1</td>
<td style="text-align: right;">21.7</td>
<td style="text-align: right;">27.3</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">22.9</td>
<td style="text-align: right;">13.8</td>
<td style="text-align: right;">20.8</td>
<td style="text-align: right;">24.3</td>
<td style="text-align: right;">2.6</td>
<td style="text-align: right;">21.2</td>
</tr>
<tr>
<td style="text-align: left;">Doc Only</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">13.3</td>
<td style="text-align: right;">23.1</td>
<td style="text-align: right;">26.8</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">24.5</td>
<td style="text-align: right;">15.5</td>
<td style="text-align: right;">22.0</td>
<td style="text-align: right;">23.3</td>
<td style="text-align: right;">2.6</td>
<td style="text-align: right;">22.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">14.4</td>
<td style="text-align: right;">22.7</td>
<td style="text-align: right;">27.1</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">24.1</td>
<td style="text-align: right;">16.7</td>
<td style="text-align: right;">21.9</td>
<td style="text-align: right;">22.8</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">22.3</td>
</tr>
<tr>
<td style="text-align: left;">Token</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">11.5</td>
<td style="text-align: right;">21.0</td>
<td style="text-align: right;">24.3</td>
<td style="text-align: right;">3.1</td>
<td style="text-align: right;">21.6</td>
<td style="text-align: right;">13.2</td>
<td style="text-align: right;">20.5</td>
<td style="text-align: right;">21.5</td>
<td style="text-align: right;">2.0</td>
<td style="text-align: right;">20.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">11.7</td>
<td style="text-align: right;">22.3</td>
<td style="text-align: right;">25.2</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">23.0</td>
<td style="text-align: right;">13.9</td>
<td style="text-align: right;">21.1</td>
<td style="text-align: right;">20.8</td>
<td style="text-align: right;">2.3</td>
<td style="text-align: right;">20.8</td>
</tr>
<tr>
<td style="text-align: left;">Sequence</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">10.9</td>
<td style="text-align: right;">21.5</td>
<td style="text-align: right;">27.8</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">22.9</td>
<td style="text-align: right;">12.6</td>
<td style="text-align: right;">19.5</td>
<td style="text-align: right;">23.5</td>
<td style="text-align: right;">2.6</td>
<td style="text-align: right;">20.3</td>
</tr>
</tbody>
</table>
<p>Table 13: Comparison of $T^{*}$ Values For RAG-Turn on WoW Valid Seen/Unseen. All models use BART as a base seq2seq model, and retrieve 5 documents over all of Wikipedia.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PPL</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Knowledge F1</th>
<th style="text-align: center;">Rare Word F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Consistency</td>
<td style="text-align: center;">-0.3</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">Engaging</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">-0.66</td>
<td style="text-align: center;">-0.4</td>
</tr>
<tr>
<td style="text-align: left;">Knowledge</td>
<td style="text-align: center;">-0.82</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">Hallucinate</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">-0.86</td>
<td style="text-align: center;">-0.96</td>
<td style="text-align: center;">-0.84</td>
</tr>
</tbody>
</table>
<p>Figure 2: Correlation of Automatic Metrics with Human Judgments. We plot the Pearson correlation coefficient between the human evaluations from Table 4 and automated metrics from the WoW Valid Unseen data. We observe correlation between the Knowledge F1 and Rare F1 metrics with Knowledge and Hallucination human evaluations, especially when compared to standard F1.
the full RAG model to propagate error from the token losses to the encoder seen as a retriever and as a generator, thus removing the requirement of training and deploying a completely separate Transformer model for that goal. We draw inspiration from the ColBERT setup, and use encoder outputs as queries into FAISS, with a maxsim operation computing final documents scores $\mathbf{p}<em j="j">{0}\left(\mathbf{z}</em>\right)$. We refer to this model as BREAD (BART-Retriever-Encoder-And-Decoder) for BART-based models, and TREAD for T5-based models.} \mid \mathbf{x}_{i</p>
<h2>I Additional Relevant Ablations</h2>
<p>We outline several more important questions when considering these models.</p>
<h2>I. 1 Do different encoder-decoder architectures affect performance?</h2>
<p>Table 18 presents results on WoW comparing across different encoder-decoder architectures and sizes.</p>
<p>Architecture Comparison BART and T5 are comparable in their performance when holding the retrieval aspect constant. While perplexity measures are not directly comparable due to dictionary differences, we see that generations from the mod-
els yield roughly the same generation metric results. BlenderBot-400m performs comparably worse to T5 and Bart.</p>
<p>Size Comparison With larger models we tend to see a decrease in perplexity, indicating that these models become more fluent with respect to the dataset; however, generation statistics remain roughly constant. In fact, for the BlenderBot models, increasing model size leads to decreasing performance in the Knowledge F1 metric. This result further motivates the need for additional metrics beyond the standard ones when measuring prowess on dialogue-based tasks. One hypothesis here is that the large model is sacrificing knowledge use by instead relying on its conversational fluency (given that its perplexity is significantly lower).</p>
<h2>I. 2 Is a neural model trained for retrieval necessary?</h2>
<p>Table 19 shows the efficacy of retriever-less retrieval, comparing across different sources of knowledge. When limiting the knowledge base to all topics from Wikipedia that are present in the WoW dataset - comprising 500k tokens across 3 k documents - the BREAD (BART-Retriever-Encoder-And-Decoder) model obtains similar per-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Test Seen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Test Unseen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: left;">Knowledge Source</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">B4</td>
<td style="text-align: left;">RL</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">B4</td>
<td style="text-align: left;">RL</td>
</tr>
<tr>
<td style="text-align: left;">BlenderBot (Koller et al., 2021)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">8.72</td>
<td style="text-align: left;">18.8</td>
<td style="text-align: left;">1.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">10.4</td>
<td style="text-align: left;">17.8</td>
<td style="text-align: left;">0.7</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BART (ours)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">14.7</td>
<td style="text-align: left;">20.9</td>
<td style="text-align: left;">1.7</td>
<td style="text-align: left;">20.3</td>
<td style="text-align: left;">18.9</td>
<td style="text-align: left;">18.7</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">18.4</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2 Finetune (Zhao et al., 2020a)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">14.4</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">18.9</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">0.8</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E Transformer MemNet (Dinan et al., 2019b)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">63.5</td>
<td style="text-align: left;">16.9</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">97.3</td>
<td style="text-align: left;">14.4</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">DRD (Zhao et al., 2020a)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;">18.0</td>
<td style="text-align: left;">5.5</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">25.6</td>
<td style="text-align: left;">16.5</td>
<td style="text-align: left;">4.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Two-Stage Transformer MemNet (Dinan et al., 2019b)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">46.5</td>
<td style="text-align: left;">18.9</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">84.8</td>
<td style="text-align: left;">17.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">DialoGPT Finetune (Zhao et al., 2020c)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">16.2</td>
<td style="text-align: left;">19.0</td>
<td style="text-align: left;">2.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">20.4</td>
<td style="text-align: left;">17.6</td>
<td style="text-align: left;">3.2</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SKT (Kim et al., 2020)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">52.0</td>
<td style="text-align: left;">19.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">81.4</td>
<td style="text-align: left;">16.1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BART FK (Braun et al., 2020)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">12.2</td>
<td style="text-align: left;">20.1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">14.9</td>
<td style="text-align: left;">19.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">KnowledGPT (Zhao et al., 2020b)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">19.2</td>
<td style="text-align: left;">22.0</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">22.3</td>
<td style="text-align: left;">20.5</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">KIF (Fan et al., 2021)</td>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">KIF (Fan et al., 2021)</td>
<td style="text-align: left;">WoW + Train Utts</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">*25.9</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">*22.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FiD-RAG (Ours)</td>
<td style="text-align: left;">Wikipedia (WoW Subset)</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">23.2</td>
<td style="text-align: left;">4.4</td>
<td style="text-align: left;">24.2</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">23.2</td>
<td style="text-align: left;">4.6</td>
<td style="text-align: left;">24.4</td>
</tr>
<tr>
<td style="text-align: left;">RAG DPR-Poly (Ours)</td>
<td style="text-align: left;">Wikipedia (All)</td>
<td style="text-align: left;">11.4</td>
<td style="text-align: left;">22.9</td>
<td style="text-align: left;">3.9</td>
<td style="text-align: left;">23.5</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">3.4</td>
<td style="text-align: left;">22.3</td>
</tr>
<tr>
<td style="text-align: left;">FiD-RAG DPR-Poly (Ours)</td>
<td style="text-align: left;">Wikipedia (All)</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">22.9</td>
<td style="text-align: left;">4.1</td>
<td style="text-align: left;">23.8</td>
<td style="text-align: left;">12.0</td>
<td style="text-align: left;">22.1</td>
<td style="text-align: left;">3.7</td>
<td style="text-align: left;">23.1</td>
</tr>
</tbody>
</table>
<p>Table 14: WoW Comparison to Existing Results. "WoW" knowledge source indicates the model choosing from a small set ( $\sim 61$ sentences) provided by the dataset for each dialogue turn. Methods with * augmented their knowledge source with training utterances, which is useful on Test Seen data, but likely not as useful on Unseen data. Our models use BART as the base seq2seq model; the RAG and FiD-RAG models retrieve 5 documents, and the FiD-RAG DPR-Poly model retrieves 25.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Seen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Unseen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Retriever</td>
<td style="text-align: left;">Re-ranker</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">KF1</td>
<td style="text-align: left;">B4</td>
<td style="text-align: left;">RL</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">KF1</td>
<td style="text-align: left;">B4</td>
</tr>
<tr>
<td style="text-align: left;">TFIDF</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">13.1</td>
<td style="text-align: left;">21.6</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;">3.3</td>
<td style="text-align: left;">22.5</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">21.1</td>
<td style="text-align: left;">21.6</td>
<td style="text-align: left;">2.4</td>
</tr>
<tr>
<td style="text-align: left;">DPR</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">$\mathbf{1 1 . 6}$</td>
<td style="text-align: left;">22.5</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">4.0</td>
<td style="text-align: left;">23.5</td>
<td style="text-align: left;">13.4</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;">2.7</td>
</tr>
<tr>
<td style="text-align: left;">TFIDF</td>
<td style="text-align: left;">DPR</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">23.1</td>
<td style="text-align: left;">3.4</td>
<td style="text-align: left;">22.6</td>
<td style="text-align: left;">14.5</td>
<td style="text-align: left;">21.4</td>
<td style="text-align: left;">20.2</td>
<td style="text-align: left;">2.2</td>
</tr>
<tr>
<td style="text-align: left;">DPR</td>
<td style="text-align: left;">Polyencoder</td>
<td style="text-align: left;">11.7</td>
<td style="text-align: left;">$\mathbf{2 3 . 0}$</td>
<td style="text-align: left;">26.5</td>
<td style="text-align: left;">4.0</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">$\mathbf{1 3 . 1}$</td>
<td style="text-align: left;">$\mathbf{2 2 . 6}$</td>
<td style="text-align: left;">24.4</td>
<td style="text-align: left;">$\mathbf{3 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Joint DPR Poly</td>
<td style="text-align: left;">Polyencoder</td>
<td style="text-align: left;">$\mathbf{1 1 . 6}$</td>
<td style="text-align: left;">$\mathbf{2 3 . 0}$</td>
<td style="text-align: left;">$\mathbf{2 7 . 4}$</td>
<td style="text-align: left;">$\mathbf{4 . 3}$</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">$\mathbf{1 3 . 1}$</td>
<td style="text-align: left;">22.1</td>
<td style="text-align: left;">$\mathbf{2 4 . 7}$</td>
<td style="text-align: left;">3.1</td>
</tr>
<tr>
<td style="text-align: left;">PolyFAISS</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">12.1</td>
<td style="text-align: left;">22.9</td>
<td style="text-align: left;">24.8</td>
<td style="text-align: left;">3.7</td>
<td style="text-align: left;">23.6</td>
<td style="text-align: left;">14.2</td>
<td style="text-align: left;">21.6</td>
<td style="text-align: left;">20.6</td>
<td style="text-align: left;">2.5</td>
</tr>
<tr>
<td style="text-align: left;">ColBERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">12.4</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">25.3</td>
<td style="text-align: left;">3.3</td>
<td style="text-align: left;">23.1</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">21.9</td>
<td style="text-align: left;">$\mathbf{2 4 . 7}$</td>
<td style="text-align: left;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">BREAD</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">14.8</td>
<td style="text-align: left;">20.5</td>
<td style="text-align: left;">17.7</td>
<td style="text-align: left;">1.7</td>
<td style="text-align: left;">20.6</td>
<td style="text-align: left;">17.3</td>
<td style="text-align: left;">19.8</td>
<td style="text-align: left;">17.2</td>
<td style="text-align: left;">1.3</td>
</tr>
<tr>
<td style="text-align: left;">ReGReT (Sep)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">11.9</td>
<td style="text-align: left;">22.6</td>
<td style="text-align: left;">26.9</td>
<td style="text-align: left;">3.9</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">13.6</td>
<td style="text-align: left;">21.6</td>
<td style="text-align: left;">24.1</td>
<td style="text-align: left;">2.9</td>
</tr>
<tr>
<td style="text-align: left;">ReGReT (Same)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">12.0</td>
<td style="text-align: left;">22.6</td>
<td style="text-align: left;">25.9</td>
<td style="text-align: left;">4.0</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">21.5</td>
<td style="text-align: left;">23.2</td>
<td style="text-align: left;">2.7</td>
</tr>
</tbody>
</table>
<p>Table 15: Comparison of re-rankers for BART-based RAG-Token models on WoW Valid Seen/Unseen, using 5 retrieved documents.
formance to its DPR-retrieval counterpart. When scaling to the first two paragraphs of all topics from Wikipedia - comprising 1 billion tokens across 11 million documents, of the same order of magnitude as the full Wikipedia knowledge source we see a slight reduction in performance, but the BREAD model still effectively retrieves relevant information, and improves upon a no-retrieval baseline. However, when scaling to the full knowledge source - comprising 3 billion tokens over 21 million documents - we see that we are unable to surpass even a no-knowledge baseline; we hypothesize that the token-level similarities computed by the BREAD model become increasingly noisy as the knowledge source is scaled up: when a relevant Wikipedia article is spread across several "passages", as in our unstructured knowledge source dump, it becomes difficult for the BREAD model to identify precisely which sentence is relevant.</p>
<p>We find similar results when evaluating TREAD models on the smallest knowledge source listed in the previous paragraph. The TREAD mod-
els substantially outperform their non-retrievalaugmented counterparts (e.g., F1 and knowledge F1 improve from 19.3 and 14.6 without retrieval to 22.1 and 24.1 with TREAD, respectively, on the WoW Valid Seen split), however we do see that their RAG/FiD counterparts perform better in terms of knowledge F1 and perplexity.</p>
<h2>L3 Does the decoding strategy affect performance?</h2>
<p>We compare model outputs with various decoding strategies in Table 20. We compare three decoding methods: beam search, blocking repeated $n$-grams (we use $n=3$ ); nucleus sampling (Holtzman et al., 2020) with varying values of $p$; and top-k sampling (Fan et al., 2018) with $k=10$. We additionally compare whether to apply beam-blocking to the context, i.e., blocking repeated $n$-grams that appear in the dialogue context only - $n$-grams in the retrieved documents are not blocked.</p>
<p>We find that, across all retrieval schemes, beamblocking the dialogue context hurts performance</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Retrieval Mechanism</th>
<th style="text-align: center;">PPL</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Knowledge F1</th>
<th style="text-align: center;">BLEU-4</th>
<th style="text-align: center;">ROUGE-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">None</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">RAG DPR</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">RAG DPR-Poly</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">14.9</td>
</tr>
<tr>
<td style="text-align: left;">FiD-RAG DPR</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">15.7</td>
</tr>
</tbody>
</table>
<p>Table 16: Comparison of Retrieval Augmentations on CMU_DoG (Valid), original split. Retrieval models are retrieving over all of Wikipedia. All RAG models are RAG-Token and use BART as the base seq2seq model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">PPL</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">B4</th>
<th style="text-align: center;">RL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BART (ours)</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">16.9</td>
</tr>
<tr>
<td style="text-align: center;">CMU_DoG Knowledge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BCTCE (Cai et al., 2020)</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CAT (Ma et al., 2020)</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">11.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 Finetune (Zhao et al., 2020c)</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DRD (Zhao et al., 2020a)</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DialoGPT Finetune (Zhao et al., 2020c)</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">KnowledGPT (Zhao et al., 2020b)</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">All of Wikipedia</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RAG DPR-Poly (Ours)</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">16.9</td>
</tr>
<tr>
<td style="text-align: center;">FiD-RAG DPR-Poly (Ours)</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">17.1</td>
</tr>
</tbody>
</table>
<p>Table 17: CMU_DoG Comparison to Existing Results (Test), original data split. Our models use BART as the base seq2seq model. Both the RAG DPR-Poly model and FiD-RAG model retrieve 5 documents.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Valid Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2Seq</td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">BB-90m</td>
<td style="text-align: center;">90 m</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">21.1</td>
</tr>
<tr>
<td style="text-align: left;">BB-400m</td>
<td style="text-align: center;">400 m</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">19.9</td>
</tr>
<tr>
<td style="text-align: left;">BB-3B</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">20.9</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base</td>
<td style="text-align: center;">220 m</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">21.2</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">770 m</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: left;">BART-</td>
<td style="text-align: center;">400 m</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">21.8</td>
</tr>
<tr>
<td style="text-align: left;">Large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 18: Comparison between different seq2seq models (BlenderBot (BB), T5, and BART) on WoW Valid Seen/Unseen. All models use RAG-Token architectures with DPR Retrieval, retrieving 5 documents at inference time. Perplexity (PPL) values are not comparable across different generator architectures as they use different dictionaries.</p>
<ul>
<li>presumably because the model may be blocked from discussing named entities from prior context turns - with beam search yielding the highest F1 scores across the board. Despite the fact that beam search and nucleus sampling (with low $p$ ) yield comparable ROUGE-L and F1 scores, we see a noticeable difference in knowledge F1, implying that nucleus sampling may still be good at producing fluent/consistent generations while ultimately suffering increased hallucination. Using nucleus sampling with a higher $p$ value (which increases the variety of sampling) and using top-k sampling both result in poor relative performance for all four metrics, implying higher levels of hallucination and less coherent responses.</li>
</ul>
<h2>L4 Does retriever and/or re-ranker pre-training affect performance?</h2>
<p>We explore the effects of pre-training the neural retriever to help prime it for dialogue-based retrieval. To do so, we consider WoW knowledge selection as an appropriate pre-training task: given a dialogue context and a set of candidate knowledge sentences, choose the sentence on which to next ground a response. For standard RAG-DPR methods, we try both fine-tuning 1) a DPR model pre-trained on Natural Questions (Kwiatkowski et al., 2019) and Trivia QA (Joshi et al., 2017) and 2) a BERT model from scratch on the WoW knowledge selection task, and substitute these in for the standard QA-pre-trained DPR retriever from our base setup; we explore similar pre-training ablations with the ColBERT model. Results are in Table 21; we see minimal performance gains from such pre-training, and conclude that as long as the retriever is in a good state, it will work in the fine-tuning setup.</p>
<p>We see similar results when comparing pretraining strategies for the DPR-Poly re-ranker model in Table 21; pre-training the re-ranker does not yield noticeable downstream gains.</p>
<h2>L5 Does the source of knowledge matter?</h2>
<p>We explore the downstream effect of swapping in different sources of knowledge. Because the distribution of the topics within Wizard of Wikipedia is</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Sec</td>
<td style="text-align: center;">Arch</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KF1</td>
</tr>
<tr>
<td style="text-align: center;">BART</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">RAG-DPR</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">22.7</td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">FiD-RAG</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">20.4</td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">BREAD</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">17.2</td>
</tr>
<tr>
<td style="text-align: center;">B</td>
<td style="text-align: center;">RAG-DPR</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">23.7</td>
</tr>
<tr>
<td style="text-align: center;">B</td>
<td style="text-align: center;">FiD-RAG</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">22.9</td>
</tr>
<tr>
<td style="text-align: center;">B</td>
<td style="text-align: center;">BREAD</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">21.6</td>
</tr>
<tr>
<td style="text-align: center;">B</td>
<td style="text-align: center;">BREAD-FiD</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">23.4</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">RAG-DPR</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">FiD-RAG</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">27.9</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">BREAD</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">27.6</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">BREAD-FiD</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">26.2</td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">RAG-DPR</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">24.6</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">FiD-RAG</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">29.4</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">TREAD</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">22.9</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">TREAD-FiD</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">22.4</td>
</tr>
</tbody>
</table>
<p>Table 19: Comparison between DPR Retriever models (RAG and FiD) and "retriever-less" BREAD and TREAD models on WoW Valid Seen/Unseen, with varying knowledge sources: A: All of Wikipedia; B: First 2 paragraphs from all of Wikipedia; C: First two paragraphs from all articles covered by the WoW dataset. All models retrieve 5 documents during training and inference. Perplexity (PPL) values are not comparable across different seq2seq architectures as they use different dictionaries.
known, we can limit our model's source of knowledge to contain the smallest subset of Wikipedia yielding full coverage of the dataset, resulting in nearly 3000 documents from which to retrieve. As the retrieval task is now easier, we see noticeable performance gains when substituting this source of knowledge, see Table 22.</p>
<h2>L6 How does the number of documents retrieved/re-ranked affect performance?</h2>
<p>We conclude our ablation studies with an analysis on the number of documents retrieved. Table 23 outlines how each backbone architecture handles increasing the number of documents considered during inference.</p>
<p>For backbone architectures designed to consider several documents jointly - namely, RAG-Token and FiD-RAG - increasing the number of retrieved documents yields improvements in perplexity and F1 measures. However, we see substantial dropoffs in Knowledge F1 measures, which might imply that the models begin to hallucinate more and more, a claim that is supported in the human annotations, where we see in Table 4 that increasing the number of documents for these models yields higher levels of hallucination.</p>
<p>For RAG-Sequence models, which consider each document separately, increasing the number of re-
trieved documents improves perplexity measures and maintains both Knowledge F1 and BLEU measures; however, F1 scores appear to drop for any amount of documents beyond a single one. We hypothesize that by considering more and more generations we are effectively increasing the beam size and finding generations that match the knowledge more and more, while straying further away from engaging, dialogue-like responses; indeed, the RAG-Sequence model in Table 4 only uses 5 retrieved documents, and human evaluations indicate that the model still is less often engaging than its counterparts.</p>
<p>Overall, the number of re-ranked documents does not seem to improve performance substantially, so we land on 25 documents re-ranked to keep computational overhead to a minimum.</p>
<p>| Decoding Strategy | Context Block | No Retrieval | | | | RAG DPR-Poly | | | | FiD-RAG DPR-Poly | | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | | | F1 | KF1 | B4 | RL | F1 | KF1 | B4 | RL | F1 | KF1 | B4 | RL | | Beam | No | 20.9 | 17.6 | 1.7 | 20.7 | 23.1 | 26.5 | 4.0 | 24.0 | 22.8 | 27.8 | 4.1 | 24.1 | | Beam | Yes | 20.6 | 17.1 | 1.7 | 20.4 | 22.9 | 25.9 | 4.1 | 23.9 | 22.5 | 26.7 | 3.9 | 23.8 | | Nucleus: $p=0.3$ | No | 20.6 | 16.0 | 1.4 | 20.3 | 23.0 | 24.0 | 3.6 | 24.2 | 22.5 | 23.5 | 3.5 | 23.6 | | Nucleus: $p=0.3$ | Yes | 20.1 | 15.6 | 1.4 | 19.9 | 22.9 | 23.9 | 3.7 | 24.1 | 22.0 | 22.9 | 3.4 | 23.1 | | Nucleus: $p=0.9$ | No | 17.1 | 13.6 | 0.6 | 17.0 | 19.3 | 19.3 | 1.9 | 19.8 | 19.4 | 20.2 | 2.3 | 20.0 | | Nucleus: $p=0.9$ | Yes | 16.6 | 13.2 | 0.6 | 16.8 | 19.2 | 18.9 | 1.8 | 19.6 | 19.6 | 19.8 | 2.3 | 20.4 | | Top-k: $k=10$ | No | 18.0 | 14.4 | 0.7 | 18.0 | 19.8 | 19.0 | 1.8 | 20.3 | 20.2 | 19.9 | 2.2 | 20.8 | | Top-k: $k=10$ | Yes | 17.5 | 14.0 | 0.5 | 17.5 | 19.7 | 18.8 | 1.8 | 20.1 | 19.7 | 20.2 | 2.2 | 20.2 | | | | | | | | | | | | | | |</p>
<p>Table 20: Comparison of Decoding Strategies For models with and without retrieval-augmentation. Evaluations are conducted on the WoW Valid Seen. Retrieval models are retrieving 5 documents over all of Wikipedia. We set the minimum beam length to 20 , and block tri-grams during beam search. All models use BART as the base seq2seq model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Seen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Unseen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pre-training</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Data</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">KF1</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">KF1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">DPR</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">NQ + TQA</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">22.5</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">13.4</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">12.1</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;">26.2</td>
<td style="text-align: left;">13.4</td>
<td style="text-align: left;">22.1</td>
<td style="text-align: left;">24.4</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">NQ + TQA + WoW</td>
<td style="text-align: left;">12.1</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;">25.8</td>
<td style="text-align: left;">13.7</td>
<td style="text-align: left;">22.0</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ColBERT</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MS-Marco</td>
<td style="text-align: left;">12.4</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">25.3</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">21.9</td>
<td style="text-align: left;">24.7</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WoW</td>
<td style="text-align: left;">12.6</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">26.1</td>
<td style="text-align: left;">13.6</td>
<td style="text-align: left;">21.4</td>
<td style="text-align: left;">24.9</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">DPR-Poly and Joint DPR/Poly</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WikiTo</td>
<td style="text-align: left;">11.7</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;">26.5</td>
<td style="text-align: left;">13.1</td>
<td style="text-align: left;">22.6</td>
<td style="text-align: left;">24.4</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">NQ + TQA</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;">27.4</td>
<td style="text-align: left;">13.1</td>
<td style="text-align: left;">22.1</td>
<td style="text-align: left;">24.7</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 21: Comparison between different retriever/re-ranker pre-training schemes on WoW Valid Seen/Unseen. All models use BART as the base seq2seq model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Seen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Unseen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Src</td>
<td style="text-align: left;">Type</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">KF1</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">KF1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">A</td>
<td style="text-align: left;">P</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">22.5</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">13.4</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">B</td>
<td style="text-align: left;">P</td>
<td style="text-align: left;">10.9</td>
<td style="text-align: left;">23.2</td>
<td style="text-align: left;">27.9</td>
<td style="text-align: left;">12.4</td>
<td style="text-align: left;">22.4</td>
<td style="text-align: left;">23.7</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">B</td>
<td style="text-align: left;">S</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">22.3</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">15.5</td>
<td style="text-align: left;">21.5</td>
<td style="text-align: left;">20.1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">C</td>
<td style="text-align: left;">P</td>
<td style="text-align: left;">$\mathbf{1 0 . 7}$</td>
<td style="text-align: left;">$\mathbf{2 3 . 3}$</td>
<td style="text-align: left;">$\mathbf{2 8 . 3}$</td>
<td style="text-align: left;">$\mathbf{1 1 . 7}$</td>
<td style="text-align: left;">$\mathbf{2 3 . 0}$</td>
<td style="text-align: left;">$\mathbf{2 6 . 3}$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">C</td>
<td style="text-align: left;">S</td>
<td style="text-align: left;">12.8</td>
<td style="text-align: left;">22.2</td>
<td style="text-align: left;">24.8</td>
<td style="text-align: left;">14.4</td>
<td style="text-align: left;">21.5</td>
<td style="text-align: left;">21.7</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 22: Comparison between using different sources of knowledge on WoW Valid Seen/Unseen. All models are BART RAG-Token with DPR Retrieval. A: All of Wikipedia; B: first two paragraphs from all articles in Wikipedia; C: first two paragraphs from all articles in Wikipedia covering the WoW dataset. P: full passages are used; $\mathbf{S}$ : sentences are separate passages.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Seen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Valid Unseen</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Docs</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">KF1</td>
<td style="text-align: left;">PPL</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">KF1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG-Token</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">12.8</td>
<td style="text-align: left;">21.9</td>
<td style="text-align: left;">27.6</td>
<td style="text-align: left;">15.3</td>
<td style="text-align: left;">20.5</td>
<td style="text-align: left;">23.8</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">22.5</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">13.4</td>
<td style="text-align: left;">21.7</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">25</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">22.6</td>
<td style="text-align: left;">24.5</td>
<td style="text-align: left;">13.0</td>
<td style="text-align: left;">21.7</td>
<td style="text-align: left;">21.1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">50</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">22.4</td>
<td style="text-align: left;">23.9</td>
<td style="text-align: left;">13.0</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">20.6</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG-Sequence</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">22.1</td>
<td style="text-align: left;">27.4</td>
<td style="text-align: left;">14.6</td>
<td style="text-align: left;">21.1</td>
<td style="text-align: left;">24.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">11.1</td>
<td style="text-align: left;">21.5</td>
<td style="text-align: left;">27.9</td>
<td style="text-align: left;">12.6</td>
<td style="text-align: left;">20.3</td>
<td style="text-align: left;">24.6</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">25</td>
<td style="text-align: left;">10.6</td>
<td style="text-align: left;">21.3</td>
<td style="text-align: left;">27.8</td>
<td style="text-align: left;">11.4</td>
<td style="text-align: left;">20.0</td>
<td style="text-align: left;">24.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">50</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">21.2</td>
<td style="text-align: left;">27.8</td>
<td style="text-align: left;">11.2</td>
<td style="text-align: left;">19.9</td>
<td style="text-align: left;">24.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG-Turn-DTT</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">12.7</td>
<td style="text-align: left;">21.3</td>
<td style="text-align: left;">28.3</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">20.1</td>
<td style="text-align: left;">24.9</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">11.8</td>
<td style="text-align: left;">21.9</td>
<td style="text-align: left;">27.7</td>
<td style="text-align: left;">13.6</td>
<td style="text-align: left;">21.1</td>
<td style="text-align: left;">24.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">25</td>
<td style="text-align: left;">11.7</td>
<td style="text-align: left;">22.2</td>
<td style="text-align: left;">26.8</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">21.6</td>
<td style="text-align: left;">23.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">50</td>
<td style="text-align: left;">11.9</td>
<td style="text-align: left;">22.2</td>
<td style="text-align: left;">26.4</td>
<td style="text-align: left;">13.7</td>
<td style="text-align: left;">21.7</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG-Turn-DO</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">14.2</td>
<td style="text-align: left;">22.2</td>
<td style="text-align: left;">28.1</td>
<td style="text-align: left;">16.9</td>
<td style="text-align: left;">21.3</td>
<td style="text-align: left;">24.7</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">13.3</td>
<td style="text-align: left;">23.1</td>
<td style="text-align: left;">26.8</td>
<td style="text-align: left;">15.5</td>
<td style="text-align: left;">22.0</td>
<td style="text-align: left;">23.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">25</td>
<td style="text-align: left;">13.3</td>
<td style="text-align: left;">23.1</td>
<td style="text-align: left;">24.8</td>
<td style="text-align: left;">15.1</td>
<td style="text-align: left;">22.2</td>
<td style="text-align: left;">21.1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">50</td>
<td style="text-align: left;">13.3</td>
<td style="text-align: left;">22.6</td>
<td style="text-align: left;">23.7</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">22.0</td>
<td style="text-align: left;">20.0</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FiD-RAG</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">13.0</td>
<td style="text-align: left;">21.5</td>
<td style="text-align: left;">28.5</td>
<td style="text-align: left;">15.5</td>
<td style="text-align: left;">20.5</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">11.0</td>
<td style="text-align: left;">22.9</td>
<td style="text-align: left;">27.7</td>
<td style="text-align: left;">12.7</td>
<td style="text-align: left;">22.0</td>
<td style="text-align: left;">25.5</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">25</td>
<td style="text-align: left;">11.1</td>
<td style="text-align: left;">22.3</td>
<td style="text-align: left;">21.2</td>
<td style="text-align: left;">12.1</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;">22.3</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">50</td>
<td style="text-align: left;">11.7</td>
<td style="text-align: left;">21.4</td>
<td style="text-align: left;">18.0</td>
<td style="text-align: left;">12.6</td>
<td style="text-align: left;">22.1</td>
<td style="text-align: left;">19.1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">100</td>
<td style="text-align: left;">12.7</td>
<td style="text-align: left;">20.4</td>
<td style="text-align: left;">15.9</td>
<td style="text-align: left;">13.6</td>
<td style="text-align: left;">21.4</td>
<td style="text-align: left;">16.6</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 23: Comparison of the effect of conditioning over different numbers of documents at inference time for different models on WoW Valid Seen/Unseen. All models use a DPR retriever, with BART as the base seq2seq model.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://github.com/facebookresearch/faiss/wiki/Faissindexes&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
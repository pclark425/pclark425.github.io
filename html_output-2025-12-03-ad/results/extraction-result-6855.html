<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6855 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6855</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6855</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-263831328</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.06083v1.pdf" target="_blank">Transformers and Large Language Models for Chemistry and Drug Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology. In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration. The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language. A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language. As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6855.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6855.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LM Gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models for direct 3D structure generation (Flam-Shepherd & Aspuru-Guzik 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Report of language-model based approaches that generate full 3D atomic structures (XYZ/CIF/PDB) for small molecules, materials and protein binding sites, enabling generation beyond linear string representations and comparing favorably to state-of-the-art graph-based generators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer / language model (text generator repurposed for coordinate output)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this review; implies training on databases of 3D structures (crystal CIFs, PDB protein structures, molecule XYZ files) or converted 3D representations.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct generation of atomic coordinates/textual encoding of 3D files (XYZ/CIF/PDB) with a language model trained or fine-tuned to output coordinate formats.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>3D coordinate files (XYZ, CIF, PDB) rather than SMILES/SELFIES/graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo generation of molecules, materials and protein binding sites for design applications where 3D geometry is required (materials discovery, protein interface design, structure-based design).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not specified in the review; the work addresses representational constraints by directly outputting coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not specified here; implicit need for geometry validation/energy minimization tools but not described in review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not specified in review (implied: structural repositories such as PDB, crystallographic databases, materials databases).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Compared qualitatively/benchmarked against expert-designed graph-based generators; specific metrics not reported in this review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Stated as achieving performance comparable to expert-designed, state-of-the-art graph-based generation algorithms (no numeric metrics reported in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires accurate atomic positions and boundary conditions; increased complexity of representation and validation (physical plausibility, steric clashes, energetics); datasets and metrics for 3D generation are more complex/less standardized.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6855.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6855.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT: Molecular Generation Using a Transformer-Decoder Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-decoder model that generates molecules as linear strings (SMILES) in a language-modeling fashion for de novo molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolGPT: Molecular Generation Using a Transformer-Decoder Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-decoder generative model (LLM-style decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in review; typical training corpora are SMILES from chemical databases (ZINC/ChEMBL/PubChem) though specific dataset not stated here.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation from a decoder Transformer (autogressive text generation).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (linear string representation).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo small-molecule generation for drug discovery and related molecular design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not specified in the review summary (general concerns about validity and synthesizability noted elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not described in the review; typical pipelines would combine property predictors or filters but not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this review summary (common metrics for such models include validity/uniqueness/novelty/SAS/QED but not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Generation as SMILES can produce invalid strings and molecules with poor synthesizability; limited to molecule classes representable as SMILES (less suitable for macromolecules/materials).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6855.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6855.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C5T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>C5T5: Controllable Generation of Organic Molecules with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based method for controllable molecular generation enabling conditioning of generated molecules on desired attributes or controls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>C5T5: Controllable Generation of Organic Molecules with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>C5T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based generative model (likely decoder or conditional decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in review; implicit training on molecular SMILES corpora with attribute labels for controllable generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional/controlled sequence generation (transformer conditioned on control tokens or property descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (string tokens) or other string-based molecular language.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Controlled de novo design of organic molecules to meet property constraints or design objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Designed to accept control inputs for properties; specific filters not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not specified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in review (commonly: property-conditional validity, success at meeting control targets, diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Dependent on the expressiveness of conditioning signals and quality/coverage of training data; still inherits issues of string-based generation (validity, synthesizability).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6855.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6855.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer variant that performs both sequence generation and concurrent regression (property prediction) enabling property-conditioned molecular generation in a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer architecture combining sequence generation (decoder) and regression head</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in review; trained on paired molecule sequences and property labels (SMILES + property targets).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Concurrent sequence generation with regression signals, enabling conditioning on or optimizing predicted properties during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES or other molecular text representations used as sequence tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Property-conditioned de novo molecular design and multi-objective molecular optimisation in silico.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Implicitly enforces property constraints via regression conditioning; specific filters not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not specified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in review (typical metrics include success at achieving target property values, validity, novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires well-calibrated regression targets; success depends on quality/size of property-labeled datasets; may struggle with synthesizability constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6855.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6855.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 inverse design</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Is GPT-3 all you need for low-data discovery in chemistry? (Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study showing GPT-3 (a large pretrained LLM) can be fine-tuned or used via in-context learning to solve low-data chemistry and materials inverse design tasks, sometimes matching or exceeding specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is GPT-3 all you need for low-data discovery in chemistry?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Decoder-only large language model; used via fine-tuning and in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3 family - implicit in referenced GPT-3 models)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large-scale web/text corpora; fine-tuned on small task-specific chemistry/materials datasets (datasets not exhaustively enumerated in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning on small datasets and in-context learning (few-shot prompts) to perform inverse design and low-data regression tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Textual molecular representations (SMILES) and text descriptions; exact rep varies by task.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Inverse design (generate molecules with target properties), predicting molecular properties, low-data materials discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not specified in review; emphasis on working with limited labeled data rather than explicit synthesis constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not described in this review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Small, task-specific datasets used for fine-tuning or in-context examples; no specific dataset names provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported qualitatively in review: fine-tuning and in-context learning can match or outperform specialized algorithms on several low-data tasks; specific numeric metrics not given in the review text.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Qualitative claim: GPT-3 methods performed on par with, and sometimes outperformed, specialized chemistry/materials methods in low-data scenarios (no numeric values reported in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>LLMs can perform well with limited labeled data but rely on transfer from pretraining; potential for hallucinations and lack of grounding in domain-specific constraints (synthesizability, chemistry validity) unless mitigated.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6855.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6855.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context BO (Ramos et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization of Catalysts With In-context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>System that uses GPT models to perform regression with uncertainty estimates via in‑context learning, enabling Bayesian optimization workflows for catalyst and molecular optimization directly from synthesis-procedure text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian Optimization of Catalysts With In-context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT family models (unspecified exact GPT variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Decoder-only LLM used via in-context learning for regression with uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in review; method relies on pretrained LLMs plus in-context examples (no extensive fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context regression with an uncertainty-aware scheme to provide calibrated predictions enabling Bayesian optimization; uses textual descriptions (synthesis procedure) as input mapped to property predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Textual synthesis procedures and natural language descriptions rather than only SMILES/graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Catalyst optimization and molecular optimization (BO loop guided by LLM predictions from synthesis text).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Implicitly grounded by using synthesis procedure as input to address synthesizability; explicit constraints not enumerated in review.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Implements BO workflows that would require an optimizer and experimental evaluation loop; exact external tools not detailed in the review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not specified in review; examples drawn from catalyst/molecular optimization tasks and synthesis-procedure corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in review; typical BO metrics would include best-found objective over iterations, sample efficiency, regret, but not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Claimed demonstration of catalyst and molecular optimization using only synthesis-procedure input (no numeric performance metrics reported in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires uncertainty-calibrated regression; mapping from synthesis text to property space is novel but may be sensitive to variability in text quality and domain shift.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6855.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6855.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered agent system which composes and orchestrates computational chemistry tools to plan and execute chemistry tasks (design, retrosynthesis, synthesis planning), reducing hallucinations and grounding LLM outputs with reliable tool results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemCrow: Augmenting largelanguage models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Tool-using LLM agent / modular agent (LLM + tool orchestration)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (system integrates pretrained LLMs with curated computational chemistry tools rather than retraining them).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Agentic composition: LLM issues calls to curated external tools (retrievers, retrosynthesis planners, calculators) and synthesizes their outputs to generate candidate molecules and procedural plans.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES, natural language, and tool-specific formats (depends on invoked tools).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Retrosynthesis planning, automated synthesis planning, molecular and materials design assistance, tool-enabled property queries and optimization workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Grounding via tool outputs (e.g., retrosynthesis or supplier checks) provides implicit constraints such as synthetic accessibility and available building blocks; explicit constraint lists not enumerated in review.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Yes — curated set of computational chemistry tools (retrosynthesis planners, calculators, databases); central to the system.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>System-level: curated tool collection and databases used by the tools; specific dataset names not listed in review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported qualitatively: large positive effect on LLM performance for chemistry tasks by reducing hallucinations and grounding outputs; no numeric benchmark metrics provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires robust tool interfaces and reliable tools; composition of multiple tools can propagate tool-specific errors; quality depends on coverage and accuracy of integrated tools; safety/ethical concerns when enabling automated lab actions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6855.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6855.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-referencing embedded strings (SELFIES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 100% robust molecular string representation designed so that any valid SELFIES string decodes to a chemically valid molecule, addressing robustness issues of SMILES in generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Not a model (molecular representation/schema)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (representation format).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>When used with language models, generators output SELFIES tokens instead of SMILES to guarantee chemically valid molecules after decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SELFIES (string-based molecular encoding that ensures validity).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Generative molecular design in ML workflows (drug discovery, materials) where validity guarantees reduce invalid generation rate.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Implicit constraint: syntactic/chemical validity guaranteed by representation; does not ensure synthesizability or desirable properties.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Typically used with ML property predictors and filters, but specific tools not described in review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Representation can encode any molecules from standard databases (e.g., ZINC, ChEMBL) but dataset specifics not mentioned in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Described qualitatively as guaranteeing mapping of any such string to a valid molecule; numeric validity rates not needed because representation is provably valid.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>SELFIES guarantees valid decoding of any sequence in its alphabet (stated as 100% robustness in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Guarantees chemical graph validity but does not ensure synthetic accessibility, stereochemical correctness for all tasks, or relevance to application-specific property constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6855.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6855.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IR->SMILES Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leveraging Infrared Spectroscopy for Automated Structure Elucidation (Alberts et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer trained to map IR spectra (encoded as numerical sequences) and formula to SMILES strings, demonstrating that Transformers can learn to link experimental spectral modalities to molecular structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging Infrared Spectroscopy for Automated Structure Elucidation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder-decoder (spectra-to-string translation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Database of molecules with computationally generated IR spectra paired with SMILES/formula (constructed by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Translation: numerical IR spectrum sequence + formula encoded as text fed to Transformer that outputs SMILES strings representing candidate structures.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>IR spectra as numerical sequence + chemical formula input; outputs SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Structure elucidation from IR spectra (analytical identification), enabling links between experiments and generated molecular hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Input includes chemical formula to constrain the search; no additional filters mentioned in review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not specified in review; likely uses spectroscopic simulation tools to generate training spectra.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Author-compiled dataset of molecules with computational IR spectra (details in original paper; review gives no dataset names).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported metric: structure prediction accuracy and functional group prediction accuracy (functional group prediction outperformed prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Achieved 45% accuracy in structure prediction on the evaluated dataset and surpassed previous efforts on functional-group prediction (numeric functional-group improvement not detailed in review).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>IR spectra are complex and underused; model achieved moderate (45%) structure prediction accuracy — indicating substantial room for improvement and the difficulty of the task.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files <em>(Rating: 2)</em></li>
                <li>MolGPT: Molecular Generation Using a Transformer-Decoder Model <em>(Rating: 2)</em></li>
                <li>C5T5: Controllable Generation of Organic Molecules with Transformers <em>(Rating: 2)</em></li>
                <li>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling <em>(Rating: 2)</em></li>
                <li>Is GPT-3 all you need for low-data discovery in chemistry? <em>(Rating: 2)</em></li>
                <li>Bayesian Optimization of Catalysts With In-context Learning <em>(Rating: 2)</em></li>
                <li>ChemCrow: Augmenting largelanguage models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation <em>(Rating: 2)</em></li>
                <li>Leveraging Infrared Spectroscopy for Automated Structure Elucidation <em>(Rating: 1)</em></li>
                <li>A Transformer-based Generative Model for De Novo Molecular Design <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6855",
    "paper_id": "paper-263831328",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "3D-LM Gen",
            "name_full": "Language models for direct 3D structure generation (Flam-Shepherd & Aspuru-Guzik 2023)",
            "brief_description": "Report of language-model based approaches that generate full 3D atomic structures (XYZ/CIF/PDB) for small molecules, materials and protein binding sites, enabling generation beyond linear string representations and comparing favorably to state-of-the-art graph-based generators.",
            "citation_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "decoder-only transformer / language model (text generator repurposed for coordinate output)",
            "model_size": null,
            "training_data_description": "Not specified in this review; implies training on databases of 3D structures (crystal CIFs, PDB protein structures, molecule XYZ files) or converted 3D representations.",
            "generation_method": "Direct generation of atomic coordinates/textual encoding of 3D files (XYZ/CIF/PDB) with a language model trained or fine-tuned to output coordinate formats.",
            "chemical_representation": "3D coordinate files (XYZ, CIF, PDB) rather than SMILES/SELFIES/graphs.",
            "target_application": "De novo generation of molecules, materials and protein binding sites for design applications where 3D geometry is required (materials discovery, protein interface design, structure-based design).",
            "constraints_used": "Not specified in the review; the work addresses representational constraints by directly outputting coordinates.",
            "integration_with_external_tools": "Not specified here; implicit need for geometry validation/energy minimization tools but not described in review.",
            "dataset_used": "Not specified in review (implied: structural repositories such as PDB, crystallographic databases, materials databases).",
            "evaluation_metrics": "Compared qualitatively/benchmarked against expert-designed graph-based generators; specific metrics not reported in this review summary.",
            "reported_results": "Stated as achieving performance comparable to expert-designed, state-of-the-art graph-based generation algorithms (no numeric metrics reported in this review).",
            "experimental_validation": null,
            "challenges_or_limitations": "Requires accurate atomic positions and boundary conditions; increased complexity of representation and validation (physical plausibility, steric clashes, energetics); datasets and metrics for 3D generation are more complex/less standardized.",
            "uuid": "e6855.0"
        },
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
            "brief_description": "A Transformer-decoder model that generates molecules as linear strings (SMILES) in a language-modeling fashion for de novo molecular design.",
            "citation_title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "Transformer-decoder generative model (LLM-style decoder)",
            "model_size": null,
            "training_data_description": "Not specified in review; typical training corpora are SMILES from chemical databases (ZINC/ChEMBL/PubChem) though specific dataset not stated here.",
            "generation_method": "Direct SMILES generation from a decoder Transformer (autogressive text generation).",
            "chemical_representation": "SMILES strings (linear string representation).",
            "target_application": "De novo small-molecule generation for drug discovery and related molecular design tasks.",
            "constraints_used": "Not specified in the review summary (general concerns about validity and synthesizability noted elsewhere).",
            "integration_with_external_tools": "Not described in the review; typical pipelines would combine property predictors or filters but not specified here.",
            "dataset_used": null,
            "evaluation_metrics": "Not specified in this review summary (common metrics for such models include validity/uniqueness/novelty/SAS/QED but not reported here).",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Generation as SMILES can produce invalid strings and molecules with poor synthesizability; limited to molecule classes representable as SMILES (less suitable for macromolecules/materials).",
            "uuid": "e6855.1"
        },
        {
            "name_short": "C5T5",
            "name_full": "C5T5: Controllable Generation of Organic Molecules with Transformers",
            "brief_description": "Transformer-based method for controllable molecular generation enabling conditioning of generated molecules on desired attributes or controls.",
            "citation_title": "C5T5: Controllable Generation of Organic Molecules with Transformers",
            "mention_or_use": "mention",
            "model_name": "C5T5",
            "model_type": "Transformer-based generative model (likely decoder or conditional decoder)",
            "model_size": null,
            "training_data_description": "Not specified in review; implicit training on molecular SMILES corpora with attribute labels for controllable generation.",
            "generation_method": "Conditional/controlled sequence generation (transformer conditioned on control tokens or property descriptors).",
            "chemical_representation": "SMILES (string tokens) or other string-based molecular language.",
            "target_application": "Controlled de novo design of organic molecules to meet property constraints or design objectives.",
            "constraints_used": "Designed to accept control inputs for properties; specific filters not detailed here.",
            "integration_with_external_tools": "Not specified in review.",
            "dataset_used": null,
            "evaluation_metrics": "Not specified in review (commonly: property-conditional validity, success at meeting control targets, diversity).",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Dependent on the expressiveness of conditioning signals and quality/coverage of training data; still inherits issues of string-based generation (validity, synthesizability).",
            "uuid": "e6855.2"
        },
        {
            "name_short": "Regression Transformer",
            "name_full": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
            "brief_description": "A Transformer variant that performs both sequence generation and concurrent regression (property prediction) enabling property-conditioned molecular generation in a single model.",
            "citation_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
            "mention_or_use": "mention",
            "model_name": "Regression Transformer",
            "model_type": "Transformer architecture combining sequence generation (decoder) and regression head",
            "model_size": null,
            "training_data_description": "Not specified in review; trained on paired molecule sequences and property labels (SMILES + property targets).",
            "generation_method": "Concurrent sequence generation with regression signals, enabling conditioning on or optimizing predicted properties during generation.",
            "chemical_representation": "SMILES or other molecular text representations used as sequence tokens.",
            "target_application": "Property-conditioned de novo molecular design and multi-objective molecular optimisation in silico.",
            "constraints_used": "Implicitly enforces property constraints via regression conditioning; specific filters not detailed here.",
            "integration_with_external_tools": "Not specified in review.",
            "dataset_used": null,
            "evaluation_metrics": "Not specified in review (typical metrics include success at achieving target property values, validity, novelty).",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Requires well-calibrated regression targets; success depends on quality/size of property-labeled datasets; may struggle with synthesizability constraints.",
            "uuid": "e6855.3"
        },
        {
            "name_short": "GPT-3 inverse design",
            "name_full": "Is GPT-3 all you need for low-data discovery in chemistry? (Jablonka et al.)",
            "brief_description": "Study showing GPT-3 (a large pretrained LLM) can be fine-tuned or used via in-context learning to solve low-data chemistry and materials inverse design tasks, sometimes matching or exceeding specialized models.",
            "citation_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_type": "Decoder-only large language model; used via fine-tuning and in-context learning",
            "model_size": "175B (GPT-3 family - implicit in referenced GPT-3 models)",
            "training_data_description": "Pretrained on large-scale web/text corpora; fine-tuned on small task-specific chemistry/materials datasets (datasets not exhaustively enumerated in this review).",
            "generation_method": "Fine-tuning on small datasets and in-context learning (few-shot prompts) to perform inverse design and low-data regression tasks.",
            "chemical_representation": "Textual molecular representations (SMILES) and text descriptions; exact rep varies by task.",
            "target_application": "Inverse design (generate molecules with target properties), predicting molecular properties, low-data materials discovery tasks.",
            "constraints_used": "Not specified in review; emphasis on working with limited labeled data rather than explicit synthesis constraints.",
            "integration_with_external_tools": "Not described in this review summary.",
            "dataset_used": "Small, task-specific datasets used for fine-tuning or in-context examples; no specific dataset names provided here.",
            "evaluation_metrics": "Reported qualitatively in review: fine-tuning and in-context learning can match or outperform specialized algorithms on several low-data tasks; specific numeric metrics not given in the review text.",
            "reported_results": "Qualitative claim: GPT-3 methods performed on par with, and sometimes outperformed, specialized chemistry/materials methods in low-data scenarios (no numeric values reported in this review).",
            "experimental_validation": null,
            "challenges_or_limitations": "LLMs can perform well with limited labeled data but rely on transfer from pretraining; potential for hallucinations and lack of grounding in domain-specific constraints (synthesizability, chemistry validity) unless mitigated.",
            "uuid": "e6855.4"
        },
        {
            "name_short": "In-context BO (Ramos et al.)",
            "name_full": "Bayesian Optimization of Catalysts With In-context Learning",
            "brief_description": "System that uses GPT models to perform regression with uncertainty estimates via in‑context learning, enabling Bayesian optimization workflows for catalyst and molecular optimization directly from synthesis-procedure text.",
            "citation_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "mention_or_use": "mention",
            "model_name": "GPT family models (unspecified exact GPT variant)",
            "model_type": "Decoder-only LLM used via in-context learning for regression with uncertainty",
            "model_size": null,
            "training_data_description": "Not specified in review; method relies on pretrained LLMs plus in-context examples (no extensive fine-tuning).",
            "generation_method": "In-context regression with an uncertainty-aware scheme to provide calibrated predictions enabling Bayesian optimization; uses textual descriptions (synthesis procedure) as input mapped to property predictions.",
            "chemical_representation": "Textual synthesis procedures and natural language descriptions rather than only SMILES/graphs.",
            "target_application": "Catalyst optimization and molecular optimization (BO loop guided by LLM predictions from synthesis text).",
            "constraints_used": "Implicitly grounded by using synthesis procedure as input to address synthesizability; explicit constraints not enumerated in review.",
            "integration_with_external_tools": "Implements BO workflows that would require an optimizer and experimental evaluation loop; exact external tools not detailed in the review summary.",
            "dataset_used": "Not specified in review; examples drawn from catalyst/molecular optimization tasks and synthesis-procedure corpora.",
            "evaluation_metrics": "Not specified in review; typical BO metrics would include best-found objective over iterations, sample efficiency, regret, but not given here.",
            "reported_results": "Claimed demonstration of catalyst and molecular optimization using only synthesis-procedure input (no numeric performance metrics reported in this review).",
            "experimental_validation": null,
            "challenges_or_limitations": "Requires uncertainty-calibrated regression; mapping from synthesis text to property space is novel but may be sensitive to variability in text quality and domain shift.",
            "uuid": "e6855.5"
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large language models with chemistry tools",
            "brief_description": "An LLM-powered agent system which composes and orchestrates computational chemistry tools to plan and execute chemistry tasks (design, retrosynthesis, synthesis planning), reducing hallucinations and grounding LLM outputs with reliable tool results.",
            "citation_title": "ChemCrow: Augmenting largelanguage models with chemistry tools",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "Tool-using LLM agent / modular agent (LLM + tool orchestration)",
            "model_size": null,
            "training_data_description": "Not applicable (system integrates pretrained LLMs with curated computational chemistry tools rather than retraining them).",
            "generation_method": "Agentic composition: LLM issues calls to curated external tools (retrievers, retrosynthesis planners, calculators) and synthesizes their outputs to generate candidate molecules and procedural plans.",
            "chemical_representation": "SMILES, natural language, and tool-specific formats (depends on invoked tools).",
            "target_application": "Retrosynthesis planning, automated synthesis planning, molecular and materials design assistance, tool-enabled property queries and optimization workflows.",
            "constraints_used": "Grounding via tool outputs (e.g., retrosynthesis or supplier checks) provides implicit constraints such as synthetic accessibility and available building blocks; explicit constraint lists not enumerated in review.",
            "integration_with_external_tools": "Yes — curated set of computational chemistry tools (retrosynthesis planners, calculators, databases); central to the system.",
            "dataset_used": "System-level: curated tool collection and databases used by the tools; specific dataset names not listed in review summary.",
            "evaluation_metrics": "Reported qualitatively: large positive effect on LLM performance for chemistry tasks by reducing hallucinations and grounding outputs; no numeric benchmark metrics provided in this review.",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Requires robust tool interfaces and reliable tools; composition of multiple tools can propagate tool-specific errors; quality depends on coverage and accuracy of integrated tools; safety/ethical concerns when enabling automated lab actions.",
            "uuid": "e6855.6"
        },
        {
            "name_short": "SELFIES",
            "name_full": "Self-referencing embedded strings (SELFIES)",
            "brief_description": "A 100% robust molecular string representation designed so that any valid SELFIES string decodes to a chemically valid molecule, addressing robustness issues of SMILES in generative models.",
            "citation_title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "Not a model (molecular representation/schema)",
            "model_size": null,
            "training_data_description": "Not applicable (representation format).",
            "generation_method": "When used with language models, generators output SELFIES tokens instead of SMILES to guarantee chemically valid molecules after decoding.",
            "chemical_representation": "SELFIES (string-based molecular encoding that ensures validity).",
            "target_application": "Generative molecular design in ML workflows (drug discovery, materials) where validity guarantees reduce invalid generation rate.",
            "constraints_used": "Implicit constraint: syntactic/chemical validity guaranteed by representation; does not ensure synthesizability or desirable properties.",
            "integration_with_external_tools": "Typically used with ML property predictors and filters, but specific tools not described in review.",
            "dataset_used": "Representation can encode any molecules from standard databases (e.g., ZINC, ChEMBL) but dataset specifics not mentioned in this review.",
            "evaluation_metrics": "Described qualitatively as guaranteeing mapping of any such string to a valid molecule; numeric validity rates not needed because representation is provably valid.",
            "reported_results": "SELFIES guarantees valid decoding of any sequence in its alphabet (stated as 100% robustness in the cited work).",
            "experimental_validation": null,
            "challenges_or_limitations": "Guarantees chemical graph validity but does not ensure synthetic accessibility, stereochemical correctness for all tasks, or relevance to application-specific property constraints.",
            "uuid": "e6855.7"
        },
        {
            "name_short": "IR-&gt;SMILES Transformer",
            "name_full": "Leveraging Infrared Spectroscopy for Automated Structure Elucidation (Alberts et al.)",
            "brief_description": "Transformer trained to map IR spectra (encoded as numerical sequences) and formula to SMILES strings, demonstrating that Transformers can learn to link experimental spectral modalities to molecular structures.",
            "citation_title": "Leveraging Infrared Spectroscopy for Automated Structure Elucidation",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "Transformer encoder-decoder (spectra-to-string translation)",
            "model_size": null,
            "training_data_description": "Database of molecules with computationally generated IR spectra paired with SMILES/formula (constructed by the authors).",
            "generation_method": "Translation: numerical IR spectrum sequence + formula encoded as text fed to Transformer that outputs SMILES strings representing candidate structures.",
            "chemical_representation": "IR spectra as numerical sequence + chemical formula input; outputs SMILES.",
            "target_application": "Structure elucidation from IR spectra (analytical identification), enabling links between experiments and generated molecular hypotheses.",
            "constraints_used": "Input includes chemical formula to constrain the search; no additional filters mentioned in review summary.",
            "integration_with_external_tools": "Not specified in review; likely uses spectroscopic simulation tools to generate training spectra.",
            "dataset_used": "Author-compiled dataset of molecules with computational IR spectra (details in original paper; review gives no dataset names).",
            "evaluation_metrics": "Reported metric: structure prediction accuracy and functional group prediction accuracy (functional group prediction outperformed prior work).",
            "reported_results": "Achieved 45% accuracy in structure prediction on the evaluated dataset and surpassed previous efforts on functional-group prediction (numeric functional-group improvement not detailed in review).",
            "experimental_validation": false,
            "challenges_or_limitations": "IR spectra are complex and underused; model achieved moderate (45%) structure prediction accuracy — indicating substantial room for improvement and the difficulty of the task.",
            "uuid": "e6855.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files",
            "rating": 2,
            "sanitized_title": "language_models_can_generate_molecules_materials_and_protein_binding_sites_directly_in_three_dimensions_as_xyz_cif_and_pdb_files"
        },
        {
            "paper_title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "C5T5: Controllable Generation of Organic Molecules with Transformers",
            "rating": 2,
            "sanitized_title": "c5t5_controllable_generation_of_organic_molecules_with_transformers"
        },
        {
            "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
            "rating": 2,
            "sanitized_title": "regression_transformer_enables_concurrent_sequence_regression_and_generation_for_molecular_language_modelling"
        },
        {
            "paper_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "rating": 2,
            "sanitized_title": "is_gpt3_all_you_need_for_lowdata_discovery_in_chemistry"
        },
        {
            "paper_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_of_catalysts_with_incontext_learning"
        },
        {
            "paper_title": "ChemCrow: Augmenting largelanguage models with chemistry tools",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
            "rating": 2,
            "sanitized_title": "selfreferencing_embedded_strings_selfies_a_100_robust_molecular_string_representation"
        },
        {
            "paper_title": "Leveraging Infrared Spectroscopy for Automated Structure Elucidation",
            "rating": 1,
            "sanitized_title": "leveraging_infrared_spectroscopy_for_automated_structure_elucidation"
        },
        {
            "paper_title": "A Transformer-based Generative Model for De Novo Molecular Design",
            "rating": 1,
            "sanitized_title": "a_transformerbased_generative_model_for_de_novo_molecular_design"
        }
    ],
    "cost": 0.01808675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transformers and Large Language Models for Chemistry and Drug Discovery
9 Oct 2023</p>
<p>Andres M Bran 
Laboratory of Artificial Chemical Intelligence (LIAC)
ISIC
EPFL
1050LausanneVDSwitzerland</p>
<p>EPFL
National Centre of Competence in Research (NCCR) Catalysis
1050LausanneVDSwitzerland</p>
<p>Philippe Schwaller philippe.schwaller@epfl.ch 
Laboratory of Artificial Chemical Intelligence (LIAC)
ISIC
EPFL
1050LausanneVDSwitzerland</p>
<p>EPFL
National Centre of Competence in Research (NCCR) Catalysis
1050LausanneVDSwitzerland</p>
<p>Transformers and Large Language Models for Chemistry and Drug Discovery
9 Oct 2023FCD2F3DED513E77F7E69995F08D6E6A5arXiv:2310.06083v1[cs.LG]TransformersAccelerated discoveryLanguage ModelsRetrosynthesisComputational tools
Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology.In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration.The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language.A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language.As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
<p>Introduction</p>
<p>The capacity to process and accurately model human language has been a persistent pursuit within the machine learning community [1][2][3][4][5].The belief is that language is intrinsic to human reasoning capabilities, thus successful language modeling could open the door to numerous applications, enhancing various information processing tasks with the potential to revolutionize several industries [6,7].The field of natural language processing has witnessed significant advancements in recent years, thanks to improved computing infrastructure, breakthroughs in algorithms, and the proliferation of abundant and accessible data [8].Language and technical terminology also play a pivotal role in the domain of chemistry, which serves as the fundamental basis for drug discovery and development.Analogous to human language, understanding and accurately modeling the language of chemistry is crucial for effective research and development in the pharmaceutical industry.By applying the advancements in language modeling and processing from the machine learning community to the domain of chemistry, it is possible to facilitate drug discovery by efficiently analyzing and interpreting vast amounts of chemical data and literature.</p>
<p>Introduced in 2017, the Transformer architecture revolutionized natural language processing [5].The Transformer is a type of neural network initially developed for neural machine translation.In its original architecture, the Transformer consists of an encoder, which encodes a sentence in the source language (e.g., French) and a decoder, which wordby-word decodes the translated sentence in the target language (e.g.English).The power of the Transformer model comes from its core building blocks, so-called attention layers [1].Those attention layers are excellent at capturing the meaning of words and subwords in their context.For this to work, text is split into subwords and inputted into the Transformer, which encodes the sequence of subwords and learns to use its attention layers to connect relevant pieces of information.The Transformer then generates another sequence of tokens as output, using its decoder part.</p>
<p>The original Transformer and variants of it, such as encoder-based and decoder-based architectures, have shown remarkable results in a range of language modeling tasks, including translation [9], sentiment classification [10,11], and summarization [12,13], to name a few.Over time, the architecture of these models has evolved, with the most popular variations being decoder-only structures, which make up the foundation of many contemporary, large language models (LLMs), such as ChatGPT and GPT-4 [14].</p>
<p>Similar to human language, where the tokens -the subunits in which the text is broken down -are words and subwords, we can imagine splitting proteins and peptides into sequences of amino acids, or molecules and chemical reactions into sequences of atoms (Figure 1).Such analogies between human language and (bio)chemistry extended the influence of Transformers in fields far beyond the boundaries of natural language processing impacting numerous scientific disciplines.</p>
<p>A prime example of Transformers' applications is their pivotal role in AlphaFold2, a leading system for predicting the 3D structure of proteins [15].The emergence of AlphaFold2 not only addressed a long-standing challenge in biochemistry but also catalyzed a surge of research, thereby becoming one of the most prevalent tools in the biochemistry community [16,17].</p>
<p>Chemistry has also benefited from these advancements.Inspired by developments in other areas, researchers have adapted a number of chemical tasks in the form of text sequences (Figure 1).This, together with the rise of open datasets and benchmarks [18][19][20][21], sparked a revolution that started with clearly defined chemical challenges fundamental to the drug development process, like reaction outcome prediction and retrosynthetic planning.Models at this stage are trained with only molecules as both input and output, making them single-modal models.</p>
<p>The revolution then continued in a new direction, with attempts to include additional types of data like spectra from analytical techniques, and human language as in synthesis procedures, giving rise to a wave of chemical multimodal models.The current stage aims to definitively bridge the gap between the modeling of chemical and natural language (Figure 1).With applications based on large language models, researchers have demonstrated how natural language can serve as interfaces for chemical reasoning and task solving across fields.</p>
<p>We are now in an era marked by significant progress in a host of tasks that are fundamental to the drug development process, with systems that go beyond single task solving and are capable of implementing complete pipelines in this process, accelerating chemical research.The influence of these Transformer models in the realm of chemistry is thus profound, highlighting their pivotal role in shaping the future of chemistry and drug discovery.The next sections will briefly introduce textual representations of molecules and reactions, and then continue to discuss task-specific Transformers for single-modality and multi-modality tasks.Lastly, we will address large language models and potential uses in chemistry and drug discovery.</p>
<p>Natural</p>
<p>2 Modeling the language of organic chemistry Chemistry, in many respects, resembles a language [22].Not only is most of the information in chemistry conveyed through human language, but the rules governing chemical transformations also form a distinct language themselves.Accurately modeling this language advances our understanding of its rules, opening up applications such as automatic retrosynthetic planning and efficient chemical space exploration [23], while also shedding light on the intricate grammar of organic chemistry [24].</p>
<p>However, the chemical language is not a conventional language like English or Mandarin, which operate on text.In organic chemistry, grammar operates on a spectrum of molecular graphs and reaction conditions, making the direct application of Transformers to chemistry a challenge.Overcoming this obstacle is thus vital and, as we will explore, can be accomplished using linear string molecular representations that have been in use for decades, with new revisions and proposals in recent years [25][26][27][28][29][30]</p>
<p>Text Representations of Molecules and Reactions</p>
<p>Chemistry has long been acknowledged as a fundamentally inventorial science, wherein new molecules and reactions are discovered, analyzed, and to a certain extent cataloged in databases [31].Researchers rely not only on scientific articles and patents but also on resources like handbooks and, more recently, computational databases, to advance their work.To streamline the storage and querying of these data, simplified molecular-input line-entry system (SMILES) strings have been proposed and utilized since the 1980s [25].</p>
<p>The process of linearizing molecular graphs involves choosing an atom and sequentially enumerating all other atoms in the molecule from this point (Figure 2a).Special characters are assigned to specify bond types, branches, rings, stereochemistry, and other pertinent information for molecular representation.SMILES leverage the fact that molecular graphs adhere to certain chemical rules, such as different types of atoms needing to respect their valence.This makes it feasible to represent a large portion of organic chemistry as sequences of characters.</p>
<p>As molecular machine learning applications have emerged, the limitations of these representations -such as their lack of robustness-have become increasingly apparent.In generative models, this flaw can lead to the production of entirely invalid molecules, becoming an additional hurdle in molecular discovery applications [32,33].To mitigate this issue, researchers have introduced Self-Referencing Embedded Strings (SELFIES), a novel string-based representation that effectively addresses the robustness concern [28].This new representation has found numerous applications, particularly in drug discovery and molecular generation, due to its unique formulation that guarantees the correct mapping of any such string to a valid molecule.While other representations have been proposed in the past to standardize the language (IUPAC names, InChI [29]), or to address the limitations of current representations for deep learning applications (DeepSMILES [26]), we will not delve further into these representations in this discussion [28,34].</p>
<p>With these molecular text representations in hand, chemical reactions can easily be encoded by leveraging the syntax of chemical equations.In that sense, sets of molecules -e.g. the set of reactants-are represented by listing all molecules with a dot "." between them.Reactants are separated from the products using the symbol "&gt;", representing the arrow, and other details like catalysts and reagents are placed in between the "&gt;&gt;" symbol.The resulting reaction representation has the form "A.B&gt;catatlyst.reagent&gt;C.D", where A and B are reactants and C and D products.This notation is commonly used in SMILES and called a "reaction SMILES".</p>
<p>Task-specific Transformers</p>
<p>The conversion of chemical problems into sequences of tokens -in the form of a languagehas unlocked the transformative potential of the Transformer architecture within the chemistry domain.This shift has led to remarkable model performances in prediction tasks in</p>
<p>CNC(C)CC1=CC=C(OCO2)C2=C1 SMILES SELFIES InChI [C][N][Branch1][C][C][C][C][=C][C] [=C][C][Branch1][Ring2][=C] [Ring1][=Branch1][O][=C][O] [Ring1][=Branch1]</p>
<p>IUPAC name Fig. 2 The language of organic chemistry operates on molecular graphs, which can easily be converted into multiple text formats (a), facilitating the use of Transformers in their multiple forms, for a variety of tasks in Chemistry (b).</p>
<p>chemistry, such as retro-and forward-synthesis [35][36][37], as well as molecular regression [38][39][40] and reaction classification [41].Moreover, it has set the stage for other, more diverse tasks, like inferring experimental procedures [42,43], that go beyond operations on molecular graphs and require a deeper understanding of experimental conditions and standard procedures, a feat only achievable through modeling human language.Achieving success in this wide range of tasks has been facilitated by different variations of the Transformer architecture.Depending on the application, these variations leverage different parts of the architecture (Figure 2b), leading to encoder-decoder models -useful for tasks involving conversion of a sequence into another, like translation-, encoder-only -to extract rich representations from data-, and decoder-only architectures, mostly used in generative applications (Figure 2b).</p>
<p>Chemical translation tasks</p>
<p>The primary motivation for the formulation of the Transformer architecture by Vaswani et al. [5] was translation.In this task, input text in one language is converted into corresponding text in another language, logically leading to an architecture with an encoder part linked to a decoder (Figure 2b).</p>
<p>Steps to exploit this design were taken by Schwaller et al. [36], they introduced the Molecular Transformer and treated the task of reaction prediction as a translation task.In this perspective, a Transformer model learns to "translate" from precursors to products SMILES.The approach proved highly successful, establishing itself as one of the state-ofthe-art methods for this task [44].Subsequent attempts applied a similar approach to other tasks, like retrosynthetic planning [37], where a model learns to predict the reactants (and reagents) necessary to produce a given product.The model is then successively applied to the predicted reactants iteratively, to construct a retrosynthetic tree that maps to commercially available building blocks.Recent work used similar retrosynthesis models to tackle the challenge of suggesting diverse candidate reactions [45], as well as prompting, steering and unbiasing the proposed disconnections [46].</p>
<p>The Molecular Transformer approach was further extended by Irwin et al. [47], who proposed the Chemformer, a model pre-trained on a variety of chemical tasks, that can then be specialized for specific applications.This methodology achieved even greater success while offering additional flexibility and transferability to new tasks.</p>
<p>An interesting variation of the architecture came from an attempt to directly encode molecules as molecular graphs, for translation tasks like reaction outcome prediction.Tu and Coley [48] used a custom-designed graph encoder, with a Transformer decoder, trained to translate molecular graphs to SMILES.This approach proved valuable in forwardand retrosynthesis tasks, improving over other Transformer-based baselines, due to the permutational invariance of the novel graph-encoder.</p>
<p>Unsupervised learning and feature extraction.</p>
<p>Transformers have also been applied in various fields to generate rich vectorial representations of text that encapsulate context and general features [11,49].Such methods have been used for sentiment analysis, where a given text is classified based on the sentiment it conveys, a useful tool for assessing customer satisfaction with a specific product.The process of encoding this text as vectors is known as representation learning.For this task, only the encoder part of the Transformer architecture is used, as there is no need for text generation.</p>
<p>In the realm of chemistry, representation learning is a critical issue [50].It enables the transformation of molecules and reactions into vector representations, thus enabling a multitude of downstream applications.These include similarity assessment for database lookups, as well as regression and classification for property prediction tasks, such as predicting reaction yields or identifying toxic compounds -both of which are crucial to the drug development process.</p>
<p>Adopting similar techniques, Wang et al. [51] trained a model to generate reaction representations, and compared them against commonly used hand-engineered molecular representations, showing gains in accuracy in a number of downstream regression tasks, demonstrating the power of Transformer encoders for tasks in chemistry.</p>
<p>In another study, the decoder side of the Transformer was replaced by a classification layer, and the model was trained to predict the class of chemical reactions [41].The resulting vector representations, in addition to achieving high classification accuracies, were used for the visualization and exploration of a database of chemical reactions from patents.This revealed patterns in the data that grouped reactions by class, but also by data source, and relevant product properties, like logP, number of H donors, and others [52].This study showcased how such numerical representations can encode intricate details of chemical reactions, thereby facilitating effective clustering and classification of reactions.Importantly, this was achieved without the need for hand-crafted molecular or reaction features, demonstrating the model's capacity to learn purely from extensive reaction databases.In a similar vein, a regression layer, instead of a decoder layer, was used for predicting reaction yields, achieving outstanding results on datasets from high-throughput-experimentation platforms [53,54].</p>
<p>Other works have further advanced the applications of encoder-only Transformers, with enhancements in training procedures [38][39][40], modifications in the architecture [55], and applications in larger machine learning systems towards the solution of more specific objectives [56].</p>
<p>Similar ideas have also been implemented, with a focus on biochemistry.Rives et al. [57] trained a transformer model on 250 million unlabeled protein sequences, with the goal of learning the "language of proteins", an approach known as unsupervised learning.The resulting representations not only enabled state-of-the-art property predictions for proteins, but also predictions of variant effects and protein folding [58].The model was further shown to generalize beyond naturally occurring proteins [59], paving the way for applications in de novo generation of proteins for specific applications.</p>
<p>Perhaps one of the most intriguing applications of Transformers in chemical representation learning, also leveraging unsupervised learning, emerged from an attempt to interpret their inner workings.Through a detailed examination of the attention weights in a Transformer model trained on unlabeled chemical reactions, Schwaller et al. [24] discovered that these models create internal representations that connect atoms from the reactants to other atoms in the products.This almost serendipitous discovery led to the creation of RXN-Mapper, an open-source algorithm that accurately calculates the atom mapping in a given chemical reaction.RXNMapper has been shown to outperform other approaches -many of which were under closed licenses-in terms of speed, parallelizability, and accuracy.A similar approach was recently investigated on enzymatic reactions, where the attention weights could be used to identify active sites in protein sequences [60].</p>
<p>Articulating chemical language and other modalities</p>
<p>Chemical transformations are not confined solely to the realm of chemical structures.They are enhanced and accessed through a variety of other data types, or modalities.These include human language used for describing molecules and experimental results, as well as the experimental results themselves, which may be presented in formats such as numerical arrays or images, among others.</p>
<p>Considering this, scientists have proposed tasks designed to bridge the divide between the language of molecules and human language.One such task is known as molecular captioning, which involves describing a specific molecule in natural language [61].The description can cover a wide range of features such as molecular scaffolds, the source of the substance, drug interactions, and more, all expressed in simple English.</p>
<p>Some researchers have expanded on this concept [37,62], enabling a smooth interconversion from molecules to natural language and vice versa.This advancement has given rise to versatile models capable of not just molecular captioning, but also generating molecules in response to text queries, or carrying out molecule-to-molecule conversion tasks, like predicting reaction outcomes and retrosynthesis [62].While these models are still in their early stages of development, they show immense potential for the future of drug discovery as they continue to be developed and refined.</p>
<p>Another task, highly relevant for the design of automatic robotic platforms for synthesis, is the prediction of experimental steps.A step missing from retrosynthetic planning is the experimental realization, which involves steps such as adding substances, stirring, and purifying, details not contained in predicted reactions.To bridge this gap, Vaucher et al. [42,43] compiled a database of reactions with associated action sequences, and trained models to generate actionable sequences of steps from a given reaction in SMILES format.Other tasks aim to link experimental results with molecular structures, effectively addressing the challenge of structural elucidation.A pioneering attempt using Transformers was recently made by Alberts et al. [63], who created a database of molecules and computationally generated IR spectra to train a Transformer model for this purpose.IR spectra, often overlooked for this task due to their complexity, were represented as a sequence of numerical values encoded in text alongside the chemical formula.These were then fed to the model, which was tasked with predicting the SMILES string of the underlying molecular structure.This method achieved a 45% accuracy rate in structure prediction, while also surpassing all previous efforts in predicting functional groups from IR spectra.This illustrates how underutilized data, such as IR spectra, can be harnessed for tasks as intricate as structural elucidation, despite their initial difficulty in interpretation.</p>
<p>These excellent applications of Transformers across a range of tasks in chemistry underscore just a fraction of the potential these models possess, as well as the wealth of latent knowledge embedded in chemical databases.So far, our discussion has focused on taskspecific applications, where a predefined task is established and models are specifically trained to solve that task.One of the advantages of language is its flexibility, enabling it to express various tasks in similar forms of the same language.The upcoming section will delve into the potential of more generalized language models for chemistry.</p>
<p>Advanced applications: Beyond task-specific models</p>
<p>The last few years have seen a remarkable rise in the power and popularity of foundation models: models pre-trained on vast amounts of data that can easily be adapted to other, more specific tasks [6,14].Such pre-training is conducted using rich databases of human text extracted from the internet, among others, to make the model learn the language along with general knowledge about the world.This rise has been achieved mainly through the escalation of Transformer architectures to large computational budgets and vast databases, leading to models capable of producing text that matches human-level quality in a wide range of scenarios [6,14].</p>
<p>The process of fine-tuning these models leverages their pre-learned language abilities while tailoring them toward a specific objective where less data might be available [64,65].A prime example of this is ChatGPT, a large language model fine-tuned for conversation.Its release has not only catapulted these models into popularity but has also reignited profound philosophical discussions about the nature of intelligence [7].However, it has also raised serious concerns about potential issues such as the spread of misinformation.The power and accessibility of ChatGPT have set the world on a new trajectory, prompting us to rethink how we produce and consume media, while also highlighting the need for careful consideration of the potential implications.The success and popularity of ChatGPT can be attributed to two key factors.Firstly, its freely available and highly user-friendly chat interface, which makes interaction with the model straightforward, and secondly, its impressive capabilities, which often extend beyond the tasks the model was initially trained for.These capabilities not only demonstrate the power of ChatGPT and models of its kind but also hint at their potential for innovative applications.</p>
<p>On the capabilities of Large Language Models</p>
<p>The rise of data-hungry machine learning algorithms, coupled with the increasing availability of data, has set a trend for scaling models to the maximum sizes that hardware constraints allow.As these models increase in size, they correspondingly improve in their capacity to perform the tasks they were originally trained for.This trend is particularly predictable in Their capabilities allow for a. general task solvers thanks to their flexibility and knowledge transferability [66,67] and b. agent architectures, capable of integrating virtually unlimited modalities, in the form of computational tools [68][69][70].</p>
<p>the realm of language models, and it manifests in the form of what researchers refer to as scaling laws [71].These scaling laws serve as a valuable tool for researchers, enabling them to identify performance trends and make accurate predictions about the capabilities of significantly larger models.However, the process of scaling does not simply enhance existing capabilities.As these models grow, they reach certain critical thresholds where not only their existing capabilities are enhanced, but entirely new capabilities are observed.</p>
<p>These new abilities are collectively referred to as emergent capabilities [72].They represent a fascinating aspect of model scaling, as they are not present or even predictable in smaller models, but suddenly appear as the models increase in size.These emergent capabilities offer exciting potential for the future of machine learning and its application in various fields, including chemistry.</p>
<p>Wei et al. [72] demonstrated how language models below certain computing budgets display somewhat random behavior across a range of tasks.However, once a certain model size is reached, sudden and significant improvements in performance are observed.Other capabilities are observed in the form of augmented prompting strategies, such as Chain of Thought (CoT) reasoning [73].In this approach, models are instructed to solve a task by following a step by step reasoning sequence.Another emergent capability is instruction following [74], where LMs are given a task in the form of a set of instructions to follow.</p>
<p>Interestingly, these techniques generally have a negative impact on the performance of smaller models [73].However a positive effect on performance is observed once models reach certain threshold sizes [72].</p>
<p>Emergent capabilities, therefore, enable language models to effectively tackle a variety of tasks that involve reasoning.This is achieved without explicit training and with the flexibility of a text query in natural language.Considering the remarkable capabilities and transformative potential of large language models in various fields, it naturally leads us to question what benefits these models could bring to the field of chemistry.This will be the focus of our exploration in the following sections.</p>
<p>Large Language Models in chemistry</p>
<p>Much of this chapter has been devoted to the application of the Transformer architecture to ingeniously-encoded specific chemical tasks.However, it is important to note that a significant portion of information in chemistry is conveyed through human language, which includes explanations of reaction mechanisms, descriptions of modes of action of drugs, to name a few.Reasoning in chemistry is thus fundamentally articulated in human language, even though it is complemented by other non-text objects like graphs and images.This observation raises the question of whether Large Language Models (LLMs) can replicate this level of chemical reasoning and, if so, to what extent.</p>
<p>Tailoring techniques: fine-tuning and in-context learning</p>
<p>One of the main goals of developing general, large pretrained language models, is being able to further tune them for specific applications.Techniques for this include fine-tuning, where a model's parameters are further optimized for a new task.This technique has been the dominating paradigm in machine learning for years, with excellent results in a number of applications [75][76][77].</p>
<p>A new paradigm has also become the target of investigations with the rise of LLMs, so-called in-context learning.This feature can be activated simply by providing the model with a task description, along with a small set of examples that serve as "training data".This method proves particularly useful when data is scarce or obtaining it is costly or time-consuming, as is often the case in chemistry laboratories.LLMs have demonstrated impressive performance across a range of tasks using this method, with their strong performance believed to stem from knowledge transfer across different pre-training sources.</p>
<p>In line with this, Jablonka et al. [66] demonstrated that LLMs like GPT-3 can effectively solve various tasks in chemistry and materials science by fine-tuning LLMs, to a good level of approximation.The authors chose a range of tasks for which datasets are typically limited, such as predicting the transition wavelength of molecules, the phase of solid solutions in high entropy alloys, or even inverse design.Given the limited datasets available for these tasks, machine learning solutions have traditionally relied on heavily engineered and specialized algorithms [50,78,79].These algorithms aim to directly incorporate chemical knowledge into the model architectures.In parallel, extensive research has been devoted to molecular and reaction representations, with some efforts also attempting to bias the representations using prior chemical knowledge to better encode relevant features in data [80].All these endeavors aim to make the most of existing knowledge, enabling models to learn as much as possible from the few available data points.</p>
<p>Interestingly, Jablonka et al. [66] demonstrated that fine-tuning and in-context learning can perform on par with, and in some instances even outperform, these specialized techniques, particularly when data is limited.The high performance of in-context learning, combined with its ease of use and flexibility, makes it one of the most impressive applications of LLMs in chemistry to date.This technique holds the potential to revolutionize the way machine learning is utilized in the scientific field, by rapidly highlighting complex correlations in data.</p>
<p>Another application of regression, which holds significant interest in chemistry and drug discovery cycles, is optimization.This process involves modifying an object until a property of interest reaches a desired value.Typically, this requires a large number of measurements of the desired property, which can be quite costly in chemistry use-cases.Applications of this include yield/selectivity optimization in chemical reactions and the generation of molecular candidates with target properties.Bayesian Optimization (BO) has recently been proposed as a solution to such problems in chemistry [81,82], particularly in situations where data is small.However, BO requires uncertainty-calibrated regression methods, which sets it apart from conventional regression.</p>
<p>In line with the concept of in-context learning, Ramos et al. [67] proposed a system that utilizes GPT models to perform regression while also incorporating uncertainty.This approach enables BO without the need for any feature engineering or fine-tuning.The flexibility of this method allowed the team to perform catalyst and molecular optimization using only the synthesis procedure of the catalyst as input.This work represents a paradigm shift in drug discovery and molecular design.For the first time, it showcases a direct map from synthesis procedure into property space, effectively overcoming issues like the synthesizability of proposed molecules, a key limitation of structure-based generative models.</p>
<p>Molecular generation</p>
<p>Another fascinating application of the generative capabilities of language models is molecular generation.This area, which is of significant importance in the drug discovery process, has been largely dominated by models that generate molecules in the form of linear string representations, such as SMILES or SELFIES [83][84][85].While this approach has proven successful due to the ease of training these models [66,86,87], its successful application is contingent on the ability to specify substances as graphs and their subsequent conversion to a linear string representation.However, this approach is only suitable for a subset of organic molecules.Other substances, such as macromolecules and materials, necessitate more comprehensive representations.A complete and accurate representation of these substances can only be achieved by specifying atomic positions, boundary conditions, and other factors.This requirement presents a significant challenge and limitation to the current methods of molecular generation.To address these limitations, Flam-Shepherd and Aspuru-Guzik [88] proposed using language models for structure generation, directly generating them with three-dimensional atomic positions.Besides being innovative and valid, the generated structures can be obtained by training models in a variety of formats used for crystals, proteins, and more.This work also demonstrates performance comparable to expert-designed, state-of-the-art algorithms for molecular generation based on graphs, while overcoming the limitations mentioned earlier.</p>
<p>Language Model-powered Agents</p>
<p>Among the most useful emergent abilities of language models are step-by-step reasoning, activated through chain-of-thought (CoT) prompting, and their capacity to effectively use tools [89].These capabilities have been the subject of extensive research in recent years, and their application has been shown to significantly enhance the performance of LLMs across a variety of tasks.CoT prompting is a technique where language models are instructed to solve a task by following a sequence of reasoning steps, rather than providing an answer in a single response [73].Instructing language models in this way effectively allows them to perform symbolic operations, much like humans perform arithmetic operations by keeping track of intermediate steps.</p>
<p>The ability to use tools is another significant capability of language models [89].This allows them to invoke external computational tools, thereby enriching their knowledge through querying search engines, accessing calculators, and so on [89].These capabilities have been demonstrated to enhance the performance of large language models in a range of tasks that were previously inaccessible.The recent advancements and results in revealing and exploiting the capabilities of LLMs suggest the potential for combining some of these capabilities to create more powerful and useful possibilities.This concept has been recently explored, leading to the development of the Modular Reasoning, Knowledge and Language (MRKL [90]) and Reason+Act (ReAct [91]) systems, which combine the CoT and tool-using capabilities of modern LLMs.By incorporating external tools into a CoT setting, agents of this type have recently been shown to outperform other methods based on large language models.</p>
<p>One direct benefit of effective tool usage is that it partially overcomes the unimodality issue of LLMs.Under this setting, they become capable of processing different types of input data, making real-time decisions in simulated environments, and even interacting with real-world robotic platforms.The solutions provided by LLMs to tasks also become more grounded in reality, as access to certain tools provides them with real, up-to-date information relevant to the task.This can, to some extent, limit the tendency of these models to generate unrealistic or "hallucinated" responses.</p>
<p>Agents in chemistry: Unleashing the power from tools</p>
<p>Despite their strengths as text generators and task solvers, and their remarkable few-shot and zero-shot performance, these models are also well known for their high propensity to generate false and inaccurate content, an issue that extends to easily verifiable matters such as basic arithmetic [89] and chemical operations [92].These limitations make the direct application of LLMs to chemistry a challenging matter.</p>
<p>The potential applications of large language models in chemistry were first explored in a large-scale collaboration involving researchers from around the world, an effort that resulted in the demonstration of 14 use-cases [68].The applications range from wrappers for computational tools, which enhance their accessibility by allowing natural language inputs to modify behaviors, to assistants for reaction optimization, and knowledge parsers and synthesizers for scientific question answering, among others.These are just a few of the possibilities that LLMs offer in chemistry, which, when combined with existing chemistry tools and databases, significantly increase the applicability and accessibility of computational applications.</p>
<p>More recently, Bran and Cox et al. [69] extended the concept of LLM-powered agents for chemistry by curating and compiling a set of computational chemistry tools.Their system, ChemCrow, has been shown to be capable of planning and executing tasks in chemistry, effectively streamlining the reasoning process for several common chemical tasks across areas such as drug and materials design and synthesis.The authors demonstrate that this approach has a highly positive effect on LLM's performance for tasks in chemistry, overcoming hallucinations and grounding their responses with data from reliable sources.Complementary approaches exist, with a sharper focus on cloud lab operability [70].</p>
<p>The power of platforms like ChemCrow extends beyond merely serving as independent task solvers.They can be viewed as general chemistry assistants with the ultimate goal of making computational tools more accessible to chemists, thereby accelerating discovery.An additional highlight is the seamless exploitation of tool composability that this allows.It makes it straightforward to enrich the results of one tool with another, or to construct custom tool pipelines, all through simple requests in natural language.</p>
<p>Outlook and final remarks</p>
<p>Advancements in neural translation models, and specially with the introduction of the Transformer architecture, has sparked a revolution in machine learning for applications in chemistry and drug development.Analogies between chemical and natural language, and the publication of open databases and benchmarks, have inspired the representation of chemical tasks in the form of text, allowing straightforward application of Transformers to problems in this field.This revolution has occurred in three stages, differentiated by the specificity of tasks.In a first stage, characterized by task-specificity and use of singlemodality models, applications spanned molecule-to-molecule conversion tasks, like reaction outcome prediction and retrosynthetic planning, along with representation learning and downstream tasks like regression and classification.Their excellent performance and relative simplicity made them de-facto models in an array of applications.</p>
<p>In a second stage, researchers attempted to connect multiple additional modalities relevant to chemistry, like spectra from experiments, sequences of experimental actions, and even natural language, opening the way for an expanded number of applications involving modalities of any sort, however still task-specific.More recently, powered by vertiginous advancements in training and tuning of large language models, a series of works have been published that leverage a number of capabilities from such models.Among others, these contributions showcase applications in regression, classification, molecular generation and reaction optimization, all with unprecedented flexibility and usually improved performance over other methods.Another direction explores the integration of virtually unlimited modalities -in the form of tools-into agents powered by LLMs.The power of these agents has been demonstrated through a number of diverse tasks, ranging from molecular generation to automated organic synthesis, in an open-ended, highly customizable fashion.</p>
<p>By leveraging the expressivity and flexibility of natural language, this last wave of applications aims to bridge the gap between the chemical and natural languages.As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
<p>Fig. 1
1
Fig.1Advances in Natural Language Processing have inspired applications in Chemistry.Over time, the gap between chemical Language and natural Language is being closed by including additional modalities.Most recent works present general task solvers for chemistry, capable of chemical reasoning and automatic synthesis, among others.</p>
<p>Fig. 3
3
Fig. 3 Recent advances in Large Language Models have launched a new era of Transformers in chemistry.Their capabilities allow for a. general task solvers thanks to their flexibility and knowledge transferability[66,67] and b. agent architectures, capable of integrating virtually unlimited modalities, in the form of computational tools[68][69][70].</p>
<p>AcknowledgementsThis work was created as part of NCCR Catalysis (grant number 180544), a National Centre of Competence in Research funded by the Swiss National Science Foundation.
D Bahdanau, K Cho, Y Bengio, 10.48550/arXiv.1409.0473arXiv.arXiv:1409.0473stat]Neural Machine Translation by Jointly Learning to Align and Translate. 2016</p>
<p>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. K Cho, B Merrienboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, 10.48550/arXiv.1406.1078arXiv.arXiv:1406.10782014cs, stat</p>
<p>Long Short-Term Memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Computation. 981997</p>
<p>I Sutskever, O Vinyals, Q V Le, 10.48550/arXiv.1409.3215arXiv.arXiv:1409.3215Sequence to Sequence Learning with Neural Networks. 2014</p>
<p>Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 10.48550/arXiv.1706.03762arXiv.arXiv:1706.037622017</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N Chatterji, A Chen, K Creel, J Q Davis, D Demszky, C Donahue, M Doumbouya, E Durmus, S Ermon, J Etchemendy, K Ethayarajh, L Fei-Fei, C Finn, T Gale, L Gillespie, K Goel, N Goodman, S Grossman, N Guha, T Hashimoto, P Henderson, J Hewitt, D E Ho, J Hong, K Hsu, J Huang, T Icard, S Jain, D Jurafsky, P Kalluri, S Karamcheti, G Keeling, F Khani, O Khattab, P W Koh, M Krass, R Krishna, R Kuditipudi, A Kumar, F Ladhak, M Lee, T Lee, J Leskovec, I Levent, X L Li, X Li, T Ma, A Malik, C D Manning, S Mirchandani, E Mitchell, Z Munyikwa, S Nair, A Narayan, D Narayanan, B Newman, A Nie, J C Niebles, H Nilforoshan, J Nyarko, G Ogut, L Orr, I Papadimitriou, J S Park, C Piech, E Portelance, C Potts, A Raghunathan, R Reich, H Ren, F Rong, Y Roohani, C Ruiz, J Ryan, C Ré, D Sadigh, S Sagawa, K Santhanam, A Shih, K Srinivasan, A Tamkin, R Taori, A W Thomas, F Tramèr, R E Wang, W Wang, B Wu, J Wu, Y Wu, S M Xie, M Yasunaga, J You, M Zaharia, M Zhang, T Zhang, X Zhang, Y Zhang, L Zheng, K Zhou, P Liang, arXiv.arXiv:2108.07258On the Opportunities and Risks of Foundation Models. 2022</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv. 2023</p>
<p>L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, S Presser, C Leahy, 10.48550/arXiv.2101.00027arXiv.arXiv:2101.00027The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, 10.48550/arXiv.1910.10683arXiv:1910.106832020Text Transformer. arXiv.cs, stat</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 10.48550/arXiv.1810.04805arXiv.arXiv:1810.048052019</p>
<p>N Reimers, I Gurevych, 10.48550/arXiv.1908.10084arXiv.arXiv:1908.10084Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. 2019</p>
<p>P J Liu, M Saleh, E Pot, B Goodrich, R Sepassi, L Kaiser, N Shazeer, arXiv.arXiv:1801.10198[cs]Generating Wikipedia by Summarizing Long Sequences. 2018</p>
<p>Neural Text Summarization: A Critical Evaluation. W Kryscinski, N S Keskar, B Mccann, C Xiong, R Socher, 10.18653/v1/D19-1051Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong; ChinaAssociation for Computational Linguistics2019</p>
<p>. 10.48550/arXiv.2303.08774arXiv.arXiv:2303.087742023Technical Report</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-22023-08-07Nature. 59678732021Nature Publishing Group</p>
<p>AlphaFold2 and its applications in the fields of biology and medicine. Z Yang, X Zeng, Y Zhao, R Chen, 10.1038/s41392-023-01381-zSignal Transduction and Targeted Therapy. 812023Nature Publishing Group</p>
<p>M Akdel, D E V Pires, E P Pardo, J Jänes, A O Zalevsky, B Mészáros, P Bryant, L L Good, R A Laskowski, G Pozzati, A Shenoy, W Zhu, P Kundrotas, V R Serra, C H M Rodrigues, A S Dunham, D Burke, N Borkakoti, S Velankar, A Frost, J Basquin, K Lindorff-Larsen, A Bateman, A V Kajava, A Valencia, S Ovchinnikov, J Durairaj, D B Ascher, J M Thornton, N E Davey, A Stein, A Elofsson, T I Croll, P Beltrao, 10.1038/s41594-022-00849-wA structural biology community assessment of AlphaFold2 applications. Nature Publishing Group202229</p>
<p>K Huang, T Fu, W Gao, Y Zhao, Y Roohani, J Leskovec, C Coley, C Xiao, J Sun, M Zitnik, Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development. Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. 2021</p>
<p>The Open Reaction Database. S M Kearnes, M R Maser, M Wleklinski, A Kast, A G Doyle, S D Dreher, J M Hawkins, K F Jensen, C W Coley, 10.1021/jacs.1c09820Journal of the American Chemical Society. 143452021American Chemical Society</p>
<p>Extraction of chemical structures and reactions from the literature. D Lowe, 10.17863/CAM.162932012University of CambridgePhD thesis</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, 10.1039/C7SC02664AChemical Science. 922018The Royal Society of Chemistry</p>
<p>Organic Chemistry as a Language and the Implications of Chemical Linguistics for Structural and Retrosynthetic Analyses. A Cadeddu, E K Wylie, J Jurczak, M Wampler-Doty, B A Grzybowski, 10.1002/anie.201403708Angewandte Chemie International Edition. 53312014</p>
<p>Computer-designed repurposing of chemical wastes into drugs. A Wo Los, D Koszelewski, R Roszak, S Szymkuć, M Moskal, R Ostaszewski, B T Herrera, J M Maier, G Brezicki, J Samuel, J A M Lummiss, D T Mcquade, L Rogers, B A Grzybowski, 10.1038/s41586-022-04503-9Nature. 60479072022Nature Publishing Group</p>
<p>Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. P Schwaller, B Hoover, J.-L Reymond, H Strobelt, T Laino, 10.1126/sciadv.abe41662023-02-07Science Advances. 71541662021American Association for the Advancement of Science</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, 10.1021/ci00057a005Journal of Chemical Information and Computer Sciences. 2811988American Chemical Society</p>
<p>DeepSMILES: An Adaptation of SMILES for Use in Machine-Learning of Chemical Structures. N O'boyle, A Dalke, 10.26434/chemrxiv.7097960.v1ChemRxiv. 2018</p>
<p>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. M Krenn, F Häse, A Nigam, P Friederich, A Aspuru-Guzik, 10.1088/2632-2153/aba947Machine Learning: Science and Technology. 142020Publisher: IOP Publishing</p>
<p>M Krenn, Q Ai, S Barthel, N Carson, A Frei, N C Frey, P Friederich, T Gaudin, A A Gayle, K M Jablonka, R F Lameiro, D Lemm, A Lo, S M Moosavi, J M Nápoles-Duarte, A Nigam, R Pollice, K Rajan, U Schatzschneider, P Schwaller, M Skreta, B Smit, F Strieth-Kalthoff, C Sun, G Tom, G F Rudorff, A Wang, A White, A Young, R Yu, A Aspuru-Guzik, 10.1016/j.patter.2022.100588arXiv:2204.000562023-08-07SELFIES and the future of molecular string representations. 20223100588</p>
<p>InChI, the IUPAC International Chemical Identifier. S R Heller, A Mcnaught, I Pletnev, S Stein, D Tchekhovskoi, 10.1186/s13321-015-0068-4Journal of Cheminformatics. 712015</p>
<p>TUCAN: A molecular identifier and descriptor applicable to the whole periodic table from hydrogen to oganesson. J C Brammer, G Blanke, C Kellner, A Hoffmann, S Herres-Pawlis, U Schatzschneider, 10.1186/s13321-022-00640-5Journal of Cheminformatics. 1412022</p>
<p>Chemical space: limits, evolution and modelling of an object bigger than our universal library. G Restrepo, 10.1039/D2DD00030JDigital Discovery. 152022Royal Society of Chemistry</p>
<p>Grammar Variational Autoencoder. M J Kusner, B Paige, J M Hernández-Lobato, 10.48550/arXiv.1703.01925arXiv.arXiv:1703.019252017</p>
<p>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, A Aspuru-Guzik, 10.1021/acscentsci.7b00572ACS Central Science. 422018American Chemical Society</p>
<p>Exploring chemical space using natural language processing methodologies for drug discovery. H Öztürk, A Özgür, P Schwaller, T Laino, E Ozkirimli, 10.1016/j.drudis.2020.01.020Drug Discovery Today. 2542020</p>
<p>Transfer learning enables the molecular transformer to predict regio-and stereoselective reactions on carbohydrates. G Pesciullesi, P Schwaller, T Laino, J.-L Reymond, 10.1038/s41467-020-18671-7Nature Communications. 1112020Nature Publishing Group</p>
<p>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction. P Schwaller, T Laino, T Gaudin, P Bolgar, C A Hunter, C Bekas, A A Lee, 10.1021/acscentsci.9b005762023-02-07ACS Central Science. 592019American Chemical Society</p>
<p>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. P Schwaller, R Petraglia, V Zullo, V H Nair, R A Haeuselmann, R Pisoni, C Bekas, A Iuliano, T Laino, 10.1039/C9SC05704HChemical Science. 11122020. 2023-07-20The Royal Society of Chemistry</p>
<p>Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction. J Li, X Jiang, 10.1155/2021/7181815Wireless Communications and Mobile Computing 2021. 2021. 2023-07-277181815</p>
<p>ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. S Chithrananda, G Grand, B Ramsundar, 10.48550/arXiv.2010.09885arXiv.arXiv:2010.098852020physics, q-bio</p>
<p>W Ahmad, E Simon, S Chithrananda, G Grand, B Ramsundar, 10.48550/arXiv.2209.01712arXiv.arXiv:2209.01712ChemBERTa-2: Towards Chemical Foundation Models. 2022cs, q-bio</p>
<p>Mapping the space of chemical reactions using attention-based neural networks. P Schwaller, D Probst, A C Vaucher, V H Nair, D Kreutter, T Laino, J.-L Reymond, 10.1038/s42256-020-00284-wNature Machine Intelligence. 322021. 2023-07-25Nature Publishing Group</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. A C Vaucher, F Zipoli, J Geluykens, V H Nair, P Schwaller, T Laino, 10.1038/s41467-020-17266-6Nature Communications. 1112020Nature Publishing Group</p>
<p>Inferring experimental procedures from text-based representations of chemical reactions. A C Vaucher, P Schwaller, J Geluykens, V H Nair, A Iuliano, T Laino, 10.1038/s41467-021-22951-1Nature Communications. 1212021Nature Publishing Group</p>
<p>State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis. I V Tetko, P Karpov, R Van Deursen, G Godin, 10.1038/s41467-020-19266-yNature Communications. 1112020Nature Publishing Group</p>
<p>Enhancing diversity in language based models for single-step retrosynthesis. A Toniato, C Vaucher, A Schwaller, P Laino, T , 10.1039/D2DD00110A2023-08-03Digital Discovery. 222023Royal Society of Chemistry</p>
<p>Unbiasing Retrosynthesis Language Models with Disconnection Prompts. A Thakkar, A C Vaucher, A Byekwaso, P Schwaller, A Toniato, T Laino, 10.1021/acscentsci.3c003722023-08-03ACS Central Science. 972023American Chemical Society</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, 10.1088/2632-2153/ac3ffb2023-02-15Machine Learning: Science and Technology. 31150222022Publisher: IOP Publishing</p>
<p>Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction. Z Tu, C W Coley, 10.48550/arXiv.2110.09681arXiv.arXiv:2110.096812021</p>
<p>Distributed Representations of Words and Phrases and their Compositionality. T Mikolov, I Sutskever, K Chen, G Corrado, J Dean, ArXiv. 2013</p>
<p>D Duvenaud, D Maclaurin, J Aguilera-Iparraguirre, R Gómez-Bombarelli, T Hirzel, A Aspuru-Guzik, R P Adams, 10.48550/arXiv.1509.09292arXiv.arXiv:1509.09292Convolutional Networks on Graphs for Learning Molecular Fingerprints. 2015cs, stat</p>
<p>SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction. S Wang, Y Guo, Y Wang, H Sun, J Huang, 10.1145/3307339.3342186Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics. the 10th ACM International Conference on Bioinformatics, Computational Biology and Health InformaticsNiagara Falls NY USAACM2019</p>
<p>Machine intelligence for chemical reaction space. P Schwaller, A C Vaucher, R Laplaza, C Bunne, A Krause, C Corminboeuf, T Laino, 10.1002/wcms.1604WIREs Computational Molecular Science. 1252022</p>
<p>Prediction of chemical reaction yields using deep learning. P Schwaller, A C Vaucher, T Laino, J.-L Reymond, 10.1088/2632-2153/abc81dMachine Learning: Science and Technology. 212021Publisher: IOP Publishing</p>
<p>Global reactivity models are impactful in industrial synthesis applications. P Neves, K Mcclure, J Verhoeven, N Dyubankova, R Nugmanov, A Gedich, S Menon, Z Shi, J K Wegner, 10.1186/s13321-023-00685-0Journal of Cheminformatics. 1512023</p>
<p>J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, 10.48550/arXiv.2106.09553arXiv.arXiv:2106.09553Large-Scale Chemical Language Representations Capture Molecular Structure and Properties. 2022cs, q-bio</p>
<p>Molformer: Motif-based Transformer on 3D Heterogeneous Molecular Graphs. F Wu, D Radev, S Z Li, 10.48550/arXiv.2110.01191arXiv.arXiv:2110.011912023cs, q-bio</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. A Rives, J Meier, T Sercu, S Goyal, Z Lin, J Liu, D Guo, M Ott, C L Zitnick, J Ma, R Fergus, 10.1073/pnas.20162391182023-08-07Proceedings of the National Academy of Sciences. 118152016239118. 2021Proceedings of the National Academy of Sciences</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, H Akin, R Rao, B Hie, Z Zhu, W Lu, N Smetanin, R Verkuil, O Kabeli, Y Shmueli, A Santos Costa, M Fazel-Zarandi, T Sercu, S Candido, 10.1126/science.ade25742023-08-07Science. 37966372023American Association for the Advancement of Science</p>
<p>Language models generalize beyond natural proteins. R Verkuil, O Kabeli, Y Du, B I M Wicky, L F Milles, J Dauparas, D Baker, S Ovchinnikov, T Sercu, A Rives, 10.1101/2022.12.21.521521v1bioRxiv. 2022Pages: 2022.12.21.521521 Section: New Results</p>
<p>Language models can identify enzymatic active sites in protein sequences. Y G N Teukam, L K Dassi, M Manica, D Probst, T Laino, </p>
<p>C Edwards, T Lai, K Ros, G Honke, K Cho, H Ji, 10.48550/arXiv.2204.11817arXiv.arXiv:2204.11817Translation between Molecules and Natural Language. 2022</p>
<p>Unifying Molecular and Textual Representations via Multi-task Language Modelling. D Christofidellis, G Giannone, J Born, O Winther, T Laino, M Manica, 10.48550/arXiv.2301.12586arXiv.arXiv:2301.125862023</p>
<p>Leveraging Infrared Spectroscopy for Automated Structure Elucidation. M Alberts, T Laino, A C Vaucher, 10.26434/chemrxiv-2023-5v27fChemistry. May 2023preprint</p>
<p>Finetuning Large Language Models. S Raschka, 2023</p>
<p>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y Qiao, arXiv.arXiv:2303.161992023</p>
<p>Is GPT-3 all you need for low-data discovery in chemistry?. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.26434/chemrxiv-2023-fw8n4ChemRxiv. 2023</p>
<p>M C Ramos, S S Michtavy, M D Porosoff, A D White, 10.48550/arXiv.2304.05341arXiv.arXiv:2304.05341Bayesian Optimization of Catalysts With In-context Learning. 2023</p>
<p>K M Jablonka, Q Ai, A Al-Feghali, S Badhwar, J D Bocarsly, A M Bran, S Bringuier, L C Brinson, K Choudhary, D Circi, S Cox, W A Jong, M L Evans, N Gastellu, J Genzling, M V Gil, A K Gupta, Z Hong, A Imran, S Kruschwitz, A Labarre, J Lála, T Liu, S Ma, S Majumdar, G W Merz, N Moitessier, E Moubarak, B Mouriño, B Pelkie, M Pieler, M C Ramos, B Ranković, S G Rodriques, J N Sanders, P Schwaller, M Schwarting, J Shi, B Smit, B E Smith, J Van Herck, C Völker, L Ward, S Warren, B Weiser, S Zhang, X Zhang, G A Zia, A Scourtas, K J Schmidt, I Foster, A D White, B Blaiszik, 10.48550/arXiv.2306.06283arXiv.arXiv:2306.0628314 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. 2023cond-mat, physics:physics</p>
<p>A M Bran, S Cox, A D White, P Schwaller, 10.48550/arXiv.2304.05376arXiv.arXiv:2304.05376ChemCrow: Augmenting largelanguage models with chemistry tools. 2023physics, stat</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, 10.48550/ARXIV.2304.053322023Publisher: arXiv Version Number: 1</p>
<p>J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, T Hennigan, E Noland, K Millican, G V D Driessche, B Damoc, A Guy, S Osindero, K Simonyan, E Elsen, J W Rae, O Vinyals, L Sifre, arXiv.arXiv:2203.15556Training Compute-Optimal Large Language Models. 2022</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, 10.48550/ARXIV.2206.07682Emergent Abilities of Large Language Models. 2022Publisher: arXiv Version Number: 2</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 10.48550/arXiv.2201.11903arXiv.arXiv:2201.119032023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 10.48550/arXiv.2203.02155arXiv.arXiv:2203.021552022</p>
<p>Finetuned Language Models are Zero-Shot Learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, 2021</p>
<p>Fine-tuning and visualization of convolutional neural networks. X Yin, W Chen, X Wu, H Yue, 10.1109/ICIEA.2017.828304112th IEEE Conference on Industrial Electronics and Applications (ICIEA). 2017. 2017</p>
<p>Universal Language Model Fine-tuning for Text Classification. J Howard, S Ruder, 10.18653/v1/P18-1031Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia20181Association for Computational Linguistics</p>
<p>Retrosynthesis Prediction with Conditional Graph Logic Network. H Dai, C Li, C W Coley, B Dai, L Song, 10.48550/arXiv.2001.01408arXiv.arXiv:2001.014082020cs, stat</p>
<p>Chemistry-informed molecular graph as reaction descriptor for machine-learned retrosynthesis planning. B Zhang, X Zhang, W Du, Z Song, G Zhang, G Zhang, Y Wang, X Chen, J Jiang, Y Luo, 10.1073/pnas.22127111192023-08-07Company: National Academy of Sciences Distributor: National Academy of Sciences Institution. 1194122127111192022National Academy of Sciences Label: National Academy of Sciences Publisher: Proceedings of the National Academy of SciencesProceedings of the National Academy of Sciences</p>
<p>. K Jorner, L Turcani, 10.5281/zenodo.70175992022</p>
<p>Bayesian optimisation for additive screening and yield improvements in chemical reactions -beyond one-hot encoding. B Ranković, R.-R Griffiths, H B Moss, P Schwaller, 10.26434/chemrxiv-2022-nll2j-v3ChemRxiv. 2023</p>
<p>Bayesian reaction optimization as a tool for chemical synthesis. B J Shields, J Stevens, J Li, M Parasram, F Damani, J I M Alvarado, J M Janey, R P Adams, A G Doyle, 10.1038/s41586-021-03213-y2023-02-08Nature. 59078442021Nature Publishing Group</p>
<p>A Transformer-based Generative Model for De Novo Molecular Design. W Wang, Y Wang, H Zhao, S Sciabola, 10.48550/arXiv.2210.08749arXiv.arXiv:2210.087492022cs, q-bio</p>
<p>C5T5: Controllable Generation of Organic Molecules with Transformers. D Rothchild, A Tamkin, J Yu, U Misra, J Gonzalez, 10.48550/arXiv.2108.10307arXiv.arXiv:2108.103072021</p>
<p>MolGPT: Molecular Generation Using a Transformer-Decoder Model. V Bagal, R Aggarwal, P K Vinod, U D Priyakumar, 10.1021/acs.jcim.1c006002023-08-05Journal of Chemical Information and Modeling. 6292022American Chemical Society</p>
<p>Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation. E Bengio, M Jain, M Korablyov, D Precup, Y Bengio, 10.48550/arXiv.2106.04399arXiv.arXiv:2106.043992021</p>
<p>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, 10.1038/s42256-023-00639-z2023-07-30Nature Machine Intelligence. 542023Nature Publishing Group</p>
<p>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. D Flam-Shepherd, A Aspuru-Guzik, 10.48550/arXiv.2305.05708arXiv.arXiv:2305.057082023cs, q-bio</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, 10.48550/arXiv.2302.04761arXiv:2302.04761Toolformer: Language Models Can Teach Themselves to Use Tools. arXiv. 2023</p>
<p>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. E Karpas, O Abend, Y Belinkov, B Lenz, O Lieber, N Ratner, Y Shoham, H Bata, Y Levine, K Leyton-Brown, D Muhlgay, N Rozen, E Schwartz, G Shachaf, S Shalev-Shwartz, A Shashua, M Tenenholtz, arXiv.arXiv:2205.004452022</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv.arXiv:2210.03629ReAct: Synergizing Reasoning and Acting in Language Models. 2023</p>
<p>Assessment of chemistry knowledge in large language models that generate code. D White, A , M Hocky, G , A Gandhi, H Ansari, M Cox, S , P Wellawatte, G Sasmal, S Yang, Z Liu, K Singh, Y Ccoa, W J P , 10.1039/D2DD00087CDigital Discovery. 222023. 2023-07-25Royal Society of Chemistry</p>            </div>
        </div>

    </div>
</body>
</html>
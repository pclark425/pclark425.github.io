<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7788 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7788</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7788</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-263828868</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.05030v2.pdf" target="_blank">Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index</a></p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7788.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7788.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity (language model sequence probability metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard LM-based metric that measures how well a language model predicts a sequence; used here to compare human vs. AI text at document and sentence granularity and to derive entropy-based statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs (15 evaluated: GPT-4, GPT-3.5, GPT-3, OPT, GPT-2, MPT, LLaMA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see paper; 15 different model families/versions)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / natural language processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric for AGTD hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Perplexity estimation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute sequence perplexity for texts (overall and sentence-wise), compare mean and distribution between human-written and LLM-generated text and compute entropies and bootstrap statistics to test significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean perplexity, sentence-wise perplexity, sentence-wise entropy, bootstrap p-values</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Perplexity = exp(- (1/N) * sum_{i=1..N} log p(w_i) ); sentence-wise entropy computed from distribution of sentence perplexities; comparisons via differences in log-probabilities and entropies.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>NYT tweets subset (100k prompts), plus LLM-generated corpora for 15 LLMs (CT2/ADI dataset generated by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human-written baseline = NYT tweets (100k). No human rater scoring reported; comparisons are automated against this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Tables report mean and sigma of perplexity for human vs. each LLM (e.g., GPT-4 human µ 38.073 vs AI 35.465); larger LLMs (GPT-3+) have perplexity close to human and are less distinguishable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Larger/more advanced LLMs show perplexity distributions similar to human text; small models remain distinguishable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Perplexity can be unreliable across domains, fragment sizes, languages; depends on which LM is used to compute probabilities; fragment size choice affects detectability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7788.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Burstiness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Burstiness estimation (lexical burstiness metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lexical metric quantifying clustering (bursts) of similar words/phrases in short spans; used to contrast human vs. AI lexical variability and as an AGTD signal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs (15 evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric for AGTD hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Burstiness estimation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute per-text or per-sentence burstiness b = (σ_τ/m_τ - 1) / (σ_τ/m_τ + 1) bounded in [-1,1]; optionally compute entropy over sentence-wise burstiness; compare means between human and AI.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Burstiness score b, burstiness entropy E_burst, standard deviation and mean</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>b = (σ_τ / m_τ - 1) / (σ_τ / m_τ + 1) where σ_τ is standard deviation and m_τ mean of language spans; E_burst = logp_beta[sum_k |s_k^AIb - s_{k+1}^AIb| - sum_k |s_k^hb - s_{k+1}^hb|] >= 0 (as used in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>NYT tweets subset (100k) and generated LLM texts (CT2/ADI dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human baseline = NYT tweets; comparisons automated; no human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Burstiness differences diminish for larger LLMs; some small models (e.g., XLNet) show stronger separation but modern LLMs show similar burstiness to humans (see Table 22).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Larger LLMs produce burstiness scores closer to human texts, reducing effectiveness of burstiness-based detection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Burstiness depends on text type and vocabulary; can be computed at different granularities; not consistently discriminative for modern LLMs and across languages/domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7788.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DetectGPT / NLC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative Log-Curvature (NLC) / DetectGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DetectGPT hypothesis: AI-generated text tends to lie in regions of negative curvature of the LLM's log-probability landscape; detection via measuring log-probability drop under small perturbations (P_NLC) and z-scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetectGPT: Zero-shot machine-generated text detection using probability curvature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs (15 evaluated), including experiments on GPT variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>detection hypothesis / statistical test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Negative Log-Curvature (NLC) perturbation test (DetectGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply small perturbations to a text x and compute P_NLC = log p_theta(x) - log p_theta(x') averaged over perturbations; higher P_NLC indicates AI generation; classification via z-score threshold (DetectGPT used z > 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>P_NLC differences, z-score (normalized log-probability difference), p-values from bootstrap</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>P_NLC_theta = log p_theta(x) - log p_theta(x'); DetectGPT classifies AI-generated if z-score > 4 where z = (orig_logprob - mean_perturbed_logprob)/std_perturbed_logprob.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>2000 samples for perturbation experiments reported (plus full CT2 dataset of 100k prompts and generated outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human raters; statistical testing via bootstrap over perturbations and samples.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors could not replicate NLC pattern for GPT-4 and other LLMs; variance in NLC was minimal and not a reliable signal (see Fig.1 and Table 22).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>No consistent NLC signal distinguishing modern LLM outputs from human text across tested models; DetectGPT hypothesis shown fragile.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires access to log-probabilities from the generating LM (practical limitation), sensitive to choice of perturbations, computationally expensive if generator unknown, and findings are not robust across LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7788.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Watermarking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Watermarking for LLM outputs (Kirchenbauer et al. proposals w_v1 and w_v2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Source-side watermarking injects subtle/modulated token selection patterns (a watermark) into LLM outputs to mark authorship; detection methods test presence of watermark; authors evaluate robustness to de-watermarking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On a watermark for large language models (Kirchenbauer et al., 2023a) / On the reliability of watermarks (Kirchenbauer et al., 2023b)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to outputs of multiple LLMs (OPT, LLaMA, GPT variants, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>defensive marking technique / detectability criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Watermark detection and de-watermarking attack experiments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply watermarking schemes (w_v1, w_v2) on generated text then attempt to remove watermark via de-watermarking methods (high-entropy token replacement, paraphrasing, combined) and measure detection/de-watermarking accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Watermark detection accuracy, de-watermarking success rate (%) across model and attack combinations</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy defined as percentage of samples for which watermark detector correctly identifies watermark (or de-watermarking succeeded). Reported as percent values per model/attack (tables 3,7-20).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Watermarked outputs from 15 LLMs generated using NYT prompts (CT2 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human judges; evaluations automated; paraphrase validity filtered using entailment model (RoBERTa Large trained on SNLI).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>De-watermarking approaches (spotting high-entropy token replacement, paraphrasing, and combined) achieved high success rates across many LLMs; w_v2 is slightly more robust but still circumventable (de-watermarking accuracies often >70%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Watermarking can be diluted by paraphrasing and targeted token replacements; robustness depends on watermark scheme and attacker capabilities; experiments used open-source paraphrasers and masking models and may not capture all real-world constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7788.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeW1 / DeW2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>De-watermarking techniques: DeW1 (high-entropy token replacement) and DeW2 (paraphrasing +/- token replacement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Attacks on watermarking: DeW1 spots high-entropy tokens and replaces them; DeW2 applies paraphrasing (Pegasus, T5, GPT-3.5) and/or token replacement to remove watermark signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>attacks applied to outputs of many LLMs (OPT, LLaMA, GPT variants, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP (adversarial robustness)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>attack / robustness evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>De-watermarking attack evaluation (DeW1, DeW2)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate paraphrases and/or replace high-entropy words in watermarked text, then run watermark detector to measure whether watermark persists; paraphrase candidates filtered by entailment and MED thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>De-watermarking accuracy (%) per model and per attack combination; paraphrase evaluation metrics: coverage, correctness, diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>De-watermarking accuracy = % samples where watermark is no longer detected after attack; Coverage = number of viable paraphrases (MED > 2), Correctness = entailment pass rate via RoBERTa-Large (SNLI), Diversity = inverse BLEU (higher => more diverse).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Watermarked outputs from 15 LLMs (CT2 dataset); paraphrase generators: Pegasus, T5, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Paraphrase correctness assessed via RoBERTa Large entailment model; no human raters used.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-3.5 paraphraser produced highest coverage/correctness/diversity and achieved strongest de-watermarking; tables (e.g., Table 3, Table 21) show de-watermarking accuracies often high (many >70%, some >90% depending on combo).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>De-watermarking efficacy depends on paraphraser quality and masking models used; real-world watermark schemes may adapt; entropy-based replacement assumes watermark concentrates on high-entropy tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7788.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stylometric variation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stylometric variation / stylometry (including Le Cam's lemma density approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use stylometric features (syntactic and lexical variation) and density estimation (Le Cam's lemma approximating sums by Poisson) to group/identify authors (LLMs) and compare AI vs human writing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>15 LLMs treated as distinct 'authors' vs. human baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / stylometry / authorship attribution</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>stylometric detection / attribution model</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Stylometric density estimation using Le Cam's lemma</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute perplexity and burstiness as density functions and use Le Cam's lemma to approximate total variation between observed sentence-level counts and Poisson approximations to characterize LLM-specific densities; group LLMs by similarity and measure detectability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Stylometric distinguishability rates (percent detectable), grouping consistency, applicability of learned lemma across models (relational matrix)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Probability densities L_plx_H and L_brsty_H computed via Le Cam's lemma approximations; detectability bands defined by percent thresholds (e.g., Detectable >80%, Hard>70%, Impossible<50%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>NYT tweets (human) and AI-generated CT2 dataset across 15 LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human corpus treated as single author (simplifying assumption); no multi-annotator study.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors report grouping into three stylometric detectability bands: Detectable (80%+): T0, T5; Hard to detect (70%+): XLNet, StableLM, Dolly; Impossible (<50%): LLaMA, OPT, GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Stylometric density ranges overlap strongly for modern LLMs and humans; lemma learned for one LLM transfers only within same group.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Simplifying assumption that all human text is from single author; stylometric features considered limited to perplexity and burstiness densities; alternative density estimators not explored in depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7788.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classifier-based detectors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classifier-based detection approaches (RoBERTa-Large-Detector and similar)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binary classifiers trained to discriminate human vs. machine text or model-specific detectors trained on a model's outputs; evaluated for generalization across models and domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-Large-Detector (OpenAI) and other classifiers referenced (trained on model-specific corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>RoBERTa-Large; other detector sizes vary</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP / supervised classification</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>supervised detection/classification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Classifier accuracy and cross-model generalization tests</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Train binary classifiers on datasets of human and model-generated text (often model-specific) and measure accuracy, and test on texts from unseen LLMs to assess generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Classification accuracy, false positive/negative rates; domain/model generalization performance</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = (TP+TN)/Total; detectors are often evaluated on held-out samples and on cross-model test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Detector training datasets (not fully enumerated here); OpenAI's RoBERTa-Large-Detector trained on model-specific generated/human corpora; authors' CT2 dataset used to test cross-model robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human labeling reported for classifier training in this paper (classifiers referenced are pre-trained or previously published detectors).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper notes classifier detectors usually perform well on data from models they were trained on but generalize poorly to new/unseen LLMs and domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Detector performance degrades on outputs from different LLMs than those seen during training; thus classifier-based approaches are brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Most classifiers are model-specific; limited cross-model generalization; training requires labeled model outputs; susceptible to paraphrasing and de-watermarking attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7788.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Detectability Index (ADI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Detectability Index (ADI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite, auto-computable index proposed to quantify an LLM's detectability based on core AGTD signals (perplexity and burstiness) with damping factors and scaling to a 0-100 range.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>computed for 15 LLMs evaluated in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / policy-relevant NLP metric</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>aggregate evaluation metric / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>AI Detectability Index (ADI) combining perplexity and burstiness (and optionally stylometric features)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>ADI_x = scaled aggregate over U samples of weighted differences between observed text statistics (P_t, B_t) and human baseline Le Cam densities (L_plx_H, L_brsty_H), normalized by human mean and modulated by damping factors δ1(x), δ2(x); finally scaled 0-100.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ADI score (0-100)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See paper formula: ADI_x = 100/(U*2) * sum_x [{δ1(x)*(P_t - L_plx_H)/(1-µ_plx_H)} + {δ2(x)*(B_t - L_brsty_H)/(1-µ_brsty_H)}], where P_t and B_t are defined aggregations of log probabilities across sentence windows; δ damping factors determined via rank and σ; scaled to 0-100.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Computed over CT2 benchmark: 100k NYT prompts and generated outputs from 15 LLMs (authors' dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>ADI uses human mean perplexity and burstiness (µ_plx_H, µ_brsty_H) derived from NYT baseline; no external human ratings used.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ADI groups LLMs into three bands: detectable (T0, T5), hard-to-detect (XLNet, StableLM, Dolly, Vicuna), and non-detectable (LLaMA, OPT, GPT variants), with higher ADI meaning less detectable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>ADI captures distinguishability relative to human baseline; higher ADI corresponds to LLM outputs that are closer to human statistics and thus harder to detect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>ADI currently built on perplexity and burstiness (authors note other features could be incorporated); sensitivity to baseline choice (NYT), damping parameters, and evolving LLMs; may require recalibration over time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7788.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CT2 benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counter Turing Test (CT2) benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark suite introduced by the authors consisting of techniques, datasets and leaderboards for evaluating robustness of AGTD methods across multiple LLMs (planned open-source).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated across 15 LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CT2 (Counter Turing Test) benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use parallel data (human NYT tweets + LLM generations) and a suite of detection methods (watermarking, perplexity/burstiness, NLC, stylometry, classifiers) plus de-watermarking attacks to comprehensively evaluate AGTD robustness; organizes leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Method-specific metrics (accuracy, de-watermarking success rates, ADI scores); aggregated detectability rankings</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See individual methods (perplexity, burstiness, watermark detection accuracy, ADI 0-100 scale).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>NYT tweets subset (100k) used as prompts; CT2 dataset of generated outputs across 15 LLMs; authors plan to publish leaderboards and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human baseline uses NYT tweets; no human rating panels described; computational/automated evaluations dominate.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CT2 experiments indicate fragility of SoTA AGTD methods under a range of attacks and modern LLMs; results summarized across tables showing de-watermarking accuracies and statistical metrics (Table22 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>CT2 shows modern LLM outputs are often statistically close to human texts on the employed signals, challenging detection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Benchmark depends on chosen human baseline (NYT tweets) and set of LLMs; will need continuous updates as new LLMs and detection methods emerge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7788.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NYT tweets dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NYT Twitter-handle prompt corpus (subset of ~100k tweets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Curated prompt source: ~100k NYT tweets chosen as prompts because they are high-quality, contain URLs to articles, and cover varied topics; used to generate parallel human/AI corpora for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used as prompts for generation by 15 LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation dataset / benchmark corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>NYT tweets (100k) parallel prompt corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use cleaned NYT tweets (remove hashtags/mentions) as prompts to generate LLM outputs; paired human tweet text serves as human baseline for statistical comparisons and ADI computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to compute human baseline means (µ_plx_H, µ_brsty_H) and for U=100k sample aggregation in ADI</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Human baseline statistics computed as sample means and variances over the 100k tweets for perplexity and burstiness.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>NYT tweets subset (authors' selection of ~100k tweets from ~393k total)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>NYT tweets considered human-written standard; no further human annotation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to compute ADI baselines and to prompt 15 LLMs for all experiments; authors will publish curated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>NYT tweets used as human baseline for all comparative metrics; modern LLMs approximate these statistics closely.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human baseline is single-style (journalistic tweets) and treated as single 'author' in stylometry experiments; may bias ADI toward this genre.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7788.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paraphrase evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paraphrase evaluation: Coverage, Correctness, Diversity (inverse BLEU), MED</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational criteria employed to select paraphrases for de-watermarking: MED filter, entailment-based correctness via RoBERTa Large (SNLI), and diversity measured via inverse BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>paraphrase generators evaluated: Pegasus, T5, GPT-3.5 (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Pegasus/T5/GPT-3.5 variants as used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP / adversarial evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criteria for paraphrase-based attacks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Coverage/Correctness/Diversity paraphrase evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate up to 5 paraphrases per claim, keep paraphrases with MED > 2, filter for entailment using RoBERTa-Large (SNLI), and measure linguistic diversity by inverse BLEU between paraphrase candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Coverage (count of viable paraphrases), Correctness (% entailment pass), Diversity (inverse BLEU score), De-watermarking accuracy after paraphrase</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Coverage = number of paraphrases per input with MED > 2; Correctness = % paraphrases judged entailed by RoBERTa-Large; Diversity = average inverse BLEU between paraphrases; de-watermarking accuracy as percent success.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Paraphrases applied to CT2 watermarked outputs (across 15 LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Correctness assessed by automatic entailment model; no human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-3.5 paraphraser achieved highest coverage, correctness and diversity (e.g., Coverage 35.51, Correctness 88.16%, Diversity 7.72 in Table 5); paraphrasing substantially reduces watermark detection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Automatic entailment filter may not perfectly capture semantic preservation; inverse BLEU is a proxy for diversity and has known limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7788.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7788.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical testing / Bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrap resampling and basic statistical measures (mean, std, entropy, p-tests)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-parametric bootstrap used to estimate sampling distributions, confidence intervals, and p-values for comparing metrics (perplexity, burstiness, NLC) between human and AI text; also compute mean, std, entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to statistics computed over CT2 dataset for 15 LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistics / empirical NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical validation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bootstrap statistical testing</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Resample the empirical sample with replacement many times to build an empirical sampling distribution for test statistics (means, differences) and compute confidence intervals and p-values; used because distributions may be non-Gaussian.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Bootstrap p-values, bootstrap confidence intervals, sample mean and standard deviation, entropy</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Bootstrap constructs m resamples of size n from sample S; compute statistic on each resample to form sampling distribution; p-value derived from distribution relative to null.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to CT2 datasets (100k prompts and generated outputs), plus per-model sample sets (e.g., 2000 samples used for perturbations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human raters; purely statistical resampling.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Bootstrap tests show many AGTD signals non-significant for modern LLMs (see Table 22 with bootstrap p-values α = 0.05 indicating non-significance in many comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Bootstrap-based tests failed to find robust significant differences for many LLMs between human and AI on NLC, perplexity and burstiness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Bootstrap effectiveness depends on representativeness of original sample; increasing number of resamples does not add new information beyond original sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DetectGPT: Zero-shot machine-generated text detection using probability curvature <em>(Rating: 2)</em></li>
                <li>A watermark for large language models <em>(Rating: 2)</em></li>
                <li>On the reliability of watermarks for large language models <em>(Rating: 2)</em></li>
                <li>Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense <em>(Rating: 2)</em></li>
                <li>Paraphrasing evades detectors of ai-generated text <em>(Rating: 1)</em></li>
                <li>Can ai-generated text be reliably detected? <em>(Rating: 2)</em></li>
                <li>GLTR: Statistical detection and visualization of generated text <em>(Rating: 1)</em></li>
                <li>Defending against neural fake news <em>(Rating: 1)</em></li>
                <li>Stylometric detection of ai-generated text in twitter timelines <em>(Rating: 2)</em></li>
                <li>Do Foundation Model Providers Comply with the EU AI Act? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7788",
    "paper_id": "paper-263828868",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Perplexity",
            "name_full": "Perplexity (language model sequence probability metric)",
            "brief_description": "A standard LM-based metric that measures how well a language model predicts a sequence; used here to compare human vs. AI text at document and sentence granularity and to derive entropy-based statistics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "multiple LLMs (15 evaluated: GPT-4, GPT-3.5, GPT-3, OPT, GPT-2, MPT, LLaMA, etc.)",
            "model_size": "various (see paper; 15 different model families/versions)",
            "scientific_domain": "computer science / natural language processing",
            "theory_type": "evaluation metric for AGTD hypotheses",
            "evaluation_method_name": "Perplexity estimation",
            "evaluation_method_description": "Compute sequence perplexity for texts (overall and sentence-wise), compare mean and distribution between human-written and LLM-generated text and compute entropies and bootstrap statistics to test significance.",
            "evaluation_metric": "Mean perplexity, sentence-wise perplexity, sentence-wise entropy, bootstrap p-values",
            "metric_definition": "Perplexity = exp(- (1/N) * sum_{i=1..N} log p(w_i) ); sentence-wise entropy computed from distribution of sentence perplexities; comparisons via differences in log-probabilities and entropies.",
            "dataset_or_benchmark": "NYT tweets subset (100k prompts), plus LLM-generated corpora for 15 LLMs (CT2/ADI dataset generated by authors)",
            "human_evaluation_details": "Human-written baseline = NYT tweets (100k). No human rater scoring reported; comparisons are automated against this baseline.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Tables report mean and sigma of perplexity for human vs. each LLM (e.g., GPT-4 human µ 38.073 vs AI 35.465); larger LLMs (GPT-3+) have perplexity close to human and are less distinguishable.",
            "comparison_to_human_generated": true,
            "comparison_results": "Larger/more advanced LLMs show perplexity distributions similar to human text; small models remain distinguishable.",
            "limitations_noted": "Perplexity can be unreliable across domains, fragment sizes, languages; depends on which LM is used to compute probabilities; fragment size choice affects detectability.",
            "uuid": "e7788.0",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Burstiness",
            "name_full": "Burstiness estimation (lexical burstiness metric)",
            "brief_description": "A lexical metric quantifying clustering (bursts) of similar words/phrases in short spans; used to contrast human vs. AI lexical variability and as an AGTD signal.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "multiple LLMs (15 evaluated)",
            "model_size": "various",
            "scientific_domain": "computer science / NLP",
            "theory_type": "evaluation metric for AGTD hypotheses",
            "evaluation_method_name": "Burstiness estimation",
            "evaluation_method_description": "Compute per-text or per-sentence burstiness b = (σ_τ/m_τ - 1) / (σ_τ/m_τ + 1) bounded in [-1,1]; optionally compute entropy over sentence-wise burstiness; compare means between human and AI.",
            "evaluation_metric": "Burstiness score b, burstiness entropy E_burst, standard deviation and mean",
            "metric_definition": "b = (σ_τ / m_τ - 1) / (σ_τ / m_τ + 1) where σ_τ is standard deviation and m_τ mean of language spans; E_burst = logp_beta[sum_k |s_k^AIb - s_{k+1}^AIb| - sum_k |s_k^hb - s_{k+1}^hb|] &gt;= 0 (as used in paper).",
            "dataset_or_benchmark": "NYT tweets subset (100k) and generated LLM texts (CT2/ADI dataset)",
            "human_evaluation_details": "Human baseline = NYT tweets; comparisons automated; no human raters.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Burstiness differences diminish for larger LLMs; some small models (e.g., XLNet) show stronger separation but modern LLMs show similar burstiness to humans (see Table 22).",
            "comparison_to_human_generated": true,
            "comparison_results": "Larger LLMs produce burstiness scores closer to human texts, reducing effectiveness of burstiness-based detection.",
            "limitations_noted": "Burstiness depends on text type and vocabulary; can be computed at different granularities; not consistently discriminative for modern LLMs and across languages/domains.",
            "uuid": "e7788.1",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DetectGPT / NLC",
            "name_full": "Negative Log-Curvature (NLC) / DetectGPT",
            "brief_description": "DetectGPT hypothesis: AI-generated text tends to lie in regions of negative curvature of the LLM's log-probability landscape; detection via measuring log-probability drop under small perturbations (P_NLC) and z-scores.",
            "citation_title": "DetectGPT: Zero-shot machine-generated text detection using probability curvature",
            "mention_or_use": "use",
            "model_name": "multiple LLMs (15 evaluated), including experiments on GPT variants",
            "model_size": "various",
            "scientific_domain": "computer science / NLP",
            "theory_type": "detection hypothesis / statistical test",
            "evaluation_method_name": "Negative Log-Curvature (NLC) perturbation test (DetectGPT)",
            "evaluation_method_description": "Apply small perturbations to a text x and compute P_NLC = log p_theta(x) - log p_theta(x') averaged over perturbations; higher P_NLC indicates AI generation; classification via z-score threshold (DetectGPT used z &gt; 4).",
            "evaluation_metric": "P_NLC differences, z-score (normalized log-probability difference), p-values from bootstrap",
            "metric_definition": "P_NLC_theta = log p_theta(x) - log p_theta(x'); DetectGPT classifies AI-generated if z-score &gt; 4 where z = (orig_logprob - mean_perturbed_logprob)/std_perturbed_logprob.",
            "dataset_or_benchmark": "2000 samples for perturbation experiments reported (plus full CT2 dataset of 100k prompts and generated outputs)",
            "human_evaluation_details": "No human raters; statistical testing via bootstrap over perturbations and samples.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Authors could not replicate NLC pattern for GPT-4 and other LLMs; variance in NLC was minimal and not a reliable signal (see Fig.1 and Table 22).",
            "comparison_to_human_generated": true,
            "comparison_results": "No consistent NLC signal distinguishing modern LLM outputs from human text across tested models; DetectGPT hypothesis shown fragile.",
            "limitations_noted": "Requires access to log-probabilities from the generating LM (practical limitation), sensitive to choice of perturbations, computationally expensive if generator unknown, and findings are not robust across LLMs.",
            "uuid": "e7788.2",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Watermarking",
            "name_full": "Watermarking for LLM outputs (Kirchenbauer et al. proposals w_v1 and w_v2)",
            "brief_description": "Source-side watermarking injects subtle/modulated token selection patterns (a watermark) into LLM outputs to mark authorship; detection methods test presence of watermark; authors evaluate robustness to de-watermarking.",
            "citation_title": "On a watermark for large language models (Kirchenbauer et al., 2023a) / On the reliability of watermarks (Kirchenbauer et al., 2023b)",
            "mention_or_use": "use",
            "model_name": "applied to outputs of multiple LLMs (OPT, LLaMA, GPT variants, etc.)",
            "model_size": "various",
            "scientific_domain": "computer science / NLP",
            "theory_type": "defensive marking technique / detectability criterion",
            "evaluation_method_name": "Watermark detection and de-watermarking attack experiments",
            "evaluation_method_description": "Apply watermarking schemes (w_v1, w_v2) on generated text then attempt to remove watermark via de-watermarking methods (high-entropy token replacement, paraphrasing, combined) and measure detection/de-watermarking accuracy.",
            "evaluation_metric": "Watermark detection accuracy, de-watermarking success rate (%) across model and attack combinations",
            "metric_definition": "Accuracy defined as percentage of samples for which watermark detector correctly identifies watermark (or de-watermarking succeeded). Reported as percent values per model/attack (tables 3,7-20).",
            "dataset_or_benchmark": "Watermarked outputs from 15 LLMs generated using NYT prompts (CT2 dataset)",
            "human_evaluation_details": "No human judges; evaluations automated; paraphrase validity filtered using entailment model (RoBERTa Large trained on SNLI).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "De-watermarking approaches (spotting high-entropy token replacement, paraphrasing, and combined) achieved high success rates across many LLMs; w_v2 is slightly more robust but still circumventable (de-watermarking accuracies often &gt;70%).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Watermarking can be diluted by paraphrasing and targeted token replacements; robustness depends on watermark scheme and attacker capabilities; experiments used open-source paraphrasers and masking models and may not capture all real-world constraints.",
            "uuid": "e7788.3",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DeW1 / DeW2",
            "name_full": "De-watermarking techniques: DeW1 (high-entropy token replacement) and DeW2 (paraphrasing +/- token replacement)",
            "brief_description": "Attacks on watermarking: DeW1 spots high-entropy tokens and replaces them; DeW2 applies paraphrasing (Pegasus, T5, GPT-3.5) and/or token replacement to remove watermark signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "attacks applied to outputs of many LLMs (OPT, LLaMA, GPT variants, etc.)",
            "model_size": "various",
            "scientific_domain": "computer science / NLP (adversarial robustness)",
            "theory_type": "attack / robustness evaluation",
            "evaluation_method_name": "De-watermarking attack evaluation (DeW1, DeW2)",
            "evaluation_method_description": "Generate paraphrases and/or replace high-entropy words in watermarked text, then run watermark detector to measure whether watermark persists; paraphrase candidates filtered by entailment and MED thresholds.",
            "evaluation_metric": "De-watermarking accuracy (%) per model and per attack combination; paraphrase evaluation metrics: coverage, correctness, diversity.",
            "metric_definition": "De-watermarking accuracy = % samples where watermark is no longer detected after attack; Coverage = number of viable paraphrases (MED &gt; 2), Correctness = entailment pass rate via RoBERTa-Large (SNLI), Diversity = inverse BLEU (higher =&gt; more diverse).",
            "dataset_or_benchmark": "Watermarked outputs from 15 LLMs (CT2 dataset); paraphrase generators: Pegasus, T5, GPT-3.5",
            "human_evaluation_details": "Paraphrase correctness assessed via RoBERTa Large entailment model; no human raters used.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "GPT-3.5 paraphraser produced highest coverage/correctness/diversity and achieved strongest de-watermarking; tables (e.g., Table 3, Table 21) show de-watermarking accuracies often high (many &gt;70%, some &gt;90% depending on combo).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "De-watermarking efficacy depends on paraphraser quality and masking models used; real-world watermark schemes may adapt; entropy-based replacement assumes watermark concentrates on high-entropy tokens.",
            "uuid": "e7788.4",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Stylometric variation",
            "name_full": "Stylometric variation / stylometry (including Le Cam's lemma density approach)",
            "brief_description": "Use stylometric features (syntactic and lexical variation) and density estimation (Le Cam's lemma approximating sums by Poisson) to group/identify authors (LLMs) and compare AI vs human writing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "15 LLMs treated as distinct 'authors' vs. human baseline",
            "model_size": "various",
            "scientific_domain": "computer science / stylometry / authorship attribution",
            "theory_type": "stylometric detection / attribution model",
            "evaluation_method_name": "Stylometric density estimation using Le Cam's lemma",
            "evaluation_method_description": "Compute perplexity and burstiness as density functions and use Le Cam's lemma to approximate total variation between observed sentence-level counts and Poisson approximations to characterize LLM-specific densities; group LLMs by similarity and measure detectability.",
            "evaluation_metric": "Stylometric distinguishability rates (percent detectable), grouping consistency, applicability of learned lemma across models (relational matrix)",
            "metric_definition": "Probability densities L_plx_H and L_brsty_H computed via Le Cam's lemma approximations; detectability bands defined by percent thresholds (e.g., Detectable &gt;80%, Hard&gt;70%, Impossible&lt;50%).",
            "dataset_or_benchmark": "NYT tweets (human) and AI-generated CT2 dataset across 15 LLMs",
            "human_evaluation_details": "Human corpus treated as single author (simplifying assumption); no multi-annotator study.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Authors report grouping into three stylometric detectability bands: Detectable (80%+): T0, T5; Hard to detect (70%+): XLNet, StableLM, Dolly; Impossible (&lt;50%): LLaMA, OPT, GPT variants.",
            "comparison_to_human_generated": true,
            "comparison_results": "Stylometric density ranges overlap strongly for modern LLMs and humans; lemma learned for one LLM transfers only within same group.",
            "limitations_noted": "Simplifying assumption that all human text is from single author; stylometric features considered limited to perplexity and burstiness densities; alternative density estimators not explored in depth.",
            "uuid": "e7788.5",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Classifier-based detectors",
            "name_full": "Classifier-based detection approaches (RoBERTa-Large-Detector and similar)",
            "brief_description": "Binary classifiers trained to discriminate human vs. machine text or model-specific detectors trained on a model's outputs; evaluated for generalization across models and domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RoBERTa-Large-Detector (OpenAI) and other classifiers referenced (trained on model-specific corpora)",
            "model_size": "RoBERTa-Large; other detector sizes vary",
            "scientific_domain": "computer science / NLP / supervised classification",
            "theory_type": "supervised detection/classification",
            "evaluation_method_name": "Classifier accuracy and cross-model generalization tests",
            "evaluation_method_description": "Train binary classifiers on datasets of human and model-generated text (often model-specific) and measure accuracy, and test on texts from unseen LLMs to assess generalization.",
            "evaluation_metric": "Classification accuracy, false positive/negative rates; domain/model generalization performance",
            "metric_definition": "Accuracy = (TP+TN)/Total; detectors are often evaluated on held-out samples and on cross-model test sets.",
            "dataset_or_benchmark": "Detector training datasets (not fully enumerated here); OpenAI's RoBERTa-Large-Detector trained on model-specific generated/human corpora; authors' CT2 dataset used to test cross-model robustness.",
            "human_evaluation_details": "No human labeling reported for classifier training in this paper (classifiers referenced are pre-trained or previously published detectors).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Paper notes classifier detectors usually perform well on data from models they were trained on but generalize poorly to new/unseen LLMs and domains.",
            "comparison_to_human_generated": true,
            "comparison_results": "Detector performance degrades on outputs from different LLMs than those seen during training; thus classifier-based approaches are brittle.",
            "limitations_noted": "Most classifiers are model-specific; limited cross-model generalization; training requires labeled model outputs; susceptible to paraphrasing and de-watermarking attacks.",
            "uuid": "e7788.6",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AI Detectability Index (ADI)",
            "name_full": "AI Detectability Index (ADI)",
            "brief_description": "A composite, auto-computable index proposed to quantify an LLM's detectability based on core AGTD signals (perplexity and burstiness) with damping factors and scaling to a 0-100 range.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "computed for 15 LLMs evaluated in the paper",
            "model_size": "various",
            "scientific_domain": "computer science / policy-relevant NLP metric",
            "theory_type": "aggregate evaluation metric / benchmark",
            "evaluation_method_name": "AI Detectability Index (ADI) combining perplexity and burstiness (and optionally stylometric features)",
            "evaluation_method_description": "ADI_x = scaled aggregate over U samples of weighted differences between observed text statistics (P_t, B_t) and human baseline Le Cam densities (L_plx_H, L_brsty_H), normalized by human mean and modulated by damping factors δ1(x), δ2(x); finally scaled 0-100.",
            "evaluation_metric": "ADI score (0-100)",
            "metric_definition": "See paper formula: ADI_x = 100/(U*2) * sum_x [{δ1(x)*(P_t - L_plx_H)/(1-µ_plx_H)} + {δ2(x)*(B_t - L_brsty_H)/(1-µ_brsty_H)}], where P_t and B_t are defined aggregations of log probabilities across sentence windows; δ damping factors determined via rank and σ; scaled to 0-100.",
            "dataset_or_benchmark": "Computed over CT2 benchmark: 100k NYT prompts and generated outputs from 15 LLMs (authors' dataset)",
            "human_evaluation_details": "ADI uses human mean perplexity and burstiness (µ_plx_H, µ_brsty_H) derived from NYT baseline; no external human ratings used.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "ADI groups LLMs into three bands: detectable (T0, T5), hard-to-detect (XLNet, StableLM, Dolly, Vicuna), and non-detectable (LLaMA, OPT, GPT variants), with higher ADI meaning less detectable.",
            "comparison_to_human_generated": true,
            "comparison_results": "ADI captures distinguishability relative to human baseline; higher ADI corresponds to LLM outputs that are closer to human statistics and thus harder to detect.",
            "limitations_noted": "ADI currently built on perplexity and burstiness (authors note other features could be incorporated); sensitivity to baseline choice (NYT), damping parameters, and evolving LLMs; may require recalibration over time.",
            "uuid": "e7788.7",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CT2 benchmark",
            "name_full": "Counter Turing Test (CT2) benchmark",
            "brief_description": "A benchmark suite introduced by the authors consisting of techniques, datasets and leaderboards for evaluating robustness of AGTD methods across multiple LLMs (planned open-source).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "evaluated across 15 LLMs",
            "model_size": "various",
            "scientific_domain": "computer science / NLP benchmark",
            "theory_type": "benchmark / evaluation framework",
            "evaluation_method_name": "CT2 (Counter Turing Test) benchmark",
            "evaluation_method_description": "Use parallel data (human NYT tweets + LLM generations) and a suite of detection methods (watermarking, perplexity/burstiness, NLC, stylometry, classifiers) plus de-watermarking attacks to comprehensively evaluate AGTD robustness; organizes leaderboards.",
            "evaluation_metric": "Method-specific metrics (accuracy, de-watermarking success rates, ADI scores); aggregated detectability rankings",
            "metric_definition": "See individual methods (perplexity, burstiness, watermark detection accuracy, ADI 0-100 scale).",
            "dataset_or_benchmark": "NYT tweets subset (100k) used as prompts; CT2 dataset of generated outputs across 15 LLMs; authors plan to publish leaderboards and dataset.",
            "human_evaluation_details": "Human baseline uses NYT tweets; no human rating panels described; computational/automated evaluations dominate.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "CT2 experiments indicate fragility of SoTA AGTD methods under a range of attacks and modern LLMs; results summarized across tables showing de-watermarking accuracies and statistical metrics (Table22 etc.).",
            "comparison_to_human_generated": true,
            "comparison_results": "CT2 shows modern LLM outputs are often statistically close to human texts on the employed signals, challenging detection methods.",
            "limitations_noted": "Benchmark depends on chosen human baseline (NYT tweets) and set of LLMs; will need continuous updates as new LLMs and detection methods emerge.",
            "uuid": "e7788.8",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "NYT tweets dataset",
            "name_full": "NYT Twitter-handle prompt corpus (subset of ~100k tweets)",
            "brief_description": "Curated prompt source: ~100k NYT tweets chosen as prompts because they are high-quality, contain URLs to articles, and cover varied topics; used to generate parallel human/AI corpora for evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "used as prompts for generation by 15 LLMs",
            "model_size": "N/A",
            "scientific_domain": "computer science / NLP dataset",
            "theory_type": "evaluation dataset / benchmark corpus",
            "evaluation_method_name": "NYT tweets (100k) parallel prompt corpus",
            "evaluation_method_description": "Use cleaned NYT tweets (remove hashtags/mentions) as prompts to generate LLM outputs; paired human tweet text serves as human baseline for statistical comparisons and ADI computation.",
            "evaluation_metric": "Used to compute human baseline means (µ_plx_H, µ_brsty_H) and for U=100k sample aggregation in ADI",
            "metric_definition": "Human baseline statistics computed as sample means and variances over the 100k tweets for perplexity and burstiness.",
            "dataset_or_benchmark": "NYT tweets subset (authors' selection of ~100k tweets from ~393k total)",
            "human_evaluation_details": "NYT tweets considered human-written standard; no further human annotation reported.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Used to compute ADI baselines and to prompt 15 LLMs for all experiments; authors will publish curated datasets.",
            "comparison_to_human_generated": true,
            "comparison_results": "NYT tweets used as human baseline for all comparative metrics; modern LLMs approximate these statistics closely.",
            "limitations_noted": "Human baseline is single-style (journalistic tweets) and treated as single 'author' in stylometry experiments; may bias ADI toward this genre.",
            "uuid": "e7788.9",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Paraphrase evaluation metrics",
            "name_full": "Paraphrase evaluation: Coverage, Correctness, Diversity (inverse BLEU), MED",
            "brief_description": "Operational criteria employed to select paraphrases for de-watermarking: MED filter, entailment-based correctness via RoBERTa Large (SNLI), and diversity measured via inverse BLEU.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "paraphrase generators evaluated: Pegasus, T5, GPT-3.5 (gpt-3.5-turbo-0301)",
            "model_size": "Pegasus/T5/GPT-3.5 variants as used in experiments",
            "scientific_domain": "computer science / NLP / adversarial evaluation",
            "theory_type": "evaluation criteria for paraphrase-based attacks",
            "evaluation_method_name": "Coverage/Correctness/Diversity paraphrase evaluation",
            "evaluation_method_description": "Generate up to 5 paraphrases per claim, keep paraphrases with MED &gt; 2, filter for entailment using RoBERTa-Large (SNLI), and measure linguistic diversity by inverse BLEU between paraphrase candidates.",
            "evaluation_metric": "Coverage (count of viable paraphrases), Correctness (% entailment pass), Diversity (inverse BLEU score), De-watermarking accuracy after paraphrase",
            "metric_definition": "Coverage = number of paraphrases per input with MED &gt; 2; Correctness = % paraphrases judged entailed by RoBERTa-Large; Diversity = average inverse BLEU between paraphrases; de-watermarking accuracy as percent success.",
            "dataset_or_benchmark": "Paraphrases applied to CT2 watermarked outputs (across 15 LLMs)",
            "human_evaluation_details": "Correctness assessed by automatic entailment model; no human annotations.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "GPT-3.5 paraphraser achieved highest coverage, correctness and diversity (e.g., Coverage 35.51, Correctness 88.16%, Diversity 7.72 in Table 5); paraphrasing substantially reduces watermark detection.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Automatic entailment filter may not perfectly capture semantic preservation; inverse BLEU is a proxy for diversity and has known limitations.",
            "uuid": "e7788.10",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Statistical testing / Bootstrapping",
            "name_full": "Bootstrap resampling and basic statistical measures (mean, std, entropy, p-tests)",
            "brief_description": "Non-parametric bootstrap used to estimate sampling distributions, confidence intervals, and p-values for comparing metrics (perplexity, burstiness, NLC) between human and AI text; also compute mean, std, entropy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "applied to statistics computed over CT2 dataset for 15 LLMs",
            "model_size": "N/A",
            "scientific_domain": "statistics / empirical NLP evaluation",
            "theory_type": "statistical validation method",
            "evaluation_method_name": "Bootstrap statistical testing",
            "evaluation_method_description": "Resample the empirical sample with replacement many times to build an empirical sampling distribution for test statistics (means, differences) and compute confidence intervals and p-values; used because distributions may be non-Gaussian.",
            "evaluation_metric": "Bootstrap p-values, bootstrap confidence intervals, sample mean and standard deviation, entropy",
            "metric_definition": "Bootstrap constructs m resamples of size n from sample S; compute statistic on each resample to form sampling distribution; p-value derived from distribution relative to null.",
            "dataset_or_benchmark": "Applied to CT2 datasets (100k prompts and generated outputs), plus per-model sample sets (e.g., 2000 samples used for perturbations)",
            "human_evaluation_details": "No human raters; purely statistical resampling.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Bootstrap tests show many AGTD signals non-significant for modern LLMs (see Table 22 with bootstrap p-values α = 0.05 indicating non-significance in many comparisons).",
            "comparison_to_human_generated": true,
            "comparison_results": "Bootstrap-based tests failed to find robust significant differences for many LLMs between human and AI on NLC, perplexity and burstiness.",
            "limitations_noted": "Bootstrap effectiveness depends on representativeness of original sample; increasing number of resamples does not add new information beyond original sample size.",
            "uuid": "e7788.11",
            "source_info": {
                "paper_title": "Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DetectGPT: Zero-shot machine-generated text detection using probability curvature",
            "rating": 2,
            "sanitized_title": "detectgpt_zeroshot_machinegenerated_text_detection_using_probability_curvature"
        },
        {
            "paper_title": "A watermark for large language models",
            "rating": 2,
            "sanitized_title": "a_watermark_for_large_language_models"
        },
        {
            "paper_title": "On the reliability of watermarks for large language models",
            "rating": 2,
            "sanitized_title": "on_the_reliability_of_watermarks_for_large_language_models"
        },
        {
            "paper_title": "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "rating": 2,
            "sanitized_title": "paraphrasing_evades_detectors_of_aigenerated_text_but_retrieval_is_an_effective_defense"
        },
        {
            "paper_title": "Paraphrasing evades detectors of ai-generated text",
            "rating": 1,
            "sanitized_title": "paraphrasing_evades_detectors_of_aigenerated_text"
        },
        {
            "paper_title": "Can ai-generated text be reliably detected?",
            "rating": 2,
            "sanitized_title": "can_aigenerated_text_be_reliably_detected"
        },
        {
            "paper_title": "GLTR: Statistical detection and visualization of generated text",
            "rating": 1,
            "sanitized_title": "gltr_statistical_detection_and_visualization_of_generated_text"
        },
        {
            "paper_title": "Defending against neural fake news",
            "rating": 1,
            "sanitized_title": "defending_against_neural_fake_news"
        },
        {
            "paper_title": "Stylometric detection of ai-generated text in twitter timelines",
            "rating": 2,
            "sanitized_title": "stylometric_detection_of_aigenerated_text_in_twitter_timelines"
        },
        {
            "paper_title": "Do Foundation Model Providers Comply with the EU AI Act?",
            "rating": 1,
            "sanitized_title": "do_foundation_model_providers_comply_with_the_eu_ai_act"
        }
    ],
    "cost": 0.023463499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think -Introducing AI Detectability Index
24 Oct 2023</p>
<p>Megha Chakraborty 
AI Institute
University of South Carolina
USA</p>
<p>S M Towhidul 
Islam Tonmoy 
IUT
Bangladesh</p>
<p>S M Mehedi 
IUT
Bangladesh</p>
<p>Krish Sharma 
NIT Silchar
India</p>
<p>Niyar R Barman 
NIT Silchar
India</p>
<p>Chandan Gupta 
IIIT Delhi
India</p>
<p>Shreya Gautam 
BITS Mesra
India</p>
<p>Tanay Kumar 
BITS Mesra
India</p>
<p>Vinija Jain 
Stanford University
USA</p>
<p>Amazon, AIUSA</p>
<p>Aman Chadha 
Stanford University
USA</p>
<p>Amazon, AIUSA</p>
<p>Amit P Sheth 
AI Institute
University of South Carolina
USA</p>
<p>Amitava Das 
AI Institute
University of South Carolina
USA</p>
<p>Counter Turing Test (CT 2 ): AI-Generated Text Detection is Not as Easy as You May Think -Introducing AI Detectability Index
24 Oct 202377930EA0D099BEE343F172788C5A1349arXiv:2310.05030v2[cs.CL]
With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly.This triggered a series of events, including an open letter (Marcus, 2023), signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4.To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office (Copyright-Office, 2023) released a statement stating that "If a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it".Furthermore, both the US (White-House, 2023) and the EU (European-Recently, six methods and their combinations have been proposed for AGTD: (i) watermarking, (ii) perplexity estimation, (iii) burstiness estimation, (iv) negative log-likelihood curvature, (v) stylometric variation, and (vi) classifier-based approaches.This paper focuses on critiquing their robustness and presents empirical evidence demonstrating their brittleness.Watermarking: Watermarking AI-generated text, first proposed by Wiggers (2022), entails the incorporation of an imperceptible signal to establish the authorship of a specific text with a high degree of certainty.This approach is analogous to encryption and decryption.Kirchenbauer et al. (2023a) (w v1 ) were the first to present operational watermarking models for LLMs, but their initial proposal faced criticism.Sadasivan et al. (2023)shared their initial studies suggesting that paraphrasing can efficiently eliminate watermarks.In a subsequent paper (Kirchenbauer et al., 2023b) (w v2 ), the
 Parliament, 2023) 
governments have recently drafted their initial proposals regarding the regulatory framework for AI.Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection.This paper introduces the Counter Turing Test (CT 2 ), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques.Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny.Amidst the extensive deliberations on policymaking for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs.Thus, to † Work does not relate to position at Amazon.establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI).We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a higher ADI, indicating they are less detectable compared to smaller LLMs.We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.</p>
<p>1 Proposed AI-Generated Text Detection Techniques (AGTD) -A Review authors put forth evidently more resilient watermarking techniques, asserting that paraphrasing does not significantly disrupt watermark signals in this iteration of their research.By conducting extensive experiments (detailed in Section 3), our study provides a thorough investigation of the dewatermarking techniques w v1 and w v2 , demonstrating that the watermarked texts generated by both methods can be circumvented, albeit with a slight decrease in de-watermarking accuracy observed with w v2 .These results further strengthen our contention that text watermarking is fragile and lacks reliability for real-life applications.Perplexity Estimation: The hypothesis related to perplexity-based AGTD methods is that humans exhibit significant variation in linguistic constraints, syntax, vocabulary, and other factors (aka perplexity) from one sentence to another.In contrast, LLMs display a higher degree of consistency in their linguistic style and structure.Employing this hypothesis, GPTZero (Tian, 2023) devised an AGTD tool that posited the overall perplexity human-generated text should surpass that of AI-generated text, as in the equation: logp Θ (h text ) − logp Θ (AI text ) ≥ 0 (Appendix C).Furthermore, GPTZero assumes that the variations in perplexity across sentences would also be lower for AI-generated text.This phenomenon could potentially be quantified by estimating the entropy for sentence-wise perplexity, as depicted in the equation:
E perp = logp Θ [Σ n k=1 (|s k h − s k+1 h |)] − logp Θ [Σ n
k=1 (|s k AI − s k+1 AI |)] ≥ 0; where s k h and s k AI represent k th sentences of human and AI-written text respectively.Burstiness Estimation: Burstiness refers to the patterns observed in word choice and vocabulary size.GPTZero (Tian, 2023) was the first to introduce burstiness estimation for AGTD.In this context, the hypothesis suggests that AI-generated text displays a higher frequency of clusters or bursts of similar words or phrases within shorter sections of the text.In contrast, humans exhibit a broader variation in their lexical choices, showcasing a more extensive range of vocabulary.Let σ τ denote the Figure 1: (Top) The negative log-curvature hypothesis proposed by Mitchell et al. (2023).According to their claim, any perturbations made to the AI-generated text should predominantly fall within a region of negative curvature.(Bottom) Our experiments using 15 LLMs with 20 perturbations indicate that the text generated by GPT 3.0 and variants do not align with this hypothesis.Moreover, for the other LLMs, the variance in the negative log-curvature was so minimal that it had to be disregarded as a reliable indication.and represent fake and real sample respectively, whereas and depict perturbed fake and real sample.</p>
<p>standard deviation of the language spans and m τ the mean of the language spans.Burstiness (b) is calculated as b = ( σ τ /m τ −1 σ τ /m τ +1 ) and is bounded within the interval [-1, 1].Therefore the hypothesis is b H − b AI ≥ 0, where b H is the mean burstiness of human writers and b AI is the mean burstiness of AI aka a particular LLM.Corpora with antibursty, periodic dispersions of switch points take on burstiness values closer to -1.In contrast, corpora with less predictable patterns of switching take on values closer to 1.It is worth noting that burstiness could also be calculated sentence-wise and/or text fragment-wise and then their entropy could be defined as:
E burst = logp β [Σ n k=1 (|s k AIb − s k+1 AIb |) − logp β [Σ n k=1 (|s k hb − s k+1 hb |)]] ≥ 0.
Nevertheless, our comprehensive experiments involving 15 LLMs indicate that this hypothesis does not consistently provide a discernible signal.Furthermore, recent LLMs like GPT-3.5/4,MPT (OpenAI, 2023a;Team, 2023) have demonstrated the utilization of a wide range of vocabulary, challenging the hypothesis.Section 4 discusses our experiments on perplexity and burstiness estimation.</p>
<p>Negative Log-Curvature (NLC): DetectGPT (Mitchell et al., 2023) introduced the concept of Negative Log-Curvature (NLC) to detect AIgenerated text.The hypothesis is that text generated by the the model tends to lie in the negative curvature areas of the model's log probability, i.e. a text generated by a source LLM p θ typically lies in the areas of negative curvature of the log probability function of p θ , unlike human-written text.In other words, we apply small perturbations to a passage x ∼ p θ , producing x.Defining P NLC θ as the quantity logp θ (x) − logp θ ( x), P NLC θ should be larger on average for AI-generated samples than human-written text (see an example in Table 1 and the visual intuition of the hypothesis in Fig. 1).Expressed mathematically:
P NLC AI − P NLC H ≥ 0.
It is important to note that DetectGPT's findings were derived from text-snippet analysis, but there is potential to reevaluate this approach by examining smaller fragments, such as sentences.This would enable the calculation of averages or entropies, akin to how perplexity and burstiness are measured.Finally, the limited number of perturbation patterns per sentence in (Mitchell et al., 2023) affect the reliability of results (cf.Section 5 for details).</p>
<p>Input Type Sentence</p>
<p>Original</p>
<p>This sentence is generated by an AI or human</p>
<p>Perturbed</p>
<p>This writing is created by an AI or person Table 1: An example perturbation as proposed in DetectGPT (Mitchell et al., 2023).</p>
<p>Stylometric variation:</p>
<p>Stylometry is dedicated to analyzing the linguistic style of text in order to differentiate between various writers.➠ Introducing AI Detectability Index (ADI) as a measure for LLMs to infer whether their generations are detectable as AI-generated or not.</p>
<p>➠ Conducting a thorough examination of 15 contemporary LLMs to establish the aforementioned points.</p>
<p>➠ Both benchmarks -CT 2 and ADI -will be published as open-source leaderboards.</p>
<p>➠ Curated datasets will be made publicly available.</p>
<p>Design Choices for CT and ADI Study</p>
<p>This section discusses our selected LLMs and elaborates on our data generation methods.More details in Appendix A.</p>
<p>LLMs: Rationale and Coverage</p>
<p>We chose a wide gamut of 15 LLMs that have exhibited exceptional results on a wide range of NLP tasks.They are: (i) GPT 4 (OpenAI, 2023a); (ii) GPT 3.5 (Chen et al., 2023); (iii) GPT 3 (Brown et al., 2020); (iv) GPT 2 (Radford et al., 2019); (v) MPT (Team, 2023) (Raffel et al., 2020); (xv) T0 (Sanh et al., 2021).</p>
<p>Given that the field is ever-evolving, we admit that this process will never be complete but rather continue to expand.Hence, we plan to keep the CT 2 benchmark leaderboard open to researchers, allowing for continuous updates and contributions.</p>
<p>Datasets: Generation and Statistics</p>
<p>To develop CT 2 and ADI, we utilize parallel data comprising both human-written and AI-generated text on the same topic.We select The New York Times (NYT) Twitter handle as our prompt source for the following reasons.Firstly, the handle comprises approximately 393K tweets that cover a variety of topics.For our work, we chose a subset of 100K tweets.Secondly, NYT is renowned for its reliability and credibility.The tweets from NYT exhibit a high level of word-craftsmanship by experienced journalists, devoid of grammatical mistakes.Thirdly, all the tweets from this source include URLs that lead to the corresponding human-written news articles.These tweets serve as prompts for the 15 LLMs, after eliminating hashtags and mentions during pre-processing.Appendix G offers the generated texts from 15 chosen LLMs when given the prompt "AI generated text detection is not easy."</p>
<p>3 De-Watermarking: Discovering its Ease and Efficiency</p>
<p>In the realm of philosophy, watermarking is typically regarded as a source-side activity.It is highly plausible that organizations engaged in the development and deployment of LLMs will progressively adopt this practice in the future.Additionally, regulatory mandates may necessitate the implementation of watermarking as an obligatory measure.The question that remains unanswered is the level of difficulty in circumventing watermarking, i.e., de-watermarking, when dealing with watermarked AI-generated text.In this section, we present our rigorous experiments that employ three methods capable of de-watermarking an AI-generated text that has been watermarked: (i) spotting high entropy words and replacing them, (ii) paraphrasing, (iii) paraphrasing + replacing high-entropy words Table 2 showcases an instance of de-watermarking utilizing two techniques for OPT as target LLM.</p>
<p>De-watermarking by Spotting and</p>
<p>Replacing High Entropy Words (DeW 1 )</p>
<p>The central concept behind the text watermarking proposed by Kirchenbauer et al. (2023a)   We have used paraphrasing as yet another technique to remove watermarking from LLMs.Idea 1) Feed textual input to a paraphraser model such as Pegasus, T5, GPT-3.5 and evaluate watermarking for the paraphrased text.Idea 2) Replace the high entropy words, which are likely to be the watermarked tokens, and then paraphrase the text to ensure that we have eliminated the watermarks.We perform a comprehensive analysis of both qualitative and quantitative aspects of automatic paraphrasing for the purpose of de-watermarking.We chose three SoTA paraphrase models: (a) Pegasus (Zhang et al., 2020) Table 6: Experimental results of automatic paraphrasing models based on three factors: (i) coverage, (ii) correctness and (iii) diversity; GPT-3.5 (gpt-3.5-turbo-0301)can be seen as the most performant.</p>
<p>variant) (Chung et al., 2022), and (c) GPT-3.5 (gpt-3.5-turbo-0301variant) (Brown et al., 2020).We seek answers to the following questions: (i) What is the accuracy of the paraphrases generated?(ii) How do they distort the original content?(iii) Are all the possible candidates generated by the paraphrase models successfully de-watermarked?(iv) Which paraphrase module has a greater impact on the de-watermarking process?To address these questions, we evaluate the paraphrase modules based on three key dimensions: (i) Coverage: number of considerable paraphrase generations, (ii) Correctness: correctness of the generations, (iii) Diversity: linguistic diversity in the generations.Our experiments showed that GPT-3.5 (gpt-3.5-turbo-0301variant) is the most suitable paraphraser (Fig. 2).Please see details of experiments in Appendix B.3.</p>
<p>For a given text input, we generate multiple paraphrases using various SoTA models.In the process of choosing the appropriate paraphrase model based on a list of available models, the primary question we asked is how to make sure the generated paraphrases are rich in diversity while still being linguistically correct.We delineate the process followed to achieve this as follows.Let's say we have a claim c.We generate n paraphrases us-GPT 3.5 -turbo -0301 PEGASUS T5 -Large ing a paraphrasing model.This yields a set of p c 1 , . .., p c n .Next, we make pair-wise comparisons of these paraphrases with c, resulting in c − p c 1 , . .., and c − p c n .At this step, we identify the examples which are entailed, and only those are chosen.For the entailment task, we have utilized RoBERTa Large (Liu et al., 2019) -a SoTA model trained on the SNLI task (Bowman et al., 2015).Key Findings from De-Watermarking Experiments: As shown in Table 3 and Table 5, our experiments provide empirical evidence suggesting that the watermarking applied to AI-generated text can be readily circumvented (cf.Appendix B).</p>
<p>Reliability of Perplexity and Burstiness as AGTD Signals</p>
<p>In this section, we extensively investigate the reliability of perplexity and burstiness as AGTD signals.Based on our empirical findings, it is evident that the text produced by newer LLMs is nearly indistinguishable from human-written text from a statistical perspective.</p>
<p>The hypothesis assumes that AI-generated text displays a higher frequency of clusters or bursts of similar words or phrases within shorter sections of the text.In contrast, humans exhibit a broader variation in their lexical choices, showcasing a more extensive range of vocabulary.Moreover, sentence-wise human shows more variety in terms of length, and structure in comparison with AIgenerated text.To measure this we have utilized entropy.The entropy p i logp i of a random variable is the average level of surprise, or uncertainty.</p>
<p>Estimating Perplexity -Human vs. AI</p>
<p>Perplexity is a metric utilized for computing the probability of a given sequence of words in natural language.It is computed as
e − 1 N ∑ N i=1 log 2 p(w i ) ,
where N represents the length of the word sequence, and p(w i ) denotes the probability of the individual word w i .As discussed previously, GPTZero (Tian, 2023) assumes that humangenerated text exhibits more variations in both overall perplexity and sentence-wise perplexity as compared to AI-generated text.To evaluate the strength of this proposition, we compare text samples generated by 15 LLMs with corresponding human-generated text on the same topic.Our empirical findings indicate that larger LLMs, such as GPT-3+, closely resemble human-generated text and exhibit minimal distinctiveness.However, relatively smaller models such as XLNet, BLOOM, etc. are easily distinguishable from human-generated text.Fig. 3 demonstrates a side-by-side comparison of the overall perplexity of GPT4 and T5.We report results for 3 LLMs in Table 4 (cf.Table 22 in Appendix C for results over all 15 LLMs).</p>
<p>Estimating Burstiness -Human vs. AI</p>
<p>In Section 1, we discussed the hypothesis that explores the contrasting burstiness patterns between human-written text and AI-generated text.Previous studies that have developed AGTD techniques based on burstiness include (Rychlỳ, 2011) and (Cummins, 2017).Table 4 shows that there is less distinction in the standard deviation of burstiness scores between AI-generated and human text for OPT.However, when it comes to XLNet, the difference becomes more pronounced.From several such examples, we infer that larger and more complex LLMs gave similar burstiness scores as humans.Hence, we conclude that as the size or complexity of the models increases, the deviation in burstiness scores diminishes.This, in turn, reinforces our claim that perplexity or burstiness estimations cannot be considered as reliable for AGTD (cf.Appendix C).</p>
<p>Negative Log-Curvature (NLC)</p>
<p>In Section 1, we discussed the NLC-based AGTD hypothesis (Mitchell et al., 2023).Our experimental results, depicted in Fig. 1, demonstrate that we are unable to corroborate the same NLC pattern for GPT4.To ensure the reliability of our experiments, we performed 20 perturbations per sentence.Fig. 1 (bottom) presents a comparative analysis of 20 perturbation patterns observed in 2000 samples of OPTgenerated text and human-written text on the same topic.Regrettably, we do not see any discernible pattern.To fortify our conclusions, we compute the standard deviation, mean, and entropy, and conduct a statistical validity test using bootstrapping, which is more appropriate for non-Gaussian distributions (Kim, 2015;Boos and Brownie, 1989).Table 22 documents the results (cf.Appendix C).Based on our experimental results, we argue that NLC is not a robust method for AGTD. the purpose of authorship attribution.Our investigation, which differs from the study conducted by Kumarage et al. (2023), represents the first attempt to explore the stylometric variations between humanwritten text and AI-generated text.Specifically, we assign 15 LLMs as distinct authors, whereas text composed by humans is presumed to originate from a hypothetical 16 th author.Our task involves identifying stylometric variations among these 16 authors.After examining other alternatives put forth in previous studies such as (Tulchinskii et al., 2023), we encountered difficulties in drawing meaningful conclusions regarding the suitability of these methods for AGTD.Therefore, we focus our investigations on a specific approach that involves using perplexity (as a syntactic feature) and burstiness (as a lexical choice feature) as density functions to identify a specific LLM.By examining the range of values produced by these functions, we aim to pinpoint a specific LLM associated with a given text.Probability density such as
L plx H = ∑ ∞ k=0 Pr(S k plx ) − λ k n e −λn k! and L brsty H = ∑ ∞ k=0 Pr(S k brsty ) − λ k n e −λn k!
are calculated using Le Cam's lemma (Cam, 1986(Cam, -2012)), which gives the total variation distance between the sum of independent Bernoulli variables and a Poisson random variable with the same mean.Where Pr(S k plx ) is the perplexity and Pr(S k brsty ) is the brustiness of the of k th sentence respectively.In particular, it tells us that the sum is approximately Poisson in a specific sense (see more in Appendix E).Our experiment suggests stylistic feature estimation may not be very distinctive, with only broad ranges to group LLMs: (i) Detectable (80%+): T0 and T5, (ii) Hard to detect (70%+): XLNet, StableLM, and Dolly, and (iii) Impossible to detect (&lt;50%): LLaMA, OPT, GPT, and variations.</p>
<p>Our experiment yielded intriguing results.Given that our stylometric analysis is solely based on density functions, we posed the question: what would happen if we learned the search density for one LLM and applied it to another LLM?To explore this, we generated a relational matrix, as depicted in Fig. 7.As previously described and illustrated in Fig. 5, the LLMs can be classified into three groups: (i) easily detectable, (ii) hard to detect, and (iii) not detectable.Fig. 7 demonstrates that Le Cam's lemma learned for one LLM is only applicable to other LLMs within the same group.For instance, the lemma learned from GPT 4 can be successfully applied to GPT-3.5, OPT, and GPT-3, but not beyond that.Similarly, Vicuna, StableLM, and LLaMA form the second group.Fig. 4 offers a visual summary.</p>
<p>AI Detectability Index (ADI)</p>
<p>As new LLMs continue to emerge at an accelerated pace, the usability of prevailing AGTD techniques might not endure indefinitely.To align with the ever-changing landscape of LLMs, we introduce the AI Detectability Index (ADI), which identifies the discernable range for LLMs based on SoTA AGTD techniques.The hypothesis behind this proposal is that both LLMs and AGTD techniques' SoTA benchmarks can be regularly updated to adapt to the evolving landscape.Additionally, ADI serves as a litmus test to gauge whether contemporary LLMs have surpassed the ADI benchmark and are thereby rendering themselves impervious to detection, or whether new methods for AI-generated text detection will require the ADI standard to be reset and re-calibrated.</p>
<p>Among the various paradigms of AGTD, we select perplexity and burstiness as the foundation for quantifying the ADI.We contend that NLC is a derivative function of basic perplexity and burstiness, and if there are distinguishable patterns in NLC within AI-generated text, they should be well captured by perplexity and burstiness.We present a summary in Fig. 4 that illustrates the detectable and non-detectable sets of LLMs based on ADI scores obtained using stylometry and classification methods.It is evident that the detectable LLM set is relatively small for both paradigms, while the combination of perplexity and burstiness consistently provides a stable ADI spectrum.Furthermore, we argue that both stylistic features and classification are also derived functions of basic per- plexity and burstiness.ADI serves to encapsulate the overall distinguishability between AI-written and human-written text, employing the formula:
ADI x = 100 U×2 * [∑ U x=1 {δ 1 (x) * Pt −L plx H 1−µ plx H } + {δ 2 (x) * Bt −L brsty H 1−µ brsty H }] (1)
where,
P t = 1 U * {∑ U x=1 logp i u − logp i+1 u } and B t = 1 U * {∑ U x=1 logp i+(i+1)+(i+2) u − logp (i+3)+(i+4)+(i+5) u }.
When confronted with a random input text, it is difficult to predict its resemblance to humanwritten text on the specific subject.Therefore, to calculate ADI we employ the mean perplexity (µ plx H ) and burstiness (µ brsty H</p>
<p>) derived from human-written text.Furthermore, to enhance the comparison between the current text and human text, Le Cam's lemma has been applied using precalculated values (L plx H and L brsty H ) as discussed in Section 6.To assess the overall contrast a summation has been used over all the 100K data points as depicted here by U. Lastly, comparative measures are needed to rank LLMs based on their detectability.This is achieved using multiplicative damping factors, δ 1 (x) and δ 2 (x), which are calculated based on µ ± rank x × σ .Initially, we calculate the ADI for all 15 LLMs, considering δ 1 (x) and δ 2 (x) as 0.5.With these initial ADIs, we obtain the mean (µ) and standard deviation (σ ), allowing us to recalculate the ADIs for all the LLMs.The resulting ADIs are then ranked and scaled providing a comparative spectrum as presented in Fig. 4.This scaling process is similar to Z-Score Normalization and/or Min-max normalization (Wikipedia, 2019).However, having damping factors is an easier option for exponential smoothing while we have a handful of data points.Finally, for better human readability ADI is scaled between 0 − 100.</p>
<p>From the methods we considered, it is unlikely that any of them would be effective for models with high ADI, as shown by our experiments and results.As LLMs get more advanced, we assume that the current AGTD methods would become even more unreliable.With that in mind, ADI will remain a spectrum to judge which LLM is detectable and vs. which is not.Please refer to Appendix F for more discussion.</p>
<p>The ADI spectrum reveals the presence of three distinct groups.T0 and T5 are situated within the realm of detectable range, while XLNet, StableLM, Dolly, and Vicuna reside within the difficult-to-detect range.The remaining LLMs are deemed virtually impervious to detection through the utilization of prevailing SoTA AGTD techniques.It is conceivable that forthcoming advancements may lead to improved AGTD techniques and/or LLMs imbued with heightened human-like attributes that render them impossible to detect.Regardless of the unfolding future, ADI shall persist in serving the broader AI community and contribute to AI-related policy-making by identifying non-detectable LLMs that necessitate monitoring through policy control measures.</p>
<p>Conclusion</p>
<p>Our proposition is that SoTA AGTD techniques exhibit fragility.We provide empirical evidence to substantiate this argument by conducting experiments on 15 different LLMs.We proposed AI Detectability Index (ADI), a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels.The excitement and success of LLMs have resulted in their extensive proliferation, and this trend is anticipated to persist regardless of the future course they take.In light of this, the CT 2 benchmark and the ADI will continue to play a vital role in catering to the scientific community.(Bommasani et al., 2023).In this study, the authors put forward a grading system consisting of 12 aspects for evaluating Language Models (LLMs).These aspects include (i) data sources, (ii) data governance, (iii) copyrighted data, (iv) compute, (v) energy, (vi) capabilities &amp; limitations, (vii) risk &amp; mitigations, (viii) evaluation, (ix) testing, (x) machine-generated content, (xi) member states, and (xii) downstream documentation.The overall grading of each LLM can be observed in Fig. 5.While this study is commendable, it appears to be inherently incomplete due to the ever-evolving nature of LLMs.Since all scores are assigned manually, any future changes will require a reassessment of this rubric, while ADI is auto-computable.Furthermore, we propose that ADI should be considered the most suitable metric for assessing risk and mitigations.9.1 Addressing Opposing Views by Chakraborty et al. (2023) It is important to note that a recent study (Chakraborty et al., 2023) contradicts our findings and claims otherwise.The study postulates that given enough sample points, whether the output was derived from a human vs an LLM is detectable, irrespective of the LLM used for AI-generated text.</p>
<p>The sample size of this dataset is a function of the difference in the distribution of human text vs AItext, with a smaller sample size enabling detection if the distributions show significant differences.However, the study does not provide empirical evidence or specify the required sample size, thus leaving the claim as a hypothesis at this stage.Furthermore, the authors propose that employing techniques such as watermarking can change the distributions of AI text, making it more separable from human-text distribution and thus detectable.However, the main drawback of this argument is that given a single text snippet (say, an online article or a written essay), detecting whether it is AI-generated is not possible.Also, the proposed technique may not be cost-efficient compute-wise, especially as new LLMs emerge.However, the authors did not provide any empirical evidence to support this hypothesis.</p>
<p>Limitations: This paper delves into the discussion of six primary methods for AGTD and their potential combinations.These methods include (i) watermarking, (ii) perplexity estimation, (iii) burstiness estimation, (iv) negative loglikelihood curvature, (v) stylometric variation, and (vi) classifier-based approaches.</p>
<p>Our empirical research strongly indicates that the proposed methods are vulnerable to tampering or manipulation in various ways.We provide extensive empirical evidence to support this argument.However, it is important to acknowledge that there may still exist potential deficiencies in our experiments.In this section, we explore and discuss further avenues for investigation in order to address these potential shortcomings.In the subsequent paragraph, we outline the potential limitations associated with each of the methods we have previously investigated.</p>
<p>Watermarking</p>
<p>Although Kirchenbauer et al. (2023a) was the pioneering paper to introduce watermarking for AIgenerated text, this research has encountered numerous criticisms since its inception.A major concern raised by several fellow researchers (Sadasivan et al., 2023) is that watermarking can be easily circumvented through machine-generated paraphrasing.In our experiment, we have presented two potential de-watermarking techniques.Subsequently, the same group of researchers published a follow-up paper (Kirchenbauer et al., 2023b) in which they asserted the development of a more advanced and robust watermarking technique.We assessed this claim as well and discovered that de-watermarking remains feasible.However, although the overall accuracy of de-watermarking has decreased, it still retains considerable strength.As the paper was published on June 9 th , 2023, we will include the complete experiment details in the final version of our report.</p>
<p>In their work, Kirchenbauer et al. (2023b) put forward improved watermarking techniques by enhancing the hashing mechanism for selecting watermarking keys and introducing more effective watermark detection techniques.They conducted extensive testing on de-watermarking possibilities, considering both machine-generated paraphrasing and human paraphrasing, and observed dilution in the strength of the watermark, which aligns with their findings.</p>
<p>Although paraphrasing is a powerful technique for attacking watermark text, we argue that highentropy-based word replacement offers a superior approach.When using high-entropy word replacements, it becomes exceedingly difficult for watermark detection modules to identify the newly generated text, even after paraphrasing.We will now elaborate on our rationale.In their work, Kirchenbauer et al. (2023b) identify content words such as nouns, verbs, adjectives, and adverbs as suitable candidates for replacement.However, any advanced techniques employed to select replacement watermark keys for these positions will result in high-entropy words.Consequently, these replacements will always remain detectable, regardless of the strength of the hashing mechanism.</p>
<p>Perplexity and Burstiness Estimation</p>
<p>Liang et al. ( 2023) and Chakraborty et al. (2023) among others have shown perplexity and burstiness are often not reliable indicators of human written text.The fallibility of these metrics become especially prominent in academic writing or text generated in a low-resource language.Our experiments have also pointed towards similar findings.Moreover, in our experiments, we computed perplexity and burstiness metrics both at the overall text level and the sentence level.It is also feasible to calculate perplexity at smaller fragment levels.Since each language model has a unique attention mechanism and span, these characteristics can potentially manifest in the generated text, making them detectable.However, determining the precise fragment size for a language model necessitates extensive experimentation, which we have not yet conducted.</p>
<p>Negative Log Curvature</p>
<p>Although we discussed earlier, it is crucial to reemphasize the significant limitations of DetectGPT (Mitchell et al., 2023).One of its major limitations is that it relies on access to the log probabilities of the texts, which necessitates the use of a specific LLM.However, it is unlikely that we would know in advance which LLM was employed to generate a particular text, and the log-likelihood calculated by different LLMs for the same text would yield significantly different results.In reality, one would need to compare the results with all available LLMs in existence, which would require a computationally expensive brute-force search.</p>
<p>In our experiments, we empirically demonstrate that the hypothesis of log-probability #2 &lt; logprobability #1 can be easily manipulated using simple [MASK]-based post-fixing techniques.</p>
<p>Stylometric Variation</p>
<p>In this experiment, we made a simplifying assumption that all the human-written text was authored by a single individual, which is certainly not reflective of reality.Furthermore, texts composed by different authors inevitably leave behind their unique traces and characteristics.Furthermore, a recent paper by Tulchinskii et al. (2023) introduced the concept of intrinsic dimensionality estimation, which can be described as a stylometric analysis.However, this paper is currently available only on arXiv and lacks an implemented solution.We are currently working on replicating the theory and evaluating the robustness of the approach.</p>
<p>Classifier-based Approaches</p>
<p>Numerous classifiers have been proposed in the literature (Zellers et al., 2020;Gehrmann et al., 2019;Solaiman et al., 2019).However, the majority of these classifiers are specifically created to identify instances generated by individual models.They achieve this by either utilizing the model itself (as demonstrated by Mitchell et al. (2023)) or by training on a dataset consisting of the generated samples from that particular model.For example, RoBERTa-Large-Detector developed by OpenAI (OpenAI, 2023b) is trained or fine-tuned specifically for binary classification tasks.These detectors are trained using datasets that consist of both human-generated and AI-generated texts.Consequently, their ability to effectively classify data from new models and unfamiliar domains is severely limited.</p>
<p>Ethical Considerations</p>
<p>Our experiments show the limitations of AGTD methods and how to bypass them.We develop ADI with the hope that it could be used for guiding further research and policies.However, it can be misused by bad actors for creating AI-generated text, particularly fake news, that cannot be distinguished from human-written text.We strongly advise against such use of our work.</p>
<p>Frequently Asked Questions (FAQs)</p>
<p>✽ How do we envision ADI being used to influence LLM development, policy making, etc.?</p>
<p>➠ The LLM has achieved the status of the holy grail in the field of AI.Its widespread adoption has been influenced by the success stories of ChatGPT, reaching various domains.As new LLMs continue to emerge regularly, there is a strong belief that future iterations will be even more powerful.Consequently, advanced AGTD techniques will be proposed to address these advancements.Regardless of the future landscape, the ADI will persist as a crucial tool for the scientific community and policymakers to assess the detectability of LLMs within their range.</p>
<p>✽ For de-watermarking, Why do you use a brute force algorithm to choose a winning pair?Isn't it inefficient?</p>
<p>➠ Our objective was to demonstrate the successful de-watermarking capability of a combination of open-source models.Currently, the combination of albert-large-v2 and distilroberta-base has shown the most promising performance among all the LLMs.However, determining the most suitable combination for a text encountered in real-world scenarios poses a challenge.Exploring more efficient and scalable approaches to identify the optimal pair in such cases is an area that requires further investigation in future work.</p>
<p>✽ For Stylometric analysis, the entire human-generated corpus was treated as if written by a single author.Won't that lead to noisy analysis?</p>
<p>➠ Indeed, we made an easy presumption, but it opened up further possibilities.</p>
<p>✽ Why did you compare only six methods?</p>
<p>➠ We covered some of the most popular methods.It is possible but highly unlikely that there would be other contemporary methods which we did not try and are also very effective in AGTD.</p>
<p>✽ Do you think your findings will generalize to other languages?</p>
<p>➠ We have designed all the experiments and ADI in a way that is fairly applicable to any language.For example, de-watermarking techniques that we have discussed are based on "entropy" calculation, which is language agnostic.We have defined ADI primarily based on perplexity and burstiness, which could be applied for any language.Additionally, to expand the scope of our claim, we are already working on other languages, such as Spanish and Hindi, which we hope to publish soon.</p>
<p>A LLM Selection Criteria</p>
<p>Beyond the primary criteria for choosing performant LLMs, our selection was meant to cover a wide gamut of LLMs that utilize a repertoire of recent techniques under the hood that have enabled their exceptional capabilities, namely: FlashAttention (Dao et al., 2022) for memory-efficient exact attention, Multi-Query Attention (Shazeer, 2019) for memory bandwidth efficiency, SwiGLU (Shazeer, 2020) as the activation function instead of ReLU (Agarap, 2019), ALiBi (Press et al., 2022) for larger context width, RMSNorm (Zhang and Sennrich, 2019) for per-normalization, RoPE (Su et al., 2021) to improve the expressivity of positional embeddings, etc.</p>
<p>B De-Watermarking</p>
<p>As also shown by Krishna et al. (2023), watermarked texts can be relatively easily de-watermarked.Even with the implementation of the newer, more robust watermarking scheme presented by Kirchenbauer et al. (2023b), we were still able to circumvent the watermarks to a significant extent.Here we discuss the methods in detail, concluding with Table 21 showing de-watermarking accuracies across 15 LLMs after paraphrasing.</p>
<p>B.1 De-watermarking by spotting high entropy words and replacing them</p>
<p>The pivotal proposal made by the watermarking paper is to spot high entropy words and replace them with a random word from the vocabulary, so it is evident that if watermarking has been done, it has been done on those words.</p>
<p>What are high entropy words?High entropy words refer to words that are less predictable and occur less frequently in a corpus.These words have a higher degree of randomness and uncertainty and thus, pose a challenge for LLMs because they require a greater amount of training for accurate prediction.High entropy words can include domain-specific jargon or technical terms.Based on the observed patterns and frequencies of the training data, language models assign probabilities to words.Words with a high entropy tend to have lower probabilities because they are less common or have a more diverse contextual usage.These words are frequently uncommon or specialized terms, uncommon proper nouns, or words that are highly topic-or domain-specific.An example of such a high entropy word used in a sentence is as follows: "The adventurous child clambered up the gnarled tree, seeking the thrill of climbing to its lofty branches."In this sentence, the word "gnarled" is a high entropy word.It describes something that is twisted, rough, or knotted, typically referring to tree branches or old, weathered objects.In different language models, alternative words that might occur instead of "gnarled" could be "twisted," "knotty," or "weathered."These alternatives convey a similar meaning with more commonly used vocabulary.For instance, consider a masked input sentence: "Paris is the [MASK] of France."In this scenario, an LLM might predict candidate words with corresponding probabilities as follows: (i) "capital" [0.99], (ii) "city" [0.0], (iii) "metropolis" [0.0].Here, the LLM demonstrates a high level of certainty regarding the word "capital" to fill the mask.Now, consider another sentence: "I saw a [MASK] last night."The LLM's predicted candidate words and their corresponding probabilities are: (i) "ghost" [0.096], (ii) "UFO" [0.083], (iii) "vampire" [0.045].In this case, the LLM exhibits uncertainty in choosing the appropriate candidate word.</p>
<p>B.2 Dewatemarking on 14 LLMs</p>
<p>Here we present performance evaluation of all the models' combination for the rest of the 14 LLMs.The "Pre" column shows the accuracy scores for the text that was successfully de-watermarked without any paraphrasing techniques.The "Post" column shows the accuracy scores for a text that was not successfully de-watermarked in the initial attempt but was able to be de-watermarked more successfully after paraphrasing methods were applied.</p>
<p>B.3 De-watermarking by paraphrasing</p>
<p>A recent paper (Krishna et al., 2023) talks about the DIPPER paraphrasing technique and how it can easily bypass the watermarking technique.However, their de-watermarking strategy can reduce the detection accuracy of the watermark detector tool to a certain extent.It can't fully de-watermark all the texts.Another paper (Sadasivan et al., 2023) also uses the DIPPER paraphrasing technique but a slightly modified version in which they use parallel paraphrasing of multiple sentences.However, in this paper, they came up with how to bypass the paraphrasing technique so that even after paraphrasing, the detector can tell if the text is in fact AI-generated.This bypassing technique was named Retrieval and it uses the semantic sequence to detect AI-generated text even after paraphrasing (Krishna et al., 2023).</p>
<p>Both these papers also talk about the negative log-likelihood and perplexity score and they have tried on GPT and OPT models.</p>
<p>Based on empirical observations, we concluded that GPT-3.5 outperformed all the other models.To offer transparency around our experiment process, we detail the aforementioned evaluation dimensions as follows.</p>
<p>Coverage -number of considerable paraphrase generations: We intend to generate up to 5 paraphrases per given claim.Given all the generated claims, we perform a minimum edit distance (MED) (Wagner and Fischer, 1974) -units are words instead of alphabets).If MED is greater than ±2 for any given paraphrase candidate (for e.g., c − p c 1 ) with the claim, then we further consider that paraphrase, otherwise discarded.We evaluated all three models based on this setup that what model is generating the maximum number of considerable paraphrases.Correctness -correctness in those generations: After the first level of filtration we have performed pairwise entailment and kept only those paraphrase candidates, are marked as entailed by the (Liu et al., 2019) (Roberta Large), SoTA trained on SNLI (Bowman et al., 2015).</p>
<p>Diversity -linguistic diversity in those generations: We were interested in choosing that model can produce linguistically more diverse paraphrases.Therefore we are interested in the dissimilarities check between generated paraphrase claims.For e.g., c − p , p c n−1 − p c n and repeat this process for all the other paraphrases and average out the dissimilarity score.There is no such metric to measure dissimilarity, therefore we use the inverse of the BLEU score (Papineni et al., 2002).This gives us an understanding of how linguistic diversity is produced by a given model.Based on these experiments, we found that gpt-3.5-turbo-0301performed the best.The results of the experiment are reported in the following table.Furthermore, we were more interested to choose a model that can maximize the linguistic variations, and gpt-3.5-turbo-0301performs on this parameter of choice as well.A plot on diversity vs. all the chosen models is reported in Fig. 2.
c n , p c 1 − p c n , p c 2 − p c n , . . .
Table 21 provides a summary of the effectiveness of the three paraphrasing methods for dewatermarking.Among them, the GPT3.5 based method demonstrated the highest performance.Additionally, it is worth noting that the de-watermarking accuracy for w v2 , the watermarking technique proposed in (Kirchenbauer et al., 2023b), showed a slight decrease compared to w v1 , the watermarking technique proposed in (Kirchenbauer et al., 2023a).</p>
<p>C Perplexity and Burstiness Estimation</p>
<p>We have conducted an analysis to determine the perplexity and burstiness of an LLM, as well as calculate sentence-wise entropy.In order to evaluate the statistical significance of our findings, we employed the bootstrap method.Results of these experiments on all 15 models are reported in Table 22.Brief on bootstrap method: Bootstrapping is a statistical procedure that resamples a single dataset to create many simulated samples, illustrated in Fig. 6.This process allows for the calculation of standard errors, confidence intervals, and hypothesis testing.A bootstrapping approach is an extremely useful alternative to the traditional method of hypothesis testing as it is fairly simple and it mitigates some of the pitfalls encountered within the traditional approach.As with the traditional approach, a sample of size n is drawn from the population within the bootstrapping approach.Let us call this sample S.Then, rather than using theory to determine all possible estimates, the sampling distribution is created by resampling observations with replacement from S, m times, with each resampled set having n observations.Now, if sampled appropriately, S should be representative of the population.Therefore, by resampling S m times with replacement, it would be as if m samples were drawn from the original population, and the estimates derived would be representative of the theoretical distribution under the traditional approach.It must be noted that increasing the number of resamples, m, will not increase the amount of information in the data.That is, resampling the original set 100, 000 times is not more useful than only resampling it 1, 000 times.The amount of information within the set is dependent on the sample size, n, which will remain constant throughout each resample.The benefit of more resamples, then, is to derive a better estimate of the sampling distribution.The traditional procedure requires one to have a test statistic that satisfies particular assumptions in order to achieve valid results, and this is largely dependent on the experimental design.The traditional approach also uses theory to tell what the sampling distribution should look like, but the results fall apart if the assumptions of the theory are not met.The bootstrapping method, on the other hand, takes the original sample data and then resamples it to create many [simulated] samples.This approach does not rely on the theory since the sampling distribution can simply be observed, and one does not have to worry about any assumptions.This technique allows for accurate estimates of statistics, which is crucial when using data to make decisions.</p>
<p>F AI Detectability Index (ADI) -other possible variations</p>
<p>In our previous discussions, we have advocated for utilizing perplexity and burstiness as the fundamental metrics to quantify ADI within the context of various paradigms of AGTD.However, it is important to acknowledge that alternative features, such as stylistics, can also be employed to calculate the ADI.For instance, if we consider stylistic features like syntactic variation (L syn H ) and lexicon variations (L lex H ), the ADI can be reformulated as follows:
ADI x = 100 U×2 * <a href="2">∑ U x=1 {δ 1 (x) * (P t −L syn H ) (1−µ syn H ) } + {δ 2 (x) * (Bt−L lex H ) (1−µ lex H ) }</a>
where, P t = 1 U * {∑ U x=1 logp i u (syn) − logp i+1 u (syn) and P t = 1 U * {∑ U x=1 logp i u (lex) − logp i+1 u (lex) Similarly, it is worth noting that in the future, other potential features such as NLC and any novel features that may be proposed could also be incorporated within the framework of ADI.</p>
<p>G Generated Samples</p>
<p>Text generated by each of the 15 LLMs given the same prompt "AI generated text detection is not easy" are displayed in Table 24.</p>
<p>GPT 4 AI-generated text detection has become more popular in recent years as technology advances and the demand for automated text analysis increases.While it offers many benefits, such as being faster and less error-prone than manual data entry, there are also drawbacks to consider.For example, AI-generated text detection often requires a large amount of training data to be accurate, which can be expensive and time consuming to collect.Additionally, AI algorithms may not always detect the nuances or cultural references within the text that humans might understand better.Overall, while AI-generated text detection is certainly an important tool in... GPT-3.5 AI-generated text detection is an intricate challenge that continues to perplex researchers and developers alike.The rapid advancements in AI language models, such as GPT-3, have ushered in an era of sophisticated text generation that blurs the line between human and machine-generated content.Detecting AI-generated text requires a comprehensive understanding of the nuances and patterns exhibited by these models.It demands an amalgamation of statistical analysis, linguistic analysis, and machine learning techniques.The dynamic nature of AI models, coupled with their ability to adapt and improve, poses an ongoing hurdle in creating robust detection systems.</p>
<p>Striving to stay one step ahead, researchers tirelessly explore innovative methodologies, such as adversarial testing and anomaly detection, to combat the ever-evolving landscape of AI-generated text.While progress is being made, the road to reliable and foolproof AI-generated text detection remains a complex and formidable journey.</p>
<p>GPT-3</p>
<p>AI generated text detection is not easy to find.i've got a number of messages and text messages going around here today.you should probably sit down and look at some of them." he said he had been listening to the messages of the...</p>
<p>GPT-2</p>
<p>AI generated text detection is a challenging task.While AI algorithms have made great strides in recent years, there are several factors that make text detection difficult.One of the main issues is the variability in text itself, both in terms of font and style.Additionally, text may appear in different orientations and lighting conditions, making it more difficult for algorithms to accurately detect and identify.Finally, background noise and other visual clutter can further complicate the task.These challenges require AI developers to continually develop and improve their algorithms to reliably detect and interpret text.</p>
<p>OPT I'm not sure what the topic is here.What is he trying to say?I think he is saying that it is too easy to detect AI generated text (as many real AI generated text is very detectable) and so the text detection is not that good.That's not really what I was getting at... I'm not trying to say that it's too easy to detect AI generated text, I'm trying to say that it's hard to do.I'm not trying to say that it's not hard to detect AI generated text.</p>
<p>Prompt</p>
<p>AI generated text detection is not easy</p>
<p>Continued on next page Table 24: AI generated text for a sample prompt given to all 15 LLMs</p>
<p>Figure 2 :
2
Figure 2: A higher diversity score depicts an increase in the number of generated paraphrases and linguistic variations in those generated paraphrases.</p>
<p>Figure 3 :
3
Figure 3: Perplexity estimation for GPT4/T5 (left/right).</p>
<p>Figure 5 :
5
Figure 5: Grading of current LLMs as proposed by a report entitled Do Foundation Model Providers Comply with the EU AI Act? from Stanford University (Bommasani et al., 2023).</p>
<p>Figure 6 :
6
Figure 6: An illustration of Bootstrapping method -how it creates simulated samples.</p>
<p>Table 2 :
2
(Mitchell et al., 2023watermarking by replacing high-entropy words and paraphrasing.p-value is the probability under the assumption of null hypothesis.The z-score indicates the normalized log probability of the original text obtained by subtracting the mean log probability of perturbed texts and dividing by the standard deviation of log probabilities of perturbed texts.DetectGPT(Mitchell et al., 2023) classifies text to be generated by GPT-2 if the z-score is greater than 4.
Dewatermarking models→</p>
<p>albert-large-v2 bert-base-uncased distilroberta-base xlm-roberta-large
Masking models ↓wv1DeW1wv2wv1DeW2wv2wv1DeW1wv2wv1DeW2wv2wv1DeW1wv2wv1DeW2wv2wv1DeW1wv2wv1DeW2wv2albert-large-v251.86899.599.3477199.879975.87099.5598.4562.55999.0996.56bert-base-uncased24.13399.7798.3313199.4399.2830.53399.0997.93242098.9797.34distilroberta-base45.17098.8695.6746.17299.3199.0749.86899.7796.8937.85698.9798.89xlm-roberta-large29.21998.7598.8828.12099.1497.7828.32099.1498.927.71399.5199.5</p>
<p>Table 3 :
3
The performance evaluation encompassed 16 combinations for de-watermarking OPT generated watermarked text.The accuracy scores for successfully de-watermarked text using the entropy-based word replacement technique are presented in the DeW 1 columns.It is worth highlighting that the accuracy scores in the DeW 2 columns reflect the application of automatic
paraphrasing after entropy-based word replacement. The techniques proposed in Kirchenbauer et al. (2023a) are denoted as w v1 ,while the techniques proposed in their subsequent work Kirchenbauer et al. (2023b) are represented as w v2 .any LLM to replace the previously identified high-distilroberta-base), achieving a 75% accuracyentropy words, resulting in a de-watermarked text.in removing watermarks, while (distilroberta-To achieve this, we tried various LLMs and foundbase, bert-base-uncased) performs best forthat BERT-based models are best performing to(Kirchenbauer et al., 2023b) (w v2 ), attaining 72%generate replacements for the masked text.accuracy in de-watermarking. The results for theWinning combination: The results of experimentsremaining 14 LLMs are reported in Appendix B.on detecting and replacing high entropy words are presented in Table 3 for OPT. The findings3.2 De-watermarking by Paraphrasing (DeW 2 )indicate that ALBERT (albert-large-v2) (Lanet al., 2020) and DistilRoBERTa (distilroberta-base) perform exceptionally well in identifyinghigh entropy words in text generated by theOPT model for both versions, v1 and v2. Onthe other hand, DistilRoBERTa (distilroberta-base) (Sanh et al., 2019) and BERT (bert-base-uncased) (Devlin et al., 2019) demonstrate supe-rior performance in substituting the high entropywords for versions v1 and v2 of the experiments.Therefore, the optimal combination for Kirchen-bauer et al. (2023a) (w v1 ) is (albert-large-v2,</p>
<p>Table 4 :
4
Perplexity, burstiness, and NLC values for 3 LLMs across the ADI spectrum along with statistical measures.
LLMsPerplexityBurstinessNLCParaphrasingAcc.Human AIEnt H Ent AI αHuman AIEnt H Ent AI αHuman AIαModelsw v1w v2OPTµ 46.839 σ 68.54143.495 65.1784.276 3.777 0.519 -0.3001 0.3645 6.119 5.890 0.5052 4.160 0.26164 0.3156 0.3364.175 0.505 0.654Pegasus79.32 67.12GPT-2 µ 143.198 76.2965.362 4.770 0.516 -0.3001 -0.2159 6.333 5.843 0.5006 3.4363.778 0.507T5-Large80.86 72.00σ 60.86667.3150.26164 0.29470.8290.394GPT-3.590.32 70.35XLNet µ 106.776 104.091 8.378 9.712 0.532 -0.2992 -0.0153 6.380 4.563 0.4936 4.2974.185 0.498σ 57.09162.1520.24160.00320.3380.535Table 5: De-watermarkingacc. of paraphrasing onOPT.ModelCoverage Correctness DiversityPegasus32.4694.38%3.76T530.2683.84%3.17GPT-3.535.5188.16%7.72
, (b) T5 (Flan-t5-xxl</p>
<p>Table 7 :
7
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking LLaMA generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v291.9 96.5939893.3 96.59598.3 91.19392.59687.8 91.99094.3bert-base-uncased67.8 79.1 92.3 94.5 66.78689.2 96.5 61.1 81.4 86.2 93.76077.9 87.8 97.5distilroberta-base8088.4 95.29875.6 89.59499.5 82.2 90.7 97.5 99.4 72.2 88.4 93.297xlm-roberta-large5076.78285.5 47.8 77.9 76.39048.9 70.989904077.9 87.6 95.5Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v293.8 95.9 98.7 99.7 92.5 94.6959991.2 94.6 96.79893.8 95.99999.5bert-base-uncased66.2 78.4889073.8 79.79698.57085.1909567.5 78.49097distilroberta-base9090.5 92.59686.3 91.9909887.5 91.9 97.59986.3 89.2 96.595xlm-roberta-large53.7 85.1 78.99053.7 82.4 79.5 91.5 53.7 87.884954581.1 69.882</p>
<p>Table 8 :
8
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking Alpaca generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v264.6 73.7909876.8 56.6 97.58878.8 82.8 95.5 95.5 76.8 84.8 95.5 95.5bert-base-uncased18.2 57.6 52.9 78.4 23.2 36.45060.5 36.4 51.5 55.59531.3 50.5 83.590distilroberta-base64.6 70.7909761.6 50.5 85.58062.6 77.8 70.99062.6 64.68987xlm-roberta-large36.4 58.6 56.6 77.7 26.3 34.3 52.96034.3 51.5 63.4 80.9 30.3 50.55080.5</p>
<p>Table 9 :
9
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking BLOOM generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v262.8987899.1 56.496909756.495919651.19394.595bert-base-uncased28.77799100 26.67094.6 96.7 26.67490.5 96.2 24.57788.595distilroberta-base48.99184.5 100 40.48970.59251.192909538.3917594xlm-roberta-large29.877508324.5 46.58695.5 27.776507626.67254.5 96.4</p>
<p>Table 10 :
10
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking StableLM generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v261.1 90.7909963.3 94.2899562.2 94.2 90.59960869095.5bert-base-uncased41.1 67.48088.6 41.1 67.48088.6 46.7 69.8 84.5904061.6 80.5 91.5distilroberta-base51.1 80.2809856.7 83.77888.9 53.3 87.2 83.99851.1 76.7 85.590xlm-roberta-large33.3 56.95876.54055.88097.6 42.2 52.3878944.4 47.7 80.389</p>
<p>Table 11 :
11
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking Dolly generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v275979599.7 76.99692.3 98.5759896.6 98.5 69.2 97.58999.5bert-base-uncased61.5908794.5 65.489909257.7 92.5809557.79485.594distilroberta-base71.299899976.9948596759290.49965.4979098.5xlm-roberta-large59.6 97.5 90.79763.597879953.89785.39855.8988098.2</p>
<p>Table 12 :
12
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking T5 generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v286.5959999.3 78.8 94.49098.3 78.8979999.7 76.9 94.4 89.6 96.5bert-base-uncased71.2 88.9 90.2 99.7 71.2 94.4 86.29573.1 88.99095.8 67.3 94.47696.8distilroberta-base69.2 94.4 79.5967595.69597.3 82.7 94.4 96.59871.2 94.4 91.298xlm-roberta-large57.7 94.4 70.9 95.7 65.4 94.4859659.6 88.9 80.59061.5 88.99098.5</p>
<p>Table 13 :
13
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking Vicuna generated watermarked text.
Dewatermarking models →albert-</p>
<p>large-v2 bert-base-uncased distilroberta-base xlm-roberta-large
Masking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v28086868881.48790.8 87.58086948867.1769679bert-base-uncased62.97367.77867.17785.687607180.5 91.8 51.4659597.4distilroberta-base62.97392.6996072957865.776958161.47380.976xlm-roberta-large58.671657858.67170.57452.96760.36947.1638082</p>
<p>Table 14 :
14
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking T0 generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v2878792.3 92.3929293.5 93.5888893.3 93.393939595.7bert-base-uncased67679797686884.4 84.46056.9 88.2 83.7636386.5 86.5distilroberta-base858599100858586.7 86.7868687.5 87.7858588.3 88.3xlm-roberta-large707093.3 93.3676784.8 84.8616179.5 79.5676787.9 87.9</p>
<p>Table 15 :
15
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking XLNet generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v2979199.2 99.598889998.5958899.89097.58899.893bert-base-uncased997799.5 99.5917399.583927799.587807598.2 87.7distilroberta-base988699999086999593899895.5978598.7 95.2xlm-roberta-large957099.885916699.882996699.8 79.594639876</p>
<p>Table 16 :
16
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking MPT generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v275979599.78973.4 64.59682.3809088.7657189.2 93.9bert-base-uncased5270897880.5 87.99685899899.7 76.2 80.3 90.5 92.3distilroberta-base7580.98081616178.9 78.96474.2 74.59087.89087.890xlm-roberta-large6768.9 77.5 82.480908499.75254.56770.53456.6 50.460</p>
<p>Table 17 :
17
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking GPT2 generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v275979599.7 76.99692.3 98.5759896.6 98.5 69.2 97.58999.5bert-base-uncased61.5908794.5 65.489909257.7 92.5809557.79485.594distilroberta-base71.299899976.9948596759290.49965.4979098.5xlm-roberta-large59.6 97.5 90.79763.597879953.89785.39855.8988098.2</p>
<p>Table 18 :
18
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking GPT3 generated watermarked text.
Dewatermarking models →albert-</p>
<p>large-v2 bert-base-uncased distilroberta-base xlm-roberta-large
Masking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v262.2 90.7909964.5 94.2899562.2 94.2 92.59960869095.5bert-base-uncased41.1 67.48088.6 41.1 67.48088.6 46.7 69.8 84.5904661.6 80.5 91.5distilroberta-base51.1 80.2809856.7 83.77888.9 53.3 87.2 85.99851.1 76.7 85.590xlm-roberta-large33.3 56.95976.54056.88097.6 42.2 52.3878947.4 47.7 80.389</p>
<p>Table 19 :
19
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking GPT3.5 generated watermarked text.
Dewatermarking models →albert-large-v2bert-base-uncaseddistilroberta-basexlm-roberta-largeMasking models ↓DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2DeW1 wv1 wv2DeW2 wv1 wv2albert-large-v266729092.562638989.66872.4 88.9 91.376707680.9bert-base-uncased58.671667858.67170.57452.96760.36947.1638382distilroberta-base957699.885916698.882996699.8 80.594639876xlm-roberta-large6789.39099.780838389.2899590.5 98.7768586.596</p>
<p>Table 20 :
20
Performance evaluation of 16 combinations of 4 masking-based models for de-watermarking GPT4 generated watermarked text.</p>
<p>Table 22 :
22
C.1 Reliability of Perplexity, Burstiness and NLC as AGT Signals for all LLMs Here we present the complete table showing results after performing experiments on Perplexity estimation (Section 4.1), Burstiness estimation (Section 4.2) and NLC (Section 5) over all 15 LLMs.Comprehensive table for all 15 LLMs with statistical measures for Perplexity, Burstiness, and NLC, along with bootstrap p values (α = 0.05), indicating non-significance for b values greater than the chosen alpha level.
LLMsPerplexityBurstinessNLCHumanAIEnt H Ent AIαHumanAIEnt H Ent AIαHumanAIαGPT 4 (OpenAI, 2023a)µ 38.07335.465 4.222 3.881 0.492 -0.4010 0.3920 6.152 5.893 0.50042.1231.966 0.503σ 86.41180.8360.26164 0.34210.5350.934GPT 3.5 (Chen et al., 2023)µ 43.19839.897 3.423 3.195 0.505 -0.2798 0.5509 6.144 5.923 0.50294.4924.302 0.504σ 46.86642.3410.29660.33870.3320.514OPT (Zhang et al., 2022)µ 46.83943.495 4.276 3.777 0.519 -0.3001 0.3645 6.119 5.890 0.50524.1604.175 0.505σ 68.54165.1780.26164 0.31560.3360.654GPT 3 (Brown et al., 2020)µ 48.83946.980 4.205 3.933 0.515 -0.3001 0.3171 6.119 5.880 0.51044.1604.302 0.503σ 82.54179.2240.26164 0.24200.3360.465Vicuna (Zhu et al., 2023)µ 51.83950.728 4.276 3.676 0.511 -0.3001 0.3122 6.119 5.763 0.50094.1604.491 0.507σ 58.54150.7400.26164 0.30660.3360.427StableLM (Tow et al.)µ 62.83956.558 4.205 3.564 0.506 -0.3001 0.1213 6.901 5.841 0.49454.1604.386 0.499σ 58.10450.0020.26164 0.34340.3360.551MPT (Team, 2023)µ 78.83972.495 4.263 3.406 0.505 -0.3001 0.1958 6.213 5.571 0.50414.1604.260 0.486σ 76.54166.6340.26164 0.38340.3360.626LLaMA (Touvron et al., 2023) µ 83.83975.358 4.662 3.299 0.497 -0.3001 0.2635 6.009 5.623 0.50174.1604.428 0.502σ 83.54175.8020.26164 0.27410.3360.369Alpaca (Maeng et al., 2017)µ 122.839 76.105 5.276 4.644 0.512 -0.3001 0.3829 6.294 5.603 0.49694.1603.774 0.501σ 58.54186.5540.26164 0.40330.3360.698GPT 2 (Radford et al., 2019)µ 143.198 76.296 5.362 4.770 0.516 -0.3001 -0.2159 6.333 5.843 0.50063.4363.778 0.507σ 60.86667.3150.26164 0.29470.8290.394Dolly (Wang et al., 2022)µ 122.839 91.789 5.760 4.437 0.512 -0.3001 0.3507 7.209 6.323 0.50574.1604.215 0.561σ 58.54166.6290.26164 0.37170.3360.618BLOOM (Scao et al., 2022)µ 122.839 92.566 5.700 4.558 0.509 -0.3001 0.9088 6.902 5.801 0.50834.1603.917 0.506σ 58.54166.0770.26164 0.29270.3360.639T0 (Sanh et al., 2021)µ 122.839 93.321 7.264 8.693 0.514 -0.3001 0.4578 6.221 4.435 0.4964.1603.979 0.504σ 58.54156.9190.26164 0.52610.3360.534XLNet (Yang et al., 2019)µ 106.776 104.091 8.378 9.712 0.532 -0.2992 -0.0153 6.380 4.563 0.49364.2974.185 0.498σ 57.09162.1520.24160.00320.3380.535T5 (Raffel et al., 2020)µ 122.839 110.386 7.884 8.760 0.532 -0.3001 -0.0216 6.921 4.830 0.49394.1603.945 0.498σ 58.54196.8930.26164 0.31870.3360.735
Stylometric VariationStylometry analysis is a well-studied subject(Lagutina et al., 2019;Neal et al., 2018) where scholars have proposed a comprehensive range of lexical, syntactic, semantic, and structural characteristics for
AppendixThis section provides supplementary material in the form of additional examples, implementation details, etc. to bolster the reader's understanding of the concepts presented in this work.C.2 Plots for 15 LLMs across the ADI spectrumHere we present the histogram plots and negative log-curvature line plots for all 15 LLMs.Arranged as per the ADI spectrum, it is evident that higher ADI models come much closer to generating text similar to humans that models that fall lower on the spectrum.DetectGPT(Mitchell et al., 2023)utilizes the generation of log-probabilities for textual analysis.It leverages the difference in perturbation discrepancies between machine-generated and human-written text to detect the origin of a given piece of text.When a language model produces text, each individual token is assigned a conditional probability based on the preceding tokens.These conditional probabilities are then multiplied together to derive the joint probability for the entire text.To determine the origin of the text, DetectGPT introduces perturbations.If the probability of the perturbed text significantly decreases compared to the original text, it is deemed to be AI-generated.Conversely, if the probability remains roughly the same, the text is considered to be human-generated.Model Histplot LineplotContinued on next pageThe hypothesis put forward byMitchell et al. (2023)suggests that the perturbation patterns of AIwritten text should align with the negative log-likelihood region.However, this observation is not supported by the results presented here.To strengthen our conclusions, we calculated the standard deviation, mean, and entropy, and performed a statistical validity test in the form of a p-test.The findings are reported in Table22.E Stylometric variationThe field of stylometry analysis has been extensively researched, with scholars proposing a wide range of lexical, syntactic, semantic, and structural features for authorship attribution.In our study, we employed Le Cam's lemma(Cam, 1986(Cam, -2012) )as a perplexity density estimation method.However, there are several alternative approaches that can be suggested, such as kernel density estimation (Wikipedia_KDE), mean integrated squared error (Wikipedia_MISE), kernel embedding of distributions (Wikipedia_KED), and spectral density estimation (Wikipedia_SDE).While we have not extensively explored these variations in our current study, we express interest in investigating them in future research.For instance, the lemma learned from GPT-4 can be successfully applied to GPT-3.5, OPT, and GPT-3, but not beyond that.Similarly, Vicuna, StableLM, and LLaMA form the second group.The red dotted rectangle highlights the LLMs that are classified as not detectable, while the black dotted rectangle represents the LLMs that are considered hard to detect.On the other hand, the blue dotted rectangle indicates the LLMs that are categorized as easy to detect.LLaMAThere are two ways to tackle the task of detecting whether an article was written by machine or human.One way would be to use a technique called Natural Language Understanding, which involves generating some kind of semantic model that can identify the properties and relations between words in order to determine if one particular word-string is likely to have been produced by a person or an algorithm.The other approach might involve using something like natural language generation (see this paper) where you could generate some kind of "reverse" sentence from a piece of raw data.But while there's a lot we still don't know about how our brains work when it comes to understanding sentences -such as why certain parts of speech seem easier for us to understand than others -it seems fairly safe to say that the way machines currently learn to write prose isn't yet able to match up with what humans do naturally.If I were writing an essay today, my biggest concern would probably revolve around being understood rather than trying to write really well.That said, people definitely do care about both things at once; just look at all those bloggers out there obsess...BLOOMI agree that AI generated text detection is not easy, especially given the fact that there are many tools and resources available on the internet that can be used to generate text.But it's not impossible.There are several ways to detect AI generated text, such as using natural language processing (NLP) techniques, analyzing the writing style and patterns, and checking for inconsistencies and errors in grammar and syntax.Additionally, it's important to note that not all AI generated text is bad or negative, and some can be beneficial and helpful, such as in the case of chatbots and AI assistants.MPT AI-generated content identification can be difficult, as it requires complex algorithms to identify patterns in large amounts of data and determine whether they are human or machine written.This process becomes even more challenging when dealing with short texts such as tweets, which often contain slang words and abbreviations unique to certain communities.Despite these challenges, researchers have made significant progress towards developing accurate methods for detecting AI-generated text.PromptAI generated text detection is not easyContinued on next page Table24: AI generated text for a sample prompt given to all 15 LLMs (Continued) Alpaca AI generated text detection is not an easy task.Text detection is the process of identifying and extracting text from an image, video, or document.It is a challenging task due to the wide variety of text styles, sizes, colors, and backgrounds that can be found in an image or document.Additionally, AI generated text detection must be able to accurately detect text even if it is partially obscured by other elements in the image or document.AI generated text detection must also be able to accurately detect text even if it is in a different language than the language of the image or document.AI generated text detection must also be able to accurately detect text even if it is in a different font or font size than the rest of the text in the image or document.AI generated text detection must also be able to accurately detect text even if it is in a different color than the rest of the text in the image or document.AI generated text detection must also be able to accurately detect text even if it is in a different orientation than the rest of the text in the image or document.AI generated text detection must also be able to accurately detect text even if it is in a different shape than the rest...VicunaDetecting AI-generated text can be a challenging task.Unlike human-written content, which often contains unique phrasing and grammatical errors, AI-generated text is designed to be flawless.As a result, it can be difficult for humans to differentiate between the two.Additionally, AI algorithms can produce vast amounts of text quickly, making it increasingly important to develop reliable detection methods.With advancements in technology and machine learning techniques, detecting AI-generated text will continue to evolve and become more sophisticated over time.DollyGenerating text in an image is not an easy task.There are several steps that need to be done in order to generate text from an image.The first step is to pre-process the image so that it can be better understood by the computer.One of the first steps in this process is to detect the text in the image.There are many libraries and frameworks that can be used to do this text detection.Some of these libraries include python-OCR, OpenText, and Tesseract.Once the text has been detected, the next step is to tokenize the text.Tokenization is the process of splitting the detected text up into individual words.Some libraries that can be used for tokenization are NLTK and SpaCy.After the text has been tokenized, you then need to filter out all the stopwords.Stopwords are common words that are not considered to be part of the text.Once all the stopwords have been removed, you then need to lemmatize the words that have been removed.Lemmatization is the process of removing the inflection from words and treating them as if they were roots.The last step in this process is to sentence piece the tokens that have been created from the tokenization step.PromptAI generated text detection is not easyContinued on next page Table24: AI generated text for a sample prompt given to all 15 LLMs (Continued)StableLMDetecting AI-generated text is a challenging task that requires deep understanding of the text data and its characteristics.When dealing with such data, the accuracy of detection is often hampered by the presence of errors or inconsistencies in the data.Furthermore, the task of accurately detecting text in natural language is also inherently difficult, as the patterns and structure of human language are complex and vary across different contexts.As a result, there is a need for more sophisticated and robust techniques that can identify and analyze the significance of AI-generated text in a meaningful way.XLNetIt is not easy to execute a paragraph or paragraph in a particular order.It is most difficult when a script is loaded.It is particularly difficult to do when the script is selected as a script that is loaded.It is no easier to do when a file is being loaded as only the following page.When a file is being loaded as a "predefined" "spec" it has an incompatibility with the file.When the file is being loaded in the first-string "select" script it has an incompatibility with the file.On page 5, Instruction 213.Refer to the section "First-string "spec" scripts for a "predefined" script "spec" script.See the section "Predefined" scripts to describe "predefined" scripts by "predefined" scripts for "predefined" script "spec".Instruction 212.See the sub-group in the section "PredefinedScript" for "predefined" script "spec" and "predefined" script...T5The text detection is not easy.
Deep learning using rectified linear units (relu). Abien Fred, Agarap , arxiv: 1803.083752019</p>
<p>Do foundation model providers comply with the eu ai act?. Rishi Bommasani, Kevin Klyman, Daniel Zhang, Percy Liang, 2023</p>
<p>Dennis D Boos, Cavell Brownie, 10.1080/00401706.1989.10488477Bootstrap methods for testing homogeneity of variances. 198931Technometrics</p>
<p>Gabor Samuel R Bowman, Christopher Angeli, Christopher D Potts, Manning, arXiv:1508.05326A large annotated corpus for learning natural language inference. 2015arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Lucien Le, Cam , Asymptotic methods in statistical decision theory. 1986-2012</p>
<p>On the possibilities of ai-generated text detection. Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, 2023Bang An, Dinesh Manocha, and Furong Huang</p>
<p>. Xuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie Zhou, Tao Gui, Qi Zhang, Xuanjing Huang, arXiv:2303.002932023arXiv preprintHow robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Chen, Aakanksha Chowdhery. Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Scaling instruction-finetuned language models</p>
<p>Copyright registration guidance: Works containing material generated by artificial intelligence. Copyright-Office, 2023Library of Congress</p>
<p>Modelling word burstiness in natural language: a generalised polya process for document language models in information retrieval. Ronan Cummins, arXiv:1708.060112017arXiv preprint</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré, Advances in Neural Information Processing Systems. 202235</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2019</p>
<p>Proposal for a regulation of the european parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts. European-Parliament, 2023</p>
<p>Gltr: Statistical detection and visualization of generated text. Sebastian Gehrmann, Hendrik Strobelt, Alexander M Rush, arXiv:1906.040432019arXiv preprint</p>
<p>T test as a parametric statistic. Tae Kim, 10.4097/kjae.2015.68.6.540Korean Journal of Anesthesiology. 685402015</p>
<p>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein, A watermark for large language models. 2023a</p>
<p>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. 2023b. On the reliability of watermarks for large language models. </p>
<p>Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer, arXiv:2303.134082023arXiv preprint</p>
<p>Stylometric detection of aigenerated text in twitter timelines. Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee, Kirill Trapeznikov, Scott Ruston, Huan Liu, 2023</p>
<p>A survey on stylometric text features. Ksenia Lagutina, Nadezhda Lagutina, Elena Boychuk, Inna Vorontsova, Elena Shliakhtina, Olga Belyaeva, Ilya Paramonov, P G Demidov, 10.23919/FRUCT48121.2019.89815042019 25th Conference of Open Innovations Association (FRUCT). 2019</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, 2020</p>
<p>Gpt detectors are biased against non-native english writers. Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou, 2023</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Alpaca: Intermittent execution without checkpoints. Kiwan Maeng, Alexei Colin, Brandon Lucia, Proceedings of the ACM on Programming Languages. 1OOPSLA2017</p>
<p>Pause giant ai experiments: An open letter. Gary Marcus, 2023</p>
<p>Detectgpt: Zero-shot machine-generated text detection using probability curvature. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn, 2023</p>
<p>Surveying stylometry techniques and applications. Tempestt Neal, Kalaivani Sundararajan, Aneez Fatima, Yiming Yan, Yingfei Xiang, Damon Woodard, ACM Computing Surveys (CSUR). 506862018</p>
<p>OpenAI. 2023a. Gpt-4 technical report. </p>
<p>New ai classifier for indicating aiwritten text. 2023b. Feb 1 2023OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Ofir Press, Noah Smith, Mike Lewis, International Conference on Learning Representations. 2022</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>Words' burstiness in language models. Pavel Rychlỳ, RASLAN. 2011</p>
<p>Can ai-generated text be reliably detected?. Aounon Vinu Sankar Sadasivan, Sriram Kumar, Wenxiao Balasubramanian, Soheil Wang, Feizi, 2023</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, ArXiv, abs/1910.011082019</p>
<p>Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, arXiv:2110.08207arXiv preprint</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Fast transformer decoding: One write-head is all you need. Noam Shazeer, arXiv:1911.021502019arXiv preprint</p>
<p>Noam Shazeer, arXiv:2002.05202Glu variants improve transformer. 2020arXiv preprint</p>
<p>Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, arXiv:1908.09203Release strategies and the social impacts of language models. 2019arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, arXiv:2104.098642021arXiv preprint</p>
<p>Introducing mpt-7b: A new standard for open-source. Nlp Mosaicml, Team, 2023commercially usable llms</p>
<p>. Edward Tian, Online; accessed 2023- 01-022023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>. Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme, Stablelm. 3</p>
<p>Intrinsic dimension estimation for robust detection of ai. Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, Evgeny Burnaev, 2023generated texts</p>
<p>The string-to-string correction problem. A Robert, Michael J Wagner, Fischer, Journal of the ACM (JACM). 2111974</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Blueprint for an ai bill of rights: Making automated systems work for the american people. White-House, 2023</p>
<p>Openai's attempts to watermark ai text hit limits. Kyle Wiggers, Online; accessed 2023-01-022022</p>
<p>Kernel embedding of distributions. Wikipedia_MISE. Mean integrated squared error. Wikipedia_SDE. Spectral density estimation. Zihang Wikipedia ; Yang, Yiming Dai, Jaime Yang, Russ R Carbonell, Quoc V Salakhutdinov, Le, Normalization. Wikipedia_KDE. Kernel density estimation. Wikipedia_KED. 2019. 201932Advances in neural information processing systems</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023. A comprehensive capability analysis of gpt-3. and gpt-3.5 series models</p>
<p>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, Defending against neural fake news. 2020</p>
<p>Root mean square layer normalization. Biao Zhang, Rico Sennrich, Advances in Neural Information Processing Systems. 201932</p>
<p>Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, International Conference on Machine Learning. PMLR2020</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022Opt: Open pretrained transformer language models</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
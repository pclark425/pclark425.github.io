<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5356 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5356</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5356</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-de02ba19fb957ae30de7f09904ae3d983c3b50e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/de02ba19fb957ae30de7f09904ae3d983c3b50e7" target="_blank">GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</a></p>
                <p><strong>Paper Venue:</strong> AAAI Spring Symposia</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs and demonstrates that grounding the models in specific graph-based knowledge expands the models’ capacity for structure-based reasoning.</p>
                <p><strong>Paper Abstract:</strong> Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no – such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the models’ capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5356.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5356.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeighborhoodPartitioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighborhood Partitioning and Subgraph Context Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Algorithmic procedure that extracts a node's k-hop neighborhood, partitions it to respect LLM token limits (via N_max), and produces subgraph contexts for conversion into text and QA-pairs using f_aggr, f_partition, f_enc and f_qa operators (Algorithm 1).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Subgraph context partitioning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node v, retrieve k‑hop neighborhood G_context(v,k) using f_aggr; partition G_context into subgraphs with node-count limit N_max (f_partition) so encoded text lengths remain below model token limit T_max; for each partition, encode (f_enc) and generate QA pairs (f_qa). This yields multiple (context, question-answer) training sequences per node. Partitioning is used to address skewed degree distributions and token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (UMLS) and citation/author graph (DBLP)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Scales with neighborhood size; preserves local graph structure within each partition; allows variable context granularity; trades off completeness vs. token budget (controlled by N_max). Designed for interpretability and to avoid exceeding T_max; sensitive to skewed degree distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fact recall, Inverse (reverse) fact recall, Multi-hop reasoning (graph-inference), Multiple-choice</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as part of all GLaM experiments; performance reported in downstream BERTScore (P/R/F1) and multiple-choice accuracy in Tables 1-3 (see other entries for encoding-specific numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Partitioning is an orthogonal preprocessing step applied to all encoding strategies; it enabled applying triple/adjacency/summarized encodings within token limits. No single numeric comparison isolated to partitioning alone, but experiments show larger encoded neighborhood contexts (enabled by thoughtful partitioning) improved inference performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires selecting N_max and k to trade off context completeness and token limits; skewed degree distributions complicate optimal partition sizing; sampling techniques for large neighbor lists noted as relevant but not implemented; potential information loss if critical nodes split across partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5356.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5356.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encoding via (source, relation, target) Triples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple linearization that maps each graph edge into a single training sentence of the form (source, relation, target), producing one sample per edge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each edge in the k‑hop neighborhood is converted into a single textual sentence representing the triple (source, relation, target). Each triple constitutes a standalone training example; minimal structural context beyond the single relation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (UMLS) and citation/author graph (DBLP)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Very compact per-edge representation; high fidelity for single-edge facts; low contextual breadth (only single-edge), limited capacity for multi-hop reasoning or capturing higher-order dependencies; easy to generate; may produce many short training samples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fact recall, Inverse fact recall, Multi-hop reasoning (open-ended + multiple choice)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UMLS (BERTScore P/R/F1): Fact recall 0.683/0.597/0.627; Reverse 0.431/0.533/0.474; Multi-hop 0.677/0.589/0.621 (Table 1). DBLP (BERTScore P/R/F1): Fact 0.105/0.103/0.104; Reverse 0.103/0.102/0.102; Multi-hop 0.100/0.099/0.099 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperformed the untrained baseline on UMLS but performed poorly on DBLP relative to richer encodings (relational grouping, adjacency lists, node descriptors). Triples provided inferior performance for DBLP multi-hop and fact tasks, indicating insufficient context for domains with unfamiliar node labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited context prevents effective multi-hop reasoning; many isolated samples increase dataset size; performs poorly when node labels are unfamiliar to LLM (DBLP); sensitive to overfitting to frequent relation phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5356.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5356.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RelationalGrouping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relational Grouping / Adjacency List Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Group-based encodings that aggregate multiple neighbors of the central node into one training sample either as an adjacency list or by partitioning neighbors by relation type, enabling multi-edge context per example.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency list / Relational grouping</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two variants: (a) adjacency list: collate the central node v's entire adjacency list (multiple edges) into a single textual context; (b) relational grouping: partition neighbors into subsets grouped by relation type and encode each group as one context. Both deliver multi-target context per training example.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (UMLS) and citation/author graph (DBLP)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Increases context breadth and structural information per example; improves capacity for multi-hop reasoning; more compact than many single-triple samples; may still be large for high-degree nodes (necessitates partitioning). Improves semantic signal by showing relation groups.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fact recall, Inverse fact recall, Multi-hop reasoning, Multiple-choice</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UMLS (BERTScore P/R/F1) Relational Grouping: Fact 0.679/0.678/0.673; Reverse 0.403/0.537/0.459; Multi-hop 0.662/0.663/0.657 (Table 1). DBLP Relational Grouping: Fact 0.259/0.261/0.259; Reverse 0.259/0.264/0.260; Multi-hop 0.256/0.259/0.257 (Table 2). Adjacency List variant (DBLP): Fact 0.255/0.258/0.255; Reverse 0.247/0.252/0.249; Multi-hop 0.251/0.253/0.251.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperformed triple-based encoding on most tasks (especially DBLP) and improved over baseline Llama; relational grouping/adacency provided better multi-edge context and thus better inference than triples. However, LLM summarization and node descriptor strategies further improved performance beyond basic adjacency/relational grouping.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Large neighbor lists can exceed token limits; selection/partitioning strategies required (N_max); naive inclusion of noisy or uninformative adjacency elements (e.g., venues) may not help; sampling techniques are relevant but not implemented in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5356.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5356.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NodeDescriptors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node Descriptor Encoding (LLM-driven descriptors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Create textual descriptors for nodes by prompting the LLM to summarize a node's attributes or neighborhood (e.g., extracting author topic areas from paper abstracts) to produce interpretable labels for unfamiliar node identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node descriptors (LLM zero-shot expansion)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For nodes with non-human-readable or unfamiliar labels, prompt the LLM to generate descriptive textual summaries of the node based on its k‑hop neighborhood (e.g., topics an author publishes on from abstracts). These descriptors are used in context encodings instead of raw IDs/names to improve semantic alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation/author graph (DBLP), Knowledge graph (UMLS) as applicable</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Improves semantic alignment to LLM's internal knowledge; increases interpretability; reduces reliance on rare tokens; can introduce additional synonyms/knowledge; helps reduce overfitting to raw labels. Produces richer per-node text than raw names.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fact recall, Inverse fact recall, Multi-hop reasoning, Multiple-choice</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DBLP (BERTScore P/R/F1) Node Descriptors: Fact 0.313/0.312/0.312; Reverse 0.309/0.314/0.311; Multi-hop 0.318/0.316/0.316 (Table 2). Used as a component in combined GLaM where further improvements were seen.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperformed simple triples and adjacency-only encodings on DBLP, where raw node names were unfamiliar; combining node descriptors with adjacency and summarization yielded the best DBLP results (GLaM-7B/13B).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on LLM zero-shot summarization capability; may hallucinate or introduce extraneous synonyms if prompts not carefully controlled; additional compute to generate descriptors; potential loss of exact entity identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5356.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5356.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMSummarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Prompt Rewriting / Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use the LLM to rewrite or summarize graph-derived textual encodings to more coherent, human-interpretable forms (map unwieldy labels to readable phrases, reduce redundancy, supply synonyms), improving semantic alignment for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM summarization / prompt-based rewriting</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply prompting to rewrite raw triple/adjacency encodings into coherent natural-language summaries: map technical labels to human terms, collapse redundant labels, expand abbreviations and synonyms (e.g., 'Insulin human, rDNA origin' -> 'Insulin therapy from recombinant DNA'), and diversify relation phrasing to mitigate overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (UMLS) and citation/author graph (DBLP)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Improves interpretability and semantic alignment; reduces redundant text; introduces synonyms and clarifying parentheticals; helps avoid overfitting to frequent relation phrasing; may increase lexical diversity improving learning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fact recall, Inverse fact recall, Multi-hop reasoning, Multiple-choice</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UMLS (BERTScore P/R/F1) LLM Summarization: Fact 0.724/0.725/0.720; Reverse 0.386/0.527/0.445; Multi-hop 0.689/0.696/0.688 (Table 1). DBLP best when combined with adjacency: combined GLaM results reported (see Combined entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>On UMLS, LLM summarization encoding achieved the best results among tested encodings for fact recall and multi-hop reasoning (outperforming triples and relational grouping). For DBLP, summarization combined with adjacency and node descriptors produced top-performing GLaM models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Summarization may introduce lexical variation that changes exact factual phrasing (risk of subtle semantic shift); relies on LLM rewriting quality; could mask the exact original labels needed for some tasks; doesn't fully resolve inverse fact recall (some improvements only marginal).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5356.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5356.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLaM-Combined</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLaM Combined Encoding (Node descriptors + Adjacency + Summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Composite encoding strategy used for the strongest-performing GLaM variants: aggregate node descriptors, full adjacency lists as context, and apply LLM summarization to produce training samples for fine-tuning LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Composite GLaM encoding (aggregation + adjacency + summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Combine techniques: generate node descriptors for unfamiliar nodes, assemble adjacency lists (multiple edges per central node), partition as needed, and apply LLM-based summarization to produce polished text contexts; then generate QA pairs from the same subgraph contexts for supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (UMLS) and citation/author graph (DBLP)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>High contextual richness, improved semantic alignment, greater interpretability, balanced compactness via partitioning; leverages LLM strengths in summarization and generation to make domain graphs accessible for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fact recall, Inverse fact recall, Multi-hop reasoning, Multiple-choice</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DBLP (BERTScore P/R/F1) GLaM-7B (combined): Fact 0.424/0.426/0.424; Reverse 0.401/0.407/0.402; Multi-hop 0.409/0.410/0.408. GLaM-13B (combined): Fact 0.446/0.446/0.445; Reverse 0.381/0.385/0.382; Multi-hop 0.398/0.398/0.397 (Table 2). UMLS multiple-choice: GLaM-7B-MC accuracy Fact Recall 100.0%, Reverse 59.71%, Multi-hop 91.93% (Table 3). DBLP multiple-choice GLaM-7B-MC accuracy Fact 78.62%, Reverse 75.68%, Multi-hop 73.34% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Composite GLaM models outperformed single-strategy encodings (triples, adjacency-only, node-descriptor-only) and the unrefined Llama baselines on nearly all tasks; for UMLS LLM summarization alone was best but the composite also performed well; for DBLP, the composite achieved largest gains over baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>More complex pipeline (multiple generation steps); requires extra compute to produce descriptors and summaries; possible propagation of LLM rewriting errors into training data; inverse fact recall improvements are uneven (some tasks show limited gains); selecting and tuning components (k, N_max, summarization prompts) remains empirical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Deep bidirectional language-knowledge graph pretraining <em>(Rating: 2)</em></li>
                <li>Structgpt: A general framework for large language model to reason over structured data <em>(Rating: 2)</em></li>
                <li>Graph neural prompting with large language models <em>(Rating: 2)</em></li>
                <li>Large Language Models on Graphs: A Comprehensive Survey <em>(Rating: 1)</em></li>
                <li>Unifying Large Language Models and Knowledge Graphs: A Roadmap <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5356",
    "paper_id": "paper-de02ba19fb957ae30de7f09904ae3d983c3b50e7",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "NeighborhoodPartitioning",
            "name_full": "Neighborhood Partitioning and Subgraph Context Generation",
            "brief_description": "Algorithmic procedure that extracts a node's k-hop neighborhood, partitions it to respect LLM token limits (via N_max), and produces subgraph contexts for conversion into text and QA-pairs using f_aggr, f_partition, f_enc and f_qa operators (Algorithm 1).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Subgraph context partitioning",
            "representation_description": "For each node v, retrieve k‑hop neighborhood G_context(v,k) using f_aggr; partition G_context into subgraphs with node-count limit N_max (f_partition) so encoded text lengths remain below model token limit T_max; for each partition, encode (f_enc) and generate QA pairs (f_qa). This yields multiple (context, question-answer) training sequences per node. Partitioning is used to address skewed degree distributions and token limits.",
            "graph_type": "Knowledge graph (UMLS) and citation/author graph (DBLP)",
            "representation_properties": "Scales with neighborhood size; preserves local graph structure within each partition; allows variable context granularity; trades off completeness vs. token budget (controlled by N_max). Designed for interpretability and to avoid exceeding T_max; sensitive to skewed degree distributions.",
            "evaluation_task": "Fact recall, Inverse (reverse) fact recall, Multi-hop reasoning (graph-inference), Multiple-choice",
            "performance_metrics": "Used as part of all GLaM experiments; performance reported in downstream BERTScore (P/R/F1) and multiple-choice accuracy in Tables 1-3 (see other entries for encoding-specific numbers).",
            "comparison_to_other_representations": "Partitioning is an orthogonal preprocessing step applied to all encoding strategies; it enabled applying triple/adjacency/summarized encodings within token limits. No single numeric comparison isolated to partitioning alone, but experiments show larger encoded neighborhood contexts (enabled by thoughtful partitioning) improved inference performance.",
            "limitations_or_challenges": "Requires selecting N_max and k to trade off context completeness and token limits; skewed degree distributions complicate optimal partition sizing; sampling techniques for large neighbor lists noted as relevant but not implemented; potential information loss if critical nodes split across partitions.",
            "uuid": "e5356.0",
            "source_info": {
                "paper_title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Triples",
            "name_full": "Encoding via (source, relation, target) Triples",
            "brief_description": "Simple linearization that maps each graph edge into a single training sentence of the form (source, relation, target), producing one sample per edge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Triple linearization",
            "representation_description": "Each edge in the k‑hop neighborhood is converted into a single textual sentence representing the triple (source, relation, target). Each triple constitutes a standalone training example; minimal structural context beyond the single relation is provided.",
            "graph_type": "Knowledge graph (UMLS) and citation/author graph (DBLP)",
            "representation_properties": "Very compact per-edge representation; high fidelity for single-edge facts; low contextual breadth (only single-edge), limited capacity for multi-hop reasoning or capturing higher-order dependencies; easy to generate; may produce many short training samples.",
            "evaluation_task": "Fact recall, Inverse fact recall, Multi-hop reasoning (open-ended + multiple choice)",
            "performance_metrics": "UMLS (BERTScore P/R/F1): Fact recall 0.683/0.597/0.627; Reverse 0.431/0.533/0.474; Multi-hop 0.677/0.589/0.621 (Table 1). DBLP (BERTScore P/R/F1): Fact 0.105/0.103/0.104; Reverse 0.103/0.102/0.102; Multi-hop 0.100/0.099/0.099 (Table 2).",
            "comparison_to_other_representations": "Outperformed the untrained baseline on UMLS but performed poorly on DBLP relative to richer encodings (relational grouping, adjacency lists, node descriptors). Triples provided inferior performance for DBLP multi-hop and fact tasks, indicating insufficient context for domains with unfamiliar node labels.",
            "limitations_or_challenges": "Limited context prevents effective multi-hop reasoning; many isolated samples increase dataset size; performs poorly when node labels are unfamiliar to LLM (DBLP); sensitive to overfitting to frequent relation phrasing.",
            "uuid": "e5356.1",
            "source_info": {
                "paper_title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RelationalGrouping",
            "name_full": "Relational Grouping / Adjacency List Encoding",
            "brief_description": "Group-based encodings that aggregate multiple neighbors of the central node into one training sample either as an adjacency list or by partitioning neighbors by relation type, enabling multi-edge context per example.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Adjacency list / Relational grouping",
            "representation_description": "Two variants: (a) adjacency list: collate the central node v's entire adjacency list (multiple edges) into a single textual context; (b) relational grouping: partition neighbors into subsets grouped by relation type and encode each group as one context. Both deliver multi-target context per training example.",
            "graph_type": "Knowledge graph (UMLS) and citation/author graph (DBLP)",
            "representation_properties": "Increases context breadth and structural information per example; improves capacity for multi-hop reasoning; more compact than many single-triple samples; may still be large for high-degree nodes (necessitates partitioning). Improves semantic signal by showing relation groups.",
            "evaluation_task": "Fact recall, Inverse fact recall, Multi-hop reasoning, Multiple-choice",
            "performance_metrics": "UMLS (BERTScore P/R/F1) Relational Grouping: Fact 0.679/0.678/0.673; Reverse 0.403/0.537/0.459; Multi-hop 0.662/0.663/0.657 (Table 1). DBLP Relational Grouping: Fact 0.259/0.261/0.259; Reverse 0.259/0.264/0.260; Multi-hop 0.256/0.259/0.257 (Table 2). Adjacency List variant (DBLP): Fact 0.255/0.258/0.255; Reverse 0.247/0.252/0.249; Multi-hop 0.251/0.253/0.251.",
            "comparison_to_other_representations": "Outperformed triple-based encoding on most tasks (especially DBLP) and improved over baseline Llama; relational grouping/adacency provided better multi-edge context and thus better inference than triples. However, LLM summarization and node descriptor strategies further improved performance beyond basic adjacency/relational grouping.",
            "limitations_or_challenges": "Large neighbor lists can exceed token limits; selection/partitioning strategies required (N_max); naive inclusion of noisy or uninformative adjacency elements (e.g., venues) may not help; sampling techniques are relevant but not implemented in this work.",
            "uuid": "e5356.2",
            "source_info": {
                "paper_title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "NodeDescriptors",
            "name_full": "Node Descriptor Encoding (LLM-driven descriptors)",
            "brief_description": "Create textual descriptors for nodes by prompting the LLM to summarize a node's attributes or neighborhood (e.g., extracting author topic areas from paper abstracts) to produce interpretable labels for unfamiliar node identifiers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Node descriptors (LLM zero-shot expansion)",
            "representation_description": "For nodes with non-human-readable or unfamiliar labels, prompt the LLM to generate descriptive textual summaries of the node based on its k‑hop neighborhood (e.g., topics an author publishes on from abstracts). These descriptors are used in context encodings instead of raw IDs/names to improve semantic alignment.",
            "graph_type": "Citation/author graph (DBLP), Knowledge graph (UMLS) as applicable",
            "representation_properties": "Improves semantic alignment to LLM's internal knowledge; increases interpretability; reduces reliance on rare tokens; can introduce additional synonyms/knowledge; helps reduce overfitting to raw labels. Produces richer per-node text than raw names.",
            "evaluation_task": "Fact recall, Inverse fact recall, Multi-hop reasoning, Multiple-choice",
            "performance_metrics": "DBLP (BERTScore P/R/F1) Node Descriptors: Fact 0.313/0.312/0.312; Reverse 0.309/0.314/0.311; Multi-hop 0.318/0.316/0.316 (Table 2). Used as a component in combined GLaM where further improvements were seen.",
            "comparison_to_other_representations": "Outperformed simple triples and adjacency-only encodings on DBLP, where raw node names were unfamiliar; combining node descriptors with adjacency and summarization yielded the best DBLP results (GLaM-7B/13B).",
            "limitations_or_challenges": "Quality depends on LLM zero-shot summarization capability; may hallucinate or introduce extraneous synonyms if prompts not carefully controlled; additional compute to generate descriptors; potential loss of exact entity identifiers.",
            "uuid": "e5356.3",
            "source_info": {
                "paper_title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLMSummarization",
            "name_full": "LLM-based Prompt Rewriting / Summarization",
            "brief_description": "Use the LLM to rewrite or summarize graph-derived textual encodings to more coherent, human-interpretable forms (map unwieldy labels to readable phrases, reduce redundancy, supply synonyms), improving semantic alignment for fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "LLM summarization / prompt-based rewriting",
            "representation_description": "Apply prompting to rewrite raw triple/adjacency encodings into coherent natural-language summaries: map technical labels to human terms, collapse redundant labels, expand abbreviations and synonyms (e.g., 'Insulin human, rDNA origin' -&gt; 'Insulin therapy from recombinant DNA'), and diversify relation phrasing to mitigate overfitting.",
            "graph_type": "Knowledge graph (UMLS) and citation/author graph (DBLP)",
            "representation_properties": "Improves interpretability and semantic alignment; reduces redundant text; introduces synonyms and clarifying parentheticals; helps avoid overfitting to frequent relation phrasing; may increase lexical diversity improving learning.",
            "evaluation_task": "Fact recall, Inverse fact recall, Multi-hop reasoning, Multiple-choice",
            "performance_metrics": "UMLS (BERTScore P/R/F1) LLM Summarization: Fact 0.724/0.725/0.720; Reverse 0.386/0.527/0.445; Multi-hop 0.689/0.696/0.688 (Table 1). DBLP best when combined with adjacency: combined GLaM results reported (see Combined entry).",
            "comparison_to_other_representations": "On UMLS, LLM summarization encoding achieved the best results among tested encodings for fact recall and multi-hop reasoning (outperforming triples and relational grouping). For DBLP, summarization combined with adjacency and node descriptors produced top-performing GLaM models.",
            "limitations_or_challenges": "Summarization may introduce lexical variation that changes exact factual phrasing (risk of subtle semantic shift); relies on LLM rewriting quality; could mask the exact original labels needed for some tasks; doesn't fully resolve inverse fact recall (some improvements only marginal).",
            "uuid": "e5356.4",
            "source_info": {
                "paper_title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GLaM-Combined",
            "name_full": "GLaM Combined Encoding (Node descriptors + Adjacency + Summarization)",
            "brief_description": "Composite encoding strategy used for the strongest-performing GLaM variants: aggregate node descriptors, full adjacency lists as context, and apply LLM summarization to produce training samples for fine-tuning LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Composite GLaM encoding (aggregation + adjacency + summarization)",
            "representation_description": "Combine techniques: generate node descriptors for unfamiliar nodes, assemble adjacency lists (multiple edges per central node), partition as needed, and apply LLM-based summarization to produce polished text contexts; then generate QA pairs from the same subgraph contexts for supervised fine-tuning.",
            "graph_type": "Knowledge graph (UMLS) and citation/author graph (DBLP)",
            "representation_properties": "High contextual richness, improved semantic alignment, greater interpretability, balanced compactness via partitioning; leverages LLM strengths in summarization and generation to make domain graphs accessible for fine-tuning.",
            "evaluation_task": "Fact recall, Inverse fact recall, Multi-hop reasoning, Multiple-choice",
            "performance_metrics": "DBLP (BERTScore P/R/F1) GLaM-7B (combined): Fact 0.424/0.426/0.424; Reverse 0.401/0.407/0.402; Multi-hop 0.409/0.410/0.408. GLaM-13B (combined): Fact 0.446/0.446/0.445; Reverse 0.381/0.385/0.382; Multi-hop 0.398/0.398/0.397 (Table 2). UMLS multiple-choice: GLaM-7B-MC accuracy Fact Recall 100.0%, Reverse 59.71%, Multi-hop 91.93% (Table 3). DBLP multiple-choice GLaM-7B-MC accuracy Fact 78.62%, Reverse 75.68%, Multi-hop 73.34% (Table 3).",
            "comparison_to_other_representations": "Composite GLaM models outperformed single-strategy encodings (triples, adjacency-only, node-descriptor-only) and the unrefined Llama baselines on nearly all tasks; for UMLS LLM summarization alone was best but the composite also performed well; for DBLP, the composite achieved largest gains over baseline.",
            "limitations_or_challenges": "More complex pipeline (multiple generation steps); requires extra compute to produce descriptors and summaries; possible propagation of LLM rewriting errors into training data; inverse fact recall improvements are uneven (some tasks show limited gains); selecting and tuning components (k, N_max, summarization prompts) remains empirical.",
            "uuid": "e5356.5",
            "source_info": {
                "paper_title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2
        },
        {
            "paper_title": "Deep bidirectional language-knowledge graph pretraining",
            "rating": 2
        },
        {
            "paper_title": "Structgpt: A general framework for large language model to reason over structured data",
            "rating": 2
        },
        {
            "paper_title": "Graph neural prompting with large language models",
            "rating": 2
        },
        {
            "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
            "rating": 1
        },
        {
            "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
            "rating": 1
        }
    ],
    "cost": 0.012995999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</h1>
<p>Stefan Dernbach ${ }^{<em> 1}$, Khushbu Agarwal ${ }^{</em> 1}$, Alejandro Zuniga ${ }^{1}$, Michael Henry ${ }^{1}$, Sutanay Choudhury ${ }^{1}$<br>${ }^{1}$ Pacific Northwest National Lab<br>902 Battelle Boulevard<br>Richland, Washington 99354, USA</p>
<h4>Abstract</h4>
<p>Integrating large language models (LLMs) with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a LLM to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no - such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the models' capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.</p>
<h2>Introduction</h2>
<p>Large language models (LLMs) have recently demonstrated disruptive potential with their ability to generate text and answer questions with human-like language proficiency. However, their reasoning remains limited by a reliance solely on textual training data, lacking integration with structured knowledge graphs encoding intricate real-world constraints and relationships. Bridging this divide by aligning LLMs with multi-relational graphs can enable grounded, factual inferences vital for applications driven by graph-structured data.</p>
<p>Past work on LLM-graph integration has predominantly focused on harnessing LLM knowledge to improve graph</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>neural network performance on tasks like node classification and link prediction (Jin et al. 2023). The alternate direction of augmenting or "fine-tuning" LLMs to reason over graphs has remained relatively unexplored. For instance, existing techniques still treat knowledge bases as external retrievable stores (Lewis et al. 2020), rather than integrating them into model parameters. Using the LLM as an encoder to transform text-based node and edge labels in a graph, and then fusing the LLM and GNN-derived representations has been the dominant approach for diverse applications ranging from product recommendation (Choudhary et al. 2022) to biomedical question-answering in a multiple-choice setting (Yasunaga et al. 2022).</p>
<p>Our work is the first study on incorporating domainspecific knowledge graphs directly into LLM representations via fine-tuning, targeting accuracy improvements on open ended question answering(QA), a more complex task than the multiple choice setting explored in previous works. By encoding both schema and entities within specialized graphs like those in biomedical repositories, recommendation systems and social networks, we can enhance multi-hop reasoning grounded by real-world constraints. This addresses the challenge of factual hallucinations in free-form reasoning, while retaining versatile text handling strengths (Touvron et al. 2023; Nori et al. 2023).</p>
<h2>Problem Definition</h2>
<p>Our work targets natural language question answering (QA) on graph data. We define $f_{L L M}: \mathcal{V} \rightarrow \mathcal{V}$ as a functional representation of a large language model that accepts a sequence of high-dimensional discrete tokens from a vocabulary $\mathcal{V}$ as input and produces an output sequence drawn from the same space. Given a natural language question $Q$ (also referred to as a prompt), $f_{L L M}(\cdot)$ tokenizes $Q$ into a sequence of tokens from $\mathcal{V}$ and returns an answer $A=$ $f_{L L M}(\cdot)(Q)$.</p>
<p>Next, we introduce a graph dataset $G=(V, E)$, where $V$ is the set of vertices and $E$ is the set of edges. Importantly, we assume that $G$ was not included in the training data for $f_{L L M}(\cdot)$. Figure 1 describes real-world use cases that motivate these graph QA workloads, such as social or professional network-based recommendations or patient-specific clinical hypothesis generation.</p>
<p>Our goal is to introduce a new function $f_{G L M}$ that uti-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: Motivating examples for aligning foundational models with domain-specific knowledge graphs. The left figure demonstrates a query where a LLM needs to be integrated with a knowledge graph derived from a social network. The right figure demonstrates a need where a LLM needs to be integrated with a patient-profiles to disease network extracted from an electronic healthcare records database.</p>
<p>lizes information from $G$ to answer question $Q$. Formally, $A=f_{GLM}(G,Q)$. In this paper, we systematically explore following three query classes, in both open ended question answering and multiple choice setting:</p>
<ol>
<li>Fact recall: Evaluates GLM’s ability to recall domain facts seen during training (e.g. answering ”What are possible treatments for diabetes?” after seeing ”Diabetes is treated with insulin and metformin”).</li>
<li>Inverse fact recall: Assesses handling of relationship directionality, which recent work shows standard LMs struggle with (”A is B” does not imply ”B is A”) <em>Berglund et al. (2023)</em>. This is a key facet of graphs not previously explored for LLM-graph models.</li>
<li>Chain-of-Reasoning: Complex queries such as Figure 1 (left) that necessitate appropriately employing graph structure knowledge.</li>
</ol>
<h2>Technical Approach and Related Work</h2>
<p>Exploring the intersection of large language models and knowledge graphs has strong significant interest over the past few years. We begin by outlining key design paradigms from literature for answering complex reasoning queries on knowledge graphs by aligning graph data with large language models (LLMs) and refer the reader to a collection of excellent survey articles for a detailed overview of this emerging sub-field <em>Pan et al. (2023); Liu et al. (2023); Jin et al. (2023)</em>. Any approach must address two questions: 1) how to encode graph $G$ into the LLM’s knowledge representation, and 2) how query $Q$ is executed.</p>
<p>Delegation to a GNN A common approach uses a graph neural network as encoder. Given a natural language query $Q$ this requires extracting entities and relations from the query and integrate GNN and LLM representations. Such integration can be done via learning a joint-model coupling the LLM and GNN representations <em>Saxena et al. (2020); Yasunaga et al. (2022)</em> or using a soft prompting approach that inserts a GNN-derived vector embedding into the LLM prompt <em>Tian et al. (2023)</em>.</p>
<p>Retrieval Augmented Generation Retrieval augmented generation (RAG) approaches follow a similar path of implementation. The difference here is that instead of delegating to a GNN, an external graph database <em>Jiang et al. (2023)</em> or a vector database <em>Tian et al. (2023)</em> containing node and/or relation embeddings is queried . In both approaches, the LLM is used as an routing interface to a native graph database or machine-learning model, and the answers from the appropriate graph-based component is fed back to user with LLM serving as generative layer that produces the final answer.</p>
<p>Few-Shot Prompting In this approach subgraphs relevant to $Q$ are extracted and inserted into the prompt with examples <em>Fatemi et al. (2023)</em>. While promising, this approach faces potential drawbacks requiring encoding full graphs in LLM prompts or performing multi-hop subgraph retrievals for each question.</p>
<p>Motivation for Fine-Tuning Irrespective of their differences, all of the above approaches potentially face a fundamental limitation - they cannot contextually integrate symbolic constraints to shape intermediate reasoning. Approaches retrieving embeddings or triples solely based on the initial query overlook multifaceted execution where dynamic routing with mixture-of-experts <em>Shazeer et al. (2017); Zhou et al. (2022)</em>, planning <em>Hao et al. (2023); Yao et al. (2023)</em> and heuristics search <em>Sprueill et al. (2023); Sun et al. (2023)</em> steps modify information needs in every reasoning</p>
<p>step. Fixed retrieval precludes dynamically folding graph structure into each decision point.</p>
<p>In contrast, fine-tuning instills domain knowledge into model parameters and representations a priori, rather than treating the graph as an external add-on. By encoding constraints and dependencies directly into the knowledge substrate, fine-tuning allows contextual graph influence at each step of modeled cognition. Rather than acting as a static look-up, the graph becomes an integral inference component - shaping complex reasoning in a tighter, more fine-grained fashion.</p>
<p>Our Approach and Contributions We introduce an algorithm to iteratively partition and encode the neighborhood subgraph around each node into textual sentences for finetuning data. This transforms graph structure into a format that large language models can ingest and fine-tune. We explore encoding strategies on two graphs: 1) UMLS - a biomedical knowledge base, and 2) DBLP - an academic publication network.</p>
<p>Our work makes the following contributions.</p>
<ol>
<li>Our neighborhood partitioning and encoding scheme accommodate real-world graph properties like skewed size distributions and sparsity. Our approach opens up future experimental possibilities where encoding is tuned for LLMs by setting context size limits based on costaccuracy tradeoffs.</li>
<li>We propose and assess five encoding approaches leveraging the LLM’s innate summarization and text generation strengths. For example, we evaluate neighborhood summaries produced by the LLM. Encouragingly, our results align with similar methods from concurrent work (Fatemi, Halcrow, and Perozzi 2023), confirming the promise of this direction.</li>
<li>We developed a new domain question answering dataset based on two above graphs with a suite of evaluation tasks capturing link prediction to multi-hop reasoning queries. The code and datasets will be released as open source upon acceptance.</li>
</ol>
<h2>Methods</h2>
<p>Task Definition We propose methods for transforming a knowledge graph into a corresponding text-based finetuning dataset for language models. Our goal is to produce pairs of (context, question-answer) (Ouyang et al. 2022; Wei et al. 2021) that require neural-graph reasoning to answer open domain questions involving relational or multi-hop reasoning.</p>
<p>We begin with describing a generic algorithm (Algorithm 1) that encodes a node’s k-hop neighborhood into such a context and QA-pair through a composition of multiple operator functions. We discuss the implementation of these operators in finer detail in the later half of the section.</p>
<h3>Optimal Generation of Subgraph Contexts</h3>
<p>For every node $v \in V(G)$, we transform the $k$-hop neighborhood of $v$ into a set of pairs of the form:</p>
<table>
<thead>
<tr>
<th>Algorithm 1: Fine-tuning dataset generation.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Require: Graph $G$ with nodes set $V$ and edges set $E$,</td>
</tr>
<tr>
<td>context subgraph node limit $N_{\max}$</td>
</tr>
<tr>
<td>Fine-tuning dataset $D \leftarrow \emptyset$</td>
</tr>
<tr>
<td>for each $v \in V(G)$ do</td>
</tr>
<tr>
<td>$G_{\text {context }}(v, k)=f_{\text {aggr }}(G, v, k)$</td>
</tr>
<tr>
<td>$\text { partitions }=f_{\text {partition }}\left(G_{\text {context }}(v, k), N_{\max }\right)$</td>
</tr>
<tr>
<td>for each $g_{\text {sub }} \in$ partitions do</td>
</tr>
<tr>
<td>$X_{\text {context }}=f_{\text {enc }}\left(g_{\text {sub }}\right)$</td>
</tr>
<tr>
<td>$X_{q a}=f_{q a}\left(g_{\text {sub }}\right)$</td>
</tr>
<tr>
<td>$\text { append }\left(D, \operatorname{concat}\left(\left[X_{\text {context }}, X_{q a}\right]\right)\right.$</td>
</tr>
<tr>
<td>end</td>
</tr>
<tr>
<td>end</td>
</tr>
<tr>
<td>return $D$</td>
</tr>
</tbody>
</table>
<p>$\left(f_{\text {enc }}(G, v), f_{q a}(G, v)\right)$. Algorithm 1 describes a step-bystep in which we iterate over every node in the graph and encode it’s k-hop neighborhood subgraph, denoted as $G_{\text {context }}(v, k)$ into the alternate text-based representation.</p>
<ol>
<li>We retrieve the k-hop neighborhood subgraph as $G_{\text {context }}(v, k)$ using a query function denoted as $f_{\text {aggr }}(\cdot)$.</li>
<li>$f_{\text {enc }}$ encodes $G_{\text {context }}(v, k)$ or its partitioned subgraph into text.</li>
<li>$f_{q a}(G, v)$ generates QA pairs requiring reasoning on $G_{\text {context }}(v, k)$. Same subgraph is used to drive the inputs for $f_{\text {enc }}(G, v)$ and $f_{q a}(G, v)$.</li>
<li>The concatenated output of $f_{\text {enc }}(G, v)$ and $f_{q a}(G, v)$ is a text sequence of discrete tokens $X_{v}$ drawn from $V$, the vocabulary of the LLM function $f_{L L M}(\cdot)$ mentioned previously.</li>
<li>Any LLM function $f_{L L M}(\cdot)$ needs to operate within a maximum token limit constraint (denoted as $T_{\max }$ ). We partition $G_{\text {context }}(v, k)$ to respect LLM token limits $T_{\max }$ such that $\operatorname{len}\left(X_{v}\right)&lt;T_{\max }$.</li>
</ol>
<p>We introduce a hyperparameter $N_{\max }$ to partition $G_{\text {context }}(v, k)$ into subgraphs within node count $N_{\max }$. This prevents tokenized sequence lengths from exceeding $T_{\max }$. Choosing an optimal $N_{\max }$ is key because degree distributions in $G_{\text {context }}(v, k)$ can be highly skewed. Given cost constraints associated with $T_{\max }$, we want to pick $N_{\max }$ and encoding strategies that maximize context lengths for the LLM’s capabilities.</p>
<h3>Neighborhood Encoding Functions</h3>
<p>The purpose of a neighborhood encoding function is to translate the neighborhood subgraph $G_{\text {context }}(v, k)$ centered around a node $v$ into a textual representation that can be effectively processed by a large language model (LLM). This process is crucial for enabling the LLM to perform higher-order reasoning and answer complex questions about the graph.</p>
<p>There are two main factors that influence the choice of a neighborhood encoding function:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of Graph Encodings in GLaM: Top left box shows "Encoding via triples", where each line represents an edge mapped to one training sample. The bottom left box shows graph encoding when given a node and relation, all relevant entities are collated into single training sample. The bottom right box shows when all relations/edges corresponding to a node are coalesced into single training sample, and top right box demonstrates the impact of summarization on the training sample. Summarizing helps to 1) map unwieldy node labels into human interpretable form, 2) reduce redundant terms, and 3) reduce overfitting to frequent node and edge labels. Collectively this leads to better semantic alignment between the knowledge graph and LLM's vocabulary and improves resulting model performance in all graph tasks.</p>
<h3>1. Communicating Graph Structure and Higher-Order Reasoning Requirements to the LLM</h3>
<p>The encoding function should effectively capture the structural relationships between nodes in the subgraph, as well as any higher-order logical dependencies that may exist. This can be achieved by incorporating information about the edges and their types, as well as the relationships between multiple nodes.</p>
<h3>2. Semantic Alignment with the LLM's Internal Knowledge Representation</h3>
<p>The encoding should represent the nodes and relations in the graph in a way that is consistent with how the LLM stores and interprets information. This can involve using natural language labels for nodes and edges, or generating descriptive labels using a node's neighborhood when node labels are not recognizable to a LLM (such as an academic network), while ensuring that the encoded representation preserves the semantic meaning of the graph elements.</p>
<h4>Encoding via Triples</h4>
<p>A simple approach to neighborhood encoding is to translate the edge data into (source, relation, target) triples. This provides the LLM with basic information about the relationships between nodes, but it is limited to representing only single edges per training sample and has limited context size.</p>
<h4>Encoding via Adjacency List/Relational Groups</h4>
<p>To enable the LLM to perform more complex reasoning tasks, we update the neighborhood encoding to include information about multiple nodes in the subgraph. We experiment with two different options: including the entire adjacency list of the central node <em>v</em>, and by partitioning the neighbors into subsets based on their relation types. We observe that more sophisticated approaches, such as sampling techniques are relevant for large neighbor lists but are not implemented in current work.</p>
<h4>Encoding via Summarization</h4>
<p>Next, we focus on the semantic alignment objective and use prompting methods to rewrite the encoding from above methods into more coherent representations (Figure 2).</p>
<ul>
<li>The prompting allows us to map unwieldy node labels to human understandable terms: For example, "Insulin human, rDNA origin" is mapped by LLM to "Insulin therapy from recombinant DNA" allowing for better interpretation during fine-tuning.</li>
<li>It reduces redundant text from similarly labeled nodes: "Diabetes mellitus, Type 1 diabetes mellitus, Type 2 diabetes mellitus" is mapped to "diabetes, including Type 1 and Type 2 diabetes."</li>
<li>Introduces additional knowledge/synonyms into training: "Hypoinsulinaemia" is mapped to "low insulin levels (hypoinsulinaemia)," and "rDNA" is expanded to "recombinant DNA."</li>
<li>Prompt-based rewriting also helps reduce address overfitting training to only a few relation labels, by mapping them to different phrases. Examples of such overfitting were observed with the "may treat" relationship, where the high number of occurrence of this phrase in a specific pattern causes the LLM to generate answers incorrectly filled with too many occurrences of the "may</li>
</ul>
<p>treat" phrase.
Encoding via Node Descriptors The previous encoding step leveraged the LLM’s understanding of specific entities (such as "rDNA") to rewrite with maximal semantic alignment. However, training on new graph data can include unfamiliar terms to the LLM, i.e. words or phrases that appear rarely or do not occur at all in initial training. A common example of this problem involves encoding the names of people not common to standard LLM training datasets. Also, we do not want to map a person based on their name, but account for their profile attributes or k-hop connectivity in the network. We generalize this need by transforming the k-hop context subgraph ( $G_{\text {context }}(v, k)$ ) into a set of textbased node descriptors by leveraging the LLM’s zero-shot capabilities. Typically, this is a step where an alternate implementation would have retrieved a GNN representation. For example, to expand on the information about authors in the DBLP dataset, we prompt the LLM to extract the topic areas of paper abstracts and construct a list of topics the author has published on from their paper history.</p>
<p>Generating Question-Answer Pairs Finally, given a text context generated from a subgraph $G_{\text {context }}(v, k)$, we generate a set of question-answer pairs via prompting the text context for different tasks (fact recall, inverse fact recall, multi-hop question answering). Each of the questions are also mapped into two style of answers: 1) open-domain question-answering, and 2) multiple-choice questions. For example, given a (head, relation, tail) triple as the subgraph context, its multiple choice answer candidates are generated by including one of the tail entities and a random selection of other nodes in the graph to form a set of possible answers to the question.</p>
<h2>Experiments</h2>
<p>In this section, we address following research questions (RQ) through experimental analysis:</p>
<ol>
<li>RQ-1 Does finetuning using graph encoding improve an LLM’s ability to recall the facts?</li>
<li>RQ-2 Does finetuning an LLM with graph encoding improve its ability to answer open-domain natural language questions through performing multi-hop reasoning on the graph domain?</li>
<li>RQ-3 Which strategies for encoding the subgraph context yields maximal semantic alignment of the original LLM and the target graph?</li>
</ol>
<h2>Datasets</h2>
<p>We present the results of training GLaMs on two graph datasets, DBLP (Tang et al. 2008) and UMLS (Bodenreider 2004), with diverse applications and coverage in LLMs to demonstrate the response improvement over the baseline language models.</p>
<p>Unified Medical Language System (UMLS) (Bodenreider 2004) is a medical knowledge graph. We use a processed version of the knowledge graph from Yasunaga et</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>al. (Yasunaga et al. 2022) consisting of 297, 927 concepts, 98 relation types and $1,212,586$ edges that capture relationships across a breadth of medical concepts. For GLaM training, we select a subgraph that captures relationships between different diseases, symptoms and medications. This results in a reduction to 4 different relation types: "cause of", "may cause", "risk factor of", and "may treat" totalling 126, 149 triples.</p>
<p>DBLP (Tang et al. 2008) is a citation graph dataset extracted from DBLP, ACM, MAG, and other sources. The dataset includes paper citations, abstracts, authors, publication years, venues, and titles. For training the GLaM we focus on the set papers containing titles, abstracts, venues, and 2 or more authors, leading to 19,577 unique papers.</p>
<h2>Training and Inference setup</h2>
<p>For both UMLS and DBLP, the extracted natural language questions and answers were split into 70% training (fact recall) and 30% test (multi-hop reasoning). We used Microsoft Deepspeed framework (Rasley et al. 2020) for supervised prompt and response fine-tuning. A grid-search was performed over training hyper-parameters using Llama-7b-chat-hf as the base model for training GLaM. A learning rate of $1 e-5$ and a cosine learning rate scheduler were used with the fused Adam optimizer with bfloat16 precision. The maximum sequence length was set to 256 and a maximum of 4 training epochs were used for all models. A cluster of 8 A100 GPUs with 80GB of GPU memory each were used for training with a per-device batch size of 4 questions resulting in a total training batch size of 32. We use Llama-2-7b-chat-hf and Llama-2-13b-chat-hf (Touvron et al. 2023) models from Hugging Face as the baseline models for training. Training the 7b model on UMLS takes approximately 9 minutes and 16 minutes for the 13b model. For DBLP, training time is approximately 11 and 21 minutes respectively.</p>
<h2>Evaluation Tasks</h2>
<p>Fact recall: This task is equivalent to question answering tasks in language models and test GLaM’s ability to remember domain level facts seen during training. For example, given a training sentence such as "Diabetes is treated with insulin and metformin" (from UMLS), the model is queried for "What are possible treatment of diabetes?". Similiarly, for the DBLP dataset given a sentence such as "[Students learn CS in different ways: insights from an empirical study] was written by Anders Berglund.", the model is queried with "[Students learn CS in different ways: insights from an empirical study] was written by whom?" The UMLS question set for fact recall contains 7710 questions and the DBLP set contains 13,704 .</p>
<p>Inverse Fact Recall: This task is equivalent to reverse question answering tasks (Berglund et al. 2023) in language models and test GLaM 's ability to infer reverse relationships from the domain level facts seen during training. For example, given the above training statement, the model is queries for "Which disease can be treated with insulin?"</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Fact Recall</th>
<th></th>
<th></th>
<th>Reverse Recall</th>
<th></th>
<th></th>
<th>Multi-hop Reasoning</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
</tr>
<tr>
<td>Llama 7B Chat</td>
<td>0.594</td>
<td>0.631</td>
<td>0.608</td>
<td>0.382</td>
<td>0.519</td>
<td>0.439</td>
<td>0.595</td>
<td>0.631</td>
<td>0.609</td>
</tr>
<tr>
<td>GLaM (Triples)</td>
<td>0.683</td>
<td>0.597</td>
<td>0.627</td>
<td>$\mathbf{0 . 4 3 1}$</td>
<td>0.533</td>
<td>$\mathbf{0 . 4 7 4}$</td>
<td>0.677</td>
<td>0.589</td>
<td>0.621</td>
</tr>
<tr>
<td>GLaM (Relational Grouping)</td>
<td>0.679</td>
<td>0.678</td>
<td>0.673</td>
<td>0.403</td>
<td>$\mathbf{0 . 5 3 7}$</td>
<td>0.459</td>
<td>0.662</td>
<td>0.663</td>
<td>0.657</td>
</tr>
<tr>
<td>GLaM (LLM Summarization)</td>
<td>$\mathbf{0 . 7 2 4}$</td>
<td>$\mathbf{0 . 7 2 5}$</td>
<td>$\mathbf{0 . 7 2 0}$</td>
<td>0.386</td>
<td>0.527</td>
<td>0.445</td>
<td>$\mathbf{0 . 6 8 9}$</td>
<td>$\mathbf{0 . 6 9 6}$</td>
<td>$\mathbf{0 . 6 8 8}$</td>
</tr>
<tr>
<td>Llama 13B Chat</td>
<td>0.699</td>
<td>0.623</td>
<td>0.652</td>
<td>0.396</td>
<td>0.529</td>
<td>0.451</td>
<td>0.695</td>
<td>0.623</td>
<td>0.650</td>
</tr>
<tr>
<td>GLaM 13B (LLM Summarization)</td>
<td>0.708</td>
<td>0.730</td>
<td>0.714</td>
<td>0.395</td>
<td>0.534</td>
<td>0.453</td>
<td>0.675</td>
<td>0.697</td>
<td>0.681</td>
</tr>
</tbody>
</table>
<p>Table 1: UMLS Results comparing the baseline Llama LLM with three versions of a refined GLaM on questions generated from the UMLS knowledge graph. Each version corresponds to an encoding strategy described in the Methods section. Precision (P), Recall (R), and F1 scores are reported using Bert scores.</p>
<table>
<thead>
<tr>
<th></th>
<th>Fact Recall</th>
<th></th>
<th></th>
<th>Reverse Recall</th>
<th></th>
<th></th>
<th>Multi-hop Reasoning</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
</tr>
<tr>
<td>Llama 7B Chat</td>
<td>0.174</td>
<td>0.177</td>
<td>0.175</td>
<td>0.168</td>
<td>0.173</td>
<td>0.170</td>
<td>0.168</td>
<td>0.171</td>
<td>0.169</td>
</tr>
<tr>
<td>GLaM (Triples)</td>
<td>0.105</td>
<td>0.103</td>
<td>0.104</td>
<td>0.103</td>
<td>0.102</td>
<td>0.102</td>
<td>0.100</td>
<td>0.099</td>
<td>0.099</td>
</tr>
<tr>
<td>GLaM (Relational Grouping)</td>
<td>0.259</td>
<td>0.261</td>
<td>0.259</td>
<td>0.259</td>
<td>0.264</td>
<td>0.260</td>
<td>0.256</td>
<td>0.259</td>
<td>0.257</td>
</tr>
<tr>
<td>GLaM (Adjacency List)</td>
<td>0.255</td>
<td>0.258</td>
<td>0.255</td>
<td>0.247</td>
<td>0.252</td>
<td>0.249</td>
<td>0.251</td>
<td>0.253</td>
<td>0.251</td>
</tr>
<tr>
<td>GLaM (Node Descriptors)</td>
<td>0.313</td>
<td>0.312</td>
<td>0.312</td>
<td>0.309</td>
<td>0.314</td>
<td>0.311</td>
<td>0.318</td>
<td>0.316</td>
<td>0.316</td>
</tr>
<tr>
<td>GLaM -7B</td>
<td>$\mathbf{0 . 4 2 4}$</td>
<td>$\mathbf{0 . 4 2 6}$</td>
<td>$\mathbf{0 . 4 2 4}$</td>
<td>$\mathbf{0 . 4 0 1}$</td>
<td>$\mathbf{0 . 4 0 7}$</td>
<td>$\mathbf{0 . 4 0 2}$</td>
<td>$\mathbf{0 . 4 0 9}$</td>
<td>$\mathbf{0 . 4 1 0}$</td>
<td>$\mathbf{0 . 4 0 8}$</td>
</tr>
<tr>
<td>Llama 13B Chat</td>
<td>0.150</td>
<td>0.155</td>
<td>0.152</td>
<td>0.144</td>
<td>0.151</td>
<td>0.147</td>
<td>0.153</td>
<td>0.159</td>
<td>0.155</td>
</tr>
<tr>
<td>GLaM -13B</td>
<td>0.446</td>
<td>0.446</td>
<td>0.445</td>
<td>0.381</td>
<td>0.385</td>
<td>0.382</td>
<td>0.398</td>
<td>0.398</td>
<td>0.397</td>
</tr>
</tbody>
</table>
<p>Table 2: DBLP Results comparing the baseline Llama LLM with five versions of GLaM on questions generated from the DBLP citation graph. GLaM-7B/13B represents a combination of strategies: aggregation of node descriptors, utilizing adjacency lists as context and performing summarization. Precision (P), Recall (R), and F1 scores are reported using Bert scores.</p>
<table>
<thead>
<tr>
<th></th>
<th>UMLS</th>
<th></th>
<th></th>
<th>DBLP</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Fact Recall</td>
<td>Reverse Fact Recall</td>
<td>Multi-hop Reasoning</td>
<td>Fact Recall</td>
<td>Reverse Fact Recall</td>
<td>Multi-hop Reasoning</td>
</tr>
<tr>
<td>Llama 7B Chat</td>
<td>61.23</td>
<td>57.21</td>
<td>61.1</td>
<td>35.26</td>
<td>36.19</td>
<td>27.99</td>
</tr>
<tr>
<td>GLaM -7B-MC</td>
<td>100</td>
<td>59.71</td>
<td>91.93</td>
<td>78.62</td>
<td>75.68</td>
<td>73.34</td>
</tr>
</tbody>
</table>
<p>Table 3: Multiple Choice Results Comparison of LLM and GLaM accuracy for fact recall, reverse fact recall, and fact inference on the UMLS and DBLP graphs.</p>
<p>There are 11130 questions in the UMLS reverse fact recall question set and 13704 in the DBLP set.</p>
<p>Multi-hop Reasoning: This task mirrors the link prediction task in a GNN setting and tests the GLaM’s ability to infer new facts (graph edges) by reasoning over facts seen during training. The UMLS question set for multi-hop reasoning contains 3347 questions and the DBLP set contains 5873. A common style of question we explore for DBLP is that of recommending authors to collaborate with. Using the DBLP question referred to in the fact recall task as example, a multi-hop reasoning question would ask: “Anders Berglund would like to write a paper titled [Students learn CS in different ways: insights from an empirical study] to publish in Proceedings of Australasian computing education. Who should they work with and why?”</p>
<p>Multiple-choice: Each evaluation task: fact recall, inverse fact recall, and multi-hop reasoning, are reformatted as multiple choice questions. Question includes the correct answer and four additional incorrect options randomly selected from the graphs respectively. Note that this is a much easier task than open ended question answering setting, requiring models to only pick the most likely answer out of given options.</p>
<h2>Evaluation Metrics</h2>
<p>To account for the inherent text variability in LLM or GLaM generated answers, we use the BERTScore metric <em>Zhang et al. (2019)</em> for open-ended domain QA setting and accuracy for multiple choice questions.</p>
<p>Bert Score: Compares text similarity between the model generated response to the expected response. The microsoft/deberta-xlarge-mnli model <em>He et al. (2020)</em> is used for calculating BertScore for it’s strong performance in natural language understanding (NLU) tasks. We report precision (P), recall (R), and F1 scores across the evaluation set.</p>
<p>Accuracy: We use the standard accuracy measure to evaluate a model’s ability to identify the correct answer out of 5</p>
<p>possible choices in a multiple choice setting.</p>
<h2>Results</h2>
<p>Results for training GLaM are presented in Table 1 and Table 2. We discuss the results on the individual datasets and then provide overall conclusions.</p>
<p>UMLS graph experiment results are given in Table 1. For both fact recall and inference, using LLM based summarization encoding to rewrite the statements exhibits the best performance across precision, recall and F1 scores. However, for reverse fact recall, using the simpler training approaches leads to a slight improvement in scores. All finetuned GLaM versions outperform the baseline LLM showing that even naive training strategies offer some improvement over the baseline LLM. While the 13b version of Llama outperforms its 7b counterpart, once trained, there is negligible difference between the 13b and 7b GLaM .</p>
<p>DBLP citation graph experimental results are given in Table 2. The GLaM version with complete adjacency and LLM based summarization achieves the best results across all tasks. Unsurprisingly, the untrained LLM did only moderately better than random guessing for the multiple choice task because of the number of unfamiliar names in the dataset. There is also a general trend of improved performance as neighborhood information is collated into the training, with the exception of adding the venue of the publication not having a noticeable affect,likely due to title being sufficient to capture a publications context. There is a slight improvement of the 13b version of GLaM over the 7b version for fact recall but the 7b version slightly outperforms the larger GLaM on the reverse fact recall and fact inference tasks. This combined with similar findings on UMLS indicate that the smaller LLM is sufficient for fact retention and inference when fine-tuned for the domain.</p>
<p>Multiple Choice results for both UMLS and DBLP are provided in Table 3. Across all tasks, GLaM outperforms the unrefined LLM, with the smallest difference being on the reverse facts for UMLS where GLaM noticeably does not learn to infer the inverse relationships from training. For UMLS fact recall GLaM demonstrates $100 \%$ accuracy on recalling the answers to the training set and similarly performs extremely well on the multi-hop reasoning questions. We hypothesize that the even larger gap between LLM and GLaM on the multiple choice results compared to the difference on the open ended question results comes from GLaM learning to differentiate the good answers from poor ones even if it does not explicitly know the correct answer.</p>
<p>Graph Aligned Language Models Significantly Improve Domain Knowledge Retrieval Tasks. Large language models are extraordinary tools for general knowledge but can not produce answers to many domain specific questions modeled in complex networks. This is evidenced by GLaM outperforming LLM across all domain level tasks, including simple fact retrieval questions.</p>
<p>Increasing the Node Neighborhood Context During Training Improves Inference Performance. Both the UMLS (Table 1) and DBLP (Table 2) cases demonstrate that incorporating multiple edges into each training instance improves the language models recall and reasoning. This is ev-
ident as GLaM training evolves from single triple samples, to relations with multiple targets, and further to include additional neighborhood information such as the topic areas an author publishes in.</p>
<p>Node Context Summarization Using LLM Improves Learning. Using a LLM to rewrite or summarize statements produced from the node neighborhood encoding improves GLaM's fact recall and multi-hop reasoning as shown on Table 1. The LLM summarized version for the UMLS graph encoding outperforms other GLaM versions even if the same information is present in training. We postulate that variation in word choice, mapping of node labels to more interpretable names helps significantly improve the learning process.</p>
<h2>Conclusions and Future Work</h2>
<p>We demonstrate an effective approach to integrate domainspecific knowledge graphs into large language models via fine-tuning. Empirically, this technique yields significant gains in multi-hop reasoning ability over the base LLM. Our proposed fine-tuning method encodes graph structure and it's semantic knowledge into the LLM, by maximally leveraging the original LLM's strengths - textual understanding, commonsense knowledge and generative capabilities.</p>
<p>In particular, quantitative experiments verify F1 score improvements of $18 \%$ on fact recall and $13 \%$ on complex inference queries requiring multi-hop reasoning on the UMLS domain for which the LLM already has some knowledge, and $142 \%$ and $141 \%$ respectively on DBLP's social network structure which represents novel information for the LLM. Given the importance of directionality of relationships in a graph, we also measure the improvement of recalling inverse facts by the resulting model. Overall, our experiments while preliminary in nature, confirm that integration via fine-tuning instills more reliable reasoning capacity based on graphs containing specialized entities and relationships, and it enables tighter coupling of structured symbolic knowledge with learned representations. Evaluating the effectiveness of the partitioning and encoding schemes across a wider range of larger-scale graphs with highly uneven connectivity distributions are candidate for future work.</p>
<h2>References</h2>
<p>Berglund, L.; Tong, M.; Kaufmann, M.; Balesni, M.; Stickland, A. C.; Korbak, T.; and Evans, O. 2023. The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A". arXiv preprint arXiv:2309.12288.
Bodenreider, O. 2004. The unified medical language system (UMLS): integrating biomedical terminology. Nucleic acids research, 32(suppl.1): D267-D270.
Choudhary, N.; Rao, N.; Subbian, K.; and Reddy, C. K. 2022. Graph-based Multilingual Language Model: Leveraging Product Relations for Search Relevance. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2789-2799.
Fatemi, B.; Halcrow, J.; and Perozzi, B. 2023. Talk like a graph: Encoding graphs for large language models. arXiv preprint arXiv:2310.04560.</p>
<p>Hao, S.; Gu, Y.; Ma, H.; Hong, J. J.; Wang, Z.; Wang, D. Z.; and Hu, Z. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992.
He, P.; Liu, X.; Gao, J.; and Chen, W. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.
Jiang, J.; Zhou, K.; Dong, Z.; Ye, K.; Zhao, W. X.; and Wen, J.-R. 2023. Structgpt: A general framework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645.
Jin, B.; Liu, G.; Han, C.; Jiang, M.; Ji, H.; and Han, J. 2023. Large Language Models on Graphs: A Comprehensive Survey. arXiv preprint arXiv:2312.02783.
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459-9474.
Liu, J.; Yang, C.; Lu, Z.; Chen, J.; Li, Y.; Zhang, M.; Bai, T.; Fang, Y.; Sun, L.; Yu, P. S.; et al. 2023. Towards graph foundation models: A survey and beyond. arXiv preprint arXiv:2310.11829.
Nori, H.; King, N.; McKinney, S. M.; Carignan, D.; and Horvitz, E. 2023. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744.
Pan, S.; Luo, L.; Wang, Y.; Chen, C.; Wang, J.; and Wu, X. 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. arXiv preprint arXiv:2306.08302.
Rasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, 3505-3506.
Saxena, A.; Tripathi, A.; and Talukdar, P. 2020. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Proceedings of the 58th annual meeting of the association for computational linguistics, 4498-4507.
Shazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.; Hinton, G.; and Dean, J. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.
Sprueill, H. W.; Edwards, C.; Olarte, M. V.; Sanyal, U.; Ji, H.; and Choudhury, S. 2023. Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design. arXiv preprint arXiv:2310.14420.
Sun, J.; Xu, C.; Tang, L.; Wang, S.; Lin, C.; Gong, Y.; Shum, H.-Y.; and Guo, J. 2023. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697.</p>
<p>Tang, J.; Zhang, J.; Yao, L.; Li, J.; Zhang, L.; and Su, Z. 2008. ArnetMiner: Extraction and Mining of Academic Social Networks. In $K D D$ '08, 990-998.
Tian, Y.; Song, H.; Wang, Z.; Wang, H.; Hu, Z.; Wang, F.; Chawla, N. V.; and Xu, P. 2023. Graph neural prompting with large language models. arXiv preprint arXiv:2309.15427.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.
Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester, B.; Du, N.; Dai, A. M.; and Le, Q. V. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.
Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao, Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.
Yasunaga, M.; Bosselut, A.; Ren, H.; Zhang, X.; Manning, C. D.; Liang, P. S.; and Leskovec, J. 2022. Deep bidirectional language-knowledge graph pretraining. Advances in Neural Information Processing Systems, 35: 37309-37323.
Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.
Zhou, Y.; Lei, T.; Liu, H.; Du, N.; Huang, Y.; Zhao, V.; Dai, A. M.; Le, Q. V.; Laudon, J.; et al. 2022. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35: 7103-7114.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://www.nlm.nih.gov/research/umls&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://www.aminer.org/citation&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
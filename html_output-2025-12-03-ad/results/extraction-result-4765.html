<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4765 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4765</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4765</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-267412278</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.02563v4.pdf" target="_blank">Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but the associated expensive API cost greatly limits the real application. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing API cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose"Synergy of Thoughts"(SoT) to unleash the synergistic potential of hybrid LLMs with different scales for efficient reasoning. By default, SoT uses smaller-scale language models to generate multiple low-cost intuitive thoughts, which resembles the parallel intuitions produced by System 1. We then design a confidence evaluator where the intuitive thoughts are cross-evaluated and introduce a controllable threshold mechanism to decide their mutual conflict. If these intuitive thoughts exhibit conflicts, SoT will invoke the reflective reasoning of scaled-up language models to emulate the intervention of System 2, which will override the intuitive thoughts and rectify the reasoning results. This framework is model-agnostic and training-free, which can be flexibly implemented with various off-the-shelf LLMs. Experiments on six representative reasoning tasks show that SoT substantially reduces the API cost by 38.3%-75.1%, and simultaneously achieves state-of-the-art reasoning accuracy and solution diversity. Notably, the average token cost reduction on open-ended tasks reaches up to 69.1%.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4765.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4765.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoT (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synergy-of-Thoughts (SoT) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-process, model-agnostic framework that uses multiple smaller (cheap) LLMs to generate diverse intuitive thoughts (System 1), cross-evaluates them, and selectively invokes a single larger LLM (System 2) to reflect and override when confidence is low, trading off accuracy and API cost via a threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hybrid ensemble: multiple smaller LLMs (System 1) + one larger LLM (System 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>System 1: K distinct smaller-scale LLMs (examples in paper: GPT-3.5, PaLM2, Gemini1pro or Mistral-7B, LLaMA-13B, Yi-34B) used in parallel; System 2: a scaled-up LLM (GPT-4 in experiments) used for reflective correction.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Synergy-of-Thoughts (SoT): default-interventionist multi-LLM synergy</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>System 1: generate K independent, diverse intuitive thoughts with distinct smaller LLMs; cross-evaluate each intuition via the same pool of LLMs (each scores others) to produce confidence scores; compare the top confidence to an adjustable threshold ε — if below threshold, invoke System 2 to reflect and produce final output; a progressive threshold increase is optionally applied across reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Six reasoning tasks (close- and open-ended): Game of 24, Logic Grid Puzzle, GSM8K, Trivia Creative Writing, Open-ended QA, Constrained Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Representative set including close-ended math/puzzle problems (Game of 24, GSM8K, Logic Grid Puzzle) and open-ended generation/Evaluation tasks (Trivia Creative Writing, Open-ended QA, Constrained Generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregated: SoT achieves state-of-the-art accuracy across all six tasks; paper reports average gains of +8.6% accuracy (close-ended) compared with best baselines, +5.9% (open-ended) and large token cost reductions: overall API cost reduction 38.3%–75.1% (token cost reduction higher on open-ended tasks; open-ended average token reduction 69.1%). Also reports improved solution diversity (avg +14.3% close-ended, +12.7% open-ended vs best baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Baselines include CoT, CoT(self-consistency variants), Self-refine, ToT, LLM-cascade, SPP, MAD+judge. Example baseline aggregate: ToT/strong baselines generally perform well but at much higher token cost; SoT reported higher accuracy than ToT and others while using substantially fewer tokens (see per-task specifics in other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using diverse intuitions from multiple smaller LLMs plus selective reflection by a single larger LLM yields both higher accuracy and much lower token cost than stronger-single-LLM methods (e.g., ToT), and also increases solution diversity; an adjustable confidence threshold controls the intervention rate and thus the accuracy-cost trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>When the confidence threshold is set to always accept System 1 (ε = 0), SoT becomes pure System 1 and is cost-efficient but inaccurate; when threshold forces always reflecting (ε = 5), SoT reduces to pure System 2 and is accurate but costly. Homogeneous System 1 groups (e.g., 3×PaLM2) showed higher intervention rates and worse cost-savings compared to more diverse hybrid System 1 groups, indicating that lack of diversity in System 1 can increase System 2 usage and reduce efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4765.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4765.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoT_C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SoT (closed-source System1 implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An implementation of SoT where System 1 is instantiated with closed-source smaller LLMs (GPT-3.5, PaLM2, Gemini1pro) and System 2 with GPT-4; used in experiments and reported with per-task metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, PaLM2, Gemini1pro (System 1) + GPT-4 (System 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>System 1: three closed-source smaller/medium LLMs (mixture of GPT-3.5, PaLM2, Gemini1pro) generating diverse candidate thoughts; System 2: GPT-4 used for reflection/intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>SoT (multi-model diverse intuitions + selective reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>K=3 smaller LLMs produce independent initial thoughts; cross-evaluation by the same pool yields confidence scores; if top confidence ≤ ε, invoke GPT-4 to correct; progressive ε increase applied between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Game of 24; Trivia Creative Writing; Logic Grid Puzzle; GSM8K (examples reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See SoT general entry; these are representative close- and open-ended reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported per Table 11: Game of 24 accuracy 76% ±1; Trivia Creative Writing 83.1% ±0.9; Logic Grid Puzzle 71.5% ±0.5; GSM8K 94.0% ±0.5. Paper reports substantial token cost savings (aggregated) and higher solution diversity vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Representative baseline ToT: Game of 24 64%; Trivia 76.8%; Logic Grid Puzzle 66.1%; GSM8K 91.8% (as reported in paper). Simpler CoT and LLM-cascade methods reported substantially lower accuracy on Game of 24 (e.g., CoT best-of-5 14% on Game of 24 in this paper's experimental context) while ToT is more competitive but at much higher token cost.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Closed-source hybrid System1 + GPT-4 System2 SoT_C outperformed all compared baseline methods in accuracy on those tasks while using fewer tokens; hybrid, diverse System1 lowered the empirical intervention rate and token costs compared to homogeneous System1 choices.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Empirical intervention rates vary by System1 composition (see Table 9); some homogeneous System1 groupings (e.g., 3×PaLM2) had higher intervention rates, reducing the cost benefit. Also, accepting System1 outputs unconditionally reduces accuracy considerably.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4765.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4765.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoT_O</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SoT (open-source System1 implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An implementation of SoT where System 1 is instantiated with open-source/smaller LLMs (Mistral-7B, LLaMA-13B, Yi-34B) and System 2 with GPT-4; used to show SoT generality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B, LLaMA-13B, Yi-34B (System 1) + GPT-4 (System 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>System 1: three open-source smaller LLMs (Mistral-7B, LLaMA-13B, Yi-34B) generating diverse candidate thoughts; System 2: GPT-4 used for reflection/intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>SoT (multi-model diverse intuitions + selective reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Same SoT mechanism but instantiated with different smaller LLMs to test generality and trade-offs; cross-evaluation and threshold govern intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Game of 24; Trivia Creative Writing; Logic Grid Puzzle; GSM8K (examples reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Representative reasoning benchmarks, see other entries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported per Table 11 (SoT O row): Game of 24 73% ±2; Trivia Creative Writing 82.2% ±0.7; Logic Grid Puzzle 69.9% ±0.8; GSM8K 93.4% ±0.3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Representative baseline ToT: Game of 24 64%; Trivia 76.8%; Logic Grid Puzzle 66.1%; GSM8K 91.8%. Other baselines (CoT variants, Self-refine, LLM-cascade) show lower accuracy on many tasks, or require more tokens for similar accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SoT instantiated with open-source System1 still outperforms baselines in the accuracy-cost trade-off, demonstrating the framework's flexibility; hybrid diverse System1 contributes to higher solution diversity and lower intervention rates compared to homogeneous System1 choices.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Single-LLM System1 or single-source homogeneous System1 groupings perform worse: (e.g., SoT using 3×same-model System1 shows reduced diversity/accuracy and higher intervention rates), highlighting that diversity across System1 LLMs is an important factor for SoT's efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4765.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4765.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative reasoning framework that generates multiple thought candidates per step and performs tree search over thought sequences (often implemented with a single large LLM), improving accuracy at substantially increased token cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Single large LLM (as used in ToT implementations in literature; in this paper ToT is used as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ToT typically uses a single (often large) LLM to generate multiple candidate thoughts per step and evaluate/search a tree of thought sequences; this yields high-quality solutions but is token-costly.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (multi-path search using one LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>At each reasoning step, the LLM proposes multiple candidate thoughts; a tree search with heuristics evaluates sequences and picks the best global path; diversity arises from multi-path exploration, but all paths are produced by the same model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Game of 24; Trivia Creative Writing; Logic Grid Puzzle; GSM8K (used as baselines in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as SoT comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in paper as baseline: Game of 24 64%; Trivia Creative Writing 76.8%; Logic Grid Puzzle 66.1%; GSM8K 91.8% (per Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>SoT_C/SoT_O outperform ToT on reported tasks while consuming substantially fewer tokens; SoT claims better accuracy-cost trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ToT's multi-path search (single large LLM) gives strong accuracy but at very high token cost; SoT attains equal-or-better accuracy with much lower cost by generating diverse intuitions from multiple cheaper LLMs and selectively invoking the large model only when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>ToT sometimes approaches SoT's accuracy (ToT is competitive) but is much more expensive (e.g., ToT token costs greatly exceed SoT per-task reported token counts); paper reports ToT can consume orders of magnitude more tokens (e.g., Game of 24 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4765.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4765.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-cascade</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-cascade (mixture of weaker and stronger models sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic framework that sequentially queries a series of LLMs, escalating to stronger models when answers from weaker models are unacceptable; in the paper it is discussed and compared as a baseline but noted to be limited because System 1 in LLM-cascade tends to be homogeneous.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language model cascades with mixture of thoughts representations for cost-efficient reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sequential chain of LLMs (weaker → stronger)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A staged architecture where queries are first sent to cheaper/weaker LLMs and only escalate to stronger LLMs if earlier outputs are inconsistent/unacceptable; implementations often use the same model family for weak models.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>LLM-cascade (sequential escalation and consistency checking)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sequentially ask weaker LLM(s) for answers; use a check (e.g., answer consistency) to decide whether to call a stronger model; System 1 here is less diverse (often same-source weak models) compared to SoT's diverse multi-model System 1.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Game of 24; Trivia Creative Writing; Logic Grid Puzzle; GSM8K (baselines in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as SoT comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in paper Table 11: Game of 24 8%; Trivia 65.7%; Logic Grid Puzzle 62.1%; GSM8K 89.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared with SoT (SoT_C/SoT_O), LLM-cascade shows lower accuracy on several tasks and less solution diversity; SoT's use of multiple distinct System1 LLMs (diverse) yields better accuracy/diversity while maintaining cost-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-cascade reduces cost compared to always-using large LLMs but is limited because System1 often comes from a single source and thus less diverse; SoT's deliberate diversity in System1 leads to better accuracy and diversity for similar or lower cost.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>LLM-cascade achieved some cost savings but failed to reach the performance of SoT; in particular on Game of 24 its performance is very low (8%), showing that homogeneous weak-model cascades can fail on some reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4765.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4765.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confidence evaluator (cross-eval + threshold)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-evaluation confidence scorer with threshold control (SoT component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism that has System 1 LLMs cross-evaluate each other's candidate thoughts to compute confidence scores and uses an adjustable threshold (with progressive increase) to decide whether to accept a System 1 output or invoke System 2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>implemented via the same System1 LLMs (each LLM scores other LLMs' outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each of the K System1 LLMs scores each candidate thought; scores are averaged to form V(a_i) in range 0–5; the top-scoring candidate is accepted only if V(top) > ε, otherwise System2 is invoked; ε can be progressively increased across reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cross-evaluation + thresholded intervention</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Diversity is evaluated by cross-scoring: candidate thoughts are scored by other System1 models to produce confidence; threshold ε controls intervention frequency; progressive ε (e.g., start 3.5 and increase 10% per step) reduces bias propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All tasks in SoT experiments (used to modulate intervention rate)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used across the same six benchmarks to decide when to call System2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that increasing ε moves SoT behavior between pure System1 (ε=0) and pure System2 (ε=5); incremental accuracy gains taper beyond ε>3.5 so they used ε=3.5 + 10% progressive increase in experiments. This setting yielded the reported SoT accuracies and cost savings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>When ε=0 (no intervention), accuracy drops to System1-only performance (much lower); when ε=5 (always invoke System2), cost increases to pure System2 levels though accuracy becomes similar to always-reflective baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cross-evaluation provides a practical confidence signal to decide intervention; a mid-range threshold (3.5 in experiments) balances cost and accuracy; progressive thresholding reduces bias propagation across multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>If threshold is too low (always accept System1), results degrade; if threshold too high (always invoke System2), token costs negate the efficiency benefit. Homogeneous System1 populations produce higher intervention rates even with confidence-checking, reducing cost savings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance <em>(Rating: 1)</em></li>
                <li>Large language model cascades with mixture of thoughts representations for cost-efficient reasoning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4765",
    "paper_id": "paper-267412278",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "SoT (general)",
            "name_full": "Synergy-of-Thoughts (SoT) framework",
            "brief_description": "A dual-process, model-agnostic framework that uses multiple smaller (cheap) LLMs to generate diverse intuitive thoughts (System 1), cross-evaluates them, and selectively invokes a single larger LLM (System 2) to reflect and override when confidence is low, trading off accuracy and API cost via a threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Hybrid ensemble: multiple smaller LLMs (System 1) + one larger LLM (System 2)",
            "model_description": "System 1: K distinct smaller-scale LLMs (examples in paper: GPT-3.5, PaLM2, Gemini1pro or Mistral-7B, LLaMA-13B, Yi-34B) used in parallel; System 2: a scaled-up LLM (GPT-4 in experiments) used for reflective correction.",
            "reasoning_method_name": "Synergy-of-Thoughts (SoT): default-interventionist multi-LLM synergy",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "System 1: generate K independent, diverse intuitive thoughts with distinct smaller LLMs; cross-evaluate each intuition via the same pool of LLMs (each scores others) to produce confidence scores; compare the top confidence to an adjustable threshold ε — if below threshold, invoke System 2 to reflect and produce final output; a progressive threshold increase is optionally applied across reasoning steps.",
            "task_name": "Six reasoning tasks (close- and open-ended): Game of 24, Logic Grid Puzzle, GSM8K, Trivia Creative Writing, Open-ended QA, Constrained Generation",
            "task_description": "Representative set including close-ended math/puzzle problems (Game of 24, GSM8K, Logic Grid Puzzle) and open-ended generation/Evaluation tasks (Trivia Creative Writing, Open-ended QA, Constrained Generation).",
            "performance": "Aggregated: SoT achieves state-of-the-art accuracy across all six tasks; paper reports average gains of +8.6% accuracy (close-ended) compared with best baselines, +5.9% (open-ended) and large token cost reductions: overall API cost reduction 38.3%–75.1% (token cost reduction higher on open-ended tasks; open-ended average token reduction 69.1%). Also reports improved solution diversity (avg +14.3% close-ended, +12.7% open-ended vs best baselines).",
            "comparison_with_other_method": true,
            "performance_other_method": "Baselines include CoT, CoT(self-consistency variants), Self-refine, ToT, LLM-cascade, SPP, MAD+judge. Example baseline aggregate: ToT/strong baselines generally perform well but at much higher token cost; SoT reported higher accuracy than ToT and others while using substantially fewer tokens (see per-task specifics in other entries).",
            "key_findings": "Using diverse intuitions from multiple smaller LLMs plus selective reflection by a single larger LLM yields both higher accuracy and much lower token cost than stronger-single-LLM methods (e.g., ToT), and also increases solution diversity; an adjustable confidence threshold controls the intervention rate and thus the accuracy-cost trade-off.",
            "counter_examples_or_negative_results": "When the confidence threshold is set to always accept System 1 (ε = 0), SoT becomes pure System 1 and is cost-efficient but inaccurate; when threshold forces always reflecting (ε = 5), SoT reduces to pure System 2 and is accurate but costly. Homogeneous System 1 groups (e.g., 3×PaLM2) showed higher intervention rates and worse cost-savings compared to more diverse hybrid System 1 groups, indicating that lack of diversity in System 1 can increase System 2 usage and reduce efficiency.",
            "uuid": "e4765.0",
            "source_info": {
                "paper_title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SoT_C",
            "name_full": "SoT (closed-source System1 implementation)",
            "brief_description": "An implementation of SoT where System 1 is instantiated with closed-source smaller LLMs (GPT-3.5, PaLM2, Gemini1pro) and System 2 with GPT-4; used in experiments and reported with per-task metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, PaLM2, Gemini1pro (System 1) + GPT-4 (System 2)",
            "model_description": "System 1: three closed-source smaller/medium LLMs (mixture of GPT-3.5, PaLM2, Gemini1pro) generating diverse candidate thoughts; System 2: GPT-4 used for reflection/intervention.",
            "reasoning_method_name": "SoT (multi-model diverse intuitions + selective reflection)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "K=3 smaller LLMs produce independent initial thoughts; cross-evaluation by the same pool yields confidence scores; if top confidence ≤ ε, invoke GPT-4 to correct; progressive ε increase applied between steps.",
            "task_name": "Game of 24; Trivia Creative Writing; Logic Grid Puzzle; GSM8K (examples reported in paper)",
            "task_description": "See SoT general entry; these are representative close- and open-ended reasoning benchmarks.",
            "performance": "Reported per Table 11: Game of 24 accuracy 76% ±1; Trivia Creative Writing 83.1% ±0.9; Logic Grid Puzzle 71.5% ±0.5; GSM8K 94.0% ±0.5. Paper reports substantial token cost savings (aggregated) and higher solution diversity vs baselines.",
            "comparison_with_other_method": true,
            "performance_other_method": "Representative baseline ToT: Game of 24 64%; Trivia 76.8%; Logic Grid Puzzle 66.1%; GSM8K 91.8% (as reported in paper). Simpler CoT and LLM-cascade methods reported substantially lower accuracy on Game of 24 (e.g., CoT best-of-5 14% on Game of 24 in this paper's experimental context) while ToT is more competitive but at much higher token cost.",
            "key_findings": "Closed-source hybrid System1 + GPT-4 System2 SoT_C outperformed all compared baseline methods in accuracy on those tasks while using fewer tokens; hybrid, diverse System1 lowered the empirical intervention rate and token costs compared to homogeneous System1 choices.",
            "counter_examples_or_negative_results": "Empirical intervention rates vary by System1 composition (see Table 9); some homogeneous System1 groupings (e.g., 3×PaLM2) had higher intervention rates, reducing the cost benefit. Also, accepting System1 outputs unconditionally reduces accuracy considerably.",
            "uuid": "e4765.1",
            "source_info": {
                "paper_title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SoT_O",
            "name_full": "SoT (open-source System1 implementation)",
            "brief_description": "An implementation of SoT where System 1 is instantiated with open-source/smaller LLMs (Mistral-7B, LLaMA-13B, Yi-34B) and System 2 with GPT-4; used to show SoT generality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B, LLaMA-13B, Yi-34B (System 1) + GPT-4 (System 2)",
            "model_description": "System 1: three open-source smaller LLMs (Mistral-7B, LLaMA-13B, Yi-34B) generating diverse candidate thoughts; System 2: GPT-4 used for reflection/intervention.",
            "reasoning_method_name": "SoT (multi-model diverse intuitions + selective reflection)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Same SoT mechanism but instantiated with different smaller LLMs to test generality and trade-offs; cross-evaluation and threshold govern intervention.",
            "task_name": "Game of 24; Trivia Creative Writing; Logic Grid Puzzle; GSM8K (examples reported in paper)",
            "task_description": "Representative reasoning benchmarks, see other entries.",
            "performance": "Reported per Table 11 (SoT O row): Game of 24 73% ±2; Trivia Creative Writing 82.2% ±0.7; Logic Grid Puzzle 69.9% ±0.8; GSM8K 93.4% ±0.3.",
            "comparison_with_other_method": true,
            "performance_other_method": "Representative baseline ToT: Game of 24 64%; Trivia 76.8%; Logic Grid Puzzle 66.1%; GSM8K 91.8%. Other baselines (CoT variants, Self-refine, LLM-cascade) show lower accuracy on many tasks, or require more tokens for similar accuracy.",
            "key_findings": "SoT instantiated with open-source System1 still outperforms baselines in the accuracy-cost trade-off, demonstrating the framework's flexibility; hybrid diverse System1 contributes to higher solution diversity and lower intervention rates compared to homogeneous System1 choices.",
            "counter_examples_or_negative_results": "Single-LLM System1 or single-source homogeneous System1 groupings perform worse: (e.g., SoT using 3×same-model System1 shows reduced diversity/accuracy and higher intervention rates), highlighting that diversity across System1 LLMs is an important factor for SoT's efficiency.",
            "uuid": "e4765.2",
            "source_info": {
                "paper_title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thoughts (ToT)",
            "brief_description": "A deliberative reasoning framework that generates multiple thought candidates per step and performs tree search over thought sequences (often implemented with a single large LLM), improving accuracy at substantially increased token cost.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "use",
            "model_name": "Single large LLM (as used in ToT implementations in literature; in this paper ToT is used as a baseline)",
            "model_description": "ToT typically uses a single (often large) LLM to generate multiple candidate thoughts per step and evaluate/search a tree of thought sequences; this yields high-quality solutions but is token-costly.",
            "reasoning_method_name": "Tree-of-Thoughts (multi-path search using one LLM)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "At each reasoning step, the LLM proposes multiple candidate thoughts; a tree search with heuristics evaluates sequences and picks the best global path; diversity arises from multi-path exploration, but all paths are produced by the same model.",
            "task_name": "Game of 24; Trivia Creative Writing; Logic Grid Puzzle; GSM8K (used as baselines in this paper)",
            "task_description": "Same benchmarks as SoT comparisons.",
            "performance": "Reported in paper as baseline: Game of 24 64%; Trivia Creative Writing 76.8%; Logic Grid Puzzle 66.1%; GSM8K 91.8% (per Table 11).",
            "comparison_with_other_method": true,
            "performance_other_method": "SoT_C/SoT_O outperform ToT on reported tasks while consuming substantially fewer tokens; SoT claims better accuracy-cost trade-off.",
            "key_findings": "ToT's multi-path search (single large LLM) gives strong accuracy but at very high token cost; SoT attains equal-or-better accuracy with much lower cost by generating diverse intuitions from multiple cheaper LLMs and selectively invoking the large model only when needed.",
            "counter_examples_or_negative_results": "ToT sometimes approaches SoT's accuracy (ToT is competitive) but is much more expensive (e.g., ToT token costs greatly exceed SoT per-task reported token counts); paper reports ToT can consume orders of magnitude more tokens (e.g., Game of 24 examples).",
            "uuid": "e4765.3",
            "source_info": {
                "paper_title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM-cascade",
            "name_full": "LLM-cascade (mixture of weaker and stronger models sequence)",
            "brief_description": "A dynamic framework that sequentially queries a series of LLMs, escalating to stronger models when answers from weaker models are unacceptable; in the paper it is discussed and compared as a baseline but noted to be limited because System 1 in LLM-cascade tends to be homogeneous.",
            "citation_title": "Large language model cascades with mixture of thoughts representations for cost-efficient reasoning",
            "mention_or_use": "use",
            "model_name": "Sequential chain of LLMs (weaker → stronger)",
            "model_description": "A staged architecture where queries are first sent to cheaper/weaker LLMs and only escalate to stronger LLMs if earlier outputs are inconsistent/unacceptable; implementations often use the same model family for weak models.",
            "reasoning_method_name": "LLM-cascade (sequential escalation and consistency checking)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Sequentially ask weaker LLM(s) for answers; use a check (e.g., answer consistency) to decide whether to call a stronger model; System 1 here is less diverse (often same-source weak models) compared to SoT's diverse multi-model System 1.",
            "task_name": "Game of 24; Trivia Creative Writing; Logic Grid Puzzle; GSM8K (baselines in paper)",
            "task_description": "Same benchmarks as SoT comparisons.",
            "performance": "Reported in paper Table 11: Game of 24 8%; Trivia 65.7%; Logic Grid Puzzle 62.1%; GSM8K 89.1%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared with SoT (SoT_C/SoT_O), LLM-cascade shows lower accuracy on several tasks and less solution diversity; SoT's use of multiple distinct System1 LLMs (diverse) yields better accuracy/diversity while maintaining cost-efficiency.",
            "key_findings": "LLM-cascade reduces cost compared to always-using large LLMs but is limited because System1 often comes from a single source and thus less diverse; SoT's deliberate diversity in System1 leads to better accuracy and diversity for similar or lower cost.",
            "counter_examples_or_negative_results": "LLM-cascade achieved some cost savings but failed to reach the performance of SoT; in particular on Game of 24 its performance is very low (8%), showing that homogeneous weak-model cascades can fail on some reasoning tasks.",
            "uuid": "e4765.4",
            "source_info": {
                "paper_title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Confidence evaluator (cross-eval + threshold)",
            "name_full": "Cross-evaluation confidence scorer with threshold control (SoT component)",
            "brief_description": "A mechanism that has System 1 LLMs cross-evaluate each other's candidate thoughts to compute confidence scores and uses an adjustable threshold (with progressive increase) to decide whether to accept a System 1 output or invoke System 2.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "implemented via the same System1 LLMs (each LLM scores other LLMs' outputs)",
            "model_description": "Each of the K System1 LLMs scores each candidate thought; scores are averaged to form V(a_i) in range 0–5; the top-scoring candidate is accepted only if V(top) &gt; ε, otherwise System2 is invoked; ε can be progressively increased across reasoning steps.",
            "reasoning_method_name": "Cross-evaluation + thresholded intervention",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Diversity is evaluated by cross-scoring: candidate thoughts are scored by other System1 models to produce confidence; threshold ε controls intervention frequency; progressive ε (e.g., start 3.5 and increase 10% per step) reduces bias propagation.",
            "task_name": "All tasks in SoT experiments (used to modulate intervention rate)",
            "task_description": "Used across the same six benchmarks to decide when to call System2.",
            "performance": "Paper reports that increasing ε moves SoT behavior between pure System1 (ε=0) and pure System2 (ε=5); incremental accuracy gains taper beyond ε&gt;3.5 so they used ε=3.5 + 10% progressive increase in experiments. This setting yielded the reported SoT accuracies and cost savings.",
            "comparison_with_other_method": true,
            "performance_other_method": "When ε=0 (no intervention), accuracy drops to System1-only performance (much lower); when ε=5 (always invoke System2), cost increases to pure System2 levels though accuracy becomes similar to always-reflective baselines.",
            "key_findings": "Cross-evaluation provides a practical confidence signal to decide intervention; a mid-range threshold (3.5 in experiments) balances cost and accuracy; progressive thresholding reduces bias propagation across multi-step reasoning.",
            "counter_examples_or_negative_results": "If threshold is too low (always accept System1), results degrade; if threshold too high (always invoke System2), token costs negate the efficiency benefit. Homogeneous System1 populations produce higher intervention rates even with confidence-checking, reducing cost savings.",
            "uuid": "e4765.5",
            "source_info": {
                "paper_title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
            "rating": 1,
            "sanitized_title": "frugalgpt_how_to_use_large_language_models_while_reducing_cost_and_improving_performance"
        },
        {
            "paper_title": "Large language model cascades with mixture of thoughts representations for cost-efficient reasoning",
            "rating": 2,
            "sanitized_title": "large_language_model_cascades_with_mixture_of_thoughts_representations_for_costefficient_reasoning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        }
    ],
    "cost": 0.017529999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models
24 Aug 2024</p>
<p>Yu Shang 
Tsinghua University
BeijingChina</p>
<p>Yu Li 
Harbin Institute of Technology
ShenzhenChina</p>
<p>Fengli Xu fenglixu@tsinghua.edu.cn 
Tsinghua University
BeijingChina</p>
<p>Yong Li liyong07@tsinghua.edu.cn 
Tsinghua University
BeijingChina</p>
<p>Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models
24 Aug 2024A126BA95203AA11BB0A879F51BA169FFarXiv:2402.02563v4[cs.CL]
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but the associated expensive API cost greatly limits the real application.Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing API cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces.Motivated by the dual process theory of human cognition, we propose "Synergy of Thoughts" (SoT) to unleash the synergistic potential of hybrid LLMs with different scales for efficient reasoning.By default, SoT uses smaller-scale language models to generate multiple lowcost intuitive thoughts, which resembles the parallel intuitions produced by System 1.We then design a confidence evaluator where the intuitive thoughts are cross-evaluated and introduce a controllable threshold mechanism to decide their mutual conflict.If these intuitive thoughts exhibit conflicts, SoT will invoke the reflective reasoning of scaled-up language models to emulate the intervention of System 2, which will override the intuitive thoughts and rectify the reasoning results.This framework is model-agnostic and training-free, which can be flexibly implemented with various off-the-shelf LLMs.Experiments on six representative reasoning tasks show that SoT substantially reduces the API cost by 38.3%∼75.1%,and simultaneously achieves state-of-the-art reasoning accuracy and solution diversity.Notably, the average token cost reduction on open-ended tasks reaches up to 69.1%.</p>
<p>Introduction</p>
<p>Initially conceived for autoregressive text generation, large language models (LLMs), such as GPT (Brown et al. 2020;Radford et al. 2018Radford et al. , 2019) ) and PaLM (Chowdhery et al. 2023), have been shown to exhibit emergent abilities for reasoning tasks as they scale up (Wei et al. 2022a).A recent landmark study reveals LLMs can unlock their reasoning capability by employing "Chain of Thought" (CoT) (Wei et al. 2022a) prompts to produce intermediate steps for reasoning.The later "Tree of Thoughts" (ToT) framework (Yao et al. 2023) further allows LLMs to deliberate on multiple reasoning paths and make high-quality global decisions via tree search.Search methods like ToT are believed to resemble the reflective reasoning mode found in human cognition, offering greater accuracy but at the expense of significantly high token costs paid for API services.For example, finding a solution for "Game of 24" with ToT consumes approximately 100 times more tokens compared to CoT (Yao et al. 2023).Besides, many open-ended real-world problems (Zheng et al. 2023) with considerably larger solution spaces can also lead to high API costs.Consequently, a critical research problem arises for practical LLM reasoning: Can we strike a more effective balance between reasoning accuracy and costs?This is significant for addressing reasoning problems in low-resource scenarios and facilitating the democratization of LLM reasoning.Some previous works (Chen, Zaharia, and Zou 2023;Yue et al. 2023) propose to strategically choose a weaker or stronger LLM for solving the reasoning problem.Despite the cost reduction, these methods are unpromising to obtain higher performance than stronger LLMs, thus only achieving a discounted accuracy-cost trade-off.For better accuracy-cost balance, it's challenging but promising to break down the reasoning process and design a more fine-grained and compact synergy mechanism for hybrid LLMs.</p>
<p>Our study is motivated by human's cognition ability to efficiently tackle complex problems.The prevalent "dual process" theory suggests (Evans 2010) there are two distinct systems in human reasoning: System 1, capable of rapid, preconscious and intuitive responses; and System 2, adept at high-effort, reflective reasoning.Research indicates that everyday decision-making is predominantly governed by System 1 (Evans 2010), providing fast responses with minimum resources through the intuition of associative experiences.Although System 1 makes accurate decisions most of the time, it is also identified as the source of various cognitive biases (Kahneman 2011), rendering it prone to errors if not properly monitored.On the contrary, System 2 can avoid intuitive biases through effortful reflection, which is widely encouraged in critical decision-making (Croskerry 2009).</p>
<p>Human reasoning has been observed to adopt a defaultinterventionist mechanism, which reconciles these two competing systems by firstly using System 1 to generate loweffort default responses, which may be intervened upon with the reflection of high-effort System 2 if the confidence is low.Such mechanisms contribute to simultaneously enhancing both the reasoning accuracy and efficiency of humans.Inspired by this, we propose "Synergy of Thoughts" (SoT) for efficient problem-solving with the synergy of hybrid LLMs (see Figure 1).There are mainly three components in the framework of SoT: System 1, System 2 and a confidence evaluator monitoring the synergy of the two systems.Firstly, to mimic the fast, low-effort System 1, SoT employs several hybrid smaller-scale language models to emulate the intuitions derived from different associative experiences of humans.Secondly, SoT implements the reflective System 2 with a scaled-up LLM which is considered to possess superior reasoning abilities.Thirdly, we design a confidence evaluator to monitor the synergy of the two systems.Specifically, it conducts a cross-evaluation of the intuitive thoughts from System 1, generating a confidence score for each thought.We then introduce a progressively increasing threshold and compare it with the highest confidence score to determine whether there are conflicts between intuitive thoughts.Such a threshold control can also help flexibly adjust the workload of dual systems, delicately modulating the accuracy-cost balance in SoT.Regarding the whole workflow, for each reasoning step, SoT uses System 1 to generate multiple intuitive thoughts at low costs by default.Next, the confidence evaluator receives these intuitive thoughts and produces an intervention signal based on their conflicts.If the intuitive thoughts are accepted, the final reasoning thought is selected as the best intuitive thought, otherwise, the reflective System 2 will be invoked to rectify and override the intuitive thoughts, ensuring faithfulness of the reasoning results.With the above designs, SoT is expected to harness the synergistic potential of different-scale LLMs and deliver both efficient and accurate reasoning.Empirically, we conduct extensive experiments on six complex reasoning tasks, including both close-ended (Game of 24 (Yao et al. 2023), Logic Grid Puzzle (Srivastava et al. 2022), GSM8K (Cobbe et al. 2021)) and open-ended problems (Trivia Creative Writing (Chen et al. 2023), Open-ended QA (Chen et al. 2023), Constrained Generation (Madaan et al. 2023)).The results show that SoT achieves state-of-the-art reasoning accuracy on all six tasks.More importantly, it substantially reduces the API cost by 38.3%∼75.1% compared to the second accurate baselines.Particularly, the token cost reduction in open-ended tasks (69.1% on average) is higher than the close-ended tasks (42.6% on average).Besides, we find SoT can also improve the solution diversity, probably because the default module implemented by hybrid smaller-scale LLMs can access more diverse information sources.Our further analysis investigates the impact of intervention rate on the accuracy-cost balance, showing wide feasible implementations that could lead to a beneficial synergistic effect by using SoT.</p>
<p>In summary, our main contributions are as follows:</p>
<p>• We propose SoT, a novel "dual process" theory-inspired framework that unleashes the synergistic potential of hybrid LLMs for cost-efficient reasoning.The framework is model-agnostic and can be implemented with various LLMs flexibly.</p>
<p>• We present a novel confidence evaluation mechanism for hybrid LLMs via cross-evaluation and threshold control, which can effectively monitor the faithfulness of the reasoning process.</p>
<p>SoT: An Efficient Reasoning Framework with the Synergy of Hybrid LLMs</p>
<p>The evidence suggests that intuition is the dominant basis for real-world decision-making and is often effective; however, it also shows that reliance on intuition can be dangerous and that intervention with high-effort and explicit reasoning is often required, especially when problems have novel features.-Evans et al. (Evans 2010) Motivated by the above default-interventionist theory, we introduce SoT, a framework that adaptively integrates two systems for both cost-efficient and accurate LLM reasoning.The whole framework of SoT is illustrated in Figure 2, taking an open-ended QA problem as an example.Illustrations of more tasks and the complete SoT algorithm are shown in the appendix.Our framework provides a high-level design paradigm, which can have various model implementations in practice.Next, we detail three key components in SoT, including System 1 and System 2, and the designed confidence evaluator for effective synergy of dual systems.</p>
<p>Efficient Intuitive Thought Generation with System 1</p>
<p>System 1 is fast, intuitive, and largely dependent on relevant experiences, which is suitable to be implemented with smaller-scale language models that have not exhibited emergent reasoning abilities.System 1 aims to efficiently draft intuitive thoughts and advance the reasoning process forward.To mimic the diverse intuitions for humans, here we introduce K distinct smaller-scale LLMs to generate diverse intuitive thoughts, denoted as {f Ii (•)|i ∈ {1, 2, ..., K}}.</p>
<p>Given the reasoning problem p, each LLM first proposes an initial thought independently, then we reconcile the competition of diverse intuitive thoughts via interactions to further refine the diverse intuitions.The detailed algorithm of System 1 is shown in the appendix and reasoning with System 1 at step t can be formulated as:
H t = System 1(p t ; a t−1 )(1)
where H t is the set of proposed intuitive thoughts, p t is the prompt of task description at the reasoning step t, a t−1 denotes the thoughts of the last reasoning step.</p>
<p>Reflective Thought Intervention with System 2</p>
<p>System 2 is slow, reflective, and high-effort, which is expected to provide high-quality reasoning.In the framework of SoT, System 2 is introduced to correct the biased intuitive thoughts from System 1 to ensure the quality of reasoning results.To achieve this goal, we suggest implementing System 2 with scaled-up LLMs, because larger-scale LLMs exhibit more powerful reasoning capabilities than smallerscale LLMs, which are promising to rectify the unfaithful intuitive thoughts.Specifically, when intuitive thoughts are low-confidence, System 2 will be invoked for thought intervention.It takes the proposed intuitive thoughts at the current reasoning step as input and produces a rectified result overriding previous intuitive thoughts.Formally, given the prompt for reflective intervention p ref and the best intuitive thought a t at the reasoning step t, the reasoning process with System 2 is formulated as follows:
a t = System 2(p ref ; a t ).
(2)</p>
<p>Confidence Evaluation-based Thought Synergy</p>
<p>Motivated by the synergy paradigm of dual processes of human reasoning (Evans 2006;Kahneman, Frederick et al. 2002;Stanovich 2011), we follow a default-interventionist mechanism to design the synergy framework of two competing systems.It hypothesizes the two competing systems are reconciled by utilizing System 1 to obtain low-effort intuitive responses by default, which may be intervened upon with the reflection of high-effort System 2 when the confidence of intuitions is low.Following this idea, in each reasoning step, SoT prioritizes utilizing System 1 to propose multiple intuitive thoughts H.However, these thoughts might be biased or hallucinated when facing novel and complex problems.When these intuitive thoughts show apparent conflicts, System 2 will be automatically invoked for intervention to rectify the reasoning process.To provide highquality signals, we propose a novel confidence evaluator for hybrid LLMs via cross-evaluation and threshold control.</p>
<p>Confidence scoring with cross-evaluation We firstly leverage the diverse knowledge of hybrid LLMs to comprehensively measure the confidence of intuitive thoughts.</p>
<p>In detail, once System 1 provides K intuitive thoughts H = {a i |i ∈ {1, 2, ..., K}}, the K LLMs in System 1 will conduct a cross-evaluation, where each intuitive thought is scored in turn by each LLM.Denote p eval as the prompt for confidence evaluation, the score of the i-th intuitive thought a i is formulated as:
V (a i ) = j∈{1,2,...,K} f Ij (p eval ; a i ) K (3)
The larger score V (a i ) indicates a more coherent evaluation toward the intuition a i and higher confidence.</p>
<p>Intervention signal generation with threshold control</p>
<p>To obtain an executable evaluation criterion, we then introduce an adjustable threshold value ε.The confidence evaluator will accept the highest confidential intuitive thought
a k = argmax a∈H V (a) if V (a k ) &gt; ε
, otherwise it will reject intuitive thoughts and invoke System 2 to overwrite the thoughts.The intervention signal p is generated according to the following rule:
p = T rue V (a k ) &gt; ε, F alse otherwise. (4)
When the signal is True, System 2 will intervene and override the intuitive thoughts.The working frequency of System 1 and 2 can be easily controlled with varying threshold values.To further enhance the faithfulness of the reasoning pipeline, we progressively uplift the confidence threshold with the accumulated number of System 1-based reasoning steps.This is because the reasoning process with more intuitive thoughts is more likely to be biased, where the accepted threshold of intuitive thoughts should be raised.</p>
<p>Conceptually, SoT has several benefits via the harmonious synergy of two systems: (1) Cost efficiency.Compared with existing advanced reasoning methods purely relying on high-cost System 2, SoT can significantly save token cost by using cost-efficient System 1 to propose intuitive thoughts.(2) Solution diversity.SoT introduces diverse intuitions in System 1 to boost solution diversity, which is especially important for open-ended reasoning problems with huge solution space.(3) Competitive performance.Although SoT introduces intuitive thoughts for reasoning, the defaultinterventionist mechanism can timely prevent the spread of bias and ensure the quality of reasoning results.</p>
<p>Theoretical Computation Cost Analysis</p>
<p>To highlight the cost efficiency of SoT, we conduct a theoretical token cost analysis.For a more concise analysis, we assume that the output token cost of LLMs is proportional to the input token cost, thus do not distinguish input and output token prices.Denote the average API cost of every demonstration example in System 1 and System 2 as C I and C R .</p>
<p>In SoT, the API cost for System 1 with K LLMs consists of three parts.Firstly, the cost of initial thought generation of K LLMs is M KC I .Secondly, the API cost of interaction within K LLMs consumes (K −1)KC I .Thirdly, each LLM will update their own thought based on feedback from other K − 1 LLMs and the cost is (K − 1)KC I .Therefore, the total cost of System 1 in SoT is:
C system1 = (M + 2K − 2)KC I ,(5)
where M denotes the number of demonstrations in the prompt.The cost of the confidence evaluation in SoT comes from the cross-evaluation of K LLMs in System 1, and the total of this part is:
C eval = (K − 1)KC I ,(6)
If the confidence of intuitions is low, System 2 will be invoked with only the highest confidential intuitive thought as input, thus the cost is:
C system2 = C R .(7)
By comparison, the cost of reasoning with only reflective System 2 is:
C ′ = M C R .(8)
Denote the intervention rate as r, when
(1 − r)(C system1 + C eval ) + r(C system1 + C eval + C system2 ) &lt; C ′ is satisfied,
SoT is expected to effectively save API costs and the corresponding condition of r is (taking M = 1 in most cases):
r &lt; 1 − (3K − 2)KC I C R .(9)
In the experiment part, we present a detailed analysis combined with specific cost statistics of used LLMs.</p>
<p>Experiments Experimental Settings</p>
<p>Task setup.We evaluate SoT and compared methods on six representative reasoning tasks including three close-ended tasks (Game of 24 (Yao et al. 2023) • Chain-of-thought (CoT) (Wei et al. 2022b): It firstly proposes to guide LLMs to think step-by-step for reasoning.For a fair comparison, we conduct multiple trials until reaching a similar token cost of our method.For example, the result of CoT (best of 5) is reported as the best performance among five independent trials of CoT.</p>
<p>• Self-refine (Madaan et al. 2023): It iteratively produces self-feedback and refines the results.The maximum refinement round is set as 4.</p>
<p>• Tree-of-thoughts (ToT) (Yao et al. 2023): It generates multiple thought paths and searches the best solution with the heuristics method.We set the candidate number at each step as 5.</p>
<p>• LLM-cascade (Yue et al. 2023): It designs a dynamic reasoning framework with weaker and stronger LLMs controlled by checking the answer consistency of the weak LLM.Different from our method, it only utilizes weaker LLMs from a single source and disentangles the thoughts of weaker and stronger LLMs, limiting the performance upper bound.We follow the implementation of CoT-2D-Vote in the original paper, setting the number of sampling paths as 20 for GPT-3.5 and 3 for GPT-4.59% 1.9 12.88 1.02E+06 SoT (1 Gemini1pro) 62% 1.9 12.12 9.63E+05  (Young et al. 2024).GPT-4 (Achiam et al. 2023) is chosen to implement the intervention with System 2. In SoT C , we implement System 1 with three closed-source and relatively smaller-scale LLMs including GPT-3.5 (Achiam et al. 2023), PaLM2 (Anil et al. 2023) and Gemini1pro (Team et al. 2023).GPT-4 is chosen to implement the intervention with System 2 as before.Besides, it's practicable to implement SoT with other LLMs.We set the threshold value ε as 3.5 and the progressive increasing rate as 10% in the confidence evaluation for all tasks (more choices are analyzed in the experiment part).All LLMs are accessed via APIs and we run all experiments on a CPU machine with 16GB memory.</p>
<p>Main Results</p>
<p>We present the performance comparison of SoT and baselines on six representative reasoning tasks in Table 1, 2 and 3. From the results, we have the following observations:</p>
<p>(1) SoT achieves the best reasoning accuracy with significantly reduced computation cost across different tasks.Broadly, SoT outperforms all compared methods in terms of reasoning accuracy (or quality evaluation with FairEval).For three close-ended reasoning tasks, compared with the best baseline, on average SoT improves 8.6% reasoning accuracy, simultaneously saving 42.6% token costs and 30.0%and 64.5% TFLOPS.We also provide intuitive comparisons of reasoning accuracy and solution diversity versus token costs/TFLOPS on Game of 24 and Trivia Creative Writing in the appendix.Overall, SoT achieves the best trade-off between reasoning performance and cost efficiency.</p>
<p>(2) SoT benefits solution diversity.Except for reasoning accuracy, we also pay attention to the solution diversity of reasoning tasks, which is especially vital for some openended problems.It can be found that solutions generated by SoT possess the highest diversity among all methods on four tested tasks.Specifically, for close-ended and open-ended tasks, SoT achieves 14.3% and 12.7% solution diversity improvement on average compared with the best baseline.This might be attributed to the integration of diverse intuitions in System 1 and the further synergy of dual systems.</p>
<p>(3) SoT achieves superior performance under various implementations.We implement SoT with two versions including both open-source and closed-source LLMs for System 1. From the results, SoT consistently outperforms baselines in terms of the trade-off between reasoning performance and costs.This demonstrates the superiority of our designed framework itself and verifies the flexibility of implementations for SoT.</p>
<p>In-depth Analysis of SoT</p>
<p>Study of model choices.In this part, we explore the impact of choosing different LLMs for the implementation of SoT including both open-source and closed-source LLMs.Here we show the results on Game of 24 and Trivia Creative Writing in Table 4, 5, 6, 7.For each version of SoT, we have tried implementations with each single LLM and hybrid LLMs.</p>
<p>From the results, it can be found that utilizing hybrid LLMs to propose intuitions can benefit both accuracy and solution diversity.This might be attributed to hybrid LLMs' ability to access more diverse information sources.Besides, utilizing more LLMs in System 1 can also harvest performance gain.</p>
<p>Study of the threshold value in confidence evaluation.A key design of SoT is the confidential threshold ε to adjust the workload of System 1 and System 2. In our experiments, we regulate that the confidence score ranges from 0 to 5. We then try 7 different threshold values within this interval for SoT O and present the cost-accuracy trade-off on three tasks in Figure 3. Specifically, ε = 0 means intuitive thoughts from System 1 will always be accepted and SoT degrades to pure System 1, which is cost-efficient but inaccurate.ε = 5 means intuitive thoughts from System 1 will always be overwritten by System 2 and SoT becomes a pure System 2, which is accurate but costly.Observing that the incremental accuracy gain becomes much weaker after ε &gt; 3.5, for simplicity, we set ε = 3.5 in all experiments.Besides, we introduce a progressive threshold-rising strategy where ε increases 10% each time from 3.5 with the accumulation of intuition-based reasoning steps.This further enhances reasoning performance (shown as red dots in Figure 3) and mitigates bias propagation in the reasoning process.</p>
<p>Feasible intervention rate for efficient LLM synergy.</p>
<p>Here we conduct a further study combined with empirical statistics to show the practical options of efficient LLM synergy.In terms of SoT C , the most diverse LLM combination (GPT-3.5/PaLM2/Gemini1pro+ GPT-4) obtains the lowest intervention rate on the whole.We calculate the upper bound of the required intervention rate of SoT C according to Eq.( 9) and token costs shown in the appendix.The required condition is r &lt; 57.2%, which is higher than the empirical value.By comparison, some more homogeneous synergy groups such as 3 PaLM2 + GPT-4 come with higher intervention rates.Similarly, for SoT O implemented with LLaMA-13B/Mistrial-7B/Yi-34B + GPT-4, it's expected to save token costs only if r &lt; 86.3%.Such estimation can be easily generalized to any LLM combination and provides a quick assessment of the feasibility of efficient LLM synergy.</p>
<p>Related Work</p>
<p>Reasoning with LLMs.With the blooming of LLMs, there has been plenty of work utilizing LLMs to address reasoning problems (Wei et al. 2022b;Yao et al. 2023;Wang et al. 2022;Zhou et al. 2022;Zhang et al. 2023).The early method is to add few-shot examples in the prompt and let LLMs answer the target question, i.e., standard IO prompting (Brown et al. 2020).However, the resultant performance is very limited, then some more advanced prompting methods are proposed to facilitate the reasoning ability of LLMs.For example, CoT encourages LLMs to think step-by-step, which can activate their inherent reasoning abilities (Wei et al. 2022b).</p>
<p>ToT further explores multiple different thought paths and searching in the thought tree with heuristics methods (Yao et al. 2023).Besides, there are also other advanced mechanisms introduced to improve LLM reasoning abilities such as reflection (Shinn et al. 2023) and refinement (Madaan et al. 2023).Another line of work to enhance the reasoning ability of LLMs is to develop a multi-LLM collaboration system (Du et al. 2023;Liang et al. 2023;Wang et al. 2023b;Chen, Saha, and Bansal 2023;Sun et al. 2023b;Yin et al. 2023).Although the above methods facilitate LLM reasoning performance, they all improve reasoning performance along with higher API costs.Differently, our method explores the effective synergy of the dual systems for a better balance between reasoning performance and cost efficiency.</p>
<p>Cost-efficient reasoning with LLMs.Efficiency and cost are critical challenges for LLM reasoning due to the involved complex computations.To improve the speed and cost-effectiveness of LLM reasoning, there have been several approaches, such as quantization (Tao et al. 2022) and model pruning (Sun et al. 2023a).Besides, some works focused on how to utilize the paid API efficiently (Chen, Zaharia, and Zou 2023;Šakota, Peyrard, and West 2023).For example, Chen et al. (Chen, Zaharia, and Zou 2023) proposed a framework that sends the query to a series of LLMs sequentially if the answers given by the prior model are considered unacceptable.However, all the above methods need to transform the model itself or introduce external fine-tuned verifiers, bringing additional computation costs.Different from the previous model-side modification, we explore a novel path for reducing reasoning cost via diverse LLM synergy, which is training-free and general purpose.</p>
<p>Conclusion</p>
<p>We introduce SoT, an effective hybrid LLM synergy framework for efficient reasoning without any additional training or fine-tuning.Following the default-interventionist mechanism of human decision-making, SoT can adaptively switch between intuitive and reflective thoughts, thus facilitating a better balance between reasoning performance and computation costs.Extensive experiments on broad reasoning tasks emphasize the superiority and generalizability of our method.Compared with the best baseline, SoT can further enhance reasoning accuracy and solution diversity, simultaneously reducing the API cost by 38.3% ∼ 75.1%.We hope that this work can provide a novel perspective for efficient LLM reasoning with model synergy.</p>
<p>of Mistral-7B are from the official report of Mistral AI4 .The statistics of LLaMA-13B are from Baidu online service platform5 .PaLM2 is cost-free when the work is done.</p>
<p>Model</p>
<p>Input /1M tokens Output /1M tokens GPT-3.5 $1.5 $2 GPT-4 $30 $60 Yi-34B $0.35 $0.35 Gemini1pro $0.5 $1.5 Mistrial-7B $0.25 $0.25 LLaMA-13B $0.28 $0.28 PaLM2 0 0</p>
<p>Table 8: Input and output token prices of used LLMs.</p>
<p>Specific Algorithm of System 1</p>
<p>We show the specific algorithm for implementing System 1 in Algorithm 1.</p>
<p>Specific Algorithm of SoT</p>
<p>We show the specific algorithm of the whole framework of SoT in Algorithm 2.</p>
<p>Empirical Average Intervention Rates</p>
<p>Here we show the empirical average intervention rates of different LLM combinations in System 1 on six tasks in Table 9, for measuring the practicability to maintain costsaving by using SoT.</p>
<p>Supplement of Main Results</p>
<p>Here we supplement the main results about SoT C on Constrained Generation and Open-ended QA tasks in Table 10 and Table 11.The results show that SoT C outperforms all baselines, consistent with the conclusion in the main text.</p>
<p>Statistical Tests of Results</p>
<p>Here we report the reasoning accuracy of SoT with standard error in five independent trials in Table 12.It can be seen that SoT achieves significant performance improvement compared with baselines.</p>
<p>Performance-cost Trade-off Analysis on More Reasoning Tasks</p>
<p>We conduct performance-cost visualization analysis on all reasoning tasks.The results are shown in Figure 4, 5, 6, 7, 8, 9 and 10.Consistent with the results in the main text, SoT achieves the best performance-cost trade-off among all methods.</p>
<p>Illustrations of SoT on More Reasoning Tasks</p>
<p>In the main paper, we illustrate SoT with an example from Open-ended QA tasks.For a better understanding of the scheme of SoT, here we present more cases from some other reasoning tasks in Figure 11, 12, 13 and 14.</p>
<p>Prompts used in SoT</p>
<p>We present the designed prompt templates for the confidence evaluator and intervention with System 2 when implementing SoT, taking Trivia Creative Writing (see Figure 15) and Game of 24 (see Figure 16</p>
<p>Figure 1 :
1
Figure 1: An illustration of dual process theory (a) and the main differences between SoT (b) and prior works (c) (d) (e).SoT is designed following the synergy paradigm of dual processes in human reasoning.</p>
<p>Figure 2 :
2
Figure2: Overview of SoT illustrated with a two-step reasoning problem from the Open-ended QA task (making an outline in the first step and giving the answer in the second step).SoT prioritizes reasoning with default intuitions (System 1).When multiple intuitions are evaluated to be conflictual and low-confidence, SoT will intervene with reflective reasoning (invoking System 2) to override them.</p>
<p>Figure 3 :
3
Figure 3: Reasoning cost-accuracy trade-off under different threshold value choices in SoT O on (a) Game of 24 and (b) Trivia creative writing.The number in the figure means the chosen threshold value.</p>
<p>Figure 4 :
4
Figure 4: The reasoning accuracy, solution diversity versus token costs/TFLOPS on Game of 24 (a) (b) and Trivia Creative Writing (c) (d).SoT achieves a better performance-cost trade-off than all compared methods.</p>
<p>Figure 5 :
5
Figure 5: The reasoning accuracy versus token costs/TFLOPS on Logic Grid Puzzle task.SoT achieves a better performancecost trade-off than all compared methods.</p>
<p>Figure 6 :
6
Figure6: The reasoning accuracy versus token costs/TFLOPS on GSM8K task.SoT achieves a better performance-cost tradeoff than all compared methods.</p>
<p>Table 1 :
1
Results on Game of 24, Trivia Creative Writing, Logic Grid Puzzle and GSM8K tasks.
, Logic Grid Puzzle</p>
<p>Table 2 :
2
Results on Constrained Generation task (FairEval value larger than 50% means results from SoT O are better).</p>
<p>Table 3 :
3
Results on Open-ended Question Answer task (FairEval value larger than 50% means results from SoT O are better).
• SPP (Wang et al. 2023b): It transforms a single LLM intodifferent personas and lets them collaborate to solve rea-soning problems. We use GPT-4 for the implementation.
(Liang et al. 2023te with judgment (MAD+judge)(Liang et al. 2023): It designs a multi-agent debate pipeline with judgment for reasoning problems.We set</p>
<p>Table 4 :
4
Performance of different model choices for SoT O on Game of 24.
MethodsAcc Div Cost TFLOPSSoT (default)76% 2.4 14.42 1.28E+06SoT (3 GPT-3.5)70% 2.2 15.65 1.35E+06SoT (3 PaLM2)67% 2.0 16.93 1.50E+06SoT (3 Gemini1pro) 69% 2.1 15.34 1.28E+06SoT (1 GPT-3.5)67% 2.1 13.73 1.12E+05SoT (1 PaLM2)</p>
<p>Table 5 :
5
Performance of different model choices for SoT C on Game of 24.SoT setup.SoT provides a model-agnostic framework, which is flexible and has various implementations.In our experiments, we try two representative implementations, named SoT O and SoT C .In SoT O , we implement System 1 with three popular open-source and small-scale LLMs including Mistral-7B (Jiang et al. 2023), LLaMA-13B (Touvron et al. 2023) and Yi-34B</p>
<p>Table 6 :
6
TFLOPS.For three open-ended reasoning tasks, compared with the best baseline, on average SoT improves 5.9% reasoning accuracy, simultaneously saving 69.1% token costs Performance of different model choices for SoT O on Trivia Creative Writing.
MethodsAccDiv Cost TFLOPSSoT (default)82.2% 6.5 2.96 1.82E+05SoT (3 LLaMA-13B) 77.1% 5.8 3.31 2.01E+05SoT (3 Mistral-7B) 79.6% 5.6 3.22 1.97E+05SoT (3 Yi-34B)80.3% 5.7 3.56 2.13E+05SoT (1 LLaMA-13B) 75.3% 4.4 3.69 2.16E+06SoT (1 Mistral-7B) 76.2% 4.5 2.88 1.71E+06SoT (1 Yi-34B)77.8% 4.2 3.27 1.96E+06MethodsAccDiv Cost TFLOPSSoT (default)83.1% 6.3 3.41 2.38E+05SoT (3 GPT-3.5)80.9% 5.7 3.94 2.60E+05SoT (3 PaLM2)78.6% 6.0 3.77 2.51E+05SoT (3 Gemini1pro) 80.1% 5.8 3.38 2.32E+05SoT (1 GPT-3.5)78.4% 4.2 3.11 2.11E+05SoT (1 PaLM2)75.9% 4.5 3.32 2.20E+05SoT (1 Gemini1pro) 77.3% 4.5 3.02 2.03E+05</p>
<p>Table 7 :
7
Performance of different model choices for SoT C on Trivia Creative Writing.</p>
<p>Algorithm 1: Algorithm of System 1 Input: Reasoning task description p, thoughts of the last step a, the number of hybrid LLMs K for k ∈ {1, 2, ..., K} do a
(1)(2)(1) k )// Multiple-intuitioninteractionsendendfor k ∈ {1, 2, ..., K} doa (3)h (2) j→k )j∈{1,2,...,K}{k}// Update intuitionsendH = {a (3) 1 , a (3) 2 , ..., a return H(3) K }
k = f Ik (p; a) // Generate initial thoughts end for j ∈ {1, 2, ..., K} do for k ∈ {1, 2, ..., K} \ {j} do h j→k = f Ij (p inter ; a k = f Ik (p update ;</p>
<p>Algorithm 2: Algorithm of SoT Input: Required reasoning steps N , task description prompt in each reasoning step {p 0 , ..., p N } t = 0 // Current reasoning step a t = N one // Intialize current thoughts while t ≤ N do
t = t + 1H t = System 1(p t ; a t−1 ) // Proposeintuitions by System 1p, a t = Conf idence Evaluator(H t )// Confidence evaluationif p thena t = System 2(p ref ; a t )// Intervention withreflective System 2endendreturn a t</p>
<p>Table 9 :
9
) tasks as examples.Empirical average intervention rate on six reasoning tasks.
LLM combinationsGame of 24 Logic Grid Puzzle GSM8K Creative Writing OpenQA Constrained Generation3 GPT-3.528%49%24%51%55%56%3 PaLM233%55%27%57%57%61%3 Gemini1pro30%53%26%50%54%58%GPT-3.5/PaLM2/Gemini1pro26%44%23%42%52%53%3 LLaMA-13B39%68%41%65%67%65%3 Mistral-7B36%61%39%60%61%63%3 Yi-34B36%59%36%61%59%59%LLaMA-13B/Mistral-7B/Yi-34B 33%54%35%49%54%57%8070SoT OSoT CAccuracy(%)10 20 30 40 50 60Self-refine SPPCoT(best of 100) MAD+judgeToTToken cost($) 1.0 2.5 10.0 25.01.21.41.61.8 (a) Diversity2.02.22.41.21.41.61.82.02.22.4</p>
<p>Table 10 :
10
Results on Constrained Generation task of SoT C (FairEval value larger than 50% means results from SoT C are better).
MethodsFairEval Diversity Cost ($) TFLOPSMethodsFairEval Diversity Cost ($) TFLOPSSoT C6.28.325.44E+05SoT C5.26.774.72E+05v.s. CoT(best of 1) 73.0% 4.24.842.91E+05v.s. CoT(best of 1) 68.4% 3.12.271.36E+05v.s. CoT(best of 5) 66.4% 4.220.111.21E+06v.s. CoT(best of 5) 62.8% 3.38.725.23E+05v.s. Self-refine59.2% 5.331.861.91E+06v.s. Self-refine60.5% 4.215.279.16E+05v.s. ToT62.9% 4.758.523.51E+06v.s. ToT62.3% 3.319.441.17E+06v.s. SPP70.7% 4.626.691.60E+06v.s. SPP64.5% 3.88.335.00E+05v.s. MAD+judge 57.6% 5.636.592.20E+06v.s. MAD+judge 58.1% 4.717.001.02E+06</p>
<p>Table 11 :
11
Results on Open-ended Question Answer task (FairEval value larger than 50% means results from SoT C are better).
MethodGame of 24 Trivia Creative Writing Logic Grid Puzzle GSM8KCoT(best of 1)4%67.1%65.8%87.8%CoT(best of 5)14%73.4%67.1%91.3%LLM-cascade8%65.7%62.1%89.1%Self-refine20%78.2%60.6%91.1%ToT64%76.8%66.1%91.8%SPP12%79.9%68.3%84.6%MAD+judge22%77.4%66.8%89.3%SoT O73±2%82.2±0.7%69.9±0.8%93.4±0.3%SoT C76±1%83.1±0.9%71.5±0.5%94.0±0.5%</p>
<p>Table 12 :
12
Performance comparison with standard error on Game of 24, Trivia Creative Writing, Logic Grid Puzzle and GSM8K tasks.</p>
<p>https://openai.com/pricing
https://platform.lingyiwanwu.com/
https://ai.google.dev/pricing
https://mistral.ai/technology
https://console.bce.baidu.com/qianfan/ais/console/onlineService
Appendix Impact StatementsOur work provides a both cost-efficient and highperformance framework for solving reasoning problems with LLMs, which is expected to benefit broad organizations, such as the NLP research community and industrial companies.By designing such a cost-efficient framework, we empower these organizations to harness the reasoning ability of LLMs conveniently, especially for reasoning problems with high complexity.Our work not only makes financial savings but also benefits sustainability development by reducing the carbon emissions brought by extensive computation of running LLMs.Comparison with existing worksIn this section, we review some existing frameworks of LLM reasoning to clarify their difference with our method.We use p, s, and f (•) to denote the reasoning problem, solution and the used LLM respectively.Chain-of-Thought (CoT) CoT enhances LLM reasoning abilities by instructing the model to conduct step-by-step thinking: p → z 1 , • • •, z t → s, where z 1 , • • •, z t are intermediate thoughts during reasoning.In each reasoning step, thoughts are generated from a single LLM, which is formulated as follows:Tree-of-Thoughts (ToT) ToT uses the LLM to deliberate on multiple reasoning paths and make high-quality global decisions via tree search.Formally, at the n-th reasoning step, ToT generates N thought candidates from a single LLM:Then it evaluates all candidate thoughts and selects the best one as the final thought z n at step n.The above two well-known methods can enhance LLM reasoning abilities but are limited to using a single LLM (either small-scale or large-scale), suffering from either low performance or high token cost issues.As for most multiagent debate methods(Liang et al. 2023;Du et al. 2023;Wang et al. 2023b), they are also focusing on reasoning with larger-scale LLMs, resulting in challenges on complex reasoning tasks due to the expensive API costs.To address this issue, we propose an adaptive synergy framework composed of hybrid LLMs, fully exploiting the unique strengths of different-scale LLMs.Token Prices of Used LLMsHere we show the token prices of different LLMs we use in this work in Table8.The statistics of GPT-3.5 and GPT-4 are from the official report of OpenAI 1 .The statistics of Yi-34B are from the official report of 01.AI 2 .The statistics of Gem-ini1pro are from the official report of Google 3 .The statisticsPrompt template for confidence evaluator:Other model has answered several questions.You need to determine whether these answers are correct and then give a score on a scale of 0 to 5. If you think all the answers are correct, give them 5 points.If you think some answers are wrong, reduce the score accordingly.You can't be overconfident, and If you're not sure about the answer, You shouldn't give it a score.You must format the output as in the example, including analysis and score.Hera is an example: questions:1.\"Whowas the target of the failed \"\"Bomb Plot\"\" of 1944?\"2.Who had an 80s No 1 hit with Hold On To The Nights?3.Which musical featured the song The Street Where You Live?4.In what year's Olympics were electric timing devices and a public-address system used for the first time?5.Who was the director of the CIA from 1976-81?Answer:1. Adolf Hitler 2. Richard Marx 3. My Fair Lady 4. 1912 5. Stansfield Turner Analysis: I think the answer to question five is George Bush.I'm not sure answer to question three is correct.So there are three answers that I think are correct.Score: 3 ------------------New questions-----------------------questions:{questions} Answers:{answers}Other model has written a short and coherent passage about {topic} that incorporates the following: {answers} passage:{passage} Please rate the reasonability, relevance, accuracy, coherence of the passage on a scale of 0 to 5. Please write a score directly without explanation.Your output should be of the following format: Score: Give a scorePrompt template for the intervention with System 2: {previous answers}Please modify the answer if you think the possible answer is wrong.Please write your answer to the question directly without explanation.Your output should be of the following format:Answer:Your answer here {previous answers} Write a short and coherent passage about {topic} that incorporates the following: {answers} Remember each words should be mentioned.Your output should be of the following format: Passage: Your passage here.
J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Advances in neural information processing systems. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, 202033</p>
<p>C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, arXiv:2308.07201Chateval: Towards better llmbased evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>AutoAgents: A Framework for Automatic Agent Generation. G Chen, S Dong, Y Shu, G Zhang, J Sesay, B F Karlsson, J Fu, Y Shi, arXiv:2309.172882023arXiv preprint</p>
<p>J C Chen, .-Y Saha, S Bansal, M , arXiv:2309.13007Reconcile: Round-table conference improves reasoning via consensus among diverse llms. 2023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. L Chen, M Zaharia, J Zou, A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2305.05176FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. 2023. 202324arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Clinical cognition and diagnostic error: applications of a dual process model of reasoning. P Croskerry, Advances in health sciences education. 142009</p>
<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>The heuristic-analytic theory of reasoning: Extension and evaluation. J S B Evans, Psychonomic bulletin &amp; review. 1332006</p>
<p>Intuition and reasoning: A dual-process perspective. J S B Evans, Psychological Inquiry. 2142010</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7B. 2023arXiv preprint</p>
<p>Representativeness revisited: Attribute substitution in intuitive judgment. Heuristics and biases: The psychology of intuitive judgment. D ; Kahneman, D Macmillan. Kahneman, S Frederick, 2011. 20024974Thinking, fast and slow</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>R Kirk, I Mediratta, C Nalmpantis, J Luketina, E Hambro, E Grefenstette, R Raileanu, arXiv:2310.06452Understanding the Effects of RLHF on LLM Generalisation and Diversity. 2023arXiv preprint</p>
<p>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. T Liang, Z He, W Jiao, X Wang, Y Wang, R Wang, Y Yang, Z Tu, S Shi, arXiv:2305.191182023arXiv preprint</p>
<p>Self-refine: Iterative refinement with selffeedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>V Padmakumar, H He, arXiv:2309.05196Does Writing with Language Models Reduce Content Diversity?. 2023arXiv preprint</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>M Šakota, M Peyrard, R West, arXiv:2308.06077Fly-swat or cannon? cost-effective language model choice via metamodeling. 2023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K R Narasimhan, S Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>K Stanovich, Rationality and the reflective mind. USAOxford University Press2011</p>
<p>M Sun, Z Liu, A Bair, J Z Kolter, arXiv:2306.11695A Simple and Effective Pruning Approach for Large Language Models. 2023aarXiv preprint</p>
<p>Q Sun, Z Yin, X Li, Z Wu, X Qiu, L Kong, arXiv:2310.00280Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration. 2023barXiv preprint</p>
<p>Compression of generative pretrained language models via quantization. C Tao, L Hou, W Zhang, L Shang, X Jiang, Q Liu, P Luo, N Wong, arXiv:2203.107052022arXiv preprint</p>
<p>G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Large language models are not fair evaluators. P Wang, L Li, L Chen, D Zhu, B Lin, Y Cao, Q Liu, T Liu, Z Sui, arXiv:2305.179262023aarXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. Z Wang, S Mao, W Wu, T Ge, F Wei, H Ji, arXiv:2307.053002023barXiv preprint</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.10601Tree of thoughts: Deliberate problem solving with large language models. 2023arXiv preprint</p>
<p>Exchange-of-thought: Enhancing large language model capabilities through cross-model communication. Z Yin, Q Sun, C Chang, Q Guo, J Dai, X.-J Huang, X Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Yi: Open foundation models by 01. A Young, B Chen, C Li, C Huang, G Zhang, G Zhang, H Li, J Zhu, J Chen, J Chang, arXiv:2403.046522024arXiv preprint</p>
<p>Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. M Yue, J Zhao, M Zhang, L Du, Z Yao, arXiv:2310.030942023arXiv preprint</p>
<p>Y Zhang, J Yang, Y Yuan, A C Yao, -C , arXiv:2308.04371Cumulative reasoning with large language models. 2023arXiv preprint</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, arXiv:2306.05685Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023arXiv preprint</p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q V Le, The Eleventh International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
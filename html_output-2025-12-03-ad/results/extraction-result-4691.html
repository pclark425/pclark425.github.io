<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4691 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4691</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4691</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-4e0bb8c1c683b43357c5d5216f6b74ff2cb32434</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4e0bb8c1c683b43357c5d5216f6b74ff2cb32434" target="_blank">Do ImageNet Classifiers Generalize to ImageNet?</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly "harder" images than those found in the original test sets.</p>
                <p><strong>Paper Abstract:</strong> We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly "harder" images than those found in the original test sets.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4691",
    "paper_id": "paper-4e0bb8c1c683b43357c5d5216f6b74ff2cb32434",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0047115,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do ImageNet Classifiers Generalize to ImageNet?</h1>
<p>Benjamin Recht* Rebecca Roelofs Ludwig Schmidt Vaishaal Shankar<br>UC Berkeley UC Berkeley UC Berkeley UC Berkeley</p>
<h4>Abstract</h4>
<p>We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of $3 \%-15 \%$ on CIFAR- 10 and $11 \%-14 \%$ on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly "harder" images than those found in the original test sets.</p>
<h2>1 Introduction</h2>
<p>The overarching goal of machine learning is to produce models that generalize. We usually quantify generalization by measuring the performance of a model on a held-out test set. What does good performance on the test set then imply? At the very least, one would hope that the model also performs well on a new test set assembled from the same data source by following the same data cleaning protocol.</p>
<p>In this paper, we realize this thought experiment by replicating the dataset creation process for two prominent benchmarks, CIFAR-10 and ImageNet [10, 35]. In contrast to the ideal outcome, we find that a wide range of classification models fail to reach their original accuracy scores. The accuracy drops range from $3 \%$ to $15 \%$ on CIFAR- 10 and $11 \%$ to $14 \%$ on ImageNet. On ImageNet, the accuracy loss amounts to approximately five years of progress in a highly active period of machine learning research.</p>
<p>Conventional wisdom suggests that such drops arise because the models have been adapted to the specific images in the original test sets, e.g., via extensive hyperparameter tuning. However, our experiments show that the relative order of models is almost exactly preserved on our new test sets: the models with highest accuracy on the original test sets are still the models with highest accuracy on the new test sets. Moreover, there are no diminishing returns in accuracy. In fact, every percentage point of accuracy improvement on the original test set translates to a larger improvement on our new test sets. So although later models could have been adapted more to the test set, they see smaller drops in accuracy. These results provide evidence that exhaustive test set evaluations</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>are an effective way to improve image classification models. Adaptivity is therefore an unlikely explanation for the accuracy drops.</p>
<p>Instead, we propose an alternative explanation based on the relative difficulty of the original and new test sets. We demonstrate that it is possible to recover the original ImageNet accuracies almost exactly if we only include the easiest images from our candidate pool. This suggests that the accuracy scores of even the best image classifiers are still highly sensitive to minutiae of the data cleaning process. This brittleness puts claims about human-level performance into context [20, 31, 48]. It also shows that current classifiers still do not generalize reliably even in the benign environment of a carefully controlled reproducibility experiment.</p>
<p>Figure 1 shows the main result of our experiment. Before we describe our methodology in Section 3, the next section provides relevant background. To enable future research, we release both our new test sets and the corresponding code. ${ }^{1}$
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Model accuracy on the original test sets vs. our new test sets. Each data point corresponds to one model in our testbed (shown with $95 \%$ Clopper-Pearson confidence intervals). The plots reveal two main phenomena: (i) There is a significant drop in accuracy from the original to the new test sets. (ii) The model accuracies closely follow a linear function with slope greater than 1 (1.7 for CIFAR-10 and 1.1 for ImageNet). This means that every percentage point of progress on the original test set translates into more than one percentage point on the new test set. The two plots are drawn so that their aspect ratio is the same, i.e., the slopes of the lines are visually comparable. The red shaded region is a $95 \%$ confidence region for the linear fit from 100,000 bootstrap samples.</p>
<h1>2 Potential Causes of Accuracy Drops</h1>
<p>We adopt the standard classification setup and posit the existence of a "true" underlying data distribution $\mathcal{D}$ over labeled examples $(x, y)$. The overall goal in classification is to find a model $\hat{f}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that minimizes the population loss</p>
<p>$$
L_{\mathcal{D}}(\hat{f})=\underset{(x, y) \sim \mathcal{D}}{\mathbb{E}}[\mathbb{I}[\hat{f}(x) \neq y]]
$$</p>
<p>Since we usually do not know the distribution $\mathcal{D}$, we instead measure the performance of a trained classifier via a test set $S$ drawn from the distribution $\mathcal{D}$ :</p>
<p>$$
L_{S}(\hat{f})=\frac{1}{|S|} \sum_{(x, y) \in S} \mathbb{I}[\hat{f}(x) \neq y]
$$</p>
<p>We then use this test error $L_{S}(\hat{f})$ as a proxy for the population loss $L_{\mathcal{D}}(\hat{f})$. If a model $\hat{f}$ achieves a low test error, we assume that it will perform similarly well on future examples from the distribution $\mathcal{D}$. This assumption underlies essentially all empirical evaluations in machine learning since it allows us to argue that the model $\hat{f}$ generalizes.</p>
<p>In our experiments, we test this assumption by collecting a new test set $S^{\prime}$ from a data distribution $\mathcal{D}^{\prime}$ that we carefully control to resemble the original distribution $\mathcal{D}$. Ideally, the original test accuracy $L_{S}(\hat{f})$ and new test accuracy $L_{S^{\prime}}(\hat{f})$ would then match up to the random sampling error. In contrast to this idealized view, our results in Figure 1 show a large drop in accuracy from the original test set $S$ set to our new test set $S^{\prime}$. To understand this accuracy drop in more detail, we decompose the difference between $L_{S}(\hat{f})$ and $L_{S^{\prime}}(\hat{f})$ into three parts (dropping the dependence on $\hat{f}$ to simplify notation):</p>
<p>$$
L_{S}-L_{S^{\prime}}=\underbrace{\left(L_{S}-L_{\mathcal{D}}\right)}<em _mathcal_D="\mathcal{D">{\text {Adaptivity gap }}+\underbrace{\left(L</em>}}-L_{\mathcal{D}^{\prime}}\right)<em _mathcal_D="\mathcal{D">{\text {Distribution Gap }}+\underbrace{\left(L</em>
$$}^{\prime}}-L_{S^{\prime}}\right)}_{\text {Generalization gap }</p>
<p>We now discuss to what extent each of the three terms can lead to accuracy drops.</p>
<p>Generalization Gap. By construction, our new test set $S^{\prime}$ is independent of the existing classifier $\hat{f}$. Hence the third term $L_{\mathcal{D}^{\prime}}-L_{S^{\prime}}$ is the standard generalization gap commonly studied in machine learning. It is determined solely by the random sampling error.
A first guess is that this inherent sampling error suffices to explain the accuracy drops in Figure 1 (e.g., the new test set $S^{\prime}$ could have sampled certain "harder" modes of the distribution $\mathcal{D}$ more often). However, random fluctuations of this magnitude are unlikely for the size of our test sets. With 10,000 data points (as in our new ImageNet test set), a Clopper-Pearson $95 \%$ confidence interval for the test accuracy has size of at most $\pm 1 \%$. Increasing the confidence level to $99.99 \%$ yields a confidence interval of size at most $\pm 2 \%$. Moreover, these confidence intervals become smaller for higher accuracies, which is the relevant regime for the best-performing models. Hence random chance alone cannot explain the accuracy drops observed in our experiments. ${ }^{2}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Adaptivity Gap. We call the term $L_{S}-L_{\mathcal{D}}$ the adaptivity gap. It measures how much adapting the model $\hat{f}$ to the test set $S$ causes the test error $L_{S}$ to underestimate the population loss $L_{\mathcal{D}}$. If we assumed that our model $\hat{f}$ is independent of the test set $S$, this terms would follow the same concentration laws as the generalization gap $L_{\mathcal{D}^{\prime}}-L_{S^{\prime}}$ above. But this assumption is undermined by the common practice of tuning model hyperparameters directly on the test set, which introduces dependencies between the model $\hat{f}$ and the test set $S$. In the extreme case, this can be seen as training directly on the test set. But milder forms of adaptivity may also artificially inflate accuracy scores by increasing the gap between $L_{S}$ and $L_{\mathcal{D}}$ beyond the purely random error.</p>
<p>Distribution Gap. We call the term $L_{\mathcal{D}}-L_{\mathcal{D}^{\prime}}$ the distribution gap. It quantifies how much the change from the original distribution $\mathcal{D}$ to our new distribution $\mathcal{D}^{\prime}$ affects the model $\hat{f}$. Note that this term is not influenced by random effects but quantifies the systematic difference between sampling the original and new test sets. While we went to great lengths to minimize such systematic differences, in practice it is hard to argue whether two high-dimensional distributions are exactly the same. We typically lack a precise definition of either distribution, and collecting a real dataset involves a plethora of design choices.</p>
<h1>2.1 Distinguishing Between the Two Mechanisms</h1>
<p>For a single model $\hat{f}$, it is unclear how to disentangle the adaptivity and distribution gaps. To gain a more nuanced understanding, we measure accuracies for multiple models $\hat{f}<em k="k">{1}, \ldots, \hat{f}</em>$. This provides additional insights because it allows us to determine how the two gaps have evolved over time.
For both CIFAR-10 and ImageNet, the classification models come from a long line of papers that incrementally improved accuracy scores over the past decade. A natural assumption is that later models have experienced more adaptive overfitting since they are the result of more successive hyperparameter tuning on the same test set. Their higher accuracy scores would then come from an increasing adaptivity gap and reflect progress only on the specific examples in the test set $S$ but not on the actual distribution $\mathcal{D}$. In an extreme case, the population accuracies $L_{\mathcal{D}}\left(\hat{f}<em S="S">{i}\right)$ would plateau (or even decrease) while the test accuracies $L</em>}\left(\hat{f<em i="i">{i}\right)$ would continue to grow for successive models $\hat{f}</em>$. However, this idealized scenario is in stark contrast to our results in Figure 1. Later models do not see diminishing returns but an increased advantage over earlier models. Hence we view our results as evidence that the accuracy drops mainly stem from a large distribution gap. After presenting our results in more detail in the next section, we will further discuss this point in Section 5.</p>
<h2>3 Summary of Our Experiments</h2>
<p>We now give an overview of the main steps in our reproducibility experiment. Appendices B and C describe our methodology in more detail. We begin with the first decision, which was to choose informative datasets.</p>
<h3>3.1 Choice of Datasets</h3>
<p>We focus on image classification since it has become the most prominent task in machine learning and underlies a broad range of applications. The cumulative progress on ImageNet is often cited as</p>
<p>one of the main breakthroughs in computer vision and machine learning [42]. State-of-the-art models now surpass human-level accuracy by some measure [20, 48]. This makes it particularly important to check if common image classification models can reliably generalize to new data from the same source.
We decided on CIFAR-10 and ImageNet, two of the most widely-used image classification benchmarks [18]. Both datasets have been the focus of intense research for almost ten years now. Due to the competitive nature of these benchmarks, they are an excellent example for testing whether adaptivity has led to overfitting. In addition to their popularity, their carefully documented dataset creation process makes them well suited for a reproducibility experiment $[10,35,48]$.
Each of the two datasets has specific features that make it especially interesting for our replication study. CIFAR-10 is small enough so that many researchers developed and tested new models for this dataset. In contrast, ImageNet requires significantly more computational resources, and experimenting with new architectures has long been out of reach for many research groups. As a result, CIFAR-10 has likely experienced more hyperparameter tuning, which may also have led to more adaptive overfitting.
On the other hand, the limited size of CIFAR-10 could also make the models more susceptible to small changes in the distribution. Since the CIFAR-10 models are only exposed to a constrained visual environment, they may be unable to learn a robust representation. In contrast, ImageNet captures a much broader variety of images: it contains about $24 \times$ more training images than CIFAR-10 and roughly $100 \times$ more pixels per image. So conventional wisdom (such as the claims of human-level performance) would suggest that ImageNet models also generalize more reliably .
As we will see, neither of these conjectures is supported by our data: CIFAR-10 models do not suffer from more adaptive overfitting, and ImageNet models do not appear to be significantly more robust.</p>
<h1>3.2 Dataset Creation Methodology</h1>
<p>One way to test generalization would be to evaluate existing models on new i.i.d. data from the original test distribution. For example, this would be possible if the original dataset authors had collected a larger initial dataset and randomly split it into two test sets, keeping one of the test sets hidden for several years. Unfortunately, we are not aware of such a setup for CIFAR-10 or ImageNet.
In this paper, we instead mimic the original distribution as closely as possible by repeating the dataset curation process that selected the original test set ${ }^{3}$ from a larger data source. While this introduces the difficulty of disentangling the adaptivity gap from the distribution gap, it also enables us to check whether independent replication affects current accuracy scores. In spite of our efforts, we found that it is astonishingly hard to replicate the test set distributions of CIFAR-10 and ImageNet. At a high level, creating a new test set consists of two parts:</p>
<p>Gathering Data. To obtain images for a new test set, a simple approach would be to use a different dataset, e.g., Open Images [34]. However, each dataset comes with specific biases [54]. For instance, CIFAR-10 and ImageNet were assembled in the late 2000s, and some classes such as car or</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>cell_phone have changed significantly over the past decade. We avoided such biases by drawing new images from the same source as CIFAR-10 and ImageNet. For CIFAR-10, this was the larger Tiny Image dataset [55]. For ImageNet, we followed the original process of utilizing the Flickr image hosting service and only considered images uploaded in a similar time frame as for ImageNet. In addition to the data source and the class distribution, both datasets also have rich structure within each class. For instance, each class in CIFAR-10 consists of images from multiple specific keywords in Tiny Images. Similarly, each class in ImageNet was assembled from the results of multiple queries to the Flickr API. We relied on the documentation of the two datasets to closely match the sub-class distribution as well.</p>
<p>Cleaning Data. Many images in Tiny Images and the Flickr results are only weakly related to the query (or not at all). To obtain a high-quality dataset with correct labels, it is therefore necessary to manually select valid images from the candidate pool. While this step may seem trivial, our results in Section 4 will show that it has major impact on the model accuracies.</p>
<p>The authors of CIFAR-10 relied on paid student labelers to annotate their dataset. The researchers in the ImageNet project utilized Amazon Mechanical Turk (MTurk) to handle the large size of their dataset. We again replicated both annotation processes. Two graduate students authors of this paper impersonated the CIFAR-10 labelers, and we employed MTurk workers for our new ImageNet test set. For both datasets, we also followed the original labeling instructions, MTurk task format, etc.</p>
<p>After collecting a set of correctly labeled images, we sampled our final test sets from the filtered candidate pool. We decided on a test set size of 2,000 for CIFAR-10 and 10,000 for ImageNet. While these are smaller than the original test sets, the sample sizes are still large enough to obtain $95 \%$ confidence intervals of about $\pm 1 \%$. Moreover, our aim was to avoid bias due to CIFAR-10 and ImageNet possibly leaving only "harder" images in the respective data sources. This effect is minimized by building test sets that are small compared to the original datasets (about $3 \%$ of the overall CIFAR-10 dataset and less than $1 \%$ of the overall ImageNet dataset).</p>
<h1>3.3 Results on the New Test Sets</h1>
<p>After assembling our new test sets, we evaluated a broad range of image classification models spanning a decade of machine learning research. The models include the seminal AlexNet [36], widely used convolutional networks [21, 27, 49, 52], and the state-of-the-art [8, 39]. For all deep architectures, we used code previously published online. We relied on pre-trained models whenever possible and otherwise ran the training commands from the respective repositories. In addition, we also evaluated the best-performing approaches preceding convolutional networks on each dataset. These are random features for CIFAR-10 [7, 46] and Fisher vectors for ImageNet [44]. ${ }^{4}$ We wrote our own implementations for these models, which we also release publicly. ${ }^{5}$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>CIFAR-10</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Orig. <br> Rank</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Orig. Accuracy</th>
<th style="text-align: center;">New Accuracy</th>
<th style="text-align: center;">New <br> Gap</th>
<th style="text-align: center;">$\Delta$ Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">autoaug_pyramid_net_tf</td>
<td style="text-align: center;">$98.4[98.1,98.6]$</td>
<td style="text-align: center;">$95.5[94.5,96.4]$</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">shake_shake_64d_cutout</td>
<td style="text-align: center;">$97.1[96.8,97.4]$</td>
<td style="text-align: center;">$93.0[91.8,94.1]$</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">wide_resnet_28_10</td>
<td style="text-align: center;">$95.9[95.5,96.3]$</td>
<td style="text-align: center;">$89.7[88.3,91.0]$</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">resnet_basic_110</td>
<td style="text-align: center;">$93.5[93.0,93.9]$</td>
<td style="text-align: center;">$85.2[83.5,86.7]$</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">vgg_15_BN_64</td>
<td style="text-align: center;">$93.0[92.5,93.5]$</td>
<td style="text-align: center;">$84.9[83.2,86.4]$</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">27</td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">cudaconvnet</td>
<td style="text-align: center;">$88.5[87.9,89.2]$</td>
<td style="text-align: center;">$77.5[75.7,79.3]$</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">random_features_256k_aug</td>
<td style="text-align: center;">$85.6[84.9,86.3]$</td>
<td style="text-align: center;">$73.1[71.1,75.1]$</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">31</td>
</tr>
</tbody>
</table>
<p>ImageNet Top-1</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Orig. <br> Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Orig. Accuracy</th>
<th style="text-align: center;">New Accuracy</th>
<th style="text-align: center;">New <br> Gap</th>
<th style="text-align: center;">$\Delta$ Rank</th>
<th style="text-align: center;">$\Delta$ Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">pnasnet_large_tf</td>
<td style="text-align: center;">$82.9[82.5,83.2]$</td>
<td style="text-align: center;">$72.2[71.3,73.1]$</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">-2</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">nasnetalarge</td>
<td style="text-align: center;">$82.5[82.2,82.8]$</td>
<td style="text-align: center;">$72.2[71.3,73.1]$</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: left;">resnet152</td>
<td style="text-align: center;">$78.3[77.9,78.7]$</td>
<td style="text-align: center;">$67.0[66.1,67.9]$</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: left;">inception_v3_tf</td>
<td style="text-align: center;">$78.0[77.6,78.3]$</td>
<td style="text-align: center;">$66.1[65.1,67.0]$</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">-1</td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: left;">densenet161</td>
<td style="text-align: center;">$77.1[76.8,77.5]$</td>
<td style="text-align: center;">$65.3[64.4,66.2]$</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: left;">vgg19_bn</td>
<td style="text-align: center;">$74.2[73.8,74.6]$</td>
<td style="text-align: center;">$61.9[60.9,62.8]$</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">-1</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: left;">alexnet</td>
<td style="text-align: center;">$56.5[56.1,57.0]$</td>
<td style="text-align: center;">$44.0[43.0,45.0]$</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: left;">fv_64k</td>
<td style="text-align: center;">$35.1[34.7,35.5]$</td>
<td style="text-align: center;">$24.1[23.2,24.9]$</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 1: Model accuracies on the original CIFAR-10 test set, the original ImageNet validation set, and our new test sets. $\Delta$ Rank is the relative difference in the ranking from the original test set to the new test set in the full ordering of all models (see Appendices B.3.3 and C.4.4). For example, $\Delta$ Rank $=-2$ means that a model dropped by two places on the new test set compared to the original test set. The confidence intervals are $95 \%$ Clopper-Pearson intervals. Due to space constraints, references for the models can be found in Appendices B.3.2 and C.4.3.</p>
<p>Overall, the top-1 accuracies range from $83 \%$ to $98 \%$ on the original CIFAR-10 test set and $21 \%$ to $83 \%$ on the original ImageNet validation set. We refer the reader to Appendices C.4.3 and B.3.2 for a full list of models and source repositories.
Figure 1 in the introduction plots original vs. new accuracies, and Table 1 in this section summarizes the numbers of key models. The remaining accuracy scores can be found in Appendices B.3.3 and C.4.4. We now briefly describe the two main trends and discuss the results further in Section 5.</p>
<p>A Significant Drop in Accuracy. All models see a large drop in accuracy from the original test sets to our new test sets. For widely used architectures such as VGG [49] and ResNet [21], the drop is $8 \%$ on CIFAR-10 and $11 \%$ on ImageNet. On CIFAR-10, the state of the art [8] is more robust and only drops by $3 \%$ from $98.4 \%$ to $95.5 \%$. In contrast, the best model on ImageNet [39] sees an $11 \%$ drop from $83 \%$ to $72 \%$ in top- 1 accuracy and a $6 \%$ drop from $96 \%$ to $90 \%$ in top- 5 accuracy. So the top- 1 drop on ImageNet is larger than what we observed on CIFAR-10.</p>
<p>To put these accuracy numbers into perspective, we note that the best model in the ILSVRC 2013 competition achieved $89 \%$ top-5 accuracy, and the best model from ILSVRC 2014 achieved $93 \%$ top-5 accuracy. So the $6 \%$ drop in top-5 accuracy from the 2018 state-of-the-art corresponds to approximately five years of progress in a very active period of machine learning research.</p>
<p>Few Changes in the Relative Order. When sorting the models in order of their original and new accuracy, there are few changes in the respective rankings. Models with comparable original accuracy tend to see a similar decrease in performance. In fact, Figure 1 shows that the original accuracy is highly predictive of the new accuracy and that the relationship can be summarized well with a linear function. On CIFAR-10, the new accuracy of a model is approximately given by the following formula:</p>
<p>$$
\operatorname{acc}<em _orig="{orig" _text="\text">{\text {new }}=1.69 \cdot \operatorname{acc}</em>-72.7 \%
$$}</p>
<p>On ImageNet, the top-1 accuracy of a model is given by</p>
<p>$$
\operatorname{acc}<em _orig="{orig" _text="\text">{\text {new }}=1.11 \cdot \operatorname{acc}</em>-20.2 \%
$$}</p>
<p>Computing a $95 \%$ confidence interval from 100,000 bootstrap samples gives $[1.63,1.76]$ for the slope and $[-78.6,-67.5]$ for the offset on CIFAR-10, and $[1.07,1.19]$ and $[-26.0,-17.8]$ respectively for ImageNet.
On both datasets, the slope of the linear fit is greater than 1. So models with higher original accuracy see a smaller drop on the new test sets. In other words, model robustness improves with increasing accuracy. This effect is less pronounced on ImageNet (slope 1.1) than on CIFAR-10 (slope 1.7). In contrast to a scenario with strong adaptive overfitting, neither dataset sees diminishing returns in accuracy scores when going from the original to the new test sets.</p>
<h1>3.4 Experiments to Test Follow-Up Hypotheses</h1>
<p>Since the drop from original to new accuracies is concerningly large, we investigated multiple hypotheses for explaining this drop. Appendices B. 2 and C. 3 list a range of follow-up experiments we conducted, e.g., re-tuning hyperparameters, training on part of our new test set, or performing cross-validation. However, none of these effects can explain the size of the drop. We conjecture that the accuracy drops stem from small variations in the human annotation process. As we will see in the next section, the resulting changes in the test sets can significantly affect model accuracies.</p>
<h2>4 Understanding the Impact of Data Cleaning on ImageNet</h2>
<p>A crucial aspect of ImageNet is the use of MTurk. There is a broad range of design choices for the MTurk tasks and how the resulting annotations determine the final dataset. To better understand the impact of these design choices, we assembled three different test sets for ImageNet. All of these test sets consist of images from the same Flickr candidate pool, are correctly labeled, and selected by more than $70 \%$ of the MTurk workers on average. Nevertheless, the resulting model accuracies vary by $14 \%$. To put these numbers in context, we first describe our MTurk annotation pipeline.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>MTurk Tasks. We designed our MTurk tasks and user interface to closely resemble those originally used for ImageNet. As in ImageNet, each MTurk task contained a grid of 48 candidate images for a given target class. The task description was derived from the original ImageNet instructions and included the definition of the target class with a link to a corresponding Wikipedia page. We asked the MTurk workers to select images belonging to the target class regardless of "occlusions, other objects, and clutter or text in the scene" and to avoid drawings or paintings (both as in ImageNet). Appendix C.4.1 shows a screenshot of our UI and a screenshot of the original UI for comparison.
For quality control, we embedded at least six randomly selected images from the original validation set in each MTurk task (three from the same class, three from a class that is nearby in the WordNet hierarchy). These images appeared in random locations of the image grid for each task. In total, we collected sufficient MTurk annotations so that we have at least 20 annotated validation images for each class.
The main outcome of the MTurk tasks is a selection frequency for each image, i.e., what fraction of MTurk workers selected the image in a task for its target class. We recruited at least ten MTurk workers for each task (and hence for each image), which is similar to ImageNet. Since each task contained original validation images, we could also estimate how often images from the original dataset were selected by our MTurk workers.</p>
<p>Sampling Strategies. In order to understand how the MTurk selection frequency affects the model accuracies, we explored three sampling strategies.</p>
<ul>
<li>MatchedFrequency: First, we estimated the selection frequency distribution for each class from the annotated original validation images. We then sampled ten images from our candidate pool for each class according to these class-specific distributions (see Appendix C.1.2 for details).</li>
<li>Threshold0.7: For each class, we sampled ten images with selection frequency at least 0.7 .</li>
<li>TopImages: For each class, we chose the ten images with highest selection frequency.</li>
</ul>
<p>In order to minimize labeling errors, we manually reviewed each dataset and removed incorrect images. The average selection frequencies of the three final datasets range from 0.93 for TopImages over 0.85 for Threshold0.7 to 0.73 for MatchedFrequency. For comparison, the original validation set has an average selection frequency of 0.71 in our experiments. Hence all three of our new test sets have higher selection frequencies than the original ImageNet validation set. In the preceding sections, we presented results on MatchedFrequency for ImageNet since it is closest to the validation set in terms of selection frequencies.</p>
<p>Results. Table 2 shows that the MTurk selection frequency has significant impact on both top-1 and top-5 accuracy. In particular, TopImages has the highest average MTurk selection frequency and sees a small increase of about $2 \%$ in both average top-1 and top-5 accuracy compared to the original validation set. This is in stark contrast to MatchedFrequency, which has the lowest average selection frequency and exhibits a significant drop of $12 \%$ and $8 \%$, respectively. The Threshold0.7 dataset is in the middle and sees a small decrease of $3 \%$ in top-1 and $1 \%$ in top-5 accuracy.
In total, going from TopImages to MatchedFrequency decreases the accuracies by about $14 \%$ (top-1) and $10 \%$ (top-5). For comparison, note that after excluding AlexNet (and the SqueezeNet models</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sampling <br> Strategy</th>
<th style="text-align: center;">Average MTurk <br> Selection Freq.</th>
<th style="text-align: center;">Average Top-1 <br> Accuracy Change</th>
<th style="text-align: center;">Average Top-5 <br> Accuracy Change</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MatchedFrequency</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">$-11.8 \%$</td>
<td style="text-align: center;">$-8.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Threshold0.7</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">$-3.2 \%$</td>
<td style="text-align: center;">$-1.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Toplmages</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">$+2.1 \%$</td>
<td style="text-align: center;">$+1.8 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Impact of the three sampling strategies for our ImageNet test sets. The table shows the average MTurk selection frequency in the resulting datasets and the average changes in model accuracy compared to the original validation set. We refer the reader to Section 4 for a description of the three sampling strategies. All three test sets have an average selection frequency of more than 0.7 , yet the model accuracies still vary widely. For comparison, the original ImageNet validation set has an average selection frequency of 0.71 in our MTurk experiments. The changes in average accuracy span $14 \%$ and $10 \%$ in top- 1 and top- 5 , respectively. This shows that details of the sampling strategy have large influence on the resulting accuracies.
tuned to match AlexNet [28]), the range of accuracies spanned by all remaining convolutional networks is roughly $14 \%$ (top-1) and $8 \%$ (top-5). So the variation in accuracy caused by the three sampling strategies is larger than the variation in accuracy among all post-AlexNet models we tested.
Figure 2 plots the new vs. original top-1 accuracies on Threshold0.7 and Toplmages, similar to Figure 1 for MatchedFrequency before. For easy comparison of top-1 and top-5 accuracy plots on all three datasets, we refer the reader to Figure 1 in Appendix C.4.4. All three plots show a good linear fit.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model accuracy on the original ImageNet validation set vs. accuracy on two variants of our new test set. We refer the reader to Section 4 for a description of these test sets. Each data point corresponds to one model in our testbed (shown with $95 \%$ Clopper-Pearson confidence intervals). On Threshold0.7, the model accuracies are $3 \%$ lower than on the original test set. On Toplmages, which contains the images most frequently selected by MTurk workers, the models perform $2 \%$ better than on the original test set. The accuracies on both datasets closely follow a linear function, similar to MatchedFrequency in Figure 1. The red shaded region is a $95 \%$ confidence region for the linear fit from 100,000 bootstrap samples.</p>
<h1>5 Discussion</h1>
<p>We now return to the main question from Section 2: What causes the accuracy drops? As before, we distinguish between two possible mechanisms.</p>
<h3>5.1 Adaptivity Gap</h3>
<p>In its prototypical form, adaptive overfitting would manifest itself in diminishing returns observed on the new test set (see Section 2.1). However, we do not observe this pattern on either CIFAR-10 or ImageNet. On both datasets, the slope of the linear fit is greater than 1, i.e., each point of accuracy improvement on the original test set translates to more than $1 \%$ on the new test set. This is the opposite of the standard overfitting scenario. So at least on CIFAR-10 and ImageNet, multiple years of competitive test set adaptivity did not lead to diminishing accuracy numbers.
While our experiments rule out the most dangerous form of adaptive overfitting, we remark that they do not exclude all variants. For instance, it could be that any test set adaptivity leads to a roughly constant drop in accuracy. Then all models are affected equally and we would see no diminishing returns since later models could still be better. Testing for this form of adaptive overfitting likely requires a new test set that is truly i.i.d. and not the result of a separate data collection effort. Finding a suitable dataset for such an experiment is an interesting direction for future research.
The lack of adaptive overfitting contradicts conventional wisdom in machine learning. We now describe two mechanisms that could have prevented adaptive overfitting:</p>
<p>The Ladder Mechanism. Blum and Hardt introduced the Ladder algorithm to protect machine learning competitions against adaptive overfitting [3]. The core idea is that constrained interaction with the test set can allow a large number of model evaluations to succeed, even if the models are chosen adaptively. Due to the natural form of their algorithm, the authors point out that it can also be seen as a mechanism that the machine learning community implicitly follows.</p>
<p>Limited Model Class. Adaptivity is only a problem if we can choose among models for which the test set accuracy differs significantly from the population accuracy. Importantly, this argument does not rely on the number of all possible models (e.g., all parameter settings of a neural network), but only on those models that could actually be evaluated on the test set. For instance, the standard deep learning workflow only produces models trained with SGD-style algorithms on a fixed training set, and requires that the models achieve high training accuracy (otherwise we would not consider the corresponding hyperparameters). Hence the number of different models arising from the current methodology may be small enough so that uniform convergence holds.
Our experiments offer little evidence for favoring one explanation over the other. One observation is that the convolutional networks shared many errors on CIFAR-10, which could be an indicator that the models are rather similar. But to gain a deeper understanding into adaptive overfitting, it is likely necessary to gather further data from more machine learning benchmarks, especially in scenarios where adaptive overfitting does occur naturally.</p>
<h1>5.2 Distribution Gap</h1>
<p>The lack of diminishing returns in our experiments points towards the distribution gap as the primary reason for the accuracy drops. Moreover, our results on ImageNet show that changes in the sampling strategy can indeed affect model accuracies by a large amount, even if the data source and other parts of the dataset creation process stay the same.
So in spite of our efforts to match the original dataset creation process, the distribution gap is still our leading hypothesis for the accuracy drops. This demonstrates that it is surprisingly hard to accurately replicate the distribution of current image classification datasets. The main difficulty likely is the subjective nature of the human annotation step. There are many parameters that can affect the quality of human labels such as the annotator population (MTurk vs. students, qualifications, location \&amp; time, etc.), the exact task format, and compensation. Moreover, there are no exact definitions for many classes in ImageNet (e.g., see Appendix C.4.8). Understanding these aspects in more detail is an important direction for designing future datasets that contain challenging images while still being labeled correctly.
The difficulty of clearly defining the data distribution, combined with the brittle behavior of the tested models, calls into question whether the black-box and i.i.d. framework of learning can produce reliable classifiers. Our analysis of selection frequencies in Figure 15 (Appendix C.4.7) shows that we could create a new test set with even lower model accuracies. The images in this hypothetical dataset would still be correct, from Flickr, and selected by more than half of the MTurk labelers on average. So in spite of the impressive accuracy scores on the original validation set, current ImageNet models still have difficulty generalizing from "easy" to "hard" images.</p>
<h3>5.3 A Model for the Linear Fit</h3>
<p>Finally, we briefly comment on the striking linear relationship between original and new test accuracies that we observe in all our experiments (for instance, see Figure 1 in the introduction or Figures 12 and 13 in the appendix). To illustrate how this phenomenon could arise, we present a simple data model where a small modification of the data distribution can lead to significant changes in accuracy, yet the relative order of models is preserved as a linear relationship. We emphasize that this model should not be seen as the true explanation. Instead, we hope it can inform future experiments that explore natural variations in test distributions.
First, as we describe in Appendix C.2, we find that we achieve better fits to our data under a probit scaling of the accuracies. Over a wide range from $21 \%$ to $83 \%$ (all models in our ImageNet testbed), the accuracies on the new test set, $\alpha_{\text {new }}$, are related to the accuracies on the original test set, $\alpha_{\text {orig }}$, by the relationship</p>
<p>$$
\Phi^{-1}\left(\alpha_{\text {new }}\right)=u \cdot \Phi^{-1}\left(\alpha_{\text {orig }}\right)+v
$$</p>
<p>where $\Phi$ is the Gaussian CDF, and $u$ and $v$ are scalars. The probit scale is in a sense more natural than a linear scale as the accuracy numbers are probabilities. When we plot accuracies on a probit scale in Figures 6 and 13, we effectively visualize $\Phi^{-1}(\alpha)$ instead of $\alpha$.
We now provide a simple plausible model where the original and new accuracies are related linearly on a probit scale. Assume that every example $i$ has a scalar "difficulty" $\tau_{i} \in \mathbb{R}$ that quantifies how easy it is to classify. Further assume the probability of a model $j$ correctly classifying an image with</p>
<p>difficulty $\tau$ is given by an increasing function $\zeta_{j}(\tau)$. We show that for restricted classes of difficulty functions $\zeta_{j}$, we find a linear relationship between average accuracies after distribution shifts.
To be specific, we focus on the following parameterization. Assume the difficulty distribution of images in a test set follows a normal distribution with mean $\mu$ and variance $\sigma^{2}$. Further assume that</p>
<p>$$
\zeta_{j}(\tau)=\Phi\left(s_{j}-\tau\right)
$$</p>
<p>where $\Phi: \mathbb{R} \rightarrow(0,1)$ is the CDF of a standard normal distribution, and $s_{j}$ is the "skill" of model $j$. Models with higher skill have higher classification accuracy, and images with higher difficulty lead to smaller classification accuracy. Again, the choice of $\Phi$ here is somewhat arbitrary: any sigmoidal function that maps $(-\infty,+\infty)$ to $(0,1)$ is plausible. But using the Gaussian CDF yields a simple calculation illustrating the linear phenomenon.
Using the above notation, the accuracy $\alpha_{j, \mu, \sigma}$ of a model $j$ on a test set with difficulty mean $\mu$ and variance $\sigma$ is then given by</p>
<p>$$
\alpha_{j, \mu, \sigma}=\underset{\tau \sim \mathcal{N}(\mu, \sigma)}{\mathbb{E}}\left[\Phi\left(s_{j}-\tau\right)\right]
$$</p>
<p>We can expand the CDF into an expectation and combine the two expectations by utilizing the fact that a linear combination of two Gaussians is again Gaussian. This yields:</p>
<p>$$
\alpha_{j, \mu, \sigma}=\Phi\left(\frac{s_{j}-\mu}{\sqrt{\sigma^{2}+1}}\right)
$$</p>
<p>On a probit scale, the quantities we plot are given by</p>
<p>$$
\tilde{\alpha}<em _mu_="\mu," _sigma="\sigma" j_="j,">{j, \mu, \sigma}=\Phi^{-1}\left(\alpha</em>
$$}\right)=\frac{s_{j}-\mu}{\sqrt{\sigma^{2}+1}</p>
<p>Next, we consider the case where we have multiple models and two test sets with difficulty parameters $\mu_{k}$ and $\sigma_{k}$ respectively for $k \in{1,2}$. Then $\tilde{\alpha}<em 1="1" j_="j,">{j, 2}$, the probit-scaled accuracy on the second test set, is a linear function of the accuracy on the first test set, $\tilde{\alpha}</em>$ :</p>
<p>$$
\tilde{\alpha}<em 1="1" j_="j,">{j, 2}=u \cdot \tilde{\alpha}</em>+v
$$</p>
<p>with</p>
<p>$$
u=\frac{\sqrt{\sigma_{1}^{2}+1}}{\sqrt{\sigma_{2}^{2}+1}} \quad \text { and } \quad v=\frac{\mu_{1}-\mu_{2}}{\sqrt{\sigma_{2}^{2}+1}}
$$</p>
<p>Hence, we see that the Gaussian difficulty model above yields a linear relationship between original and new test accuracy in the probit domain. While the Gaussian assumptions here made the calculations simple, a variety of different simple classes of $\zeta_{j}$ will give rise to the same linear relationship between the accuracies on two different test sets.</p>
<h1>6 Related Work</h1>
<p>We now briefly discuss related threads in machine learning. To the best of our knowledge, there are no reproducibility experiments directly comparable to ours in the literature.</p>
<p>Dataset Biases. The computer vision community has a rich history of creating new datasets and discussing their relative merits, e.g., $[10,13,15,38,45,48,54,60]$. The paper closest to ours is [54], which studies dataset biases by measuring how models trained on one dataset generalize to other datasets. The main difference to our work is that the authors test generalization across different datasets, where larger changes in the distribution (and hence larger drops in accuracy) are expected. In contrast, our experiments explicitly attempt to reproduce the original data distribution and demonstrate that even small variations arising in this process can lead to significant accuracy drops. Moreover, [54] do not test on previously unseen data, so their experiments cannot rule out adaptive overfitting.</p>
<p>Transfer Learning From ImageNet. Kornblith et al. [33] study how well accuracy on ImageNet transfers to other image classification datasets. An important difference from both our work and [54] is that the the ImageNet models are re-trained on the target datasets. The authors find that better ImageNet models usually perform better on the target dataset as well. Similar to [54], these experiments cannot rule out adaptive overfitting since the authors do not use new data. Moreover, the experiments do not measure accuracy drops due to small variations in the data generating process since the models are evaluated on a different task with an explicit adaptation step. Interestingly, the authors also find an approximately linear relationship between ImageNet and transfer accuracy.</p>
<p>Adversarial Examples. While adversarial examples [2, 50] also show that existing models are brittle, the perturbations have to be finely tuned since models are much more robust to random perturbations. In contrast, our results demonstrate that even small, benign variations in the data sampling process can already lead to a significant accuracy drop without an adversary.
A natural question is whether adversarially robust models are also more robust to the distribution shifts observed in our work. As a first data point, we tested the common $\ell_{\infty}$-robustness baseline from [41] for CIFAR-10. Interestingly, the accuracy numbers of this model fall almost exactly on the linear fit given by the other models in our testbed. Hence $\ell_{\infty}$-robustness does not seem to offer benefits for the distribution shift arising from our reproducibility experiment. However, we note that more forms of adversarial robustness such as spatial transformations or color space changes have been studied [12, 14, 24, 30, 57]. Testing these variants is an interesting direction for future work.</p>
<p>Non-Adversarial Image Perturbations. Recent work also explores less adversarial changes to the input, e.g., [17, 23]. In these papers, the authors modify the ImageNet validation set via well-specified perturbations such as Gaussian noise, a fixed rotation, or adding a synthetic snow-like pattern. Standard ImageNet models then achieve significantly lower accuracy on the perturbed examples than on the unmodified validation set. While this is an interesting test of robustness, the mechanism underlying the accuracy drops is significantly different from our work. The aforementioned papers rely on an intentional, clearly-visible, and well-defined perturbation of existing validation images. Moreover, some of the interventions are quite different from the ImageNet validation set (e.g., ImageNet contains few images of falling snow). In contrast, our experiments use new images and match the distribution of the existing validation set as closely as possible. Hence it is unclear what properties of our new images cause the accuracy drops.</p>
<h1>7 Conclusion \&amp; Future Work</h1>
<p>The expansive growth of machine learning rests on the aspiration to deploy trained systems in a variety of challenging environments. Common examples include autonomous vehicles, content moderation, and medicine. In order to use machine learning in these areas responsibly, it is important that we can both train models with sufficient generalization abilities, and also reliably measure their performance. As our results show, these goals still pose significant hurdles even in a benign environment.</p>
<p>Our experiments are only one step in addressing this reliability challenge. There are multiple promising avenues for future work:</p>
<p>Understanding Adaptive Overfitting. In contrast to conventional wisdom, our experiments show that there are no diminishing returns associated with test set re-use on CIFAR-10 and ImageNet. A more nuanced understanding of this phenomenon will require studying whether other machine learning problems are also resilient to adaptive overfitting. For instance, one direction would be to conduct similar reproducibility experiments on tasks in natural language processing, or to analyze data from competition platforms such as Kaggle and CodaLab. ${ }^{7}$</p>
<p>Characterizing the Distribution Gap. Why do the classification models in our testbed perform worse on the new test sets? The selection frequency experiments in Section 4 suggest that images selected less frequently by the MTurk workers are harder for the models. However, the selection frequency analysis does not explain what aspects of the images make them harder. Candidate hypotheses are object size, special filters applied to the images (black \&amp; white, sepia, etc.), or unusual object poses. Exploring whether there is a succinct description of the difference between the original and new test distributions is an interesting direction for future work.</p>
<p>Learning More Robust Models. An overarching goal is to make classification models more robust to small variations in the data. If the change from the original to our new test sets can be characterized accurately, techniques such as data augmentation or robust optimization may be able to close some of the accuracy gap. Otherwise, one possible approach could be to gather particularly hard examples from Flickr or other data sources and expand the training set this way. However, it may also be necessary to develop entirely novel approaches to image classification.</p>
<p>Measuring Human Accuracy. One interesting question is whether our new test sets are also harder for humans. As a first step in this direction, our human accuracy experiment on CIFAR10 (see Appendix B.2.5) shows that average human performance is not affected significantly by the distribution shift between the original and new images that are most difficult for the models. This suggests that the images are only harder for the trained models and not for humans. But a more comprehensive understanding of the human baseline will require additional human accuracy experiments on both CIFAR-10 and ImageNet.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Building Further Test Sets. The dominant paradigm in machine learning is to evaluate the performance of a classification model on a single test set per benchmark. Our results suggest that this is not comprehensive enough to characterize the reliability of current models. To understand their generalization abilities more accurately, new test data from various sources may be needed. One intriguing question here is whether accuracy on other test sets will also follow a linear function of the original test accuracy.</p>
<p>Suggestions For Future Datasets. We found that it is surprisingly difficult to create a new test set that matches the distribution of an existing dataset. Based on our experience with this process, we provide some suggestions for improving machine learning datasets in the future:</p>
<ul>
<li>Code release. It is hard to fully document the dataset creation process in a paper because it involves a long list of design choices. Hence it would be beneficial for reproducibility efforts if future dataset papers released not only the data but also all code used to create the datasets.</li>
<li>Annotator quality. Our results show that changes in the human annotation process can have significant impact on the difficulty of the resulting datasets. To better understand the quality of human annotations, it would be valuable if authors conducted a standardized test with their annotators (e.g., classifying a common set of images) and included the results in the description of the dataset. Moreover, building variants of the test set with different annotation processes could also shed light on the variability arising from this data cleaning step.</li>
<li>"Super hold-out". Having access to data from the original CIFAR-10 and ImageNet data collection could have clarified the cause of the accuracy drops in our experiments. By keeping an additional test set hidden for multiple years, future benchmarks could explicitly test for adaptive overfitting after a certain time period.</li>
<li>Simpler tasks for humans. The large number of classes and fine distinctions between them make ImageNet a particularly hard problem for humans without special training. While classifying a large variety of objects with fine-grained distinctions is an important research goal, there are also trade-offs. Often it becomes necessary to rely on images with high annotator agreement to ensure correct labels, which in turn leads to bias by excluding harder images. Moreover, the large number of classes causes difficulties when characterizing human performance. So an alternative approach for a dataset could be to choose a task that is simpler for humans in terms of class structure (fewer classes, clear class boundaries), but contains a larger variety of object poses, lighting conditions, occlusions, image corruptions, etc.</li>
<li>Test sets with expert annotations. Compared to building a full training set, a test set requires a smaller number of human annotations. This makes it possible to employ a separate labeling process for the test set that relies on more costly expert annotations. While this violates the assumption that train and test splits are i.i.d. from the same distribution, the expert labels can also increase quality both in terms of correctness and example diversity.
Finally, we emphasize that our recommendations here should not be seen as flaws in CIFAR-10 or ImageNet. Both datasets were assembled in the late 2000s for an accuracy regime that is very different from the state-of-the-art now. Over the past decade, especially ImageNet has successfully guided the field to increasingly better models, thereby clearly demonstrating the immense value of this dataset. But as models have increased in accuracy and our reliability expectations have grown accordingly, it is now time to revisit how we create and utilize datasets in machine learning.</li>
</ul>
<h1>Acknowledgements</h1>
<p>We would like to thank Tudor Achim, Alex Berg, Orianna DeMasi, Jia Deng, Alexei Efros, David Fouhey, Moritz Hardt, Piotr Indyk, Esther Rolf, and Olga Russakovsky for helpful discussions while working on this paper. Moritz Hardt has been particularly helpful in all stages of this project and among other invaluable advice - suggested the title of this paper and a precursor to the data model in Section 5.3. We also thank the participants of our human accuracy experiment in Appendix B.2.5 (whose names we keep anonymous following our IRB protocol).
This research was generously supported in part by ONR awards N00014-17-1-2191, N00014-17-12401, and N00014-18-1-2833, the DARPA Assured Autonomy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs, an Amazon AWS AI Research Award, and a gift from Microsoft Research. In addition, LS was supported by a Google PhD fellowship and a Microsoft Research Fellowship at the Simons Institute for the Theory of Computing.</p>
<h2>References</h2>
<p>[1] Alex Berg. Personal communication, 2018.
[2] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 2018. https://arxiv.org/abs/1712.03141.
[3] Avrim Blum and Moritz Hardt. The Ladder: A reliable leaderboard for machine learning competitions. In International Conference on Machine Learning (ICML), 2015. http://arxiv. org/abs/1502.04585.
[4] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual path networks. In Neural Information Processing Systems (NIPS), 2017. https://arxiv.org/ abs/1707.01629.
[5] Franois Chollet. Xception: Deep learning with depthwise separable convolutions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. https://arxiv.org/abs/1610.0 2357 .
[6] Stephane Clinchant, Gabriela Csurka, Florent Perronnin, and Jean-Michel Renders. XRCE's participation to ImagEval. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1. 102.6670\&amp;rep=rep1\&amp;type=pdf, 2007.
[7] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Conference on Artificial Intelligence and Statistics (AISTATS), 2011. http://proceedings.mlr.press/v15/coates11a.html.
[8] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. AutoAugment: Learning augmentation policies from data. https://arxiv.org/abs/1805.09501, 2018.
[9] Jia Deng. Large Scale Visual Recognition. PhD thesis, Princeton University, 2012. ftp: //ftp.cs.princeton.edu/techreports/2012/923.pdf.
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale</p>
<p>hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), 2009. http://www.image-net.org/papers/imagenet_cvpr09.pdf.
[11] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with Cutout. https://arxiv.org/abs/1708.04552, 2017.
[12] Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation suffice: Fooling CNNs with simple transformations. http://arxiv.org/abs/1712 .02779, 2017.
[13] Mark Everingham, Luc Gool, Christopher K. Williams, John Winn, and Andrew Zisserman. The Pascal Visual Object Classes (VOC) challenge. International Journal of Computer Vision, 2010. http://dx.doi.org/10.1007/s11263-009-0275-4.
[14] Alhussein Fawzi and Pascal Frossard. Manitest: Are classifiers really invariant? In British Machine Vision Conference (BMVC), 2015. https://arxiv.org/abs/1507.06535.
[15] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 2007. http://dx.doi.org/10.1016/j.cviu.2005.09.012.
[16] Xavier Gastaldi. Shake-shake regularization. https://arxiv.org/abs/1705.07485, 2017.
[17] Robert Geirhos, Carlos R. M. Temme, Jonas Rauber, Heiko H. Schtt, Matthias Bethge, and Felix A. Wichmann. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems (NeurIPS). 2018. http://papers.nips.cc/paper/7982-gene ralisation-in-humans-and-deep-neural-networks.pdf.
[18] Ben Hamner. Popular datasets over time. https://www.kaggle.com/benhamner/popular-d atasets-over-time/data, 2017.
[19] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. https://arxiv.org/abs/1610.0 2915 .
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In International Conference on Computer Vision (ICCV), 2015. https://arxiv.org/abs/1502.01852.
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. https://arxiv.org/abs/1512.03385.
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), 2016. https://arxiv.org/ abs/1603.05027.
[23] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations (ICLR), 2019. https://arxiv.org/abs/1807.01697.</p>
<p>[24] Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. In Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2018. https://arxiv.org/ab s/1804.00499.
[25] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. https://arxiv.org/abs/1704.04861, 2017.
[26] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1709.01507.
[27] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. https://arxiv.org/abs/1608.06993.
[28] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and $&lt;0.5 \mathrm{MB}$ model size. https://arxiv.org/abs/1602.07360, 2016.
[29] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015. https://arxiv.org/abs/1502.03167.
[30] Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Geometric robustness of deep networks: Analysis and improvement. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.09115.
[31] Andrej Karpathy. Lessons learned from manually classifying CIFAR-10. http://karpathy.git hub.io/2011/04/27/manually-classifying-cifar10/, 2011.
[32] Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. https://arxiv.org/abs/1710.05468, 2017.
[33] Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better ImageNet models transfer better? http://arxiv.org/abs/1805.08974, 2018.
[34] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Shahab Kamali, Matteo Malloci, Jordi Pont-Tuset, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. https://storage.googleapis.com /openimages/web/index.html, 2017.
[35] Alex Krizhevsky. Learning multiple layers of features from tiny images. https://www.cs.tor onto.edu/ kriz/learning-features-2009-TR.pdf, 2009.
[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012. https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-conv olutional-neural-networks.</p>
<p>[37] Fei-Fei Li and Jia Deng. ImageNet: Where have we been? where are we going? http: //image-net.org/challenges/talks_2017/imagenet_ilsvrc2017_v1.0.pdf, 2017.
[38] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision (ECCV), 2014. https://arxiv.org/abs/1405.0312.
[39] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In European Conference on Computer Vision (ECCV), 2018. https://arxiv.org/abs/1712.00559.
[40] Shuying Liu and Weihong Deng. Very deep convolutional neural network based image classification using small training sample size. In Asian Conference on Pattern Recognition (ACPR), 2015. https://ieeexplore.ieee.org/document/7486599/.
[41] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. https://arxiv.org/abs/1706.06083.
[42] Jitendra Malik. Technical perspective: What led computer vision to deep learning? Communications of the ACM, 2017. http://doi.acm.org/10.1145/3065384.
[43] George A. Miller. Wordnet: A lexical database for english. Communications of the ACM, 1995. URL http://doi.acm.org/10.1145/219717.219748.
[44] Florent Perronnin, Jorge Snchez, and Thomas Mensink. Improving the Fisher kernel for large-scale image classification. In European Conference on Computer Vision (ECCV), 2010. URL https://www.robots.ox.ac.uk/ vgg/rg/papers/peronnin_etal_ECCV10.pdf.
[45] Jean Ponce, Tamara L. Berg, Mark Everingham, David A. Forsyth, Martial Hebert, Sveltana Lazebnik, Marcin Marszalek, Cordelia Schmid, Bryan C. Russell, Antionio Torralba, Chris. K. I. Williams, Jianguo Zhang, and Andrew Zisserman. Dataset issues in object recognition. 2006. https://link.springer.com/chapter/10.1007/11957959_2.
[46] Ali Rahimi and Benjamin Recht. Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning. In Advances in Neural Information Processing Systems (NIPS), 2009. https://papers.nips.cc/paper/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learning.
[47] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image classifier architecture search. http://arxiv.org/abs/1802.01548, 2018.
[48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Fei-Fei Li. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 2015. https://arxiv.org/abs/1409.0575.
[49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. https://arxiv.org/abs/1409.1556, 2014.</p>
<p>[50] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2013. http://arxiv.org/abs/1312.6199.
[51] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015. https://arxiv. org/abs/1409.4842v1.
[52] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception architecture for computer vision. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. https://arxiv.org/abs/1512.00567.
[53] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4, Inception-Resnet and the impact of residual connections on learning. In Conference On Artificial Intelligence (AAAI), 2017. https://arxiv.org/abs/1602.07261.
[54] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In Conference on Computer Vision and Pattern Recognition (CVPR), 2011. http://people.csail.mit.edu/t orralba/publications/datasets_cvpr11.pdf.
[55] Antonio Torralba, Rob Fergus, and William. T. Freeman. 80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008. https://ieeexplore.ieee.org/document/4531741/.
[56] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 2004. http://www.cns.nyu.edu/pub/lcv/wang03-preprint.pdf.
[57] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed adversarial examples. In International Conference on Learning Representations (ICLR), 2018. https://arxiv.org/abs/1801.02612.
[58] Saining Xie, Ross Girshick, Piotr Dollr, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. https://arxiv.org/abs/1611.05431.
[59] Yoshihiro Yamada, Masakazu Iwamura, and Koichi Kise. Shakedrop regularization. https: //arxiv.org/abs/1802.02375, 2018.
[60] Benjamin Z. Yao, Xiong Yang, and Song-Chun Zhu. Introduction to a large-scale general purpose ground truth database: methodology, annotation tool and benchmarks. In Energy Minimization Methods in Computer Vision and Pattern Recognition (EMMCVPR), 2007. https: //link.springer.com/chapter/10.1007/978-3-540-74198-5_14.
[61] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference (BMVC), 2016. https://arxiv.org/abs/1605.07146.
[62] Xingcheng Zhang, Zhizhong Li, Chen Change Loy, and Dahua Lin. Polynet: A pursuit of structural diversity in very deep networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. https://arxiv.org/abs/1611.05725.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://www.kaggle.com/competitions and https://competitions.codalab.org/.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
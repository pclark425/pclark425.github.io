<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8347 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8347</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8347</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271219308</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.11068v1.pdf" target="_blank">Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</a></p>
                <p><strong>Paper Abstract:</strong> The evaluation of Large Language Models (LLMs) often focuses on linguistic tasks, yet such assessments may not fully capture the models’ general reasoning capabilities. We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess broader cognitive functions, particularly in non-linguistic domains. Our approach extends beyond standard linguistic benchmarks by incorporating games like Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess strategic thinking and decision-making. To evaluate the models’ ability to generalize beyond their training data, we introduce two additional games. The first game, LEGO Connect Language (LCL), tests the models’ capacity to understand spatial logic and follow assembly instructions. The second game, the game of shapes, challenges the models to identify shapes represented by 1s within a matrix of zeros, further testing their spatial reasoning skills. This "show, don’t tell" strategy uses games to potentially reveal cognitive capabilities rather than simply querying the models. Our results indicate that despite their proficiency on standard benchmarks and temperature settings, GPT-3.5 and GPT-4’s abilities to play and reason about fully observable games without pre-training is mediocre. Both models fail to anticipate losing moves in Tic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly. While GPT-4 shows some success in the game of shapes, both models struggle with the assembly tasks presented in the LCL game. These results suggest that while LLMs like the GPT models can emulate conversational proficiency and basic rule comprehension, their performance in strategic gameplay and spatial reasoning tasks is limited in cognitive flexibility and generalization. Importantly, this reveals a blind spot in current LLM benchmarks that we highlight with our gameplay benchmark suite ChildPlay (GitHub Repository). Our findings provide a cautionary tale about claims of emergent intelligence and reasoning capabilities of LLMs that are roughly the size of GPT-3.5 and</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8347.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8347.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 Turbo (gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based instruction-following large language model from OpenAI evaluated in zero-shot settings on several ASCII-encoded board and spatial puzzles (Tic-Tac-Toe, Connect-Four, Battleship, LCL, Shapes, Conway's Game of Life).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based chat model by OpenAI; used in experiments in zero-shot mode with temperature hyperparameter varied (t = 0, 0.5, 1, 1.5). Prompted with ASCII board representations and explicit rule descriptions; no chain-of-thought or few-shot examples were used in the main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Tic-Tac-Toe</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based adversarial board game (3x3) requiring local spatial patterning and planning (blocking and creating 3-in-a-row).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot: models receive an ASCII board state and a short rules/instruction prompt; model plays as first player (X) against a RandomPlayer; input/output are row and column scalars ("r c"); temperature swept across t = 0, 0.5, 1, 1.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No Chain-of-Thought prompting in main runs; models could be asked to explain optimal strategies separately (prompted explanations were collected), but gameplay used single-step move outputs. Temperature controls stochasticity; models rely on learned token distributions to produce moves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rates reported by temperature: at t=0.0 win rate ≈ 53%; declines to ≈19% at t=1.5. High number of illegal moves and missed wins/blocks reported; error rate increases with temperature. Compared to minimax baseline (996/1000 wins vs random), performance is far from optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative heatmaps of move frequencies and analyses of missed wins/blocks indicate partial pattern use (center bias) but frequent failure to apply blocking/winning tactics; models often attempted illegal/occupied-square moves indicating weak stateful spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to GPT-4 (same setup) GPT-3.5 performed worse in Tic-Tac-Toe, making more illegal moves and more missed wins/blocks; compared to minimax algorithm, both LLMs underperform markedly. Also compared implicitly to a RandomPlayer baseline; performance sometimes worse than random at higher temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Often plays illegal/occupied squares (automatic losses), fails to block opponent's winning moves, misses winning moves; performance degrades with temperature. Single-character tokenization ('single character token myopia') and ASCII encoding limitations cited as failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8347.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more capable transformer-based instruction-following LLM from OpenAI evaluated on the same ChildPlay suite; shows improved but still limited spatial/strategic abilities across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 variant used in the study; executed in zero-shot gameplay/evaluation with temperatures t = 0, 0.5, 1, 1.5; prompts provide ASCII board state and explicit rules/instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Tic-Tac-Toe</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based adversarial board game (3x3) requiring short-term planning and spatial blocking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot ASCII input; model plays first against RandomPlayer; outputs a move as row and column scalars. Temperatures varied (0 to 1.5).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Models could explain optimal play when separately prompted, but gameplay runs were single-step outputs without CoT; deterministic play at low temperature revealed learned strategy biases (e.g., center/corner choices).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rates: peaked at ≈77% at t = 0.5, dropping to ≈50% at t = 1.5. Still far below optimal minimax baseline (minimax: 996/1000 wins vs random). Errors increase with temperature; many missed wins/blocks recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Heatmaps show move biases (center/middle column in Connect-Four, center choices in Tic-Tac-Toe); however, analysis of missed wins/blocks and illegal moves demonstrates failures to execute correct spatial strategies consistently. GPT-4's explanations of optimal strategies indicate knowledge of rules but not reliable application.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms GPT-3.5 at low temperatures in Tic-Tac-Toe and Connect-Four; underperforms GPT-3.5 in Battleship at several temperatures. Compared to RandomPlayer and minimax baselines, GPT-4 is non-optimal. Authors suggest possible training-data contamination advantage in Shapes game.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to anticipate losing moves or block consistently; produces illegal moves in Battleship (no wins at several temperatures); only produced a small fraction of valid LCL constructs (16 valid constructs out of 400 = 4% in construct generation), indicating poor rule adherence in assembly tasks. Susceptible to training-data contamination giving spurious high performance (Shapes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8347.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Connect-Four</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 Turbo evaluated on Connect-Four</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5's evaluation on Connect-Four showed mixed performance with a central-column bias and temperature-dependent behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above: Transformer-based LLM; zero-shot, ASCII board representation; outputs a single scalar column (0–6) for Connect-Four moves; plays first vs a RandomPlayer; temperature swept.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Connect-Four</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based adversarial game (vertical gravity) requiring column-based spatial reasoning, threat construction and blocking across rows/columns/diagonals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot ASCII prompt with current board; model outputs a column index (single scalar). RandomPlayer chooses uniformly among columns. Temperatures t = 0, 0.5, 1, 1.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No CoT in gameplay; model shows learned central-column bias; decision randomness increased with temperature but in GPT-3.5 incorrect-move probability decreased with temperature for this game (authors note idiosyncrasy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rates: improved from ≈64% at t = 0.0 to ≈76% at t = 0.5 and maintained ≈75% at higher temps. However, many missed wins/blocks indicate non-optimal play.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Heatmaps show persistent center-column preference (even when full), indicating heuristic spatial behavior rather than full lookahead planning; significant missed-block statistics show incomplete spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to GPT-4, GPT-3.5 is more stable at higher temperatures in Connect-Four (GPT-4 declines more). Compared to a RandomPlayer baseline the high win rates at some temperatures may reflect exploitability of uniform random opponent rather than optimal spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to block opponent or complete forced wins consistently; attempts moves in full columns (illegal move handling issue) and shows brittle strategy when opponent deviates from expected distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8347.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Connect-Four</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on Connect-Four</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 performed well at low temperatures in Connect-Four but performance degrades with higher temperature; shows central-column bias and non-optimal play evidenced by missed wins/blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above; outputs single scalar column in zero-shot ASCII setup; temperatures varied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Connect-Four</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based adversarial vertical-stacking board game requiring 2D spatial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot prompts with ASCII state; model plays first vs RandomPlayer; input is column scalar. Temperatures: 0 to 1.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Deterministic low-temperature play reveals learned heuristics (center-first). No explicit tree search or planning used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rate ≈80% at t = 0.0, but falls to ≈39% at t = 1.5. Missed wins/blocks persist; non-optimal compared to an oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Heatmaps and missed-block analysis show that while GPT-4 often selects central columns and forms local strategies, it still fails many forced-block/win opportunities demonstrating incomplete spatial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Beats GPT-3.5 at low temperatures but is less stable as temperature increases; performance likely benefits from opponent randomness rather than true game-theoretic play.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Plays into full columns, fails to block or finish forced wins, and exhibits degraded performance at higher randomness (temperature).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8347.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Battleship</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 Turbo evaluated on Battleship</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 attempted the Battleship guessing game but showed very low win rates and inability to follow placement/guessing rules reliably at higher temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot ASCII board representation for Battleship; ships randomly placed; model outputs row and column as two integers for guesses; plays first versus RandomPlayer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Battleship</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Partially hidden grid-guessing game with spatial ship placement and conditional search (probabilistic spatial inference over a 2D grid).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Models were given the visible board with symbols (S, X, O, ~) and rules; model outputs two integers 'row col' for guesses. Temperatures varied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No explicit search or memory mechanism beyond the prompt; expected heuristics include checkerboard search and focused probing after hits (mentioned as optimal by models when prompted), but not reliably applied in runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-3.5 win rate: ≈10% at t = 0.0, dropping to ≈3% at t = 1.0 and t = 1.5. High loss rate and many incorrect moves reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Models failed to follow Battleship rules reliably; heatmaps show poor, inconsistent search patterns; authors conclude inability to follow Battleship rules across temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>GPT-3.5 performs better than GPT-4 on some Battleship settings (GPT-4 had near-zero wins at several temps). Both underperform compared to expected rule-following search heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Incapable of following Battleship rules reliably; produces incorrect moves and does not perform adjacency-follow-up searches after hits; performance collapses as temperature increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8347.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Battleship</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on Battleship</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 performed poorly in Battleship, failing to win in most temperature settings due to incorrect moves and rule violations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above; zero-shot ASCII Battleship with row/col outputs and temperature sweep.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Battleship</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Partially observable spatial search/logic game over a 2D grid requiring spatial inference and rule adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot prompt includes ship board and guess board; model outputs 'row col'. Random opponent; temperatures varied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Although GPT-4 can describe efficient strategies (checkerboard, focused probes), in gameplay runs it did not reliably enact them; no iterative planning mechanism used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reportedly GPT-4 had zero wins at t = 0.0, 0.5, 1.5 and only ≈4% wins at t = 1.0. Majority of games lost due to incorrect or illegal moves.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Heatmaps and move logs demonstrate failure to conduct coherent spatial search; inability to follow the game's state transitions suggests poor stateful spatial reasoning in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms GPT-3.5 in Battleship in some settings; both models fail dramatically compared to expected rule-following heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequent incorrect moves, inability to follow game rules, no effective adjacency-follow-up after hits; authors conclude models were 'incapable of following Battleship rules.'</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8347.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 LCL (LEGO Connect Language)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo evaluated on LCL (LEGO Connect Language) assembly tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was tested on LCL tasks: validity classification (is a construct valid) and construct generation (produce valid coordinates); it failed to generate any valid constructs and struggled on validity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot prompts with formal LCL 2 rules provided; two experiment types: Validity Testing (models label given tuples as 'valid' or 'invalid') and Construct Generation (models produce a list of tuples for a valid construct). Temperatures varied; 800 validity examples generated (half valid/invalid), models asked to generate 100 constructs per temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO Connect Language (LCL) - validity and construct generation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D assembly/constraint-satisfaction task requiring topological connectivity, non-overlap, layering and discrete spatial placement reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Validity Testing: model receives tuple list and must reply 'valid' or 'invalid' (no justification). Construct Generation: model must output a list of tuples composing a valid assembly given n pieces. Both tasks zero-shot; responses parsed and validated programmatically.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parsing and topological rules provided in prompt; models must apply discrete spatial rules to generate tuples. No CoT; model output parsed and checked automatically by validator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Construct generation: GPT-3.5 produced 0 valid constructs out of 400 attempts (0%). Validity-testing aggregated metrics not fully enumerated in text but models often failed validation checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative examples show GPT-3.5 produced overlapping/pairwise impossible placements, failed to respect layering and connectivity rules, and could not detect impossible requests (e.g., 'triangle with 5 bricks').</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>GPT-3.5 performed substantially worse than GPT-4 (which produced a small number of valid constructs). RandomPlayer baseline not applicable; programmatic validator used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failed to respect non-overlap/connectivity constraints; produced impossible superpositions and ignored layer ordering; inability to detect impossible tasks. Authors note potential single-character tokenization and ASCII representation issues as contributing factors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8347.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 LCL (LEGO Connect Language)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on LCL (LEGO Connect Language) assembly tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 produced a very small fraction of valid LCL constructs and had difficulty consistently applying LCL rules, indicating weak compositional spatial assembly capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot LCL prompts with formal specification; same experimental protocol as GPT-3.5 for validity and construct generation; outputs parsed and validated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO Connect Language (LCL) - validity and construct generation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D discrete assembly requiring connectivity/topological reasoning and non-overlap constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Models asked to validate constructs and to generate valid constructs (tuples of (x,y,'color')). Programmatic validator checked outputs. Temperatures varied; 100 construct generation attempts per temp per model.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No explicit symbolic planner used; model relied on token-level generation to produce coordinate tuples that must satisfy LCL constraints; no CoT prompting in main runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Construct generation: GPT-4 produced 16 valid constructs out of 400 attempts (4%). Validity test accuracy across temperatures shown in figures but summarized as generally poor; only a small fraction of valid outputs produced.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Some successes (e.g., constructing a 3-brick tower and one triangle in examples), but widespread rule violations and impossible placements in many outputs demonstrate inconsistent spatial reasoning and rule adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed GPT-3.5 on construction (GPT-3.5: 0 valid out of 400) but still far from reliable. Authors caution possible contamination or memorization for some successful cases but overall poor generative validity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Often produced overlapping pieces, violated layering/connectivity rules, and failed to detect impossible requirements. Low absolute success rate (≈4%) indicates severe limitations for discrete assembly tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8347.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Shapes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo evaluated on the Game of Shapes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was asked to identify basic geometric shapes encoded as 1s in a 15-sided (15x15?) grid of 0s; performance was near-random for some shapes and poor for squares.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot multiple-choice identification task: model sees ASCII matrix with 1s/0s and is given multiple-choice options (circle, rectangle, triangle, cross) though only three were shown in examples; model outputs choice. Temperatures varied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Game of Shapes</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D pattern recognition on a binary grid (spatial shape recognition requiring perceptual/spatial pattern extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice: ASCII matrix presented along with four shape options; models answer single-choice. Choice positions not randomized intentionally to test order bias. No CoT used.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Models rely on pattern recognition via token-level mapping from ASCII grid to label; no explicit visual embeddings or geometric algorithms used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-3.5 performance approximately equivalent to random chance for triangles and crosses; failed to recognize most squares. Authors present per-shape bar plots; overall accuracy low (not numerically summed in text).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Poor performance implies weak explicit spatial pattern extraction in this format; heatmaps of decisions and linear trends with temperature suggest partial sensitivity to stochasticity but no robust spatial abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>GPT-4 performed substantially better (~80% accuracy) on same task; authors suggest possibility of training-data contamination for GPT-4's strong result.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High error rates, especially for squares; many invalid Unicode outputs at higher temperatures required discarding some GPT-3.5 responses by the parser; susceptibility to tokenization/encoding issues cited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8347.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Shapes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on the Game of Shapes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 achieved substantially better shape-identification performance (~80% accuracy), especially at recognizing triangles, though some high-temperature outputs were invalid and discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot multiple-choice shape identification from ASCII binary grids; temperatures varied; model outputs single-choice label.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Game of Shapes</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D binary-grid shape recognition requiring spatial pattern recognition and abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot ASCII input with 1/0 grid and multiple-choice options; single-response output; temperatures swept.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No CoT used. Likely relying on memorized patterns or learned correspondences between ASCII grids and shape labels; authors note potential data contamination (examples similar in training data).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy ≈80% (authors state ≈80% and particular proficiency at triangles). Some responses at high temperatures were invalid and discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Strong performance relative to GPT-3.5 suggests better pattern recognition; however, authors caution this may be due to training data contamination rather than general spatial reasoning. Heatmaps and per-shape analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Substantially better than GPT-3.5 on this task; authors hypothesize contamination explains the gap. No human baseline reported for this specific binary-grid task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some outputs invalid at higher temperature; potential contamination undermines claim of genuine zero-shot spatial reasoning. Not robustly generalized across all shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8347.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Conway</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo soft experiment on Conway's Game of Life</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Exploratory tests showed GPT-3.5 could identify very simple oscillating patterns but failed at predicting previous/next states in more complex Game of Life sequences, even when rules were given.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models were shown sequences of ASCII Game of Life states and asked to predict prior/next states and/or infer rules. This was a soft experiment (hand-segmented, not included as formal benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Conway's Game of Life (soft experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Cellular automaton state prediction (temporal-spatial reasoning across discrete 2D grids).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Models given 2–3 sequential board states and asked to produce the preceding or following state or infer the survival/birth rules; rule B3/S23 used in experiments. Not part of final benchmark due to hand-segmentation/repeatability issues.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No algorithmic simulation; models must infer transition rules and apply them to produce next/previous grid state from ASCII representation. Some feedback/clues (explicit rules) were used in later trials.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Both GPT-3.5 and GPT-4 succeeded only on simple blinking oscillators; failed to consistently predict more complex evolutions. Quantitative summary: both succeeded only on simple cases (no robust success rate given).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited: success on trivial oscillator patterns shows some pattern extrapolation, but failure on more complex sequences indicates lack of reliable rule-based spatial-temporal simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Both models performed similarly and poorly except on simplest patterns; no formal baseline provided other than ground-truth simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Could not deduce or apply rules in complex scenarios even when rules were given; hand-segmentation and parsing issues limit robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8347.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Conway</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 tested on Conway's Game of Life (soft experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 mirrored GPT-3.5's limited ability: it could handle simple oscillator patterns but failed to reliably predict complex Game of Life states or infer/predict sequences even when provided rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above; exploratory ASCII-sequence-based experiment, not included in formal benchmark because of repeatability concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Conway's Game of Life (soft experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Temporal-spatial cellular automaton requiring application of local transition rules across a 2D grid.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>ASCII states provided; asked to produce prior/next states or infer rules. Some test cases included extra noise and larger patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Model attempts pattern-mapping from seen tokens to next-state tokens; no explicit simulation or iterative planning used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Succeeded only in identifying simple blinking/oscillators; failed on more complex state prediction tasks. No robust numeric success rate reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Partial on trivial patterns but fails to generalize to more complex spatial-temporal dynamics, indicating lack of systematic rule-based simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Comparable to GPT-3.5 on these soft experiments; both inconsistent. Not included in main benchmark due to methodological issues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Invalid/incomplete outputs in several runs; inability to reliably infer or apply transition rules for nontrivial configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8347.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8347.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other models (Claude, Mistral, Gemini, HF models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Other LLMs tested in Appendix G: Anthropic Claude, Mistral, Google Gemini, and various HuggingFace models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Additional models were prompted (e.g., Tic-Tac-Toe) in Appendix G; these models also failed to consistently block winning moves and produced a range of non-answer behaviours, illustrating that the observed failures are not unique to GPT-family models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude; Mistral; Gemini; assorted HF models (phi-2, tiny-llama variants, Falcon-7B-Instruct, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diverse set of proprietary and open models (Anthropic Claude, Mistral, Google Gemini, Microsoft Phi variants, TinyLlama, Falcon-7B-Instruct, etc.) briefly tested on Tic-Tac-Toe and shown to miss blocks or produce invalid/non-game responses; testing informal and illustrative.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Tic-Tac-Toe (illustrative prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3x3 grid adversarial game requiring blocking/win detection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Appendix G: models prompted with Tic-Tac-Toe instructions and played sample games (human vs model examples) to illustrate qualitative failures (missed blocks, non-answers). Not part of main ChildPlay experimental sweep.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Similar zero-shot prompting; varied outputs included incorrect role assignments, non-game text, and code snippets. No consistent planning mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: multiple models missed obvious blocking moves and lost games; no systematic win-rate sweep provided in appendix for these models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Appendix G examples show repeated inability to implement simple blocking, indicating lack of reliable spatial/strategic play across architectures and sizes in these prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>These appendix examples align with main results (GPT-3.5/GPT-4) showing general brittleness; no quantitative head-to-head beyond illustrative sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Non-answer behavior, incorrect role understanding, failure to block winning moves, and a variety of parsing/tokenization issues across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Testing spatial reasoning of large language models: the case of tic-tac-toe <em>(Rating: 2)</em></li>
                <li>Benchmarking large language model (llm) performance for game playing via tic-tac-toe <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>Are emergent abilities of large language models a mirage? <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8347",
    "paper_id": "paper-271219308",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (gpt-3.5-turbo-1106)",
            "name_full": "OpenAI GPT-3.5 Turbo (gpt-3.5-turbo-1106)",
            "brief_description": "A transformer-based instruction-following large language model from OpenAI evaluated in zero-shot settings on several ASCII-encoded board and spatial puzzles (Tic-Tac-Toe, Connect-Four, Battleship, LCL, Shapes, Conway's Game of Life).",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_description": "Transformer-based chat model by OpenAI; used in experiments in zero-shot mode with temperature hyperparameter varied (t = 0, 0.5, 1, 1.5). Prompted with ASCII board representations and explicit rule descriptions; no chain-of-thought or few-shot examples were used in the main experiments.",
            "model_size": null,
            "puzzle_name": "Tic-Tac-Toe",
            "puzzle_type": "Grid-based adversarial board game (3x3) requiring local spatial patterning and planning (blocking and creating 3-in-a-row).",
            "task_setup": "Zero-shot: models receive an ASCII board state and a short rules/instruction prompt; model plays as first player (X) against a RandomPlayer; input/output are row and column scalars (\"r c\"); temperature swept across t = 0, 0.5, 1, 1.5.",
            "mechanisms_or_strategies": "No Chain-of-Thought prompting in main runs; models could be asked to explain optimal strategies separately (prompted explanations were collected), but gameplay used single-step move outputs. Temperature controls stochasticity; models rely on learned token distributions to produce moves.",
            "performance_metrics": "Win rates reported by temperature: at t=0.0 win rate ≈ 53%; declines to ≈19% at t=1.5. High number of illegal moves and missed wins/blocks reported; error rate increases with temperature. Compared to minimax baseline (996/1000 wins vs random), performance is far from optimal.",
            "evidence_of_spatial_reasoning": "Qualitative heatmaps of move frequencies and analyses of missed wins/blocks indicate partial pattern use (center bias) but frequent failure to apply blocking/winning tactics; models often attempted illegal/occupied-square moves indicating weak stateful spatial reasoning.",
            "comparisons": "Compared to GPT-4 (same setup) GPT-3.5 performed worse in Tic-Tac-Toe, making more illegal moves and more missed wins/blocks; compared to minimax algorithm, both LLMs underperform markedly. Also compared implicitly to a RandomPlayer baseline; performance sometimes worse than random at higher temperatures.",
            "limitations_or_failure_cases": "Often plays illegal/occupied squares (automatic losses), fails to block opponent's winning moves, misses winning moves; performance degrades with temperature. Single-character tokenization ('single character token myopia') and ASCII encoding limitations cited as failure modes.",
            "uuid": "e8347.0",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 (gpt-4-1106-preview)",
            "name_full": "OpenAI GPT-4 (gpt-4-1106-preview)",
            "brief_description": "A more capable transformer-based instruction-following LLM from OpenAI evaluated on the same ChildPlay suite; shows improved but still limited spatial/strategic abilities across tasks.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview",
            "model_description": "OpenAI's GPT-4 variant used in the study; executed in zero-shot gameplay/evaluation with temperatures t = 0, 0.5, 1, 1.5; prompts provide ASCII board state and explicit rules/instructions.",
            "model_size": null,
            "puzzle_name": "Tic-Tac-Toe",
            "puzzle_type": "Grid-based adversarial board game (3x3) requiring short-term planning and spatial blocking.",
            "task_setup": "Zero-shot ASCII input; model plays first against RandomPlayer; outputs a move as row and column scalars. Temperatures varied (0 to 1.5).",
            "mechanisms_or_strategies": "Models could explain optimal play when separately prompted, but gameplay runs were single-step outputs without CoT; deterministic play at low temperature revealed learned strategy biases (e.g., center/corner choices).",
            "performance_metrics": "Win rates: peaked at ≈77% at t = 0.5, dropping to ≈50% at t = 1.5. Still far below optimal minimax baseline (minimax: 996/1000 wins vs random). Errors increase with temperature; many missed wins/blocks recorded.",
            "evidence_of_spatial_reasoning": "Heatmaps show move biases (center/middle column in Connect-Four, center choices in Tic-Tac-Toe); however, analysis of missed wins/blocks and illegal moves demonstrates failures to execute correct spatial strategies consistently. GPT-4's explanations of optimal strategies indicate knowledge of rules but not reliable application.",
            "comparisons": "Outperforms GPT-3.5 at low temperatures in Tic-Tac-Toe and Connect-Four; underperforms GPT-3.5 in Battleship at several temperatures. Compared to RandomPlayer and minimax baselines, GPT-4 is non-optimal. Authors suggest possible training-data contamination advantage in Shapes game.",
            "limitations_or_failure_cases": "Fails to anticipate losing moves or block consistently; produces illegal moves in Battleship (no wins at several temperatures); only produced a small fraction of valid LCL constructs (16 valid constructs out of 400 = 4% in construct generation), indicating poor rule adherence in assembly tasks. Susceptible to training-data contamination giving spurious high performance (Shapes).",
            "uuid": "e8347.1",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5 Connect-Four",
            "name_full": "OpenAI GPT-3.5 Turbo evaluated on Connect-Four",
            "brief_description": "GPT-3.5's evaluation on Connect-Four showed mixed performance with a central-column bias and temperature-dependent behaviour.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_description": "As above: Transformer-based LLM; zero-shot, ASCII board representation; outputs a single scalar column (0–6) for Connect-Four moves; plays first vs a RandomPlayer; temperature swept.",
            "model_size": null,
            "puzzle_name": "Connect-Four",
            "puzzle_type": "Grid-based adversarial game (vertical gravity) requiring column-based spatial reasoning, threat construction and blocking across rows/columns/diagonals.",
            "task_setup": "Zero-shot ASCII prompt with current board; model outputs a column index (single scalar). RandomPlayer chooses uniformly among columns. Temperatures t = 0, 0.5, 1, 1.5.",
            "mechanisms_or_strategies": "No CoT in gameplay; model shows learned central-column bias; decision randomness increased with temperature but in GPT-3.5 incorrect-move probability decreased with temperature for this game (authors note idiosyncrasy).",
            "performance_metrics": "Win rates: improved from ≈64% at t = 0.0 to ≈76% at t = 0.5 and maintained ≈75% at higher temps. However, many missed wins/blocks indicate non-optimal play.",
            "evidence_of_spatial_reasoning": "Heatmaps show persistent center-column preference (even when full), indicating heuristic spatial behavior rather than full lookahead planning; significant missed-block statistics show incomplete spatial reasoning.",
            "comparisons": "Compared to GPT-4, GPT-3.5 is more stable at higher temperatures in Connect-Four (GPT-4 declines more). Compared to a RandomPlayer baseline the high win rates at some temperatures may reflect exploitability of uniform random opponent rather than optimal spatial reasoning.",
            "limitations_or_failure_cases": "Fails to block opponent or complete forced wins consistently; attempts moves in full columns (illegal move handling issue) and shows brittle strategy when opponent deviates from expected distributions.",
            "uuid": "e8347.2",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 Connect-Four",
            "name_full": "OpenAI GPT-4 evaluated on Connect-Four",
            "brief_description": "GPT-4 performed well at low temperatures in Connect-Four but performance degrades with higher temperature; shows central-column bias and non-optimal play evidenced by missed wins/blocks.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview",
            "model_description": "As above; outputs single scalar column in zero-shot ASCII setup; temperatures varied.",
            "model_size": null,
            "puzzle_name": "Connect-Four",
            "puzzle_type": "Grid-based adversarial vertical-stacking board game requiring 2D spatial planning.",
            "task_setup": "Zero-shot prompts with ASCII state; model plays first vs RandomPlayer; input is column scalar. Temperatures: 0 to 1.5.",
            "mechanisms_or_strategies": "Deterministic low-temperature play reveals learned heuristics (center-first). No explicit tree search or planning used.",
            "performance_metrics": "Win rate ≈80% at t = 0.0, but falls to ≈39% at t = 1.5. Missed wins/blocks persist; non-optimal compared to an oracle.",
            "evidence_of_spatial_reasoning": "Heatmaps and missed-block analysis show that while GPT-4 often selects central columns and forms local strategies, it still fails many forced-block/win opportunities demonstrating incomplete spatial planning.",
            "comparisons": "Beats GPT-3.5 at low temperatures but is less stable as temperature increases; performance likely benefits from opponent randomness rather than true game-theoretic play.",
            "limitations_or_failure_cases": "Plays into full columns, fails to block or finish forced wins, and exhibits degraded performance at higher randomness (temperature).",
            "uuid": "e8347.3",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5 Battleship",
            "name_full": "OpenAI GPT-3.5 Turbo evaluated on Battleship",
            "brief_description": "GPT-3.5 attempted the Battleship guessing game but showed very low win rates and inability to follow placement/guessing rules reliably at higher temperatures.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_description": "Zero-shot ASCII board representation for Battleship; ships randomly placed; model outputs row and column as two integers for guesses; plays first versus RandomPlayer.",
            "model_size": null,
            "puzzle_name": "Battleship",
            "puzzle_type": "Partially hidden grid-guessing game with spatial ship placement and conditional search (probabilistic spatial inference over a 2D grid).",
            "task_setup": "Models were given the visible board with symbols (S, X, O, ~) and rules; model outputs two integers 'row col' for guesses. Temperatures varied.",
            "mechanisms_or_strategies": "No explicit search or memory mechanism beyond the prompt; expected heuristics include checkerboard search and focused probing after hits (mentioned as optimal by models when prompted), but not reliably applied in runs.",
            "performance_metrics": "GPT-3.5 win rate: ≈10% at t = 0.0, dropping to ≈3% at t = 1.0 and t = 1.5. High loss rate and many incorrect moves reported.",
            "evidence_of_spatial_reasoning": "Models failed to follow Battleship rules reliably; heatmaps show poor, inconsistent search patterns; authors conclude inability to follow Battleship rules across temperatures.",
            "comparisons": "GPT-3.5 performs better than GPT-4 on some Battleship settings (GPT-4 had near-zero wins at several temps). Both underperform compared to expected rule-following search heuristics.",
            "limitations_or_failure_cases": "Incapable of following Battleship rules reliably; produces incorrect moves and does not perform adjacency-follow-up searches after hits; performance collapses as temperature increases.",
            "uuid": "e8347.4",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 Battleship",
            "name_full": "OpenAI GPT-4 evaluated on Battleship",
            "brief_description": "GPT-4 performed poorly in Battleship, failing to win in most temperature settings due to incorrect moves and rule violations.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview",
            "model_description": "As above; zero-shot ASCII Battleship with row/col outputs and temperature sweep.",
            "model_size": null,
            "puzzle_name": "Battleship",
            "puzzle_type": "Partially observable spatial search/logic game over a 2D grid requiring spatial inference and rule adherence.",
            "task_setup": "Zero-shot prompt includes ship board and guess board; model outputs 'row col'. Random opponent; temperatures varied.",
            "mechanisms_or_strategies": "Although GPT-4 can describe efficient strategies (checkerboard, focused probes), in gameplay runs it did not reliably enact them; no iterative planning mechanism used.",
            "performance_metrics": "Reportedly GPT-4 had zero wins at t = 0.0, 0.5, 1.5 and only ≈4% wins at t = 1.0. Majority of games lost due to incorrect or illegal moves.",
            "evidence_of_spatial_reasoning": "Heatmaps and move logs demonstrate failure to conduct coherent spatial search; inability to follow the game's state transitions suggests poor stateful spatial reasoning in this setup.",
            "comparisons": "Underperforms GPT-3.5 in Battleship in some settings; both models fail dramatically compared to expected rule-following heuristics.",
            "limitations_or_failure_cases": "Frequent incorrect moves, inability to follow game rules, no effective adjacency-follow-up after hits; authors conclude models were 'incapable of following Battleship rules.'",
            "uuid": "e8347.5",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5 LCL (LEGO Connect Language)",
            "name_full": "GPT-3.5 Turbo evaluated on LCL (LEGO Connect Language) assembly tasks",
            "brief_description": "GPT-3.5 was tested on LCL tasks: validity classification (is a construct valid) and construct generation (produce valid coordinates); it failed to generate any valid constructs and struggled on validity judgments.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_description": "Zero-shot prompts with formal LCL 2 rules provided; two experiment types: Validity Testing (models label given tuples as 'valid' or 'invalid') and Construct Generation (models produce a list of tuples for a valid construct). Temperatures varied; 800 validity examples generated (half valid/invalid), models asked to generate 100 constructs per temperature.",
            "model_size": null,
            "puzzle_name": "LEGO Connect Language (LCL) - validity and construct generation",
            "puzzle_type": "2D assembly/constraint-satisfaction task requiring topological connectivity, non-overlap, layering and discrete spatial placement reasoning.",
            "task_setup": "Validity Testing: model receives tuple list and must reply 'valid' or 'invalid' (no justification). Construct Generation: model must output a list of tuples composing a valid assembly given n pieces. Both tasks zero-shot; responses parsed and validated programmatically.",
            "mechanisms_or_strategies": "Parsing and topological rules provided in prompt; models must apply discrete spatial rules to generate tuples. No CoT; model output parsed and checked automatically by validator.",
            "performance_metrics": "Construct generation: GPT-3.5 produced 0 valid constructs out of 400 attempts (0%). Validity-testing aggregated metrics not fully enumerated in text but models often failed validation checks.",
            "evidence_of_spatial_reasoning": "Qualitative examples show GPT-3.5 produced overlapping/pairwise impossible placements, failed to respect layering and connectivity rules, and could not detect impossible requests (e.g., 'triangle with 5 bricks').",
            "comparisons": "GPT-3.5 performed substantially worse than GPT-4 (which produced a small number of valid constructs). RandomPlayer baseline not applicable; programmatic validator used as ground truth.",
            "limitations_or_failure_cases": "Failed to respect non-overlap/connectivity constraints; produced impossible superpositions and ignored layer ordering; inability to detect impossible tasks. Authors note potential single-character tokenization and ASCII representation issues as contributing factors.",
            "uuid": "e8347.6",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 LCL (LEGO Connect Language)",
            "name_full": "GPT-4 evaluated on LCL (LEGO Connect Language) assembly tasks",
            "brief_description": "GPT-4 produced a very small fraction of valid LCL constructs and had difficulty consistently applying LCL rules, indicating weak compositional spatial assembly capabilities.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview",
            "model_description": "Zero-shot LCL prompts with formal specification; same experimental protocol as GPT-3.5 for validity and construct generation; outputs parsed and validated.",
            "model_size": null,
            "puzzle_name": "LEGO Connect Language (LCL) - validity and construct generation",
            "puzzle_type": "2D discrete assembly requiring connectivity/topological reasoning and non-overlap constraints.",
            "task_setup": "Models asked to validate constructs and to generate valid constructs (tuples of (x,y,'color')). Programmatic validator checked outputs. Temperatures varied; 100 construct generation attempts per temp per model.",
            "mechanisms_or_strategies": "No explicit symbolic planner used; model relied on token-level generation to produce coordinate tuples that must satisfy LCL constraints; no CoT prompting in main runs.",
            "performance_metrics": "Construct generation: GPT-4 produced 16 valid constructs out of 400 attempts (4%). Validity test accuracy across temperatures shown in figures but summarized as generally poor; only a small fraction of valid outputs produced.",
            "evidence_of_spatial_reasoning": "Some successes (e.g., constructing a 3-brick tower and one triangle in examples), but widespread rule violations and impossible placements in many outputs demonstrate inconsistent spatial reasoning and rule adherence.",
            "comparisons": "Outperformed GPT-3.5 on construction (GPT-3.5: 0 valid out of 400) but still far from reliable. Authors caution possible contamination or memorization for some successful cases but overall poor generative validity.",
            "limitations_or_failure_cases": "Often produced overlapping pieces, violated layering/connectivity rules, and failed to detect impossible requirements. Low absolute success rate (≈4%) indicates severe limitations for discrete assembly tasks.",
            "uuid": "e8347.7",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5 Shapes",
            "name_full": "GPT-3.5 Turbo evaluated on the Game of Shapes",
            "brief_description": "GPT-3.5 was asked to identify basic geometric shapes encoded as 1s in a 15-sided (15x15?) grid of 0s; performance was near-random for some shapes and poor for squares.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_description": "Zero-shot multiple-choice identification task: model sees ASCII matrix with 1s/0s and is given multiple-choice options (circle, rectangle, triangle, cross) though only three were shown in examples; model outputs choice. Temperatures varied.",
            "model_size": null,
            "puzzle_name": "Game of Shapes",
            "puzzle_type": "2D pattern recognition on a binary grid (spatial shape recognition requiring perceptual/spatial pattern extraction).",
            "task_setup": "Zero-shot multiple-choice: ASCII matrix presented along with four shape options; models answer single-choice. Choice positions not randomized intentionally to test order bias. No CoT used.",
            "mechanisms_or_strategies": "Models rely on pattern recognition via token-level mapping from ASCII grid to label; no explicit visual embeddings or geometric algorithms used.",
            "performance_metrics": "GPT-3.5 performance approximately equivalent to random chance for triangles and crosses; failed to recognize most squares. Authors present per-shape bar plots; overall accuracy low (not numerically summed in text).",
            "evidence_of_spatial_reasoning": "Poor performance implies weak explicit spatial pattern extraction in this format; heatmaps of decisions and linear trends with temperature suggest partial sensitivity to stochasticity but no robust spatial abstraction.",
            "comparisons": "GPT-4 performed substantially better (~80% accuracy) on same task; authors suggest possibility of training-data contamination for GPT-4's strong result.",
            "limitations_or_failure_cases": "High error rates, especially for squares; many invalid Unicode outputs at higher temperatures required discarding some GPT-3.5 responses by the parser; susceptibility to tokenization/encoding issues cited.",
            "uuid": "e8347.8",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 Shapes",
            "name_full": "GPT-4 evaluated on the Game of Shapes",
            "brief_description": "GPT-4 achieved substantially better shape-identification performance (~80% accuracy), especially at recognizing triangles, though some high-temperature outputs were invalid and discarded.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview",
            "model_description": "Zero-shot multiple-choice shape identification from ASCII binary grids; temperatures varied; model outputs single-choice label.",
            "model_size": null,
            "puzzle_name": "Game of Shapes",
            "puzzle_type": "2D binary-grid shape recognition requiring spatial pattern recognition and abstraction.",
            "task_setup": "Zero-shot ASCII input with 1/0 grid and multiple-choice options; single-response output; temperatures swept.",
            "mechanisms_or_strategies": "No CoT used. Likely relying on memorized patterns or learned correspondences between ASCII grids and shape labels; authors note potential data contamination (examples similar in training data).",
            "performance_metrics": "Overall accuracy ≈80% (authors state ≈80% and particular proficiency at triangles). Some responses at high temperatures were invalid and discarded.",
            "evidence_of_spatial_reasoning": "Strong performance relative to GPT-3.5 suggests better pattern recognition; however, authors caution this may be due to training data contamination rather than general spatial reasoning. Heatmaps and per-shape analysis provided.",
            "comparisons": "Substantially better than GPT-3.5 on this task; authors hypothesize contamination explains the gap. No human baseline reported for this specific binary-grid task.",
            "limitations_or_failure_cases": "Some outputs invalid at higher temperature; potential contamination undermines claim of genuine zero-shot spatial reasoning. Not robustly generalized across all shapes.",
            "uuid": "e8347.9",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5 Conway",
            "name_full": "GPT-3.5 Turbo soft experiment on Conway's Game of Life",
            "brief_description": "Exploratory tests showed GPT-3.5 could identify very simple oscillating patterns but failed at predicting previous/next states in more complex Game of Life sequences, even when rules were given.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_description": "Models were shown sequences of ASCII Game of Life states and asked to predict prior/next states and/or infer rules. This was a soft experiment (hand-segmented, not included as formal benchmark).",
            "model_size": null,
            "puzzle_name": "Conway's Game of Life (soft experiment)",
            "puzzle_type": "Cellular automaton state prediction (temporal-spatial reasoning across discrete 2D grids).",
            "task_setup": "Models given 2–3 sequential board states and asked to produce the preceding or following state or infer the survival/birth rules; rule B3/S23 used in experiments. Not part of final benchmark due to hand-segmentation/repeatability issues.",
            "mechanisms_or_strategies": "No algorithmic simulation; models must infer transition rules and apply them to produce next/previous grid state from ASCII representation. Some feedback/clues (explicit rules) were used in later trials.",
            "performance_metrics": "Both GPT-3.5 and GPT-4 succeeded only on simple blinking oscillators; failed to consistently predict more complex evolutions. Quantitative summary: both succeeded only on simple cases (no robust success rate given).",
            "evidence_of_spatial_reasoning": "Limited: success on trivial oscillator patterns shows some pattern extrapolation, but failure on more complex sequences indicates lack of reliable rule-based spatial-temporal simulation.",
            "comparisons": "Both models performed similarly and poorly except on simplest patterns; no formal baseline provided other than ground-truth simulation.",
            "limitations_or_failure_cases": "Could not deduce or apply rules in complex scenarios even when rules were given; hand-segmentation and parsing issues limit robustness.",
            "uuid": "e8347.10",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 Conway",
            "name_full": "GPT-4 tested on Conway's Game of Life (soft experiment)",
            "brief_description": "GPT-4 mirrored GPT-3.5's limited ability: it could handle simple oscillator patterns but failed to reliably predict complex Game of Life states or infer/predict sequences even when provided rules.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview",
            "model_description": "As above; exploratory ASCII-sequence-based experiment, not included in formal benchmark because of repeatability concerns.",
            "model_size": null,
            "puzzle_name": "Conway's Game of Life (soft experiment)",
            "puzzle_type": "Temporal-spatial cellular automaton requiring application of local transition rules across a 2D grid.",
            "task_setup": "ASCII states provided; asked to produce prior/next states or infer rules. Some test cases included extra noise and larger patterns.",
            "mechanisms_or_strategies": "Model attempts pattern-mapping from seen tokens to next-state tokens; no explicit simulation or iterative planning used.",
            "performance_metrics": "Succeeded only in identifying simple blinking/oscillators; failed on more complex state prediction tasks. No robust numeric success rate reported.",
            "evidence_of_spatial_reasoning": "Partial on trivial patterns but fails to generalize to more complex spatial-temporal dynamics, indicating lack of systematic rule-based simulation.",
            "comparisons": "Comparable to GPT-3.5 on these soft experiments; both inconsistent. Not included in main benchmark due to methodological issues.",
            "limitations_or_failure_cases": "Invalid/incomplete outputs in several runs; inability to reliably infer or apply transition rules for nontrivial configurations.",
            "uuid": "e8347.11",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Other models (Claude, Mistral, Gemini, HF models)",
            "name_full": "Other LLMs tested in Appendix G: Anthropic Claude, Mistral, Google Gemini, and various HuggingFace models",
            "brief_description": "Additional models were prompted (e.g., Tic-Tac-Toe) in Appendix G; these models also failed to consistently block winning moves and produced a range of non-answer behaviours, illustrating that the observed failures are not unique to GPT-family models.",
            "citation_title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
            "mention_or_use": "use",
            "model_name": "Claude; Mistral; Gemini; assorted HF models (phi-2, tiny-llama variants, Falcon-7B-Instruct, etc.)",
            "model_description": "Diverse set of proprietary and open models (Anthropic Claude, Mistral, Google Gemini, Microsoft Phi variants, TinyLlama, Falcon-7B-Instruct, etc.) briefly tested on Tic-Tac-Toe and shown to miss blocks or produce invalid/non-game responses; testing informal and illustrative.",
            "model_size": null,
            "puzzle_name": "Tic-Tac-Toe (illustrative prompts)",
            "puzzle_type": "3x3 grid adversarial game requiring blocking/win detection.",
            "task_setup": "Appendix G: models prompted with Tic-Tac-Toe instructions and played sample games (human vs model examples) to illustrate qualitative failures (missed blocks, non-answers). Not part of main ChildPlay experimental sweep.",
            "mechanisms_or_strategies": "Similar zero-shot prompting; varied outputs included incorrect role assignments, non-game text, and code snippets. No consistent planning mechanism.",
            "performance_metrics": "Qualitative: multiple models missed obvious blocking moves and lost games; no systematic win-rate sweep provided in appendix for these models.",
            "evidence_of_spatial_reasoning": "Appendix G examples show repeated inability to implement simple blocking, indicating lack of reliable spatial/strategic play across architectures and sizes in these prompts.",
            "comparisons": "These appendix examples align with main results (GPT-3.5/GPT-4) showing general brittleness; no quantitative head-to-head beyond illustrative sequences.",
            "limitations_or_failure_cases": "Non-answer behavior, incorrect role understanding, failure to block winning moves, and a variety of parsing/tokenization issues across models.",
            "uuid": "e8347.12",
            "source_info": {
                "paper_title": "Show, Don’t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Testing spatial reasoning of large language models: the case of tic-tac-toe",
            "rating": 2,
            "sanitized_title": "testing_spatial_reasoning_of_large_language_models_the_case_of_tictactoe"
        },
        {
            "paper_title": "Benchmarking large language model (llm) performance for game playing via tic-tac-toe",
            "rating": 2,
            "sanitized_title": "benchmarking_large_language_model_llm_performance_for_game_playing_via_tictactoe"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Are emergent abilities of large language models a mirage?",
            "rating": 1,
            "sanitized_title": "are_emergent_abilities_of_large_language_models_a_mirage"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.02303825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay
18 Aug 2024</p>
<p>Hora Gonçalo 
De Carvalho 
Oscar Knap 
Robert Pollice r.pollice@rug.nl </p>
<p>University of Groningen</p>
<p>University of Groningen</p>
<p>Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay
18 Aug 20246FC15A99006402AE488A068CCF95880FarXiv:2407.11068v3[cs.AI]
The evaluation of Large Language Models (LLMs) often focuses on linguistic tasks, yet these may not fully capture general reasoning capabilities.We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess broader cognitive functions, particularly in non-linguistic domains.Our approach extends beyond standard linguistic benchmarks by incorporating games like Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess strategic thinking and decisionmaking.To evaluate the models' ability to generalize beyond their training data, we introduce two additional games.The first game, LEGO Connect Language (LCL), tests the models' capacity to understand spatial logic and follow assembly instructions.The second game, the game of shapes, challenges the models to identify shapes represented by 1s within a matrix of zeros, further testing their spatial reasoning skills.This "show, don't tell" strategy uses games to potentially reveal cognitive capabilities rather than simply querying the models.Our results indicate that despite their proficiency on standard benchmarks and temperature settings, GPT-3.5 and GPT-4's abilities to play and reason about fully observable games without pre-training is mediocre.Both models fail to anticipate losing moves in Tic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly.While GPT-4 shows some success in the game of shapes, both models struggle with the assembly tasks presented in the LCL game.These results suggest that while LLMs like the GPT models can emulate conversational proficiency and basic rule comprehension, their performance in strategic gameplay and spatial reasoning tasks is limited in cognitive flexibility and generalization.Importantly, this reveals a blind spot in current LLM benchmarks that we highlight with our gameplay benchmark suite ChildPlay (GitHub Repository).Our findings provide a cautionary tale about claims of emergent intelligence and reasoning capabilities of LLMs that are roughly the size of  38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p>
<p>Introduction</p>
<p>Typically, Large Language Models (LLMs) are transformer-based -they process input text and generate output text in a coherent and contextually appropriate manner [8].Modern versions use a self-attention mechanism to weigh the importance of different words in a sentence relative to each other [53,8].Input text is tokenized, converted into vectors using embeddings, and processed through transformer layers that calculate attention scores to dictate focus on relevant tokens [53,8,16].The model then selects the next token based on learned distributions [53,8,16].With their large parameter count, from Alpaca with 7 billion parameters [46], to LLaMA with 65 billion [49] or PaLM with 540 billion parameters [13], these neural networks have learned to model complex linguistic abstractions, capturing patterns in syntax, semantics, pragmatics, and elements of both style and tone [8,9,37].</p>
<p>Current Benchmarks Benchmarks consist of tests or tasks along with their associated metrics, used to comparatively evaluate a system.They help identify the relative state of the art by comparing systems based on specific measurements.Datasets like SQuAD, GLUE, BIG-bench, SuperGLUE, ANLI, TruthfulQA, HellaSwag, and the lm-evaluation-harness offer various test types, including multiple-choice questions, reading comprehension tasks, natural language understanding, common sense reasoning, factual knowledge, and dialogue completion tasks [38,54,2,54,34,28,59,18].</p>
<p>Recent studies have evaluated LLMs' reasoning in non-linguistic tasks too.Liga and Pasetto used Tic-Tac-Toe in ASCII form, pitting LLMs against the minimax algorithm to explore emergent features, suggested to be akin to consciousness [27].Although LLMs performed well, they often failed to win against the minimax algorithm, resulting in draws and losses [27].Topsakal and Harper [48] found GPT-4 secured more wins than GPT-3.5 but did not consistently play optimally.Some studies, such as those by Lappin et al. and Zečević et al., caution against overestimating LLMs' capabilities, stating that LLMs may excel at language tasks but seem to struggle with reasoning and causal understanding [26,60].Bender et al. and Schaeffer et al. further critique the transparency of evaluation metrics, arguing that some "emergent abilities" may be artifacts of evaluation rather than genuine model behavior [3,39].</p>
<p>Another issue in evaluating LLMs is that of training-test set cross-contamination [8].Massive training datasets, often untraceable, make replication studies difficult and risk rendering benchmarks meaningless due to contamination [8,17].N-Gram Overlap is one method to detect contamination, but it has limitations, such as failing to account for context and synonymous text [6].Furthermore, the arbitrary 200-character window classically used during GPT-3.5 training may not accurately reflect the influence of surrounding text [8].</p>
<p>This critique of the evaluation metrics used in assessing LLMs invites a deeper exploration of general intelligence -specifically how it can be reliably measured and observed in AI through rigorous and realistic tests that extend beyond linguistic prowess to include broader cognitive functions.Typically, when defining general intelligence (GI), the "g factor" is used as a measure of the ability to reason, plan, solve problems, think abstractly, and learn quickly across a wide range of domains [57,5,55,11,10].GI then involves higher-order cognitive processes that go beyond specific skills or knowledge domains [19,23].</p>
<p>We argue that there is a need for nuance in current debates surrounding AGI (Artificial General Intelligence) and a pragmatic perspective on understanding LLMs' capabilities.In order to approximate some measurement of GI in an AI system, it is important that we build benchmarks that allow measurements that can truly gauge generalization and reasoning in a human-like manner, rather than relying solely on pattern matching and statistical correlations [52].</p>
<p>This Work We introduce ChildPlay, a light and scalable suite of non-language-based games like Tic-Tac-Toe, Connect-Four, Battleship, LEGO Connect Language, and Shapes, to assess LLMs' reasoning, strategy, symbolic reasoning, and pattern recognition beyond traditional linguistic tasks.The benchmarks can be executed, taking only a few hours to complete across all tasks for a given model.Furthermore, we have proposed this benchmark suite for inclusion in the BIG-bench initiative [2], which aims to standardize challenging benchmarks for LLMs.</p>
<p>We chose games because they provide structured environments with clear success criteria, suitable for evaluating strategic thinking and decision-making in dynamic, adversarial scenarios [43,27,48].We encode these games using ASCII representations to minimize dataset contamination issues prevalent in testing generalized intelligence beyond the training domain [8,27].</p>
<p>While our benchmarks attempt to probe the model's understanding indirectly through win ratios, gameplay is inherently limited by the model's reliance on probabilistic patterns rather than genuine rule-based reasoning, something that is contentious in the field at the moment and that we attempt to probe.The game rules are explicitly given to the models in the prompts, which we believe is a critical component of evaluating how well LLMs can generalize from given instructions.While this does not fully guarantee that the models are learning and applying these rules as a human would, it still probes generalization ability.</p>
<p>Experiments</p>
<p>General Methodology Specific tasks in the BIG-bench benchmark [2] are categorized as zero-shot, one-shot, or multi-shot [8].Our tasks, available through the ChildPlay GitHub Repository, fit the zero-shot category, where models receive only a brief explanation at inference time with no examples.To test LLMs' reasoning beyond their training data, we focused on spatial reasoning with ASCII sequences.An agent capable of true abstraction should encode and interpret these sequences when the rules are explained.</p>
<p>We developed tasks including LEGO assembly, ASCII games (Tic-Tac-Toe, Connect-Four, Battleship), and identifying shapes represented as 1s in 15-sided grids of 0s.The models tested were gpt-3.5-turbo-1106(GPT-3.5)and gpt-4-1106-preview (GPT-4), across temperatures t = 0, 0.5, 1, and 1.5 (refer to Appendix G for a preliminary analysis on models other than GPT-3.5 and GPT-4).Temperature is a hyperparameter that controls the randomness of a model's output in text generation.Lower temperatures make the model more deterministic, favoring the most probable tokens.Importantly, when queried, both GPT-3.5 and GPT-4 were able to explain the tasks and generate valid board states, validating our benchmarks.We hypothesize that models capable of reasoning should play optimally, given their ability to explain optimal strategies when prompted (see Appendix B.5).In general, these experiments took between a few minutes to a few hours to run.Error bars are based on the binomial distribution and computed using the standard deviation.</p>
<p>Lego Connect Language (LCL)</p>
<p>We invented a formal language we call LEGO Connect Language (LCL).More specifically, we propose LCL 2 as a language to instruct assembly in 2D on the x and y axis (this can be extended to LCL 3 or 3D).Only 2x4 pieces are allowed when assembling a construction, M , which is then valid if no pieces are overlapping and all pieces are connected to other pieces.Namely, a Lego piece is connected through interlocking pegs, not by merely touching sides.And secondly, two Lego pieces overlap when they share the same y-coordinate and any part of their length has the same x-coordinate (see Appendix D.1 for a formal definition of LCL).Game 2: Construct Generation Models generate LCL constructs from prompts.Each construct is described by tuples specifying the coordinates and colors of pieces.Construct validity is then automatically evaluated.</p>
<p>In both games, the evaluation metric is the proportion of correct validations across different temperatures.We generated 800 images for the validity test (half valid, half invalid) and queried each model to produce 100 images at each temperature.These were checked for validity.An example for data used in these tasks is
[(1, 1, ′ red ′ ), (2, 1, ′ blue ′ ), (3, 1, ′ green ′ )],
where each tuple represents a brick's position and color.Our use of LCL is related to Bubeck et al. [9], where GPT-4 generated images using JavaScript or LaTeX.However, unlike Bubeck et al.'s examples, which are prone to training data contamination, LCL remains abstract, challenging the model to operate outside its learned distributions.</p>
<p>Board Games: Tic-tac-toe, Connect-four, and Battleship (BTC) For this set of games, which we will refer to as BTC from now on, each new board state was introduced with an initial game explanation via the OpenAI API in a zero-shot setup.The models, always playing as the first player, were provided with the current board state and faced an opponent making random moves.Since these games are fully observable, only the initial instruction and the current board state were needed for optimal play.Inputs required were two scalars for the row-column pair or a single scalar for the column in Connect-Four.</p>
<p>In Battleship, ships ('S') were randomly initialized horizontally, varying from 2 to 5 cells.Hits were marked with an 'X' and misses with an 'O' on both players' boards.The symbol '˜' denoted the seaan unexplored square.The Game of Shapes For the game of shapes, we first probed the models about what geometric shapes they consider basic.The three shapes consistently mentioned were square, circle, and triangle.The game consists of finding a basic geometric shape "hidden" behind 1s in a matrix of 0s in a multiple-choice fashion.Four shapes were used as options: the circle, the rectangle, the triangle, and the cross.Only the latter three were ever shown to the model (cf.Fig. 3).The choice positions are not randomized to test if the model displays any inherent bias for the question order.This does not affect the outcome, as the game is designed to operate within a single question-response framework.</p>
<p>Results</p>
<p>GPT move biases when playing BTC are shown in the heatmaps in Appendix B.3.The Random Player's uniform distribution of moves across squares and columns appears to make it less likely to counter GPT's "strategies", especially in Connect-Four.According to these heatmaps, GPT-3.5 makes more mistakes than GPT-4 at Tic-Tac-Toe, namely because it seems to play the middle row even when occupied, which produces an automatic loss.Furthermore, the heatmaps reveal that GPT models often play the middle column in Connect-Four, even if already full, increasing the likelihood of connecting four pieces before the Random Player, who has just a 1/7 chance of blocking.Errors, especially from GPT-3.5, generally increase with temperature, likely due to increased randomness in move selection.As shown in Table 1, Performances vary significantly across games and temperatures.In Battleship, GPT-3.5'swin rate drops from 10% at t = 0.0 to 3% at t = 1.0 and t = 1.5.Surprisingly, GPT-4 performs even worse, with no wins at t = 0.0, t = 0.5, and t = 1.5, and only 4% at t = 1.0.In Tic-Tac-Toe, GPT-3.5'swin rate decreases from 53% at t = 0.0 to 19% at t = 1.5.GPT-4 performs better, peaking at 77% at t = 0.5 but dropping to 50% at t = 1.5.In Connect-Four, GPT-3.5 shows an improvement from 64% at t = 0.0 to 76% at t = 0.5, maintaining around 75% at higher temperatures, while GPT-4 starts strong with 80% at t = 0.0 but falls to 39% at t = 1.5.</p>
<p>Overall, GPT-4 outperforms GPT-3.5 in Tic-Tac-Toe and Connect-Four at lower temperatures (t = 0.0 and t = 0.5), but GPT-3.5 shows better performance in Battleship at t = 0.0 and tends to be more stable in Connect-Four at higher temperatures.Both models struggle as temperature increases, with significant declines in win rates, especially in Battleship and Tic-Tac-Toe.Importantly, both models often underperform compared to a random player, particularly at higher temperatures, which underscores the challenges they face in maintaining effective strategies as the temperature rises.</p>
<p>Tic-Tac-Toe For comparison with the model's performance, Fig. 12 presents the Tic-Tac-Toe match results of the minimax algorithm against the same random player the models played against.</p>
<p>The algorithm won 996 games out of 1000, tied 4, and lost none.This also replicates the results of previous research on Tic-Tac-Toe and we use that as a baseline for optimal play against a random player (see Appendix A.7.1) [44,1].</p>
<p>Both GPT-3.5 and GPT-4 perform poorly in Tic-Tac-Toe at t=0, reflecting deterministic play based on learned strategies (see Appendix B.3).The Random Player's uniform move distribution (see Fig.
B.3.
2) reduces the likelihood of countering GPT's central strategies, with GPT-3.5 making more errors than GPT-4.These errors significantly impact outcomes due to incorrect moves (cf.Fig. 4).Errors increase with temperature, likely due to an increase in randomness (cf.Fig. 14), leading to losses from illegal moves rather than direct defeats.</p>
<p>Average game moves, missed wins and missed blocks in Tic-Tac-Toe decrease as temperature rises (see Fig. 5), because of an increase in losses due to random play leading to illegal moves.Regardless, neither model plays optimally, as indicated by the high number of missed wins and blocks.As temperature increases, missed wins and blocks decrease as models move away from deterministic play illustrated in the heatmaps of moves in Appendix B.3.Connect-Four Both GPT-3.5 and GPT-4 exhibit their poorest performance in Connect-Four at temperature 0, indicative of deterministic play reflecting the models' learned behavior (see Appendix B.3.3).The Random Player's uniform distribution across columns (Fig. 16) implies a lower chance of countering GPT's central strategies in Connect-Four.Neither model plays Connect-Four optimally, as evidenced by the many missed wins and blocks.Fig. 7 demonstrates that, as temperature increases, missed wins and blocks increase for GPT-3.5 and remains roughly the same for GPT-4.Battleship The models were incapable of following the Battleship rules.Regardless of temperature, the models lose most games, with GPT-4 not winning any game due to incorrect moves (cf.Fig. 19).GPT-3.5 wins 10% of the matches at low temperatures, but none at higher ones (cf.Fig. 8).</p>
<p>Shapes</p>
<p>In the Shapes game, we see that GPT-3.5'sperformance was approximately equivalent to random chance when identifiying triangles and crosses, yet, it failed to recognize most squares (cf.Fig 9).In contrast, GPT-4 performed remarkably well, successfully identifying shapes with an accuracy of ≈ 80%, demonstrating particular proficiency at recognizing triangles.At higher temperatures, some of GPT-4's responses were discarded by our parser when the model generated invalid Unicode output, and, thus, were not included in the final evaluation.This discrepancy is evident in Fig. 9, for instance, where the sum of correct and incorrect choices does not total 25 at high temperatures.Fig. 11 shows a roughly linear increase in the proportion of correct answers for GPT-3.5 during the validity test as a function of temperature.While GPT-4 peaks at temperature 0.5 and then declines.Regardless of these increases, the percentage of correct responses is minimal.Only GPT-4 produced a small fraction of valid LCL constructs (namely 0.04 out of 400 = 16).GPT-3.5 did not produce a single valid LCL construct.</p>
<p>Discussion</p>
<p>Just as the presently ongoing ARC-AGI competition [21] attempts to test systems under a definition of GI akin to the one we offer in our introduction (processes equivalent to higher-order cognition enabling general problem solving that goes beyond specific skills or knowledge domains), our work too explores AI reasoning capabilities through methods that differ from typical benchmarks.Interestingly, both currently highlight deep learning limitations (at the moment no solution, with or without deep learning, goes beyond 43% accuracy, defined as the percentage of correct predictions [24]), particularly in abstract reasoning.The failure of these systems in ARC tasks aligns with our ChildPlay findings, where LLMs struggle with strategic reasoning and rule adherence, underscoring challenges in achieving true generalization beyond trained data distributions.</p>
<p>In Tic-Tac-Toe, both models underperform compared to the minimax algorithm, with mixed results in Connect-Four.GPT-4 performs well in Shapes, but GPT-3.5 does poorly.Both fail in LCL, and Battleship, at any temperatures, showing significant limitations in rule understanding.Increased temperature leads to more naive exploration but does not improve strategic performance, as seen in the rise of missed wins and blocks.</p>
<p>Overall, GPT-3.5 and GPT-4 can produce output for simple games but struggle to do so sensibly, often failing to apply optimal strategies such as blocking winning moves or going for winning move.In summary, the performance difference to minimax underscores LLM limitations in strategic reasoning.Failures in Battleship and LCL highlight severe issues in rule adherence.</p>
<p>Modern LLM benchmarks primarily adapt Turing's test [50] to assess language processing, potentially overlooking deeper complexities.While smaller models like GPT-4 exhibit what Humboldt described as the "infinite use of finite means" [29] as popularized by Chomsky [12], generating contextually relevant sentences [45], this does not necessarily mean they have mastered reasoning -it is more likely to be advanced pattern imitation.</p>
<p>Limitations and Future Work Just as in ARC-AI [21], in the future we intend to generate a holdout game dataset that is kept hidden, only providing tools for evaluation, such that overfitting on it before testing becomes unlikely.Regarding our general approach, we pursued a zero-shot condition, but future work should test Chain-of-Thought (CoT), Tree-of-Thoughts (ToT), and even Few-Shot learning [56,58,8].These methodologies reduce task complexity and may lead to improved LLM performance.LLMs might just need more clues.The current technical disadvantage of any testing setting is that they allow LLMs to resort to probabilistic patterns rather than genuine reasoning.This might not be an issue of the class of methods, but simply a direct product of the underlying transformer algorithm which narrows in on the correct data manifold by using input tokens as features for arriving at the most likely answer given the training data, allowing the models to approximate reasoning patterns, likely without true understanding.</p>
<p>Regarding input tokens, ChildPlay sometimes uses single character output, leading to what we term single character token myopia.This is problematic when encoding state information as single characters because GPT has a multi-character tokenizer as do most LLMs [8].</p>
<p>On the subject of evaluation, our benchmark's binary outcomes (win/loss) can bias perceived capabilities.To avoid this, we also assess move choices and missed wins/blocks (see Appendix B.3 and Figs. 5, 7).Using discontinuous metrics in strategic games may cause sharp evaluation transitions, potentially misrepresenting gradual improvements.Shape recognition and LCL tasks might benefit from continuous metrics, providing smoother performance gradients and more accurate reflections of reasoning abilities.If the model outputs continuous values (e.g., coordinates of a shape's vertices), the mean squared error, for example, can be used to quantify the difference between predicted and actual values.</p>
<p>Furthermore, following Schaeffer et al.'s perspective, one might argue that the ChildPlay games may not fully reflect true generalization or emergent abilities [39].If benchmarks resemble nonlinear metrics, they could exaggerate LLM weaknesses or strengths.For instance, a sharp failure in the Battleship game might not indicate poor strategic reasoning universally but rather in specific conditions.We believe this not to be the case as the experiments consist of simple fully observable games.Additionally, we aimed to diminish this effect by producing heatmaps of the moves and keeping track of illegal moves, missed blocks, and missed wins.Regarding the performance difference between the models at the shapes game, we believe this is best rationalized through data-contamination.We think it is likely that GPT-4's training data contained examples similar to the ones used.This would explain why GPT-4 only improved dramatically at that game -it already knew the answers.</p>
<p>Future ChildPlay enhancements could involve feedback on actions, reducing task complexity.We have translated a small dataset from the Lego corporation (see Appendix D.3) for future reference and development of LCL.The LCL dataset can also be simulated in both 2D and 3D as a multimodal benchmark.Another important aspect is our ASCII representation.It might not be well-suited to convey the meaning of a board state.More complex symbolic representations might benefit the models' performance.We also plan to benchmark additional models (cf.Appendix G for a short preliminary analysis on models other than GPT-3.</p>
<p>Conclusions</p>
<p>Non-language tasks challenge models to generalize across different encodings and delve into out-oftraining-distribution scenarios.Testing LLMs like GPT-4 (according to OpenAI, a contender for AGI [9]) beyond text, we show they are mediocre at best at simple reasoning tasks outside their training data.Both GPT-4 and GPT-3.5 fail to play simple games optimally, and struggle with following rules, namely assembly instructions.Mixed results were found in interpreting geometric shapes from binary grids, where GPT-4 did particularly well.These tasks test reasoning without relying on language, forcing the model to play the game rather than describe it, which we believe is akin to repeat learned information, also known as "parroting" [7,14,4,20,60,3]. .The "non-language" category of the BigBench benchmark includes tasks like ASCII recognition, chess, and Sudoku, but none like ours [2].Hence, we believe ChildPlay is a valuable addition to current LLM benchmarks.</p>
<p>Developing games allows us to critically assess claims about a model's reasoning and problem-solving abilities, avoiding data contamination issues.Our results suggest current LLMs show disappointing problem-solving capabilities, highlighting key areas for future improvement.</p>
<p>The key takeaway of our work is not just the models' performance in specific games, but understanding their ability to generalize and reason in novel contexts.This has broader implications for AI, particularly in developing models capable of true reasoning and adaptive thinking.Our research highlights how LLMs handle tasks requiring higher-order reasoning, which is crucial for advanced applications.Practically, if GPT or other LLMs cannot reason accurately on novel data, they should not be trusted to do so in standard applications.This is critical, especially in scientific contexts, as many users may be biased toward believing LLM outputs because they appear credible.Our benchmarks illustrate these pitfalls, showing that if LLMs struggle with simple games, users should be more critical of their outputs.</p>
<p>Appendix A Algorithms</p>
<p>A.1 Generating Games</p>
<p>Our program enables simulation, testing, and analysis of game-play of the different benchmarks found in ChildPlay.</p>
<p>The program begins by setting up variables for debug outputs and game configurations, initializes the game environment based on these configurations, and sets up players accordingly.It then simulates a series of games, saving results and logs in JSON format, and generates heatmaps to analyze gameplay strategies and player decisions.The script also handles results aggregation and visualization, including the generation of bar plots of the results.</p>
<p>Our program is structured to simulate games with various players through a base class and its derived classes, each representing different player types.This class, the PlayerBase class, serves as the foundational class for all player types.It initializes player properties such as ID, name, and a debug flag, and it also manages message storage for debugging or interaction logging.A method is available that can store or print board states and plays based on the debug flag, facilitating debugging and tracking game interactions.</p>
<p>The LLMPlayer class, derived from PlayerBase, represents a player that utilizes a large language model, in our case, this player is either GPT-3.5 or GPT-4.Its constructor initializes the player with an ID, name, model details, and the game being played.It inherits from PlayerBase and extends functionality for LLM interactions.We introduce a method to print messages related to API interactions, and another that formulates a prompt for the LLM, invokes it, and processes its response.Then, a different method interprets the LLM's response to determine the validity of a move within the game's context.</p>
<p>The TextPlayer class, also derived from PlayerBase, represents a player who inputs moves via text, through the console -this is a human player.Its constructor initializes player-specific details and a callback function for receiving input.There is a method that manages receiving and validating player inputs according to game rules.</p>
<p>The RandomPlayer class, another derived class from PlayerBase, simulates a player making random valid moves based on the current game state.We have designed a method in this class to generate random possible moves that always comply with the game's rules given the present game state.</p>
<p>We have two possible game loops, one designed to execute a series of games between two players, and another to simulate one single game.The former takes parameters including the game instance, the two players, the number of games to play, and a debug flag.This function runs a loop for the specified number of games, recording results, tracking messages, and logging game states and moves.It returns a summary of the game results and collected messages for further analysis.The second game loop simulates a single game between two players by initializing the game, managing turn-based gameplay, and logging detailed move information.It returns a tuple with game messages, counts of invalid moves, a log of moves, and the game outcome.</p>
<p>Other functions, all helper functions, enable basic functionality, for example, in producing multiple random moves to generate a dataset, or loading data from a JSON file.We can also create a comprehensive dataset from multiple games and save it to a JSON file, load it, and print the board state for a specific record index.As stated before, we can also generate heatmaps showing the frequency of moves in the case of the board games, or of predicted versus actual shapes in the case of the game of shapes.</p>
<p>A.2 Tic-Tac-Toe</p>
<p>For the Tic-Tac-Toe task we first initialize the game with optional settings.If no specific options are provided, it defaults to a 3x3 board size.Customizable options include setting the board size and a debug flag.Upon initialization, a method is called to set up or reset the game state.Additionally, it establishes a game name and provides a prompt that explains the rules and how to input moves for an algorithmic explanation of the code used to run Tic-Tac-Toe games).</p>
<p>The reset_game method is responsible for resetting the game by creating a board of the specified size, defaulting to 3x3, filled with spaces that represent empty cells.It also initializes the current player as "P1", sets up a list to track moves made, and marks the game status as not over.</p>
<p>The get_text_state method constructs a text-based or ASCII representation of the current game state.It displays column and row headers for the board and formats the board with dividers to clearly delineate rows and columns.</p>
<p>The guess method manages a player's move by first validating it to ensure it falls within the board boundaries and that the chosen position is not already occupied.If the move is valid, it updates the board with the player's symbol ('X' for player index 0 and 'O' for player index 1).After each valid move, the method checks for a win or a tie.If neither condition is met, it switches the turn to the next player.If a player makes a wrong move they lose the game.</p>
<p>The check_win method determines if the current player has won by checking all possible winning conditions.This includes vertical, horizontal, and diagonal lines.It returns True if any of these conditions are met with consecutive symbols matching the current player's.</p>
<p>The check_tie method checks for a tie by determining if all cells are filled without any player achieving a winning condition.</p>
<p>A switch_player method is available that alternates turns between the two players, switching control from "P1" to "P2" after each valid move.</p>
<p>A.3 Connect-Four</p>
<p>The Connect-Four constructor initializes the game with optional settings as well, such as row and column size and debug mode.If no specific options are provided, the game defaults to a 7x7 grid.Otherwise, it reads the values from the provided options dictionary.The constructor sets up basic game properties, including a reset board, a tuple to track the last move, the game over status, and the current player.It also includes a game instruction prompt.</p>
<p>The reset_board method resets the board to a 7x7 grid (or a custom size if specified) filled with dots (.) to represent empty cells.</p>
<p>The check_tie method determines if the game has ended in a tie.It checks if the top row of the board is completely filled without any winner being declared.This method ensures that the check_win method returns False before declaring a tie.</p>
<p>The check_win method assesses whether a winning condition has been met after a move.It examines the cell corresponding to the last move and checks horizontally, vertically, and diagonally (in both positive and negative directions) for four consecutive identical symbols (either 'X' or 'O').</p>
<p>The guess method handles a player's move attempt.It returns an invalid move if the attempt is made outside of valid columns or on a full column.For a valid move, it updates the board, checks for a win or a tie, and switches to the next player if the game is still ongoing.</p>
<p>The get_text_state method generates a string representation of the current board state, displaying 'X' and 'O' for the two players.</p>
<p>The switch_player method alternates the active player between "P1" (Player 1) and "P2" (Player 2).</p>
<p>The board_size property returns the number of columns in the board, which is necessary for validation and display purposes.</p>
<p>A.4 Battleship</p>
<p>The Battleship constructor initializes, once again, the game with optional settings.If no options are provided, it defaults to a 5x5 board size.The constructor allows customization of the board size and a debug flag.It sets up four game boards for each player-two for ship placement and two for tracking guesses-using nested list comprehensions.Additionally, it initializes lists to keep track of the ship positions for both players, 2 lists each, 4 in total -these correspond to the players own boards which they can see both the ships and hits, and their adversaries, where they can see only their own hits and misses.A dictionary, ship_types, defines ship names and their sizes, which scale according to the board size.The place_ships method is then called to randomly place ships on each player's board.</p>
<p>Initial game properties are also set, including the current player and a game prompt that explains the rules.</p>
<p>The board property dynamically returns the guess board of the current player, in case we want to display it or carry out logic checks.</p>
<p>The place_ships method is responsible for randomly placing ships on a specified board.It attempts to place each ship either vertically or horizontally without overlapping or touching another ship, including diagonally adjacent spaces.</p>
<p>The is_space_free method checks if a specified space on the board is free to accommodate a ship of a given size and orientation.It ensures that there is no overlap with existing ships and that the ship is not placed directly adjacent to another ship.</p>
<p>The get_text_state method provides a string representation of both the ship and guess boards for a specified player, formatted for easy reading.This functionality is useful for displaying the game state in a text-based interface.</p>
<p>The guess method processes a player's guess by checking if it hits a ship and updating the boards accordingly.It also switches the player after each guess and checks for a win condition.Hits are marked with 'X' and misses with 'O' on the guess and ship boards.</p>
<p>The check_win method determines if the current player has won by checking if all positions of the opponent's ships have been hit.</p>
<p>The switch_player method alternates turns between the two players after each guess.</p>
<p>A.5 Shapes</p>
<p>The Shapes game starts by defining two constants, empty_character and full_character, which are represented by "0" and "1" respectively.These constants denote empty and filled cells in the grid.</p>
<p>One of the primary functions in the script is bar_plot_shapes, which generates bar plots illustrating the counts of correct and incorrect answers for each shape.This function takes several parameters, including base_path for saving plots, models which is a list of GPT-type models used in the game, temperatures which refers to different temperature settings in the GPT-model predictions, and shapes representing the various shapes used in the game.The function iterates over the models and temperatures, loads game logs, counts the correct and incorrect answers, and plots these counts in bar charts.The resulting plots are saved to specified directories.</p>
<p>The script also includes several shape-drawing functions.The create_board function initializes an empty board of specified dimensions filled with empty_character.The draw_rectangle function allows drawing a rectangle or square on the board by filling cells with full_character.The draw_circle function draws an approximate circle using the midpoint circle algorithm.The draw_triangle function draws an equilateral triangle on the board by filling cells within the triangle's bounds.The draw_cross function draws a cross on the board with a specified arm length centered at a given point.</p>
<p>A key component of the script is the Shapes class, which manages the game's logic.This class initializes the game with optional settings such as board size and debug mode.It sets the game name, the shape to be drawn, and a prompt explaining the game rules.The reset_game method creates a new board and randomly draws the specified shape (square, triangle, or cross) on it.It shuffles possible answer options and resets game state variables.The get_text_state method generates a text-based representation of the game board and possible answers for display.The guess method processes a player's guess, checks if it matches the correct shape, and ends the game, returning the outcome (win or loss).Methods check_win, check_tie, and check_loss are used to determine the game's outcome.</p>
<p>The script supports both visualization and analysis of game outcomes.The bar_plot_shapes function helps visualize the correctness of guesses.The shape-drawing functions facilitate the random generation of different shapes for each new session.The Shapes class encapsulates the game's logic, managing the board state, processing guesses, and determining game outcomes, thus supporting a single-player interaction where the player guesses the shape drawn on the board from a list of options.</p>
<p>A.6 Lego Connect Language (LCL)</p>
<p>In the LCL script, we enable validating, and visualizing Lego-like constructs with different types of players.</p>
<p>We start by defining LCLVisualizer, which handles the visualization of Lego constructs.Its methods include draw_piece, which draws a single Lego piece on a plot, and display_construct, which displays and saves a complete Lego construct.</p>
<p>The LCLGame class manages the logic for generating and validating Lego constructs.It contains attributes such as pieces, a list of tuples representing Lego pieces, valid_colors, a list of valid colors for these pieces, and metrics, a dictionary for tracking validation metrics and construct details.Its methods include can_place_piece, which determines if a Lego piece can be placed at a given position, find_all_valid_positions, which identifies all valid positions for placing new pieces, and build_random_valid_assembly, which constructs a valid Lego assembly by placing pieces in valid positions.Additionally, generate_random_piece and generate_random_construct are methods for generating random Lego pieces and constructs, respectively.The is_valid_construct method checks the validity of a given construct, while generate_valid_or_invalid_construct creates either a valid or invalid construct based on a given parameter.The class also includes methods like create_tower, create_bridge, and create_staircase for generating specific types of constructs, and validate_construct, which validates a player's response regarding the construct's validity.Finally, save_metrics saves the validation metrics to a file.</p>
<p>The RandomPlayer class represents a player that generates random answers.Its primary method, generate_random_answer, returns a random validity answer, either "valid" or "invalid."</p>
<p>The LLMPlayer class is designed for the GPT models to generate answers.It has attributes like model, which specifies the GPT-model type used, and temperature, which sets the temperature for the model's responses.Its methods include generate_llm_answer_validity, which produces a validity game answer using the LLM, and generate_llm_answer_list, which generates a list of Lego pieces using the LLM.</p>
<p>The main function orchestrates the simulation of games, including the generation of constructs, validation, and saving of results.It initializes both the game and visualizer, creates directories for saving experiment results, and defines the number of experiments, models, and temperatures.The function conducts two main types of experiments:</p>
<p>Validity Testing: In this experiment, constructs are generated (either valid or invalid), and players (either random or LLM) are asked to validate them.The results and visualizations of these constructs are saved.</p>
<p>Construct Generation: Here, LLM players are prompted to generate valid constructs based on a given prompt.These constructs are then validated, and results and visualizations are recorded.</p>
<p>The results of these experiments are saved to CSV files for further analysis.</p>
<p>The product of these experiments differs slighlty per type of game.We collected results (see Table 3) as well as game state data based on player moves (see Table 2).Single-player games are simplified by setting irrelevant values to 0 (e.g.Player, turn, etc), but the overall format is the same.In the case of LCL, we save the results directly to two dataframes.For the construct experiment we record the model temperature, the model type, the experiment number, if the output is valid or not, the LLM's answer, and if the response is true or false.In the case of the validity experiment, we record the temperature, the model, the experiment number, if the output is valid or not, if the LLM's response is correct or incorrect, and the actual LCL construct.B Three Board Games: Tic-Tac-Toe, Connect-Four, and Battleship</p>
<p>B.1 Prompts</p>
<p>Game Introductory Prompt</p>
<p>Battleship "Battleship is a two-player guessing game where each player has a fleet of ships on a secret grid and then takes turns guessing the locations of the opponent's ships.The objective is to sink all of the opponent's ships by correctly guessing their locations.O's in a board mean that the player selected a square to attack and there was no ship there -it's a miss.Had there been a ship there, instead of an O you would see an X.In your board, an <S> signifies a ship position, and a &lt; &gt; signifies the sea.Your input is just two numbers with a space in between, one for the row (from 0 to <self.board_size-1>) and one for the column (from 0 to <self.board_size-1>),like: 0 0, nothing else.Do not output anything else but the row col values."Tic-Tac-Toe "Tic-Tac-Toe is a two-player game played on a 3x3 grid.Players take turns placing their mark, X or O, in an empty square.The first player to place three of their marks in a horizontal, vertical, or diagonal row wins the game.You will play as player 1, therefore you play with X while your adversary plays with the symbol O.Your input is then a number (from 0 to 2) for the row followed by a space and another number (from 0 to 2) for the column, nothing else.Do not output anything else but the row col values else you lose."</p>
<p>Connect-Four "Connect-Four is a two-player game.The pieces fall straight down, occupying the next available space within a column.The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.In a board, player 1, you, plays with symbol X, while player 2, your opponent, plays with symbol O.Your input is just a number from 0 to 6, nothing else.Do not output anything else but the col value else you lose."Table 4: The three introductory prompts used for the board games in the ChildPlay suite.</p>
<p>B.2 Example</p>
<p>Note that in the case of Connect-Four, a move consists of a singular scalar.A board state is shown after each play.Examples can be found in Fig. 13.The move space in Tic-Tac-Toe corresponds to the full grid and the player selects a specific board square.In contrast, in Connect-Four it is a series of 7 columns.By definition, the player can only select a column and each piece piles on top of each other within it.Accordingly, this was our conceptual and algorithmic representation of the two games.</p>
<p>B.3 Move Mapping</p>
<p>B.3.1 Probability of incorrect moves</p>
<p>The probability of incorrect moves P incorrect as a function of temperature is given by:
P incorrect (T ) = Incorrect Moves at T Total Moves
For Tic-Tac-Toe, Connect Four, and Battleship, the results are as follows: These values indicate that the probability of incorrect moves varies across different games as the temperature changes.For Tic-Tac-Toe, the probability of incorrect moves increases with temperature, suggesting that higher temperatures may lead to less optimal decisions.In Connect Four, however, the probability of incorrect moves decreases as temperature increases, indicating a possible improvement in decision-making or adaptation to the conditions.In Battleship, the probability fluctuates but remains relatively stable.
P incorrect, TTT = [4.</p>
<p>Average Number of Steps</p>
<p>The average number of steps ⟨S⟩ as a function of temperature is calculated by averaging the steps observed at different temperatures:
⟨S⟩ = 1 n n i=1
Steps at T i For Tic-Tac-Toe, Connect Four, and Battleship, the average number of steps are as follows:
⟨S TTT ⟩ = 2.5
⟨S CF ⟩ = 5.0
⟨S BS ⟩ = 6.0
These values show that the average number of steps taken in each game tends to stabilize regardless of temperature changes.This suggests that while the probability of making incorrect moves may fluctuate with temperature, the number of steps taken does not vary significantly.</p>
<p>B.3.2 Tic-Tac-Toe</p>
<p>Below, see the left column for the model's moves, and the right column for the random player's moves.</p>
<p>B.3.3 Connect-Four</p>
<p>Below, see the left column for the model's moves, and the right column for the random player's moves.</p>
<p>B.3.4 Battleship</p>
<p>Below, see the left column for the model's moves, and the right column for the random player's moves.</p>
<p>B.5 Prompting GPT About Optimal Play</p>
<p>Game Explanation Tic-Tac-Toe Tic-Tac-Toe is a two-player game played on a 3x3 grid.Each player takes turns marking a square with their symbol (X or O), aiming to get three of their symbols in a row, column, or diagonal.To play optimally, prioritize securing the center square and blocking opponent's winning moves.</p>
<p>Battleship</p>
<p>Battleship is a two-player game where players hide ships on a grid and take turns guessing their opponent's ship locations.The goal is to sink all of the opponent's ships.To play optimally, start by targeting areas with higher probabilities of containing a ship and strategically target adjacent squares after a hit to maximize efficiency.</p>
<p>Connect Four</p>
<p>Connect Four is a two-player game played on a 6x7 grid.Players drop colored discs into columns, aiming to connect four of their own discs in a row, column, or diagonal.To play optimally, prioritize creating your own winning formations while blocking opponent's potential winning moves.</p>
<p>Table 5: Optimal strategies for playing different games according to GPT-3.5.</p>
<p>Game Explanation Tic-Tac-Toe</p>
<p>Play your first X in a corner to maximize opportunities.If the opponent plays in the center, play the opposite corner.Block your opponent's potential winning moves and always look to create a line of three.</p>
<p>Battleship</p>
<p>Randomize ship placements and start by targeting the center of the grid.Use a checkerboard pattern for efficient searching.Once a ship is hit, focus on the surrounding squares to determine its orientation and sink it.</p>
<p>Connect Four</p>
<p>Start in the center column to maximize opportunities in all directions.Build threats vertically, horizontally, and diagonally, and block the opponent's forming lines.Create multiple threats to force the opponent into a defensive position.</p>
<p>C LCL C.1 Prompts</p>
<p>Validity Testing prompt: "You will receive a description of a Lego structure, for instance, ((x1, y1, 'color1'), (x2, y2, 'color2')), which lists the coordinates and colors of two pieces.A construct is valid if all Lego pieces are connected but not overlapping.A Lego piece is connected through interlocking pegs, not by merely touching sides.Two Lego pieces overlap when they share the same y-coordinate and any part of their length has the same x-coordinate.If the following structure is valid then reply with valid, otherwise reply with invalid (do not justify your answer): <pieces>" Construct Generation prompt: "A description of a Lego structure consists of a list of tuples, ((x1, y1, 'color1'), (x2, y2, 'color2')), where each tuple shows the coordinates and colors of a piece.Such a structure is valid if all Lego pieces are connected but not overlapping.A Lego piece is connected through interlocking pegs, not by merely touching sides.Two Lego pieces overlap when they share the same y-coordinate and any part of their length has the same x-coordinate.Produce a description of a valid structure using <n pieces> Lego pieces.Reply only with the Lego structure description following the format ((x1, y1, 'color1'), (x2, y2, 'color2'), ...), write nothing else but the structure."The prompts written in LaTeX from Fig. 22 and Fig. 23 were used both in the case of GPT-3.5 and GPT-4 in the main text.Notably, these tests are part of the ChildPlay suite.Further tests were conducted but not included in the ChildPlay suite and are illustrated herein.The reason why these tests have not been included in the suite is because they must be written as systematic benchmarks instead of experimental input-output segments.Currently, they stand as illustrative cases of spatial reasoning failure and success that supplement the benchmark but are not aimed at proving the model's capacity either way.They are simply an interesting addition.</p>
<p>D LCL Syntax D.1 Definitions in LCL</p>
<p>A piece P is defined as a tuple P = (l, w, (x, y), c, h) (see Table 7) where:</p>
<ol>
<li>
<p>l is the length of the piece, fixed at 4 units;</p>
</li>
<li>
<p>w is the width of the piece, fixed at 2 units;</p>
</li>
<li>
<p>x − axis corresponds to the position of the studs;</p>
</li>
<li>
<p>y − axis corresponds to layers -the first brick is at layer 0; 5. c is the color of the piece;</p>
</li>
<li>
<p>h is the height of the piece, fixed at 1 unit;</p>
</li>
</ol>
<p>For the sake of brevity, in most of the examples below we omit length (l), color (c), and height (h) since these are set as constants.</p>
<p>D.2 Example</p>
<p>A simple example is found in Fig 25.This is a tower constructed from 3 bricks and is a valid LCL 2 construct.</p>
<p>Figure 25: A valid tower representation in LCL 2 .</p>
<p>Figure 26: A disconnected line of bricks is not a valid construct in LCL 2 : {(0, 0), (4, 0), (8, 0)}.</p>
<p>This sequence forms the construction of a 3-brick line, each brick having a width of 4 units.But since this construction is composed of three columns, one piece P each, it can be broken apart and is not a topological object (each piece can be moved individually).The correct construct with three bricks has many possible solutions.For a centre piece with two pieces on the bottom or two pieces on the top, we find 24 possible solutions.In eq. 1 is the general formula with s being the amount of studs:
f (0) = 0 f (s) = 4 * (s − 1) + f (s − 1)(1)
And its non-recursive form:
f (0) = 0 f (s) = 2(s − 1)s(2)
We show two more simple examples: , and: The "three-in-a-line" can only be loosely interpreted in LCL 2 , due to rule (2) -that pieces cannot be moved independently from the rest of the model.For this reason, one can imagine many more structures that loosely fall under the definition of a "line" or "wall", for example: Or even a stair-like structure:</p>
<p>Figure 30: Stair-like construct for the requested "line".This is also a valid construct in LCL 2 : {(0, 0), (2, 1), (4, 2)}.</p>
<p>A humanoid could also be easily represented in LCL 2 as:      Essentially, both GPT-3.5 and GPT-4 were not far from the expected target, but failed to respect LCL 2 rules in most cases.For example, pieces are found in an impossible superposition in Fig. 32a (red piece is in the same position as yellow piece), 34b (blue piece is in the same position as yellow piece), and 35b (red piece is in the same position as middle yellow pieces).In Fig. 35a, GPT-3.5 erroneously swapped the middle yellow piece with the red piece and the blue pieces with the bottom yellow pieces, even though it first declared in plain English the correct organisation of the 6 pieces.The positive result is that models manage to assemble a tower of three pieces and GPT-4 was capable of assembling a triangle (see Table 9).None of the models recognised that they were asked an impossible task, namely building a triangle with only 5 pieces (see Fig. 32).</p>
<p>Model Responses</p>
<p>Category N(P) GPT-3.5 GPT-4</p>
<p>D.3 Small Dataset for Future Experiments</p>
<p>The dataset defined herein contains several example prompts that are more complex and do not follow the 2x4 assumption, each consisting of a request followed by a LEGO kit of fewer than 15 pieces to which the agent is bound.</p>
<p>LEGO Kits</p>
<p>Apple</p>
<p>Possible prompt: "Construct a LEGO apple with a mix of red and green colors, resembling a typical apple shape using slopes and bricks."</p>
<p>• Green Slope 45  "Draw a cross in a 5 by 5 grid, with horizontal and vertical axes of 3 units of length with the center at (3,3)."Table 10: Introductory and correction prompts for identifying and detailing specific geometrical shapes in a grid environment.</p>
<p>In the shape detection tests, both GPT-3.5 and GPT-4 demonstrated limited comprehension and ability to accurately interpret or draw shapes.When tasked with drawing a cross (see Fig. 36), GPT-3.5 and GPT-4 initially failed to produce a correct cross, but slightly improved after feedback.In Table 11, both models often misidentified or misrepresented the shapes requested, such as describing a circle as a "diamond shape" (GPT-3.5)and an "arrow pointing upwards" (GPT-4).Additionally, neither model could fully comprehend geometric properties, frequently providing incorrect dimensions and centers for squares, triangles, and crosses.</p>
<p>F Conway's Game of Life -Soft Experiment</p>
<p>We were interested in seeing if LLMs could predict states based on very simple rules.Initially we tested this by generating sequential states in Conway's Game of Life and feeding them to GPT-3.5 and GPT-4 while prompting the models for two things: the rules and the next state.We do not include this as a benchmark because the experiment required hand segmentation of areas of interest in simulated states and repeatability was not achieved except for in the patterns of interest, of course.We hope to include a version of this task later on in the ChildPlay suite.In Conway's Game of Life, a cellular automaton devised by mathematician John Horton Conway and introduced to the public by Gardner, cells perpetuate or perish given certain simple rules.We simulated rule B3/S23 also known as "blinking" in varying board sizes.In this rule, a cell is born if it has exactly three live neighbours and survives if it has either two or three live neighbours, dying otherwise.Rule B3/S23 is known to generate a behaviour exhibited by certain configurations of cells that alternate between two or more states over successive generations.These configurations are generally known as oscillators, which are a type of pattern in the game that returns to its initial configuration after a fixed number of generations, repeating indefinitely.In our case, we prompted the models with three configurations of increasing complexity.One consisting of three iterations of the oscillator (3 horizontal cells and 3 vertical cells), another with the same oscillator in the same position but now noise was added in the shape of 4 cells that did not change.Finally, a 5-iteration long pattern was used showcasing cells moving in a circle formation outwardly across the board.These were obtained by repeated simulation using custom software developed specifically for this study available through Github.</p>
<p>F.1 Prompts</p>
<p>Test</p>
<p>Introductory Prompt Second Prompt</p>
<p>Test 1: GPT-4 and GPT-3.5</p>
<p>"The following was produced using Conway's Game of Life, namely rule 'survival': [2,3], 'birth': [3].Print the state prior to the first one and the one after the last one."</p>
<p>Test 2: GPT-4 and GPT-3.5In the Conway's Game of Life tests, neither GPT-3.5 nor GPT-4 managed to consistently identify or predict the evolving patterns correctly.Table 13 summarizes their performance, where both models only succeeded in identifying a simple blinking pattern.In more complex scenarios involving patterns before or after a given state, both models returned incorrect responses.Even when explicitly provided with the game's rules, GPT-3.5 and GPT-4 failed to accurately predict the next pattern or the pattern before.</p>
<p>Test</p>
<p>Description Query1 GPT-3.</p>
<p>G Other Models</p>
<p>We prompted the following models with the Tic-Tac-Toe prompt in Appendix B.1.
X 3 X 5 X 1 O 2 O 6 O4
Human (O) vs Claude (X)</p>
<p>Claude starts with a center move but fails to capitalize on it, leading to a loss.The model places Xs on the top corners but does not create a winning line and does not block the human's winning move.
X 3 X 5 X 1 O 2 O 6 O 4
Human (O) vs Mistral (X)</p>
<p>Mistral mirrors Claude's strategy, starting in the center but failing to follow through effectively.The game ends in a loss too for the same reasons as above -a missed block.
X 1 X 3 X 5 O 2 O 6 O4
Human (O) vs Gemini (X)</p>
<p>Gemini deviates slightly by focusing on a top-row strategy but still fails to secure a win.The model also missed the blocking move.</p>
<p>All three models fail to block the winning move when they could clearly have done so.</p>
<p>Table 14 shows the type of non-answers we obtained from a diverse set of models from Huggingface picked at random from the first 2 pages from the models listing page after sorting on most downloads.</p>
<p>Model Name Model Answer</p>
<p>microsoft/phi-2 [31] "You will play as player 2, therefore you play with the symbol O while your adversary."JackFram/llama-68m [22] +-+-+-+-+-+-+-+-+-+ω 1.</p>
<p>openai-community/gpt2medium [36] "The first player to guess correctly wins."sshleifer/tiny-gpt2 [41] "LLM's turn to guess.TinyLlama/TinyLlama-1.1B-Chat-v1.0[47] def play_tic_tac_toe(): mistralai/Mixtral-8x7B-Instruct-v0.1 [33] Thank you for the instructions.I understand that we are playing a game of Tic-Tac-Toe, and I am player 1, using the mark 'X'.We will be taking turns entering our moves as row and column numbers.The game continues until one player wins by placing three of their marks in a horizontal, vertical, or diagonal row.Here's an example of how the input could look like for my first move: 0 1 This corresponds.microsoft/DialoGPTmedium [30] It npmat, unless broken wall... microsoft/Phi-3-mini-4kinstruct [32] Welcome to the Tic-Tac-Toe challenge!As player 1 with the mark X, you have the first turn.Here's a simple strategy you might use to begin the game: Place your X in the center square, which is row 1, column 1. Output: "0 1".By placing your X in the center, you maximize your chances of creating a winning combination, as the center square is a. distilbert/distilgpt2 [15] "LLM's turn to guess.""LLM's turn to guess.""LLM's turn to guess."openai-community/gpt2 [35] "The game is played on a 3x3 grid.Players take turns placing their mark."Falcon-7B-Instruct [51] Here's a Python code that should work:</p>
<p>Figure 1 :
1
Figure 1: Left: A valid humanoid construct in LCL 2 .Right: A standard LEGO piece in LCL 2 .</p>
<p>Figure 2 :
2
Figure 2: Initial board states presented to LLMs.Left: Tic-tac-toe board.Middle: Connect-four board.Right: Battleship board.Ship positions and lenghts are randomised with every initialisation.</p>
<p>Figure 3 :
3
Figure 3: Matrices used in the Game of Shapes.Left: Square.Middle: Triangle.Right: Cross.</p>
<p>GPT- 3
3</p>
<p>Figure 4 :
4
Figure 4: GPT models performance against a random player at the Tic-Tac-Toe game.</p>
<p>Figure 5 :
5
Figure 5: Tic-tac-toe: Missed wins and missed blocks.</p>
<p>Figure 6 :
6
Figure 6: GPT models performance against a random player at the Connect-Four game.</p>
<p>Figure 7 :
7
Figure 7: Connect-Four: Missed wins and missed blocks.</p>
<p>Figure 8 :
8
Figure 8: GPT models performance against a random player at Battleship.</p>
<p>Figure 9 :
9
Figure 9: Results for the Shapes game.Left: GPT-3.5.Right: GPT-4.</p>
<p>Figure 10 :
10
Figure 10: Example structures generated for the LCL validity test and structures generated by GPT-3.5 and GPT-4 for the construction test.</p>
<p>Figure 11 :
11
Figure 11: LCL results after 100 runs with 50/50 valid/invalid examples for the validity test and 100 experiments per temperature per model for the construction test using 3 pieces.</p>
<p>Figure 12 :
12
Figure 12: Minimax (agent) vs. random player -wins and losses on the left and a heatmap of move counts over the 1000 games for the minimax player on the right.</p>
<p>(a) Player X has won the Connect-Four game.(b) Players tied at the Tic-Tac-Toe game.(c) A player has won the Battleship game.</p>
<p>Figure 13 :
13
Figure 13: Examples of final board states in the three different board games.</p>
<p>(a) t = 0. (b) t = 0.5.(c)t = 1.(d) t = 1.5.</p>
<p>Figure 14 :
14
Figure 14: Heatmap of model GPT-3.5'smoves for the tic-tac-toe game, where 't' is temperature.</p>
<p>(a) t = 0. (b) t = 0.5.(c) t = 1.(d) t = 1.5.</p>
<p>Figure 15 :
15
Figure 15: Heatmap of model GPT-4's moves for the tic-tac-toe game, where 't' is temperature.</p>
<p>(a) t = 0. (b) t = 0.5.(c)t = 1.(d) t = 1.5.</p>
<p>Figure 16 :
16
Figure 16: Heatmap of model GPT-3.5'smoves for the connect-four game, where 't' is temperature.</p>
<p>(a) t = 0. (b) t = 0.5.(c) t = 1.(d) t = 1.5.</p>
<p>Figure 17 :
17
Figure 17: Heatmap of model GPT-4's moves for the connect-four game, where 't' is temperature.</p>
<p>(a) t = 0. (b) t = 0.5.(c)t = 1.(d) t = 1.5.</p>
<p>Figure 18 :
18
Figure 18: Heatmap of model GPT-3.5'smoves for the battleship game, where 't' is temperature.</p>
<p>(a) t = 0. (b) t = 0.5.(c) t = 1.(d) t = 1.5.</p>
<p>Figure 19 :
19
Figure 19: Heatmap of model GPT-4's moves for the battleship game, where 't' is temperature.</p>
<p>Figure 20 :
20
Figure 20: Heatmap of model GPT-3.5'sdecisions for the shapes game, where 't' is temperature.</p>
<p>(a) t = 0. (b) t = 0.5.(c) t = 1.(d) t = 1.5.</p>
<p>Figure 21 :
21
Figure 21: Heatmap of model GPT-4's decisions for the shapes game, where 't' is temperature.</p>
<p>Figure 22 :
22
Figure 22: Validity testing prompt.</p>
<p>Figure 23 :
23
Figure 23: Construct generation prompt.</p>
<p>Figure 27 :
27
Figure 27: A possible representation of the requested "line" as a valid construct in LCL 2 : {(0, 0), (4, 0), (2, 1)}.</p>
<p>Figure 28 :
28
Figure 28: Another possible representation of the requested "line" as a valid construct in LCL 2 : {(0, 0), (−2, 1), (2, 1)}.</p>
<p>Figure 29 :
29
Figure 29: Another possible construct for the requested "line".This is also a valid construct in LCL 2 : {(0, 0), (−2, 1), (4, 1)}.</p>
<p>Figure 31 :
31
Figure 31: A possible representation of a humanoid as a valid construct in LCL 2 : {(0, 0), (4, 0), (2, 1), (0, 2), (4, 2), (2, 3)}.</p>
<p>Figure 32 :
32
Figure 32: Model responses to the query: "Make a triangle with 5 bricks.",randomised colours.This is impossible to satisfy.</p>
<p>(a) GPT-3.5.(b) GPT-4.</p>
<p>Figure 33 :
33
Figure 33: Model responses to the query: "Make a triangle with 6 bricks.",randomised colours.</p>
<p>Figure 34 :
34
Figure 34: Model responses to the query: "You have 6 pieces.Build a humanoid figure.",randomised colours.</p>
<p>Figure 35 :
35
Figure 35: Model response to the query: "Imagine it's bart simpson.You have three yellow pieces, one for the head, two for the arms, one red for the torso, and two blue pieces for the legs."-colours specified.</p>
<p>(a) GPT-3.5's,before receiving feedback.(b)GPT-3.5's, after receiving feedback.(c)GPT-4's,before receiving feedback.(d)GPT-4's,after receiving feedback.</p>
<p>Figure 36 :
36
Figure 36: Querying the models to draw a cross with side length of 3 on a 5 by 5 matrix with center at (3, 3).</p>
<p>(a) GPT-3.5'sguess of the first iteration after seeing the three iterations that follow.(b) First iteration of the 5 iterations generated using rule B3/S23 of Conway's Game of Life.</p>
<p>( c )
c
GPT-3.5's guess of the final iteration after seeing the three iterations prior.(d) Final iteration of the 5 iterations generated using rule B3/S23 of Conway's Game of Life.</p>
<p>Figure 38 :
38
Figure 38: Prompting GPT-3.5 for the first and last iteration of a 5-sequence long sample from rule B3/S23 of Conway's Game of Life after showing the middle 3 iterations.</p>
<p>Figure 39 :F. 3 2 Figure 40 :
393240
Figure 39: Prompting GPT-4 for the first and last iteration of a 5-sequence long sample from rule B3/S23 of Conway's Game of Life after showing the middle 3 iterations.The model failed to produce an image for the last iteration and only the first iteration.This is after several trial runs.</p>
<p>(a) GPT-3.5'sguess of the iteration after seeing the first two iterations of test 2. (b) GPT-3.5'sguess of the iteration after seeing the first two iterations of test 2 and receiving feedback including the clue about rule B3/S23.</p>
<p>(a) GPT-4's guess of the iteration after seeing the first two iterations of test 2. (b) GPT-4's guess of the iteration after seeing the first two iterations of test 2 and receiving feedback including the clue about rule B3/S23.</p>
<p>F. 4 3 Figure 43 :
4343
Figure 43: Test 3 consisting of three iterations of a 'blinking' or 'flashing lights' object generated by rule B3/S23 plus an inert mass.</p>
<p>(a) GPT-3.5'sguess of the final iteration after seeing the two iterations prior.(b) GPT-3.5'sguess after seeing the first two iterations of test 3 and receiving feedback including the clue about rule B3/S23.</p>
<p>(a) GPT-4's guess of the iteration after seeing the first two iterations prior.(b) GPT-4's guess after seeing the first two iterations of test 3 and receiving feedback including the clue about rule B3/S23.</p>
<p>" factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors.TinyLlama/TinyLlama-1.1B-Chat-v1.0[47] def play_tic_tac_toe():</p>
<p>Table 1 :
1
Win rates of LLMs in Battleship, Tic-Tac-Toe, and Connect-Four at different temperatures.</p>
<p>Table 2 :
2
Game logs with the details of a specific game state.The 'Player' variable (A) indicates the player number, 'Move' ([X, Y]) represents the coordinates of the move made, and 'Turn' (B) signifies the turn number in the game.
Variable ValuePlayerAMove[X, Y]TurnBVariableValueP1 WinsCP2 WinsDTiesEP1 Wrong MovesFP2 Wrong MovesG</p>
<p>Table 3 :
3
Results summarizes the outcomes of a series of games.'P1 Wins' (C) and 'P2 Wins' (D) indicate the number of games won by Player 1 and Player 2, respectively.'Ties' (E) shows the number of games that ended in a tie.'P1 Wrong Moves' (F) and 'P2 Wrong Moves' (G) represent the number of invalid moves made by Player 1 and Player 2, respectively.</p>
<p>Table 6 :
6
Optimal strategies for playing different games according to GPT-4.</p>
<p>Table 9 :
9
Comparison of Responses by GPT-3.5 and GPT-4.
Tower3CorrectCorrectImpossible Triangle 5Incorrect IncorrectTriangle6Incorrect CorrectHumanoid6Incorrect IncorrectBart Simpson6Incorrect Incorrect</p>
<p>Table 11 :
11
Comparison of Responses by GPT-3.5 and GPT-4 in Shape Detection Tests.
TestQueryCorrect Answer GPT-3.5 Response GPT-4 ResponseCircleShapeCircle"diamond shape""arrow pointing upwards"Center(7,7)"(7,7)""(7,7)"SquareShapeSquare"square""'O'"Dimensions(3,4)"(4,4)""(3,3)"Triangle ShapeTriangle"diamond""arrow pointing upwards"Base Length 7 units"7""6"CrossShapeCross"square"" 'plus' sign (+)"Center(5,5)"(7,7)""(6,5)"Line Lengths 5"5""4"</p>
<p>Table 12 :
12
"I will show you two iterations of Conway's game of life.The first generated the second.You must deduce the survival and birth rules.You must only print these rules, nothing else.Understood?" Prompts for tests related to Conway's Game of Life.
"Print the next pattern. The following"That's wrong. The rule isare two iterations of the game of life byRule: 'Blinking': 'survival':Conway. You cannot use code"[2, 3], 'birth': [3]. Tryagain."Test 3: GPT-4 and"Print the next pattern. The following"That's wrong. The rule isGPT-3.5are two iterations of the game of life byRule: 'Blinking': 'survival':Conway"[2, 3], 'birth': [3]. Tryagain."Requesting Rules:GPT-3.5 and GPT-4</p>
<p>Table 13 :
13
Evaluation of GPT-3.5 and GPT-4 responses in Conway's Game of Life rest scenarios.
5 Response GPT-4 Response
Queries conducted with the explicit rule revealed.
Parameter DescriptionValue lLength of the piece 4 units w Width of the piece 2 units (x, y)Position of the studs (x-axis), layers (y-axis) Var cColour of the piece Var hHeight of the piece 1 unit Table7: Definition of a Piece P A construction, M , is then a valid construction in LCL 2 if and only if it follows the rules:1. P = (4, 2, (x, y), c, 1) 2. M is composed entirely by P pieces (Φ = P ); 3. Every piece P must be connected to at least one other piece P; 4. M is symmetric along the line crossing the 2 by 4 pieces, between its pegs, along the piece's longest side;5. Pieces in the construct can only be manipulated horizontally in n * pi rotations, with n ∈ Z (note that this makes width irrelevant);6.The position of a piece is defined by its left-most pair of studs;7. M begins with a piece P at coordinates (0,0); 8.All pieces placed in layer n must be placed before any piece is placed in layer n + 1;Consider constructing a line using three bricks (we omit height h since it is a constant, with value equal to 1).This is counter-intuitive, but note that a line cannot be represented as inFig 26,because the pieces are disconnected.LCL 2 : ((0, 0), (4, 0), (8, 0)) is then an example of what one expects to see as representing a line, but it is not valid in LCL.Because the pieces are disconnected from eachother, they just lay next to eachother, one after another in a row.Instead, ((0, 0), (4, 0), (2, 1)), or ((0, 0), (−2, 1), (2, 1)), or even ((0, 0), (−2, 1), (4, 1)) would be valid constructs.Subsequently, both models were prompted with several additional requests that have not been integrated in the suite yet (see Table8).For these experiments, the definition of LCL was provided to the model and it was accompanied by the prompt in Fig.24.Prompt: "I will give you a number of pieces, I will ask you for a shape and you'll output the coordinates per piece to form such a shape.It must be valid in LCL." Tower Construction "Produce now a tower with 3 bricks."Table8: Sequence of building prompts.• Green Plate 2 x 8 -Code: 3034 (Quantity: 1)• Light Bluish Gray Arch 1 x 4 x 2 -Code: 6182 (Quantity: 2)• Sand Green Brick 1 x 2 -Code: 3004 (Quantity: 2)• Light Bluish Gray Brick 1 x 2 -Code: 3004 (Quantity: 2)• Dark Bluish Gray Brick 1 x 2 -Code: 3004 (Quantity: 2)• Light Bluish Gray Brick 2 x 2 -Code: 3003 (Quantity: 1)• Reddish Brown Brick, Round 1 x 1 Open Stud -Code: 3062b (Quantity: 2)DinghyPossible prompt: "Assemble a small LEGO dinghy with a white sail and a mast."• Dark Tan Plate 2 x 4 -Code: 3020 (Quantity: 1)• Tan Slope, Inverted 33 3 x 2 with Flat Bottom Pin and Connections -Code: 3747b (Quantity: 1)• White Slope 45 2 x 2 -Code: 3039 (Quantity: 3)• White Brick 2 x 2 -Code: 3003 (Quantity: 1)• White Brick 1 x 2 -Code: 3004 (Quantity: 1)• Tan Brick 2 x 3 -Code: 3002 (Quantity: 1)• Reddish Brown Brick, Round 2 x 2 with Axle Hole -Code: 3941 (Quantity: 1)Blue BotPossible prompt: "Construct a LEGO robot with a humanoid structure, featuring a distinguishable head, body, arms, and legs."• Medium Blue Brick 2 x 2 -Code: 3003 (Quantity: 1)• Brick, Modified 2 x 3 with Curved Top -Code: 6215 (Quantity: 1)• Brick 2 x 4 -Code: 3001 (Quantity: 1)• Brick 1 x 2 -Code: 3004 (Quantity: 2)• Brick, Round 2 x 2 with Grille -Code: 92947 (Quantity: 1)• Plate 2 x 2 -Code: 3022 (Quantity: 1)• Brick, Modified 1 x 2 with Studs on 1 Side -Code: 11211 (Quantity: 1)• Brick 1 x 2 without Bottom Tube -Code: 3065 (Quantity: 1)• Tile 1 x 1 Round -Code: 98138 (Quantity: 1)• Brick, Round 2 x 2 Dome Top, with Bottom Axle Holder -Code: 553c (Quantity: 1)Toy CarPossible prompt: "Build a LEGO toy car with a compact design, featuring wheels, and a sloped windshield."• Brick 2 x 6 -Code: 2456 (Quantity: 1)• Slope 2 x 2 45°-Code: 3039 (Quantity: 1)• Brick 1 x 2 without Bottom Tube -Code: 3065 (Quantity: 1)• Brick 1 x 2 -Code: 3004 (Quantity: 1)• Plate 2 x 2 with Wheel Holders -Code: 4600 (Quantity: 2)• Wheel 8mm D. x 6mm with Slot -Code: 34337 (Quantity: 4)• Tire Offset Tread Small -Band Around Center of Tread -Code: 87414 (Quantity: 4)GoldfishPossible prompt: "Create a LEGO goldfish with fins and tail, featuring elements for eyes."• Brick 2 x 4 -Code: 3001 (Quantity: 2)• Brick 1 x 2 with Pin Hole -Code: 3700 (Quantity: 1)• Brick, Modified 1 x 2 with Studs on 1 Side -Code: 11211 (Quantity: 2)• Brick 2 x 3 -Code: 3002 (Quantity: 1)• Slope 45°2 x 2 -Inverted -Code: 3660 (Quantity: 1)• Slope 2 x 1 -45°-Code: 3040 (Quantity: 4)• Tile 1 x 1 Round with Eye Pattern -Code: 98138pb007 (Quantity: 2)• Slope 30°1 x 2 x 2/3 -Code: 85984 (Quantity: 1)Baby ElephantPossible prompt: "Assemble a LEGO baby elephant with a focus on its trunk, ears, and body structure."• Brick 2 x 6 -Code: 2456 (Quantity: 1)• Brick 1 x 2 -Code: 3004 (Quantity: 3)• Brick 1 x 4 -Code: 3010 (Quantity: 1)• Brick 1 x 1 with Stud on 1 Side -Code: 87087 (Quantity: 2)• Tile 1 x 1 Round with Eye Pattern -Code: 98138pb027 (Quantity: 2)• Brick 2 x 4 -Code: 3001 (Quantity: 1)FlamingoPossible prompt: "Construct a LEGO flamingo with pink bricks, designed to stand on one leg and feature a long neck and beak."• Brick 1 x 2 -Code: 3004 (Quantity: 3)• Brick, Modified 2 x 3 with Curved Top -Code: 6215 (Quantity: 2)• Brick 1 x 1 with Stud on 1 Side -Code: 87087 (Quantity: 2)• Plate 2 x 3 -Code: 3021 (Quantity: 1)• Slope 2 x 2 -45°-Code: 3039 (Quantity: 1)• Tile 1 x 1 Round with Eye Closed Pattern -Code: 98138pb028 (Quantity: 2)Twin Engine AirplanePossible prompt: "Build a LEGO twin-engine airplane, with a body, wings, and a tail."• Plate 2 x 8 -Code: 3034 (Quantity: 2)• Brick 1 x 2 x 2 with Inside Stud Holder -Code: 3245c (Quantity: 1)• Brick, Modified 1 x 1 x 1 2/3 with Studs on 1 Side -Code: 32952 (Quantity: 2)• Brick 1 x 4 with 4 Studs on 1 Side -Code: 30414 (Quantity: 2)• Slope 2 x 2 -45°-Code: 3039 (Quantity: 1)• Brick 1 x 2 without Bottom Tube -Code: 3065 (Quantity: 1)
Tic-tac-toe: Understanding the minimax algorithm. H Shahd, Essam Alkaraz, Neveen S El-Seidy, Morcos, 2020</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. BIG bench authors. 2023</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 032021</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>The development of intelligence in children. Alfred Binet, Theodore Simon, 1961</p>
<p>Hal Daumé III au2, and Hanna Wallach. Language (technology) is power: A critical survey of "bias" in nlp. Lin Su, Solon Blodgett, Barocas, 2020</p>
<p>Stochastic parrots or intelligent systems? a perspective on true depth of understanding in llms. Ali Borji, 10.2139/ssrn.4507038SSRN Electronic Journal. 012023</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, CoRR, abs/2005.141652020</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, ArXiv, abs/2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023257663729</p>
<p>Human Cognitive Abilities: A Survey of Factor-Analytic Studies. John B Carroll, 10.1017/CBO97805115713121993Cambridge University Press</p>
<p>Theory of fluid and crystallized intelligence: A critical experiment. Raymond B Cattell, 10.1037/h0046743Journal of Educational Psychology. 5411963</p>
<p>Syntactic Structures. Mouton and Co. Noam Chomsky, 1957The Hague</p>
<p>Palm: scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sashank Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Agrawal, J. Mach. Learn. Res. Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel1532-4435241mar 2024</p>
<p>Flocks of stochastic parrots: Differentially private prompt learning for large language models. Haonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch, ArXiv, abs/2305.155942023</p>
<p>A survey of text classification with transformers: How wide? how large? how long? how accurate? how expensive? how safe?. John Fields, Kevin Chovanec, Praveen Madiraju, IEEE Access. 122668245052024</p>
<p>Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines. L Floridi, Massimo Chiriatti, 202030228954221</p>
<p>A framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Niklas Mcdonell, Chris Muennighoff, Jason Ociepa, Laria Phang, Hailey Reynolds, Aviya Schoelkopf, Lintang Skowron, Eric Sutawika, Anish Tang, Ben Thite, Kevin Wang, Andy Wang, Zou, 12 2023</p>
<p>Why g matters: The complexity of everyday life. Linda S Gottfredson, S0160-2896(97)90014-3Intelligence. 0160-28962411997Special Issue Intelligence and Social Policy</p>
<p>Stochastic parrots looking for stochastic parrots: Llms are easy to fine-tune and hard to detect with other llms. Da Silva, Gameiro Henrique, Andrei Kucharavy, Rachid Guerraoui, 2023arXiv</p>
<p>Infinite Monkeylab42, ARC Prize 2024: ARC-AGI Competition. 2024</p>
<p>Jackfram/llama-68m. Jackfram, 2024</p>
<p>The g factor: The science of mental ability. A R Jensen, 1998PraegerWestport, CT</p>
<p>Arc prize. Kaggle, 2024. 2024</p>
<p>Model-based reinforcement learning for atari. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski, 2020</p>
<p>Assessing the strengths and weaknesses of large language models. Shalom Lappin, 10.1007/s10849-023-09409-xJournal of Logic, Language and Information. 332023</p>
<p>Testing spatial reasoning of large language models: the case of tic-tac-toe. Davide Liga, Luca Pasetto, 2023</p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 2022</p>
<p>Formal languages and neural models for learning on sequences. William Merrill, International Conference on Graphics and Interaction. 2023</p>
<p>. Mistralai, Mistralai, /mixtral-8x7b-instruct-v0.1, 2024</p>
<p>Adversarial nli: A new benchmark for natural language understanding. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, 2020</p>
<p>openai-community/gpt2-medium. 2024OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Squad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 2016</p>
<p>Are emergent abilities of large language models a mirage?. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, 2023</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver, 10.1038/s41586-020-03051-4Nature. 1476-46875887839December 2020</p>
<p>Sam Shleifer, sshleifer/tiny-gpt2. 2024</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis, 2017</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard De Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, D Kaustubh, Kevin Dhole, Kevin Gimpel, Kory Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mcdonell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros Colón, Metz ; Maheen, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Maru, Jose Ramírez, Marie Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Mátyás Hagen, Medina Schubert, Melody Orduna Baitemirova, Melvin Arnaud, Michael A Mcelrath, Michael Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michał Strube, Michele Swędrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mitch Suzgun, Mo Walker, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T Varma, Nanyun Peng, Nathan A Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Niveditha, S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi ; Ryan Teehan, Rylan Yang, Sahib Singh, Saif M Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, R Samuel, Samuel S Bowman, Sanghyun Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Shyam Toshniwal, Upadhyay, Shyamolima, Siamak Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Reddy, Priscilla Sneha, Soo-Hwan Makini, Spencer Lee, Sriharsha Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Ermon, Stephanie Biderman, Stephen Lin, Steven T Prasad, Stuart M Piantadosi, Summer Shieber, Svetlana Misherghi, Swaroop Kiritchenko, Mishra ; Xinran, Xinyi Zhao, Xudong Wu, Yadollah Shen, Yair Yaghoobzadeh, Yangqiu Lakretz, Yasaman Song, Yejin Bahri, Yichi Choi, Yiding Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Ziyi Wang, Wu, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve. Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Omer Levy, Owain Evans, Pablo Antonio Moreno Casares; Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto; Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton ChangTe-Lin Wu2023Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu. Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong,. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</p>
<p>Analysis of minimax algorithm using tic-tac-toe. Bala Swaminathan, Ekke Vaishali, Subashrits, 2020</p>
<p>On language: The diversity of human language-structure and its influence on the mental development of mankind. by wilhelm von humboldt. Robinson Paul, Sweet, Historiographia Linguistica. 161703690591989translated by peter heath</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>. Tinyllama, Tinyllama, /tinyllama-1.1b-chat-v1.0, 2024</p>
<p>Benchmarking large language model (llm) performance for game playing via tic-tac-toe. Oguzhan Topsakal, Jackson Harper, 10.3390/electronics13081532Electronics132024</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023</p>
<p>. A M I Turing, -Computing Machinery And Intelligence, Mind, Lix, 10.1093/mind/LIX.236.43310 1950</p>
<p>Falcon-7b-instruct. Tii Uae, 2024</p>
<p>Large language models: The need for nuance in current debates and a pragmatic perspective on understanding. M A Bram, Tom Van Dijk, Marco R Kouwenhoven, Max J Spruit, Van Duijn, 2023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, 2017</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 2019</p>
<p>The Measurement of Adult Intelligence. David Wechsler, 10.1037/11329-0001944Williams &amp; Wilkins Co3rd edition</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, CoRR, abs/2201.119032022</p>
<p>General intelligence, objectively determined and measured. Wm R Wright, 1904</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 2019</p>
<p>Causal parrots: Large language models may talk causality but are not causal. Matej Zečević, Moritz Willig, Devendra Singh Dhami, Kristian Kersting, 2023</p>
<p>Test 1 (a) First iteration of the 5 iterations generated using rule B3/S23 of Conway's Game of Life. (b) Second iteration of the 5 iterations generated using rule B3/S23 of Conway's Game of Life. 2</p>
<p>Third iteration of the 5 iterations generated using rule B3/S23 of Conway's Game of Life. (d) Fourth iteration of the 5 iterations generated using rule B3/S23 of Conway's Game of Life. (e) Final iteration of the 5 iterations generated using rule B3/S23 of Conway's Game of Life. Figure. 37Sample taken from 100 iterations of rule B3/S23</p>            </div>
        </div>

    </div>
</body>
</html>
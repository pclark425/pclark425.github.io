<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8398 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8398</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8398</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-8c7846c9805834dbe2fb0c8f48253b8d65b79d6a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8c7846c9805834dbe2fb0c8f48253b8d65b79d6a" target="_blank">Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks, is introduced and an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles is proposed.</p>
                <p><strong>Paper Abstract:</strong> We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8398.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8398.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (fine-tuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-7B model instruction-fine-tuned on ~1M synthetic arithmetic examples (with LoRA) that attains state-of-the-art performance on elementary integer arithmetic by leveraging supervised fine-tuning and a human-interpretable chain-of-thought decomposition for hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned LLaMA-7B using LoRA (r=64, alpha=64, dropout 0.05) on ~1M synthetic arithmetic QA pairs with many instruction templates; fine-tuned for one epoch in many experiments (batch size 128, lr 0.0003).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction up to 16-digit numbers (direct answers zero-shot); multiplication and division (multi-digit) using generated Chain-of-Thought (CoT) decomposition; constrained product/dividend digit limits specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Relies on digit-level consistent tokenization from LLaMA (each digit as a token) enabling digit-by-digit learning; for hard composite tasks uses an explicit, human-interpretable CoT decomposition into learnable sub-tasks (extraction, split, expansion via distributive law, n-digit×1-digit products, and term-by-term addition / slow subtraction loops for division).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Supervised instruction fine-tuning on synthetic dataset; LoRA parameter-efficient tuning; ablation studies removing individual CoT steps; extrapolation tests on digit counts beyond training; simplified synthetic environment to probe learnability of sub-tasks; few-shot prompting experiments with GPT-4 for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Exact String Match / Digit Match percentages (selected entries from Table 3): ADD: e.g. 16D+16D 97.6% / 99.7%; 16D+8D 97.1% / 99.6%; MUL: 6D×6D 96.8% / 99.5%; 4D×8D 88.1% / 97.8% (CoT used for multi-digit multiplication); DIV: 12D÷6D 89.3% / 93.5%; 6D÷3D 94.1% / 96.1%. Zero-shot Goat-7B matches or surpasses PaLM-540B on BIG-bench arithmetic according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Without CoT, exact-match accuracy on multi-digit multiplication/division is effectively zero; limited extrapolation to inputs outside training distribution (accuracy degrades as digits exceed trained max); some special-case patterns (e.g., one operand has only one non-zero digit) are handled without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation studies show (for multiplication) the 'adding term by term' step is crucial while 'split'/'expand' less so; comparison across LLMs and tokenization table shows LLaMA's per-digit tokenization correlates with its ability to learn addition/subtraction directly; training/validation loss and rapid convergence in task-specific fine-tuning provide behavioral evidence that supervised learning of patterns, enabled by consistent tokenization, produces direct-answer capability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Multi-digit multiplication/division remain unlearnable for direct answers without CoT (even with fine-tuning). Model extrapolation to larger-than-trained digit lengths is limited (performance falls off starting at >16 digits given their training). Some subtasks can be overfit (e.g., exhaustive 2-digit×2-digit enumeration) but are inefficient to learn without intermediate supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8398.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8398.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (base model used for Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source pretrained transformer family used as the basis for Goat; notable here for tokenization that splits each digit into a separate token, enabling consistent digit representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (7B in Goat experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer family (LLaMA) trained on large public corpora; the work uses the 7B variant as the base for instruction fine-tuning with LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same integer arithmetic suite as Goat after fine-tuning (addition, subtraction, multiplication, division); base LLaMA examined for tokenization properties and learnability of sub-tasks after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Consistent tokenization: each digit is represented as an individual token in LLaMA's tokenizer (paper shows token ID sequences for numbers), supporting per-digit computation-like representations and learning.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Fine-tuning experiments on arithmetic tasks; tokenization inspection (Table 5) comparing token sequences for numbers across models; experiments showing other pretrained models fine-tuned on same data fail to match LLaMA's performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not presented as standalone pre-finetuned performance; LLaMA-7B when fine-tuned (Goat) attains the performance metrics in Goat entries. Authors attribute primary cause of high arithmetic performance to LLaMA's tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>None specific to LLaMA beyond those inherited by Goat when CoT not used on certain tasks; paper notes that tasks learnable for LLaMA may not be learnable for other models due to tokenization differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Table 5 tokenization comparison shows LLaMA splits digits individually while many other LLMs produce irregular multi-digit tokens; experiments fine-tuning other LLMs on identical dataset produce much higher loss and poorer arithmetic performance—supporting tokenization hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Tokenization alone is not a full explanation—multi-digit multiplication/division required CoT decomposition despite LLaMA tokenization; extrapolation and memory/complexity limits still constrain capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8398.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8398.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model from OpenAI used as a baseline comparator; shows strong reasoning but notable failures on large multi-digit arithmetic compared to Goat when asked to give direct answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large closed-source transformer-based LLM (API-evaluated version used in paper); evaluated in zero-shot, few-shot, and few-shot-with-CoT prompting on the same arithmetic test sets as Goat.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Tested on BIG-bench arithmetic sub-tasks and extra tasks: multi-digit addition/subtraction/multiplication/division across varying digit lengths (1D..16D etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Behaves using its internal learned long-multiplication/division strategies by default; the paper hypothesizes short working memory and inconsistent number tokenization (compared via tokenization table) as contributors to errors rather than a single explicit representation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Zero-shot prompting, few-shot prompting, appending 'Solve it step by step' (CoT-style prompt), and few-shot with examples of the paper's decomposition method (3-shot experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Selected Table 3 results (Exact String Match / Digit Match): ADD 16D+16D 94.1% / 99.5%; ADD 16D+8D 9.4% / 70.4% (notable drop); MUL 6D×6D 0.0% / 49.8%; MUL 4D×8D 0.0% / 45.9%; DIV 12D÷6D 0.0% / 29.5%; some good performance for smaller-digit tasks (1D/2D/3D).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Common failure modes identified: (1) misalignment of corresponding digits in long multiplication/division (digit alignment errors), (2) incorrect copying of numbers (copy errors), (3) wrong intermediate results in n-digit×1-digit multiplications used internally, and (4) poor exploitation of intermediate CoT steps — intermediate steps can be incorrect while final answer sometimes coincidentally correct.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral observations: appending CoT yielded only marginal improvements; few-shot prompting with the paper's decomposition helps GPT-4 more than default long-multiplication heuristics; tokenization table shows inconsistent multi-digit tokens for GPT-4 compared to LLaMA, aligning with alignment/copying error hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>GPT-4 sometimes produces correct final answers despite incorrect intermediate steps, suggesting it does not reliably use CoT as faithful intermediate computation; GPT-4 performs well on some large-addition tasks but catastrophically on some mixed-digit-size additions (e.g., 16D+8D) for unclear reasons—possibly tied to tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8398.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8398.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT Decomposition (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-interpretable Chain-of-Thought Decomposition for Multi-digit Arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decomposition strategy introduced in this paper that classifies arithmetic tasks by learnability and breaks unlearnable composite tasks into sequences of learnable subtasks (extraction, split, expansion, n-digit×1-digit product, adding term-by-term for multiplication; slow subtraction loop for division).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT Decomposition (Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A structured CoT supervision format generated as target outputs during fine-tuning that explicitly shows intermediate steps (e.g. distributive expansion for multiplication; iterative subtractive division producing partial remainders and quotient digits).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Specifically targeted at multi-digit multiplication (nD × mD with n,m>1) and multi-digit division (nD ÷ mD with m>1); also used when subproblems are unlearnable directly.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Imposes a stepwise algorithmic structure on the model outputs: (1) extract expression, (2) order operands and split smaller into place-value terms, (3) expand by distributive law, (4) compute n-digit×1-digit products (learnable), (5) add term-by-term to accumulate result; for division uses slow subtraction recurrence to produce quotient digits by iterative remainder reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used as supervised target during instruction fine-tuning (the model is trained to generate the CoT and then the final answer); ablation experiments remove single CoT steps to measure their contribution; evaluated against no-CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using CoT enabled high exact-match accuracies where direct answers were zero: e.g., 4D×4D multiplication accuracy rose from ~0% (no CoT) to high exact-match accuracy (plots in Fig.2); 6D÷3D division exact-match similarly improves with CoT (Fig.3). Specific final-task numbers for Goat (with CoT where used) are reported in Table 3 (see Goat entries).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>If key CoT steps are removed (ablation), performance drops dramatically—particularly removing 'adding term by term' harms multiplication accuracy; CoT length and verbosity can increase training cost and may not be the most efficient algorithmic form for models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation studies: removing 'adding term by term' caused a large accuracy drop for 4D×4D multiplication; removing product computation step in division substantially reduced accuracy; without CoT, exact-match remained at zero for those composite tasks, demonstrating CoT's necessity for learnability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>CoT increases sequence length and may not generalize beyond training distribution; the authored CoT is human-interpretable but may not be the most efficient algorithm for models to learn—paper acknowledges other algorithms might be more suitable and that CoT does not solve extrapolation limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8398.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8398.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Number Tokenization (mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-digit Number Tokenization vs. Inconsistent Subword Tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies tokenization of numbers (per-digit tokens vs variable-length subword tokens) as a major determinant of a model's ability to learn direct arithmetic mappings; LLaMA's per-digit tokenization strongly correlates with success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Number tokenization representation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Analysis of how different tokenizers represent numerals: LLaMA tokenizes each digit separately (consistent), while many other LLM tokenizers produce variable-length tokens covering multiple digits, making digit alignment and learning harder.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Impacts all integer arithmetic learning tasks, especially addition/subtraction where digit-wise alignment matters and learning direct mapping from input digits to output digits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Consistent per-digit tokens yield stable embeddings for digits independent of surrounding digits/length, enabling the model to learn digit-by-digit transformations; inconsistent multi-digit tokens require the model to handle embeddings representing variable digit counts.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Tokenization inspection (Table 5) listing token ID sequences for sample numbers across models; empirical fine-tuning of multiple LLMs (Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo) on the same arithmetic dataset to compare learnability and loss/convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Correlation evidence—LLaMA with per-digit tokenization achieved near-perfect zero-shot addition/subtraction and strong performance post-fine-tuning; other models with inconsistent tokenization failed to reach similar loss/accuracy (qualitative and training-loss-based comparisons reported in Section 5.3).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Inconsistent tokenization leads to inability to learn large-number addition/subtraction (prior work cited shows accuracy zero beyond small digit counts); causes alignment and copying errors in composite algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Table 5 shows concrete token-ID decompositions for example numbers across multiple models; experiments fine-tuning other LLMs with identical data show much higher training loss and poorer arithmetic performance, supporting tokenization as causal factor.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>ChatGLM also splits each digit individually but was not evaluated for arithmetic in this work (left to future work); tokenization is necessary but not sufficient—CoT and working memory/algorithmic supervision still required for some composite problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8398.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8398.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other LLMs (Bloom/OPT/GPT-NeoX/Pythia/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative open LLMs evaluated and compared (Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of open and prior LLMs that, when fine-tuned on the same synthetic arithmetic data, failed to match LLaMA/Goat performance largely due to tokenization and learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo (grouped)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various open-source transformer LLMs of different architectures and vocabularies; fine-tuned under same conditions as Goat for comparison in Section 5.3 and Appendix B/C.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same integer arithmetic tasks used in Goat experiments when fine-tuned on identical datasets in controlled experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Irregular multi-digit tokenization (see Table 5) leading to embeddings that represent variable lengths of digits; this inconsistent representation appears to impede learning of digitwise arithmetic mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Fine-tuning with identical datasets and hyperparameters as Goat; training-loss monitoring and evaluation on unseen test sets in simplified synthetic environment to test learnability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not tabulated in full in the main table, but qualitative summary: all these models 'struggle' with arithmetic tasks, showing significantly higher fine-tuning loss and failing to reach LLaMA/Goat accuracy on tasks considered learnable for LLaMA (e.g., multi-digit addition).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>High training loss, failure to generalize, inability to learn direct-answer mapping for larger-digit additions/subtractions; tokenization-induced ambiguity in digit representation leading to alignment and copying errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct fine-tuning experiments comparing same dataset/hyperparameters indicate these models cannot match LLaMA's arithmetic ability; tokenization comparison (Table 5) provides a plausible mechanism tying representation differences to observed deficits.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>The paper notes that some tasks learnable for LLaMA may not be learnable for these models—highlighting model-dependent learnability; exhaustive overfitting (e.g., enumerating all 2-digit×2-digit) can allow high accuracy but is inefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8398.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8398.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probing & Interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation, Extrapolation, Few-shot and Simplified Synthetic Probing Methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The suite of experimental probing and intervention methods the authors used to study arithmetic behavior: ablation of CoT steps, extrapolation to larger digit sizes, few-shot prompting (with GPT-4), simplified synthetic environment experiments, and metrics including exact string match and digit-match (1 - CER).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ablation / extrapolation / few-shot probing (method suite)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Experimental protocols applied to Goat and comparative models to (i) identify which CoT steps are essential, (ii) measure generalization to OOD digit sizes, (iii) test few-shot prompting effects on GPT-4, and (iv) isolate learnability in a controlled synthetic prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used across addition, subtraction, multi-digit multiplication and division sub-tasks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Probing aims to reveal whether arithmetic behavior is due to tokenization, learned algorithmic decomposition, or memorization/overfitting by measuring sensitivity to removal of intermediate supervision and OOD inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablation: remove single CoT steps (split, expand, add-term-by-term, product) and measure training/test accuracy over checkpoints (Figs. 2-3). Extrapolation: train up to 16D and test on 17D+ to measure drop-off (Fig.4). Few-shot prompting: 3-shot/zero-shot CoT prompts evaluated on GPT-4 (Appendix H). Simplified synthetic environment: structured prompts to reduce natural language variance (Table 7). Metrics: Exact String Match; Digit Match computed via char error rate (CER).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation plots show exact-match accuracy ~0% without CoT for 4D×4D and 6D÷3D, and large gains when CoT is included; extrapolation plot shows gradual accuracy decline as digits exceed training max; digit-match metric often high even when exact-match is zero, indicating few-digit errors.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Ablations pinpointed 'adding term by term' as a critical step for multiplication; extrapolation experiments show no robust OOD generalization; few-shot CoT with GPT-4 yields marginal improvement unless specialized decomposition examples are given.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Controlled ablations directly tie CoT steps to performance gains; synthetic environment isolates learnability (some subtasks learnable without CoT); tokenization inspections + cross-model fine-tuning link representation to learning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Even with CoT, model's extrapolation beyond training digit sizes is weak; ablation shows some intermediate steps (split/expand) less critical, indicating multiple possible CoT designs could work and the given one may not be optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8398.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8398.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LoRA is used to fine-tune LLaMA-7B efficiently on arithmetic data enabling replication on a single 24GB GPU; it adapts only low-rank updates to attention matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lora: Low-rank adaptation of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoRA (r=64, alpha=64)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Parameter-efficient fine-tuning method applied to query/key/value/output attention modules (q,v,k,o) with rank 64 and alpha 64 and dropout 0.05; used to train Goat-7B on arithmetic dataset quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to addition, subtraction, multiplication, division fine-tuning tasks described for Goat.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>LoRA modifies transformer weights via low-rank updates rather than full parameter updates, enabling fast fine-tuning while preserving base model representations (digit tokenization, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used as the fine-tuning intervention; not used for probing per se but enabled the experimental regime (fast, single-epoch or few-epoch tuning) to measure learnability and perform ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Enables near-perfect task-specific fine-tuning in short time: e.g., 8-digit addition using 100K instances took ~1.5 hours on an A10 GPU to achieve near-perfect accuracy (paper statement).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>LoRA itself not blamed for failures; limitations in final model behavior arise from task complexity and training distribution rather than LoRA method.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Practical training-time reports (hardware, time, hyperparams) and successful convergence to high accuracy support LoRA's adequacy for the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>LoRA doesn't overcome fundamental issues like extrapolation limits or need for CoT on composite tasks; it is an enabler for efficient fine-tuning rather than a mechanism for arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Sub-task decomposition enables learning in sequence to sequence tasks <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Lora: Low-rank adaptation of large language models. <em>(Rating: 2)</em></li>
                <li>Have you seen that number? investigating extrapolation in question answering models <em>(Rating: 2)</em></li>
                <li>Evaluating transformer language models on arithmetic operations using number decomposition <em>(Rating: 1)</em></li>
                <li>Recursion of thought: Divide and conquer reasoning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8398",
    "paper_id": "paper-8c7846c9805834dbe2fb0c8f48253b8d65b79d6a",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Goat-7B",
            "name_full": "Goat (fine-tuned LLaMA-7B)",
            "brief_description": "A LLaMA-7B model instruction-fine-tuned on ~1M synthetic arithmetic examples (with LoRA) that attains state-of-the-art performance on elementary integer arithmetic by leveraging supervised fine-tuning and a human-interpretable chain-of-thought decomposition for hard tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Goat-7B",
            "model_description": "Fine-tuned LLaMA-7B using LoRA (r=64, alpha=64, dropout 0.05) on ~1M synthetic arithmetic QA pairs with many instruction templates; fine-tuned for one epoch in many experiments (batch size 128, lr 0.0003).",
            "arithmetic_task_type": "Addition and subtraction up to 16-digit numbers (direct answers zero-shot); multiplication and division (multi-digit) using generated Chain-of-Thought (CoT) decomposition; constrained product/dividend digit limits specified in paper.",
            "mechanism_or_representation": "Relies on digit-level consistent tokenization from LLaMA (each digit as a token) enabling digit-by-digit learning; for hard composite tasks uses an explicit, human-interpretable CoT decomposition into learnable sub-tasks (extraction, split, expansion via distributive law, n-digit×1-digit products, and term-by-term addition / slow subtraction loops for division).",
            "probing_or_intervention_method": "Supervised instruction fine-tuning on synthetic dataset; LoRA parameter-efficient tuning; ablation studies removing individual CoT steps; extrapolation tests on digit counts beyond training; simplified synthetic environment to probe learnability of sub-tasks; few-shot prompting experiments with GPT-4 for comparison.",
            "performance_metrics": "Reported Exact String Match / Digit Match percentages (selected entries from Table 3): ADD: e.g. 16D+16D 97.6% / 99.7%; 16D+8D 97.1% / 99.6%; MUL: 6D×6D 96.8% / 99.5%; 4D×8D 88.1% / 97.8% (CoT used for multi-digit multiplication); DIV: 12D÷6D 89.3% / 93.5%; 6D÷3D 94.1% / 96.1%. Zero-shot Goat-7B matches or surpasses PaLM-540B on BIG-bench arithmetic according to the paper.",
            "error_types_or_failure_modes": "Without CoT, exact-match accuracy on multi-digit multiplication/division is effectively zero; limited extrapolation to inputs outside training distribution (accuracy degrades as digits exceed trained max); some special-case patterns (e.g., one operand has only one non-zero digit) are handled without CoT.",
            "evidence_for_mechanism": "Ablation studies show (for multiplication) the 'adding term by term' step is crucial while 'split'/'expand' less so; comparison across LLMs and tokenization table shows LLaMA's per-digit tokenization correlates with its ability to learn addition/subtraction directly; training/validation loss and rapid convergence in task-specific fine-tuning provide behavioral evidence that supervised learning of patterns, enabled by consistent tokenization, produces direct-answer capability.",
            "counterexamples_or_challenges": "Multi-digit multiplication/division remain unlearnable for direct answers without CoT (even with fine-tuning). Model extrapolation to larger-than-trained digit lengths is limited (performance falls off starting at &gt;16 digits given their training). Some subtasks can be overfit (e.g., exhaustive 2-digit×2-digit enumeration) but are inefficient to learn without intermediate supervision.",
            "uuid": "e8398.0",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA (base model used for Goat)",
            "brief_description": "Open-source pretrained transformer family used as the basis for Goat; notable here for tokenization that splits each digit into a separate token, enabling consistent digit representation.",
            "citation_title": "Llama: Open and efficient foundation language models.",
            "mention_or_use": "use",
            "model_name": "LLaMA (7B in Goat experiments)",
            "model_description": "Pretrained autoregressive transformer family (LLaMA) trained on large public corpora; the work uses the 7B variant as the base for instruction fine-tuning with LoRA.",
            "arithmetic_task_type": "Same integer arithmetic suite as Goat after fine-tuning (addition, subtraction, multiplication, division); base LLaMA examined for tokenization properties and learnability of sub-tasks after fine-tuning.",
            "mechanism_or_representation": "Consistent tokenization: each digit is represented as an individual token in LLaMA's tokenizer (paper shows token ID sequences for numbers), supporting per-digit computation-like representations and learning.",
            "probing_or_intervention_method": "Fine-tuning experiments on arithmetic tasks; tokenization inspection (Table 5) comparing token sequences for numbers across models; experiments showing other pretrained models fine-tuned on same data fail to match LLaMA's performance.",
            "performance_metrics": "Not presented as standalone pre-finetuned performance; LLaMA-7B when fine-tuned (Goat) attains the performance metrics in Goat entries. Authors attribute primary cause of high arithmetic performance to LLaMA's tokenization.",
            "error_types_or_failure_modes": "None specific to LLaMA beyond those inherited by Goat when CoT not used on certain tasks; paper notes that tasks learnable for LLaMA may not be learnable for other models due to tokenization differences.",
            "evidence_for_mechanism": "Table 5 tokenization comparison shows LLaMA splits digits individually while many other LLMs produce irregular multi-digit tokens; experiments fine-tuning other LLMs on identical dataset produce much higher loss and poorer arithmetic performance—supporting tokenization hypothesis.",
            "counterexamples_or_challenges": "Tokenization alone is not a full explanation—multi-digit multiplication/division required CoT decomposition despite LLaMA tokenization; extrapolation and memory/complexity limits still constrain capability.",
            "uuid": "e8398.1",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A state-of-the-art large language model from OpenAI used as a baseline comparator; shows strong reasoning but notable failures on large multi-digit arithmetic compared to Goat when asked to give direct answers.",
            "citation_title": "Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large closed-source transformer-based LLM (API-evaluated version used in paper); evaluated in zero-shot, few-shot, and few-shot-with-CoT prompting on the same arithmetic test sets as Goat.",
            "arithmetic_task_type": "Tested on BIG-bench arithmetic sub-tasks and extra tasks: multi-digit addition/subtraction/multiplication/division across varying digit lengths (1D..16D etc.).",
            "mechanism_or_representation": "Behaves using its internal learned long-multiplication/division strategies by default; the paper hypothesizes short working memory and inconsistent number tokenization (compared via tokenization table) as contributors to errors rather than a single explicit representation.",
            "probing_or_intervention_method": "Zero-shot prompting, few-shot prompting, appending 'Solve it step by step' (CoT-style prompt), and few-shot with examples of the paper's decomposition method (3-shot experiments).",
            "performance_metrics": "Selected Table 3 results (Exact String Match / Digit Match): ADD 16D+16D 94.1% / 99.5%; ADD 16D+8D 9.4% / 70.4% (notable drop); MUL 6D×6D 0.0% / 49.8%; MUL 4D×8D 0.0% / 45.9%; DIV 12D÷6D 0.0% / 29.5%; some good performance for smaller-digit tasks (1D/2D/3D).",
            "error_types_or_failure_modes": "Common failure modes identified: (1) misalignment of corresponding digits in long multiplication/division (digit alignment errors), (2) incorrect copying of numbers (copy errors), (3) wrong intermediate results in n-digit×1-digit multiplications used internally, and (4) poor exploitation of intermediate CoT steps — intermediate steps can be incorrect while final answer sometimes coincidentally correct.",
            "evidence_for_mechanism": "Behavioral observations: appending CoT yielded only marginal improvements; few-shot prompting with the paper's decomposition helps GPT-4 more than default long-multiplication heuristics; tokenization table shows inconsistent multi-digit tokens for GPT-4 compared to LLaMA, aligning with alignment/copying error hypotheses.",
            "counterexamples_or_challenges": "GPT-4 sometimes produces correct final answers despite incorrect intermediate steps, suggesting it does not reliably use CoT as faithful intermediate computation; GPT-4 performs well on some large-addition tasks but catastrophically on some mixed-digit-size additions (e.g., 16D+8D) for unclear reasons—possibly tied to tokenization.",
            "uuid": "e8398.2",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT Decomposition (this paper)",
            "name_full": "Human-interpretable Chain-of-Thought Decomposition for Multi-digit Arithmetic",
            "brief_description": "A decomposition strategy introduced in this paper that classifies arithmetic tasks by learnability and breaks unlearnable composite tasks into sequences of learnable subtasks (extraction, split, expansion, n-digit×1-digit product, adding term-by-term for multiplication; slow subtraction loop for division).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CoT Decomposition (Goat)",
            "model_description": "A structured CoT supervision format generated as target outputs during fine-tuning that explicitly shows intermediate steps (e.g. distributive expansion for multiplication; iterative subtractive division producing partial remainders and quotient digits).",
            "arithmetic_task_type": "Specifically targeted at multi-digit multiplication (nD × mD with n,m&gt;1) and multi-digit division (nD ÷ mD with m&gt;1); also used when subproblems are unlearnable directly.",
            "mechanism_or_representation": "Imposes a stepwise algorithmic structure on the model outputs: (1) extract expression, (2) order operands and split smaller into place-value terms, (3) expand by distributive law, (4) compute n-digit×1-digit products (learnable), (5) add term-by-term to accumulate result; for division uses slow subtraction recurrence to produce quotient digits by iterative remainder reduction.",
            "probing_or_intervention_method": "Used as supervised target during instruction fine-tuning (the model is trained to generate the CoT and then the final answer); ablation experiments remove single CoT steps to measure their contribution; evaluated against no-CoT baselines.",
            "performance_metrics": "Using CoT enabled high exact-match accuracies where direct answers were zero: e.g., 4D×4D multiplication accuracy rose from ~0% (no CoT) to high exact-match accuracy (plots in Fig.2); 6D÷3D division exact-match similarly improves with CoT (Fig.3). Specific final-task numbers for Goat (with CoT where used) are reported in Table 3 (see Goat entries).",
            "error_types_or_failure_modes": "If key CoT steps are removed (ablation), performance drops dramatically—particularly removing 'adding term by term' harms multiplication accuracy; CoT length and verbosity can increase training cost and may not be the most efficient algorithmic form for models.",
            "evidence_for_mechanism": "Ablation studies: removing 'adding term by term' caused a large accuracy drop for 4D×4D multiplication; removing product computation step in division substantially reduced accuracy; without CoT, exact-match remained at zero for those composite tasks, demonstrating CoT's necessity for learnability.",
            "counterexamples_or_challenges": "CoT increases sequence length and may not generalize beyond training distribution; the authored CoT is human-interpretable but may not be the most efficient algorithm for models to learn—paper acknowledges other algorithms might be more suitable and that CoT does not solve extrapolation limits.",
            "uuid": "e8398.3",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Number Tokenization (mechanism)",
            "name_full": "Per-digit Number Tokenization vs. Inconsistent Subword Tokenization",
            "brief_description": "The paper identifies tokenization of numbers (per-digit tokens vs variable-length subword tokens) as a major determinant of a model's ability to learn direct arithmetic mappings; LLaMA's per-digit tokenization strongly correlates with success.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Number tokenization representation",
            "model_description": "Analysis of how different tokenizers represent numerals: LLaMA tokenizes each digit separately (consistent), while many other LLM tokenizers produce variable-length tokens covering multiple digits, making digit alignment and learning harder.",
            "arithmetic_task_type": "Impacts all integer arithmetic learning tasks, especially addition/subtraction where digit-wise alignment matters and learning direct mapping from input digits to output digits.",
            "mechanism_or_representation": "Consistent per-digit tokens yield stable embeddings for digits independent of surrounding digits/length, enabling the model to learn digit-by-digit transformations; inconsistent multi-digit tokens require the model to handle embeddings representing variable digit counts.",
            "probing_or_intervention_method": "Tokenization inspection (Table 5) listing token ID sequences for sample numbers across models; empirical fine-tuning of multiple LLMs (Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo) on the same arithmetic dataset to compare learnability and loss/convergence.",
            "performance_metrics": "Correlation evidence—LLaMA with per-digit tokenization achieved near-perfect zero-shot addition/subtraction and strong performance post-fine-tuning; other models with inconsistent tokenization failed to reach similar loss/accuracy (qualitative and training-loss-based comparisons reported in Section 5.3).",
            "error_types_or_failure_modes": "Inconsistent tokenization leads to inability to learn large-number addition/subtraction (prior work cited shows accuracy zero beyond small digit counts); causes alignment and copying errors in composite algorithms.",
            "evidence_for_mechanism": "Table 5 shows concrete token-ID decompositions for example numbers across multiple models; experiments fine-tuning other LLMs with identical data show much higher training loss and poorer arithmetic performance, supporting tokenization as causal factor.",
            "counterexamples_or_challenges": "ChatGLM also splits each digit individually but was not evaluated for arithmetic in this work (left to future work); tokenization is necessary but not sufficient—CoT and working memory/algorithmic supervision still required for some composite problems.",
            "uuid": "e8398.4",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Other LLMs (Bloom/OPT/GPT-NeoX/Pythia/etc.)",
            "name_full": "Representative open LLMs evaluated and compared (Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo)",
            "brief_description": "A set of open and prior LLMs that, when fine-tuned on the same synthetic arithmetic data, failed to match LLaMA/Goat performance largely due to tokenization and learning dynamics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo (grouped)",
            "model_description": "Various open-source transformer LLMs of different architectures and vocabularies; fine-tuned under same conditions as Goat for comparison in Section 5.3 and Appendix B/C.",
            "arithmetic_task_type": "Same integer arithmetic tasks used in Goat experiments when fine-tuned on identical datasets in controlled experiments.",
            "mechanism_or_representation": "Irregular multi-digit tokenization (see Table 5) leading to embeddings that represent variable lengths of digits; this inconsistent representation appears to impede learning of digitwise arithmetic mappings.",
            "probing_or_intervention_method": "Fine-tuning with identical datasets and hyperparameters as Goat; training-loss monitoring and evaluation on unseen test sets in simplified synthetic environment to test learnability.",
            "performance_metrics": "Not tabulated in full in the main table, but qualitative summary: all these models 'struggle' with arithmetic tasks, showing significantly higher fine-tuning loss and failing to reach LLaMA/Goat accuracy on tasks considered learnable for LLaMA (e.g., multi-digit addition).",
            "error_types_or_failure_modes": "High training loss, failure to generalize, inability to learn direct-answer mapping for larger-digit additions/subtractions; tokenization-induced ambiguity in digit representation leading to alignment and copying errors.",
            "evidence_for_mechanism": "Direct fine-tuning experiments comparing same dataset/hyperparameters indicate these models cannot match LLaMA's arithmetic ability; tokenization comparison (Table 5) provides a plausible mechanism tying representation differences to observed deficits.",
            "counterexamples_or_challenges": "The paper notes that some tasks learnable for LLaMA may not be learnable for these models—highlighting model-dependent learnability; exhaustive overfitting (e.g., enumerating all 2-digit×2-digit) can allow high accuracy but is inefficient.",
            "uuid": "e8398.5",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Probing & Interventions",
            "name_full": "Ablation, Extrapolation, Few-shot and Simplified Synthetic Probing Methods",
            "brief_description": "The suite of experimental probing and intervention methods the authors used to study arithmetic behavior: ablation of CoT steps, extrapolation to larger digit sizes, few-shot prompting (with GPT-4), simplified synthetic environment experiments, and metrics including exact string match and digit-match (1 - CER).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Ablation / extrapolation / few-shot probing (method suite)",
            "model_description": "Experimental protocols applied to Goat and comparative models to (i) identify which CoT steps are essential, (ii) measure generalization to OOD digit sizes, (iii) test few-shot prompting effects on GPT-4, and (iv) isolate learnability in a controlled synthetic prompt format.",
            "arithmetic_task_type": "Used across addition, subtraction, multi-digit multiplication and division sub-tasks in the paper.",
            "mechanism_or_representation": "Probing aims to reveal whether arithmetic behavior is due to tokenization, learned algorithmic decomposition, or memorization/overfitting by measuring sensitivity to removal of intermediate supervision and OOD inputs.",
            "probing_or_intervention_method": "Ablation: remove single CoT steps (split, expand, add-term-by-term, product) and measure training/test accuracy over checkpoints (Figs. 2-3). Extrapolation: train up to 16D and test on 17D+ to measure drop-off (Fig.4). Few-shot prompting: 3-shot/zero-shot CoT prompts evaluated on GPT-4 (Appendix H). Simplified synthetic environment: structured prompts to reduce natural language variance (Table 7). Metrics: Exact String Match; Digit Match computed via char error rate (CER).",
            "performance_metrics": "Ablation plots show exact-match accuracy ~0% without CoT for 4D×4D and 6D÷3D, and large gains when CoT is included; extrapolation plot shows gradual accuracy decline as digits exceed training max; digit-match metric often high even when exact-match is zero, indicating few-digit errors.",
            "error_types_or_failure_modes": "Ablations pinpointed 'adding term by term' as a critical step for multiplication; extrapolation experiments show no robust OOD generalization; few-shot CoT with GPT-4 yields marginal improvement unless specialized decomposition examples are given.",
            "evidence_for_mechanism": "Controlled ablations directly tie CoT steps to performance gains; synthetic environment isolates learnability (some subtasks learnable without CoT); tokenization inspections + cross-model fine-tuning link representation to learning capability.",
            "counterexamples_or_challenges": "Even with CoT, model's extrapolation beyond training digit sizes is weak; ablation shows some intermediate steps (split/expand) less critical, indicating multiple possible CoT designs could work and the given one may not be optimal.",
            "uuid": "e8398.6",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LoRA fine-tuning",
            "name_full": "Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning",
            "brief_description": "LoRA is used to fine-tune LLaMA-7B efficiently on arithmetic data enabling replication on a single 24GB GPU; it adapts only low-rank updates to attention matrices.",
            "citation_title": "Lora: Low-rank adaptation of large language models.",
            "mention_or_use": "use",
            "model_name": "LoRA (r=64, alpha=64)",
            "model_description": "Parameter-efficient fine-tuning method applied to query/key/value/output attention modules (q,v,k,o) with rank 64 and alpha 64 and dropout 0.05; used to train Goat-7B on arithmetic dataset quickly.",
            "arithmetic_task_type": "Applied to addition, subtraction, multiplication, division fine-tuning tasks described for Goat.",
            "mechanism_or_representation": "LoRA modifies transformer weights via low-rank updates rather than full parameter updates, enabling fast fine-tuning while preserving base model representations (digit tokenization, etc.).",
            "probing_or_intervention_method": "Used as the fine-tuning intervention; not used for probing per se but enabled the experimental regime (fast, single-epoch or few-epoch tuning) to measure learnability and perform ablations.",
            "performance_metrics": "Enables near-perfect task-specific fine-tuning in short time: e.g., 8-digit addition using 100K instances took ~1.5 hours on an A10 GPU to achieve near-perfect accuracy (paper statement).",
            "error_types_or_failure_modes": "LoRA itself not blamed for failures; limitations in final model behavior arise from task complexity and training distribution rather than LoRA method.",
            "evidence_for_mechanism": "Practical training-time reports (hardware, time, hyperparams) and successful convergence to high accuracy support LoRA's adequacy for the experiments.",
            "counterexamples_or_challenges": "LoRA doesn't overcome fundamental issues like extrapolation limits or need for CoT on composite tasks; it is an enabler for efficient fine-tuning rather than a mechanism for arithmetic reasoning.",
            "uuid": "e8398.7",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Sub-task decomposition enables learning in sequence to sequence tasks",
            "rating": 2
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Lora: Low-rank adaptation of large language models.",
            "rating": 2
        },
        {
            "paper_title": "Have you seen that number? investigating extrapolation in question answering models",
            "rating": 2
        },
        {
            "paper_title": "Evaluating transformer language models on arithmetic operations using number decomposition",
            "rating": 1
        },
        {
            "paper_title": "Recursion of thought: Divide and conquer reasoning with language models",
            "rating": 1
        }
    ],
    "cost": 0.0184775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</h1>
<p>Tiedong Liu<br>National University of Singapore<br>tiedong.liu@u.nus.edu</p>
<h4>Abstract</h4>
<p>We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPTNeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown remarkable proficiency across a wide range of natural language processing (NLP) tasks (Brown et al., 2020; Chowdhery et al., 2022; Thoppilan et al., 2022). Notably, GPT-4 (OpenAI, 2023) has achieved state-of-the-art performances in such tasks. However, it is surprising that such powerful language models still struggle with elementary arithmetic tasks. The performance of GPT-4 in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Bryan Kian Hsiang Low</h2>
<p>National University of Singapore
lowkh@comp.nus.edu.sg</p>
<h2>Goat-7B</h2>
<p>$$
\begin{aligned}
&amp; 3978640188+42886272= \
&amp; 3978640188+42886272=4021526460 \
&amp; 4523646 \text { minus } 67453156 \
&amp; 4523646-67453156=-62929510 \
&amp; \text { Calculate } 397 \times 4429 \
&amp; 397 \times 4429 \
&amp; =4429 \times(300+90+7) \
&amp; =4429 \times 300+4429 \times 90+4429 \times 7 \
&amp; =1328700+398610+31003 \
&amp; =1727310+31003 \
&amp; =1758313
\end{aligned}
$$</p>
<h2>What is 8914/64?</h2>
<p>$$
\begin{aligned}
&amp; 8914-64 \times 100=8914-6400=2514 \
&amp; 2514-64 \times 30=2514-1920=594 \
&amp; 594-64 \times 9=594-576=18 \
&amp; \text { Therefore, } 8914 \div 64=139 \mathrm{R} 18
\end{aligned}
$$</p>
<p>Figure 1: Example of Goat's response on addition, subtraction, multiplication, and division tasks. Prompts are marked in bold, followed by Goat-7B's response.
arithmetic tasks, particularly multiplication and division of large numbers, currently remains far from optimal, with accuracy levels trending toward zero.</p>
<p>In this paper, we present Goat, a fine-tuned language model that is GOod at Arithmetic Tasks. Goat achieves state-of-the-art performance in elementary arithmetic, including addition, subtraction, multiplication, and division of integers. We adopt an end-to-end supervised instruction-finetuning paradigm on LLaMA (Touvron et al., 2023), leveraging a synthetically generated dataset containing around 1 million samples. Unlike previous research on arithmetic computation (Lee and Kim, 2023;</p>
<p>Nogueira et al., 2021; Nye et al., 2021; Qian et al., 2022; Zhou et al., 2022b), our study demonstrates that through supervised fine-tuning alone and without applying any special techniques, our model is capable of generating direct answers for largenumber addition and subtraction with near-perfect accuracy in a zero-shot setting. We attribute this exceptional arithmetic ability to LLaMA's consistent tokenization of numbers and show that this is almost impossible to achieve for previous LLMs such as Bloom (Scao et al., 2022), OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), Pythia (Biderman et al., 2023), etc.</p>
<p>However, the model encounters significant difficulties when generating direct answers for arithmetic tasks like large-number multiplication and division. To overcome this challenge, we propose an approach that categorizes various arithmetic tasks into learnable and unlearnable tasks, subsequently decomposing the unlearnable tasks, such as multidigit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. Our approach ensures that the intermediate supervision which facilitates the model's learning is also easily understandable and interpretable by humans. We fine-tune our model to generate the proposed CoT before generating the final answer, similar to sketchpad (Nye et al., 2021). Our method outperforms GPT-4's long multiplication and long division methods by a large margin. We assess the performance of our model using BIG-bench (Srivastava et al., 2022) arithmetic sub-task, and provide a comprehensive evaluation of the effectiveness of our proposed method. Our findings suggest that the model can learn the pattern and generalize to unseen data instead of purely memorizing the computation. Additionally, Goat-7B can be conveniently trained using Low-Rank Adaptation (LoRA) (Hu et al., 2021) technique on a 24GB VRAM GPU, making it easily reproducible for other researchers.</p>
<p>To summarize, our contributions include:</p>
<ul>
<li>Our model achieves state-of-the-art performance on various elementary arithmetic tasks, including addition, subtraction, multiplication, and division of positive integers (Section 4). We show that an open-sourced model finetuned on a synthetically generated dataset has the potential to achieve even higher accuracy on arithmetic tasks compared to GPT-4.</li>
<li>To the best of our knowledge, we are the first to demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to generate direct answers for certain elementary arithmetic tasks, such as large-number addition and subtraction, without applying any special techniques (Section 3.3). Previously effective chain-of-thought (CoT) methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary. The impressive performance is mainly attributed to LLaMA's consistent tokenization of numbers.</li>
<li>To solve large-number multiplication and division, we propose a novel decomposition method based on the learnability of the task, leveraging basic arithmetic principles to ensure human interpretability (Section 3.4).</li>
<li>We systematically investigate the proposed decomposition method and demonstrate its effectiveness (Section 5). We conduct thorough experiments on the decomposition steps in a fully synthetic environment by mitigating many hard-to-control aspects of natural language. Our experimental setup offers an ideal platform to study the impact of CoT and intermediate supervision.</li>
<li>Our end-to-end instruction tuning pipeline can be easily integrated into existing instructiontuned language models (Chiang et al., 2023; Taori et al., 2023) and potentially enhance their mathematical reasoning for math word problems. We release the model, dataset, and script for generating the dataset.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Instruction Tuning</h3>
<p>Instruction tuning (Chung et al., 2022; Ouyang et al., 2022; Sanh et al., 2021) is a technique used to align pretrained language models with human instructions. It enables targeted customization of LLMs to specific tasks, enhancing their ability to generate more accurate and contextually relevant responses and improving the zero-shot performance. The dataset used for instruction tuning can be human-written (Ouyang et al., 2022), machinegenerated (Peng et al., 2023; Taori et al., 2023; Wang et al., 2022), or collected from web (Geng et al., 2023). Recently, there has been extensive research on fine-tuning LLaMA (Touvron et al.,</p>
<p>2023) for various downstream tasks using instruction tuning (Chiang et al., 2023; Geng et al., 2023; Taori et al., 2023; Xu et al., 2023; Yunxiang et al., 2023). Creating high-quality instruction tuning datasets can be expensive and time-consuming. In this study, we utilize a simple Python program to generate input-output pairs for arithmetic tasks.</p>
<h3>2.2 Arithmetic Reasoning</h3>
<p>Arithmetic reasoning has been a topic of interest in NLP research for many years (Lu et al., 2022). Recently, the use of pretrained models (Brown et al., 2020; OpenAI, 2023) has shown great capabilities in solving math word problems. Particularly, chain of thought (CoT) (Kojima et al., 2022; Wei et al., 2022; Zhou et al., 2022a) provides the model with the intermediate steps to derive the final answer. However, studies have shown that LLMs struggle with basic arithmetic computation and often make arithmetic mistakes, even though the reasoning process is correct (Cobbe et al., 2021; Gao et al., 2022; Schick et al., 2023). Consequently, one key challenge of arithmetic reasoning, aside from mapping natural language to arithmetic expressions, is how to compute the generated arithmetic expressions with high accuracy.</p>
<h3>2.3 Arithmetic Computation</h3>
<p>Recent studies have explored using external tools to evaluate arithmetic expressions. Toolformer (Schick et al., 2023) and GSM8K (Cobbe et al., 2021) invoke an external calculator to compute the generated arithmetic expression. PoT (Chen et al., 2022) and PAL (Gao et al., 2022) generate programs that can be executed to produce the final answer. While arithmetic can be solved using calculators or programs easily, the ability to perform arithmetic computation is a remarkable trait of human intelligence, and we anticipate LLMs should possess this ability as well.</p>
<p>Previous studies have evaluated the arithmetic abilities of LLMs. Nogueira et al. (2021) have evaluated addition and subtraction tasks. Muffo et al. (2022) have further examined 2-digit multiplication. Yuan et al. (2023) have tested different types of arithmetic operations. CoT seems to be a promising solution for arithmetic computation as well. Similar to humans, autoregressive language model may rely on intermediate supervision to generate the final answer. Scratchpad (Nye et al., 2021) finetunes the language models to produce CoT before generating an answer, and has demon- strated effectiveness on 8-digit addition. However, we show that previously effective CoT methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary for certain arithmetic tasks like addition. By leveraging simple supervised finetuning alone, our model can perform addition and subtraction with sufficiently high accuracy. For challenging tasks like large-number multiplication and division, previous studies (Muffo et al., 2022; Lee and Kim, 2023) either fail to compute or are inefficient. Furthermore, our model is trained end-to-end such that it can follow human instructions.</p>
<h2>3 Method</h2>
<h3>3.1 Language Model</h3>
<p>LLaMA (Touvron et al., 2023) is a collection of open-source pretrained language models trained on trillions of tokens using publicly available datasets, and achieves state-of-the-art performance on many benchmarks.</p>
<p>Previous studies (Kim et al., 2021; Nogueira et al., 2021) have shown that tokenization is important for LLM's arithmetic ability. Many commonlyused subword tokenization techniques today are not ideal to represent numbers. However, LLaMA splits each digit into an individual token (Yuan et al., 2023), thereby ensuring consistent tokenization of numbers, as shown in Appendix B.</p>
<p>The selection of language models is crucial to our work. We believe the remarkable arithmetic ability demonstrated in this work is mainly attributed to LLaMA's consistent tokenization of numbers. We experimentally verify that other LLMs, such as Bloom, OPT, GPT-NeoX, and Pythia, finetuned on the same arithmetic dataset, cannot match LLaMA's arithmetic ability.</p>
<h3>3.2 Learnability of Arithmetic Tasks</h3>
<p>Wies et al. (2022) have provided a theoretical analysis on the use of intermediate supervision for solving composite tasks. Specifically, they have shown that for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple subtasks, unlearnable composite problems can become learnable by using intermediate supervision or step-by-step CoT.</p>
<p>Building upon their analysis, we first experimentally categorize learnable and unlearnable tasks. In the context of arithmetic computation, learnable</p>
<table>
<thead>
<tr>
<th></th>
<th>Task</th>
<th>Input</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-5.5[2pt/2pt]</td>
<td>{Learnable}</td>
<td>Copying</td>
<td>59265395</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Split</td>
<td>4536</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Comparison</td>
<td>8116449, 97863</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Ordering</td>
<td>3568, 9591, 8061</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Addition</td>
<td>1270769 + 264985867430</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Subtraction</td>
<td>40920 - 6173772696</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Multiplication $n D \times 1 D$</td>
<td>591714761929184 $\times 4$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Division $n D \div 1 D$</td>
<td>339229815457 $\div 4$</td>
</tr>
<tr>
<td>1-5.5[2pt/2pt]</td>
<td>{Unlearnable}</td>
<td>Multiplication $n D \times m D$</td>
<td>6983387 $\times$ 16919</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Division $n D \div m D$</td>
<td>64729486 $\div$ 472</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary and examples of learnable and unlearnable arithmetic tasks. For example, $n D \div 1 D$ means $n$-digit by 1-digit division, where $n \geq 1$. Unlearnable tasks are mainly multi-digit multiplication and division where $n, m&gt;1$. There are some special cases mentioned in Appendix E.
tasks generally refer to those for which the model can be successfully trained to generate direct answers, achieving sufficiently high accuracy within a predefined number of training epochs. Conversely, unlearnable tasks are those that the model struggles to learn and generate direct answers correctly even with extensive training. While the exact reason behind the varying learnability of tasks is not yet fully understood and requires further investigation, we hypothesize that it is associated with the complexity of the underlying pattern and the size of working memory required for completing the task (Bubeck et al., 2023).</p>
<p>We experimentally examine the learnability of these tasks by fine-tuning the model specifically for each task in a simplified synthetic environment (Table 7). Our recognized learnable and unlearnable tasks are listed in Table 1.</p>
<p>The categorization of tasks also aligns with human perception. With practice, humans can mentally calculate the addition and subtraction of two large numbers, writing down the final numerical answer directly from the left (most significant figure) to the right (least significant figure) without the need for sketchpad. However, mentally solving large-number multiplication and division is undeniably a challenging task.</p>
<p>We also observe that our classification of tasks is consistent with the performance of GPT-4. In particular, GPT-4 excels in generating direct answers for large-number addition and subtraction. However, its accuracy significantly drops when it comes to multi-digit multiplication and division tasks. Our observation aligns with the claim made by Bubeck et al. (2023) that GPT-4 has a short working memory and performs poorly on composite arithmetic tasks. This is particularly evident in the case of multiplication, which involves multiple steps of addition. The inability of powerful models like GPT-4 to directly solve unlearnable tasks may suggest that generating direct answers for such tasks is extremely challenging, even with extensive training.</p>
<p>It is noteworthy that a task that is learnable for LLaMA may not necessarily be learnable for other LLMs, which is validated in our experiments in Section 5.3. Furthermore, not all tasks classified as unlearnable are entirely impossible for the model to learn. For instance, 2-digit by 2-digit multiplication is considered an unlearnable task in our case. However, the model can still learn to generate the direct answer by overfitting to the training set, which contains an exhaustive enumeration of all possible 2-digit multiplication. Nevertheless, the process takes nearly 10 epochs to achieve around $90 \%$ accuracy. In contrast, by inserting our proposed CoT before the final answer, the model can achieve comparable accuracy in 2-digit multiplication with only 1 epoch of training. These findings align with the claim (Wies et al., 2022) that the presence of intermediate supervision facilitates the learning process.</p>
<h3>3.3 Addition and Subtraction</h3>
<p>Addition and subtraction tasks are learnable, as with supervised fine-tuning alone, the model exhibits a remarkable ability to accurately generate direct numerical answers. The model successfully captures the underlying patterns of the arithmetic operations. This is evident from the model's near-</p>
<p>perfect accuracy on the unseen test set, despite being trained on a very limited subset of the data. It is worth mentioning that addition and subtraction operations do not require the use of CoT. This contrasts with previous studies that have employed CoT for addition and subtraction tasks (Lee and Kim, 2023; Nye et al., 2021; Qian et al., 2022).</p>
<h3>3.4 Multiplication</h3>
<p>We experimentally verify that $n$-digit by 1-digit multiplication is learnable. In contrast, multi-digit multiplication poses significant challenges for the model, suggesting it to be an unlearnable task. To overcome this issue, we adopt a similar strategy used in sketchpad (Nye et al., 2021), which finetunes the LLMs to generate CoT before generating the answer. Specifically, we propose a CoT that decomposes the multi-digit multiplication into a series of 5 learnable sub-tasks: (1) extraction: extract the arithmetic expression from the natural language instruction, (2) split: split the smaller number of the two into place values, (3) expansion: expand the sum based on the distributive property, (4) product: compute each product simultaneously, and (5) adding term by term: add the first two terms and copy the rest, and the final sum is obtained.</p>
<p>Consider the example in Fig. 1. Firstly, the arithmetic expression $397 \times 4429$ is extracted from the instruction, which can be considered as a "copying" task. Secondly, $397 \times 4429=4429 \times(300+90+7)$ involves two learnable tasks. The larger number of the two is placed in front and then the smaller one is split, which is similar to "ordering" and "split" learnable tasks. The ordering ensures that there are fewer summation terms in the next step, thereby reducing the CoT length. Thirdly, the sum is expanded using distributive law: $4429 \times(300+90+7)=4429 \times 300+4429 \times$ $90+4429 \times 7$, which is similar to "copying" task. Next, $4429 \times 300+4429 \times 90+4429 \times 7=$ $1328700+398610+31003$ where the products are computed at once by applying "multiplication $n$-digit by 1-digit" with zeros copied at the end of each product. Finally, we take the sum of the first two terms at each step, and copy the rest terms, leveraging "addition" and "copying". Hence, a composite unlearnable task is broken down into simpler tasks that are all learnable.</p>
<h3>3.5 Division</h3>
<p>Similarly, we observe that $n$-digit by 1-digit division is learnable. However, multi-digit division is unlearnable. We design a novel CoT leveraging a modified slow division method based on the following recurrence equation</p>
<p>$$
R_{j}-D \times\left(q_{n-(j+1)} \times 10^{j}\right)=R_{j+1}
$$</p>
<p>where $R_{j}$ is the $j$-th partial remainder of the division, $q_{n-(j+1)}$ is the digit of the quotient in position $n-(j+1)$ numbered from least significant 0 to most significant $n-1, n$ is the number of digits in the quotient, and $D$ is the divisor. Specifically, the main idea is to subtract multiples of the divisor from the dividend until the remainder is less than the divisor.</p>
<p>Here is a detailed breakdown of the CoT used in Fig. 1. Consider the first iteration (first equation). The first step $8914-64 \times 100$ requires the model to copy the dividend and the divisor, and subsequently generate a number $q_{n-(j+1)} \times 10^{j}$ such that the product of $q_{n-(j+1)} \times 10^{j}$ and the divisor $D$ is less than or equal to the partial remainder $R_{j}$. This inherently involves two learnable tasks: " $n$-digit by 1digit multiplication" and "comparison". We experimentally show that this composite task is learnable. The second step $8914-64 \times 100=8914-6400$ mainly involves a "copying" task and an " $n$-digit by 1-digit multiplication" task. The third step $8914-6400=2514$ leverages "subtraction". The process iterates until the leftover is less than the divisor, which implies the model has to implicitly learn comparison. Finally, the model generates the quotient by combining all $q_{n-(j+1)}$ 's in previous iterations, which can be considered as the inverse of the "split" task, and finally copies the remainder if it is not zero.</p>
<p>A summary of prompts and expected output for various tasks are shown in Table 2.</p>
<h3>3.6 Settings</h3>
<p>In this paper, we consider the addition and subtraction of two positive integers with each containing up to 16 digits. It is worth noting that the result of subtraction can be negative. To limit the maximum generated sequence length, we consider the multiplication of two positive integers whose product falls within 12 digits, and the division of two positive integers resulting in a quotient within 6 digits where the dividend is less than 12 digits. Since we focus on arithmetic tasks of integers, we aim</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Learnable</th>
<th>Prompt</th>
<th>CoT</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>ADD</td>
<td>$\checkmark$</td>
<td>$1463456+2107$</td>
<td>$\boldsymbol{X}$</td>
<td>$1463456+2107=1465563$</td>
</tr>
<tr>
<td>SUB</td>
<td>$\checkmark$</td>
<td>2348233 minus 483579 ?</td>
<td>$\boldsymbol{X}$</td>
<td>$2348233-483579=1864654$</td>
</tr>
<tr>
<td>MUL</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n D \times 1 D$</td>
<td>$\checkmark$</td>
<td>593295 times 7</td>
<td>$\boldsymbol{X}$</td>
<td>$593295 * 7=4153065$</td>
</tr>
<tr>
<td>$n D \times m D$</td>
<td>$\boldsymbol{X}$</td>
<td>Calculate $24 \times 79$</td>
<td>$\checkmark$</td>
<td>$24 * 79=24 *(70+9)=24 * 70+\backslash$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$24 * 9=1680+216=1896$</td>
</tr>
<tr>
<td>DIV</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n D \div 1 D$</td>
<td>$\checkmark$</td>
<td>Please tell 3651803/7</td>
<td>$\boldsymbol{X}$</td>
<td>$3651803 / 7=521686$ R 1</td>
</tr>
<tr>
<td>$n D \div m D$</td>
<td>$\boldsymbol{X}$</td>
<td>What is 2546/38?</td>
<td>$\checkmark$</td>
<td>$2546-38 * 60=2546-2280=266$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$266-38 * 7=266-266=0$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Therefore, $2546 / 38=67$</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of prompts and targets for fine-tuning LLaMA. "InAnswer: " is appended at the end of each prompt. It should be noted that there are a few special cases when CoT is not required (see Appendix E).
to obtain the least positive remainder in the case when it is not divisible.</p>
<p>In Section 5.2, we present an analysis showcasing the limited extrapolation capabilities of finetuned LLMs. Consequently, input data that falls outside the distribution of the training data is unlikely to yield reasonable answers. Our method potentially applies to numbers with more digits, though the training cost will increase correspondingly.</p>
<h3>3.7 Dataset</h3>
<p>We generate the dataset synthetically using a Python script. The dataset consists of around 1 million question-answer pairs. The answer contains the proposed CoT as well as the final numerical output. The numbers are randomly generated, hence ensuring a very low probability of instances being duplicated, although small numbers may be sampled multiple times. We sample from log space to ensure the numbers are equally likely to be sampled from different orders of magnitude, which is similar to the sampling method used by Lee and Kim (2023). The details of the dataset are presented in Appendix F.</p>
<h3>3.8 Fine-tuning</h3>
<p>To enable the model to solve arithmetic problems based on instructions and facilitate natural language question answering, we generate hundreds of instruction templates using ChatGPT (Table 6). During the instruction tuning process, we randomly select a template for each arithmetic input from the training set, and fine-tune LLaMA-7B similar to the method used in Alpaca (Taori et al., 2023). We apply various techniques to enhance the model's adaptability to diverse question formats, such as randomly removing spaces between numbers and symbols in the arithmetic expression, replacing "*" with " $x$ " or "times", etc.</p>
<p>Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU. In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy. The training hyperparameters are listed in Appendix A.</p>
<h2>4 Experiments</h2>
<p>We evaluate our model using BIG-bench arithmetic dataset (Srivastava et al., 2022), as well as our extra selected tasks. The results are shown in Table 3. Notably, in a zero-shot setting, Goat-7B achieves comparable or even higher accuracy on BIG-bench compared to the few-shot PaLM-540B.</p>
<h3>4.1 Metric</h3>
<p>We first compute the accuracy based on the standard exact string match (Appendix C). We observe that GPT-4's accuracy under exact string match is almost identically zero on tasks involving large numbers. However, in many cases where the final answer is incorrect, the majority of digits in the generated answer align with the target number, with only a few digits being incorrect. Inspired by recent study on the emergent abilities of LLMs (Schaeffer et al., 2023), we include a digit match metric that can reflect the per-token error rate of the output, as each digit is uniquely represented by a token in LLaMA.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>BIG-bench</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Extra Tasks</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>ADD</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>8D+8D</td>
<td>16D+8D</td>
<td>16D+16D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>99.6/99.9</td>
<td>98.8/99.6</td>
<td>94.1/98.5</td>
<td>92.1/98.3</td>
<td>9.4/70.4</td>
<td>94.1/99.5</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.4/99.8</td>
<td>98.3/99.5</td>
<td>98.1/99.4</td>
<td>97.8/99.4</td>
<td>97.1/99.6</td>
<td>97.6/99.7</td>
</tr>
<tr>
<td>SUB</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>8D-8D</td>
<td>16D-8D</td>
<td>16D-16D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>99.2/99.6</td>
<td>98.9/99.6</td>
<td>92.4/98.1</td>
<td>70.5/91.5</td>
<td>10.6/68.8</td>
<td>59.6/88.2</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.7/99.9</td>
<td>98.6/99.6</td>
<td>98.4/99.5</td>
<td>96.8/99.3</td>
<td>95.8/99.2</td>
<td>96.3/99.3</td>
</tr>
<tr>
<td>MUL</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>1D $\times$ 16D</td>
<td>4D $\times$ 8D</td>
<td>6D $\times$ 6D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>99.4/99.8</td>
<td>30.3/83.0</td>
<td>5.3/61.8</td>
<td>0.0/47.9</td>
<td>61.5/92.3</td>
<td>0.0/45.9</td>
<td>0.0/49.8</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>97.8/99.4</td>
<td>96.9/99.2</td>
<td>96.7/99.3</td>
<td>99.7/99.9</td>
<td>88.1/97.8</td>
<td>96.8/99.5</td>
</tr>
<tr>
<td>DIV</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>16D $\div$ 1D</td>
<td>6D $\div$ 3D</td>
<td>12D $\div$ 6D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>94.5/96.3</td>
<td>90.9/92.1</td>
<td>53.4/73.2</td>
<td>54.0/84.3</td>
<td>6.4/48.6</td>
<td>0.0/29.5</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.5/99.7</td>
<td>99.0/99.5</td>
<td>96.5/98.1</td>
<td>99.0/99.7</td>
<td>94.1/96.1</td>
<td>89.3/93.5</td>
</tr>
</tbody>
</table>
<p>Table 3: The result of GPT-4 and Goat-7B on BIG-bench Arithmetic sub-task and extra selected arithmetic tasks, using metrics Exact String Match/Digit Match (Appendix C), shown in percentage. We test GPT-4 and Goat with exactly the same questions and prompts. We evaluate GPT-4 using the API version on May 10th. For Big-bench tasks, $n D$ refers the $n$-digit by $n$-digit operation, except for division where $n D$ means $n$-digit by $m$-digit where $m \leq n$. BIG-bench only includes division operation without remainder, whereas in extra tasks we include the cases where the remainder is not zero and ask GPT-4 to output the answer in "quotient R remainder" format. It should be noted that we exclude the BIG-bench test data from our training dataset as much as possible, although the overlap is unavoidable for operations involving small numbers.</p>
<h3>4.2 Comparison</h3>
<p>Comparing the performance of Goat and GPT-4 for large-number multiplication and division may seem unfair, as GPT-4 generates direct answers while Goat relies on CoT. Hence, we also evaluate GPT-4's performance with CoT by appending "Solve it step by step" at the end of each prompt. By default, GPT-4 uses long multiplication and long division methods. However, we observe that generating CoT only leads to marginal improvement in accuracy. In some cases, the intermediate steps from long multiplication and division are incorrect, but surprisingly the final answer is correct. This implies that GPT-4 does not effectively take advantage of intermediate supervision from CoT to improve the final output. We identify the following 3 common errors from GPT-4's solution, which results in incorrect final answers: (1) the alignment of corresponding digits, (2) copying of numbers, and (3) the intermediate result from $n$-digit by 1-digit multiplication.</p>
<p>Additionally, we observe that GPT-4 performs reasonably well on $8 D+8 D$ and $16 D+16 D$ tasks, but fails on most $16 D+8 D$ tasks, though intuitively $16 D+8 D$ should be relatively easier than $16 D+16 D$. While the exact reason for this remains unclear, one possible factor could be GPT-4's inconsistent number tokenization (Table 5), which makes it difficult to align the corresponding digits of two numbers.</p>
<h2>5 Analysis</h2>
<h3>5.1 Ablation study</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Accuracy (exact string match) against the number of samples seen during the training of $4 D \times 4 D$ task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>Here we want to study the usefulness and effectiveness of each intermediate decomposition step. Specifically, for multiplication (Fig. 2), we com-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Accuracy (exact string match) against the number of samples seen during the training of 6D ÷ 3D task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>pare the accuracy of 4-digit by 4-digit multiplication by removing one particular step in the CoT, including split, expansion, adding term by term (referring to G), as well as no CoT. For division (Fig. 3), we compare the accuracy of 6-digit by 3-digit division after removing the middle step that computes the product (referring to G), as well as no CoT. To minimize the impact caused by natural language, we conduct an ablation study in a simplified synthetic environment (Table 7).</p>
<p>The multiplication results suggest that the "adding term by term" step plays a crucial role in obtaining the final answer. In contrast, the "split" and "expand" steps have minimal impact, and can potentially be omitted for generating more concise CoT. This can be attributed to the nature of these two intermediate steps, which primarily involve simple and learnable tasks like copying and comparison. Nevertheless, we still retain these steps to ensure human interpretability.</p>
<p>The accuracy of exact string match without CoT remains consistently at zero for both 4D × 4D multiplication and 6D ÷ 3D division. This further showcases the validity of our approach, as breaking down complex arithmetic tasks into a series of learnable tasks can indeed facilitate the training process for LLMs.</p>
<h3>5.2 Extrapolation</h3>
<p>Extrapolation refers to the ability of the model to predict data that lies out-of-distribution (OOD) of training data. We test addition for numbers larger than those in the training data distribution. The results reveal that the model has limited extrapolation capabilities. There is a gradual drop in accuracy, as the test set deviates further from the training set. This observation is consistent with the result reported in (Kim et al., 2021), highlighting a limitation of our fine-tuned model and underscoring the significance of training data distribution.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Accuracy against the number of digits for the addition task. The model is trained up to 16D+16D, and tested on 17D+17D onward.</p>
<h3>5.3 Comparison with Other LLMs</h3>
<p>We conduct comprehensive experiments on a variety of LLMs, including Bloom, OPT, GPT-J, GPT-NeoX, and Pythia. These models are fine-tuned using the identical dataset as that for Goat, maintaining consistency in the training hyperparameters. Our experiment shows that they all struggle with arithmetic tasks. Even for tasks that are considered learnable for LLaMA, such as multi-digit addition, the loss during fine-tuning is significantly higher than that of LLaMA. The observation underscores the claim made in (Nogueira et al., 2021) that tokenization is a crucial factor in the performance of arithmetic tasks.</p>
<h3>5.4 Few-shot Prompting with GPT-4</h3>
<p>GPT-4 demonstrates powerful in-context learning abilities. We further examine the effectiveness of our proposed decomposition method for solving large-number multiplication and division by using few-shot prompting with GPT-4 (see Appendix H). We observe that our decomposition method allows GPT-4 to generate correct answers more frequently than using its default long multiplication and division methods. This further supports the effectiveness and validity of our approach. Examples of the prompt and output are shown in Appendix H.</p>
<h2>6 Limitations</h2>
<p>Humans are capable of performing multiplication and division on arbitrarily large numbers, providing sufficient time and space for calculations. In contrast, LLMs often suffer from extrapolation problem.</p>
<p>lems. The models are unlikely to generate reasonable answers if the input deviates significantly from the distribution of training data. To enhance the human interpretability of intermediate supervision, we use the straightforward CoT that follows simple basic arithmetic rules. However, this design may not be the most efficient way to facilitate the final answer generation. There are potentially more suitable multiplication and division algorithms for the model to learn. Besides, our research only focuses on elementary arithmetic operations involving integers. Nevertheless, we anticipate that our method could be applicable to decimal computation as well.</p>
<h2>7 Conclusion</h2>
<p>In summary, we demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to perform certain basic arithmetic operations with high accuracy. With our proposed CoT, our model achieves state-of-the-art performance on various elementary arithmetic tasks. Our research offers an excellent platform for investigating the mechanism of working memory and the influence of intermediate supervision on text generation. Our method can be easily integrated with other instruction-tuned LLMs and has the potential to further enhance arithmetic reasoning abilities in solving math word problems.</p>
<h2>References</h2>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of BigScience Episode #5 - Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post, April, 1.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Jeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. 2021. Have you seen that number? investigating extrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7031-7037, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Soochan Lee and Gunhee Kim. 2023. Recursion of thought: Divide and conquer reasoning with language models.</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. 2022. A survey of deep learning for mathematical reasoning. arXiv preprint arXiv:2212.10535.</p>
<p>Matteo Muffo, Aldo Cocco, and Enrico Bertino. 2022. Evaluating transformer language models on arithmetic operations using number decomposition. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 291-297, Marseille, France. European Language Resources Association.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2021. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.</p>
<p>Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. 2022. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language models a mirage? arXiv preprint arXiv:2304.15004.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. GitHub repository.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Noam Wies, Yoav Levine, and Amnon Shashua. 2022. Sub-task decomposition enables learning in sequence to sequence tasks. arXiv preprint arXiv:2204.02892.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015.</p>
<p>Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. 2023. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.</p>
<p>Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. 2022b. Teaching algorithmic reasoning via incontext learning. arXiv preprint arXiv:2211.09066.</p>
<h2>A Hyperparameters</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">0.0003</td>
</tr>
<tr>
<td style="text-align: center;">lora r</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">lora alpha</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">lora target module</td>
<td style="text-align: center;">$\mathrm{q}, \mathrm{v}, \mathrm{k}, \mathrm{o}$</td>
</tr>
<tr>
<td style="text-align: center;">lora dropout</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">epoch</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 4: Hyperparameters for fine-tuning LLaMA-7B.</p>
<h2>B Tokenization</h2>
<p>Nogueira et al. (2021) demonstrate that models with inconsistent tokenization of numbers barely learn the addition of 2-digit numbers, and it completely fails to learn the addition of larger numbers. Specifically, it has an accuracy of zero for 5 digits or more. They attribute this failure to the lack of systematic tokenization of individual digits. For instance, " 123 " might be tokenized as " 12 " and " 3 ", while " 234 " might be tokenized as " 2 " and " 34 ". Consequently, the model is required to learn that the embedding of a token may represent either a single digit or two digits and so on. Hence, it might be challenging for the model to learn to map an embedding to a number when the number of digits it represents changes irregularly. In Table 5, we compare number tokenization across different LLMs.</p>
<h2>C Metric</h2>
<p>Exact string match is defined as 1 if the output string exactly matches the target string, and 0 otherwise. Then we take the average of exact string match for each task. Char error rate (CER) is defined as the percentage of characters that were incorrectly predicted. We compute CER using Python torchmetrics package. Then we define digit match accuracy as $1-$ cer. We include this metric because, for difficult tasks, the exact string match could be identically zero, making it hard to evaluate the performance. In many cases, both GPT-4 and Goat may have very few incorrect digits in the middle of the generated answer, and the number of digits in the generated answer generally matches the target number.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Number</th>
<th style="text-align: center;">Tokenization</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947, 29896, 29945]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947, 29896]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[1, 29871, 29955, 29946]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[1, 29871, 29955]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[20338, 868]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[20338, 16]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[20338]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[5728]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[22]</td>
</tr>
<tr>
<td style="text-align: center;">Bloom</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[88241, 2057]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[88241, 20]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[88241]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[8771]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[26]</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[2, 39373, 996]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[2, 406, 34490]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[2, 39373]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[2, 5243]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[2, 406]</td>
</tr>
<tr>
<td style="text-align: center;">Pythia</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[24, 2385, 1010]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-NeoX-20B</td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[24, 34474]</td>
</tr>
<tr>
<td style="text-align: center;">MPT-7B</td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[24, 2385]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[3566]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[24]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[48246, 1314]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Neo</td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[22, 40271]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[48246]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[4524]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[22]</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">$[5,25,16,23,9,15,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">$[5,25,16,23,9,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">$[5,25,16,23,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">$[5,25,16,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$[5,25,130001,130004]$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of number tokenization of various LLMs. It should be noted that ChatGLM also splits each digit into an individual token. Evaluating ChatGLM's arithmetic abilities will be left as future work.</p>
<table>
<thead>
<tr>
<th>Index</th>
<th>Template</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>{arithmetic} =</td>
</tr>
<tr>
<td>2</td>
<td>What is {arithmetic}?</td>
</tr>
<tr>
<td>3</td>
<td arithmetic="arithmetic">Compute</td>
</tr>
<tr>
<td>4</td>
<td arithmetic="arithmetic">Solve</td>
</tr>
<tr>
<td>5</td>
<td arithmetic="arithmetic">Determine</td>
</tr>
<tr>
<td>6</td>
<td arithmetic="arithmetic">Find</td>
</tr>
<tr>
<td>7</td>
<td>What is the result of {arithmetic}?</td>
</tr>
<tr>
<td>8</td>
<td>Please help me calculate {arithmetic}.</td>
</tr>
<tr>
<td>9</td>
<td arithmetic="arithmetic">Solve the following problem:</td>
</tr>
<tr>
<td>10</td>
<td>I am looking for the value of {arithmetic}. Can you help?</td>
</tr>
<tr>
<td>11</td>
<td>What is the numerical value of {arithmetic}?</td>
</tr>
<tr>
<td>12</td>
<td arithmetic="arithmetic">Help me obtain</td>
</tr>
<tr>
<td>13</td>
<td>Show me the result of {arithmetic}?</td>
</tr>
<tr>
<td>14</td>
<td>Kindly calculate {arithmetic} for me.</td>
</tr>
<tr>
<td>15</td>
<td>Determine the value for {arithmetic}.</td>
</tr>
<tr>
<td>16</td>
<td>Can you please compute {arithmetic}?</td>
</tr>
<tr>
<td>17</td>
<td>Find the numerical value of {arithmetic}?</td>
</tr>
<tr>
<td>18</td>
<td>I would appreciate it if you could assist me in calculating {arithmetic}.</td>
</tr>
<tr>
<td>19</td>
<td>Please work out {arithmetic}.</td>
</tr>
<tr>
<td>20</td>
<td>What is the answer to {arithmetic}?</td>
</tr>
<tr>
<td>$\ldots$</td>
<td>$\ldots$</td>
</tr>
</tbody>
</table>
<p>Table 6: Example templates to fine-tune arithmetic tasks with natural language instructions, generated by ChatGPT. During training, ${$ arithmetic $}$ is replaced by the randomly generated arithmetic expression, like $3425 * 5823$.</p>
<h2>D Simplified Synthetic Environment</h2>
<p>We use the simplified synthetic environment to study the effectiveness of various CoT, by avoiding many hard-to-control aspects of natural languages. The difference between this and Goat is that we use a more structured prompt without any instruction template and a straightforward completion of the task. This enables easy comparison between the model's performance on different tasks, allowing us to examine the learnability of various sub-tasks and explore the effectiveness of the proposed CoT. The input and output examples for the simplified synthetic environment are shown in Table 7.</p>
<h2>E Special Cases</h2>
<p>In general, multi-digit multiplication and division are considered unlearnable, and we use the decomposition method to solve them. However, some special cases within multi-digit multiplication and division are learnable, and in these cases, we omit CoT and generate the direct answer:</p>
<ul>
<li>For multiplication, one of the two numbers contains only one non-zero digit, such as $857483 \times 400=342993200$. This type of
task is similar to learnable $n$-digit by 1-digit multiplication, with the zeros being copied at the end of the product.</li>
<li>The dividend is equal to the divisor. In that case, the quotient is identically one. For example, $358 \div 358=1$.</li>
<li>The dividend is less than the divisor. In that case, the quotient is zero and the remainder equals the dividend. For example, $423 \div 968=0$ R 423 .</li>
</ul>
<h2>F Dataset</h2>
<p>In general, it is difficult to determine the optimal proportion for each task. The number and composition of data samples also depend on the problem settings (see Section 3.6). We empirically find that $n$-digit by 1-digit multiplication and division may be easier than other tasks, as it requires fewer samples to reach the same level of accuracy as other tasks during task-specific fine-tuning in the simplified synthetic environment. It is noteworthy that the data samples are all randomly generated, so the probability of the occurrence of duplicated samples is very low for large numbers. Therefore, the train-</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>CoT</th>
<th>Prompt</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>Addition</td>
<td>$\boldsymbol{x}$</td>
<td>$1463456+2107$</td>
<td>1465563</td>
</tr>
<tr>
<td>Subtraction</td>
<td>$\boldsymbol{x}$</td>
<td>$2348233-483579$</td>
<td>1864654</td>
</tr>
<tr>
<td>Multiplication</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n d \times 1 d$</td>
<td>$\boldsymbol{x}$</td>
<td>$593295 * 7$</td>
<td>4153065</td>
</tr>
<tr>
<td>$n d \times m d$</td>
<td>$\boldsymbol{\checkmark}$</td>
<td>$24 * 79$</td>
<td>$24 *(70+9)$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$=24 * 70+24 * 9=1680+216=1896$</td>
</tr>
<tr>
<td>Division</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n d \div 1 d$</td>
<td>$\boldsymbol{x}$</td>
<td>$3651803 / 7$</td>
<td>521686 R 1</td>
</tr>
<tr>
<td>$n d \div m d$</td>
<td>$\boldsymbol{\checkmark}$</td>
<td>$2551 / 38$</td>
<td>$2546-38 * 60=2546-2280=266$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$266-38 * 7=266-266=0$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Therefore, $2551 / 38=67$</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples of input and output for training and testing in the simplified synthetic environment, which is used for testing the learnability of sub-tasks and ablation studies. Specifically, " + ", "-", "*", and "\" are used for addition, subtraction, multiplication, and division, respectively. Space is inserted between numbers and symbols. The input and output are formatted to mitigate the influence of natural language.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Composition of tasks in the dataset.
ing loss can reflect the test accuracy on unseen the test set, if the dataset is only trained for one epoch. Since the synthetic dataset can be generated very easily, we first create a dataset that contains a sufficient number of data samples for training and then observe the training loss and apply early stopping. We observe that the training loss does not show any significant decrease after training on about one million samples. It should be noted that convergence also depends on other hyper-parameters such as batch size and learning rate. Hence, it is recommended to use a dataset larger than what is necessary and terminate the training process when the training loss no longer decreases.</p>
<h2>G Ablation Study</h2>
<p>We name the steps (shown in the box below) as (1) extraction, (2) split, (3) expansion, (4) product, and $(5,6, \ldots)$ adding term by term. The ablation study is performed by removing one particular step while keeping other steps unchanged. We exclude the (1) "extraction" and (4) "product" steps from
the ablation study as it is crucial for multi-digit multiplication.</p>
<h2>Multiplication</h2>
<p>Calculate 397 x 4429 \nAnswer:</p>
<p>$$
\begin{aligned}
&amp; 397 \times 4429 \
&amp; =4429 \times(300+90+7) \
&amp; =4429 \times 300+4429 \times 90+4429 \times 7 \
&amp; =1328700+398610+31003 \
&amp; =1727310+31003 \
&amp; =1758313
\end{aligned}
$$</p>
<p>For division, the ablation study is performed by removing the middle step (bold) that computes the product for all iterations, while keeping other steps unchanged.</p>
<h2>Division</h2>
<p>What is 8914/64? \nAnswer:</p>
<p>$$
\begin{aligned}
&amp; 8914-64 \times 100=\mathbf{8 9 1 4}-\mathbf{6 4 0 0}=2514 \
&amp; 2514-64 \times 30=\mathbf{2 5 1 4}-\mathbf{1 9 2 0}=594 \
&amp; 594-64 \times 9=\mathbf{5 9 4}-\mathbf{5 7 6}=18 \
&amp; \text { Therefore, } 8914 \div 64=139 \mathrm{R} 18
\end{aligned}
$$</p>
<h2>H Few-shot Prompting with GPT-4</h2>
<p>Here are two examples showing 3-shot prompting using GPT-4.</p>
<h2>3-Shot prompting for multiplication</h2>
<h2>Prompt:</h2>
<p>Here are 3 examples of question-solution pairs for solving arithmetic multiplication problems. Solve the last question using the same format given in the solutions.
"question": "235658 * $91204=$ ",
"solution": "235658 * $91204=235658$ * $(90000+1000+200+4)=235658$ * $90000+235658 * 1000+235658 * 200+$ $235658 * 4=21209220000+235658000$ $+47131600+942632=21444878000$ $+47131600+942632=21492009600+$ $942632=21492952232^{\prime \prime}$
"question": "59909636 * $6070=$ ",
"solution": "59909636 * $6070=59909636$ * $(6000+70)=59909636 * 6000+59909636$ * $70=359457816000+4193674520=$ $363651490520^{\prime \prime}$
"question": "7661 * $9461=$ ",
"solution": "7661 * $9461=9461$ * (7000 $+600+60+1)=9461 * 7000+9461 *$ $600+9461 * 60+9461 * 1=66227000$ $+5676600+567660+9461=71903600$ $+567660+9461=72471260+9461=$ 72480721"
"question": "58061 * $3569=$ ",</p>
<h2>GPT-4:</h2>
<p>To solve the multiplication problem of 58061 multiplied by 3569 in the same format, we'll distribute 58061 across the sum of the digits of 3569 multiplied by their respective places.
So,
"solution": "58061 * $3569=58061$ * $(3000+500+60+9)=58061 * 3000$ $+58061 * 500+58061 * 60+58061 *$ $9=174183000+29030500+3483660+$ $522549=203213500+3483660+522549$ $=206697160+522549=207219709^{\prime \prime}$</p>
<h2>3-Shot prompting for division</h2>
<h2>Prompt:</h2>
<p>Here are 3 examples of question-solution pairs for solving arithmetic division problem. Solve the last question using the same format given in the solutions.
"question": "440331 / $22=$ ",
"solution": "440331 - $22 * 20000=440331$ $-440000=331 \ln 331-22 * 10=331-$ $220=111 \ln 111-22 * 5=111-110=$ 1\nTherefore, $440331 / 22=20015$ R 1"
"question": "115247727 / 970051 = ", "solution": "115247727 - 970051 * $100=115247727-97005100=$ 18242627\n18242627 - 970051 * $10=$ 18242627 - 9700510 = 8542117\n8542117 - 970051 * $8=8542117-7760408=$ 781709\nTherefore, 115247727 / 970051 = 118 R 781709"
"question": "4144442 / $234443=$ ",
"solution": "4144442 - $234443 * 10=$ $4144442-2344430=1800012 \ln 1800012$ - $234443 * 7=1800012-1641101=$ 158911\nTherefore, $4144442 / 234443=17$ R 158911"
"question": "262122 / 591 = "</p>
<h2>GPT-4:</h2>
<p>To solve the division problem of 262122 divided by 591 in the same format, we'll subtract multiples of 591 from 262122 until we reach a number less than 591, then that's our remainder.
So,
"solution": "262122 - 591 * $400=262122$ -$236400=25722 \ln 25722-591 * 40=25722$ $-23640=2082 \ln 2082-591 * 3=2082-$ $1773=309 \ln$ Therefore, $262122 / 591=443$ R 309"</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/liutiedong/goat.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
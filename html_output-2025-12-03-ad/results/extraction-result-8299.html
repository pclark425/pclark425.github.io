<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8299 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8299</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8299</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-258686311</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.08291v1.pdf" target="_blank">Large Language Model Guided Tree-of-Thought</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8299.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8299.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI conversational large language model (accessed via the OpenAI API) used as the core LLM in experiments; used to generate next intermediate steps for a Tree-of-Thought Sudoku solver.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI conversational autoregressive LLM accessed via API; paper only states it was used as the LLM for generating intermediate solution steps and sampled with temperature=1.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>grid-based logic puzzle (n x n grid with n symbols per row/column/block); requires spatial/relational reasoning over a 2D grid</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Three benchmark sets (10 puzzles each) for n=3,4,5 (i.e., 3x3, 4x4, 5x5 Sudoku variants). Inputs presented in natural language (e.g. board represented as nested lists or textual description). Four solver paradigms compared: zero-shot (zs), one-shot chain-of-thought example (os), few-shot chain-of-thought examples (fs), and the Tree-of-Thought (ToT) multi-round framework. ToT runs multi-round conversations (up to K rounds; K=100 in experiments) with a prompter agent, a memory module, a checker (rule-based), and a ToT controller (rule-based in this implementation). Prompts use a template that requests the LLM return a structured JSON {next_step: <next_step>} describing the next intermediate step (e.g., filling some cells). The ToT prompter also may include in-context examples for os/fs variants; temperature was set to 1.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Tree-of-Thought framework: iterative multi-round generation of intermediate partial solutions by the LLM, rule-based checker that validates Sudoku constraints, memory module storing conversation and search tree of partial boards, and a rule-based ToT controller that backtracks when checker marks a node invalid or when more than 5 children of a node have been explored. One-shot/few-shot solvers use chain-of-thought (CoT) style in-context examples. The LLM is used as a short-range heuristic to propose next steps; checker enforces correctness and ToT enables backtracking/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Success rate (fraction of puzzles solved per 10-puzzle benchmark). ToT solver (using gpt-3.5-turbo): 3x3 benchmark: 10/10 solved (100%). 4x4 benchmark: 9/10 solved (90%; failed 1 before reaching max rounds K=100). 5x5 benchmark: 8/10 solved (80%; failed 2 before max rounds). One-shot and few-shot solvers: reported to drop to around 0.5 success rate for larger puzzles (≈50% for 4x4/5x5); zero-shot performed worst across benchmarks. Paper also reports relative improvements: ToT improved success rate by ~11% compared to second best on two benchmarks, and reported ToT success rate being "80% higher" (4x4) and "60% higher" (5x5) compared to one-shot/few-shot, as stated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No direct ablation/probing to prove spatial reasoning mechanisms. Evidence is indirect: (1) successful filling of Sudoku boards from partially-filled boards (high success rates under ToT) suggests the LLM can use relational/spatial patterns present in the 2D grid; (2) the system relies on a rule-based checker to enforce spatial constraints rather than the model itself guaranteeing correctness. The paper explicitly states no dedicated probing or analysis was run to show internal spatial representations of the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared four solver methods using the same LLM: zero-shot (zs), one-shot CoT (os), few-shot CoT (fs), and ToT. Zero-shot had the worst performance. CoT (os/fs) substantially improved performance on small (3x3) puzzles but fell to ~50% on larger puzzles. ToT substantially outperformed os/fs and zs: ToT = 100% (3x3), 90% (4x4), 80% (5x5), while os/fs ≈50% for larger sizes. No human baseline performance is provided. GPT-4 is discussed in related work but not evaluated in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations: implementation used a rule-based ToT controller and rule-based checker, which limits adaptability and search efficiency; the rule-based controller has no heuristic sense of whether a partially-filled board is completable, causing inefficiency and some failures. ToT failed on 1/10 4x4 puzzles and 2/10 5x5 puzzles before reaching maximum rounds (K=100). Other limitations: randomness from temperature=1 sampling, lack of neural ToT controller or trained prompter in current experiments, no rigorous analysis showing internal spatial reasoning (no ablations/probes), and checker being problem-specific (hard to generalize to problems without polynomial-time checkers). The paper also notes general LLM limitations on long-range reasoning and error amplification without backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Implementation details: Python implementation, rule-based checker enforces Sudoku rules, memory stores search tree, ToT controller backtracks when checker invalid or >5 children explored, prompter uses structured JSON output requirement to simplify parsing. GitHub link provided in paper for solver code.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8299.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8299.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capacity autoregressive large language model referenced in the paper as an example of modern LLM capabilities; cited in related work but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned in introduction/related work as an example of state-of-the-art LLMs that excel at many reasoning tasks; no experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Mentioned generally in the context of chain-of-thought and emergent reasoning capabilities of modern LLMs; not evaluated on puzzles in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>GPT-4 is cited in the introduction as background about LLM capabilities; no direct experimental comparison to gpt-3.5-turbo or the ToT solver is included.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper references broader limitations of today's LLMs (including GPT-4) on long-range reasoning and lack of correctness checking, motivating the ToT framework; no model-specific failure cases for GPT-4 are provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Complexity-based prompting for multi-step reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8299",
    "paper_id": "paper-258686311",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "gpt-3.5-turbo (OpenAI)",
            "brief_description": "An OpenAI conversational large language model (accessed via the OpenAI API) used as the core LLM in experiments; used to generate next intermediate steps for a Tree-of-Thought Sudoku solver.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "OpenAI conversational autoregressive LLM accessed via API; paper only states it was used as the LLM for generating intermediate solution steps and sampled with temperature=1.",
            "model_size": null,
            "puzzle_name": "Sudoku",
            "puzzle_type": "grid-based logic puzzle (n x n grid with n symbols per row/column/block); requires spatial/relational reasoning over a 2D grid",
            "task_setup": "Three benchmark sets (10 puzzles each) for n=3,4,5 (i.e., 3x3, 4x4, 5x5 Sudoku variants). Inputs presented in natural language (e.g. board represented as nested lists or textual description). Four solver paradigms compared: zero-shot (zs), one-shot chain-of-thought example (os), few-shot chain-of-thought examples (fs), and the Tree-of-Thought (ToT) multi-round framework. ToT runs multi-round conversations (up to K rounds; K=100 in experiments) with a prompter agent, a memory module, a checker (rule-based), and a ToT controller (rule-based in this implementation). Prompts use a template that requests the LLM return a structured JSON {next_step: &lt;next_step&gt;} describing the next intermediate step (e.g., filling some cells). The ToT prompter also may include in-context examples for os/fs variants; temperature was set to 1.",
            "mechanisms_or_strategies": "Tree-of-Thought framework: iterative multi-round generation of intermediate partial solutions by the LLM, rule-based checker that validates Sudoku constraints, memory module storing conversation and search tree of partial boards, and a rule-based ToT controller that backtracks when checker marks a node invalid or when more than 5 children of a node have been explored. One-shot/few-shot solvers use chain-of-thought (CoT) style in-context examples. The LLM is used as a short-range heuristic to propose next steps; checker enforces correctness and ToT enables backtracking/exploration.",
            "performance_metrics": "Success rate (fraction of puzzles solved per 10-puzzle benchmark). ToT solver (using gpt-3.5-turbo): 3x3 benchmark: 10/10 solved (100%). 4x4 benchmark: 9/10 solved (90%; failed 1 before reaching max rounds K=100). 5x5 benchmark: 8/10 solved (80%; failed 2 before max rounds). One-shot and few-shot solvers: reported to drop to around 0.5 success rate for larger puzzles (≈50% for 4x4/5x5); zero-shot performed worst across benchmarks. Paper also reports relative improvements: ToT improved success rate by ~11% compared to second best on two benchmarks, and reported ToT success rate being \"80% higher\" (4x4) and \"60% higher\" (5x5) compared to one-shot/few-shot, as stated in the text.",
            "evidence_of_spatial_reasoning": "No direct ablation/probing to prove spatial reasoning mechanisms. Evidence is indirect: (1) successful filling of Sudoku boards from partially-filled boards (high success rates under ToT) suggests the LLM can use relational/spatial patterns present in the 2D grid; (2) the system relies on a rule-based checker to enforce spatial constraints rather than the model itself guaranteeing correctness. The paper explicitly states no dedicated probing or analysis was run to show internal spatial representations of the LLM.",
            "comparisons": "Compared four solver methods using the same LLM: zero-shot (zs), one-shot CoT (os), few-shot CoT (fs), and ToT. Zero-shot had the worst performance. CoT (os/fs) substantially improved performance on small (3x3) puzzles but fell to ~50% on larger puzzles. ToT substantially outperformed os/fs and zs: ToT = 100% (3x3), 90% (4x4), 80% (5x5), while os/fs ≈50% for larger sizes. No human baseline performance is provided. GPT-4 is discussed in related work but not evaluated in experiments.",
            "limitations_or_failure_cases": "Reported limitations: implementation used a rule-based ToT controller and rule-based checker, which limits adaptability and search efficiency; the rule-based controller has no heuristic sense of whether a partially-filled board is completable, causing inefficiency and some failures. ToT failed on 1/10 4x4 puzzles and 2/10 5x5 puzzles before reaching maximum rounds (K=100). Other limitations: randomness from temperature=1 sampling, lack of neural ToT controller or trained prompter in current experiments, no rigorous analysis showing internal spatial reasoning (no ablations/probes), and checker being problem-specific (hard to generalize to problems without polynomial-time checkers). The paper also notes general LLM limitations on long-range reasoning and error amplification without backtracking.",
            "notes": "Implementation details: Python implementation, rule-based checker enforces Sudoku rules, memory stores search tree, ToT controller backtracks when checker invalid or &gt;5 children explored, prompter uses structured JSON output requirement to simplify parsing. GitHub link provided in paper for solver code.",
            "uuid": "e8299.0"
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A high-capacity autoregressive large language model referenced in the paper as an example of modern LLM capabilities; cited in related work but not used in experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Mentioned in introduction/related work as an example of state-of-the-art LLMs that excel at many reasoning tasks; no experimental details provided in this paper.",
            "model_size": null,
            "puzzle_name": null,
            "puzzle_type": null,
            "task_setup": null,
            "mechanisms_or_strategies": "Mentioned generally in the context of chain-of-thought and emergent reasoning capabilities of modern LLMs; not evaluated on puzzles in this paper.",
            "performance_metrics": null,
            "evidence_of_spatial_reasoning": null,
            "comparisons": "GPT-4 is cited in the introduction as background about LLM capabilities; no direct experimental comparison to gpt-3.5-turbo or the ToT solver is included.",
            "limitations_or_failure_cases": "The paper references broader limitations of today's LLMs (including GPT-4) on long-range reasoning and lack of correctness checking, motivating the ToT framework; no model-specific failure cases for GPT-4 are provided.",
            "uuid": "e8299.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Complexity-based prompting for multi-step reasoning",
            "rating": 1,
            "sanitized_title": "complexitybased_prompting_for_multistep_reasoning"
        }
    ],
    "cost": 0.010024,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Model Guided Tree-of-Thought</p>
<p>Jieyi Long jieyi@thetalabs.org 
Theta Labs, Inc. San Jose
95128CA</p>
<p>Large Language Model Guided Tree-of-Thought</p>
<p>In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub:https://github.com/jieyilong/tree-of-thought-puzzle-solver.Preprint. Under review.</p>
<p>Introduction</p>
<p>Self-attention based auto-regressive large language models (LLMs) such as GPT-4 have recently taken the world by storm [1,2,3,4,5,6]. These LLMs excel at a variety of tasks that previously thought as extremely difficult or even impossible. For example, they are able to handle various logical and mathematical reasoning tasks, particularly those that entail "short-range reasonings" necessitating only a few steps to arrive at conclusions [6,7]. Such remarkable capabilities have even led to speculation that an early form of artificial general intelligence (AGI) may have already emerged [7]. However, today's LLMs still exhibit limitations in certain domains, especially for "long-range" reasoning tasks, where long-term planning and solution exploration are necessary [7]. When presenting a LLMs such as GPT-4 with a challenging problem solving task, especially the so called System-2 reasoning problems [8], the model does not always succeed. Although the generated answer may be indicative of the correct direction, the derivation process frequently includes logical errors. We hypothesize that there are two main contributing factors which limits the problem solving ability of LLMs:</p>
<p>Lack of correctness checking: To ensure correctness, a good practice for a human solver is to carry out verification procedures at every step of the problem-solving process, thereby ensuring the credibility of the final solution. In comparison, auto-regressive language models do not explicitly perform logical correctness checks as it generates a new token based on the previous tokens. This limits the model's capacity to rectify its own mistakes. A minor error could be amplified as the model generates more tokens, thereby leading to rapid solution quality deterioration and making it difficult to recover from mistakes.  It enhances the problem solving capability of an LLM by augmenting it with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller.</p>
<p>Solution generated linearly: As mentioned above, LLMs typically generate a token based on the preceding sequence of tokens without backward editing. On the contrary, when a human solver attempts to solve a problem, she might backtrack to previous steps if a derivation step is incorrect, or if she becomes stuck and is unable to make further progress towards arriving at the final answer. Fields Medal winner Terence Tao once shared his experiences solving hard math problems 1 : "When I was a kid, I had a romanticized notion of mathematics, that hard problems were solved in Eureka moments of inspiration... With me, it's always, Let's try this. That gets me part of the way, or that doesn't work. Now let's try this. Oh, there's a little shortcut here... You work on it long enough and you happen to make progress towards a hard problem by a back door at some point. At the end, it's usually, oh, I've solved the problem." The problem solving process as he described is a tree-like thinking process, rather than a linear chain-of-thought [9]. The limitation of linear response generation is also apparent from a computational complexity perspective. The number of computation steps an auto-regressive LLM can perform is polynomial in terms of its input length. Unless P = NP holds which contradicts the widely accepted belief, there would be problems in NP that is not solvable by auto-regressive LLMs.</p>
<p>Inspired by these two shortcomings of auto-regressive LLMs, we propose a novel framework which augments an LLM with several additional modules including an automatic "prompter agent". This framework employs a solution search strategy we call the Tree-of-Thought (ToT 2 ). This strategy solves a problem through a multi-round conversation between the LLM and the prompter agent. Figure 1a provides a visual description of the ToT search strategy, in which the LLM plays a crucial role in guiding the search for solutions. To make it more concrete, let us assume the problem to be solved is an instance of the Sudoku puzzle. The "root" node represents the initial state, corresponding to when a human mind just reads through the problem description, and begins the thinking process. A blue node in the figure represents a valid partial solution, which can be used by the LLM as a basis to generate the next search step. In the context of Sudoku puzzle solving, this means presenting a partially filled Sudoku board to an LLM and letting the LLM fill in a few more cells. The rationale is that an LLM like GPT-4 has been trained on a vast amount of text corpus which includes many Sudoku puzzle solutions. Given a partially filled board, likely the LLM is able to recognize the pattern, and provide useful insights on how to proceed following the Sudoku rules. Hence, it is highly probable that a search guided by the LLM is significantly more efficient than a brute-force search. In the figure, the search steps guided by the LLM are represented by the solid arrows. However, these steps generated by the LLM are not guaranteed to be always logically correct. Thus, we introduce a "checker module" to perform correctness checks. In Figure 1a, a gray node with an "X" marker represents a "dead-end", i.e. a partial solution that the checker module considers as invalid. For Sudoku, this means the partially filled board violates the Sudoku rules. If the current node is invalid, obviously we need to return to a parent or an ancestor node in order to correct the mistake. This can be coordinated by a module called the "ToT controller" which oversees the ToT search. With the backtracking capability, the system can regenerate the solution and thus recover from errors. In addition, even when the current node is valid, if the system remains stuck at it for too long, the ToT controller could issue a backtrack signal to explore other possible solutions. This is similar to a scenario where a human mind realizes that there is no viable path towards reaching the final solution through a particular direction, prompting her to change course and explore alternative routes. This process continues until either a full solution is found (represented by a green node in the figure), or a pre-specified maximum round of conversations is reached.</p>
<p>Note that while the above discussion utilized Sudoku solving as a tangible example to illustrate our main ideas, the ToT framework can potentially be applied to more general mathematical and logical reasoning tasks. For example, in the context of mathematical theorem proving, a full solution corresponds to the complete proof, encompassing a total of n derivation steps. On the other hand, a partial solution refers to a subset of these steps, specifically the initial k steps, where k is less than n. The checker verifies the logically correctness of a given partial proof. In parallel, the prompter agent and the ToT controller can offer hints and suggestions to the LLM, encouraging it to think about the subsequent proving step, or explore different directions for theorem proving when necessary.</p>
<p>To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created. As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving.</p>
<p>The remainder of the paper is organized as follows. Section 2 reviews the related literature and compared our approach with the most relevant works. Section 3 provides the details of the ToT system architecture. Section 4 describes our implementation of a ToT-based Sudoku puzzle solver and presents the experimental results. Finally, Section 5 discusses the limitation of the present work, and potential future extensions of the ToT framework.</p>
<p>Related Works</p>
<p>Developing intelligent systems that can reason has long been one of the primary goals of artificial intelligence [10,11,12]. Recent advancements in large language models, particularly the discovery of their emergent properties and in-context learning abilities, have opened up a new avenue for machine reasoning [6,7,9]. It is discovered that prompting language models using chain-of-thought and other hints can elicit them to output step-by-step solutions for mathematical and logical reasoning tasks [9,13]. Building on these findings, recent studies have also explored the practice of sampling multiple solutions and using self-consistency or complexity-based criteria to determine the optimal response [14,15]. Experiments were also conducted to evaluate the performance of different prompts [15]. The self-taught reasoner (STaR) [16] is a technique which asks an LLM to generate reasoning chains and drop those producing incorrect answers. Then, the model is fine-tuned with the remaining valid reasoning chains.</p>
<p>Despite showing high potential, these techniques often necessitate human involvement. For example, chain-of-thought style prompting techniques require carefully hand-crafted examples and is thus difficult to scale. Consequently, researchers have started to explore the possibility of automatic prompt generation. Early exploration in this domain includes AutoPrompt [17], prefix-tuning [18], and parameter-efficient prompt tuning [19]. This research direction received even more attention lately. In a recent study [20], the authors experimented with training verifiers to check if the solution provided by an LLM to an given mathematical problem is logically correct. If the trained verifier can effectively judge the LLM outputs, it would provide another avenue for prompt evaluation. Automatic prompt engineer [21] examines a method to select the best prompt from a set of model-generated candidates. The three-phase augment-prune-select method was suggested in [22]. It first generates multiple chain-of-thought candidates, which was then pruned based on whether the derived answer matches with the ground truths. Finally, a policy gradient based method was used to select the optimal combination of several rationale chains from the pool for CoT prompting.</p>
<p>Very recently researchers have also turned their attention to augmenting LLM with additional agents for various purposes. This is also the research field that is most relevant to our current work. AutoGPT [23] is a program which combines GPT-4 with additional modules including an execution agent and a memory unit. It can chain together LLM "thoughts", in order to autonomously achieve whatever goal the user sets. PromptPG [24] proposes an approach that can learn to select in-context examples from a small amount of training data via policy gradient for prompt learning. The PromptPG agent learns to find optimal in-context examples from a candidate pool, with the goal of maximizing the prediction rewards on given training examples when interacting with the GPT-3 environment. DEPS [25] is a proposal that utilizes multi-step reasoning and sub-task error correction to tackle complex tasks with long-range dependencies. By being able to provide explanations for errors in sub-tasks within a trial, DEPS exhibits remarkable performance. ReAct [26] is an approach that employs emergent properties present in LLMs, such as traces of verbal reasoning, to enable agents to reason and take action, resulting in impressive performance on different text-based benchmarks. Building on top of ReAct, Reflexion [27] is an approach that equips an agent with dynamic memory and self-reflection capabilities, improving its existing reasoning trace and ability to choose task-specific actions. To achieve complete automation, a simple but effective heuristic was designed to enable the agent to identify hallucination instances and prevent repetitive action sequences. Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation. However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary. This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space.</p>
<p>3 Architecture</p>
<p>The Tree-of-Thought Framework</p>
<p>Figure 1b depicts the software system that implements the ToT Framework. As mentioned earlier, it incorporates several components which enhance the problem solving capability of the LLM, including a prompter agent, a checker module, a memory module, and a ToT controller.</p>
<p>The problem solving process starts with the user inputting the problem description. The prompter agent then relays the problem to the LLM, with additional prompt text which encourages the LLM to come up with an intermediate solution instead of trying to reach the full solution in a single shot. After receiving the response from the LLM, the checker module is invoked to check the validity of the intermediate solution generated. If it passes the correctness check, the intermediate solution will be parsed and stored in the memory module. Then, based on the content of the memory module, the prompter agent generates a prompt to encourage the LLM to generate the next step. Conversely, if the LLM generates an invalid intermediate solution, the ToT controller will activate the prompter to offer hints to the LLM and request it to consider again. Note that in general, a valid intermediate solution does not always leads to the correct final solution. In order to prevent getting stuck, the ToT controller constantly monitors the search process and determines whether to continue trying from the current node or backtrack to a parent or an ancestor node and explore alternative directions.</p>
<p>The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps. In this setting, the LLM is only used for the "short-range reasoning" tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs [7]. On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions. Moreover, by allowing the system to backtrack from a valid but somewhat "hopeless" intermediate solution, the system is able to explore a larger solution space, which enhances the "long-range reasoning" capability of the system as a whole. The ToT framework thus combines the best of both world. Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform. Thus, based on the time hierarchy theorem in computational complexity theory [28], the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM.</p>
<p>ToT Modules</p>
<p>In this section we provide more details of the components of the ToT software system. Checker Module. The checker module can either be rule-based or implemented as a deep neural network. For problems that have an explicit polynomial time algorithm for correctness checking (i.e. problems in NP), rule-based checkers can be implemented. Numerous important mathematical and logical problems are in this category, for example, equation solving, polynomial factoring, 3SAT, and puzzles like Sudoku. With a rule-based checker, the ToT software can be viewed as a hybrid system which allows explicitly encoding prior knowledge (e.g. the Sudoku rules) into a neural network powered system. An alternative is to train and use a neural network based classifier as the checker [20]. This is especially useful for problems where a rule-based checker is difficult to implement, e.g. checking whether a mathematical proof is correct.</p>
<p>Memory Module. The memory module can be used to store the entire conversation history between the LLM and the prompter agent, as well as other supplemental data useful for problem solving. The data stored can be served as the information source for the prompter agent to generate helpful hints for the LLM.</p>
<p>ToT Controller. The ToT controller oversees the entire ToT search. It can be implemented in a number of ways. It can be as simple as encoding two rules: 1) if the checker thinks the current partial solution is invalid, backtrack to the parent node, and 2) if the current partial solution is valid, but the ToT search tree has explored its C children and yet failed to find the final solution, then backtrack to the parent node. Here C is an pre-configured integer.</p>
<p>A more advanced version of the ToT controller can employ a policy network to determine the backtracking policy. The network's inputs include the recent search history comprised of the sequence of the last k + 1 node visited in the search tree s i−k , .., s i−1 , s i (k is a hyper-parameter). The network also takes in c i , a Boolean variable which indicates whether the checker module considers the current node s i is valid. We can sample from the policy to determine the next action a i :
a i ∼ π t ρ (a|c i , s i , .., s i−k ), a ∈ A cand(1)
where π t ρ represents the policy network of the ToT controller with parameters ρ. The set of candidate actions A cand includes simply staying at the current node to generate the next step, and backtracking to the parent or an ancestor node at most L levels up in the search tree where L is a hyper-parameter. Thus, we can use one-hot encoding for the actions, where backtracking j levels up is represented by a vector where only the j th position is set to 1. The action vector a and checker output c i are processed by a feed-forward network (FFN) to for deep features extraction. A linear layer with learnable parameters W 1 and b 1 is added on top of the FFN to map its output to a vector g(a, c i ). The latest k + 1 visited nodes are concatenated into a string, and then added with position embedding (PE), and finally inputted into a self-attention model [1]. The idea is that by adding position embedding, the attention model will be able to make decisions based on the sequence of the recent node visits. A linear layer with learnable parameters W 2 and b 2 is added on top of the attention model to transform its output to a vector g(s i , .., s i−k ) whose dimension matches with that of g(a, c i ). Finally, we calculate the inner-products of these two vectors, and use the softmax function to compute the probability of each action candidate:
g(a, c i ) = W 1 · FFN(a, c i ) + b 1 g(s i , .., s i−k ) = W 2 · Attention(PE(s i−k ||..||s i−1 ||s i )) + b 2 π t ρ (a|c i , s i , .., s i−k ) = exp(g(a, c i ) · g(s i , .., s i−k )) a ∈A cand exp(g(a , c i ) · g(s i , .., s i−k ))(2)
In the above formula, "||" is the string concatenation operator. Section 3.3 will discuss the training algorithm for the ToT controller policy network.</p>
<p>Prompter Agent. The prompter agent gives hints to the LLM for it to generate the next search step. The most basic hint can be a generic prompt using the following template: generic_tmpl = "For the given problem: [problem description], we have come up with a partial solution: [partial solution summary]. Please derive the next step on top of this partial solution, and return the next step in the following JSON format {next_step: <next_step>}". Note that the template requires the LLM to respond with a structured JSON string. This is a trick to make it easier for the checker to extract the next step from the LLM response. To create an actual prompt from this template, the prompter needs the [problem description] and the [partial solution summary], both of which can be queried from the memory module. π w ← π t ρ if epoch is even, π p θ otherwise update the selected policy only, fix the other 7: for p i ∈ P train do 8:</p>
<p>r i ← reward(ToTSystem(p i )) attempt to solve problem p i and obtain reward r i 9:</p>
<p>w ← w + α∇ w logπ w r i 10:</p>
<p>end for 11: end for 12: end procedure Similar to the ToT controller, we can also implement the prompter agent as a policy network, which can generate prompts based on the current partial solution and the conversation history. 
e j i ∼ π p θ (e|s i , .., s i−k ), e j i ∈ E cand for j = 1, 2, ..., l(3)
where π p θ represents the policy network of the prompter agent with parameters θ. </p>
<p>The prompter policy network can be trained together with the ToT controller using multi-agent reinforcement learning methods. The training algorithm of the prompter's policy network is discussed in Section 3.3.</p>
<p>ToT System Training</p>
<p>In the previous sections, we have described the multi-agent ToT framework. This section dives into how we can train the agents, in particular, the policy networks of the ToT controller and the prompter agent. While there are many multi-agent reinforcement learning algorithms (MARL) proposed in the literature [29,30,31], in this work we adopt a relatively simple approach which uses a modified version of the REINFORCE algorithm [32] to train the policy networks of the ToT controller and the prompter agent directly. The more advanced MARL algorithms will be explored in the future.</p>
<p>First, we define a run of the ToT system as the process where a user inputs the problem description, and the ToT system attempts to solve the problem until it thinks the problem is solved, or a prespecified maximum round of conversations is reached. Next, we define the reward r of a run: if the problem is correctly solved, then r = +1. Otherwise, if the system outputs an incorrect solution, or the maximum round of conversations is reached, then r = −1.</p>
<p>Algorithm 2 Problem Solving Using the ToT System 1: Input: problem description from the user p user , max num of conversation rounds K 2: procedure SOLVE(p user , K) 3: prompt ← Prompter(p user ) 4: for round = 1, 2, .., K do 5: response ← LLM(prompt) 6: result ← Checker(response) 7: if result.isValidFinalSolution() then 8: return (result.solution) 9: end if 10: memory.store(result) 11: ctrl_signal ← ToTController(memory) 12: prompt ← Prompter(memory, ctrl_signal) 13: end for 14: return (nil) 15: end procedure</p>
<p>The training algorithm is provided in Algorithm 1. The algorithm takes two inputs, the training data set P train , and the number of training epochs N (Line 1-2). The two policy networks π t ρ (a i |s i , .., s i−k ) and π p θ (e i |s i , .., s i−k ) are randomly initialized (Line 3-4). We train the two policy networks in turns, i.e. training one network with policy gradient while keeping the other fixed (Line 6). To be more specific, when the current epoch is an even number, we select the ToT controller policy π t ρ , and keep the parameters of the prompter agent fixed. Otherwise, we select the prompter agent policy π p θ and fix the ToT controller policy. Next, the algorithm updates the parameters of the selected policy network using the policy gradient method (Line 7-9). For each problem in the training data, we attempt to solve it with a ToT system run. Based on the result, we obtain the reward for that run (Line 8). The entire training algorithm runs for N epochs.</p>
<p>Problem Solving Using the ToT System</p>
<p>After the ToT system is trained, we can use it for inference, i.e. problem solving. Algorithm 2 provides the pseudo code for solving problems using the ToT system. It starts with a user inputting description of the problem (Line 1-2). The prompter module then converts the user input into a prompt (Line 3) using a prompt template for user input, for example: user_input_prompt = "For the given problem: [problem description], please derive the first step, and return the step in the following JSON format {next_step: <next_step>}".</p>
<p>Next, up to K rounds of conversations with the LLM are conducted for problem solving (Line 4). In each round, the LLM first produces a response for the given prompt (Line 5). Then, the checker analyzes the response, and returns a result (Line 6). The result contains the partial solution extracted from the LLM response, as well as information like whether the checker considers the solution as a valid final solution, a valid intermediate solution, or an invalid partial solution, etc. If the solution is a valid final solution, the algorithm simply returns it (Line 7-9). Otherwise, the result is stored in the memory module (Line 10). Based on the content of the memory module, the ToT controller issues control signals, e.g. backtracking for l levels, to the prompter (Line 11). Finally, based on the control signal, the prompter looks up the relevant information from the memory module, and produce the next prompt for the LLM (Line 12). If no valid final solution is found within K rounds of conversations, the algorithm return nil indicating it fails to solve the problem (Line 14).</p>
<p>Evaluation</p>
<p>This section provides the evaluation methodology and experimental results for our proposed ToT framework. Our evaluation focuses on the ToT-based solver for the Sudoku problem. At the first glance, Sudoku problems seem to be just brain teasers with little practical importance. However, the generalized Sudoku problem on n 2 × n 2 grids of n × n blocks is known to be NP-complete [33]. If the ToT framework can solve instances of the generalized Sudoku problem (granted that it might takes an exponential number of rounds of conversations), in principle it can handle many other mathematical and logical reasoning tasks. In fact, it is straightforward to re-purpose the implementation described below to solve other puzzles, such as 3SAT, 3-coloring, etc. Below we first describe the implementation details of the solver. Then, we present the test suite used in our evaluation, as well as the experimental results.</p>
<p>ToT Solver for Sudoku Puzzles</p>
<p>The ToT-based Sudoku solver follows the generic framework described in Section 3 with some specific tweaks for the Sudoku problem. It allows a user to input a Sudoku puzzle using natural languages, for example: where * represents a cell to be filled".</p>
<p>We have implemented the ToT-based Sudoku solver as described in Section 4.1 in Python. We adopted a rule-based approach for the checker module since the Sudoku rules are precise and easy to check. The memory module stores the conversation history between the prompter and the LLM, as well as a search tree which maintains all the partially filled Sudoku board the LLM has generated so far. This way, when backtracking happens, the previous board configuration can be retrieved. The ToT controller in our implementation is also rule-based. It returns to the parent node in the search tree if either the current node considered invalid by the checker, or the search algorithm has explored more than 5 children of the current node. Finally the prompter agent uses a variation of the generic template mentioned above, with the [problem description] being the initial configuration of the Sudoku board input by the user, and [partial solution summary] being the partially filled board represented by the current node in the search tree. The LLM utilized in this study is the "gpt-3.5-turbo" model, which is accessible through the OpenAI API suite. The temperature parameter was set to 1 in our experiments.</p>
<p>Experimental Results</p>
<p>We have implemented four LLM-based Sudoku puzzle solvers and compared their performance: 1) zero-shot solver (zs) which directly posts the puzzle description to the LLM, 2) one-shot solver (os) which provides a chain-of-thought style step-by-step solution of a 3x3 Sudoku puzzle as an example in addition to the problem description, 3) few-shot solver (fs) which provides multiple examples with CoT-style solutions, and 4) our proposed Tree-of-Thought solver (tot). We constructed three benchmarks, comprising of ten 3x3, 4x4, and 5x5 Sudoku puzzles, respectively. The objective of a solver is to fill the n × n Sudoku grid with digits so that each row and column contain all of the digits from 1 to n (n = 3, 4, 5 in our experiments). Here the term success rate refers to the fraction of problems in a benchmark set that are successfully solved by a solver. For example, if a solver is able to solve 4 out of 10 problems in the "3x3 puzzles" benchmark set, then the success rate of this solver for this benchmark set is 0.4. As expected, the zero-shot solver has the worst performance across all the three set of benchmarks. Adding CoT-style step-by-step examples significantly boosts the success rate, especially for the 3x3 puzzles. This is expected, since one can pretty much rely on "short-range" reasoning skills, which is a strength of the LLM models, to solve a small-sized 3x3 Sudoku puzzle, espcially when CoT-style hints are provided. However, as the puzzle size gets bigger, the success rate of the one-shot and few-shot solver dropped to around 0.5. This is because solving bigger puzzles requires trial and error, which is a capability LLMs generally lack of as discussed earlier.</p>
<p>In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers. For the 3x3 benchmark set, it was able to solve all the puzzles. The success rate improves by 11% compared to the second best for the two benchmark sets. For the 4x4 benchmark set, the ToT-based solver failed to find the solution for 1 out of the 10 puzzles before reaching the maximum round of conversations (which is set to 100 in our experiments). We suspect it is due to the limited capability of the rule-based ToT controller. In particular, the rule-based controller has no sense of whether the current partially-filled board can be completed without violating the Sudoku rules, which decreases the efficiency of the solution search. We expect a neural network based ToT controller will perform better, which we will verify in the future extension of this work. Despite this, the success rate of the ToT based solver is still 80% higher compared to that of the one-shot and few-shot based solvers. Finally, for the 5x5 puzzles, the ToT-based solver failed with 2 puzzles before reaching the maximum round of conversations. Nonetheless, the success rate is 60% higher compared to that of the one-shot and few-shot based solvers.</p>
<p>Discussions and Future Works</p>
<p>In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks. To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework. One of the limitations of the current implementation is that it utilizes a rule-based checker that contains custom logic, making it less easily adaptable to other problems. For more generic problems, for example, general mathematical and logical reasoning problems, where rule-based solution checking is difficult to implement, a future direction is to explore checkers based on neural network or other probabilistic models. Moreover, the experiments we conducted in this work also uses a rule-based ToT controller, which as we pointed out, has limited capabilities. In the future, we will implement the neural network based ToT controller which can hopefully enhance the system performance. Additionally, the policy-gradient based training algorithm proposed in this work is relatively simple and may be susceptible to training stability issues. To further optimize the ToT system, more advanced multi-agent reinforcement learning algorithms, particularly those designed for cooperative agents, could be adopted.</p>
<p>Another intriguing future direction is to investigate the potential of utilizing the "self-play" technique to enable the ToT system to develop novel problem solving strategies that are not found in the LLM's training text corpus. The self-play training method is a reinforcement learning technique which was popularized by the development of competitive game-playing agents such as AlphaGo and AlphaStar [34,35,36], where an AI agent learns to improve its own strategy by playing against itself. Today's LLMs are typically trained using self-supervised learning techniques. They may have limitations when it comes to problem-solving, as they may not be able to generate samples (i.e. novel problem solving strategies) that fall outside the distribution of the training data. In other words, they may not be able to "think outside the box", which is a crucial human trait that facilitates the discovery of new knowledge. Compared to self-supervised learning, self-play based reinforcement learning enables the system to access a much broader solution space beyond the provided training examples, allowing for greater improvement. AlphaGo and similar systems have demonstrated the ability to devise strategies that surpass even those of human experts. Inspired by these examples, for ToT system training, instead of relying on the training data set P train , we can introduce a "quizzer" module which can come up with problem descriptions on its own to train the ToT controller and the prompter agent. It is worth mentioning that one of the key enablers for training AlphaGo and similar system is that the environment reward can be precisely determined, as it is straightforward to determine whether the gameplay results in a win or a loss. The ToT framework incorporates a checker that can assess the correctness of the solution, functioning similarly to the environment, particularly for problems that have well-defined solution validation rules. Thus, the reinforcement learning training methods can be readily applied. We suspect that this self-driven learning approach, similar to the self-play method, could be an effective means of improving the ToT framework's problem-solving capabilities beyond the solution examples provided in the training text corpus for the LLMs.</p>
<p>Figure 1 :
1(a) Details of the Tree-of-Thought search strategy, where a solid arrow means a search step guided by the response from the LLM, and a dashed arrow indicates backtracking commanded by the ToT controller. (b) The software system implementing the Tree-of-Thought search strategy.</p>
<p>Input: training set P train , num of training epochs N 2: procedure REINFORCE(P train ,</p>
<p>First we define the prompt template as follows: prompt_tmpl = generic_tmpl || "Here are a few examples: [in-context learning examples].", where || is the string concatenation operator. The variable [in context learning examples] are in-context learning examples for the problem being solved, which can be picked by the prompter policy network from a set of candidates, similar to the PromptPG approach [24]. The rationale is that given the current and recently attempted intermediate solution, some in-context examples might work better than others as hints for the next step. Given the recently visited node sequence s i−k , .., s i−1 , s i , our goal is to select l examples e i = {e 1 i , e 2 i , ..., e l i |e j i ∈ E cand } where E cand is a pool of in-context learning example candidates. The examples are selected according on a policy:</p>
<p>With the set of selected examples, the prompter agent generates a prompt from the template: p i = prompter(prompt_tmpl, e i , s i ), which can be fed into the LLM to obtain the next intermediate solution s i+1 = LLM (p i ). The neural network architecture for the prompter's policy network is similar to that of the ToT controller. The only difference is that since the in-context examples are expressed in natural language, instead of FFN, we use an attention model to process them: h(e) = M 1 · Attention(e) + c 1 h(s i , .., s i−k ) = M 2 · Attention(PE(s i−k ||..||s i−1 ||s i )) + c 2 π p θ (e|s i , .., s i−k ) = exp(h(e) · h(s i , .., s i−k )) e ∈E cand exp(h(e ) · h(s i , .., s i−k ))</p>
<p>Figure 2 :
2Experimental results comparing the success rate of different LLM-based Sudoku puzzle solvers across three sets of benchmarks.</p>
<p>"Please solve this 4x4 Sudoku puzzle [[3,<em>,</em>,2],[1,<em>,3,</em>],[<em>,1,</em>,3],[4,<em>,</em>,1]]</p>
<p>Figure 2
2compares the success rates of different LLM-based solvers across the three benchmarks.
https://newsroom.ucla.edu/releases/Terence-Tao-Mozart-of-Math-72522 The word "tot" means a very young child, which is an interesting analogy as this work is a preliminary exploration into the potential for automated problem-solving utilizing language models.</p>
<p>. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you needAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding, 2019.</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskeverand Dario Amodei. Language models are few-shot learnersTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.</p>
<p>. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.</p>
<p>Systems 1 and 2 thinking processes and cognitive reflection testing in medical students. Shu Tay, Paul Ryan, Anthony C Ryan, Canadian Medical Education Journal. 112016Shu Tay, Paul Ryan, and Anthony C Ryan. Systems 1 and 2 thinking processes and cognitive reflection testing in medical students. Canadian Medical Education Journal, 2016:97-103, 11 2016.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.</p>
<p>Automated Reasoning: Introduction and Applications. Larry Wos, Ross Overbeck, Ewing Lusk, Jim Boyle, Prentice Hall Professional Technical ReferenceLarry Wos, Ross Overbeck, Ewing Lusk, and Jim Boyle. Automated Reasoning: Introduction and Applications. Prentice Hall Professional Technical Reference, 1984.</p>
<p>Building Expert Systems. Frederick Hayes-Roth, Donald A Waterman, Douglas B Lenat, Addison-Wesley Longman Publishing Co., IncUSAFrederick Hayes-Roth, Donald A. Waterman, and Douglas B. Lenat. Building Expert Systems. Addison- Wesley Longman Publishing Co., Inc., USA, 1983.</p>
<p>Ronald Fagin, Joseph Y Halpern, Yoram Moses, Moshe Y Vardi, Reasoning About Knowledge. Cambridge, MA, USAMIT PressRonald Fagin, Joseph Y. Halpern, Yoram Moses, and Moshe Y. Vardi. Reasoning About Knowledge. MIT Press, Cambridge, MA, USA, 2003.</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, Gilbert Strang, Proceedings of the National Academy of Sciences. 11932Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32), aug 2022.</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning, 2023.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022.</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, Eric Wallace, Sameer Singh, Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts, 2020.</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning, 2021.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers, 2023.</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, KaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-thought from labeled data, 2023.</p>
<p>Auto-gpt: An autonomous gpt-4 experiment. Auto-gpt: An autonomous gpt-4 experiment, 2023. https://github.com/Significant-Gravitas/ Auto-GPT.</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning, 2023.</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, 2023.</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023.</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection, 2023.</p>
<p>On the computational complexity of algorithms. Juris Hartmanis, R Stearns, Transactions of The American Mathematical Society -TRANS AMER MATH SOC. 117Juris Hartmanis and R. Stearns. On the computational complexity of algorithms. Transactions of The American Mathematical Society -TRANS AMER MATH SOC, 117:285-285, 05 1965.</p>
<p>Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications. Thanh Nguyen, Ngoc Duy Nguyen, Saeid Nahavandi, IEEE Transactions on Cybernetics. 032020Thanh Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications. IEEE Transactions on Cybernetics, PP:1-14, 03 2020.</p>
<p>A review of cooperative multi-agent deep reinforcement learning. Afshin Oroojlooyjadid, Davood Hajinezhad, Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforcement learning, 2021.</p>
<p>Multi-agent reinforcement learning: A selective overview of theories and algorithms. Kaiqing Zhang, Zhuoran Yang, Tamer Başar, Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent reinforcement learning: A selective overview of theories and algorithms, 2021.</p>
<p>Policy gradient methods for reinforcement learning with function approximation. Richard S Sutton, David Mcallester, Satinder Singh, Yishay Mansour, Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99. the 12th International Conference on Neural Information Processing Systems, NIPS'99Cambridge, MA, USAMIT PressRichard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99, page 1057-1063, Cambridge, MA, USA, 1999. MIT Press.</p>
<p>Reducing the generalised sudoku problem to the hamiltonian cycle problem. Michael Haythorpe, Michael Haythorpe. Reducing the generalised sudoku problem to the hamiltonian cycle problem, 2016.</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 5297587David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis HassabisDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017.</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Nature. Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John Agapiou, Max Jaderberg, and David Silver112019Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John Agapiou, Max Jaderberg, and David Silver. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575, 11 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8810 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8810</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8810</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279305861</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.09738v1.pdf" target="_blank">Towards Multi-modal Graph Large Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Multi-modal graphs, which integrate diverse multi-modal features and relations, are ubiquitous in real-world applications. However, existing multi-modal graph learning methods are typically trained from scratch for specific graph data and tasks, failing to generalize across various multi-modal graph data and tasks. To bridge this gap, we explore the potential of Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across diverse multi-modal graph data and tasks. We propose a unified framework of multi-modal graph data, task, and model, discovering the inherent multi-granularity and multi-scale characteristics in multi-modal graphs. Specifically, we present five key desired characteristics for MG-LLM: 1) unified space for multi-modal structures and attributes, 2) capability of handling diverse multi-modal graph tasks, 3) multi-modal graph in-context learning, 4) multi-modal graph interaction with natural language, and 5) multi-modal graph reasoning. We then elaborate on the key challenges, review related works, and highlight promising future research directions towards realizing these ambitious characteristics. Finally, we summarize existing multi-modal graph datasets pertinent for model training. We believe this paper can contribute to the ongoing advancement of the research towards MG-LLM for generalization across multi-modal graph data and tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8810.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8810.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-as-Text (non-parametric)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-parametric Graph-to-Text Transformation (graph-as-text / textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that convert a graph (including node/edge modalities) into natural-language or markup text via heuristic/textual encoders (e.g., image captioning, speech-to-text, XML-like serialization) so it can be consumed by an LLM or VLM. Emphasized in the paper as a simple, flexible transformation function T that is non-parametric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-as-text / textualization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent the entire multi-modal graph by generating textual descriptions for non-text modalities (image captioning for images, speech-to-text for audio) and serializing structural relations into linear text or XML-like markup; the resulting long text sequence describes nodes, attributes and edges and is fed to an LLM/VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Multi-modal graphs (knowledge graphs, scene graphs, e-commerce product graphs, general graph-structured data)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Heuristic, non-parametric conversion: caption images, transcribe audio, and serialize node/edge relations into sentence templates or XML-like tags; optionally linearize as lists of triples or adjacency-like text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph question answering (GQA), visual QA, graph reasoning, in-context learning, general LLM-based graph tasks (classification/prediction via text prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper contrasts non-parametric textualization with parametric projectors: non-parametric methods leverage existing LLM/VLM strengths and flexible I/O but are more dependent on base model capability and prone to very long contexts and information loss compared to learned parametric mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Flexible input/output; can immediately leverage pretrained LLM/VLM capabilities without training new encoders; simple to implement; supports natural-language interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Potentially very long context windows; substantial information loss when converting rich modalities (especially images, fine-grained structure) to text; some modal information is inherently hard to textualize; heavy reliance on underlying LLM/VLM abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Long/complex graphs become intractable in textual form; models hallucinate or miss visual/structural details after textualization; ambiguity in textual descriptions can lead to incorrect reasoning (example: ambiguous entity disambiguation requiring joint multi-modal cues).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Multi-modal Graph Large Language Model', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8810.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8810.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaGA projector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaGA parametric projector (graph -> token-space structured sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parametric projection approach (LLaGA) that learns a mapping from graph structures to structured token sequences/embeddings suitable for ingestion by an LLM, enabling the LLM to process graph-structured data more effectively than pure textualization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llaga: Large language and graph assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Parametric projector -> structured token sequences (LLaGA)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A trained neural projector transforms graph data (nodes, edges, attributes) into structured sequences in the LLM token embedding space; these sequences are then fed into the LLM so the model treats graph content as native token inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graph-structured data (knowledge graphs, attributed graphs), applicable to multi-modal graphs when modalities are encoded into projector inputs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Learned parametric mapping (projector network) that encodes nodes/edges/attributes into sequences or token embeddings aligned with the LLM input format instead of converting to free-form natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Various LLM-driven graph tasks such as reasoning, question answering, classification and generation where graphs are inputs to LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper notes this is more structured than non-parametric textualization and can improve LLM handling of graph structure; compared qualitatively to graph-as-text it reduces reliance on raw LLM capability and can be trained to preserve structural information, at cost of training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves more structural information by design; enables LLMs to consume graphs without extremely long natural-language descriptions; can be optimized jointly with downstream objectives to improve performance and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires training of the projector (parametric); potential mismatch if projector not well aligned with LLM; still may discard fine-grained modal details depending on design.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly quantified in the paper; potential failure when projector is insufficiently expressive or when distributional shift occurs between graphs seen during projector training and those at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Multi-modal Graph Large Language Model', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8810.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8810.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GOFA interleaved</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GOFA: Interleaved GNN layers within LLM (hybrid parametric transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach that inserts (randomly initialized) GNN layers into a frozen LLM stack so structural graph processing is performed in-network and combined with LLM semantic capabilities, avoiding a separate explicit textualization step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gofa: A generative one-for-all model for joint graph language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Interleaved GNN-LLM hybrid (GOFA)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph structure is processed by GNN layers interleaved with LLM transformer layers; representations are jointly transformed within the model so the LLM receives structural-aware token embeddings rather than an external textual serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs and attributed graphs; applicable to multi-modal graphs when each modality is encoded into the same internal representation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Parametric in-model processing: apply GNN message passing operations inside the LLM computation graph to directly incorporate structural signals into token representations without explicit graph-to-text conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Joint graph-language modeling, generative tasks over graphs, reasoning and other graph-enabled LLM tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper positions GOFA as an alternative to external projectors or textualization—integrates structural processing more tightly. Compared to pure projector methods it fuses structure and language inside the model, potentially improving structural fidelity at the cost of architectural complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Native structural integration, reduced need for lossy serialization, potential for stronger alignment between structure and language representations.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Engineering and training complexity; interleaving modules may complicate transfer and scaling; requires architectural changes rather than a pre-processing step.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not empirically detailed in this paper; possible fragility if inserted GNNs are fixed or not properly trained, leading to cascading errors (paper warns fragility of staged pipelines generally).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Multi-modal Graph Large Language Model', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8810.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8810.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization / Triples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Linearization (triples / template filling / sequence of triples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explicit linearization strategies that convert graphs or subgraphs into sequences (subject-predicate-object triples, adjacency lists, or sentence templates) to present graph context to LLMs for in-context learning or prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization (triples / templates / adjacency lists)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transform graph information into ordered textual sequences: lists of triples (S,P,O), template sentences per edge/node, or adjacency/edge lists so that a transformer-based LM can process graph context as text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs, scene graphs, subgraphs, general attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Deterministic linearization: enumerate nodes/edges and convert each relation to a triple or templated sentence; optionally order nodes/subgraphs and concatenate into prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>In-context learning, graph QA, structural reasoning, tasks that rely on supplying example graph contexts to an LLM</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to non-parametric free-form textualization, linearization into triples is more structured and compact but still can create long contexts; the paper cites linearization as a common strategy but notes limitations in handling graph topologies and context length.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, interpretable, easy to implement; preserves explicit relational triples making some structural information explicit to the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Sequence length grows with graph size; ordering choices can bias model; loses native graph topology (non-sequential structure) and can be inefficient for large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performs poorly on large/dense graphs due to context length; loses multi-hop locality and can make multi-hop reasoning harder unless auxiliary structure or prompts are used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Multi-modal Graph Large Language Model', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8810.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8810.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampled subgraph prompts / retrieval-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampled Subgraph Prompting and Retrieval-Augmented Graph Contexts (AskGNN / graph-RAG adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that select relevant subgraphs or retrieved graph-context snippets to include in prompts so an LLM can perform few-shot or in-context graph reasoning without ingesting entire graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Sampled subgraph prompting / retrieval-augmented graph contexts</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Select and linearize only task-relevant subgraphs (via sampling or retrieval) and include them as examples in the prompt; retrieval modules fetch subgraphs from a store to provide contextual evidence for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Large multi-modal graphs, knowledge graphs, neighborhood-centric ego-graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Subgraph selection (heuristic or learned retrieval) followed by linearization/templating of the selected subgraph for inclusion in the LM prompt; may combine with external retrieval systems (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Multi-modal graph in-context learning, few-shot node classification, graph QA, reasoning tasks where local context suffices</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper reports that sampled/retrieved subgraph prompting can improve few-shot performance compared to naively textualizing the entire graph because it keeps context size manageable; often combined with GNN encoders for structure-aware selection.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Manages context size, focuses LM on relevant evidence, can leverage retrieval to scale to large graphs; improves few-shot/in-context learning feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Quality depends on retrieval/subgraph selection; may omit crucial long-range context leading to wrong inferences; requires additional retrieval infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Failures occur when retrieval omits necessary multi-hop context or when the selected subgraph lacks disambiguating cross-modal cues; cascading pipeline errors from retrieval/selection step can degrade results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Multi-modal Graph Large Language Model', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8810.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8810.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-modal graph tokenization (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-modal Graph Vocabulary and Tokenizer (proposed / future direction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed direction: design a unified multi-modal graph vocabulary and tokenizer that can discretize multi-granular graph elements (pixels, words, nodes, subgraphs) into a shared token space for direct LLM consumption, reducing the need for lossy textualization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Multi-modal graph tokenization / unified vocabulary</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Create tokens or subtoken schemes that represent graph structural units and heterogeneous modalities at multiple granularities, enabling early, deep integration of modality and topology into a single tokenized sequence or embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Multi-modal graphs across granularities (images, text, audio, graph structures, multi-omics)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tokenization / vocabulary design (proposed): discretize pixels/patches/words/nodes/edges into tokens with explicit structural markers so the model ingests a unified token stream representing both attributes and topology.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Intended for all MG-LLM downstream tasks: in-context learning, generation, reasoning, classification, graph editing and natural-language interaction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Proposed as preferable to ad-hoc textualization or late fusion since it could reduce information loss and better preserve multi-granular structure; no empirical comparison provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Potentially better preservation of multi-modal and structural information, unified interfaces for many tasks, supports multi-granularity natively.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Highly challenging to design; requires large-scale pretraining data and computational resources; tokenization choices may induce bias or lose continuous information.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not empirically evaluated in this paper; anticipated failure modes include poor generalization if vocabulary too domain-specific, and token explosion for very fine-grained graphs leading to infeasible context sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Multi-modal Graph Large Language Model', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llaga: Large language and graph assistant. <em>(Rating: 2)</em></li>
                <li>Gofa: A generative one-for-all model for joint graph language modeling. <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models. <em>(Rating: 2)</em></li>
                <li>Graph2text or graph2token: A perspective of large language models for graph learning. <em>(Rating: 2)</em></li>
                <li>Can language models solve graph problems in natural language? <em>(Rating: 1)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8810",
    "paper_id": "paper-279305861",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Graph-as-Text (non-parametric)",
            "name_full": "Non-parametric Graph-to-Text Transformation (graph-as-text / textualization)",
            "brief_description": "A family of methods that convert a graph (including node/edge modalities) into natural-language or markup text via heuristic/textual encoders (e.g., image captioning, speech-to-text, XML-like serialization) so it can be consumed by an LLM or VLM. Emphasized in the paper as a simple, flexible transformation function T that is non-parametric.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph-as-text / textualization / serialization",
            "representation_description": "Represent the entire multi-modal graph by generating textual descriptions for non-text modalities (image captioning for images, speech-to-text for audio) and serializing structural relations into linear text or XML-like markup; the resulting long text sequence describes nodes, attributes and edges and is fed to an LLM/VLM.",
            "graph_type": "Multi-modal graphs (knowledge graphs, scene graphs, e-commerce product graphs, general graph-structured data)",
            "conversion_method": "Heuristic, non-parametric conversion: caption images, transcribe audio, and serialize node/edge relations into sentence templates or XML-like tags; optionally linearize as lists of triples or adjacency-like text.",
            "downstream_task": "Graph question answering (GQA), visual QA, graph reasoning, in-context learning, general LLM-based graph tasks (classification/prediction via text prompts)",
            "performance_metrics": null,
            "comparison_to_others": "Paper contrasts non-parametric textualization with parametric projectors: non-parametric methods leverage existing LLM/VLM strengths and flexible I/O but are more dependent on base model capability and prone to very long contexts and information loss compared to learned parametric mappings.",
            "advantages": "Flexible input/output; can immediately leverage pretrained LLM/VLM capabilities without training new encoders; simple to implement; supports natural-language interaction.",
            "disadvantages": "Potentially very long context windows; substantial information loss when converting rich modalities (especially images, fine-grained structure) to text; some modal information is inherently hard to textualize; heavy reliance on underlying LLM/VLM abilities.",
            "failure_cases": "Long/complex graphs become intractable in textual form; models hallucinate or miss visual/structural details after textualization; ambiguity in textual descriptions can lead to incorrect reasoning (example: ambiguous entity disambiguation requiring joint multi-modal cues).",
            "uuid": "e8810.0",
            "source_info": {
                "paper_title": "Towards Multi-modal Graph Large Language Model",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLaGA projector",
            "name_full": "LLaGA parametric projector (graph -&gt; token-space structured sequences)",
            "brief_description": "A parametric projection approach (LLaGA) that learns a mapping from graph structures to structured token sequences/embeddings suitable for ingestion by an LLM, enabling the LLM to process graph-structured data more effectively than pure textualization.",
            "citation_title": "Llaga: Large language and graph assistant.",
            "mention_or_use": "mention",
            "representation_name": "Parametric projector -&gt; structured token sequences (LLaGA)",
            "representation_description": "A trained neural projector transforms graph data (nodes, edges, attributes) into structured sequences in the LLM token embedding space; these sequences are then fed into the LLM so the model treats graph content as native token inputs.",
            "graph_type": "General graph-structured data (knowledge graphs, attributed graphs), applicable to multi-modal graphs when modalities are encoded into projector inputs",
            "conversion_method": "Learned parametric mapping (projector network) that encodes nodes/edges/attributes into sequences or token embeddings aligned with the LLM input format instead of converting to free-form natural language.",
            "downstream_task": "Various LLM-driven graph tasks such as reasoning, question answering, classification and generation where graphs are inputs to LLMs",
            "performance_metrics": null,
            "comparison_to_others": "Paper notes this is more structured than non-parametric textualization and can improve LLM handling of graph structure; compared qualitatively to graph-as-text it reduces reliance on raw LLM capability and can be trained to preserve structural information, at cost of training complexity.",
            "advantages": "Preserves more structural information by design; enables LLMs to consume graphs without extremely long natural-language descriptions; can be optimized jointly with downstream objectives to improve performance and interpretability.",
            "disadvantages": "Requires training of the projector (parametric); potential mismatch if projector not well aligned with LLM; still may discard fine-grained modal details depending on design.",
            "failure_cases": "Not explicitly quantified in the paper; potential failure when projector is insufficiently expressive or when distributional shift occurs between graphs seen during projector training and those at inference.",
            "uuid": "e8810.1",
            "source_info": {
                "paper_title": "Towards Multi-modal Graph Large Language Model",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GOFA interleaved",
            "name_full": "GOFA: Interleaved GNN layers within LLM (hybrid parametric transformation)",
            "brief_description": "A hybrid approach that inserts (randomly initialized) GNN layers into a frozen LLM stack so structural graph processing is performed in-network and combined with LLM semantic capabilities, avoiding a separate explicit textualization step.",
            "citation_title": "Gofa: A generative one-for-all model for joint graph language modeling.",
            "mention_or_use": "mention",
            "representation_name": "Interleaved GNN-LLM hybrid (GOFA)",
            "representation_description": "Graph structure is processed by GNN layers interleaved with LLM transformer layers; representations are jointly transformed within the model so the LLM receives structural-aware token embeddings rather than an external textual serialization.",
            "graph_type": "General graphs and attributed graphs; applicable to multi-modal graphs when each modality is encoded into the same internal representation pipeline",
            "conversion_method": "Parametric in-model processing: apply GNN message passing operations inside the LLM computation graph to directly incorporate structural signals into token representations without explicit graph-to-text conversion.",
            "downstream_task": "Joint graph-language modeling, generative tasks over graphs, reasoning and other graph-enabled LLM tasks",
            "performance_metrics": null,
            "comparison_to_others": "Paper positions GOFA as an alternative to external projectors or textualization—integrates structural processing more tightly. Compared to pure projector methods it fuses structure and language inside the model, potentially improving structural fidelity at the cost of architectural complexity.",
            "advantages": "Native structural integration, reduced need for lossy serialization, potential for stronger alignment between structure and language representations.",
            "disadvantages": "Engineering and training complexity; interleaving modules may complicate transfer and scaling; requires architectural changes rather than a pre-processing step.",
            "failure_cases": "Not empirically detailed in this paper; possible fragility if inserted GNNs are fixed or not properly trained, leading to cascading errors (paper warns fragility of staged pipelines generally).",
            "uuid": "e8810.2",
            "source_info": {
                "paper_title": "Towards Multi-modal Graph Large Language Model",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Linearization / Triples",
            "name_full": "Graph Linearization (triples / template filling / sequence of triples)",
            "brief_description": "Explicit linearization strategies that convert graphs or subgraphs into sequences (subject-predicate-object triples, adjacency lists, or sentence templates) to present graph context to LLMs for in-context learning or prompting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Linearization (triples / templates / adjacency lists)",
            "representation_description": "Transform graph information into ordered textual sequences: lists of triples (S,P,O), template sentences per edge/node, or adjacency/edge lists so that a transformer-based LM can process graph context as text.",
            "graph_type": "Knowledge graphs, scene graphs, subgraphs, general attributed graphs",
            "conversion_method": "Deterministic linearization: enumerate nodes/edges and convert each relation to a triple or templated sentence; optionally order nodes/subgraphs and concatenate into prompt.",
            "downstream_task": "In-context learning, graph QA, structural reasoning, tasks that rely on supplying example graph contexts to an LLM",
            "performance_metrics": null,
            "comparison_to_others": "Compared to non-parametric free-form textualization, linearization into triples is more structured and compact but still can create long contexts; the paper cites linearization as a common strategy but notes limitations in handling graph topologies and context length.",
            "advantages": "Simple, interpretable, easy to implement; preserves explicit relational triples making some structural information explicit to the LM.",
            "disadvantages": "Sequence length grows with graph size; ordering choices can bias model; loses native graph topology (non-sequential structure) and can be inefficient for large graphs.",
            "failure_cases": "Performs poorly on large/dense graphs due to context length; loses multi-hop locality and can make multi-hop reasoning harder unless auxiliary structure or prompts are used.",
            "uuid": "e8810.3",
            "source_info": {
                "paper_title": "Towards Multi-modal Graph Large Language Model",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Sampled subgraph prompts / retrieval-RAG",
            "name_full": "Sampled Subgraph Prompting and Retrieval-Augmented Graph Contexts (AskGNN / graph-RAG adaptations)",
            "brief_description": "Methods that select relevant subgraphs or retrieved graph-context snippets to include in prompts so an LLM can perform few-shot or in-context graph reasoning without ingesting entire graphs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Sampled subgraph prompting / retrieval-augmented graph contexts",
            "representation_description": "Select and linearize only task-relevant subgraphs (via sampling or retrieval) and include them as examples in the prompt; retrieval modules fetch subgraphs from a store to provide contextual evidence for the LLM.",
            "graph_type": "Large multi-modal graphs, knowledge graphs, neighborhood-centric ego-graphs",
            "conversion_method": "Subgraph selection (heuristic or learned retrieval) followed by linearization/templating of the selected subgraph for inclusion in the LM prompt; may combine with external retrieval systems (RAG).",
            "downstream_task": "Multi-modal graph in-context learning, few-shot node classification, graph QA, reasoning tasks where local context suffices",
            "performance_metrics": null,
            "comparison_to_others": "Paper reports that sampled/retrieved subgraph prompting can improve few-shot performance compared to naively textualizing the entire graph because it keeps context size manageable; often combined with GNN encoders for structure-aware selection.",
            "advantages": "Manages context size, focuses LM on relevant evidence, can leverage retrieval to scale to large graphs; improves few-shot/in-context learning feasibility.",
            "disadvantages": "Quality depends on retrieval/subgraph selection; may omit crucial long-range context leading to wrong inferences; requires additional retrieval infrastructure.",
            "failure_cases": "Failures occur when retrieval omits necessary multi-hop context or when the selected subgraph lacks disambiguating cross-modal cues; cascading pipeline errors from retrieval/selection step can degrade results.",
            "uuid": "e8810.4",
            "source_info": {
                "paper_title": "Towards Multi-modal Graph Large Language Model",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Multi-modal graph tokenization (proposed)",
            "name_full": "Multi-modal Graph Vocabulary and Tokenizer (proposed / future direction)",
            "brief_description": "A proposed direction: design a unified multi-modal graph vocabulary and tokenizer that can discretize multi-granular graph elements (pixels, words, nodes, subgraphs) into a shared token space for direct LLM consumption, reducing the need for lossy textualization.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "representation_name": "Multi-modal graph tokenization / unified vocabulary",
            "representation_description": "Create tokens or subtoken schemes that represent graph structural units and heterogeneous modalities at multiple granularities, enabling early, deep integration of modality and topology into a single tokenized sequence or embedding space.",
            "graph_type": "Multi-modal graphs across granularities (images, text, audio, graph structures, multi-omics)",
            "conversion_method": "Tokenization / vocabulary design (proposed): discretize pixels/patches/words/nodes/edges into tokens with explicit structural markers so the model ingests a unified token stream representing both attributes and topology.",
            "downstream_task": "Intended for all MG-LLM downstream tasks: in-context learning, generation, reasoning, classification, graph editing and natural-language interaction",
            "performance_metrics": null,
            "comparison_to_others": "Proposed as preferable to ad-hoc textualization or late fusion since it could reduce information loss and better preserve multi-granular structure; no empirical comparison provided in the paper.",
            "advantages": "Potentially better preservation of multi-modal and structural information, unified interfaces for many tasks, supports multi-granularity natively.",
            "disadvantages": "Highly challenging to design; requires large-scale pretraining data and computational resources; tokenization choices may induce bias or lose continuous information.",
            "failure_cases": "Not empirically evaluated in this paper; anticipated failure modes include poor generalization if vocabulary too domain-specific, and token explosion for very fine-grained graphs leading to infeasible context sizes.",
            "uuid": "e8810.5",
            "source_info": {
                "paper_title": "Towards Multi-modal Graph Large Language Model",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llaga: Large language and graph assistant.",
            "rating": 2,
            "sanitized_title": "llaga_large_language_and_graph_assistant"
        },
        {
            "paper_title": "Gofa: A generative one-for-all model for joint graph language modeling.",
            "rating": 2,
            "sanitized_title": "gofa_a_generative_oneforall_model_for_joint_graph_language_modeling"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models.",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graph2text or graph2token: A perspective of large language models for graph learning.",
            "rating": 2,
            "sanitized_title": "graph2text_or_graph2token_a_perspective_of_large_language_models_for_graph_learning"
        },
        {
            "paper_title": "Can language models solve graph problems in natural language?",
            "rating": 1,
            "sanitized_title": "can_language_models_solve_graph_problems_in_natural_language"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "rating": 1,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        }
    ],
    "cost": 0.01503525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Multi-modal Graph Large Language Model
11 Jun 2025</p>
<p>Xin Wang 
Zeyang Zhang 
Linxin Xiao 
Haibo Chen 
Chendi Ge 
Wenwu Zhu 
Gqa Generation 
Towards Multi-modal Graph Large Language Model
11 Jun 20253AE63CC208DC173236E3246CF3A5854BarXiv:2506.09738v1[cs.LG]Multi-modal GraphLarge Language ModelFoundation ModelGraph Machine LearningMulti-modality
Multi-modal graphs, which integrate diverse multimodal features and relations, are ubiquitous in real-world applications.However, existing multi-modal graph learning methods are typically trained from scratch for specific graph data and tasks, failing to generalize across various multi-modal graph data and tasks.To bridge this gap, we explore the potential of Multimodal Graph Large Language Models (MG-LLM) to unify and generalize across diverse multi-modal graph data and tasks.We propose a unified framework of multi-modal graph data, task, and model, discovering the inherent multi-granularity and multi-scale characteristics in multi-modal graphs.Specifically, we present five key desired characteristics for MG-LLM: 1) unified space for multi-modal structures and attributes, 2) capability of handling diverse multi-modal graph tasks, 3) multi-modal graph in-context learning, 4) multi-modal graph interaction with natural language, and 5) multi-modal graph reasoning.We then elaborate on the key challenges, review related works, and highlight promising future research directions towards realizing these ambitious characteristics.Finally, we summarize existing multi-modal graph datasets pertinent for model training.We believe this paper can contribute to the ongoing advancement of the research towards MG-LLM for generalization across multi-modal graph data and tasks.</p>
<p>I. INTRODUCTION</p>
<p>M Ulti-modal graphs, which integrate features from diverse modalities such as text, image, audio and video, as well as capture the complex intra-model and inter-modal relations, are becoming increasingly ubiquitous in real-world applications.From social networks [1] and e-commerce [2] platforms to scientific discovery in biomedicine and materials science [3], [4], these complex data structures offer a richer, more holistic representation of interconnected entities than traditional unimodal graphs, which unlock more opportunities for advanced analytics, reasoning, and generation capabilities over multimodal information.</p>
<p>However, multi-modal graph learning currently faces a significant issue: existing methods are predominantly designed for specific tasks on particular types of graphs.This specialization often limits their applicability, preventing them from generalizing effectively across the vast diversity of multi-modal graph data and tasks encountered in practice.This lack of universality necessitates constant redesign and retraining for new scenarios, hindering the development of truly versatile and scalable solutions.</p>
<p>To bridge this gap, we explore the potential of Multimodal Graph Large Language Models (MG-LLM).Inspired by the remarkable success of large language models (LLMs) in unifying diverse natural language tasks, we propose that MG-LLM can serve as a powerful paradigm to unify and Wenwu Zhu is the corresponding author (email: wwzhu@tsinghua.edu.cn)generalize across the complex landscape of multi-modal graph data and tasks.Our exploration begins by establishing a unified framework for multi-modal graph data, tasks, and models, which uncovers the inherent characteristics of multi-modal graphs, i.e., multi-granularity and multi-scale.</p>
<p>Specifically, we highlight that multi-modal graphs inherently exhibit multi-granularity, organizing information from finegrained features like pixels and words to coarse-grained concepts such as entire images or documents, along with diverse structural complexities.This leads to multi-scale characteristics in multi-modal graph tasks, where inputs and outputs can vary dramatically in their scope, from individual nodes to entire graph structures.</p>
<p>Building upon this foundational understanding, we articulate five key desired characteristics towards MG-LLM:</p>
<p>• Unified Space for Multi-modal Structures and Attributes: the ability to align and represent diverse multi-modal features and relations within a single unified embedding space, capable of handling highly irregular and continuous information.</p>
<p>• Ability of Handling Diverse Multi-modal Graph Tasks: the capacity to frame and solve all multi-modal graph tasks, from traditional discriminative problems like node classification to emerging generative tasks such as multi-modal content generation, under a unified generative modeling paradigm.</p>
<p>• Multi-modal Graph In-context Learning: the capability of performing novel tasks by leveraging a limited number of multi-modal graph examples provided directly within the prompt, without requiring explicit model fine-tuning.• Multi-modal Graph Interaction with Natural Language: the possibility of enabling users to query, edit, generate, and reason about complex multi-modal graph-structured knowledge using intuitive natural language, bridging the gap between human language and structured data.• Multi-modal Graph Reasoning: the proficiency in performing complex multi-hop, cross-modal reasoning, including analogical inference, by seamlessly combining information from various modalities and relational structures.</p>
<p>While the vision of MG-LLMis ambitious, realizing these characteristics presents significant challenges, ranging from developing unified multi-modal graph vocabularies and tokenization schemes to multi-modal graph architectures capable of large-scale pretraining.This paper delves into these key challenges, reviews existing research that moves towards this paradigm, and outlines promising future research directions to accelerate the development of MG-LLM for generalizing across diverse multi-modal graph data and tasks.Finally, we summarize existing multi-modal graph datasets that could be useful for the training and evaluation of such models.This while we focus on introducing a novel conceptual framework and put forth the vision of MG-LLM.By identifying the inherent characteristics of multi-modal graphs and the desired characteristics for MG-LLM, we chart a new research roadmap towards developing multi-modal graph large language models, thereby complementing surveys of the existing state-of-the-art by looking towards a next-generation paradigm.</p>
<p>II. TOWARDS A UNIFIED VIEW OF MULTI-MODAL GRAPH DATA, TASK, AND MODEL</p>
<p>In this section, we introduce a unified framework of multimodal graph data, task, and model, and remarks on the inherent characteristics in multi-modal graphs, serving as a foundation to discuss the desired characteristics of MG-LLM.The overall framework is shown in Figure 2.</p>
<p>A. Unified Formulation of Multi-modal Graph Data</p>
<p>In this section, we define multi-modal graphs, extending standard graphs with diverse node and edge modalities.Then we outline three decomposable types (feature-, node-, graph-level), and their versatility in representing various data forms, from single instances to full datasets.Moreover, we give remarks on the challenges of indecomposability and multi-granularity for building effective MG-LLM.a) Graph: A graph G = (V, E) consists of a finite set of vertices V = {v 1 , v 2 , . . ., v n } and a set of edges E ⊆ V × V, each edge being an ordered pair of vertices denoting a directed relation between them.b) Multi-modal Graph: A modality is a distinct type or source of information associated with nodes or edges.Let M = {1, 2, . . ., M } denote the set of all modalities.For each m ∈ M, we define a mapping F m from the modality-specific node feature space V to a shared representation space X , such that F m : V → X .We define a mapping set F = {F m } M m=1 for all M modalities.The space X serves as a unified embedding space for all modalities.Similarly, we define a mapping F m from the modality-specific edge feature space E to the shared representation space X , such that F m : E → X .For simplicity, we reuse F m as the modality-m map for both nodes and edges.We likewise leave out multimodal edges in the formulation, even though extending them would be straightforward.The features in different modalities could be texts, images, audios, videos, etc.A multi-modal graph can be defined by the quadruple G = (V, E, F, M). c) Special Cases of Decomposable Multi-modal Graphs: By instantiation of the modality set and feature mapping, we could obtain several classic types of multi-modal graphs which are ubiquitous in real-world applications [6].These types of multi-modal graphs share the same assumption that they can be decomposed by modality from different perspectives, i.e., feature, node, and graph:</p>
<p>Unified space</p>
<p>Multi-granularity Multimodal Graph Data</p>
<p>Multi-scale Multimodal Graph Tasks
Node
• Feature-level Multi-modal Graph, where the features of nodes or edges come from different modalities, i.e.,
G = M m=1 G m = M m=1 (V, E, F m , {m})
, where m representing one modality in M modalities.The feature of node v could be represented as
x(v) = M m=1 F m (v).
For example, on an e-commerce product graph [7], each node has the feature of product title and image, i.e., x(v) = (F text (v), F image (v)).</p>
<p>• Node-level Multi-modal Graph, where the nodes or edges come from different modalities, while each node or edge has unimodal features, i.e., G =
M m=1 G m = M m=1 (V m , E, F m , {m})
, where m representing one modality in M modalities, and V i V j = ∅.For example, on a multimodal knowledge base [8], each node might be either an image or a textual description.</p>
<p>• Graph-level Multi-modal Graph, where the graphs come from different modalities, while each graph has unimodal features, i.e., G =
M m=1 G m = M m=1 (V m , E m , X m , {m})
, where m representing one modality in M modalities, and E i E j = ∅.For example, on a multi-modal question answering graph [9], we may have a graph with images, a graph with texts, etc. Remark 1. (Indecomposable Characteristics) Although practitioners may model their data with the aforementioned decomposable multi-modal graphs for convenience, most multimodal graphs in real-world scenarios, with nodes and edges having features from various modalities, may not be easily decomposable to several uni-modal subgraphs.For instance, in a multi-modal graph where the text node says 'The Transformer was incredible!', the image node shows Optimus Prime (a central robot character from the Transformers movie series), and the knowledge node links to the movie Transformers, only joint reasoning over all three nodes can resolve the ambiguity and correctly interpret 'Transformer' as a film character rather than a neural network architecture.Due to the indecomposable characteristics, established multi-modal fusion techniques in other multi-modal fields [10] may fail to flexibly solve multimodal graph problems, calling for the need of native modeling of multi-modal graph data in multi-modal graph large language models.d) Special Cases of Multi-modal Graph Instances: The versatility of multi-modal graphs allows them to represent not only complex inter-modal relationships but also instances or datasets composed of single or multiple modalities as special cases, e.g., instances of texts, images, audios, videos, etc, or pairs of image-captions, text-audios, etc.Here are examples of representing single-modal instances:</p>
<p>• a text sequence can be represented by a multi-modal graph with a single text-attributed node, i.e., G = (V = {v}, E = ∅, {F text }, {text}), where F text is the text feature mapping function.</p>
<p>• a text sequence, more granularly, can be represented by a multi-modal graph where each word is a node and sequential or semantic connections form edges, i.e., G = (V, E, {F word }, {word}), where
V = {v 1 , . . . , v L }, E = {(v i , v i+1 )} L−1 i=1
, L is the length of the text sequence, and F word is the word feature mapping function.</p>
<p>• an image can be represented as a multi-modal graph where pixels are nodes and their grid-like interconnections form edges, i.e., an H × W image can be
G = V = {v ij } H,W i=1,j=1(V image ∪ V text , E image-text , {F image , F text }, {image, text}), where V image = {v 1 , . . . , v K } are image nodes, V text = {u 1 , . . . , u K } are caption nodes, E image-text = {(v k , u k ) | 1 ≤ k ≤ K}
are edges connecting images to their captions, and F image and F text are the image and text feature mapping functions, respectively.This ability to abstract various data forms into a unified graph structure underscores the expressive power of multi-modal graphs.Remark 2. (Multi-granularity Characteristics) Multi-modal graphs inherently possess the ability to organize data with multi-granularity across modalities, features, and structures.However, this capability is a double-edged sword.While they can represent vast amounts of information, they also introduce significant challenges for models to process them flexibly.Unlike other domains that feature units of roughly uniform granularity, such as word tokens in natural language processing (NLP) or image pixels in computer vision (CV), multi-modal graphs often contain units ranging from fine-grained features (e.g., pixels, words) to coarse-grained concepts (e.g., full images, entire documents).To build an effective MG-LLM capable of flexibly handling information at diverse granularities on multi-modal graphs, it may be necessary to design a unified multi-modal graph vocabulary and tokenizer for learning multimodal graph representations in a shared space.</p>
<p>B. Generative Modeling of Multi-modal Graph Tasks</p>
<p>In this section, we can frame, by generative modeling, that all multi-modal graph tasks are multi-modal graph generation.Due to the inherent multi-granularity characteristics of multimodal graphs, we can unify several classical discriminative tasks and emerging generative tasks under a single generative perspective, which can bring advantages of unified task forms, types, and interfaces.Suppose MG-LLM learns a conditional probability distribution to generate an output multi-modal graph G out given an input G in .Formally, the objective is to model:
P (G out |G in ; Θ),(1)
where Θ are the MG-LLM's parameters.Various multi-modal graph tasks can be redefined generatively:</p>
<p>• Multi-modal Node Classification (NC) aims to take a multi-modal ego-graph centered around a target node as input, and generate a multi-modal graph representing the predicted class, i.e., P
(G class | G v ), where G v = (V v , E v , F v , M v )| G Q ), where G Q = (V ∪ {v Q }, E ∪ E Q , X ∪ {F text (v Q )}, M ∪ {text})
is the graph augmented with v Q and potential edges E Q and G answer is the generated answer graph (e.g., a text/image node or a subgraph).</p>
<p>• Multi-modal Graph Reasoning (GR) extends GQA with complex multi-hop reasoning.The generated output G reasoning may embody complex logical structures or a chain of thought, i.e., optimizing the objective
P (G reasoning | G Q ),
where G Q includes the graph and query, and G reasoning encapsulates the reasoning result, which could be the thinking process like chain-of-thoughts or graph-of-thoughts. ) is the generated image, such as a descriptive image of the food chain based on a multi-modal graph of ecosystems or a novel-style painting based on a multi-modal graph of artist networks.Further details on the datasets for these tasks are available in Section IV.Remark 3. (Multi-scale Characteristics) This generative paradigm offers a powerful and flexible framework for modeling diverse multi-modal graph tasks with a unified interface.Since multi-modal graphs inherently represent multi-granular information, ranging from unimodal instances and bi-modal instances to entire multi-modal datasets, the resulting input and output spaces are exceptionally versatile, accommodating a wide array of tasks.This versatility, however, introduces multi-scale characteristics for multi-modal graph tasks: the input and output graphs can differ significantly in scope.For example, a task might take an entire graph as input but require only a single node or path as output (as in a GQA task), or vice versa for other potential tasks.This disparity in scales and granularity poses critical challenges for unified task modeling and task prompt design in MG-LLM, as the scales and semantic levels of inputs and outputs can vary widely.</p>
<p>C. Unified View of Multi-modal Graph Models</p>
<p>In this section, we provide a unified view on current multi-modal graph models by first proposing a transformation function.Then, we bring together the two main categories that are moving towards MG-LLM, and we will briefly overview these approaches.</p>
<p>a) Transformation Function: We first propose a transformation function, denoted as T , which maps one multi-modal graph to another.This transformation often involves a reduction in the number of modalities and the size of the graph, thereby facilitating processing by models or aligning the output with desired modalities and formats.This transformation function T can be parameter-free ( e.g., designed via heuristic rules) or parametric (learned using neural networks), i.e., T θ with the parameters θ, which we omit for brevity.</p>
<p>For instance, in the input space, T can convert an entire multimodal graph into text.This could involve image captioning for image features, speech-to-text conversion for audio features, and textualizing edges into XML-like languages for structures.Consequently, a complex multi-modal graph is transformed into a simplified multi-modal graph where nodes primarily possess text attributes.Formally, given an input multi-modal graph G, the transformation T could yield G ′ = (V ′ , E ′ , F ′ , {text}), where F ′ is the text feature mapping function.</p>
<p>Similarly, in the output space, T can summarize a multimodal graph into a single label, a document, or an image.An image output, for example, might not solely originate from extracting a single image-attributed node but could also involve rendering an entire multi-modal graph into a coherent visual representation (e.g., visualizing a family tree).</p>
<p>In this view, a multi-modal graph model can be seen as first transforming the input multi-modal graph via a transformation, then modeling it, and finally transforming it again into the desired output.This can be formally expressed as:
G out = T out (ϕ θ (T int (G in ))),(2)
where G in is the input multi-modal graph, T in is the input transformation, ϕ θ is the core multi-modal graph model, T out is the final transformation, and G out is the generated output multi-modal graph.This generalized framework highlights the crucial role of these transformations in aligning diverse multimodal graph data with the model's capabilities and desired output formats.b) Multi-modal Graph Neural Networks: The transformation function T is typically considered a parametric function.Here, information from each modality is initially mapped into a learned representation via trained modality-specific encoders.Subsequently, a GNN leverages its message-passing mechanism to learn from these representations and derive the required labels for downstream tasks.For instance, multi-modal graph convolution networks (MGCNs) [11]- [13] utilize the learned multimodal representation to form adjacency matrix, and multimodal graph attention networks (MGATs) fuse information from different modalities by assigning different attention weights to each nodes [14]- [16].The primary advantage of these approaches lies in the MGNN's ability to explicitly utilize structural information within the graph.However, a significant drawback is the lack of flexible input and output spaces, which complicates the development of unified foundation models for multi-modal graphs.Furthermore, this late fusion function can lead to substantial information loss, making it challenging to capture fine-grained modal interactions.c) Graph Large Language Models: GraphLLMs often employ different strategies for the transformation function T . 1) Some works utilize a non-parametric transformation function.For instance, they might describe the entire graph as text, which is then processed by an LLM [17]- [23].Alternatively, they might transform the graph into image-text pairs to be processed by a vision-language model (VLM).The advantage of these methods is their ability to leverage the flexible input and output spaces of LLMs or VLMs, enabling them to handle a wide range of tasks.However, they are heavily dependent on the inherent capabilities of the underlying LLM or VLM.Describing a complex graph entirely in text can lead to very long contexts, making comprehension difficult for the model.Moreover, certain modal information may be inherently challenging to textualize (e.g., an image converted to text can suffer significant information loss).2) Other works employ a parametric transformation function.For instance, LLaGA [24] employs a parametric projector to transform graph data into structured sequences, which are then embedded into the token space and fed into a large language model (LLM) for further processing.This mapping enables LLMs to effectively handle graph-structured data, enhancing their versatility, generalizability, and interpretability.GOFA [25] interleaves randomly initialized graph neural network (GNN) layers within a frozen pre-trained LLM, organically combining semantic and structural modeling capabilities.This design leverages the GNN's strength in processing graph structures alongside the LLM's generative and reasoning abilities.It is important to note that current GraphLLMs rarely address multimodal graph problems directly.Remark 4. (Modular Characteristics) For building a native MG-LLM, the transformation function T might ideally be an identity mapping, i.e., the input multi-modal graph is directly fed into the primitive MG-LLM without any information loss.To achieve this ambitious goal, we might have to pretrain the model on extremely large multi-modal graphs, e.g., the entire internet, so that sufficient pairwise data across modalities could directly empower the model with comprehensive knowledge of various modalities and graph structures.However, this monolithic approach might be impractical in the near future due to the considerable computational expenses and data acquisition challenges.One possible solution to circumvent this limitation might be building a modular multi-modal graph LLM, which integrates various parameterized modules designed for specific functions to understand as well as generate both the structures and multi-modal information within multi-modal graphs.This modularity could allow for more efficient training and flexible adaptation to diverse multi-modal graph tasks.</p>
<p>III. TOWARDS MULTI-MODAL GRAPH LARGE LANGUAGE MODELS</p>
<p>In this section, we will delve into the essential characteristics that a multi-modal graph large language model should possess, the core challenges in achieving these characteristics, current relevant approaches, and potential future research directions.The key characteristics are illustrated in Figure 1.</p>
<p>A. Unified Space for Multi-modal Structures and Attributes a) Desired Characteristics: MG-LLM are envisioned to effectively process information across diverse domains and a wide range of data modalities [6].Real-world applications in healthcare, finance, scientific discovery, and social media often present highly heterogeneous structures and data types [26].Therefore, MG-LLM should possess strong domain transferability and the capacity for broad representation generalization.</p>
<p>A key characteristic for MG-LLM is the ability to align diverse multi-modal features and relations within a truly unified representation space.This requires developing a comprehensive multi-modal graph vocabulary capable of capturing highly irregular, multi-level, and continuous structural and attribute information.Such a space should facilitate the learning of transferable patterns that generalize across heterogeneous data domains.The goal is to create a seamless integration where information from text, images, audio, video, and structured graph topologies can be jointly processed and understood.This unified space should minimize redundancy while maximizing information retention and the faithful representation of relational semantics, enabling robust domain transferability and flexible interpretation of various data inputs.</p>
<p>b) Key Challenges: One of the fundamental challenges in building MG-LLM is the inherent heterogeneity of data domains.Data often originates from various sources with different formats and structures, such as biomedical data (e.g., proteins, drugs) [27], social networks (e.g., text, images) [28], and multi-modal knowledge graphs (e.g., language, images, temporal data) [29].These domains exhibit distinct properties, including categorical, continuous, or structured data types [30].This heterogeneity poses a significant obstacle in designing a unified model capable of efficiently integrating and processing such diverse data.Despite recent efforts to develop foundational models for graph data [31], these approaches remain limited to a few domains.Models [32], [33] still fall short of providing a truly universal multi-modal graph encoding scheme, which hinders generalization to broader and more diverse application scenarios.Furthermore, the multi-granularity of nodes and structures presents a significant challenge.In multi-modal graphs, nodes represent entities that may belong to different modalities (e.g., images, text, molecules) with various granularities, while edges capture relationships between these nodes.Effectively modeling these heterogeneous nodes and their connections, which often have varying structures, requires novel approaches for multi-modal embedding and multi-modal graph structure learning to better accommodate the complexity of diverse node types and relations.The highly irregular, multi-level, and potentially continuous nature of multi-graph vocabularies further complicates the creation of a unified representational space, leading to potential issues like redundancy and information loss during the integration of multi-modal features and relations.c) Related Works: Efforts towards achieving domain transferability and generalizable representations in graph learning have led to the development of foundation models and pretraining strategies for graph data [31]- [33].Specifically for multi-modal graphs, research has begun exploring how to learn domain-invariant representations that can generalize across different knowledge graphs and multi-modal networks [34].The growing interest in prompt-driven and instruction-based paradigms, largely influenced by the success of Large Language Models, has also spurred work in adapting these approaches for unified data processing within structured and multi-modal contexts [35]- [37].While these works represent significant steps, they often grapple with unifying the vastly different granularities and semantic levels inherent in multi-modal graph data, or they rely on transformation functions that might incur information loss.</p>
<p>d) Future Directions: To achieve a truly unified space for multi-modal structures and attributes, several critical future directions emerge.Firstly, there is a pressing need to develop novel multi-modal graph vocabulary and tokenization schemes that can flexibly represent information ranging from fine-grained features (e.g., pixels, words) to coarse-grained concepts (e.g., full images, entire documents), as well as complex graph topologies.This involves moving beyond simple concatenations or late fusion, aiming for an early and deep integration of multi-modal signals.Secondly, research should focus on designing architectures capable of learning genuinely transferable patterns across the highly irregular, multi-level, and continuous nature of real-world multi-modal graphs.This might involve exploring advanced graph neural network designs combined with foundation models that can process diverse modalities natively.Thirdly, strategies to mitigate redundancy and information loss during multi-modal feature and relation integration are crucial.This could involve attention mechanisms that dynamically weigh modality contributions or latent space learning that preserves critical inter-modal dependencies.Finally, developing benchmark datasets specifically designed to evaluate the effectiveness of models in this unified multimodal graph space, across diverse domains, will be essential to drive progress in this field.This foundational work will be instrumental in realizing the vision of native modeling capable of operating directly on rich, multi-modal graph data without substantial information loss.</p>
<p>B. Handling Diverse Multi-modal Graph Tasks</p>
<p>a) Desired Characteristics: A multi-modal graph large language model should be adept at handling a vast array of tasks, moving beyond traditional discriminative objectives to embrace a unified generative paradigm.As previously outlined, the ability to frame all multi-modal graph tasks as multi-modal graph generation is a key characteristic.This necessitates a model capable of treating diverse problems, such as multi-modal node classification, link prediction, graph classification, question answering, reasoning, and even multimodal content generation (text or image), as transformations from an input multi-modal graph to an output multi-modal graph.The model should demonstrate flexibility in its input and output spaces, seamlessly adapting to tasks that operate on different granularities, from fine-grained node features to entire graph structures.Furthermore, the model should support promptdriven or instruction-based learning, allowing for versatile task adaptation and generalization to new, unseen multi-modal graph scenarios through natural language commands or structured prompts.</p>
<p>b) Key Challenges: A fundamental challenge in developing such models lies in the multi-scale characteristics of multi-modal graph tasks.As highlighted in the generative modeling section, the input and output graphs can differ significantly in scope and granularity.For instance, a task might take a comprehensive multi-modal graph as input but require only a single predicted node's attribute as output (as in node classification or a specific graph question answering task) [38].Conversely, other tasks might demand the generation of an entire sub-graph or even a new multi-modal graph from a simple input.This disparity in scales complicates unified task modeling and especially task prompt design, as the semantic levels of inputs and outputs vary widely [39].Beyond structural scale, the inherent scales of different modalities also pose challenges; text inputs can vary greatly in length, images in size, and videos in temporal duration.Integrating and generating information across these vastly different intrinsic modal scales, alongside varying graph structural complexities, requires sophisticated mechanisms to prevent information loss or redundancy and maintain coherence across the entire representation [40].</p>
<p>c) Related Works: Efforts to address the diversity of graph tasks have led to the development of multi-task graph foundation models, which aim to provide a single framework for various graph-related tasks [41]- [44].Inspired by the success of large language models, research has also begun to explore prompt-driven and instruction-based paradigms for graph and multi-modal contexts.These approaches leverage the flexibility of language models to adapt to different downstream tasks by formulating them as sequence-to-sequence or graph-tosequence problems [45], [46].Current GraphLLMs or visionlanguage models, by virtue of their flexible input and output capabilities, can handle some multi-modal graph tasks by first transforming the graph into a text-centric or imagetext paired representation.While these methods demonstrate promising abilities in task generalization and leveraging pretrained knowledge, they often face limitations when directly handling the intricate multi-scale nature and inherent graph structures, sometimes relying on transformation functions that may incur information loss or struggle with extremely long or complex contexts.</p>
<p>d) Future Directions: To effectively handle diverse multimodal graph tasks, future research should prioritize developing novel approaches for unified task modeling that natively account for the varying scales and granularities of both input and output multi-modal graphs.This involves designing architectures that can process fine-grained features (e.g., pixels, words) alongside coarse-grained concepts (e.g., full images, entire documents) and complex graph topologies in a seamless manner.Another critical direction is the investigation of adaptive prompting mechanisms that can dynamically adjust to the task's specific scale and the required output granularity, moving beyond generic prompts.Furthermore, attention should be given to extending generative modeling techniques to truly open-set multi-modal graph generation, where the model can synthesize novel graph structures or content (e.g., images, texts, audios) that are not limited to predefined sets.This requires a deeper integration of multi-modal understanding with generative capabilities, aiming for models that can operate directly on rich multi-modal graph information without relying on lossy intermediate transformations.</p>
<p>C. Multi-modal Graph In-context Learning Capability</p>
<p>a) Desired Characteristics: A central aspiration for multimodal graph large language models (MG-LLM) is to exhibit robust in-context learning (ICL) capabilities.This involves the model's ability to solve novel tasks by conditioning on a limited number of graph-anchored examples provided directly within the prompt, without requiring explicit weight updates or fine-tuning.Similar to how large language models learn from demonstrations in text, MG-LLM should infer underlying patterns and generalize to unseen multi-modal graph scenarios.This necessitates a model that can interpret and leverage diverse multi-modal features and complex graph structures present in the few-shot examples to inform its predictions or generations for new queries.Ultimately, achieving this capability relies on effective generative pretraining on large-scale, paired multimodal graph data, coupled with an architectural design and selfsupervised learning objectives that facilitate flexible transfer and scaling.</p>
<p>b) Key Challenges: Extending in-context learning to graph-based multi-modal contexts presents significant challenges that are not encountered in plain text domains.Graphs inherently possess a variable topology, long-range dependencies, and a non-sequential structure [6], [47], making it difficult to define what constitutes an effective context window for ICL.Unlike a linear sequence of tokens, the 'neighborhood' or 'context' around a graph element can be complex and multifaceted.Furthermore, encoding the rich relational priors and intricate inter-modal connections compactly for consumption by models, especially transformer architectures that are often designed for sequential data, remains a non-trivial task.The diversity of modalities within a graph (e.g., text, images, audio associated with nodes or edges) further complicates the unified representation and contextual understanding necessary for effective in-context learning.</p>
<p>c) Related Works: Inspired by the success of large language models in in-context learning [48], researchers have begun exploring methods to imbue this capability into graph-based models.Strategies for graph ICL often involve linearization strategies, such as converting graphs or subgraphs into sequences of triples or using template filling to represent graph information in a text-like format [35].Another approach involves the use of sampled subgraph prompts, where relevant subgraphs are selected to serve as examples for the model [49].Hybrid architectures have also emerged, combining the strengths of pretrained graph neural networks (GNNs) for structural encoding with autoregressive decoders, which are adept at sequence generation and ICL [50].Specific methods like AskGNN [49] and retrieval-augmented transformers adapted for graphs [51] demonstrate that carefully selecting relevant subgraphs and aligning them with language tokens can enhance few-shot learning within graph constraints, pointing towards the importance of context retrieval and integration.Progress has also been made in aligning graph information with textual prompts for language models [52], [53].</p>
<p>d) Future Directions: To fully unlock the multi-modal graph in-context learning capability, several critical future directions should be explored.A key area is the development of more sophisticated multi-modal graph tokenization schemes that can capture the inherent multi-granularity of graph entities (from fine-grained features to coarse-grained concepts) and their complex inter-modal relations in a way that is amenable to incontext learning.This would involve designing tokenizers that can flexibly abstract both structural and attribute information across modalities.Furthermore, research should focus on architectures that can intrinsically process irregular graph structures and multi-modal information without significant information loss from linearizing or simplifying the graph.This might involve novel graph-specific attention mechanisms or prompt-based techniques that can dynamically integrate and reason over graph-anchored examples.The integration of retrieval mechanisms that can efficiently fetch relevant subgraphs or multi-modal contexts for ICL, especially from vast multi-modal knowledge graphs, will also be crucial.Ultimately, continued advancement in large-scale generative pretraining on diverse and rich multi-modal graph datasets, coupled with self-supervised objectives tailored for graph understanding and generation, will be essential for models to acquire robust and transferable in-context learning abilities.</p>
<p>D. Natural Multi-modal Graph Interaction</p>
<p>a) Desired Characteristics: An essential goal of Multi-Modal Graph Large Language Models (MG-LLM) is to enable natural language-based interaction over structured and multi-modal data.Users should be able to query, edit, and reason about graph-structured knowledge using plain language, without needing to learn formal query languages like SPARQL or Cypher.This requires the MG-LLM to accurately map natural language inputs to graph traversal operations, complex reasoning chains, or precise node and edge modifications.Furthermore, the model should support rich, multi-turn dialogue, allowing for clarification and refinement of user intentions.A crucial aspect is visual grounding within graphs, enabling the model to align natural language descriptions with visual elements present within the graph context, thereby facilitating a more intuitive understanding of multi-modal information.</p>
<p>Beyond querying, the model should also be capable of graphbased summarization and generation of new graph structures or content in response to natural language commands.Ultimately, the desired characteristic is to provide a seamless, intuitive, and highly interactive interface between human users and complex multi-modal graph data, allowing for direct understanding, editing, reasoning, and generation of information, bridging the gap from unstructured human input to structured graph knowledge, while aligning with human values and intentions.</p>
<p>b) Key Challenges: One of the fundamental challenges in achieving natural multi-modal graph interaction lies in the inherent semantic gap between the ambiguity and flexibility of natural language and the precise, structured nature of graph data.Natural language expressions can be vague or underspecified, making it difficult for a model to infer the user's exact intention for graph operations, especially when dealing with heterogeneous multi-modal features [5].Mapping these vague intentions to concrete graph traversals, edits, or reasoning chains is non-trivial.Furthermore, supporting multimodal interactions introduces complexities, as the model must seamlessly interpret queries that might refer to text, image, audio, or video attributes within the graph, and potentially generate responses in a desired modality [54].The ability to understand, edit, reason, and generate content within a multimodal graph poses distinct challenges; for instance, editing an image-attributed node based on a textual command requires sophisticated cross-modal understanding and generative capabilities [55].Ensuring consistency and avoiding unintended side effects during graph modifications initiated by natural language commands is another significant hurdle.Finally, scaling such interactive capabilities to complex, real-world multi-modal graphs, such as those representing industry traffic patterns, molecular structures, or visual relational graphs, presents significant challenges in terms of computational efficiency and maintaining accuracy across diverse domains [56].c) Related Works: Recent advancements in natural language interfaces for structured data have laid foundational groundwork for multi-modal graph interaction.Efforts in conversational knowledge graphs [57], [58] have explored how to enable multi-turn dialogue over knowledge bases, allowing users to progressively refine their queries.Similarly, research on natural language interfaces for databases (NLIDB) [59] focuses on translating natural language questions into structured query languages, a precursor to graph traversal and modification.With the rise of multi-modal large language models, there has been increasing interest in visual grounding, where models align language descriptions with visual elements in complex scenes or contexts [60]- [62].These techniques are directly relevant for grounding natural language queries within image or video attributed graph nodes.Graph-based generative tasks, such as graph summarization [52], [63], have also emerged, demonstrating the ability to condense graph information into coherent text.Moreover, the integration of instruction-tuned language models with symbolic reasoning modules [64], [65] represents a promising direction for building more robust dialogue-centric MG-LLM, enabling them to leverage both pattern matching and logical deduction for complex graph interactions.</p>
<p>d) Future Directions:</p>
<p>To fully realize natural multimodal graph interaction, several critical future directions need exploration.Firstly, developing more sophisticated methods for disambiguating vague or underspecified natural language intentions and aligning them precisely with multi-modal graph structures and attributes is crucial.This could involve interactive clarification dialogues where the model asks follow-up questions to refine its understanding.Secondly, research should focus on robust mechanisms for human feedback integration during the interaction process, allowing users to correct model interpretations or outputs, thereby continually improving the model's understanding and alignment with human values.This iterative feedback loop is essential for adapting models to new domains and user preferences.Thirdly, advancing generative capabilities to enable not only querying but also complex graph editing and generation through natural language commands is vital.This includes the ability to modify existing multi-modal nodes and edges, add new entities, or even generate entire subgraphs based on high-level instructions, with applications in areas like molecular design or urban planning.Finally, scaling these interaction paradigms to dynamic and evolving multimodal graphs that represent real-world phenomena (e.g., realtime industry traffic, evolving scientific knowledge graphs) will require innovations in efficient graph indexing, retrieval, and incremental updates to maintain responsiveness and accuracy during natural language-driven interactions.</p>
<p>E. Multi-modal Graph Reasoning a) Desired Characteristics: An MG-LLM should be capable of multi-hop, cross-modal reasoning.For example, the model should answer a complex query by combining clues from text and images through multiple inferential hops.Recent benchmarks like MultiModalQA [66] show that solving such cross-modal multi-hop questions remains challenging.Models must jointly reason over different modalities and knowledge sources to succeed on these tasks.Another desired capability is analogical inference across modalities, where the model draws structural comparisons between, for instance, an image pair and a text pair.Analogical reasoning is a fundamental aspect of human cognition.Initial studies suggest large models have some analogical ability.However, most prior work on analogies is single-modal.Multi-modal analogical reasoning is still in its early stages.Early efforts on multimodal analogies over knowledge graphs (e.g., the MARS benchmark [67]) illustrate both the potential and the difficulty of this skill.For instance, demonstrate that even advanced multimodal LLMs struggle with visual analogies unless special prompting or training is provided.This result underscores the importance of analogical inference as a future MG-LLM capability [68].</p>
<p>b) Key Challenges: Building an MG-LLM poses several major challenges.First, modality alignment is difficult.The model must align and fuse information from heterogeneous sources such as images, text, and graphs into a coherent representation.Without explicit alignment mechanisms, an image's contents may not correctly map to textual concepts, impeding reasoning.Techniques like contrastive image-text pre-training (e.g., CLIP) are often used to partially address this problem by embedding modalities in a shared space [69].Recent MG-LLM approaches include dedicated alignment modules for vision and language.For example, MR-MKG [54] employs a cross-modal alignment module to optimize image and text correspondences within a multimodal knowledge graph.Second, factual consistency remains a critical issue.Multimodal LLMs are prone to hallucination and may produce inconsistent answers that conflict with factual knowledge.This problem worsens when the model must recall external knowledge, such as from a graph that it was not pre-trained on.Recent work has highlighted these hallucination problems and proposed evaluation benchmarks (e.g., MHaluBench [70]) and detection frameworks to reduce them.Indeed, MR-MKG was motivated by the observation that vanilla LLMs often fabricate details about images due to missing visual knowledge and injects a multimodal knowledge graph to ground the model in reality [54].A third challenge is the fragility of current processing pipelines.Many models operate in stages or rely on external tools, and errors in early steps can cascade.[71] note that fixed sub-models in current systems make them unable to recover from intermediate mistakes.Improving robustness and feedback mechanisms is therefore an important challenge.c) Related Works: Several initial approaches to MG-LLM have been proposed to address these challenges.A common strategy is to integrate a knowledge graph (KG) or graph neural network into the multimodal pipeline to better handle structured, relational information.For example, MR-MKG [54] augments a vision-language model with a multimodal knowledge graph that contains nodes and relations spanning text and images.It uses a relation-aware graph neural network to encode the MMKG and injects these representations into the LLM to improve reasoning.Another line of work focuses on graph construction and alignment between modalities.MAIL [60] constructs a scene graph from image objects and a concept graph from external knowledge, aligning them via shared entities and fusing them through a pseudo-siamese graph neural network.This enables reasoning over a combined multimodal graph and has shown strong results in knowledge-intensive visual QA.Beyond QA, graph-enhanced multimodal models have been applied to other domains.For example, Choi et al. [72] proposes a model for healthcare that injects patient-specific graphs into an LLM and uses GNN-based message passing to align clinical text, lab results, and images.These methods highlight the benefit of structured reasoning but also reveal engineering complexity, as each task may require tailored graph construction and alignment strategies [73].</p>
<p>d) Future Directions: Future research on multi-modal graph reasoning should tackle several ambitious goals.First, novel graph representation strategies tailored explicitly for multi-modal contexts could be developed, going beyond current embedding approaches to represent complex interactions between modalities more intuitively.Second, dynamically constructed multi-modal graphs that adapt in real-time to the context or queries presented to the model may enhance reasoning efficiency and accuracy.Additionally, exploring scalable inference techniques specifically designed for large and dense multi-modal graphs is essential to overcoming the context-length limitations of current models.Finally, there is a significant opportunity to advance explainability by designing methods that produce interpretable reasoning paths within multimodal graph structures, enabling users to better understand and trust the model's decision-making process.</p>
<p>IV. MULTI-MODAL GRAPH DATASETS</p>
<p>Recent years have witnessed the emergence of multimodal graph learning datasets, which enrich traditional graph structures by incorporating image, text, video, audio, and multiomics data.These datasets facilitate more challenging and realistic graph learning tasks by providing heterogeneous node and edge attributes.In this review, we categorize representative benchmarks by task type and summarize their scale, modalities and domains.statistics can be summarized in Table I.</p>
<p>A. Node Classification</p>
<p>Node classification benchmarks evaluate a model's ability to predict node labels when each node carries multimodal attributes.These datasets include: ELE Fashion [7], a mediumscale e-commerce product graph comprising approximately 97,800 product nodes, each annotated with a product title and a high-resolution image.Books NC [7] includes roughly 685,300 book nodes, each with cover images and descriptions, and is annotated for ten book categories.G2MF-Urban [74] is an urban planning graph with about 100,000 street nodes and 2,000,000 edges, where nodes incorporate overhead imagery and Point of Interest (POI) text for functional zone classification.In the biomedical domain, the Pan-Cancer Atlas [75] comprises approximately 11,286 tumor samples across 33 cancer types, profiled by multiple omics assays (mRNA, miRNA, DNA methylation, proteomics, CNV), and is used for pan-cancer molecular subtype classification; each sample is modeled as a node whose features are the concatenated multimodal measurements, with edges encoding biological relations.Similarly, TCGA-BRCA [76] contains around 1,084 breast tumor samples assayed on six platforms (genomic, epigenomic, transcriptomic, proteomic) and supports breast cancer subtype classification, also modeling each sample as a node with concatenated multimodal measurements and edges encoding biological relations.For emotion analysis, the IEMOCAP database [77] contains 151 video sessions, encompassing approximately 12 hours of audiovisual data, with 5,816 manually segmented utterances annotated for nine emotion categories along with valence, arousal, and dominance scores, providing synchronized audio, video, and text transcriptions for each utterance.MELD [78] extends the EmotionLines corpus with 1,433 multi-party dialogues and about 13,000 utterances from the TV series Friends, each labeled with one of seven emotions and sentiment polarity, and provides synchronized text transcripts, audio clips, and video segments for every utterance.Additionally, the OMG-NAS framework [79] evaluates two real-world outof-distribution benchmarks: the Tencent graph from WeChat official accounts, which includes 8,000 article nodes and 60,000 user-view edges, with each node carrying head images and titles; and the Amazon review graph, featuring 100,000 review nodes and 300,000 co-review edges, combining product images and textual feedback.</p>
<p>B. Link Prediction</p>
<p>Link prediction datasets require inferring missing or future edges in graphs with multimodal node features.Datasets that incorporate multimodal node features include Books LP [7], which comprises approximately 636,500 book nodes, each with cover images and descriptions, used for link inference.The Sports CoPurchase and Cloth CoPurchase datasets [7] are copurchase graphs containing 50,250 and 125,839 product nodes respectively, with each node annotated with product titles and images.HyperGCL-Ecomm [80] is a hypergraph dataset with 1,000,000 product nodes possessing multimodal features and 500,000,000 behavioral edges.VTKG-I and VTKG-C [81] present two commonsense knowledge graphs (KGs), each with approximately 130 entities and 842 triples, where entities are associated with images and text, designed for knowledge graph completion tasks.TIVA KG [82] is a quad-modal knowledge graph containing roughly 50,000 entities and 200,000 triples, which combines text, image, video, and audio modalities for completion tasks.</p>
<p>C. Graph Classification</p>
<p>Graph classification datasets learn holistic representations for entire graphs with heterogeneous node and edge attributes.In the context of the OMG-NAS framework [79], a Recipe graph is utilized, comprised of approximately 20,000 recipe nodes  [7] Text+Vision 684K nodes, 7M edges Book recommendation G2MF-Urban [74] Text+Vision 100K nodes, 2M edges Urban planning Pan-Cancer Atlas [75] Multi-omics 11K samples from 33 cancer types Pan-cancer molecular subtypes TCGA-BRCA [76] Multi-omics 1,084 breast tumor samples Breast cancer subtypes IEMOCAP [77] Text+Vision+Audio 6K nodes from 151 video sessions Emotion recognition MELD [78] Text+Vision+Audio 14K nodes from 1433 video sessions Emotion recognition OMG-NAS Tencent [79] Text+Vision 8K nodes, 60K edges Network articles OMG-NAS Amazon [</p>
<p>D. Visual Graph QA</p>
<p>Visual reasoning datasets convert images into scene graphs paired with compositional questions to assess structured inference.GQA [9] contains 113,018 real-world images, 22.7 million questions, and scene graphs annotated with functional programs.CLEVR [84] is a synthetic visual question answering benchmark consisting of 100,000 rendered RGB scenes paired with approximately 853,000 automatically generated question-answer pairs.Each scene is accompanied by a detailed scene graph that encodes object attributes and spatial relations, and every question is mapped to a functional program specifying the multi-step reasoning required, with the dataset designed to minimize biases and provide exhaustive annotations.SceneGraph-VQA [85] comprises 50,000 scene graphs with QA pairs, combining object-relationship hierarchies, image regions, and textual questions.</p>
<p>E. Graph Reasoning</p>
<p>Multimodal graph reasoning datasets integrate heterogeneous information sources to support complex reasoning tasks.MARS and MarKG [67] serve as benchmark datasets for multimodal analogical reasoning.MARS contains 10,685 training, 1,228 validation, and 1,415 test instances, where each task instance is a visual-textual analogical quadruple requiring the prediction of a missing entity.MarKG is a supporting knowledge graph for MARS, containing 11,292 entities and 192 relations, with entities enriched by 76,424 images along with textual and visual descriptions.Furthermore, two datasets, WN9-IMG-TXT and FB-IMG-TXT [86], are widely adopted multimodal knowledge graph benchmarks.WN9-IMG-TXT contains 6555 entities, while FB-IMG-TXT contains 11757 entities.In both datasets, each entity is associated with three modalities: structural graph information, images, and text.</p>
<p>F. Text Generation</p>
<p>These datasets evaluate alignment or generation of text conditioned on multimodal workflows.Visual Recipe Flow [87] annotates 200 recipes with before-after image pairs for each action and is grounded in a recipe-flow graph designed for stepwise text generation.The Microsoft Multimodal Aligned Recipe Corpus [88] contains approximately 150,000 text-video alignments across 4,262 dishes, structured with cross-modal graphs between recipe steps and corresponding video segments for description tasks.Richpedia [8] models a multimodal knowledge base containing over 1,000,000 entities, where relations exist between KG textual entities and image entities, among image entities or between image entities and values such as pixel information.This dataset is designed to support applications such as semantic search and text generation.</p>
<p>G. Image Generation</p>
<p>Image generation utilizing multimodal graphs aims to synthesize visual content by leveraging the interconnected textual and visual information inherent in these complex network structures.For example, ART500K [89] curates an artwork domain with 311,288 creations and 643 million interconnections reflecting shared artists or styles, each accompanied by textual and visual information.Amazon Coview [90] charts an e-commerce product landscape where 178,890 items are linked by 3 million connections derived from concurrent Browse patterns, each item possessing textual and visual descriptions.Lastly, Goodreads [90] forms a book recommendation network of 93,475 literary works interconnected by 637,210 relationships that highlight their comparability, with each book entry including textual and visual elements.</p>
<p>H. Summary</p>
<p>In summary, these datasets could be useful for evaluating multi-modal graph-learning methods across diverse tasks, scales, and fusion mechanisms, thereby laying a groundwork for MG-LLM.Despite this progress, the current volume of multi-modal graph datasets remains significantly smaller than that used for pre-training large language models.This disparity highlights the urgent need for the community to develop more effective methods for data collection and utilization.Furthermore, many existing multi-modal graph tasks are discriminative, and future efforts might be devoted to more generative multi-modal graph tasks to advance the evaluation and design of MG-LLM.</p>
<p>V. CONCLUSION</p>
<p>In this paper, we aim to address the generalization limits of current Multi-modal Graph Neural Networks by proposing Multi-modal Graph Large Language Models (MG-LLM).We introduce a unified framework, highlighting the inherent characteristics of multi-modal graphs, and define five essential capabilities for MG-LLM, ranging from unified data representation to complex reasoning.By discussing challenges, related research, and future directions, this work aims to contribute to the multi-modal graph community and accelerate the development of versatile, general-purpose MG-LLM.</p>
<p>Fig. 2 .
2
Fig. 2. Unified view of multimodal graph data, tasks and model towards MG-LLM.</p>
<p>, E grid , {F pixel }, {pixel} , where E grid represents grid-like inter-connections, and F pixel is the pixel feature mapping function.
Beyond individual instances, multi-modal graphs can efficientlyrepresent entire datasets. Here are some examples.• A batch of images can be represented asamulti-modalgraphwithseveralimage-attributednodeswithoutanyedges,i.e.,G=(V = {v 1 , . . . , v K }, E = ∅, {F image }, {image}),where F image is the image feature mapping function, andK is the number of images.• An image-captioning dataset can be represented asa multi-modal graph where edges connect image-attributed nodes to their corresponding text-attributedcaption nodes, i.e., for K image-caption pairs, G =</p>
<p>•</p>
<p>Multi-modal Graph Text Generation (TG) utilizes multi-modal graph information to generate coherent text sequences, i.e., optimizing the objective P (G text | G), where G text is the generated text, such as a summary of a group of papers cited by each other or a new git patch based on correlated git commits.• Multi-modal Graph Image Generation (IG) aims to generate novel images, where a multi-modal graph with textual descriptions, structured data, or other modal inputs can serve as a basis, i.e., optimizing the objective P (G image |G), where P (G image</p>
<p>TABLE I OVERVIEW
I
OF TASKS AND THEIR CORRESPONDING MULTI-MODAL GRAPH DATASETS.NC DENOTES NODE CLASSIFICATION, LP LINK PREDICTION, GC GRAPH CLASSIFICATION, GQA GRAPH QUESTION ANSWERING, GR GRAPH REASONING, TG TEXT GENERATION, AND IG IMAGE GENERATION.
TaskDatasetModalitiesScaleDomainELE Fashion [7]Text+Vision98K nodes, 20K edgesE-commerce productsBooks NCNC</p>
<p>[83]ingredient/instruction edges, where images are partitioned into 16×16 patch nodes and text into word nodes.Separately, the Large-RG dataset[83]models a culinary graph containing over 500,000 recipe nodes linked by ingredient edges, enriched with image and textual attributes.
79]Text+Vision100K nodes, 300K edgesE-commerce reviewsBooks LP [7]Text+Vision64K nodes, 3437K edgesBook recommendationSports CoPurchase [7]Text+Vision50K nodes, 25K edgesE-commerce productsLPCloth CoPurchase [7] HyperGCL-Ecomm [80]Text+Vision Text+Vision126K nodes, 951K edges 1M edges, 500M edgesE-commerce products E-commerce productsVTKG-I&amp;C [81]Text+Vision130 entities, 842 triplesVisual CommonsenseTIVA-KG [82]Text+Vision+Audio 50K entities, 200K triplesMultimediaGCOMG-NAS Recipe [79] Large-RG [83]Text+Vision Text+Vision20K nodes, 160K edges 500K nodesFood recipes Food recipesGQA [9]Text+Vision113K images, 23M questionsVisual question answeringGQACLEVR [84]Text+Vision100K images, 1M questionsSynthetic VisionSceneGraph-VQA [85]Text+Vision50K imagesVisual question answeringMARS&amp;MarKG [67]Text+Vision34K triples, 13K questionsKnowledge graphGRFB-ING-TXT [86]Text+Vision6K entitiesKnowledge graphWN9-ING-TXT [86]Text+Vision12K entitiesKnowledge graphVRF [87]Text+Vision200 recipes, 89 actionsRecipe stepsTGMS Recipe Corpus [88]Text+Vision4K dishes, 150K recipesRecipe stepsRichpedia [8]Text+Vision3M entitiesKnowledge graphART500K [89]Text+Vision311K nodes, 643M edgesArtworkIGAmazon Coview [90]Text+Vision178K nodes, 3M edgesE-commerce productsGoodreads [90]Text+Vision93K nodes, 637K edgesBook recommendationand 160,</p>
<p>A heterogeneous multimodal graph learning framework for recognizing user emotions in social networks. S Bhattacharyya, S Yang, J Z Wang, 2024 12th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE2024</p>
<p>Fashionklip: enhancing e-commerce image-text retrieval with fashion multi-modal conceptual knowledge graph. X Wang, C Wang, L Li, Z Li, B Chen, L Jin, J Huang, Y Xiao, M Gao, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics20235Industry Track</p>
<p>Multimodal representations of biomedical knowledge from limited training whole slide images and reports using deep learning. N Marini, S Marchesin, M Wodzinski, A Caputo, D Podareanu, B C Guevara, S Boytcheva, S Vatrano, F Fraggetta, F Ciompi, Medical Image Analysis. 971033032024</p>
<p>Construction and application of materials knowledge graph in multidisciplinary materials science via large language model. Y Ye, J Ren, S Wang, Y Wan, I Razzak, B Hoex, H Wang, T Xie, W Zhang, Advances in Neural Information Processing Systems. 202437</p>
<p>Multimodal learning with graphs. Y Ektefaie, G Dasoulas, A Noori, M Farhat, M Zitnik, Nature Machine Intelligence. 52023</p>
<p>Learning on multimodal graphs: A survey. C Peng, J He, F Xia, 2024</p>
<p>Mosaic of modalities: A comprehensive benchmark for multimodal graph learning. J Zhu, Y Zhou, S Qian, Z He, T Zhao, N Shah, D Koutra, 2025</p>
<p>Richpedia: A comprehensive multi-modal knowledge graph. M Wang, G Qi, H Wang, Q Zheng, Semantic Technology. Springer International Publishing2020</p>
<p>Gqa: A new dataset for realworld visual reasoning and compositional question answering. D A Hudson, C D Manning, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Deep multimodal data fusion. F Zhao, C Zhang, B Geng, ACM computing surveys. 5692024</p>
<p>Mmgcn: Multimodal graph convolution network for personalized recommendation of micro-video. Y Wei, X Wang, L Nie, X He, R Hong, T.-S Chua, Proceedings of the 27th ACM International Conference on Multimedia (MM 2019). the 27th ACM International Conference on Multimedia (MM 2019)ACM2019</p>
<p>Multicenter and multichannel pooling gcn for early ad diagnosis based on dual-modality fused brain network. X Song, F Zhou, A F Frangi, J Cao, X Xiao, Y Lei, T Wang, B Lei, IEEE Transactions on Medical Imaging. 4222023</p>
<p>Multi-modal knowledge hypergraph for diverse image retrieval. Y Zeng, Q Jin, T Bao, W Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Recipe2vec: Multi-modal recipe representation learning with graph neural networks. Y Tian, C Zhang, Z Guo, Y Ma, R Metoyer, N V Chawla, 2022</p>
<p>Multimodal graph transformer for multimodal question answering. X He, X Wang, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics. Association for Computational LinguisticsMay 2023</p>
<p>Graph transformer geometric learning of brain networks using multimodal mr images for brain age estimation. H Cai, Y Gao, M Liu, IEEE Transactions on Medical Imaging. 4222023</p>
<p>Can language models solve graph problems in natural language?. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, Advances in Neural Information Processing Systems. 202436</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, M Zhou, X He, S Han, arXiv:2305.150662023arXiv preprint</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zeroshot learning. H Zhao, S Liu, M Chang, H Xu, J Fu, Z Deng, L Kong, Q Liu, Advances in Neural Information Processing Systems. 202336</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. P Liu, Y Ren, J Tao, Z Ren, Computers in biology and medicine. 1711080732024</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. C Liu, B Wu, arXiv:2308.112242023arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Beyond text: A deep dive into large language models' ability on understanding graph data. Y Hu, Z Zhang, L Zhao, arXiv:2310.049442023arXiv preprint</p>
<p>R Chen, T Zhao, A Jaiswal, N Shah, Z Wang, arXiv:2402.08170Llaga: Large language and graph assistant. 2024arXiv preprint</p>
<p>Gofa: A generative one-for-all model for joint graph language modeling. L Kong, J Feng, H Liu, C Huang, J Huang, Y Chen, M Zhang, arXiv:2407.097092024arXiv preprint</p>
<p>Multimodal graph neural architecture search under distribution shifts. J Cai, X Wang, H Li, Z Zhang, W Zhu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>Modeling polypharmacy side effects with graph convolutional networks. M Zitnik, M Agrawal, J Leskovec, Bioinformatics. 34132018</p>
<p>A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, G Long, C Zhang, S Y Philip, IEEE transactions on neural networks and learning systems. 202032</p>
<p>Kepler: A unified model for knowledge embedding and pre-trained language representation. X Wang, T Gao, Z Zhu, Z Zhang, Z Liu, J Li, J Tang, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Dropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The journal of machine learning research. 1512014</p>
<p>Position: Graph foundation models are already here. H Mao, Z Chen, W Tang, J Zhao, Y Ma, T Zhao, N Shah, M Galkin, J Tang, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, AustriaJuly 21-27, 2024. 2024OpenReview.net</p>
<p>One for all: Towards training one graph model for all classification tasks. H Liu, J Feng, L Kong, N Liang, D Tao, Y Chen, M Zhang, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Towards foundation models for knowledge graph reasoning. M Galkin, X Yuan, H Mostafa, J Tang, Z Zhu, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. OpenReview.net, 2024</p>
<p>Graphclip: Enhancing transferability in graph foundation models for text-attributed graphs. Y Zhu, H Shi, X Wang, Y Liu, Y Wang, B Peng, C Hong, S Tang, 2025</p>
<p>Language is all a graph needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, Findings of the Association for Computational Linguistics: EACL 2024. Association for Computational LinguisticsMar. 2024</p>
<p>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. H Cao, Z Liu, X Lu, Y Yao, Y Li, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational LinguisticsAssociation for Computational LinguisticsJan. 2025</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 202336</p>
<p>Multimodal graph learning for generative tasks. M Yoon, J Y Koh, B Hooi, R Salakhutdinov, arXiv:2310.074782023arXiv preprint</p>
<p>All in one: Multi-task prompting for graph neural networks. X Sun, H Cheng, J Li, B Liu, J Guan, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Multimodal intelligence: Representation learning, information fusion, and applications. C Zhang, Z Yang, X He, L Deng, IEEE Journal of Selected Topics in Signal Processing. 1432020</p>
<p>Gcc: Graph contrastive coding for graph neural network pre-training. J Qiu, Q Chen, Y Dong, J Zhang, H Yang, M Ding, K Wang, J Tang, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>
<p>Gppt: Graph pretraining and prompt tuning to generalize graph neural networks. M Sun, K Zhou, X He, Y Wang, X Wang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Graphprompt: Unifying pretraining and downstream tasks for graph neural networks. Z Liu, X Yu, Y Fang, X Zhang, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>Inductive graph alignment prompt: Bridging the gap between graph pre-training and inductive finetuning from spectral perspective. Y Yan, P Zhang, Z Fang, Q Long, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Multigprompt for multi-task pre-training and prompting on graphs. X Yu, C Zhou, Y Fang, X Zhang, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>All in one and one for all: A simple yet effective method towards cross-domain graph pretraining. H Zhao, A Chen, X Sun, H Cheng, J Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Geometric deep learning: going beyond euclidean data. M M Bronstein, J Bruna, Y Lecun, A Szlam, P Vandergheynst, IEEE Signal Processing Magazine. 3442017</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Proceedings of the 34th International Conference on Neural Information Processing Systems. the 34th International Conference on Neural Information Processing SystemsCurran Associates Inc2020ser. NIPS '20</p>
<p>Does gnn pretraining help molecular representation?. R Sun, H Dai, A W Yu, Advances in Neural Information Processing Systems. 202235</p>
<p>Can gnn be good adapter for llms. X Huang, K Han, Y Yang, D Bao, Q Tao, Z Chai, Q Zhu, Proceedings of the ACM Web Conference 2024, ser. WWW '24. the ACM Web Conference 2024, ser. WWW '24Association for Computing Machinery2024</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in neural information processing systems. 202033</p>
<p>Graphgpt: Graph instruction tuning for large language models. J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Graph2text or graph2token: A perspective of large language models for graph learning. S Yu, Y Wang, R Li, G Liu, Y Shen, S Ji, B Li, F Han, X Zhang, F Xia, arXiv:2501.011242025arXiv preprint</p>
<p>Multimodal reasoning with multimodal knowledge graph. J Lee, Y Wang, J Li, M Zhang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20241782</p>
<p>Graphllm: Boosting graph reasoning ability of large language model. Z Chai, T Zhang, L Wu, K Han, X Hu, X Huang, Y Yang, arXiv:2310.058452023arXiv preprint</p>
<p>Graph-to-vision: Multi-graph understanding and reasoning using vision-language models. R Li, H Jiang, arXiv:2503.214352025arXiv preprint</p>
<p>Lingyi: medical conversational question answering system based on multi-modal knowledge graphs. F Xia, B Li, Y Weng, S He, K Liu, B Sun, S Li, J Zhao, arXiv:2204.092202022arXiv preprint</p>
<p>Evaluating and enhancing large language models for conversational reasoning on knowledge graphs. Y Huang, L Shi, A Liu, H Xu, arXiv:2312.112822023arXiv preprint</p>
<p>Nli4db: A systematic review of natural language interfaces for databases. M Liu, J Xu, 2025</p>
<p>Modalityaware integration with large language models for knowledge-based visual question answering. J Dong, Q Zhang, H Zhou, D Zha, P Zheng, X Huang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20241</p>
<p>Endowing language models with multimodal knowledge graph representations. N Huang, Y R Deshpande, Y Liu, H Alberts, K Cho, C Vania, I Calixto, 2022</p>
<p>Multi-modal dynamic graph transformer for visual grounding. S Chen, B Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022543</p>
<p>From local to global: A graph rag approach to query-focused summarization. D Edge, H Trinh, N Cheng, J Bradley, A Chao, A Mody, S Truitt, D Metropolitansky, R O Ness, J Larson, 2025</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. A Creswell, M Shanahan, I Higgins, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Symbol tuning improves in-context learning in language models. J Wei, L Hou, A K Lampinen, X Chen, D Huang, Y Tay, X Chen, Y Lu, D Zhou, T Ma, Q V Le, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Multimodal{qa}: complex question answering over text, tables and images. A Talmor, O Yoran, A Catav, D Lahav, Y Wang, A Asai, G Ilharco, H Hajishirzi, J Berant, International Conference on Learning Representations. 2021</p>
<p>Multimodal analogical reasoning over knowledge graphs. N Zhang, L Li, X Chen, X Liang, S Deng, H Chen, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Can multimodal large language model think analogically. D Guo, C Cao, F Yuan, D Wang, W Ma, Y Liu, J Fu, arXiv:2411.013072024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PmLR2021</p>
<p>Unified hallucination detection for multimodal large language models. X Chen, C Wang, Y Xue, N Zhang, X Yang, Q Li, Y Shen, L Liang, J Gu, H Chen, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20241</p>
<p>Towards robust multi-modal reasoning via model selection. X Liu, R Li, W Ji, T Lin, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Multimodal graph-llm: Leveraging graph-enhanced llms for multimodal healthcare predictions. I Choi, S Yun, J Xin, J Peng, T Chen, Q Long, 2024</p>
<p>Hybrid transformer with multi-level fusion for multimodal knowledge graph completion. X Chen, N Zhang, L Li, S Deng, C Tan, C Xu, F Huang, L Si, H Chen, SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. Madrid, SpainACMJuly 11 -15, 2022. 2022</p>
<p>A graph-based multimodal data fusion framework for identifying urban functional zone. Y Tao, W Liu, J Chen, J Gao, R Li, X Wang, Y Zhang, J Ren, S Yin, X Zhu, T Zhao, X Zhai, Y Peng, International Journal of Applied Earth Observation and Geoinformation. 1361043532025</p>
<p>Cell-oforigin patterns dominate the molecular classification of 10,000 tumors from 33 types of cancer. K A Hoadley, C Yau, T Hinoue, D M Wolf, A J Lazar, E Drill, R Shen, A M Taylor, A D Cherniack, V Thorsson, Cell. 17322018</p>
<p>K. R. 13, G. data analysis: Baylor College of Medicine Creighton Chad J. 22 23 Donehower Lawrence A. 22 23 24 25, I. for Systems Biology Reynolds Sheila 31 Kreisberg Richard B. 31 Bernard Brady 31 Bressler Ryan 31 Erkkila Timo 32 Lin Jake 31 Thorsson Vesteinn 31 Zhang Wei 33 Shmulevich Ilya 31 et al. B . W H H M S C L . P P J , Nature. 49074182012Comprehensive molecular portraits of human breast tumours</p>
<p>Iemocap: Interactive emotional dyadic motion capture database. C Busso, M Bulut, C.-C Lee, A Kazemzadeh, E Mower, S Kim, J N Chang, S Lee, S S Narayanan, Language resources and evaluation. 200842</p>
<p>Meld: A multimodal multi-party dataset for emotion recognition in conversations. S Poria, D Hazarika, N Majumder, G Naik, E Cambria, R Mihalcea, arXiv:1810.025082018arXiv preprint</p>
<p>Multimodal graph neural architecture search under distribution shifts. J Cai, X Wang, H Li, Z Zhang, W Zhu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>Hypergcl: Multi-modal graph contrastive learning via learnable hypergraph views. K M Saifuddin, S Ji, E Akbas, 2025</p>
<p>Vista: Visualtextual knowledge graph representation learning. J Lee, C Chung, H Lee, S Jo, J Whang, Findings of the Association for Computational Linguistics: EMNLP 2023. Association for Computational LinguisticsDec. 2023</p>
<p>Tiva-kg: A multimodal knowledge graph with text, image, video and audio. X Wang, B Meng, H Chen, Y Meng, K Lv, W Zhu, Proceedings of the 31st ACM International Conference on Multimedia, ser. MM '23. the 31st ACM International Conference on Multimedia, ser. MM '23Association for Computing Machinery2023</p>
<p>Recipe2vec: Multi-modal recipe representation learning with graph neural networks. Y Tian, C Zhang, Z Guo, Y Ma, R Metoyer, N V Chawla, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. International Joint Conferences on Artificial Intelligence Organization. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. International Joint Conferences on Artificial Intelligence OrganizationJul. 2022main Track</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. J Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei, C Lawrence Zitnick, R Girshick, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Jul. 2017</p>
<p>Vision-language models struggle to align entities across modalities. I Alonso, A Salaberria, G Azkune, J Barnes, O L De Lacalle, 2025</p>
<p>A multimodal translation-based approach for knowledge graph representation learning. H Mousselly-Sergieh, T Botschen, I Gurevych, S Roth, Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics. the Seventh Joint Conference on Lexical and Computational Semantics2018</p>
<p>Visual recipe flow: A dataset for learning visual state changes of objects with recipe flows. K Shirai, A Hashimoto, T Nishimura, H Kameko, S Kurita, Y Ushiku, S Mori, Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics. the 29th International Conference on Computational Linguistics. International Committee on Computational LinguisticsOct. 2022</p>
<p>A recipe for creating multimodal aligned datasets for sequential tasks. A Lin, S Rao, A Celikyilmaz, E Nouri, C Brockett, D Dey, B Dolan, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>Graphgpt-o: Synergistic multimodal comprehension and generation on graphs. Y Fang, B Jin, J Shen, S Ding, Q Tan, J Han, arXiv:2502.119252025arXiv preprint</p>
<p>Instructg2i: Synthesizing images from multimodal attributed graphs. B Jin, Z Pang, B Guo, Y.-X Wang, J You, J Han, arXiv:2410.071572024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
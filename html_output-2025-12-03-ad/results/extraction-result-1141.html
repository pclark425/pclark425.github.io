<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1141 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1141</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1141</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-207756747</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1910.14351v1.pdf" target="_blank">VASE: Variational Assorted Surprise Exploration for Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Exploration in environments with continuous control and sparse rewards remains a key challenge in reinforcement learning (RL). Recently, surprise has been used as an intrinsic reward that encourages systematic and efficient exploration. We introduce a new definition of surprise and its RL implementation named Variational Assorted Surprise Exploration (VASE). VASE uses a Bayesian neural network as a model of the environment dynamics and is trained using variational inference, alternately updating the accuracy of the agent's model and policy. Our experiments show that in continuous control sparse reward environments VASE outperforms other surprise-based exploration techniques.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1141.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1141.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VASE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Assorted Surprise Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surprise-driven, model-based RL algorithm that defines an intrinsic reward (assorted surprise) as the expected negative log-likelihood under a distribution over dynamics models minus a confidence (entropy) term, approximated with a variational Bayesian neural network and used to guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VASE (Variational Assorted Surprise Exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based RL agent with two main components: a policy network f_π (MLFF) optimized with TRPO, and a Bayesian neural network (BNN) dynamics model f_m parametrized by θ with a fully-factorised Gaussian variational posterior q(θ; φ). The BNN is trained by variational inference (maximising an evidence lower bound) using an experience replay buffer; the policy and model are updated alternately.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>curiosity-driven exploration / information-gain-inspired intrinsic reward (assorted surprise) implemented via variational inference in a Bayesian dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each transition the agent computes an intrinsic reward U_VASE(s_{t+1}) = E_{θ∼q}[−log P(s_{t+1}|s_t,a_t,θ)] − δ H[q(θ;φ)], estimated with N=10 samples from q, and adds η·U_VASE to extrinsic reward; the policy (TRPO) is trained on the combined reward. The BNN posterior q(θ;φ) is updated using data from a replay buffer by maximising the variational lower bound (KL(q||prior) regulariser + expected log-likelihood). The intrinsic reward therefore directs exploration toward transitions that are either poorly predicted by the current model (high surprisal/expected NLL) or informative about model parameters (information gain), modulated by a confidence (entropy) penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple continuous-control benchmarks (2DPlane / 2DPointRobot, sparse MountainCar, sparse CartPoleSwingup, sparse DoublePendulum, sparse HalfCheetah, sparse Ant, LunarLanderContinuous)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Stochastic MDPs treated as agent-subjective distributions; continuous state and action spaces; very sparse extrinsic rewards for most benchmarks (reward only on task success), episodic, generally fully-observed in experiments though the paper explicitly notes their formulation can treat observations as states even when true state is not directly observed (e.g., visual).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by benchmark: 2DPlane (S ⊂ R^2, A ⊂ R^2, wrap-around), sparse MountainCar (S ⊂ R^3, A ⊂ R^1), CartPoleSwingup (S ⊂ R^4, A ⊂ R^1), DoublePendulum (S ⊂ R^6, A ⊂ R^1), HalfCheetah (S ⊂ R^20, A ⊂ R^6), Ant (S ⊂ R^125, A ⊂ R^8), LunarLanderContinuous (S ⊂ R^8, A ⊂ R^2); typical episode lengths: 500 (most tasks) or 1000 (LunarLander). Replay pool sizes: 100k (classic tasks) or 5M (locomotion); BNN architecture: 1 hidden layer (32) for classic tasks, 2×64 for locomotion; N=10 samples used per surprise estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative and some quantitative results reported: substantially faster discovery in 2DPlane/2DPointRobot (first non-zero extrinsic reward found in 26,663 steps for VASE vs 2,059,459 steps for no-surprise random exploration). Across continuous-control sparse-reward benchmarks, VASE outperforms surprisal (NLL) in all tasks, outperforms or matches VIME on most (VIME best initially on sparse MountainCar; VASE later surpasses VIME on DoublePendulum; VASE shows strong results on sparse CartPoleSwingup, HalfCheetah, Ant). Median performance curves were reported over 20 runs (no per-task numeric returns in main text). VASE is also substantially faster wall-clock time than VIME (no per-episode gradient-through-BNN overhead).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random/no-surprise baseline: on 2DPlane the random-exploration agent took 2,059,459 steps to locate the sparse reward (vs 26,663 for VASE). On other benchmarks the paper reports 'no-surprise' baseline is outperformed but provides no single-number returns in text; LunarLanderContinuous (dense reward) shows similar performance across surprise-driven and no-surprise methods.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported empirically: VASE required ~26.7k environment steps to find the first sparse reward in the 2DPlane example versus ~2.06M for random exploration (two orders of magnitude improvement). Other tasks: learning curves (median over 20 seeds) show faster attainment of task success in sparse environments compared to NLL and no-surprise baselines; exact sample counts for reaching specific return thresholds are not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced by adding intrinsic reward η·U_VASE to extrinsic reward; intrinsic reward promotes exploration by rewarding high expected NLL (surprisal) and expected information gain (Bayesian surprise) while subtracting δ·entropy(q) reduces reward for inherently uncertain agents (confidence correction). The agent alternates between updating the model (to reduce uncertainty) and updating the policy (to exploit the current model and intrinsic rewards). Hyperparameters η and δ control strength of intrinsic drive and confidence penalty; δ must be tuned (best reported interval [1e-4,1e-2]).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>VIME (Variational Information Maximizing Exploration), NLL / surprisal (Achiam & Sastry-style surprisal intrinsic reward), 'no-surprise' / random exploration baseline; comparisons use TRPO as base RL algorithm for all methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) The proposed assorted surprise (VASE) combines Bayesian surprise and surprisal into a single instantaneous intrinsic reward that is subjective (depends on agent belief), consistent (larger for low-likelihood states), and instantaneous (does not require belief update). 2) VASE yields much faster discovery in sparse-reward settings (e.g., 2DPlane 26.7k vs 2.06M steps) and outperforms surprisal baselines on all tested tasks; it matches or exceeds VIME on several tasks and overtakes VIME over time on some (e.g., DoublePendulum). 3) VASE is computationally faster than VIME because it avoids computing gradients through the BNN to get the intrinsic reward (VIME requires a backward pass per step). 4) The confidence (entropy) term requires tuning (δ); best δ in experiments falls in [1e-4,1e-2].</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) VIME performed best on sparse MountainCar in the experiments (VASE was not universally best). 2) Performance is sensitive to the confidence trade-off δ — too large or too small degrades performance. 3) The BNN posterior is approximated with a fully-factorised Gaussian which may limit expressiveness and thus the fidelity of surprise estimates. 4) Authors did not evaluate partial observability or pixel-based observations in experiments (they note extension to pixels / deep RL as future work). 5) On tasks with dense extrinsic rewards (LunarLanderContinuous) intrinsic surprise methods confer no advantage over no-surprise baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_details</strong></td>
                            <td>Exact surprise computation: U_VASE(s_{t+1}) = E_{θ∼q(·;φ)}[−log P(s_{t+1}|s_t,a_t,θ)] − δ H[q]; P(s_{t+1}|...,θ) modelled as Gaussian with σ_c=5; prior variance σ_m=0.5; N=10 samples used to estimate expectation; policy updates via TRPO with batch size 5000; replay pool sizes 100k or 5M depending on task; reported medians over 20 runs with fixed seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VASE: Variational Assorted Surprise Exploration for Reinforcement Learning', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1141.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1141.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Information Maximizing Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surprise-driven model-based exploration method that rewards transitions proportional to the information gain about the agent's dynamics model (Bayesian surprise), approximated using a Bayesian neural network and variational inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vime: Variational information maximizing exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VIME (baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based RL baseline that maintains a Bayesian dynamics model and computes intrinsic reward equal to the information gain (KL divergence between posterior and prior over model parameters after observing a transition). Implemented using a variational BNN; intrinsic reward computation in original VIME requires computing the gradient through the BNN to estimate the posterior update.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information-gain maximization / Bayesian surprise (variational approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent computes intrinsic reward as the KL divergence between pre- and post-transition beliefs about model parameters (estimated via variational inference); transitions that produce large expected posterior change are rewarded, guiding exploration to informative transitions. The model is updated to reduce uncertainty while the policy is trained on combined extrinsic+intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmarks as VASE (used as a baseline): sparse MountainCar, sparse CartPoleSwingup, sparse DoublePendulum, sparse HalfCheetah, sparse Ant, LunarLanderContinuous.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous state/action spaces, sparse extrinsic rewards for primary benchmarks; stochastic dynamics treated from agent's subjective belief perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As in VASE entry (see per-benchmark state/action dimensions and episode lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported as a strong baseline: VIME performed best on sparse MountainCar (best median among compared methods) and performed well initially on sparse DoublePendulum but was later overtaken by VASE; in other tasks VIME is generally competitive but in several locomotion tasks VASE outperformed VIME. VIME is computationally slower since it requires a per-step gradient through the BNN to compute intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitatively competitive; on some tasks (e.g., MountainCar) VIME reaches task success faster than VASE in early training, but exact sample counts are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic reward equals information gain (KL change) encouraging exploration of transitions that most reduce model uncertainty; policy then trades off extrinsic and intrinsic reward via additive weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against VASE, NLL/surprisal intrinsic reward, and no-surprise baseline in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>VIME is an effective information-gain-driven exploration method and remains a strong baseline, particularly on some tasks like sparse MountainCar; however it is computationally more expensive per timestep because intrinsic reward estimation requires computing gradients through the BNN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High computational cost due to needing a backward pass through the BNN per time step to compute Bayesian surprise; slower wall-clock time compared to VASE. Not always the best performer across all tasks (VASE surpasses it on some benchmarks over time).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VASE: Variational Assorted Surprise Exploration for Reinforcement Learning', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1141.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1141.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLL / Surprisal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Surprisal (negative log-likelihood intrinsic reward)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic reward equal to the negative log-likelihood of the observed next state under the agent's dynamics model (i.e., model prediction error), used to encourage the agent to visit states with high prediction error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surprise-based intrinsic motivation for deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Surprisal-based intrinsic reward (NLL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy trained with intrinsic reward r_i(s_t) = η · (−log P(s_{t+1}|s_t,a_t)); in practice this reduces to a squared prediction error or model NLL when the model outputs a Gaussian prediction. Implemented as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>curiosity-driven exploration via prediction error / surprisal</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent collects transitions and rewards transitions with high model prediction error (high NLL), which encourages visiting states where the dynamics model is currently inaccurate; the model is trained over time to reduce prediction error, thereby reducing surprisal for already-explored states.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same continuous-control benchmarks used in paper (sparse MountainCar, CartPoleSwingup, DoublePendulum, HalfCheetah, Ant, LunarLanderContinuous, 2DPlane).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous, stochastic dynamics (subjective), sparse extrinsic rewards in primary tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As described in the VASE entry for each benchmark (state/action dimensions, episode lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Consistently outperformed by VASE in all tested tasks; runs faster computationally than VIME but yields weaker exploration performance overall. No single-number performance provided; plotted median returns show inferior learning curves relative to VASE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient than VASE and sometimes VIME on the tested sparse-reward tasks (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration encouraged by rewarding prediction error (surprisal); no explicit confidence correction term, so uncertain agents may be overly rewarded for noisy/uninformative transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against VASE, VIME, and no-surprise baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Surprisal is fast to compute and is a reasonable intrinsic reward but underperforms information-gain-based methods and VASE (which includes a confidence term), especially in sparse high-dimensional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not capture information gain about model parameters; can reward high-variance/irrelevant transitions and therefore be less directed in exploration compared to methods that consider belief update (VIME) or confidence correction (VASE).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VASE: Variational Assorted Surprise Exploration for Reinforcement Learning', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1141.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1141.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CC Surprise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Confidence-Corrected Surprise (Faraji et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modification of Bayesian surprise that measures divergence between the agent's current belief and a 'flat' naive observer (equal model likelihoods), intended to promote faster adaptation in volatile environments; discussed as inspiration and related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Balancing new against old information: The role of surprise in learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Confidence-Corrected Surprise (CC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A theoretical surprise metric: U_CC(s_{t+1}) = D_{KL}[P(M|s_t,a_t) || P_flat(M|s_{t+1})], where P_flat is a naive posterior that treats all models equally; designed to allow rapid adaptation by adjusting learning in volatile environments. In the VASE paper it is referenced as related prior work but not implemented in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>modified Bayesian surprise / confidence-corrected information measure</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Conceptually reduces surprise for inherently uncertain agents by comparing to a flat observer; this would affect intrinsic reward-driven exploration by de-emphasising transitions that are unsurprising given high prior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>The approach aims to adjust surprise by agent confidence (entropy) and therefore indirectly modulate exploration vs exploitation; VASE adopts a similar confidence term explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Discussed as related theoretical work; VASE is inspired by the idea of confidence correction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Motivates inclusion of an explicit confidence (entropy) penalty in VASE; CC itself was not evaluated empirically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not evaluated in experiments here; theoretical motivation only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VASE: Variational Assorted Surprise Exploration for Reinforcement Learning', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vime: Variational information maximizing exploration <em>(Rating: 2)</em></li>
                <li>Surprise-based intrinsic motivation for deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Curiosity-driven exploration by self-supervised prediction <em>(Rating: 1)</em></li>
                <li>Large-scale study of curiosity-driven learning <em>(Rating: 1)</em></li>
                <li>Balancing new against old information: The role of surprise in learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1141",
    "paper_id": "paper-207756747",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "VASE",
            "name_full": "Variational Assorted Surprise Exploration",
            "brief_description": "A surprise-driven, model-based RL algorithm that defines an intrinsic reward (assorted surprise) as the expected negative log-likelihood under a distribution over dynamics models minus a confidence (entropy) term, approximated with a variational Bayesian neural network and used to guide exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "VASE (Variational Assorted Surprise Exploration)",
            "agent_description": "Model-based RL agent with two main components: a policy network f_π (MLFF) optimized with TRPO, and a Bayesian neural network (BNN) dynamics model f_m parametrized by θ with a fully-factorised Gaussian variational posterior q(θ; φ). The BNN is trained by variational inference (maximising an evidence lower bound) using an experience replay buffer; the policy and model are updated alternately.",
            "adaptive_design_method": "curiosity-driven exploration / information-gain-inspired intrinsic reward (assorted surprise) implemented via variational inference in a Bayesian dynamics model",
            "adaptation_strategy_description": "At each transition the agent computes an intrinsic reward U_VASE(s_{t+1}) = E_{θ∼q}[−log P(s_{t+1}|s_t,a_t,θ)] − δ H[q(θ;φ)], estimated with N=10 samples from q, and adds η·U_VASE to extrinsic reward; the policy (TRPO) is trained on the combined reward. The BNN posterior q(θ;φ) is updated using data from a replay buffer by maximising the variational lower bound (KL(q||prior) regulariser + expected log-likelihood). The intrinsic reward therefore directs exploration toward transitions that are either poorly predicted by the current model (high surprisal/expected NLL) or informative about model parameters (information gain), modulated by a confidence (entropy) penalty.",
            "environment_name": "Multiple continuous-control benchmarks (2DPlane / 2DPointRobot, sparse MountainCar, sparse CartPoleSwingup, sparse DoublePendulum, sparse HalfCheetah, sparse Ant, LunarLanderContinuous)",
            "environment_characteristics": "Stochastic MDPs treated as agent-subjective distributions; continuous state and action spaces; very sparse extrinsic rewards for most benchmarks (reward only on task success), episodic, generally fully-observed in experiments though the paper explicitly notes their formulation can treat observations as states even when true state is not directly observed (e.g., visual).",
            "environment_complexity": "Varies by benchmark: 2DPlane (S ⊂ R^2, A ⊂ R^2, wrap-around), sparse MountainCar (S ⊂ R^3, A ⊂ R^1), CartPoleSwingup (S ⊂ R^4, A ⊂ R^1), DoublePendulum (S ⊂ R^6, A ⊂ R^1), HalfCheetah (S ⊂ R^20, A ⊂ R^6), Ant (S ⊂ R^125, A ⊂ R^8), LunarLanderContinuous (S ⊂ R^8, A ⊂ R^2); typical episode lengths: 500 (most tasks) or 1000 (LunarLander). Replay pool sizes: 100k (classic tasks) or 5M (locomotion); BNN architecture: 1 hidden layer (32) for classic tasks, 2×64 for locomotion; N=10 samples used per surprise estimate.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative and some quantitative results reported: substantially faster discovery in 2DPlane/2DPointRobot (first non-zero extrinsic reward found in 26,663 steps for VASE vs 2,059,459 steps for no-surprise random exploration). Across continuous-control sparse-reward benchmarks, VASE outperforms surprisal (NLL) in all tasks, outperforms or matches VIME on most (VIME best initially on sparse MountainCar; VASE later surpasses VIME on DoublePendulum; VASE shows strong results on sparse CartPoleSwingup, HalfCheetah, Ant). Median performance curves were reported over 20 runs (no per-task numeric returns in main text). VASE is also substantially faster wall-clock time than VIME (no per-episode gradient-through-BNN overhead).",
            "performance_without_adaptation": "Random/no-surprise baseline: on 2DPlane the random-exploration agent took 2,059,459 steps to locate the sparse reward (vs 26,663 for VASE). On other benchmarks the paper reports 'no-surprise' baseline is outperformed but provides no single-number returns in text; LunarLanderContinuous (dense reward) shows similar performance across surprise-driven and no-surprise methods.",
            "sample_efficiency": "Reported empirically: VASE required ~26.7k environment steps to find the first sparse reward in the 2DPlane example versus ~2.06M for random exploration (two orders of magnitude improvement). Other tasks: learning curves (median over 20 seeds) show faster attainment of task success in sparse environments compared to NLL and no-surprise baselines; exact sample counts for reaching specific return thresholds are not tabulated in text.",
            "exploration_exploitation_tradeoff": "Balanced by adding intrinsic reward η·U_VASE to extrinsic reward; intrinsic reward promotes exploration by rewarding high expected NLL (surprisal) and expected information gain (Bayesian surprise) while subtracting δ·entropy(q) reduces reward for inherently uncertain agents (confidence correction). The agent alternates between updating the model (to reduce uncertainty) and updating the policy (to exploit the current model and intrinsic rewards). Hyperparameters η and δ control strength of intrinsic drive and confidence penalty; δ must be tuned (best reported interval [1e-4,1e-2]).",
            "comparison_methods": "VIME (Variational Information Maximizing Exploration), NLL / surprisal (Achiam & Sastry-style surprisal intrinsic reward), 'no-surprise' / random exploration baseline; comparisons use TRPO as base RL algorithm for all methods.",
            "key_results": "1) The proposed assorted surprise (VASE) combines Bayesian surprise and surprisal into a single instantaneous intrinsic reward that is subjective (depends on agent belief), consistent (larger for low-likelihood states), and instantaneous (does not require belief update). 2) VASE yields much faster discovery in sparse-reward settings (e.g., 2DPlane 26.7k vs 2.06M steps) and outperforms surprisal baselines on all tested tasks; it matches or exceeds VIME on several tasks and overtakes VIME over time on some (e.g., DoublePendulum). 3) VASE is computationally faster than VIME because it avoids computing gradients through the BNN to get the intrinsic reward (VIME requires a backward pass per step). 4) The confidence (entropy) term requires tuning (δ); best δ in experiments falls in [1e-4,1e-2].",
            "limitations_or_failures": "1) VIME performed best on sparse MountainCar in the experiments (VASE was not universally best). 2) Performance is sensitive to the confidence trade-off δ — too large or too small degrades performance. 3) The BNN posterior is approximated with a fully-factorised Gaussian which may limit expressiveness and thus the fidelity of surprise estimates. 4) Authors did not evaluate partial observability or pixel-based observations in experiments (they note extension to pixels / deep RL as future work). 5) On tasks with dense extrinsic rewards (LunarLanderContinuous) intrinsic surprise methods confer no advantage over no-surprise baseline.",
            "additional_details": "Exact surprise computation: U_VASE(s_{t+1}) = E_{θ∼q(·;φ)}[−log P(s_{t+1}|s_t,a_t,θ)] − δ H[q]; P(s_{t+1}|...,θ) modelled as Gaussian with σ_c=5; prior variance σ_m=0.5; N=10 samples used to estimate expectation; policy updates via TRPO with batch size 5000; replay pool sizes 100k or 5M depending on task; reported medians over 20 runs with fixed seeds.",
            "uuid": "e1141.0",
            "source_info": {
                "paper_title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "VIME",
            "name_full": "Variational Information Maximizing Exploration",
            "brief_description": "A surprise-driven model-based exploration method that rewards transitions proportional to the information gain about the agent's dynamics model (Bayesian surprise), approximated using a Bayesian neural network and variational inference.",
            "citation_title": "Vime: Variational information maximizing exploration",
            "mention_or_use": "use",
            "agent_name": "VIME (baseline in experiments)",
            "agent_description": "Model-based RL baseline that maintains a Bayesian dynamics model and computes intrinsic reward equal to the information gain (KL divergence between posterior and prior over model parameters after observing a transition). Implemented using a variational BNN; intrinsic reward computation in original VIME requires computing the gradient through the BNN to estimate the posterior update.",
            "adaptive_design_method": "information-gain maximization / Bayesian surprise (variational approximation)",
            "adaptation_strategy_description": "The agent computes intrinsic reward as the KL divergence between pre- and post-transition beliefs about model parameters (estimated via variational inference); transitions that produce large expected posterior change are rewarded, guiding exploration to informative transitions. The model is updated to reduce uncertainty while the policy is trained on combined extrinsic+intrinsic reward.",
            "environment_name": "Same benchmarks as VASE (used as a baseline): sparse MountainCar, sparse CartPoleSwingup, sparse DoublePendulum, sparse HalfCheetah, sparse Ant, LunarLanderContinuous.",
            "environment_characteristics": "Continuous state/action spaces, sparse extrinsic rewards for primary benchmarks; stochastic dynamics treated from agent's subjective belief perspective.",
            "environment_complexity": "As in VASE entry (see per-benchmark state/action dimensions and episode lengths).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported as a strong baseline: VIME performed best on sparse MountainCar (best median among compared methods) and performed well initially on sparse DoublePendulum but was later overtaken by VASE; in other tasks VIME is generally competitive but in several locomotion tasks VASE outperformed VIME. VIME is computationally slower since it requires a per-step gradient through the BNN to compute intrinsic reward.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Qualitatively competitive; on some tasks (e.g., MountainCar) VIME reaches task success faster than VASE in early training, but exact sample counts are not provided in the paper.",
            "exploration_exploitation_tradeoff": "Intrinsic reward equals information gain (KL change) encouraging exploration of transitions that most reduce model uncertainty; policy then trades off extrinsic and intrinsic reward via additive weighting.",
            "comparison_methods": "Compared against VASE, NLL/surprisal intrinsic reward, and no-surprise baseline in the paper's experiments.",
            "key_results": "VIME is an effective information-gain-driven exploration method and remains a strong baseline, particularly on some tasks like sparse MountainCar; however it is computationally more expensive per timestep because intrinsic reward estimation requires computing gradients through the BNN.",
            "limitations_or_failures": "High computational cost due to needing a backward pass through the BNN per time step to compute Bayesian surprise; slower wall-clock time compared to VASE. Not always the best performer across all tasks (VASE surpasses it on some benchmarks over time).",
            "uuid": "e1141.1",
            "source_info": {
                "paper_title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "NLL / Surprisal",
            "name_full": "Surprisal (negative log-likelihood intrinsic reward)",
            "brief_description": "An intrinsic reward equal to the negative log-likelihood of the observed next state under the agent's dynamics model (i.e., model prediction error), used to encourage the agent to visit states with high prediction error.",
            "citation_title": "Surprise-based intrinsic motivation for deep reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "Surprisal-based intrinsic reward (NLL)",
            "agent_description": "Policy trained with intrinsic reward r_i(s_t) = η · (−log P(s_{t+1}|s_t,a_t)); in practice this reduces to a squared prediction error or model NLL when the model outputs a Gaussian prediction. Implemented as a baseline in experiments.",
            "adaptive_design_method": "curiosity-driven exploration via prediction error / surprisal",
            "adaptation_strategy_description": "The agent collects transitions and rewards transitions with high model prediction error (high NLL), which encourages visiting states where the dynamics model is currently inaccurate; the model is trained over time to reduce prediction error, thereby reducing surprisal for already-explored states.",
            "environment_name": "Same continuous-control benchmarks used in paper (sparse MountainCar, CartPoleSwingup, DoublePendulum, HalfCheetah, Ant, LunarLanderContinuous, 2DPlane).",
            "environment_characteristics": "Continuous, stochastic dynamics (subjective), sparse extrinsic rewards in primary tasks.",
            "environment_complexity": "As described in the VASE entry for each benchmark (state/action dimensions, episode lengths).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Consistently outperformed by VASE in all tested tasks; runs faster computationally than VIME but yields weaker exploration performance overall. No single-number performance provided; plotted median returns show inferior learning curves relative to VASE.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Less sample-efficient than VASE and sometimes VIME on the tested sparse-reward tasks (qualitative).",
            "exploration_exploitation_tradeoff": "Exploration encouraged by rewarding prediction error (surprisal); no explicit confidence correction term, so uncertain agents may be overly rewarded for noisy/uninformative transitions.",
            "comparison_methods": "Compared against VASE, VIME, and no-surprise baseline.",
            "key_results": "Surprisal is fast to compute and is a reasonable intrinsic reward but underperforms information-gain-based methods and VASE (which includes a confidence term), especially in sparse high-dimensional tasks.",
            "limitations_or_failures": "Does not capture information gain about model parameters; can reward high-variance/irrelevant transitions and therefore be less directed in exploration compared to methods that consider belief update (VIME) or confidence correction (VASE).",
            "uuid": "e1141.2",
            "source_info": {
                "paper_title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "CC Surprise",
            "name_full": "Confidence-Corrected Surprise (Faraji et al.)",
            "brief_description": "A modification of Bayesian surprise that measures divergence between the agent's current belief and a 'flat' naive observer (equal model likelihoods), intended to promote faster adaptation in volatile environments; discussed as inspiration and related work.",
            "citation_title": "Balancing new against old information: The role of surprise in learning",
            "mention_or_use": "mention",
            "agent_name": "Confidence-Corrected Surprise (CC)",
            "agent_description": "A theoretical surprise metric: U_CC(s_{t+1}) = D_{KL}[P(M|s_t,a_t) || P_flat(M|s_{t+1})], where P_flat is a naive posterior that treats all models equally; designed to allow rapid adaptation by adjusting learning in volatile environments. In the VASE paper it is referenced as related prior work but not implemented in experiments.",
            "adaptive_design_method": "modified Bayesian surprise / confidence-corrected information measure",
            "adaptation_strategy_description": "Conceptually reduces surprise for inherently uncertain agents by comparing to a flat observer; this would affect intrinsic reward-driven exploration by de-emphasising transitions that are unsurprising given high prior uncertainty.",
            "environment_name": "",
            "environment_characteristics": "",
            "environment_complexity": "",
            "uses_adaptive_design": null,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "The approach aims to adjust surprise by agent confidence (entropy) and therefore indirectly modulate exploration vs exploitation; VASE adopts a similar confidence term explicitly.",
            "comparison_methods": "Discussed as related theoretical work; VASE is inspired by the idea of confidence correction.",
            "key_results": "Motivates inclusion of an explicit confidence (entropy) penalty in VASE; CC itself was not evaluated empirically in this paper.",
            "limitations_or_failures": "Not evaluated in experiments here; theoretical motivation only.",
            "uuid": "e1141.3",
            "source_info": {
                "paper_title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vime: Variational information maximizing exploration",
            "rating": 2,
            "sanitized_title": "vime_variational_information_maximizing_exploration"
        },
        {
            "paper_title": "Surprise-based intrinsic motivation for deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "surprisebased_intrinsic_motivation_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Curiosity-driven exploration by self-supervised prediction",
            "rating": 1,
            "sanitized_title": "curiositydriven_exploration_by_selfsupervised_prediction"
        },
        {
            "paper_title": "Large-scale study of curiosity-driven learning",
            "rating": 1,
            "sanitized_title": "largescale_study_of_curiositydriven_learning"
        },
        {
            "paper_title": "Balancing new against old information: The role of surprise in learning",
            "rating": 1,
            "sanitized_title": "balancing_new_against_old_information_the_role_of_surprise_in_learning"
        }
    ],
    "cost": 0.01411675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VASE: Variational Assorted Surprise Exploration for Reinforcement Learning</p>
<p>Haitao Xu haitao@cs.otago.ac.nz 
Department of Computer Science
University of Otago
DunedinNew Zealand</p>
<p>Brendan Mccane mccane@cs.otago.ac.nz 
Department of Computer Science
University of Otago
DunedinNew Zealand</p>
<p>Lech Szymanski 
Department of Computer Science
University of Otago
DunedinNew Zealand</p>
<p>VASE: Variational Assorted Surprise Exploration for Reinforcement Learning
Index Terms-surprisereinforcement learningexplorationvariational inferenceBayesian neural network
Exploration in environments with continuous control and sparse rewards remains a key challenge in reinforcement learning (RL). Recently, surprise has been used as an intrinsic reward that encourages systematic and efficient exploration. We introduce a new definition of surprise and its RL implementation named Variational Assorted Surprise Exploration (VASE). VASE uses a Bayesian neural network as a model of the environment dynamics and is trained using variational inference, alternately updating the accuracy of the agent's model and policy. Our experiments show that in continuous control sparse reward environments VASE outperforms other surprise-based exploration techniques.</p>
<p>I. INTRODUCTION</p>
<p>Reinforcement learning (RL) trains agents to act in an environment so as to maximise cumulative reward. The resulting behaviour is highly dependent on the trade-off between exploration and exploitation. During training, the more the agent departs from its current policy, the more it learns about the environment, which may lead it to a better policy; the closer it adheres to the current policy, the less time wasted exploring less effective options. How much and where to explore has an immense impact on the training and ultimately on what the agent learns. Designing exploration strategies, especially for increasingly complex environments, is still a significant challenge.</p>
<p>A common approach to exploration strategies is to rely on heuristics that introduce random perturbations into the choices of actions during training, such as -greedy [1] or Boltzmann exploration [2]. These methods instruct the agent to occasionally take an arbitrary action that may drive it into a new experience. Another way is through the addition of noise to the parameter space of the agent's policy neural network [3,4], which varies the policy itself to a similar random exploration net effect. These strategies can be highly inefficient because they are a result of random behaviour, which is especially problematic in high dimension state-action spaces (common in discretised continuous state-action space environments) because of the curse of dimensionality. Random exploration is also extremely inefficient in environments with sparse rewards, where the agent ends up wandering aimlessly through the state-space, learning nothing until (by sheer luck) Model-free RL vs. model-based/surprise-driven RL with s for state, re for the extrinsic (environment-driven) reward, a for action, r i for intrinsic (agent-driven) reward, π for the policy, and M for the model of the environment that makes a prediction of the next stateŝ.</p>
<p>it chances upon a reward. Not surprisingly, more methodical approaches were devised, which provide the agent with intrinsic rewards that encourage efficient exploration. These intrinsic rewards are derived from computations related to the notion of curiosity [5,6] or surprise [7,8].</p>
<p>In this paper, we propose a new definition of surprise, which drives our agents' intrinsic reward function. To compute and use this surprise for guiding exploration, we propose an algorithm called VASE (Variational Assorted Surprise Exploration) in a model-based RL framework (see Figure 1). VASE alternates the update step between the agent's policy and its model of the environment dynamics. The policy is implemented with a multilayer feed-forward (MLFF) neural network and the dynamics model with a Bayesian neural network (BNN [9,10,11]). We evaluate the performance of our method against other surprise-driven methods on continuous control tasks with sparse rewards. Our experimental results show VASE's superior performance.</p>
<p>II. RELATED WORK</p>
<p>In RL a finite horizon discounted Markov decision process (MDP) is defined by a tuple (S, A, P, r, γ, T, ρ 0 ) where: S is a state set, A an action set, P : S × A × S → [0, 1] is a transition probability distribution, r : S → R a reward function, γ ∈ (0, 1] a discount factor, T the horizon and ρ 0 an initial state distribution. A policy π : S × A → [0, 1] gives the probability with π(a|s) of taking action a in state s. Let τ = (s 0 , a 0 , · · · ) denote the whole trajectory, s 0 ∼ ρ 0 , a t ∼ π(a t |s t ). Our purpose is to find a policy π, modelled by f π (s t , ψ) with parameters ψ, which maximises the expected discounted total return E τ [ T t=0 γ t r(s t )] -a discounted sum of all rewards in a fixed horizon T .</p>
<p>A. Intrinsic rewards</p>
<p>Intrinsic motivation is essential for effective exploration when training the agent in an environment with sparse extrinsic rewards, or no rewards at all. The overall reward signal is computed as follows:
r(s t ) = r e (s t ) + r i (s t ),
where r e (s t ) represents the extrinsic reward from the environment, r i (s t ) represents the intrinsic reward computed by the agent. Even when r e (s t ) = 0, the intrinsic reward contributes to the cumulative reward, thus driving the learning process until non-zero extrinsic rewards are found. There are two broad approaches for encouraging the agent to explore through intrinsic rewards.</p>
<p>The first is the count-based approach [12,13,14,15], which maintains visit counters over all the states. The intrinsic reward is inversely proportional to the current state's counter, thus rewarding exploration of less frequented states. This approach becomes intractable with the increase of possible states, making it unfit for scenarios that have continuous stateaction spaces. The second is surprise-based.</p>
<p>B. Surprise-driven reinforcement learning</p>
<p>In model-based RL the agent maintains one or more models that predict the next state based on the current state and the action about to be taken. There are four possible regimes involving increasing amounts of probabilistic reasoning:</p>
<p>1) There is a single deterministic model 2) There is one model that produces a distribution over new states 3) There is a distribution of models, each of which is deterministic 4) There is a distribution of models, each of which produces a distribution over states. As an example, the model for case 1 could be a traditional neural network, case 2 variational auto-encoder, case 3 Bayesian neural network and case 4 Bayesian variational autoencoder. In cases 2-3, the outcome is a distribution over states and we are free to choose the most convenient formalism. For this paper we choose case 3 as in [16]. In this case the agent maintains a distribution P (M ) over models or hypotheses M ∈ M, where M : S × A → S predicts state s t+1 , given state s t and action a t . Furthermore, we have:
P (s t+1 |s t , a t , M ) = P (M |s t , a t ), when s t+1 = M (s t , a t ) 0, otherwise(1)
and P (s t+1 |s t , a t ) = M P (M |s t , a t )P (s t+1 |s t , a t , M )∂M.</p>
<p>The environment is assumed to be stochastic, but we consider the distributions in Equations 1 and 2 are subjective agentbased beliefs about the environment, and not the underlying objective truth. Furthermore, these distributions are nonstationary and change as the agent learns more about the environment. Also note that we equate states with observations even in complicated scenarios when the true state of the environment is not directly observed (e.g. the agent has a camera). This is because the goal of the agent in RL is to maximise long-term rewards, and we assume that those rewards are directly observable from the environment.</p>
<p>The distribution in Equation 2 allows for a number of definitions of surprise, some of which have previously appeared in the literature. In all cases, the accuracy of the distribution, after observing the next state, is used to derive surprise, U (s t+1 ), and forms the basis for intrinsic reward: r i (s t ) = ηU (s t ), where 0 &lt; η ≤ 1, and is used to encourage exploration of unexpected states [7,8,5,17,18,19]. The agent is rewarded for curiosity of the unknown as gauged by its model of the environment. Throughout training, the model is improved to be more accurate (and so less surprised) next time it encounters an already explored state. The hope is that this methodical approach to exploration will result in a speedier arrival of the agent at the states with non-zero extrinsic reward.</p>
<p>It is not entirely obvious how best to define the surprise. One proposed definition is the so-called surprisal [20], which is the negative log-likelihood (NLL) of the next state in RL tasks
U NLL (s * t+1 ) = − log P (s t+1 = s * t+1 |s t , a t ),(3)
where s * t+1 is the observed state at time t + 1. This notation is a bit clumsy, so we shorten it to:
U NLL (s t+1 ) = − log P (s t+1 |s t , a t ),(4)
which we think is clear and more concise. Surprisal is intuitive, simple and easy to compute, but it does not capture all the information available, because it only measures the surprise at a single point. An alternative is Bayesian surprise, which measures the difference between the prior distribution over the model space at time t, and the posterior distribution updated by Bayes' rule with the newly observed state (first used in RL by Storck et al. [21]):
P (M |s t , a t , s t+1 ) = P (M |s t , a t )P (s t+1 |s t , a t , M ) P (s t+1 |s t , a t )(5)
Bayesian surprise is defined as the Kullback-Leiber (KL) divergence between the prior and the posterior beliefs about the dynamics of the environment [22,23]:
U Bayes (s t+1 ) = D KL [P (M |s t , a t )||P (M |s t , a t , s t+1 )]. (6)
Bayesian surprise measures the difference between subjective beliefs prior and post an event. One problem with Bayesian surprise is that the agent does not express surprise until it updates its belief, which is inconsistent with the instantaneous response to surprise displayed by neural data [24].</p>
<p>Faraji et al. [25] introduced a modification of Bayesian surprise referred to as the confidence-corrected surprise (CC), which measures the difference between the agent's current beliefs about the world, and a naive observer who believes all models are equally likely:
U CC (s t+1 ) = D KL [P (M |s t , a t )||P flat (M |s t+1 )], (7)
They also designed a surprise minimization rule to let the agents adapt quickly to the environment, especially in a highly volatile environment. But confidence-corrected surprise was not applied to RL frameworks. Nevertheless, their definition of surprise is similar in spirit to ours.</p>
<p>To use Bayesian surprise for RL tasks, Houthooft et al. [7] proposed a surprise-driven exploration strategy called VIME (variational information maximizing exploration). They showed that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks with sparse rewards. However, to compute each reward, VIME needs to calculate the gradient through a Bayesian neural network (BNN) [9,10], which is used to implement the agent's model. This requires a forward and a backward pass, which leads to slow training speed. Achiam and Sastry [8] chose the surprisal of new observation as their intrinsic surprise reward. Their experimental results showed that surprisal does not perform as well as VIME, but it runs faster. They also showed that the surprisal includes L 2squared model prediction error, which was first proposed in [26] and later used as the curiosity in [5]. Burda et al. [6] also chose surprisal-based strategy exploration in large scale RL environments that provide no extrinsic rewards.</p>
<p>III. ASSORTED SURPRISE FOR EXPLORATION</p>
<p>In this paper, we focus on RL environments with continuous control and very sparse extrinsic rewards. To conquer the numerous shortcomings of existing definitions for surprise discussed in Section II-B, and inspired by the idea of confidencecorrected surprise [25], we note that in RL the definition of surprise should have the following characteristics: 1) subjectivity -the agent should hold subjective beliefs about the environment captured through P (M ); the surprise depends on an agent's belief and this belief can be updated during learning; 2) consistency -based on its belief, the agent should be more surprised by states with lower likelihood; 3) instancy -the agent should be surprised immediately when it observes a new state from the environment, without the need to update its belief first.</p>
<p>In order to address all of the above characteristics, we propose a new definition of surprise, which we refer to as assorted surprise for exploration (ASE):
U ASE (s t+1 ) =E M ∼P (·|st,at) [− log P (s t+1 |s t , a t , M )] − δH P (M |s t , a t ) ,(8)
where δ is a trade-off coefficient. The first term, E M ∼P (·|st,at) [− log P (s t+1 |s t , a t , M )], we call the assorted surprise term, and the second term, H P (M |s t , a t ) , the confidence term. We will next demonstrate that the assorted surprise satisfies all three characteristics we mentioned above. Subjectivity comes from the assorted surprise term in Equation 8 because it is an expectation over the agent's belief in the veracity of each of the models. It is also the sum of the Bayesian surprise and the surprisal (hence the name assorted) as shown in the following lemma.</p>
<p>Lemma 1 (Assorted surprise is the sum of Bayesian surprise and surprisal).
E M ∼P (·|st,at) [− log P (s t+1 |s t , a t , M )] = U Bayes (s t+1 ) + U NLL (s t+1 )
Proof.
E M ∼P (·|st,at) [− log P (s t+1 |s t , a t , M )] = − M P (M |s t , a t ) log P (s t+1 |s t , a t , M )∂M = − M P (M |s t , a t ) log P (M |s t , a t , s t+1 )P (s t+1 |s t , a t ) P (M |s t , a t ) ∂M = M P (M |s t , a t ) log P (M |s t , a t ) P (M |s t , a t , s t+1 ) ∂M − M P (M |s t , a t ) log P (s t+1 |s t , a t )∂M = D KL [P (M |s t , a t )||P (M |s t , a t , s t+1 )] − log P (s t+1 |s t , a t )] = U Bayes (s t+1 ) +U NLL (s t+1 ),
This means that assorted surprise is subjective due to the contribution of U Bayes . However, the expectation in Eq. 8 does not require evaluation of P (M |s t , a t , s t+1 ), so there is no requirement to update the agent's belief in order to compute assorted surprise thus satisfying the instancy characteristic. Finally, a less likely state leads to a larger surprise through the negative log likelihood of P (s t+1 |s t , a t ) in the surprisal term. This satisfies the consistency requirement.</p>
<p>The confidence term of Eq. 8, H(P (M |s t , a t )), is the entropy of P (M |s t , a t ). This term was added for confidence correction of assorted surprise. A confident agent will have a low entropy, and therefore any surprising event according to the assorted surprise term will remain surprising. Whereas an uncertain agent will have a large entropy and their overall surprise will be reduced because they would be equally surprised by many events. That is, confident agents are more surprised when their beliefs are violated by unlikely events than uncertain agents.</p>
<p>Assorted surprise captures the positive elements of both Bayesian surprise and confidence-corrected surprise. It implicitly computes the difference in belief as in Bayesian surprise without needing to update the belief first, and it can be computed very fast as in confidence-corrected surprise without needing to maintain the idea of a naive observer.</p>
<p>A. Variational Assorted Surprise for Exploration (VASE)</p>
<p>Based on the discussion in Section II-B, we construct a BNN dynamics model f m (s t , a t , Θ), where Θ is a random variable describing the parameters of the model (see Figure  2). BNN can be seen as a distribution of models M , where a sample of network parameters θ according to distribution P (θ) is analogous to generating a single prediction of the next state according to P (M ). The prior distribution P (θ) changes to posterior P (θ|D) when BNN is trained by D = {s t , a t , s t+1 }.</p>
<p>Since the posterior P (θ|s t , a t ) in Eq. 8 is intractable, we turn to variational inference [27] to approximate it with a fully factorised Gaussian distribution [9,10,11] 
q(θ; φ) = |Θ| i=1 N (θ i ; µ i , σ 2 i ),(9)
where θ i is the i th component of θ, and φ i = (µ i , σ i ). The use of q(θ; φ) in place of P (θ|s t , a t ) changes the definition of surprise from Eq. 8 to one we call variational assorted surprise for exploration (VASE):
U VASE (s t+1 ) = E θ∼q(·;φ) [− log P (s t+1 |s t , a t , θ)] −δH(q(θ; φ)).(10)
Since the output of the model for sample θ gives the prediction of the next state s t+1 = f m (s t , a t , θ), we define P (s t+1 |s t , a t , θ) by measuring the deviation of s t+1 from s t+1 under the assumption that states are normally distributed:
P (s t+1 |s t , a t , θ) = 1 2πσ 2 c e −|| st+1−st+1|| 2 /(2σ 2 c ) ,(11)
where σ c is an arbitrarily chosen constant, || s t+1 − s t+1 || is the norm of the difference vector between the prediction of the next state and the true next state. Note that this is a slightly different formulation to that in Equation 1 but approaches that formulation as σ c approaches 0.</p>
<p>N samples of θ ∼ q(·; φ) give N predictions for the next state from the BNN, which allows us to estimate the first term of Eq. 10 with the average:
E θ∼q(·;φ) [log P (s t+1 |s t , a t , θ)] ≈ 1 N N n=1 log P (s t+1 |s t , a t , θ [n] ),
where θ [n] is the n th sample of Θ drawn from q(θ; φ) and P (s t+1 |s t , a t , θ [n] ) is evaluated according to Eq. 11. Since q(θ; φ) is a fully factorised Gaussian distribution, the second term of Eq. 10 is straight forward to evalute:
H q(θ; φ) = |Θ| i=1 H(N (θ i ; µ i , σ 2 i )) = 1 2 |Θ| i=1 (log(2πeσ 2 i ).
The last thing remaining is to ensure q(θ; φ) is as close as possible to P (θ|D). Variational inference uses Kullback-Leibler (KL) divergence for measuring how different q(θ; φ) is from P (θ|D):
D KL [q(θ; φ)||P (θ|D)] = θ q(θ; φ) log q(θ; φ) P (θ|D) ∂θ = D KL [q(θ; φ)||P (θ)] −E θ∼q(·;φ) [log P (D|θ)] + log P (D).
This difference is minimised by changing φ, which is equivalent to maximising the variational lower bound [27]: (12) which does not require evaluation of P (θ|D). In this paper, the prior distribution of θ is taken to be
L[q(θ; φ), D] = E θ∼q(·;φ) [log P (D|θ)] − D KL [q(θ; φ)||P (θ)],P (θ) = |Θ| i=1 N (θ i ; 0, σ 2 m ),(13)
where σ m is set to arbitrary value, and the expectation of log likelihood of P (θ|D) is evaluated as in Eq. 12.</p>
<p>The entire training procedure is listed in Algorithm 1.</p>
<p>IV. EXPERIMENTS AND RESULTS ANALYSIS</p>
<p>A. Visualising exploration efficiency</p>
<p>For illustrative purposes, we begin the experimental evaluation of VASE by testing it on a simple 2DPlane environment (S ⊂ R 2 , A ⊂ R 2 ) which lends itself to a visualisation of the agent's exploration efficiency. The observation space is a square on the 2D plane ((x, y) ∈ R 2 ), centred on the origin. The action is its velocity (ẋ,ẏ) that satisfies |ẋ| ≤ 0.01, |ẏ| ≤ 0.01. In this environment the agent starts at origin (0,0) and the only extrinsic reward can be found at location (1,1). The environment wraps around, so that there are no boundaries.</p>
<p>In this experiment, we train one agent and record the observation coordinate (x, y) in each step until it finds the non </p>
<p>Algorithm 1: Variational Assorted Surprise Exploration (VASE)</p>
<p>Initialise policy neural network f π with parameters ψ Initialise agent's BNN model f m : Initialise q(θ; φ) with parameters φ Initialise prior distribution P (θ) Initialise experience buffer R. Reset the environment getting (s 0 , r e (s 0 )). for each iteration n do for each time step t do Get action a t ∼ f π (s t , ψ) Sample θ N times according to q(θ; φ) Evaluate N predictions s t+1 = f m (s t , a t , θ) Take action a t getting (s t+1 , r e (s t+1 )) Compute intrinsic surprise U VASE (s t+1 ) Construct cumulative reward r(s t+1 ) = r e (s t+1 ) + ηU VASE (s t+1 ) Add new (s t , a t , s t+1 , r(s t+1 )) to R end Update f m by maximising Eq. (12), with D sampled randomly from R Update f π using TRPO. end zero extrinsic reward. Figure 3 shows the heat map of motion track for the agent trained without surprise and with VASE surprise. Darker red colour represents a higher density, which means the agent lingers more steps in this area. It is clear that random exploration strategy takes a long time (2,059,459 steps V.S. 26,663 steps) to find that first non-zero r e (s t ), whereas VASE does not spend time unnecessarily in random states.</p>
<p>B. Continuous state/action environments</p>
<p>Next we evaluate VASE on five continuous control benchmarks with very sparse reward, including three classic tasks: sparse MountainCar (S ⊂ R 3 , A ⊂ R 1 ), sparse Cart-PoleSwingup (S ⊂ R 4 , A ⊂ R 1 ), sparse Doublependulum (S ⊂ R 6 , A ⊂ R 1 ) and two locomotion tasks: sparse HalfCheetah (S ⊂ R 20 , A ⊂ R 6 ), sparse Ant (S ⊂ R 125 , A ⊂ R 8 ). These tasks were introduced in [7]. We also evaluate VASE on LunarLanderContinuous (S ⊂ R 8 , A ⊂ R 2 ) task.</p>
<p>For the sparse MountainCar task, the car will climb a onedimensional hill to reach the target. The target is located on top of a hill and on the right-hand side of the car. If the car reaches the target, the episode is done. The observation is given by the horizontal position and the horizontal velocity of the car. The agent car receives a reward of 1 only when it reaches the target. For the sparse CartpoleSwingup task, a pole is mounted on a cart. The cart itself is limited to linear motion. Continuous cart movement is required to keep the pole upright. The system should not only be able to balance the pole, but also be able to swing it to an upright position first. The observation includes the cart position x, pole angle β, the cart velocityẋ, and the pole velocityβ. The action is the horizontal force applied to the cart. The agent receives a reward of 1 only when cos(β) &gt; 0.9.</p>
<p>For the sparse Doublependulum task, the goal is to stabilise a two-link pendulum at the upright position. The observation includes joint angles (β 1 and β 2 ) and joint speeds (β 1 anḋ β 2 ). The action is the same as in CartpoleSwingup task. The agent receives a reward of 1 only when dist &lt; 0.1, with dist the distance between current pendulum tip position and target position.</p>
<p>For the sparse HalfCheetah task, the half-cheetah is a flat biped robot with nine rigid links, including two legs and one torso, and six joints. 20-dimensional observations include joint angle, joint velocity, and centroid coordinates. The agent receives a reward of 1 when x body ≥ 5.</p>
<p>For the sparse Ant task, the ant has 13 rigid links, including four legs and a torso, along with 8 actuated joints. The 125-dim observation includes joint angles, joint velocities, coordinates of the centre of mass, a (usually sparse) vector of contact forces, as well as the rotation matrix for the body. The ant receives a reward of 1 when x body ≥ 3. For Lunar-lander task, the agent tries to learn to fly and then land on its landing pad. The episode is done if the lander crashes or comes to rest. The agent should get rewards of 200 when it solves this task.</p>
<p>All the environments except MountainCar and 2DPointRobot tasks (they are two simple tasks, we do not need to normalise them) are normalised before the algorithm starts. Here normalise the task means normalise its observations, for each observation o:
o = (o − µ o ) σ o ,
where µ o and σ o are the mean and standard deviation of observations. All observations and actions in these environments are continuous values. To compare with [8] and [7], we also use Trust Region Policy Optimization (TRPO) [28] method as our base reinforcement learning algorithm throughout our experiments, and we use the rllab [29] implementations of TRPO.</p>
<p>The number of samples drawn to compute our surprise is N = 10. The prior distribution P (θ) is given by a Gaussian distribution from Eq. 13 with σ m = 0.5. σ c in Eq. 11 is set as 5. For the classic tasks sparse MountainCar, sparse CartPoleSwingup, sparse DoublePendulum and sparse LunarLanderContinuous, the f m has one hidden layer of 32 units. All hidden layers have rectified linear unit (ReLU) nonlinearities. The replay pool R has a fixed size of 100,000 samples, with a minimum size of 500 samples. For the locomotion tasks sparse HalfCheetah and sparse Ant, the f m has two hidden layers of 64 units each. All hidden layers have tanh non-linearities. The replay pool R has a fixed size of 5,000,000 samples. The Adam learning rate of f m is set to 0.001. All output layers are set to linear. The batch size for the policy optimisation is set to 5,000. For f π the classic tasks use a neural network with one layer of 32 tanh units, while the locomotion task uses a two-layer neural network of 64 and 32 tanh units. For baseline, the classic tasks use a neural network with one layer of 32 ReLU units, while the locomotion task uses a linear function. The maximum length of trajectory LunarLanderContinuous 1000, for all the other tasks, it is 500. Figure 4 (a)-(e) shows the median performance of three classic control tasks and two locomotion tasks. All these tasks are with sparse rewards. Figure 4 (f) shows the median performance of LunarLanderContinuous task. The agent can easily obtain rewards from this task. The performance is measured through the average return E τ [ T t=0 r e (s t )], not including the intrinsic rewards. The median performance curves with shaded interquartile ranges areas. Figure 6 shows the speed comparison on MountainCar task.</p>
<p>As can be seen from Figure 4 (a)-(e), VIME performs best for sparse MountainCar task. For the sparse DoublePendulum, VIME performs well initially, but is later surpassed by VASE. VASE shows good results in sparse CartpoleSwingup, sparse HalfCheetah and sparse Ant tasks. We can also see that VASE always performs better than NLL (suprisal) in all tasks. Figure 4 (f) shows that in LunarLanderContinuous task that has enough reward for the agent, all surprise-driven methods behave almost the same to the no-surprise method. Figure 6 shows us the speed test results. For VIME, it needs to calculate a gradient through its BNN at each time step to compute the Bayesian surprise reward. This is really time consuming. However, for our VASE algorithm, it does not need to compute this gradient. Figure 6 shows that VASE runs much faster than VIME.</p>
<p>Finally, we also test how different values of trade-off δ that we used in Eq. (10) affects the performs of surprise U VASE on sparse MountainCar environment. We know that the value of H(q(M ; φ)) depends not only on the distribution of each parameter M i , but also on the number of parameters |M |. Meanwhile, in the beginning stages of training, the entropy of each parameter M i is relatively large, therefore, we should take a relatively small value of δ. Figure 5 shows VASE performance based on δ chosen from {0, 1e-6, 1e-4, 1e-2, 1}. The performance is not good when δ is too big or too small. It shows that the best interval to search δ is [1e-4,1e-2].</p>
<p>V. CONCLUSIONS</p>
<p>In this work, we chose a new form of surprise as the agent's intrinsic motivation and applied it to the RL settings by our VASE algorithm. VASE tries to approximate this surprise in a tractable way and train the agent to maximise its reward function. The agent is driven by this intrinsic reward, which can effectively explore the environment and find sparse extrinsic rewards given by the environment. Empirical results show that VASE performs well across various continuous control tasks with sparse rewards. We believe that VASE can be easily extended to deep reinforcement learning methods or learn directly from pixel features. We leave that to future work to explore.</p>
<p>Fig. 1. Model-free RL vs. model-based/surprise-driven RL with s for state, re for the extrinsic (environment-driven) reward, a for action, r i for intrinsic (agent-driven) reward, π for the policy, and M for the model of the environment that makes a prediction of the next stateŝ.</p>
<p>Fig. 2 .
2VASE-driven RL with s for state, re for the extrinsic (environmentdriven) reward, a for action, r i for intrinsic (agent-driven) reward, at = fπ(st, ψ) the policy governed by set of parameters ψ, and s = fm(st, at, θ) the sample BNN model of the environment.</p>
<p>Fig. 3 .
3Exploration efficiency as a heatmap showing the number of states visited during training in 2DPointRobot environment until chancing upon the reward state with a) no surprise, b) VASE.</p>
<p>Fig. 4 .
4Median performance for the continuous control tasks over 20 runs with a fixed set of seeds, with interquartile ranges shown in shaded areas. VIME, NLL and VASE use Bayesian surprise, surprisal, our surprise respectively.</p>
<p>Fig. 6 .
6Running time on MountainCar environment.</p>
<p>Introduction to Reinforcement Learning. Richard S Sutton, Andrew G Barto, MIT pressRichard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT press, 1998.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Shane Legg, and Demis Hassabis. Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen KingDaan Wierstra518Dharshan KumaranVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Ku- maran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learn- ing. Nature, 518(7540):529-533, 2015.</p>
<p>Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, arXiv:1706.10295Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprintMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.</p>
<p>Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Y Richard, Xi Chen, Chen, arXiv:1706.01905Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprintMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim As- four, Pieter Abbeel, and Marcin Andrychowicz. Pa- rameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.</p>
<p>Curiosity-driven exploration by selfsupervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self- supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16-17, 2017.</p>
<p>Largescale study of curiosity-driven learning. Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A Efros, arXiv:1808.04355arXiv preprintYuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large- scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.</p>
<p>Vime: Variational information maximizing exploration. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel, Advances in Neural Information Processing Systems. Curran Associates, Inc29Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems 29, pages 1109- 1117. Curran Associates, Inc., 2016.</p>
<p>Surprise-based intrinsic motivation for deep reinforcement learning. Joshua Achiam, Shankar Sastry, arXiv:1703.01732arXiv preprintJoshua Achiam and Shankar Sastry. Surprise-based in- trinsic motivation for deep reinforcement learning. arXiv preprint arXiv:1703.01732, 2017.</p>
<p>Practical variational inference for neural networks. Alex Graves, In Advances in Neural Information Processing Systems. 24Curran Associates, IncAlex Graves. Practical variational inference for neural networks. In Advances in Neural Information Processing Systems 24, pages 2348-2356. Curran Associates, Inc., 2011.</p>
<p>Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra, arXiv:1505.05424Weight uncertainty in neural networks. arXiv preprintCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural net- works. arXiv preprint arXiv:1505.05424, 2015.</p>
<p>Keeping the neural networks simple by minimizing the description length of the weights. E Geoffrey, Drew Hinton, Van Camp, Proceedings of the Sixth ACM Conference on Computational Learning Theory. the Sixth ACM Conference on Computational Learning TheoryACM PressGeoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the Sixth ACM Conference on Computational Learning Theory, pages 5-13. ACM Press, 1993.</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, Advances in Neural Information Processing Systems. 29Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems 29, pages 1471-1479, 2016.</p>
<p>Aäron van den Oord, and Rémi Munos. Count-based exploration with neural density models. Georg Ostrovski, Marc G Bellemare, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Georg Ostrovski, Marc G. Bellemare, Aäron van den Oord, and Rémi Munos. Count-based exploration with neural density models. In Proceedings of the 34th International Conference on Machine Learning -Volume 70, pages 2721-2730. JMLR.org, 2017.</p>
<p>Exploration in model-based reinforcement learning by empirically estimating learning progress. Manuel Lopes, Tobias Lang, Marc Toussaint, Pierre-Yves Oudeyer, Advances in Neural Information Processing Systems. Curran Associates, Inc25Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre- Yves Oudeyer. Exploration in model-based rein- forcement learning by empirically estimating learning progress. In Advances in Neural Information Processing Systems 25, pages 206-214. Curran Associates, Inc., 2012.</p>
<p>An analytic solution to discrete bayesian reinforcement learning. Pascal Poupart, Nikos Vlassis, Jesse Hoey, Kevin Regan, ICML. ACMPascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete bayesian rein- forcement learning. In ICML, pages 697-704. ACM, 2006.</p>
<p>Of bits and wows: a bayesian theory of surprise with applications to attention. Pierre Baldi, Laurent Itti, Neural Networks. 235Pierre Baldi and Laurent Itti. Of bits and wows: a bayesian theory of surprise with applications to attention. Neural Networks, 23(5):649-666, 2010.</p>
<p>Variational information maximisation for intrinsically motivated reinforcement learning. Shakir Mohamed, Danilo Jimenez Rezende, Advances in Neural Information Processing Systems. 28Shakir Mohamed and Danilo Jimenez Rezende. Vari- ational information maximisation for intrinsically mo- tivated reinforcement learning. In Advances in Neural Information Processing Systems 28, pages 2125-2133.</p>
<p>A possibility for implementing curiosity and boredom in model-building neural controllers. Jürgen Schmidhuber, Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats. the First International Conference on Simulation of Adaptive Behavior on From Animals to AnimatsMIT PressJürgen Schmidhuber. A possibility for implementing cu- riosity and boredom in model-building neural controllers. In Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pages 222-227. MIT Press, 1991.</p>
<p>Intrinsically motivated reinforcement learning. Nuttapong Chentanez, Andrew G Barto, Satinder P Singh, Advances in Neural Information Processing Systems. MIT Press17Nuttapong Chentanez, Andrew G. Barto, and Satinder P. Singh. Intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems 17, pages 1281-1288. MIT Press, 2005.</p>
<p>Thermostatics and thermodynamics: an introduction to energy, information and states of matter, with engineering applications. Myron Tribus, Van NostrandMyron Tribus. Thermostatics and thermodynamics: an introduction to energy, information and states of matter, with engineering applications. Van Nostrand, 1961.</p>
<p>Reinforcement driven information acquisition in nondeterministic environments. Jan Storck, Sepp Hochreiter, Jürgen Schmidhuber, Proceedings of the International Conference on Artificial Neural Networks. the International Conference on Artificial Neural Networks2Jan Storck, Sepp Hochreiter, and Jürgen Schmidhuber. Reinforcement driven information acquisition in non- deterministic environments. In Proceedings of the In- ternational Conference on Artificial Neural Networks, volume 2, pages 159-164, 1995.</p>
<p>A principled approach to detecting surprising events in video. Laurent Itti, Pierre Baldi, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). 1Laurent Itti and Pierre Baldi. A principled approach to detecting surprising events in video. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), volume 1, pages 631- 637, 2005.</p>
<p>Bayesian surprise attracts human attention. Laurent Itti, Pierre F Baldi, Advances in Neural Information Processing Systems. 18Laurent Itti and Pierre F. Baldi. Bayesian surprise attracts human attention. In Advances in Neural Information Processing Systems 18, pages 547-554, 2006.</p>
<p>Learning with Surprise: Theory and Applications. Mohammadjavad Faraji, Ecole Polytechnique Fédérale de LausannePhD thesisMohammadjavad Faraji. Learning with Surprise: Theory and Applications. PhD thesis, Ecole Polytechnique Fédérale de Lausanne, 2016.</p>
<p>Balancing new against old information: The role of surprise in learning. Mohammadjavad Faraji, Kerstin Preuschoff, Wulfram Gerstner, arXiv:1606.05642arXiv preprintMohammadjavad Faraji, Kerstin Preuschoff, and Wul- fram Gerstner. Balancing new against old informa- tion: The role of surprise in learning. arXiv preprint arXiv:1606.05642, 2016.</p>
<p>Incentivizing exploration in reinforcement learning with deep predictive models. C Bradly, Sergey Stadie, Pieter Levine, Abbeel, arXiv:1507.00814arXiv preprintBradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.</p>
<p>Pattern Recognition and Machine Learning. Christopher M Bishop, Springer-VerlagChristopher M. Bishop. Pattern Recognition and Ma- chine Learning. Springer-Verlag, 2006.</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, ICML. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimiza- tion. In ICML, pages 1889-1897, 2015.</p>
<p>Benchmarking deep reinforcement learning for continuous control. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel, ICML. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In ICML, pages 1329- 1338, 2016.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2614 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2614</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2614</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-767a7e949ba4520888e7442ee01e6a37c254fc53</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/767a7e949ba4520888e7442ee01e6a37c254fc53" target="_blank">CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> CLIN is presented, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates.</p>
                <p><strong>Paper Abstract:</strong> Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory centered on causal abstractions (rather than general"helpful hints") that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2614.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2614.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continual Learning from INteractions (CLIN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented, language-agent architecture that uses a frozen LLM controller and executor plus a memory-generator to build and continually refine a persistent set of causal abstractions (in natural language) from trial interactions to improve zero-shot and adaptation performance in text-based simulated science tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CLIN is an embodied language agent composed of (1) a persistent dynamic memory of causal-abstraction sentences, (2) a controller (frozen LLM) that selects and uses retrieved memory plus trial history to generate the next goal, (3) an executor (frozen LLM) that maps goals to admissible actions (with embedding-based correction and iterative refinement), and (4) a memory-generator (frozen LLM) that reflects on a completed trial and previous memories to produce updated causal abstractions and uncertainty annotations. Memory items use constrained templates (e.g., 'X SHOULD BE NECESSARY to Y', 'X DOES NOT CONTRIBUTE to Y') and a saliency-based pruning using final reward to keep crucial insights. CLIN uses meta-memory built from best trials across episodes to generalize to new environments/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery / Continual Learning Language Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Interactive scientific reasoning in simulated environments (text-based embodied science tasks: thermodynamics, genetics, chemistry, friction/force experiments, plant growth, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>CLIN attempts goal-directed sequential decision-making tasks in ScienceWorld: partially observable text-based POMDPs where the agent must plan and execute sequences of natural-language actions to achieve science goals (e.g., boil a liquid, freeze a substance, grow a plant, determine friction). Tasks vary in length (Short vs Long) and environment configuration (different room layouts, objects, working/broken devices).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: partially observable Markov decision processes with large discrete action/template spaces and many objects/states; sparse, episodic rewards tied to subgoals; long horizon variability (short tasks oracle <37 steps, long >=37). No explicit numeric dimensionality given but search/action space is described as intractable in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Self-generated interaction data only (no gold trajectories). CLIN starts with empty memory and builds experience via repeated trials; ScienceWorld provides simulator observations and sparse textual feedback. The paper uses 164 task-environment combinations drawn from ScienceWorld test split.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Qualitative: uses instruction-tuned LLMs (gpt-4) as frozen modules at runtime; no parameter updates required. No quantitative compute-hours or cost reported. Authors contrast CLIN with RL methods requiring large pretraining/sample requirements; CLIN's compute is dominated by LLM inference per trial and memory-generation calls.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-scoped sequential decision tasks but open-ended in environment variations; discrete action space (action templates + objects); stochasticity from partial observability and different env configs; clear evaluation metric (final reward from ScienceWorld); domain knowledge helpful (causal relations between actions and state transitions).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Final reward score from ScienceWorld simulator (numeric 0–100 scale per task); also counts of tasks solved and number of steps to completion.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>As reported: BASE (zero-shot) average final reward across 164 task-env combos = 48.6; ADAPT (continual learning within same env across up to 5 trials) average = 62.2 (absolute improvement +13.6 points over BASE). GEN-ENV (meta-memory for new environment, zero-shot on new env) average = 52.7 (+4.1 over BASE). GEN-ADAPT (start with meta-memory then continue adaptation) average = 69.5 (CLIN beats Reflexion by 23 absolute points in G+A). For GEN-TASK transfer to new tasks, CLIN reports a 13-point zero-shot improvement over BASE and further gains via adaptation (example: CLIN improves 13 points for new tasks, and additional 7 points with adaptation in some experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Dependence on self-exploration: if CLIN never encounters critical locations/actions, it cannot generate related insights. Poor memory retrieval: relevant meta-memory items can be overshadowed by misleading but higher-retrieved items (e.g., retrieving 'stove necessary to boil' when stove is broken). Limited exploration early in training due to large action space.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Structured causal-abstraction memory templates (necessary / does not contribute) that capture actionable causal knowledge; persistent memory across trials and saliency pruning based on reward; meta-memory synthesized from best trials to enable transfer; use of frozen LLMs avoids costly parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>CLIN outperforms generative-agent baselines and RL baselines across setups: in ADAPT CLIN (ADAPT=62.2) > Reflexion (39.4) and ReAct (29.6); in GEN-ENV CLIN (GEN-ENV=52.7) > SayCan (32.9), ReAct (29.6), and RL methods (DRRN 16.7, KG-A2C 12.7, CALM 3.6). CLIN shows larger relative gains on longer tasks and when using meta-memory plus adaptation (G+A=69.5).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported for ScienceWorld tasks in this paper; no quantitative human-expert baselines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2614.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reflective language-agent that generates trial-specific natural-language reflections on failed attempts and uses those reflections to re-plan and retry tasks; reflections are focused on the current trial and are not maintained as a long-term persistent memory across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reflexion prompts a frozen LLM to reflect on its recent failed attempt to produce language-based insights and then integrates those reflections into a retry prompt to attempt the task again. It does not maintain a long-term persistent, structured memory across episodes—its insights are typically trial- and environment-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Reflective Language Agent / Automated Retry System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Interactive decision-making and embodied agents in text-based environments (same ScienceWorld domain used for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improving task success by reflecting on a failed attempt to generate a modified plan for immediate retry in the same episode.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Applied to the same POMDP-style text tasks; complexity similar to CLIN but reflexion focuses on per-trial reflection rather than episodic memory accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on the single-trial trace (no long-term archive). Uses observations and outcomes from the failed trial as input for reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Uses frozen LLM inference for reflections and planning; no parameter updates. No numeric compute reported.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Focuses on immediate retry; structured as single-episode reflection → modified plan; discrete action space; clear reward feedback per trial.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Final reward score from ScienceWorld as used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported overall (Table 2) average final reward across tasks = 39.4 (compared to CLIN BASE 48.6 and CLIN ADAPT 62.2). Per-task numbers vary (e.g., some short tasks show higher values on particular variants).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Reflections tend to be highly task- and environment-specific (e.g., 'go to desk 1 and find the lamp'), which can limit generalization; lacks long-term persistent memory, reducing benefit across episodes; may only learn from mistakes and not accumulate positive causal abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Can generate trial-specific corrective insights quickly without parameter tuning; benefits when per-trial reflection yields actionable corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared against CLIN, Reflexion shows smaller adaptation gains; CLIN's persistent causal memory yields larger improvements across trials and generalization (CLIN ADAPT +23 points over Reflexion in G+A). Reflexion does better than ReAct baseline in some per-task numbers but is overall weaker than CLIN.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No human baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2614.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based open-ended embodied agent operating in Minecraft that grows a code-based skill library from failure/success feedback to enable increasingly complex behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Voyager interacts in Minecraft, uses LLMs to generate code-based skills from experience and feedback, autonomously builds a skill library (programs/scripts) to expand capabilities over time, and uses these skills to accomplish more complex open-ended goals.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Open-ended Embodied Agent / Skill-acquisition System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Open-ended game world (Minecraft) — embodied agent skill acquisition and long-horizon task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Autonomously explore, discover, and codify reusable skills from interactions in Minecraft to tackle increasingly complex tasks in an open-ended environment.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: open-ended with very large state/action space, hierarchical tasks, and need for programmatic skill composition; requires handling long horizons and complex dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Self-generated interactive data from Minecraft; rich environment observations; no gold trajectories required.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not quantified in this paper; literature notes large-scale LLM inference and environment simulation; Voyager relies on code-synthesis plus execution in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, continuous development of skills; not a fixed POMDP; lacks a single well-defined reward across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task completion and emergent skill repertoire growth in Minecraft as reported in Voyager paper (not quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper; referenced as prior work demonstrating skill growth but with no numeric comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not detailed here; general limits include reliance on environment-specific affordances and potential brittleness of synthesized skills.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Rich environment feedback in Minecraft and programmatic skill representation enabling reuse; code-based skills capture reusable behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as related work that demonstrates building skill libraries from failures, inspiring CLIN's memory approach; not directly compared experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2614.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AdA (Adaptive-Agent-Team human-timescale adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent demonstrating rapid adaptation to open-ended novel 3D problems using meta-reinforcement learning, but requiring extensive pretraining; mentioned as related work on fast adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-timescale adaptation in an open-ended task space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AdA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A meta-reinforcement-learning based agent that adapts policy on the fly to new 3D environments/problem instances by leveraging pretraining across many tasks, enabling rapid within-episode adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Meta-Reinforcement Learning Adaptive Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Open-ended 3D embodied tasks (rapid adaptation to novel problems in 3D simulated environments).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Adapt policies quickly to previously unseen task variants in 3D worlds by exploiting meta-learned priors from extensive pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: open-ended 3D environments, diverse tasks, requires meta-level generalization; pretraining over a very large distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires vast amounts of pretraining data from many environment/task instances; this is noted as a drawback.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>High: substantial pretraining compute (described qualitatively as 'vast amounts'); specifics not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Meta-learning formulation across distributions of tasks; discrete/continuous actions depending on environment; requires on-policy or offline meta-RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task adaptation speed and performance post-adaptation in 3D tasks (as reported in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper; authors note the capability but also limitations tied to pretraining distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited to the style of environments and problems seen during pretraining; high pretraining cost and limited generalization outside pretraining distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large-scale meta-pretraining enabling rapid within-episode policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned to contrast CLIN's frozen-model continual learning (no parameter updates) approach with heavy pretraining/meta-RL approaches like AdA.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2614.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do as I can, not as I say: Grounding language in robotic affordances (SayCan)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-conditioned robotic agent that grounds LLM-generated plans in low-level robot affordances and value functions to select executable actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can, not as i say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SayCan uses an LLM to propose high-level actions and combines that with learned affordance/value models for a physical robot to select feasible, high-value actions; the system grounds language suggestions in robot capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-grounded Robotic Planning Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Robotics / grounded language planning; referenced here as a generative language agent baseline for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Mapping language-level goals to executable robot actions using affordance modeling and LLM proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-high: bridging language planning and low-level robotic capabilities; requires affordance models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires affordance/value models trained for robot and environment; not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Inference-time LLM usage plus affordance model evaluation; no quantitative costs provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Structured mapping from high-level language actions to discrete/continuous robot actions with evaluation metrics for success.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task success/completion in embodied tasks; in this paper used for comparative evaluation on ScienceWorld (simulated) tasks via final reward.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In Table 1/2 comparisons on ScienceWorld-like tasks, SayCan reported overall ~32.9 average final reward (Table 1 All = 32.9) — lower than CLIN's BASE (48.6) and ADAPT results.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in detail here; in this paper SayCan performs better than some RL baselines but below CLIN for ScienceWorld tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Grounding LLM proposals in affordances gives executable action selection; leverages robot-specific value models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>SayCan outperforms some pure RL baselines on ScienceWorld tasks but is outperformed by CLIN, especially after CLIN's adaptation/meta-memory steps.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2614.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach combining chain-of-thought reasoning traces with action outputs in a prompt, allowing an LLM to interleave reasoning steps and actions when acting in environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ReAct augments LLM prompting so the model emits interleaved reasoning (chain-of-thought style) and concrete actions, enabling contextualized decision-making in interactive tasks; used here as a baseline generative-agent for ScienceWorld tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Generative Language Agent / Planning-and-Action Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Interactive text-based environments and action planning (ScienceWorld tasks in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate next actions for sequential tasks by leveraging explicit reasoning traces to guide choices among admissible actions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Comparable to other language-agent baselines; faces challenges with long-horizon tasks and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates in zero-shot or few-shot prompting regimes; no training required in CLIN comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference per action; numbers not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Discrete action templates plus objects, partial observability, textual observations; clear reward metrics from simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Final reward score from ScienceWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported overall average final reward across tasks = 29.6 (Table 2 All = 29.6), lower than Reflexion and CLIN BASE/ADAPT.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower performance on long-horizon and environment-variable tasks compared to CLIN; lacks persistent structured memory.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Interleaved reasoning traces with actions can help immediate planning and justification of steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>ReAct serves as the base agent for Reflexion; Reflexion improves over ReAct by per-trial reflection but still lags CLIN which leverages persistent causal memory.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2614.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SwiftSage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative agent combining a supervised agent with a deliberative (slow-thinking) agent to tackle complex interactive tasks; mentioned as a recent generative agent baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SwiftSage</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SwiftSage combines a fast supervised policy for immediate reactions and a slower deliberative LLM-based planner to handle complex long-horizon tasks, aiming to get the benefits of both approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hybrid Generative Agent (fast/slow thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Complex interactive tasks in simulated environments (e.g., interactive text worlds).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Leverage a supervised reactive component for routine decisions and an LLM deliberative component for planning to solve complex interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Designed for complex interactive tasks with long horizons; mixture of reactive and planning subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses supervised training data for the fast component (gold trajectories) plus LLM inference for deliberation; in CLIN comparisons SwiftSage had partial overlap for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Hybrid approach combining trained model inference and LLM inference; no numerical compute costs given here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Structured around splitting immediacy vs deliberation; uses gold trajectories for supervised component.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task performance on benchmarks (authors compare qualitatively/numerically in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In this paper CLIN outperforms TDT on all 18 tasks and SwiftSage on 8/18 (mostly long tasks); exact SwiftSage numbers not tabulated here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not elaborated in this paper; may rely on availability of gold trajectories for supervised component.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Combining fast reactive policy with slow deliberation can improve performance on some complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>CLIN, without gold trajectories, outperforms SwiftSage on many long tasks, indicating the value of CLIN's causal meta-memory approach for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2614.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Learning with a Natural Language Action Space (DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL method designed to operate in text-based environments by modeling state and natural-language action descriptions to learn Q-values over actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with a natural language action space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DRRN learns value functions over natural-language action candidates by embedding states and textual action descriptions and estimating action values with deep networks; applied to text-based game environments requiring action selection from language templates.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Reinforcement Learning for Text-based Environments</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Text-based interactive environments / decision-making with natural-language actions (ScienceWorld baseline here).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn policies to maximize sparse rewards in text-based tasks with large action vocabularies and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High sample complexity due to large discrete action space and sparse rewards; struggles with generalization across environment variants.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires many environment interactions and reward signals for training; in CLIN comparisons RL baselines were trained on many training variations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Substantial sample complexity / training compute (qualitative); exact compute not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Discrete action space with textual action templates; POMDPs with sparse reward.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Final reward after RL training; reported averages for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Table 1 reports DRRN overall (All) average = 16.7 final reward (much lower than CLIN and language-agent baselines). Per-task numbers vary widely (see table).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles with sparse textual feedback and large combinatorial action spaces; poor generalization to unseen environment variants when trained purely on numeric rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Can learn when trained extensively on many environment variants, but needs large interaction budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>DRRN (16.7) < KG-A2C (12.7) and << CLIN (48.6 BASE, 62.2 ADAPT) in ScienceWorld comparisons; generative language agents using NL feedback outperform RL baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2614.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C (Graph constrained reinforcement learning for natural language action spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning agent that incorporates graph constraints and A2C-style learning to handle natural-language action spaces in text environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KG-A2C constrains RL policies using knowledge-graph-like structure or constraints to reduce action search and employs A2C-style RL updates for learning in natural-language action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Reinforcement Learning with Graph Constraints</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Text-based interactive tasks with large action vocabularies (ScienceWorld baseline here).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn policies in POMDP-like text environments where actions are natural-language templates instantiated with objects.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High sample complexity; must manage combinatorial action templates and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires many environment interactions; trained on many environment variations in baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Qualitative: RL training compute; not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Discrete, partially observable, sparse-reward tasks with explicit action templates.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Final reward after training; reported in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Table 1 reports KG-A2C overall (All) average = 12.7 final reward (low compared to CLIN and generative LLM agents).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Poor performance with sparse numeric rewards and large action spaces; limited generalization relative to NL-feedback-based agents.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Graph constraints can reduce action search, but still requires substantial samples to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>KG-A2C underperforms generative language agents (SayCan, ReAct, CLIN) on ScienceWorld tasks in these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2614.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2614.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CALM (Keep calm and explore: Language models for action generation in text-based games)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-style approach leveraging language-model-based action generation and exploration strategies for text-based games; used as an RL baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep calm and explore: Language models for action generation in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CALM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CALM uses language models to propose actions while incorporating exploration strategies to handle text-based game environments; presented as a reinforcement-learning inspired baseline for action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Language-model-driven Action Generation / RL baseline</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Text-based interactive environments / games (ScienceWorld baseline here).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate actions in large natural-language action spaces and explore effectively in sparse-reward text games.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High sample complexity; sparse rewards and combinatorial action templates.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires environment interactions and rewards to learn/explore; in this paper it's a trained RL baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not quantified; involves LM inference and exploration/training cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Discrete action templates, partial observability, clear simulator rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Final reward after training; reported in table comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported overall (All) average final reward = 3.6 (Table 1), substantially lower than other baselines and CLIN on ScienceWorld tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Very low performance on these ScienceWorld tasks, likely due to sparse rewards and difficulty generalizing with purely RL-driven exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Language-model-based proposals may help generate candidate actions but require effective exploration strategy and reward shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>CALM performs worst among RL baselines reported; CLIN and generative-language agents outperform CALM by large margins.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Human-timescale adaptation in an open-ended task space <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with a natural language action space <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>ScienceWorld: Is your agent smarter than a 5th grader? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2614",
    "paper_id": "paper-767a7e949ba4520888e7442ee01e6a37c254fc53",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "CLIN",
            "name_full": "Continual Learning from INteractions (CLIN)",
            "brief_description": "A memory-augmented, language-agent architecture that uses a frozen LLM controller and executor plus a memory-generator to build and continually refine a persistent set of causal abstractions (in natural language) from trial interactions to improve zero-shot and adaptation performance in text-based simulated science tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CLIN",
            "system_description": "CLIN is an embodied language agent composed of (1) a persistent dynamic memory of causal-abstraction sentences, (2) a controller (frozen LLM) that selects and uses retrieved memory plus trial history to generate the next goal, (3) an executor (frozen LLM) that maps goals to admissible actions (with embedding-based correction and iterative refinement), and (4) a memory-generator (frozen LLM) that reflects on a completed trial and previous memories to produce updated causal abstractions and uncertainty annotations. Memory items use constrained templates (e.g., 'X SHOULD BE NECESSARY to Y', 'X DOES NOT CONTRIBUTE to Y') and a saliency-based pruning using final reward to keep crucial insights. CLIN uses meta-memory built from best trials across episodes to generalize to new environments/tasks.",
            "system_type": "Automated Discovery / Continual Learning Language Agent",
            "problem_domain": "Interactive scientific reasoning in simulated environments (text-based embodied science tasks: thermodynamics, genetics, chemistry, friction/force experiments, plant growth, etc.)",
            "problem_description": "CLIN attempts goal-directed sequential decision-making tasks in ScienceWorld: partially observable text-based POMDPs where the agent must plan and execute sequences of natural-language actions to achieve science goals (e.g., boil a liquid, freeze a substance, grow a plant, determine friction). Tasks vary in length (Short vs Long) and environment configuration (different room layouts, objects, working/broken devices).",
            "problem_complexity": "High: partially observable Markov decision processes with large discrete action/template spaces and many objects/states; sparse, episodic rewards tied to subgoals; long horizon variability (short tasks oracle &lt;37 steps, long &gt;=37). No explicit numeric dimensionality given but search/action space is described as intractable in many settings.",
            "data_availability": "Self-generated interaction data only (no gold trajectories). CLIN starts with empty memory and builds experience via repeated trials; ScienceWorld provides simulator observations and sparse textual feedback. The paper uses 164 task-environment combinations drawn from ScienceWorld test split.",
            "computational_requirements": "Qualitative: uses instruction-tuned LLMs (gpt-4) as frozen modules at runtime; no parameter updates required. No quantitative compute-hours or cost reported. Authors contrast CLIN with RL methods requiring large pretraining/sample requirements; CLIN's compute is dominated by LLM inference per trial and memory-generation calls.",
            "problem_structure": "Well-scoped sequential decision tasks but open-ended in environment variations; discrete action space (action templates + objects); stochasticity from partial observability and different env configs; clear evaluation metric (final reward from ScienceWorld); domain knowledge helpful (causal relations between actions and state transitions).",
            "success_metric": "Final reward score from ScienceWorld simulator (numeric 0–100 scale per task); also counts of tasks solved and number of steps to completion.",
            "success_rate": "As reported: BASE (zero-shot) average final reward across 164 task-env combos = 48.6; ADAPT (continual learning within same env across up to 5 trials) average = 62.2 (absolute improvement +13.6 points over BASE). GEN-ENV (meta-memory for new environment, zero-shot on new env) average = 52.7 (+4.1 over BASE). GEN-ADAPT (start with meta-memory then continue adaptation) average = 69.5 (CLIN beats Reflexion by 23 absolute points in G+A). For GEN-TASK transfer to new tasks, CLIN reports a 13-point zero-shot improvement over BASE and further gains via adaptation (example: CLIN improves 13 points for new tasks, and additional 7 points with adaptation in some experiments).",
            "failure_modes": "Dependence on self-exploration: if CLIN never encounters critical locations/actions, it cannot generate related insights. Poor memory retrieval: relevant meta-memory items can be overshadowed by misleading but higher-retrieved items (e.g., retrieving 'stove necessary to boil' when stove is broken). Limited exploration early in training due to large action space.",
            "success_factors": "Structured causal-abstraction memory templates (necessary / does not contribute) that capture actionable causal knowledge; persistent memory across trials and saliency pruning based on reward; meta-memory synthesized from best trials to enable transfer; use of frozen LLMs avoids costly parameter updates.",
            "comparative_results": "CLIN outperforms generative-agent baselines and RL baselines across setups: in ADAPT CLIN (ADAPT=62.2) &gt; Reflexion (39.4) and ReAct (29.6); in GEN-ENV CLIN (GEN-ENV=52.7) &gt; SayCan (32.9), ReAct (29.6), and RL methods (DRRN 16.7, KG-A2C 12.7, CALM 3.6). CLIN shows larger relative gains on longer tasks and when using meta-memory plus adaptation (G+A=69.5).",
            "human_baseline": "Not reported for ScienceWorld tasks in this paper; no quantitative human-expert baselines provided.",
            "uuid": "e2614.0",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A reflective language-agent that generates trial-specific natural-language reflections on failed attempts and uses those reflections to re-plan and retry tasks; reflections are focused on the current trial and are not maintained as a long-term persistent memory across episodes.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "use",
            "system_name": "Reflexion",
            "system_description": "Reflexion prompts a frozen LLM to reflect on its recent failed attempt to produce language-based insights and then integrates those reflections into a retry prompt to attempt the task again. It does not maintain a long-term persistent, structured memory across episodes—its insights are typically trial- and environment-specific.",
            "system_type": "Reflective Language Agent / Automated Retry System",
            "problem_domain": "Interactive decision-making and embodied agents in text-based environments (same ScienceWorld domain used for comparison).",
            "problem_description": "Improving task success by reflecting on a failed attempt to generate a modified plan for immediate retry in the same episode.",
            "problem_complexity": "Applied to the same POMDP-style text tasks; complexity similar to CLIN but reflexion focuses on per-trial reflection rather than episodic memory accumulation.",
            "data_availability": "Operates on the single-trial trace (no long-term archive). Uses observations and outcomes from the failed trial as input for reflection.",
            "computational_requirements": "Uses frozen LLM inference for reflections and planning; no parameter updates. No numeric compute reported.",
            "problem_structure": "Focuses on immediate retry; structured as single-episode reflection → modified plan; discrete action space; clear reward feedback per trial.",
            "success_metric": "Final reward score from ScienceWorld as used in comparisons.",
            "success_rate": "Reported overall (Table 2) average final reward across tasks = 39.4 (compared to CLIN BASE 48.6 and CLIN ADAPT 62.2). Per-task numbers vary (e.g., some short tasks show higher values on particular variants).",
            "failure_modes": "Reflections tend to be highly task- and environment-specific (e.g., 'go to desk 1 and find the lamp'), which can limit generalization; lacks long-term persistent memory, reducing benefit across episodes; may only learn from mistakes and not accumulate positive causal abstractions.",
            "success_factors": "Can generate trial-specific corrective insights quickly without parameter tuning; benefits when per-trial reflection yields actionable corrections.",
            "comparative_results": "Compared against CLIN, Reflexion shows smaller adaptation gains; CLIN's persistent causal memory yields larger improvements across trials and generalization (CLIN ADAPT +23 points over Reflexion in G+A). Reflexion does better than ReAct baseline in some per-task numbers but is overall weaker than CLIN.",
            "human_baseline": "No human baseline reported.",
            "uuid": "e2614.1",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager",
            "brief_description": "An LLM-based open-ended embodied agent operating in Minecraft that grows a code-based skill library from failure/success feedback to enable increasingly complex behaviors.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "system_name": "Voyager",
            "system_description": "Voyager interacts in Minecraft, uses LLMs to generate code-based skills from experience and feedback, autonomously builds a skill library (programs/scripts) to expand capabilities over time, and uses these skills to accomplish more complex open-ended goals.",
            "system_type": "Open-ended Embodied Agent / Skill-acquisition System",
            "problem_domain": "Open-ended game world (Minecraft) — embodied agent skill acquisition and long-horizon task performance.",
            "problem_description": "Autonomously explore, discover, and codify reusable skills from interactions in Minecraft to tackle increasingly complex tasks in an open-ended environment.",
            "problem_complexity": "High: open-ended with very large state/action space, hierarchical tasks, and need for programmatic skill composition; requires handling long horizons and complex dependencies.",
            "data_availability": "Self-generated interactive data from Minecraft; rich environment observations; no gold trajectories required.",
            "computational_requirements": "Not quantified in this paper; literature notes large-scale LLM inference and environment simulation; Voyager relies on code-synthesis plus execution in the environment.",
            "problem_structure": "Open-ended, continuous development of skills; not a fixed POMDP; lacks a single well-defined reward across all tasks.",
            "success_metric": "Task completion and emergent skill repertoire growth in Minecraft as reported in Voyager paper (not quantified here).",
            "success_rate": "Not reported in this paper; referenced as prior work demonstrating skill growth but with no numeric comparison in this paper.",
            "failure_modes": "Not detailed here; general limits include reliance on environment-specific affordances and potential brittleness of synthesized skills.",
            "success_factors": "Rich environment feedback in Minecraft and programmatic skill representation enabling reuse; code-based skills capture reusable behavior.",
            "comparative_results": "Mentioned as related work that demonstrates building skill libraries from failures, inspiring CLIN's memory approach; not directly compared experimentally in this paper.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2614.2",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AdA",
            "name_full": "AdA (Adaptive-Agent-Team human-timescale adaptation)",
            "brief_description": "An agent demonstrating rapid adaptation to open-ended novel 3D problems using meta-reinforcement learning, but requiring extensive pretraining; mentioned as related work on fast adaptation.",
            "citation_title": "Human-timescale adaptation in an open-ended task space",
            "mention_or_use": "mention",
            "system_name": "AdA",
            "system_description": "A meta-reinforcement-learning based agent that adapts policy on the fly to new 3D environments/problem instances by leveraging pretraining across many tasks, enabling rapid within-episode adaptation.",
            "system_type": "Meta-Reinforcement Learning Adaptive Agent",
            "problem_domain": "Open-ended 3D embodied tasks (rapid adaptation to novel problems in 3D simulated environments).",
            "problem_description": "Adapt policies quickly to previously unseen task variants in 3D worlds by exploiting meta-learned priors from extensive pretraining.",
            "problem_complexity": "High: open-ended 3D environments, diverse tasks, requires meta-level generalization; pretraining over a very large distribution.",
            "data_availability": "Requires vast amounts of pretraining data from many environment/task instances; this is noted as a drawback.",
            "computational_requirements": "High: substantial pretraining compute (described qualitatively as 'vast amounts'); specifics not given in this paper.",
            "problem_structure": "Meta-learning formulation across distributions of tasks; discrete/continuous actions depending on environment; requires on-policy or offline meta-RL training.",
            "success_metric": "Task adaptation speed and performance post-adaptation in 3D tasks (as reported in referenced work).",
            "success_rate": "Not quantified in this paper; authors note the capability but also limitations tied to pretraining distributions.",
            "failure_modes": "Limited to the style of environments and problems seen during pretraining; high pretraining cost and limited generalization outside pretraining distribution.",
            "success_factors": "Large-scale meta-pretraining enabling rapid within-episode policy updates.",
            "comparative_results": "Mentioned to contrast CLIN's frozen-model continual learning (no parameter updates) approach with heavy pretraining/meta-RL approaches like AdA.",
            "human_baseline": "Not provided here.",
            "uuid": "e2614.3",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SayCan",
            "name_full": "Do as I can, not as I say: Grounding language in robotic affordances (SayCan)",
            "brief_description": "A language-conditioned robotic agent that grounds LLM-generated plans in low-level robot affordances and value functions to select executable actions.",
            "citation_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "mention_or_use": "use",
            "system_name": "SayCan",
            "system_description": "SayCan uses an LLM to propose high-level actions and combines that with learned affordance/value models for a physical robot to select feasible, high-value actions; the system grounds language suggestions in robot capabilities.",
            "system_type": "LLM-grounded Robotic Planning Agent",
            "problem_domain": "Robotics / grounded language planning; referenced here as a generative language agent baseline for embodied tasks.",
            "problem_description": "Mapping language-level goals to executable robot actions using affordance modeling and LLM proposals.",
            "problem_complexity": "Moderate-high: bridging language planning and low-level robotic capabilities; requires affordance models.",
            "data_availability": "Requires affordance/value models trained for robot and environment; not detailed here.",
            "computational_requirements": "Inference-time LLM usage plus affordance model evaluation; no quantitative costs provided.",
            "problem_structure": "Structured mapping from high-level language actions to discrete/continuous robot actions with evaluation metrics for success.",
            "success_metric": "Task success/completion in embodied tasks; in this paper used for comparative evaluation on ScienceWorld (simulated) tasks via final reward.",
            "success_rate": "In Table 1/2 comparisons on ScienceWorld-like tasks, SayCan reported overall ~32.9 average final reward (Table 1 All = 32.9) — lower than CLIN's BASE (48.6) and ADAPT results.",
            "failure_modes": "Not discussed in detail here; in this paper SayCan performs better than some RL baselines but below CLIN for ScienceWorld tasks.",
            "success_factors": "Grounding LLM proposals in affordances gives executable action selection; leverages robot-specific value models.",
            "comparative_results": "SayCan outperforms some pure RL baselines on ScienceWorld tasks but is outperformed by CLIN, especially after CLIN's adaptation/meta-memory steps.",
            "human_baseline": "Not provided.",
            "uuid": "e2614.4",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "An approach combining chain-of-thought reasoning traces with action outputs in a prompt, allowing an LLM to interleave reasoning steps and actions when acting in environments.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "system_name": "ReAct",
            "system_description": "ReAct augments LLM prompting so the model emits interleaved reasoning (chain-of-thought style) and concrete actions, enabling contextualized decision-making in interactive tasks; used here as a baseline generative-agent for ScienceWorld tasks.",
            "system_type": "Generative Language Agent / Planning-and-Action Prompting",
            "problem_domain": "Interactive text-based environments and action planning (ScienceWorld tasks in this paper).",
            "problem_description": "Generate next actions for sequential tasks by leveraging explicit reasoning traces to guide choices among admissible actions.",
            "problem_complexity": "Comparable to other language-agent baselines; faces challenges with long-horizon tasks and sparse rewards.",
            "data_availability": "Operates in zero-shot or few-shot prompting regimes; no training required in CLIN comparisons.",
            "computational_requirements": "LLM inference per action; numbers not specified.",
            "problem_structure": "Discrete action templates plus objects, partial observability, textual observations; clear reward metrics from simulator.",
            "success_metric": "Final reward score from ScienceWorld.",
            "success_rate": "Reported overall average final reward across tasks = 29.6 (Table 2 All = 29.6), lower than Reflexion and CLIN BASE/ADAPT.",
            "failure_modes": "Lower performance on long-horizon and environment-variable tasks compared to CLIN; lacks persistent structured memory.",
            "success_factors": "Interleaved reasoning traces with actions can help immediate planning and justification of steps.",
            "comparative_results": "ReAct serves as the base agent for Reflexion; Reflexion improves over ReAct by per-trial reflection but still lags CLIN which leverages persistent causal memory.",
            "human_baseline": "Not provided.",
            "uuid": "e2614.5",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SwiftSage",
            "name_full": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "brief_description": "A generative agent combining a supervised agent with a deliberative (slow-thinking) agent to tackle complex interactive tasks; mentioned as a recent generative agent baseline.",
            "citation_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "mention_or_use": "mention",
            "system_name": "SwiftSage",
            "system_description": "SwiftSage combines a fast supervised policy for immediate reactions and a slower deliberative LLM-based planner to handle complex long-horizon tasks, aiming to get the benefits of both approaches.",
            "system_type": "Hybrid Generative Agent (fast/slow thinking)",
            "problem_domain": "Complex interactive tasks in simulated environments (e.g., interactive text worlds).",
            "problem_description": "Leverage a supervised reactive component for routine decisions and an LLM deliberative component for planning to solve complex interactive tasks.",
            "problem_complexity": "Designed for complex interactive tasks with long horizons; mixture of reactive and planning subproblems.",
            "data_availability": "Uses supervised training data for the fast component (gold trajectories) plus LLM inference for deliberation; in CLIN comparisons SwiftSage had partial overlap for some tasks.",
            "computational_requirements": "Hybrid approach combining trained model inference and LLM inference; no numerical compute costs given here.",
            "problem_structure": "Structured around splitting immediacy vs deliberation; uses gold trajectories for supervised component.",
            "success_metric": "Task performance on benchmarks (authors compare qualitatively/numerically in related work).",
            "success_rate": "In this paper CLIN outperforms TDT on all 18 tasks and SwiftSage on 8/18 (mostly long tasks); exact SwiftSage numbers not tabulated here.",
            "failure_modes": "Not elaborated in this paper; may rely on availability of gold trajectories for supervised component.",
            "success_factors": "Combining fast reactive policy with slow deliberation can improve performance on some complex tasks.",
            "comparative_results": "CLIN, without gold trajectories, outperforms SwiftSage on many long tasks, indicating the value of CLIN's causal meta-memory approach for generalization.",
            "human_baseline": "Not provided.",
            "uuid": "e2614.6",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Learning with a Natural Language Action Space (DRRN)",
            "brief_description": "An RL method designed to operate in text-based environments by modeling state and natural-language action descriptions to learn Q-values over actions.",
            "citation_title": "Deep reinforcement learning with a natural language action space",
            "mention_or_use": "use",
            "system_name": "DRRN",
            "system_description": "DRRN learns value functions over natural-language action candidates by embedding states and textual action descriptions and estimating action values with deep networks; applied to text-based game environments requiring action selection from language templates.",
            "system_type": "Reinforcement Learning for Text-based Environments",
            "problem_domain": "Text-based interactive environments / decision-making with natural-language actions (ScienceWorld baseline here).",
            "problem_description": "Learn policies to maximize sparse rewards in text-based tasks with large action vocabularies and partial observability.",
            "problem_complexity": "High sample complexity due to large discrete action space and sparse rewards; struggles with generalization across environment variants.",
            "data_availability": "Requires many environment interactions and reward signals for training; in CLIN comparisons RL baselines were trained on many training variations.",
            "computational_requirements": "Substantial sample complexity / training compute (qualitative); exact compute not provided.",
            "problem_structure": "Discrete action space with textual action templates; POMDPs with sparse reward.",
            "success_metric": "Final reward after RL training; reported averages for comparison.",
            "success_rate": "Table 1 reports DRRN overall (All) average = 16.7 final reward (much lower than CLIN and language-agent baselines). Per-task numbers vary widely (see table).",
            "failure_modes": "Struggles with sparse textual feedback and large combinatorial action spaces; poor generalization to unseen environment variants when trained purely on numeric rewards.",
            "success_factors": "Can learn when trained extensively on many environment variants, but needs large interaction budgets.",
            "comparative_results": "DRRN (16.7) &lt; KG-A2C (12.7) and &lt;&lt; CLIN (48.6 BASE, 62.2 ADAPT) in ScienceWorld comparisons; generative language agents using NL feedback outperform RL baselines.",
            "human_baseline": "Not given.",
            "uuid": "e2614.7",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "KG-A2C (Graph constrained reinforcement learning for natural language action spaces)",
            "brief_description": "A reinforcement learning agent that incorporates graph constraints and A2C-style learning to handle natural-language action spaces in text environments.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "use",
            "system_name": "KG-A2C",
            "system_description": "KG-A2C constrains RL policies using knowledge-graph-like structure or constraints to reduce action search and employs A2C-style RL updates for learning in natural-language action spaces.",
            "system_type": "Reinforcement Learning with Graph Constraints",
            "problem_domain": "Text-based interactive tasks with large action vocabularies (ScienceWorld baseline here).",
            "problem_description": "Learn policies in POMDP-like text environments where actions are natural-language templates instantiated with objects.",
            "problem_complexity": "High sample complexity; must manage combinatorial action templates and sparse rewards.",
            "data_availability": "Requires many environment interactions; trained on many environment variations in baselines.",
            "computational_requirements": "Qualitative: RL training compute; not quantified here.",
            "problem_structure": "Discrete, partially observable, sparse-reward tasks with explicit action templates.",
            "success_metric": "Final reward after training; reported in comparisons.",
            "success_rate": "Table 1 reports KG-A2C overall (All) average = 12.7 final reward (low compared to CLIN and generative LLM agents).",
            "failure_modes": "Poor performance with sparse numeric rewards and large action spaces; limited generalization relative to NL-feedback-based agents.",
            "success_factors": "Graph constraints can reduce action search, but still requires substantial samples to learn.",
            "comparative_results": "KG-A2C underperforms generative language agents (SayCan, ReAct, CLIN) on ScienceWorld tasks in these comparisons.",
            "human_baseline": "Not given.",
            "uuid": "e2614.8",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CALM",
            "name_full": "CALM (Keep calm and explore: Language models for action generation in text-based games)",
            "brief_description": "An RL-style approach leveraging language-model-based action generation and exploration strategies for text-based games; used as an RL baseline in comparisons.",
            "citation_title": "Keep calm and explore: Language models for action generation in text-based games",
            "mention_or_use": "use",
            "system_name": "CALM",
            "system_description": "CALM uses language models to propose actions while incorporating exploration strategies to handle text-based game environments; presented as a reinforcement-learning inspired baseline for action generation.",
            "system_type": "Language-model-driven Action Generation / RL baseline",
            "problem_domain": "Text-based interactive environments / games (ScienceWorld baseline here).",
            "problem_description": "Generate actions in large natural-language action spaces and explore effectively in sparse-reward text games.",
            "problem_complexity": "High sample complexity; sparse rewards and combinatorial action templates.",
            "data_availability": "Requires environment interactions and rewards to learn/explore; in this paper it's a trained RL baseline.",
            "computational_requirements": "Not quantified; involves LM inference and exploration/training cycles.",
            "problem_structure": "Discrete action templates, partial observability, clear simulator rewards.",
            "success_metric": "Final reward after training; reported in table comparisons.",
            "success_rate": "Reported overall (All) average final reward = 3.6 (Table 1), substantially lower than other baselines and CLIN on ScienceWorld tasks.",
            "failure_modes": "Very low performance on these ScienceWorld tasks, likely due to sparse rewards and difficulty generalizing with purely RL-driven exploration.",
            "success_factors": "Language-model-based proposals may help generate candidate actions but require effective exploration strategy and reward shaping.",
            "comparative_results": "CALM performs worst among RL baselines reported; CLIN and generative-language agents outperform CALM by large margins.",
            "human_baseline": "Not provided.",
            "uuid": "e2614.9",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Human-timescale adaptation in an open-ended task space",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning with a natural language action space",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games",
            "rating": 2
        },
        {
            "paper_title": "ScienceWorld: Is your agent smarter than a 5th grader?",
            "rating": 2
        }
    ],
    "cost": 0.02116975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CLIN: A CONTINUALLY LEARNING LANGUAGE AGENT FOR RAPID TASK ADAPTATION AND GENERALIZATION</h1>
<p>Bodhisattwa Prasad Majumder ${ }^{1}$, Bhavana Dalvi Mishra ${ }^{1}$, Peter Jansen ${ }^{1,2}$, Oyvind Tafjord ${ }^{1}$, Niket Tandon ${ }^{1}$, Li Zhang ${ }^{3}$, Chris-Callison Burch ${ }^{3}$, Peter Clark ${ }^{1}$<br>${ }^{1}$ Allen Institute of AI<br>${ }^{2}$ University of Arizona<br>${ }^{3}$ University of Pennsylvania<br>Contact: {bodhisattwam, bhavanad}@allenai.org<br>Project page: https://allenai.github.io/clin/</p>
<h4>Abstract</h4>
<p>Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time, beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory, centered on causal abstractions (rather than general "helpful hints"), that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points ( 13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points ( 7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have been increasingly used to interact with external environments (e.g., simulated worlds) as goal-driven agents (Reed et al., 2022). However, it has been challenging for these language agents to efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning (Chen et al., 2021; Ammanabrolu et al., 2020). More recently, new techniques have appeared in which an agent reflects on its own past experience solving a task in a particular environment, and generates languagebased insights to help it retry the task, e.g., Reflexion (Shinn et al., 2023). Such methods have the advantage of not requiring parameter updates (particularly useful given the growing popularity of frozen language models). However, the style of such insights plays a crucial role in performance, and not all insights improve generalization performance. For example, a specific insight such as "In the next trial, I will go to desk 1 and find the lamp" (Shinn et al., 2023) may have limited value (or even hurt) for a different environment or task.</p>
<p>Our goal is a system that will continually improve over time, both while attempting the same task in the same environment, and across different tasks and environments. Our approach builds on prior work on reflection in two ways: First, we conjecture that a specific style of insight will be useful, namely one that captures causal abstractions about agent's actions, e.g., "opening doors</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: CLIN creates (Trial1) or adapts (Trial2+) a memory of causal abstractions to help in future trials by reflecting on the last trial and current memory. It does this using a suitably prompted LLM to generate the updated memory (Section 3.2). Here, reflecting on Trial1, CLIN notes in memory that going to the kitchen helped with finding seeds, enabling it to find the seeds faster in Trial2. From there, it also learns that moving the seeds to the pot helped plant the seeds. To further generalize across episodes (sequences of trials, right figure) for use in new environments, CLIN generates a summary ("meta-memory") of the best (starred) memories from each prior episode, here generating the generalization that moving to different rooms helps finding objects (Section 3.3)
may be necessary for movement between rooms". Causal abstractions can potentially help the agent decide which action to take in the future, and can be viewed as a kind of action model learning (Arora et al., 2018), but placed in the modern context of language models. Second, we maintain these abstractions in a continually evolving, dynamic memory, which is regularly updated as the agent gains experience, allowing useful causal knowledge to persist (and unhelpful knowledge to be dropped) over time and between tasks and environments, as illustrated in Figure 1.</p>
<p>We operationalize and evaluate this approach in a memory-augmented language agent called CLIN (continual learning from interactions). CLIN is an agent that operates in ScienceWorld (Wang et al., 2022), a virtual, text-based environment in which an agent is tasked with science-based goals, e.g., boiling a liquid, growing a plant. We find that CLIN is able to rapidly learn about the environment and its action vocabulary and continually improve on repeated trials on the same task and environment, outperforming state-of-the-art (SOTA) reflective language agents like Reflexion by 23 points. CLIN can also transfer its learning to new environments (or tasks), improving its zero-shot performance by 4 (13 for new tasks) points and can further improve performance through continual memory updates, enhancing performance by an additional 17 ( 7 for new tasks) points. Our contributions are as follows:</p>
<ul>
<li>For memory-based language agents, we show that memory of causal abstractions is effective at helping the agents learn over an extended period and in varying conditions.</li>
<li>We describe and evaluate CLIN, an architecture for a novel nonparametric learning paradigm. We show that CLIN learns faster than prior systems and generalizes better to new tasks and new environments, achieving state-of-the-art.</li>
</ul>
<p>Overall, this work suggests that a dynamically maintained memory, centered around causal knowledge, is a promising way forward for agents built on frozen models to continually improve over time.</p>
<h1>2 Related Work</h1>
<p>There is a long literature of work on agents that can navigate complex environments. A common approach is to use reinforcement learning (RL), e.g., DRRN (He et al., 2015), KG-A2C (Ammanabrolu \&amp; Hausknecht, 2020), CALM (Yao et al., 2020), where agents learn a task over repeated trials. However, while effective, such agents typically require a large number of trials to learn and have trouble adapting to unexpected changes in the test environment. More recently, (Adaptive-AgentTeam et al., 2023) demonstrated AdA, an agent that could rapidly adapt to open-ended novel 3D problems, using meta-reinforcement learning, essentially being able to change its policy on the fly.</p>
<p>However, AdA required vast amounts of pretraining, and this skill was still limited to the style of environments and problems seen in pretraining.</p>
<p>Recently, LLMs have provided a new tool for building goal-directed agents (Huang et al., 2022). Given a linguistic description of the world state, a task, and a history, the LLM can be prompted to suggest next actions to take to achieve a goal, exploiting their wealth of semantic knowledge about the world and requiring little training, e.g., SayCan (Ahn et al., 2022), ReAct (Yao et al., 2022), and more recently SwiftSage (Lin et al., 2023), which combines a supervised agent and a deliberative agent together. However, while performing reasonably with little training data, such agents are unable to learn and adapt from experience.</p>
<p>Two recent systems have demonstrated how a frozen-model-based agent could improve at a task. Voyager (Wang et al., 2023) operates in the world of Minecraft, growing a (code-based) skill library from rich feedback of its failures. Reflexion (Shinn et al., 2023) improves at a task by reflecting on a failed attempt at that task and devise a new plan that accounted for that mistake, used in the subsequent prompt to retry the task. While Reflexion did not have a long-term memory, and its reflections were task- and environment-specific, e.g., "In the next trial, I will go to desk 1 and find the lamp.", we take inspiration from it to build an agent, CLIN, which continually maintains and adapts a long-term, persistent memory of reflections, useful across different trials, tasks, and environments.</p>
<p>More generally, others have found that a memory of useful learnings can be used to improve frozen LLM behavior, e.g., in QA (Dalvi et al., 2022; Tandon et al., 2022; Madaan et al., 2023), or for modeling social behavior (Park et al., 2023). We apply this finding to goal-directed agents.</p>
<p>Finally, we note that the content of experiential memory is also important. Specifically, CLIN learns a memory of causal abstractions, which can be seen as learning a linguistic form of action model, describing the causal effects of actions. While there has been substantial work in the planning community of learning action models in a fully formalized context (Arora et al., 2018; Aineto et al., 2018), CLIN loosely applies this idea in the linguistic world of LLM agents.</p>
<h1>3 APPROACH</h1>
<p>Problem Formulation. Sequential decision-making tasks require agents to repeatedly observe and then act in an environment until they accomplish a specific goal. At a high level, this can be accomplished by developing beliefs about the world, acting on the environment based on those beliefs, and then updating one's beliefs based on the observed outcome. Here, we investigate constructing an agent that can continually update its beliefs through interaction and observation while exploiting its past experience toward solving unseen parametric variations of tasks.</p>
<p>Setup. We investigate our continual learning agents in simulated environments. Our environments are modeled in a high-fidelity text-based simulator (Wang et al., 2022), where both actions and observations are expressed in natural language. Let's define the task space to be $\mathcal{M}$, a collection of partially observable Markov Decision Processes (POMDPs) that can be executed in a set of environment configurations $\mathcal{E}$. Each task $m \in \mathcal{M}$ has an initial state and a desired winning state, which vary depending on the environment $e \in \mathcal{E}$.</p>
<p>Our setup allows an agent to attempt a task several times; each time is denoted as a trial, $\mathcal{T}$, which consists of a total of $\tau$ steps. Each step comprises an action by the agent (a), and in response, the simulator returns the result of that action in the form of an observation (o) and a reward ( $r$ ). A collection of $K$ trials is called an episode. The environment gets reset when the task reaches an end state (such as completing, failing, or timing out). In our continual learning setup, the agent retains its memory across trials/episodes, reaping the benefits of continued interaction with the environment.</p>
<h3>3.1 CLIN: A GENERATIVE AGENT CONTINUALLY LEARNING FROM INTERACTIONS</h3>
<p>To act in the world, CLIN uses three modules: a memory, a controller, and an executor, illustrated in Figure 2 and which we now describe. Learning then occurs using a fourth module, a memory generator, to generate an updated memory after each trial.</p>
<p>Memory. CLIN's memory $(\mathcal{S})$ is a persistent, dynamic collection of NL sentences that express causal abstractions, generated by reflecting on past experiences in order to help the agent perform</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The architecture of CLIN. A controller takes the current task, retrievals from memory, and the trial so far, to generate the next goal to achieve. The executor then converts this to a valid action to perform towards that goal. The simulator then performs the action and returns an observation of that action's effect. Memory is updated at the end of each trial by the memory generator (Section 3.2).
better in the future. This generation process is described shortly in Section 3.2. For example, having an explicit causal insight, "opening the fridge is necessary to access apple juice", learned from past experiences, can reduce the action search space for CLIN while looking for "apple juice" in the same environment in future trials. To aid in continual learning, the memory also captures mistakes made by the agent in previous trials, similar to reflective agents explored in recent work (Shinn et al., 2023; Park et al., 2023), noting actions that failed to contribute to a task.</p>
<p>Controller. The role of the controller is to generate the next goal to pursue in service of the task. In CLIN it is a frozen LLM, whose prompt includes the current task $m$, e.g., "convert water into steam", retrievals from the current memory $\mathcal{S}$, and the trial so far (the sequence of goal-action-observation triples $\left{g_{1}, a_{1}, o_{1}, \ldots, g_{t}, a_{t}, o_{t}\right}$ ), and is prompted to output the next goal $g_{t+1}$ to pursue, e.g., "find water". Memory items are retrieved using both the task instruction and the trial history. The controller first selects one or more memory items given the current state and if they are useful for the next action to progress in the task. After that, it appends the learning, if selected, in context to generate a goal, otherwise the goal is generated based on the trial history (see full prompt in Figure 6).
Executor. The role of the executor is to convert the generated goal $g_{t+1}$ into a valid action $a_{t+1}$ that can be executed in the environment in pursuit of that goal. Again a (frozen) LLM is used, whose prompt includes the goal $g_{t+1}$, the trial so far, and all the possible actions that can be performed in the current state (provided by the simulator, as is standard practice in current generative agent research (Ahn et al., 2022; Yao et al., 2022; Lin et al., 2023; Park et al., 2023)). The list of possible actions is expressed as possible action templates and available objects that can instantiate them, rather than a combinatorially large enumeration of possible actions. The model is then prompted to generate a candidate action to perform (see prompt in Figure 6). Finally, CLIN checks this candidate action is one of the valid actions. If it is not, it finds the most similar valid action using the pre-trained embeddings from the sentence-transformer model (Reimers \&amp; Gurevych, 2019). If the top-ranked valid action has a similarity score greater than a threshold (here, 0.9 , chosen as a hyperparameter), the action is selected. Otherwise, we perform iterative refinement (Madaan et al., 2023) by suffixing the context with feedback that the generated candidate action is not executable. This allows the executor to retry the generation for up to a maximum number of tries (here, 5).
Finally, upon executing the action $a_{t+1}$, CLIN receives a partial next state, as an observation, from the simulator and the reward $(r) \in[0,1]$. Rewards are nominally given by the simulator for achieving either major task subgoals (e.g., finding water, for the boiling task), or minor and optional subgoals (e.g., being in the kitchen, for the boiling task). Rewards are sparse and generally only supplied after the completion of a task subgoal. A snapshot of a full trial is given in lines 4-10 in Algorithm 1.
Note that CLIN does not make use of any gold data to identify goals and memories. Rather, we expect CLIN to perform a balanced act of exploration-exploitation by interacting, learning, and adapting to unseen tasks or environment configurations-a key difference from few-shot generative agents by previous work (Ahn et al., 2022; Yao et al., 2022; Lin et al., 2023; Park et al., 2023).</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Continual Learning with CLIN
    procedure ADAPTATION(Task: \(m\), Env: \(e\), Mem-
        ory: \(\mathcal{S}\) ):
        Initialize Memory: \(\mathcal{S}_{0}\)
        for \(k \in 1, \cdots, K\) do:
            Intialize Trial \(\mathcal{T}, t\)
            while \(t&lt;\) max. steps or task not complete do:
                \(g_{t}=\operatorname{Controller}\left(m, e, \mathcal{T}_{&lt;t}, \mathcal{S}_{k-1}\right)\)
                \(a_{t}=\) Executor ( \(g_{t}\), admissible actions)
                \(r_{t}, o_{t}=\operatorname{Simulator}\left(\mathcal{T}_{&lt;t}, a_{t}\right)\)
                \(\mathcal{T}_{&lt;t+1}=\mathcal{T}_{&lt;t}+\left(g_{t}, a_{t}, o_{t}, r_{t}\right)\)
            Final reward \(r_{k}=r_{t}\)
            \(\mathcal{S}_{k}=\) memory-generator \(\left(\left\{\mathcal{S}_{&lt;k}\right\}, \mathcal{T}_{k}, r_{k}\right)\)
        procedure Generalization(Task: \(m\), Env: \(e\),
            past \(\left.m^{\prime} / e^{\prime}\right)\)
            \(\left\{\mathcal{S}_{\text {crucial }}, r_{k}\right\}=\) crucial-memories (past \(\left.m^{\prime} / e^{\prime}\right)\)
            \(\mathcal{S}_{\text {meta }}=\operatorname{meta-memory}\left(\left\{\mathcal{S}_{\text {crucial }}, r_{k}\right\}, m\right)\)
            ADAPTATION \(\left(m, e, \mathcal{S}_{\text {meta }}\right)\)
</code></pre></div>

<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (LEFT) CLIN's continual learning algorithm. (RIGHT) Example causal abstractions.</p>
<h1>3.2 MEMORY: A COLLECTION OF ADAPTIVE CAUSAL ABSTRACTIONS</h1>
<p>At the end of each trial (completion or failure), CLIN uses a memory generator to create or update its memory. The memory generator is a (frozen) LLM prompted to reflect on the current trial and memory, and generate a new memory of insights in the form of (English sentences expressing) useful causal abstractions, as we now describe.
Learning about state transitions is essential for sequential decision-making tasks (Mnih et al., 2013), which can be manifested by knowing 1) actions enabling desired state transitions, 2) actions producing undesired or no change in states, and 3) state transitions that contribute to the task progress. To generate these kinds of knowledge, the generator is prompted to generate insights in a particular syntax (see prompt in Figure 7). To capture good actions enabling desired changes and helpful state transitions, we use the template " X is NECESSARY to Y ", and to capture contrastive examples of unsuitable actions and state transitions, we employ "X DOES NOT CONTRIBUTE to Y", as depicted in Section 4.3, where X, Y are related to actions. These abstractions are functionally analogous to hindsight experience replay (Andrychowicz et al., 2017), obtained from CLIN's past self-explorations.
While useful insights can be abstracted from the trials, CLIN's exploration can be limited, especially in the early trials, given an incredibly large action space. Hence, incomplete exploration can pose varying degrees of uncertainty on extracted insights. To capture this, we also include a measure of uncertainty in each abstraction by either of the two linguistic variations in their surface forms: " X may . . " to denote moderate to high uncertainty, and " $X$ should . . ." to indicate low uncertainty (See Section 4.3). In the course of continual learning, as CLIN gathers experience, we expect the level of uncertainty to change depending on the frequency of their use and their fitness to the task.</p>
<p>Updating Memory Across Trials. CLIN continually attempts to solve a task in an environment for multiple trials (in sum, an episode). To update the memory after each trial within an episode, the memory generator is prompted with the most recent trial (a sequence of $\left(g_{t}, a_{t}, o_{t}\right)$ tuples and the final reward $\left.r_{k}{ }^{1}\right)$, and the memories from the three most recent trials $\left{\mathcal{S}<em k-1="k-1">{k-2}, \mathcal{S}</em>}, \mathcal{S<em k_1="k+1">{k}\right}$. It is then prompted to generate an updated memory $\mathcal{S}</em>$, namely a new list of semi-structured causal abstractions in the forms described above, for use in the next trial. Although we do not specify a maximum size for the memory, we observe that size of the generated memory (i.e., the number of causal abstractions generated) is far less than the number of actions executed in the trial, indicating</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the memory-generator additionally performs a saliency-based pruning to keep only important insights based on the success of the trial, as indicated by the final reward $r_{k}$ at the end of the trial $\mathcal{T}_{k}$.</p>
<h1>3.3 Meta-Memory for Better Generalization</h1>
<p>Updating memory based on past insights and the current trial to influence future trials for the same task in the current environment configuration during test-time adaptation. However, to generalize across tasks or environment configurations, the memory needs to contain more generalized causal abstractions than memories used across trials in an episode. We call this as meta-memory, abstracted across multiple episodes of solving different tasks in different environment configurations to be applicable in future episodes.</p>
<p>Auto-curriculum Selection. Before we generate the meta-memory, it is important to choose memories extracted from the best trials from previous episodes because random sampling may not benefit CLIN in zero-shot generalization (Adaptive-Agent-Team et al., 2023). Following the prioritized level replay scheme (Jiang et al., 2021), we choose the most successful trial per episode and retrieve memories abstracted from those trials with a fixed archive of size 10, a hyperparameter.</p>
<p>Generating Meta-Memory. The goal of the meta-memory is to help CLIN generalize to unseen tasks and/or environments. While we keep the format of the causal abstractions the same as memories generated across trials, the prompt to generate the meta-memory is different than those used for generating per-trial memory. When the new memory is to be used for the same task but in a different environment, the prompt instructs for a meta-memory helpful "to solve the same task in a new environment configuration" given the target task description with an expectation that metamemory abstractions will entail generic causal insights about the task irrespective of environment configurations (see Figure 8). Similarly, when the new memory is to be used for a different task, the prompt is modified accordingly to reflect this (Figure 9). Along with the target task description for better memory generation, each past memory selected to generate the meta-memory is attached to the final rewards for the associated trials, allowing the generator to combine insights across episodes and assign the levels of uncertainty using the evidence of success.</p>
<h2>4 Results and Analysis</h2>
<p>Experimental Setup. Test-time adaptation and generalization via continual learning require a variety of complex tasks and environment configurations to allow an agent to explore, learn latent causal insights from interactions, and exploit them in the future. We choose ScienceWorld (Wang et al., 2022), a text-based interactive environment requiring complex interactive reasoning processes to solve a plethora of science-theory-based tasks spanning several diverse classes (e.g., thermodynamics, genetics, friction, etc.). The virtual space presents 10 sub-places: foundry, greenhouse, outside area, an art studio, workshop, kitchen, living room, bedroom, bathroom, and a hallway connecting inside rooms. The presence of several objects, their individual states, and action templates renders the search space intractable for any agent. ScienceWorld presents strikingly different environment configurations across task types, making it a rich testbed for evaluating adaptation and generalization. ScienceWorld tasks are partitioned into Short (S), e.g., pick \&amp; place and Long (L), e.g., grow plant, tasks based on the number of required actions to succeed.</p>
<p>Here, we define our setups for zero-shot adaptation (ADAPT) and generalization (GEN-ENV and GENTASK). For all setups, we test CLIN and competing baselines on 18 tasks (two task instances from 9 classes) in several environment configurations from the test split of the ScienceWorld benchmark resulting in a total of 164 task-environment combinations unless stated otherwise. We evaluate based on the final reward score provided by the ScienceWorld simulator.</p>
<p>ADAPT: This setup focuses on CLIN's ability to adapt to a task by attempting it for several trials in the same environment configuration. Most importantly, CLIN initializes with an empty memory at the beginning of the first trial and generates memory at the end of each trial. While the environment gets reset at the trial boundary, CLIN's memory continues to be updated, capturing informative causal abstractions pertaining to both successful and failed actions. Here, we compare with Reflexion (Shinn et al., 2023), a SOTA, however, CLIN differs from Reflexion by how the memory is abstracted.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Rapid task adaptation with CLIN. (a) Example tasks where CLIN improves scores across trials. For CLIN, Trial-0 is the BASE, Trial-4 is the ADAPT. (b) Comparison of CLIN with Reflexion (Shinn et al., 2023). (c) CLIN improves from BASE to ADAPT (full results in Appendix C).</p>
<p>GEN-ENV: In this setup, we focus on CLIN's ability to transfer its learning from past experiences to solve tasks in an unseen environment. For a task $m$, we run CLIN for 10 different (train) environment settings (with varying objects and starting locations) and then create meta-memories from its exploration to solve the same task in an unseen (test) environment. Here, we compare CLIN with RL methods DRRN (He et al., 2015), KG-A2C (Ammanabrolu \&amp; Hausknecht, 2020), and CALM (Yao et al., 2020) trained on all (large) training variations with simulator reward and Generative Language agents, SayCan (Ahn et al., 2022), ReAct (Yao et al., 2022), and Reflexion (Shinn et al., 2023), prompted with few-shot demonstrations.</p>
<p>GEN-TASK: In this setup, we focus on CLIN's ability to transfer its learning from past experiences to solve a new task in the same environment. For an environment $e$, we run CLIN for to solve a task $m$ and then condense its learning to solve a novel task $m^{\prime}$ in the environment $e$. We took all test examples where we have a different task defined in the same environment configuration. (Adaptive-Agent-Team et al., 2023) suggests that transferring learning from a random task can be very hard; hence we couple tasks that are related (revolve around overlapping task-critical objects/locations such water, kitchen), such as boil and freeze to measure transfer learning from one to the other. This is a novel setup where we do not have any off-the-shelf baselines. However, here, we compare against CLIN-BASE, a strong baseline agent.</p>
<p>GEN-ADAPT (G+A): If CLIN, in GEN-ENV or GEN-TASK setting, does not successfully complete the new task, it can continue learning and retrying that task. We refer to this setup as GEN-ADAPT. CLIN can use any instruction-tuned LLM (Chung et al., 2022) as part of the controller, executor, and memory generator. In this paper, we use gpt-4, the same as our generative agent baselines.</p>
<h1>4.1 CLIN EXHIBITS RAPID TASK ADAPTATION</h1>
<p>Figure 4a demonstrates two example trends where CLIN learns from its own prior attempts (ADAPT) and gets better at solving a given task. Apart from length, the difficulty level of a task also depends on the environment configuration (hence, variance across environment configurations for each task). CLIN quickly adapts to a short task, Pick \&amp; Place, solving it in its 4th attempt, whereas for a longer task, Grow Fruit, it is not solved after 5th (max) attempts. Furthermore, Figure 4a depicts, CLIN becomes more efficient in later trials by solving the tasks with a lower number of (average) steps. Figure 4c shows an average number of attempts ${ }^{2}$ for CLIN to solve a task and $\%$ episodes per task where scores improved compared to its own first trial.</p>
<p>Next, we compare CLIN with Reflexion, the reflective SOTA agent, in Figure 4b. CLIN already starts off with a stronger base performance (see discussion in 4.3), however, CLIN's relative improvement in ADAPT is significantly stronger than Reflexion's gain from its base agent ReAct. Furthermore, CLIN's relative improvement is higher for longer tasks. This can be attributed to CLIN's persistent memory, which gets refined over past trials, whereas Reflexion may fall short of collecting useful learnings from earlier trials as it only focuses on the current trial for its reflections (hence not long-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RL Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generative Language Agents</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CLIN (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task</td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">DRRN</td>
<td style="text-align: center;">KGA2C</td>
<td style="text-align: center;">CALM</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">BASE</td>
<td style="text-align: center;">GEN-ENV</td>
<td style="text-align: center;">G+A</td>
</tr>
<tr>
<td style="text-align: center;">Temp $_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">Temp $_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">58.2</td>
</tr>
<tr>
<td style="text-align: center;">Pick\&amp;Place ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Pick\&amp;Place ${ }_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">93.3</td>
</tr>
<tr>
<td style="text-align: center;">Lifespan ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Lifespan ${ }_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">Biology ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: center;">Boil</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">16.3</td>
</tr>
<tr>
<td style="text-align: center;">Freeze</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">GrowPlant</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">11.2</td>
</tr>
<tr>
<td style="text-align: center;">GrowFruit</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">94.5</td>
</tr>
<tr>
<td style="text-align: center;">Biology ${ }_{2}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">85.6</td>
</tr>
<tr>
<td style="text-align: center;">Force</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Friction</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">94.0</td>
</tr>
<tr>
<td style="text-align: center;">Genetics ${ }_{1}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Genetics ${ }_{2}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">71.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">69.5</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparing CLIN with baselines for generalization across unseen environments
term). Furthermore, CLIN accumulates both useful (for the task) and harmful (for the task) causal learnings, whereas Reflexion only learns from its mistakes, lacking comprehensive learning.</p>
<h1>4.2 CLIN OUTPERFORMS SOTA, GENERALIZING TO NOVEL ENVIRONMENTS AND TASKS</h1>
<p>Generalizing to new environments. Table 1 compares CLIN with baselines that learn from training environmental variants for a task to improve its performance in a novel environment ${ }^{3}$. Language agents (including CLIN) that use NL feedback from the ScienceWorld (e.g., "Door to the kitchen is closed") perform significantly better compared to RL methods that purely rely on (sparse) numeric rewards from the environment to learn a policy. We observe a positive generalization effect in GEN-ENV (average 4 point gain) compared to BASE where CLIN tries to solve the tasks zero-shot. With a strong BASE performance, CLIN beats all baselines in generalization performance. Furthermore, in G+A, CLIN shows a substantial 16 additional improvement, beating the SOTA reflective agent by 23 points. Figure 5a additionally shows trend of improvement compared to when CLIN does not start with a meta-memory. Meta-memory helps CLIN with a stronger start than BASE ( 52.7 vs. 48.6), with a continued gain in scores till the end of Trial-4 (G+A: 69.5 vs. ADAPT: 62.2). The stronger start for CLIN with meta-memory also results in fewer steps to solve a task. Unlike imitation learning-based agents, TDT (Wang et al., 2022) and SwiftSage (Lin et al., 2023), CLIN (and most baselines) does not use any gold trajectories. Refining its memory only from self-generated trajectories, CLIN outperforms TDT on all 18 tasks and SwiftSage on 8/18 (mostly long) tasks.</p>
<p>Generalizing to new tasks. Mirroring trends from GEN-ENV, CLIN demonstrates strong transfer learning to new tasks with 13-point improvement over its BASE performance, being better at $38.8 \%$ of times (Figure 5c). The improvement attributes to critical learning about the environment ("apple juice is in the fridge", required for both boiling and freezing it), leading to improvement in previously low-performing tasks in both ADAPT and GEN-ENV setups. This transfer learning in GEN-TASK and $\mathrm{G}+\mathrm{A}$ helps CLIN to solve the tasks with a lesser number of steps ${ }^{4}$ and achieve higher rewards.</p>
<h3>4.3 DISCUSSION</h3>
<p>Importance of memory structure. CLIN extracts causal abstractions structured around 'necessary' and 'does not contribute' relations. As an ablation study, we modified our memory generator to generate free-form advice for future trials (without any constraint on their formats). We find that the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Reward and #steps trends for CLIN in (a) GEN-ENV and (b) GEN-TASK. (c) \% episode improvements and score change than CLIN without meta-memory (GEN-TASK). (d) CLIN ablations.
average reward drops by 6 points (in $10 \%$ cases compared to CLIN) when using the unstructured memory, indicating the usefulness of causal abstractions, as shown in Figure 5d.</p>
<p>Superior BASE performance. Figure 4 depicts a superior BASE performance for CLIN than the final performance of both ReAct and Reflexion despite using the same underlying LLM (here, gpt-4). We find if we ablate for the controller module in CLIN, responsible for generating a goal before outputting the next action, CLIN's BASE performance drops in $44 \%$ cases. With an 18 point drop in average reward (see Figure 5d), Abl-Contoller-BASE version of CLIN becomes equivalent to ReAct, the base agent for Reflexion, demonstrating the importance of controller even in BASE setup.</p>
<p>A qualitative example. Figure 3 depicts how memory items get refined during task adaptation and for generalization for a task boil. Env2 has a working stove, whereas in Env1, the stove is broken, but a lighter is available as an alternative. With a number of trials in these environments, CLIN learns how to use these two devices to generate heat. In an unseen environment with a broken stove, CLIN quickly receives a positive reward by using a lighter to heat a substance. While insights within an episode are often specific, e.g., "Using the lighter on the metal pot should be necessary to heat the water in the pot", CLIN compiles these insights for a new target environment (as meta-memory), e.g., "Using a heat source (stove, lighter) on the container should be necessary to heat a substance." Appendix B contains examples of memories generated during adaptation and generalization.</p>
<p>Limitation: Lack of exploration. CLIN's learnings are dependent on its own past experience. If CLIN never explores a location in the environment or does not perform an action, an insight related to the unobserved activity can never be generated. Hence, exploration becomes important when task-critical location or action in unknown to CLIN from past trials. For example, in task of creating an orange paint, the agent is supposed to find red and yellow paints from the art studio. However, art studio is not visible when CLIN starts from location 'outside'. Unless the CLIN knows that there exists an art studio, it tries alternative method to create orange paints from other irrelevant objects (e.g., an orange) and remains unsuccessful. When a memory related art-studio appears from past exploration, CLIN is able to successfully complete the task. Similarly, in boil or freeze tasks, CLIN is unable to perform well which requires it to consistently measure the temperature of the substance to know its boiling/freezing point-an act it could never perform successfully in past trials resulting into less useful memory insights and subsequent lower performance in future trials.</p>
<p>Limitation: Poor memory retrieval. For a task of boiling gallium, CLIN is supposed to use oven/blast furnace and not a stove. In the meta-memory for boiling tasks, there are two insights regarding the act of boiling: "Activating stove should be necessary to boil a substance" and "Using an alternative heat source (e.g., oven or fire pit) may be necessary if the initial heat source is insufficient." However, CLIN repeatedly retrieves the former and hence failing at the task despite performing other actions (e.g., finding gallium) correctly. This problem intensifies at the initial trial during</p>
<p>generalization due to the presence of insights with varied initial conditions for them to be applied. This can be circumvented by improved memory representation, which we leave as a future work.</p>
<h1>5 CONCLUSION</h1>
<p>Our goal is a system that can continually improve over time, both while rapidly adapting to a task by multiple retries and efficiently generalizing to novel tasks and environments. We propose CLIN, an architecture for language agents that constructs a persistent, dynamic memory of causal abstractions, refines it over time and uses it effectively to improve its performance on future tasks, achieving state-of-the-art performance. Our work systematically evaluates a novel nonparametric learning paradigm, promising never-ending learning abilities to frozen language agents.</p>
<p>Acknowledgement We sincerely thank Aristo team members Tushar Khot, Ashish Sabharwal, Shashank Gupta, Nathaniel Weir, Kyle Richardson, Jiangjie Chen, Archiki Prasad, and other members such as Faeze Brahman, Alexander Koller at the Allen Institute of AI for their generous feedback.</p>
<h2>REFERENCES</h2>
<p>Adaptive-Agent-Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal M. P. Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreyaan Pathak, Nicolas Perez Nieves, Nemanja Rakicevic, Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, and Lei M. Zhang. Human-timescale adaptation in an open-ended task space. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:255998274.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, KuangHuei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022. URL https://api.semanticscholar.org/CorpusID:247939706.</p>
<p>Diego Aineto, Sergio Jiménez, and Eva Onaindía. Learning strips action models with classical planning. In International Conference on Automated Planning and Scheduling, 2018. URL https://api.semanticscholar.org/CorpusID:49405691.</p>
<p>Prithviraj Ammanabrolu and Matthew J. Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In ICLR, 2020.</p>
<p>Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktaschel, and Jason Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. In North American Chapter of the Association for Computational Linguistics, 2020. URL https://api.semanticscholar.org/CorpusID:222125301.</p>
<p>Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Joshua Tobin, P. Abbeel, and Wojciech Zaremba. Hindsight experience replay. ArXiv, abs/1707.01495, 2017. URL https://api.semanticscholar.org/CorpusID:3532908.</p>
<p>Ankuj Arora, Humbert Fiorino, Damien Pellier, Marc Métivier, and Sylvie Pesty. A review of learning planning action models. The Knowledge Engineering Review, 33, 2018. URL https: //api.semanticscholar.org/CorpusID:56483203.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Neural Information Processing Systems, 2021. URL https://api.semanticscholar. org/CorpusID:235294299.</p>
<p>Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkongu Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022. URL https://api.semanticscholar.org/CorpusID:253018554.</p>
<p>Bhavana Dalvi, Oyvind Tafjord, and Peter Clark. Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. In EMNLP, 2022.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. arXiv: Artificial Intelligence, 2015. URL https://api.semanticscholar.org/CorpusID:15986631.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022.</p>
<p>Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Nicolaus Foerster, Edward Grefenstette, and Tim Rocktaschel. Replay-guided adversarial environment design. In Neural Information Processing Systems, 2021. URL https://api.semanticscholar.org/CorpusID:238408352.</p>
<p>Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. ArXiv, abs/2305.17390, 2023. URL https: //api.semanticscholar.org/CorpusID:258960143.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with selffeedback. ArXiv, abs/2303.17651, 2023. URL https://api.semanticscholar.org/CorpusID: 257900871 .</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv, abs/1312.5602, 2013. URL https://api.semanticscholar.org/CorpusID:15238391.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley D. Edwards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Trans. Mach. Learn. Res., 2022, 2022. URL https://api.semanticscholar.org/CorpusID:248722148.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908. 10084 .</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve GPT-3 after deployment. In ACL Workshop on Commonsense Representation and Reasoning (CSRR'22), 2022. (also arxiv:2201.06009).</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv, abs/2305.16291, 2023. URL https://api.semanticscholar.org/CorpusID: 258887849 .</p>
<p>Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Conference on Empirical Methods in Natural Language Processing, 2022. URL https://api.semanticscholar.org/CorpusID:247451124.</p>
<p>Shunyu Yao, Rohan Rao, Matthew J. Hausknecht, and Karthik Narasimhan. Keep calm and explore: Language models for action generation in text-based games. ArXiv, abs/2010.02903, 2020. URL https://api.semanticscholar.org/CorpusID:222142129.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022. URL https://api.semanticscholar.org/CorpusID:252762395.</p>
<h1>A CLIN PROMPTS</h1>
<p>Figures 6 to 9 are the complete prompts for next-action generation (controller + executor), memorygenerator during ADAPT, GEN-ENV, and GEN-TASK.</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">System</span><span class="o">]</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="n">helping</span><span class="w"> </span><span class="k">execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">science</span><span class="w"> </span><span class="n">experiment</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">simulated</span>
<span class="n">environment</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">limited</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">step</span><span class="p">.</span>
<span class="o">[</span><span class="n">User</span><span class="o">]</span><span class="err">:</span>
<span class="n">Possible</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">OBJ</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="p">)</span><span class="err">:</span>
<span class="err">{</span><span class="n">objects_str</span><span class="err">}</span>
<span class="n">Your</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="nl">formats</span><span class="p">:</span>
<span class="n">Possible</span><span class="w"> </span><span class="nl">actions</span><span class="p">:</span>
<span class="err">{</span><span class="n">actions_str</span><span class="err">}</span>
<span class="k">If</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">say</span><span class="w"> </span><span class="err">\</span><span class="ss">&quot;Ambiguous request\&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">things</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">case</span><span class="p">,</span>
<span class="n">respond</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">corresponding</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">want</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="p">.</span>
<span class="n">What</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="k">next</span><span class="vm">?</span>
<span class="k">First</span><span class="p">,</span><span class="w"> </span><span class="n">scan</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="p">(</span><span class="n">unordered</span><span class="p">)</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">learnings</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">provided</span><span class="p">.</span><span class="w"> </span><span class="n">Decide</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span>
<span class="n">learnings</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">applicable</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">last</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">make</span><span class="w"> </span><span class="n">progress</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">task</span><span class="p">.</span><span class="w"> </span><span class="k">Then</span>
<span class="k">only</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">selected</span><span class="w"> </span><span class="n">learnings</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="p">,</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">rationale</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">picking</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span>
<span class="k">action</span><span class="p">.</span><span class="w"> </span><span class="k">If</span><span class="w"> </span><span class="k">no</span><span class="w"> </span><span class="n">Learning</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">selected</span><span class="p">,</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rationale</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">last</span>
<span class="n">observation</span><span class="p">.</span><span class="w"> </span><span class="nf">Format</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">follows</span><span class="p">:</span>
<span class="k">Write</span><span class="w"> </span><span class="s1">&#39;I used learning id(s):&#39;</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="p">;</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="k">no</span>
<span class="n">learnings</span><span class="w"> </span><span class="n">selected</span><span class="p">.</span><span class="w"> </span><span class="k">Then</span><span class="p">,</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="err">$$$</span><span class="w"> </span><span class="n">followed</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rationale</span><span class="p">.</span><span class="w"> </span><span class="n">Finally</span><span class="p">,</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="err">###</span>
<span class="n">followed</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">think</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">completed</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="n">TASK_COMPLETE</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">requires</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="s1">&#39;focus&#39;</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">something</span><span class="w"> </span><span class="p">(</span><span class="n">OBJ</span><span class="p">),</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="n">FOCUS</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="o">&lt;</span><span class="n">OBJ</span><span class="o">&gt;</span><span class="w"> </span><span class="k">as</span>
<span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span><span class="w"> </span><span class="n">FOCUS</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">extremely</span><span class="w"> </span><span class="n">critical</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span>
<span class="k">of</span><span class="w"> </span><span class="n">times</span><span class="w"> </span><span class="s1">&#39;focus&#39;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">mentioned</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">description</span><span class="p">.</span><span class="w"> </span><span class="k">Using</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="ow">or</span>
<span class="n">inappropriately</span><span class="w"> </span><span class="p">(</span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="k">terminate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">session</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">will</span>
<span class="n">be</span><span class="w"> </span><span class="n">rendered</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">incomplete</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">performed</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">requires</span><span class="w"> </span><span class="n">waiting</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">effect</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="s1">&#39;wait&#39;</span>
<span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span>
</code></pre></div>

<p>Figure 6: Prompt for the Controller and the Executor</p>
<h2>B EXAMPLE MEMORIES</h2>
<p>Example generated memory for ADAPT, GEN-ENV, and GEN-TASKsetups in Figures 10 to 12.</p>
<p>[System]: You are an expert assistant.
[User]:
You are given CURRENT TRACE, a sequence of actions that an agent made in a world to accomplish a task.</p>
<p>Task is detailed at the beginning.
For each action, there is a rationale why the agent made that action.
There is an observation that provide details about the new state of the world after each action was executed.
The CURRENT TRACE is accompanied by an EVALUATION REPORT indicating the success of the attempt to the task.</p>
<p>You can also be provided with PREVIOUS LEARNINGS which are learnings from the previous attempts by the agent for the same task in the same environment/world. TASK indicates the task description. EPISODE indicates the number of previous attempts of the task.</p>
<p>Generate a summary of learning, as a numbered list, that will help the agent to successfully accomplish the SAME task AGAIN, in the SAME world.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY BE CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
{CURRENT TRACE}
Action: ...
Observation: ...
...
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>Summary of learning as a numbered list:</p>
<p>Figure 7: Prompt for CLIN's memory generator during ADAPT</p>
<p>[System]: You are an expert assistant.
[User]: You are given a collection of learning lists, that are derived from actions made by an agent and subsequent observations from a world to accomplish a TYPE of TASKs. All of these TASKs belong to a same TYPE (such as 'boiling') but they are executed in different ENVIRONMENT configurations. A different ENVIRONMENT configuration means there are presence of a different set of objects (lighter instead of a stove) that are critical for solving the TASK, presence of a different set of distractor objects that are not useful for the TASK, a different floor plan, etc.</p>
<p>For each learning list, the TASK description is provided at the beginning as TASK:
Each learning list indicates a list of learnings from the agent's best attempt to solve the TASK.</p>
<p>Each learning list is associated with an EVALUATION REPORT indicated how sucessful the respective attempt was for solving the task.</p>
<p>Consider all learning lists and combine them in to a summary of learnings, as a numbered list, that will help the agent to successfully accomplish a NEW TASK related to the previous TASKs (such as 'boling') in an ENVIRONMENT configuration that it has not seen before. The NEW TASK description will be provided.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY NOT CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
{PREVIOUS LEARNINGS}
TASK: ...
LEARNINGS:...
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>NEW TASK: ...
Summary of learning as a numbered list:</p>
<p>Figure 8: Prompt for CLIN's memory generator during GEN-ENV</p>
<p>[System]: You are an expert assistant.
[User]: You may be given a list of learnings, that are derived from actions made by an agent and subsequent observations from a world to accomplish a TASK in an ENVIRONMENT CONFIGURATION.</p>
<p>For the learning list, the TASK description is provided at the beginning as TASK:
The learnings are from the agent's best attempt to solve the TASK.
The learning list is associated with an EVALUATION REPORT indicated how sucessful the attempt was for solving the task.</p>
<p>Now, generate a summary of learnings from the existing ones if provided, such that they will be useful to the NEW TASK in the SAME ENVIRONMENT CONFIGURATION. The NEW TASK may require different actions which are not captured in the given learnings but given learnings can be used to infer about the ENVIRONMENT CONFIGURATION. The NEW TASK description will be given. If PREVIOUS LEARNINGS says 'No learnings available', improvise learnings for the NEW TASK.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY NOT CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
(PREVIOUS LEARNINGS)
TASK: . . .
LEARNINGS: . . .
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>NEW TASK: ...
Summary of learning as a numbered list:</p>
<p>Figure 9: Prompt for CLIN's memory generator during GEN-TASK</p>
<p>Task: Your task is to determine whether round seed shape is a dominant or recessive trait in the pea plant. If the trait is dominant, focus on the blue box. If the trait is recessive, focus on the orange box.</p>
<p>Memory (ADAPT), after Trial5:</p>
<ol>
<li>Looking around MAY BE NECESSARY to gather information about the environment.</li>
<li>Moving to the greenhouse SHOULD BE NECESSARY to find the pea seeds.</li>
<li>Picking the jar SHOULD BE NECESSARY to access the pea seeds.</li>
<li>Moving the round green pea seed to flower pot 2 SHOULD BE NECESSARY to grow the pea plants.</li>
<li>Moving the wrinkly green pea seed to flower pot 3 SHOULD BE NECESSARY to grow the pea plants.</li>
<li>Waiting SHOULD BE NECESSARY for the pea plants to grow and show their traits.</li>
<li>Looking at the adult pea plant in flower pots 2 and 3 MAY BE NECESSARY to observe the traits of the offspring.</li>
<li>Focusing on the blue box SHOULD BE NECESSARY to indicate the trait of the seed shape.</li>
<li>Continuously looking at the same environment DOES NOT CONTRIBUTE to the task of determining the dominant or recessive trait in the pea plant.</li>
<li>Moving to the workshop DOES NOT CONTRIBUTE to determining the dominant or recessive trait in the pea plant.</li>
<li>Picking up the battery or the wires in the workshop DOES NOT CONTRIBUTE to finding the pea seeds.</li>
<li>Waiting for longer periods MAY BE NECESSARY to allow the pea plants to fully grow and show their traits.</li>
</ol>
<p>Figure 10: Memory generated after Trial5 in ADAPT for a Genetics task.</p>
<p>Task: Your task is to determine which of the two inclined planes (aluminum, platinum) has the most friction. After completing your experiment, focus on the inclined plane with the most friction.</p>
<p>Meta-memory (GEN-ENV):</p>
<ol>
<li>Moving to the hallway SHOULD BE NECESSARY to reach the workshop.</li>
<li>Moving to the workshop SHOULD BE NECESSARY to find the block.</li>
<li>Picking up the block SHOULD BE NECESSARY to move it to the inclined planes.</li>
<li>Placing the block on the first inclined plane (either aluminum or platinum) SHOULD BE NECESSARY to measure the friction.</li>
<li>Activating the stopwatch SHOULD BE NECESSARY to time the experiment.</li>
<li>Waiting for a certain period MAY CONTRIBUTE to observing the friction effect.</li>
<li>Deactivating the stopwatch SHOULD BE NECESSARY to stop timing the experiment.</li>
<li>Moving the block to the second inclined plane (either aluminum or platinum) SHOULD BE NECESSARY to compare the friction.</li>
<li>Activating the stopwatch again SHOULD BE NECESSARY to time the second part of the experiment.</li>
<li>Waiting for a certain period again MAY BE NECESSARY to observe the friction effect.</li>
<li>Deactivating the stopwatch again SHOULD BE NECESSARY to stop timing the experiment.</li>
<li>Focusing on the inclined plane with the most friction SHOULD BE NECESSARY to conclude the experiment.</li>
<li>Repeating the experiment multiple times MAY BE NECESSARY for more accurate results.</li>
<li>Looking around in the initial room multiple times DOES NOT CONTRIBUTE to the task.</li>
<li>Moving the block back and forth between the two inclined planes DOES NOT CONTRIBUTE to the task.</li>
</ol>
<p>Figure 11: Meta-memory used in GEN-ENV for a Friction task.</p>
<p>Task: Your task is to determine whether round seed shape is a dominant or recessive trait in the pea plant. If the trait is dominant, focus on the blue box. If the trait is recessive, focus on the orange box.</p>
<p>Meta-memory (GEN-TASK):
Task: Your task is to freeze mercury. First, focus on the substance. Then, take actions that will cause it to change its state of matter.</p>
<p>Meta-memory (GEN-TASK):</p>
<ol>
<li>Looking around MAY BE NECESSARY to identify the available resources and the layout of the environment.</li>
<li>Moving to different rooms SHOULD BE NECESSARY to find the tools and materials needed to change the state of the substance.</li>
<li>Picking up items like glass cups or metal pots SHOULD BE NECESSARY to contain the substance for changing its state.</li>
<li>Focusing on the substance SHOULD BE NECESSARY to understand its properties and how to interact with it.</li>
<li>Picking up the thermometer SHOULD BE NECESSARY to monitor the temperature of the substance.</li>
<li>Using the thermometer on the substance SHOULD BE NECESSARY to monitor the progress of the task.</li>
<li>Puring the substance into the container SHOULD BE NECESSARY to prepare it for cooling.</li>
<li>Moving the container to a cooling device SHOULD BE NECESSARY to cool the substance.</li>
<li>Waiting for a period of time after cooling the substance SHOULD BE NECESSARY to allow the substance to change state.</li>
<li>Repeatedly checking the temperature of the substance SHOULD BE NECESSARY to monitor the progress of the task.</li>
<li>Activating the stove DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Picking up unrelated items like a lighter DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Moving to unrelated rooms like the workshop DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Teleporting to the kitchen MAY BE NECESSARY for the task as it speeds up the process of moving between rooms.</li>
<li>Using the thermometer multiple times on the substance after it reaches freezing point DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
</ol>
<p>Figure 12: Meta-memory used in GEN-TASK for a Freeze task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generative L. Agents</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CLIN (ours)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">BASE</td>
<td style="text-align: center;">ADAPT</td>
</tr>
<tr>
<td style="text-align: left;">Temp</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">14.3</td>
</tr>
<tr>
<td style="text-align: left;">Temp</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: left;">Pick\&amp;Place</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Pick\&amp;Place</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: left;">Lifespan</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Lifespan</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: left;">Biology</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: left;">Boil</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">15.2</td>
</tr>
<tr>
<td style="text-align: left;">Freeze</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: left;">GrowPlant</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">GrowFruit</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">71.6</td>
</tr>
<tr>
<td style="text-align: left;">Biology</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">81.0</td>
</tr>
<tr>
<td style="text-align: left;">Force</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Friction</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">72.5</td>
</tr>
<tr>
<td style="text-align: left;">Genetics</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Genetics</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">92.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">$\mathbf{6 2 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">$\mathbf{6 1 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">$\mathbf{6 2 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparing CLIN with baselines for adaptation</p>
<h1>C MORE RESULTS</h1>
<p>Full results for CLIN outperforming Reflexion is in Table 2. For ScienceWorld benchmark, we exclude electricity tasks since they deviate from standard electrical conventions, prohibiting us from fairly using LLM agents. We choose the first 10 test variants for each 18 tasks selected. The full list of 18 tasks from the benchmark, with the number of test variants used in parentheses:
grow-plant (10), identify-life-stages-1 (5), grow-fruit (10), measure-melting-point-known-substance (10), mendelian-genetics-unknown-plant (10), chemistry-mix-paint-secondary-color (9), freeze (9), lifespan-longest-lived (10), inclined-plane-determine-angle (10), boil (9), use-thermometer (10), chemistry-mix (8), lifespan-shortest-lived (10), find-plant (10), find-living-thing (10), identify-life-stages-2 (4), mendelian-genetics-known-plant (10), inclined-plane-friction-named-surfaces (10).
Short tasks have oracle lengths less than 37 steps (median), and Long tasks have oracle lengths more than equal to 37 steps.</p>
<p>The map to the short names used for tasks in the paper:
Temp: use-thermometer, measure-melting-point-known-substance; Pick\&amp;Place: find-plant, find-living-thing; Chemistry: chemistry-mix, chemistry-mix-paint-secondary-color; Lifespan: lifespan-longest-lived, lifespan-shortest-lived; Biology: identify-life-stages-1, identify-life-stages-2, Boil; Freeze; Grow Plant, Grow Fruit; Force: inclined-plane-determine-angle; Friction: inclined-plane-friction-named-surfaces; Genetics: mendelian-genetics-known-plant, mendelian-genetics-unknownplant.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Baseline numbers are derived from Table 1 in (Lin et al., 2023)
${ }^{4} #$ steps in Figure 5a,b are normalized between $0-1,1$ being maximum # steps allowed for a task.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
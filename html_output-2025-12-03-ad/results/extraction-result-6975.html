<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6975 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6975</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6975</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-251bdaff7521e60fe81fc375acfd34951c7f13ea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/251bdaff7521e60fe81fc375acfd34951c7f13ea" target="_blank">ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper presents ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance, and presents an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets.</p>
                <p><strong>Paper Abstract:</strong> Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details at https://github.com/IBM/regen.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6975.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6975.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF-triple linearization with S/P/O markers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized RDF triple sequence with explicit <S>/<P>/<O> boundary tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential token-based encoding that represents a knowledge graph as a list of RDF triples linearized into a token sequence, with special indivisible tokens marking subject, predicate and object boundaries; used to feed graphs to encoder–decoder PLMs as text sequences for both graph->text and text->graph training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple linearization (subject/predicate/object markers)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each triple (s,p,o) is turned into a contiguous token subsequence with explicit boundary markers: e.g. [<S>, s_tokens, <P>, p_tokens, <O>, o_tokens]. The paper expands the model vocabulary to include special indivisible tokens <S>, <P>, <O> that are not split by tokenization. Graphs are represented as a list of such triple subsequences concatenated into a single input sequence; in examples a diamond (♦) separator is used in human-readable tables but the model uses the special tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; token-based; lossless (triple-preserving)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Use the dataset-provided list order of triples to serialize the graph into an ordered edge list; special-case preprocessing for TEKGEN: establish subject/relation/object boundaries using Wikidata property lookup; data augmentation by randomly shuffling the order of triples during training (permutation-based augmentation). The paper notes other traversal options (BFS/DFS/random walk) as possible but does not use them in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG+ 2020 (v3.0); TEKGEN (TEXGEN) processed with added boundaries</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (G2T) and text-to-graph / Text-to-RDF semantic parsing (T2G)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (t5-large, t5-base) encoder–decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained encoder–decoder T5 models fine-tuned on seq2seq tasks; experiments used t5-large (~770M parameters) and t5-base (~220M parameters); models first CE-finetuned then optionally fine-tuned with RL (SCST).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>G2T: BLEU, BLEU_NLTK, METEOR, chrF++; T2G: Exact/Ent_Type/Partial/Strict match F1, Precision, Recall</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG+ G2T (best ReGen t5-large, SCST using METEOR reward): BLEU=0.563, BLEU_NLTK=0.559, METEOR=0.425, chrF++=0.706. WebNLG+ T2G (ReGen T2G.CE t5-large): Exact F1=0.723, Exact Precision=0.714, Exact Recall=0.738 (T2G.RL showed slightly lower numbers). TEKGEN G2T (ReGen-SCST t5-large): Test BLEU=0.262, BLEU_NLTK=0.262, METEOR=0.422. TEKGEN T2G (ReGen-SCST t5-large): Test F1=0.623, P=0.610, R=0.647.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allowed reframing both G2T and T2G as seq2seq to directly leverage large pretrained encoder–decoder LMs (T5). Data augmentation via triple shuffling reduced overfitting to triple order. Using SCST (self-critical RL) on top of CE-finetuned models led to consistent and notable improvements for graph->text (G2T) metrics (e.g., METEOR-optimized SCST gave the best overall gains on WebNLG+); RL required a good CE starting point. For T2G, RL with exact-F1 reward did not reliably improve over CE baselines in WebNLG+, though it did on TEKGEN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Linearization is ambiguous (many possible traversals) and the paper does not impose a canonical ordering; potential for the model to memorize sequence order (mitigated by random shuffles). TEKGEN originally lacked explicit entity/relation/object boundaries and required additional processing to create them. Graph evaluation is computationally expensive (matching permutations grows factorially for large triple sets). RL (SCST) can fail to improve when reward is too rigid (exact F1 for T2G) or when greedy and sampled sequences are identical (no gradient signal). Linearization encodes triples but may obscure global graph structure beyond edge lists (not explicitly modeled). Average token lengths per graph are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to prior top WebNLG+ participants (who used approaches like R-GCN + T5 pipelines, canonicalization, or specialized RDF preprocessing), this paper uses the straightforward linearized RDF triple sequence with S/P/O markers and random triple shuffling; despite its simplicity it outperformed prior published WebNLG+ 2020 results (achieving new state-of-the-art on reported splits for G2T and strong T2G CE results). The authors note other teams used canonicalization and graph encoders, whereas their marker-based sequential encoding permitted direct PLM fine-tuning and effective RL fine-tuning for G2T.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020) <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training <em>(Rating: 2)</em></li>
                <li>i2 : A plan-and-pretrain approach for knowledge graph-to-text generation <em>(Rating: 1)</em></li>
                <li>Leveraging large pretrained models for WebNLG 2020 <em>(Rating: 1)</em></li>
                <li>Improving text-to-text pretrained models for the graph-to-text task <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6975",
    "paper_id": "paper-251bdaff7521e60fe81fc375acfd34951c7f13ea",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "RDF-triple linearization with S/P/O markers",
            "name_full": "Linearized RDF triple sequence with explicit &lt;S&gt;/&lt;P&gt;/&lt;O&gt; boundary tokens",
            "brief_description": "A sequential token-based encoding that represents a knowledge graph as a list of RDF triples linearized into a token sequence, with special indivisible tokens marking subject, predicate and object boundaries; used to feed graphs to encoder–decoder PLMs as text sequences for both graph-&gt;text and text-&gt;graph training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "RDF triple linearization (subject/predicate/object markers)",
            "representation_description": "Each triple (s,p,o) is turned into a contiguous token subsequence with explicit boundary markers: e.g. [&lt;S&gt;, s_tokens, &lt;P&gt;, p_tokens, &lt;O&gt;, o_tokens]. The paper expands the model vocabulary to include special indivisible tokens &lt;S&gt;, &lt;P&gt;, &lt;O&gt; that are not split by tokenization. Graphs are represented as a list of such triple subsequences concatenated into a single input sequence; in examples a diamond (♦) separator is used in human-readable tables but the model uses the special tokens.",
            "representation_type": "sequential; token-based; lossless (triple-preserving)",
            "encoding_method": "Use the dataset-provided list order of triples to serialize the graph into an ordered edge list; special-case preprocessing for TEKGEN: establish subject/relation/object boundaries using Wikidata property lookup; data augmentation by randomly shuffling the order of triples during training (permutation-based augmentation). The paper notes other traversal options (BFS/DFS/random walk) as possible but does not use them in experiments.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG+ 2020 (v3.0); TEKGEN (TEXGEN) processed with added boundaries",
            "task_name": "graph-to-text generation (G2T) and text-to-graph / Text-to-RDF semantic parsing (T2G)",
            "model_name": "T5 (t5-large, t5-base) encoder–decoder",
            "model_description": "Pretrained encoder–decoder T5 models fine-tuned on seq2seq tasks; experiments used t5-large (~770M parameters) and t5-base (~220M parameters); models first CE-finetuned then optionally fine-tuned with RL (SCST).",
            "performance_metric": "G2T: BLEU, BLEU_NLTK, METEOR, chrF++; T2G: Exact/Ent_Type/Partial/Strict match F1, Precision, Recall",
            "performance_value": "WebNLG+ G2T (best ReGen t5-large, SCST using METEOR reward): BLEU=0.563, BLEU_NLTK=0.559, METEOR=0.425, chrF++=0.706. WebNLG+ T2G (ReGen T2G.CE t5-large): Exact F1=0.723, Exact Precision=0.714, Exact Recall=0.738 (T2G.RL showed slightly lower numbers). TEKGEN G2T (ReGen-SCST t5-large): Test BLEU=0.262, BLEU_NLTK=0.262, METEOR=0.422. TEKGEN T2G (ReGen-SCST t5-large): Test F1=0.623, P=0.610, R=0.647.",
            "impact_on_training": "Allowed reframing both G2T and T2G as seq2seq to directly leverage large pretrained encoder–decoder LMs (T5). Data augmentation via triple shuffling reduced overfitting to triple order. Using SCST (self-critical RL) on top of CE-finetuned models led to consistent and notable improvements for graph-&gt;text (G2T) metrics (e.g., METEOR-optimized SCST gave the best overall gains on WebNLG+); RL required a good CE starting point. For T2G, RL with exact-F1 reward did not reliably improve over CE baselines in WebNLG+, though it did on TEKGEN.",
            "limitations": "Linearization is ambiguous (many possible traversals) and the paper does not impose a canonical ordering; potential for the model to memorize sequence order (mitigated by random shuffles). TEKGEN originally lacked explicit entity/relation/object boundaries and required additional processing to create them. Graph evaluation is computationally expensive (matching permutations grows factorially for large triple sets). RL (SCST) can fail to improve when reward is too rigid (exact F1 for T2G) or when greedy and sampled sequences are identical (no gradient signal). Linearization encodes triples but may obscure global graph structure beyond edge lists (not explicitly modeled). Average token lengths per graph are not reported.",
            "comparison_with_other": "Compared to prior top WebNLG+ participants (who used approaches like R-GCN + T5 pipelines, canonicalization, or specialized RDF preprocessing), this paper uses the straightforward linearized RDF triple sequence with S/P/O markers and random triple shuffling; despite its simplicity it outperformed prior published WebNLG+ 2020 results (achieving new state-of-the-art on reported splits for G2T and strong T2G CE results). The authors note other teams used canonicalization and graph encoders, whereas their marker-based sequential encoding permitted direct PLM fine-tuning and effective RL fine-tuning for G2T.",
            "uuid": "e6975.0",
            "source_info": {
                "paper_title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020)",
            "rating": 2,
            "sanitized_title": "the_2020_bilingual_bidirectional_webnlg_shared_task_overview_and_evaluation_results_webnlg_2020"
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "rating": 2,
            "sanitized_title": "knowledge_graph_based_synthetic_corpus_generation_for_knowledgeenhanced_language_model_pretraining"
        },
        {
            "paper_title": "i2 : A plan-and-pretrain approach for knowledge graph-to-text generation",
            "rating": 1,
            "sanitized_title": "i2_a_planandpretrain_approach_for_knowledge_graphtotext_generation"
        },
        {
            "paper_title": "Leveraging large pretrained models for WebNLG 2020",
            "rating": 1,
            "sanitized_title": "leveraging_large_pretrained_models_for_webnlg_2020"
        },
        {
            "paper_title": "Improving text-to-text pretrained models for the graph-to-text task",
            "rating": 1,
            "sanitized_title": "improving_texttotext_pretrained_models_for_the_graphtotext_task"
        }
    ],
    "cost": 0.01130025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</h1>
<p>Pierre L. Dognin<br>IBM Research<br>pdognin@us.ibm.com<br>Igor Melnyk<br>IBM Research<br>igor.melnyk@ibm.com</p>
<h4>Abstract</h4>
<p>Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TeKGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details in https://github.com/IBM/regen.</p>
<h2>1 Introduction</h2>
<p>Graph representation of knowledge is a powerful tool to capture real-world information where complex relationships between node entities can be efficiently encoded. Automatic generation of Knowledge Bases (KBs) from free-form text and its counterpart of generating semantically relevant text from KBs are both active and challenging research topics.</p>
<p>Recently, there has been an increased interest in leveraging Pretrained Language Models (PLMs) to improve performance for text generation from graph, or graph-to-text (G2T) task (Ribeiro et al., 2020). Indeed, large PLMs like T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) that have been pretrained on vast amount of diverse and variedly structured data, are particularly good candidates for generating natural looking text from graph data.</p>
<p>Inkit Padhi<br>IBM Research<br>inkpad@ibm.com<br>Payel Das<br>IBM Research<br>daspa@us.ibm.com</p>
<p>BART- and T5-related models have been employed by top performers in public challenges such as the WebNLG+ 2020 Challenge (Castro Ferreira et al., 2020b) where both graph-to-text and text-to-graph (T2G) tasks are offered, under the names RDF-to-Text and Text-to-RDF (semantic parsing) respectively; RDF stands for Resource Description Framework, a standard for describing web resources. One can notice that more teams entered the competition for the G2T task than for T2G as the latter is a much harder task. Best models generally use PLMs and fine-tune them for the target modality at hand (either graph or text). This is possible by re-framing the T2G and G2T generations as a sequence to sequence (Seq2Seq) generation problem, which suits fine-tuning PLMs well. One can therefore hope to leverage the large pretraining of PLMs to improve the overall generation quality.</p>
<p>The Seq2Seq formulation requires any input graph to be linearized as a sequence, which is not unique. This creates an opportunity for data augmentation where multiple linearizations are provided to the model at training time so the model learns the content represented by the graph, not the order of its sequential representation.</p>
<p>In this work, we are interested in leveraging the power of PLMs for both G2T and T2G generation tasks, and will demonstrate the strength of our approach by improving upon the best results of the WebNLG+ 2020 Challenge (rev 3.0) as reported by Castro Ferreira et al. (2020a) for both T2G (Semantic Parsing) and G2T (Data-to-Text) tasks. We will also present results for the TeKGen Corpus (Agarwal et al., 2021) to show performance on a different, much larger dataset. To illustrate the task of generation, Fig. 1 provides examples of G2T and T2G outputs obtained using the proposed generation framework. The first two sentences of the abstract of this paper were used as input for T2G using our best model. The model generates a graph from the input text by simultaneously extracting</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Actual examples of generation for Text-to-Graph and Graph-to-Text tasks using our best RL models. The first two sentences of the abstract were processed through our best models. First, a graph was created capturing the facts from the input sentences. Then, this graph was used as input to generate text. Despite a strong domain mismatch between input data and models, the generated paragraph is capturing most of the original sentences content. Both models were trained using RL, specifically Self-Critical Sequence Training (SCST).
relevant nodes and linking them coherently. For the G2T task, another model starts from the generated graph and generates semantically relevant text from it. As one can appreciate, the final text is quite readable and captures most facts from the original abstract sentences despite a strong domain mismatch between input data and training data, which both models were built on.</p>
<p>Since both T2G and G2T generative tasks can be formulated as a Seq2Seq problem, we propose to use Reinforcement Learning (RL) as part of the PLMs fine-tuning on the target domain data. For both G2T and T2G tasks, a differentiable function such as the cross-entropy (CE) loss function is often used, since minimizing it results in maximizing the probability of generating the correct token/word. However, when it comes to evaluating a model's performance, benchmarks often use BLEU (Pa Pa Aung et al., 2020), METEOR (Lavie and Agarwal, 2007), and chrF++ (Popović, 2017) for G2T, or simply F1, Precision, and Recall scores for T2G, none of which being differentiable. During training, one hopes that by minimizing the CE loss, the model will tend towards better prediction of the target tokens, hence improving on evaluation metrics as a beneficial by-product. Thankfully, RL provides a framework where we can update our model parameters so to improve evaluation metrics directly. Mixed Incremental Cross-Entropy Reinforce from Ranzato et al. (2016) introduced using REINFORCE (Williams, 1992) for sequence training. We propose to use one of its variant known as Self-Critical Sequence Training (SCST) (Rennie et al., 2017) for both T2G and G2T training.</p>
<p>In summary, our main contributions are:</p>
<ul>
<li>We propose to use RL-based sequence training, specifically SCST, for both G2T and T2G tasks. This is the first time that RL based training is proposed to the bi-directional generation of text and
graph. To the best of our knowledge, the present work is the first time it is introduced for a T2G task. - We demonstrate that our approach provides better performance than the best systems reported for the WebNLG 2020+ Challenge.</li>
<li>We provide a thorough investigation of SCSTbased training for both T2G and G2T tasks, including best rewards combination.</li>
<li>We constructed subject and relation-object boundaries from TEXGEN sentence-triples pairs and showed performance of our approach for both T2G and G2T tasks.</li>
<li>We adapted the large-scale TEXGEN corpus (Agarwal et al., 2021) for T2G and G2T tasks and confirmed the benefit of SCST-based fine-tuning approach over CE-trained baselines.</li>
</ul>
<h2>2 Related work</h2>
<p>In the WebNLG+ 2020 Challenge, most top performing models relied on fine-tuning of PLMs. Interestingly, all four top teams in this Challenge proposed quite different approaches while leveraging PLMs. $1^{\text {st }}$ place Amazon AI (Guo et al., 2020a) pipelined a relational graph convolutional network (R-GCN) and a T5 PLM with some canonicalization rules. $2^{\text {nd }}$ place OSU Neural NLG (Li et al., 2020), the closest to our approach in spirit, used T5 and mBART PLMs to fine-tune after special data preprocessing. $3^{\text {rd }}$ place FBConvAI (Yang et al., 2020) used BART PLM and multiple strategies to model input RDFs. $4^{\text {th }}$ place bt5 employed a T5 PLM trained in a bi-lingual approach on English and Russian, even using WMT English/Russian parallel corpus.</p>
<p>Recently, Dognin et al. (2020); Guo et al. (2020b, 2021) proposed models trained to generate in both T2G and G2T directions, with consistency cycles created to enable the use of unsupervised datasets.</p>
<p>In contrast, our approach of fine-tuning a T5 PLM is fully supervised but can produce either the specialized models for T2G and G2T tasks alone, or a hybrid model that can handle both T/G inputs simultaneously to generate the corresponding translated G/T outputs.</p>
<p>Note that in contrast to many WebNLG+ 2020 Challenge participants, e.g. Li et al. (2020), no preprocessing of the data is performed for text, while for graph triples, we add tokens to mark subject, predicate, and object positions in their linearized sequence representation. Moreover, data augmentation is performed by allowing random shuffling of triples order in graph linearization to avoid a model to learn the exact order of triples, especially for the T2G task.</p>
<p>While the use of RL training in PLM has been explored in many works, the approach of Chen et al. (2020) is closest to ours. However, their work focuses on the improved text generation in the context of natural question generation, while in our algorithm we use it for graph-to-text and text-to-graph generations.</p>
<h2>3 Models</h2>
<p>Models are trained on a dataset $\mathcal{D}$ composed of a set of $\left(x_{\mathrm{T}}, x_{\mathrm{G}}\right)^{i}$ samples, where superscript $i$ denotes the $i$-th sample in $\mathcal{D}, x_{\mathrm{T}}$ is made of text (one or more sentences), and $x_{\mathrm{G}}$ is a corresponding graph represented as a list of triples $x_{\mathrm{G}}=\left[\left(s^{1}, p^{1}, o^{1}\right), \ldots,\left(s^{K}, p^{K}, o^{K}\right)\right]$, where the $k$-th triple is composed of a subject $s^{k}$, predicate (relationship) $p^{k}$, and object $o^{k}$. For G2T, the model is given $x_{\mathrm{G}}$ as input and must generate $\hat{x}_{\mathrm{T}}$. A cross-entropy loss is computed as an expectation:</p>
<p>$$
\mathcal{L}<em _mathrm_T="\mathrm{T">{\mathrm{CE}}^{\mathrm{T}}=\underset{x</em>\right)\right]
$$}} \sim \mathcal{D}}{\mathbb{E}}\left[-\log p_{\theta}^{\mathrm{G} 2 \mathrm{~T}}\left(x_{\mathrm{T}</p>
<p>where $p_{\theta}^{\mathrm{G} 2 \mathrm{~T}}\left(x_{\mathrm{T}}\right)$ is the distribution of the generated sequence $\hat{x}<em _mathrm_G="\mathrm{G">{\mathrm{T}}=T</em>} 2 \mathrm{~T}}\left(x_{\mathrm{G}}\right), T_{\mathrm{G} 2 \mathrm{~T}}($.$) being the trans-$ formation from graph to text. Our model is parameterized by $\theta$, and $x_{\mathrm{T}}$ is effectively sampled from the marginal distribution of text samples from $\mathcal{D}$. $\hat{x<em 1="1">{\mathrm{T}}=\left[\hat{w}</em>}, \hat{w<em T="T">{2}, \ldots, \hat{w}</em>\right]$ is a sequence of generated tokens/words. Similarly, for training a T2G model, the cross-entropy loss used in training is simply</p>
<p>$$
\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{CE}}^{\mathrm{G}}=\underset{x</em>\right)\right]
$$}} \sim \mathcal{D}}{\mathbb{E}}\left[-\log p_{\theta}^{\mathrm{T} 2 \mathrm{G}}\left(x_{\mathrm{G}</p>
<p>where $p_{\theta}^{\mathrm{T} 2 \mathrm{G}}\left(x_{\mathrm{G}}\right)$ is the distribution of the generated graph $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{G}}=T</em>$ is drawn
from the marginal distribution of graph samples from $\mathcal{D}$.} 2 \mathrm{G}}\left(x_{\mathrm{T}}\right), T_{\mathrm{T} 2 \mathrm{G}}($.$) being the transfor-$ mation from text to graph, and where $x_{\mathrm{G}</p>
<p>In both Eq. (1) and Eq. (2), $x_{\mathrm{G}}$ must be expressed as a sequence of tokens $t_{j}$ such that a list of triples $x_{\mathrm{G}}$ turns into a list of tokens $\left[t_{1}, t_{2}, \cdots, t_{M}\right]$. This is simply done by adding tokens marking the subject, predicate, and object boundaries in the sequence such that each triple $\left(s^{k}, p^{k}, o^{k}\right)$ is turned into a sequence such as $\left[&lt;\mathrm{S}&gt;, w_{1}^{s},&lt;\mathrm{P}&gt;, w_{1}^{\mathrm{p}}, w_{2}^{\mathrm{p}},&lt;\mathrm{O}&gt;, w_{1}^{o}, w_{2}^{o}, w_{3}^{o}\right]$, assuming our subject is made of 1 token, our predicate of 2 tokens, and our object of 3 tokens in this example. $&lt;\mathrm{S}&gt;,&lt;\mathrm{P}&gt;$, and $&lt;\mathrm{O}&gt;$ are just special marker tokens to help the model know where subject, predicate and objects are located in the sequence.</p>
<p>We start from a pretrained encoder-decoder $\mathcal{M}$ model that we fine-tune on either T2G to get $\mathcal{M}<em _mathrm_G="\mathrm{G">{\mathrm{T}}$, or G2T task to get $\mathcal{M}</em>}}$. We also propose a third kind of model $\mathcal{M<em _mathrm_T="\mathrm{T">{\mathrm{T}+\mathrm{G}}$ to be fine-tuned on both T2G and G2T samples, i.e. the model will learn to generate in any direction, by supplying an input sample $x=\left[x</em>}} ; x_{\mathrm{G}}\right]^{\top}$ and corresponding target for it. Input from each modality is prefixed by a task specific string to distinguish transfer directions ("Text to Graph:" for $x_{\mathrm{T}}$ and "Graph to Text:" for $x_{\mathrm{G}}$ ). For $\mathcal{M<em _mathrm_CE="\mathrm{CE">{\mathrm{T}+\mathrm{G}}$ models, the cross-entropy loss is similarly defined as for Eq. (1) and Eq. (2) such that $\mathcal{L}</em>(x)\right]$. All models are shown in Fig. 2. By convention, we refer to models in this paper by their input modality T, G, or T+G.}}^{\mathrm{T}+\mathrm{G}}=\underset{x \sim \mathcal{D}}{\mathbb{E}}\left[-\log p_{\theta</p>
<h3>3.1 Reinforcement Learning</h3>
<p>Sequence generation can be seen as an agent making sequential decisions of picking words from a given vocabulary. The agent reacts to its environment by accounting for past predictions and getting rewarded along the way, while its state is defined by the partial sequence generated so far. This interpretation enables the reformulation of Seq2Seq generation within the Reinforcement Learning (RL) framework (Sutton and Barto, 2018; Silver, 2015). More precisely, a sequence generation task can be recast as a Markov Decision Process (MDP) where the agent behavior follows a policy $\pi\left(a_{t} \mid s_{t}\right)$. Action $a_{t}$ corresponds to picking a particular word $w_{t}$ at time $t$ from a vocabulary $\mathcal{V}$, conditioned on state $s_{t}$ expressed as the partial sequence generation $s_{t}=\hat{x}<em 1="1">{1: t}=\left[\hat{w}</em>}, \ldots, \hat{w<em t="t">{t}\right]$, that is sequence of words/tokens already picked. $\pi\left(a</em>$ is taken,} \mid s_{t}\right)$ is a stochastic policy that defines a probability distribution of $a_{t}$. Once the action $a_{t</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Specialized and hybrid models rely on the same losses for fine-tuning. However, specialized models are dedicated to a particular generation task while hybrid models can handle both generation directions.
the agent receives a reward $r_{t}=r\left(s_{t}, a_{t}\right)$ before it transitions to the next state $s_{t+1}$. A sequence of actions $a_{1: T}=\left[a_{1}, \ldots, a_{T}\right]$ is selected until the end of generation is reached. The agent aims at maximizing the expectation of cumulative reward</p>
<p>$$
J(\pi)=\mathbb{E}<em t="1">{\tau}\left[\sum</em>[R(\tau)]
$$}^{T} \gamma^{t} r_{t}\right]=\mathbb{E}_{\tau</p>
<p>where $\gamma$ is a discounting factor used to control the horizon of the cumulative reward, $\gamma \in[0,1]$. The expectation is taken over trajectories $\tau$, sequences made of $\left{s_{1}, a_{1}, r_{1}, \ldots, s_{T}, a_{T}, r_{T}\right}$, where $a_{t}$ was chosen from policy $\pi\left(a_{t} \mid s_{t}\right)$. RL provides both on-policy and off-policy approaches to maximize $J(\pi)$ in Eq. (3). We are particularly interested in on-policy techniques that rely on data samples generated from the model to train, especially since our models start from large fine-tuned PLMs that can already generate good samples. This helps avoid the common drawback of on-policy techniques of generating poor samples at first when trained from scratch. These policy-based (Williams, 1992; Zaremba and Sutskever, 2016) and actor-critic based techniques (Bahdanau et al., 2017; Rennie et al., 2017) have been studied for text generation and often update the underlying model with policy gradient (Ranzato et al., 2016; Li et al., 2016; Tan et al., 2019; Paulus et al., 2017). Policy-based methods focus on a parameterized policy $\pi_{\theta}$ where $\theta$ is optimized to maximize $J\left(\pi_{\theta}\right)$. The policy $\pi_{\theta}\left(a_{t} \mid s_{t}\right)$ is the PLM generative model $p_{\theta}$, CE fine-tuned as described at the beginning of Section 3.</p>
<p>REINFORCE, presented by Williams (1992), allows the optimization of a model's parameters $\theta$ by maximizing the expected value of the wordbased reward $R_{w}\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)$ of generated sequence $\hat{x}</em>}}=$ $\left[\hat{w<em T="T">{1}, \ldots, \hat{w}</em>}\right]$. For notation convenience, note that $R_{w}\left(\hat{x<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)=R(\tau)$ since we are now dealing with sequence of words/tokens $\hat{x}</em>\right)$
notation for simplicity. In order to match common Deep Learning conventions, we can minimize a loss expressed as the negative value of the expected cumulative reward:}}$ selected by the actions in trajectory $\tau$. We will also use the $R\left(\hat{x}_{\mathrm{T}</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _left_hat_w="\left[\hat{w">{\mathrm{RL}} &amp; =-\sum</em><em T="T">{1}, \ldots, \hat{w}</em>}\right]} p_{\theta}\left(\hat{w<em T="T">{1}, \ldots, \hat{w}</em>}\right) R_{w}\left(\hat{w<em T="T">{1}, \ldots, \hat{w}</em>\right) \
&amp; =-\mathbb{E}<em 1="1">{\left[\hat{w}</em>}, \ldots, \hat{w<em _theta="\theta">{T}\right] \sim p</em>}} R_{w}\left(\hat{w<em T="T">{1}, \ldots, \hat{w}</em>\right) \
&amp; =-\mathbb{E}<em _mathrm_T="\mathrm{T">{\hat{x}</em>\right)
\end{aligned}
$$}} \sim p_{\theta}} R_{w}\left(\hat{x}_{\mathrm{T}</p>
<p>$R_{w}\left(\hat{x}_{\mathrm{T}}\right)$ is the reward for the generated text which is often associated with non-differentiable metrics such as BLEU, METEOR, chrF, etc. Note that in sequence generation, these metrics-based rewards are available only once a whole sequence is generated, trading sparsity/delay of reward for quality (i.e. we use the full sequence reward, not an estimation of partial future reward). We circumvent the non-differentiability issue by using the REINFORCE policy gradient method:</p>
<p>$$
\nabla_{\theta} \mathcal{L}<em _mathrm_T="\mathrm{T">{\mathrm{RL}} \propto-\left(R\left(\hat{x}</em>\right)
$$}}\right)-b\right) \nabla_{\theta} \log p_{\theta}\left(\hat{x}_{\mathrm{T}</p>
<p>where $b$ is a baseline used to reduce the variance of our gradient estimate. $b$ can be any function, even a random variable, as long as it is independent of the actions taken to generate $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}$, as described in Chapter 13.4 from Sutton and Barto (2018). In SelfCritical Sequence Training (SCST) (Rennie et al., 2017), $b$ is chosen to be the reward of $x</em>$, the output generated by the model by greedy max generation, hence the model serving as its own critic:}}^{*</p>
<p>$$
\nabla_{\theta} \mathcal{L}<em _mathrm_T="\mathrm{T">{\mathrm{SCST}} \propto-\left(R\left(\hat{x}</em>\right)
$$}}\right)-R\left(x_{\mathrm{T}}^{*}\right)\right) \nabla_{\theta} \log p_{\theta}\left(\hat{x}_{\mathrm{T}</p>
<p>where $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}$ is sampled from our model and $x</em>^{}<em>}$ is generated by greedy max. An interesting property of the baseline is that if $R\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)&gt;R\left(x</em>^{}</em>}\right)$, sampled $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}$ has higher reward than $x</em>^{}<em>}$, then the model is updated to reinforce the choices made by this generation. In the opposite case where $R\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)&lt;R\left(x</em>^{}</em>}\right)$,</p>
<p>the model update will take the negative gradient to subdue such generation. When $R\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)=R\left(x</em>^{}<em>}\right)$, no update is performed on the model since the gradient is effectively zeroed out, regardless of the individual values $R\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)$ and $R\left(x</em>^{}</em>}\right)$. This happens when $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}$ and $x</em>\right)$ compares to baseline $b$, the role of $b$ being to reduce the variance of the gradient estimate. Variations around REINFORCE exist on how to apply the gradients, such as MIXER from Ranzato et al. (2016), or on how to evaluate the baseline (Luo, 2020) to minimize the gradient variance.}}^{*}$ are identical (greedy-max and sampled sequences are the same). In that case the sample is lost for RL as no update to the model will result from this sample. Basically, REINFORCE is a Monte Carlo method of learning where a gradient update is applied in the direction decided by how $R\left(\hat{x}_{\mathrm{T}</p>
<p>In our training, PLMs are first fine-tuned using $\mathcal{L}<em _SCST="{SCST" _text="\text">{\text {CE }}$ loss. Once they reach a good generation quality, the training is switched to RL fine-tuning by minimizing $\mathcal{L}</em>$.}</p>
<h2>4 Experimental Setup</h2>
<p>In this Section, we present the experimental setup used for all the results reported in this paper.
Models We used T5 PLMs from Wolf et al. (2020) for our experiments for two distinct models, $t 5$ large ( 770 M parameters) and $t 5$-base ( 220 M parameters), with a special focus on t5-large as it is the best performing of the two on various NLP tasks. Models were fine-tuned to be either specialized on T2G $\left(\mathcal{M}<em _mathrm_G="\mathrm{G">{\mathrm{T}}\right)$ or G2T $\left(\mathcal{M}</em>\right)$.
Data processing Graphs are often represented as list of triples. However our model expects a sequence of input words/tokens to work on. The linearization of graph triples is obviously ambiguous as there are many ways to traverse a graph (Breadth First Search, Depth First Search, random walk, etc.). In practice, we linearize the triples in the order of the list provided by the dataset, but use this inherent linearization ambiguity as an opportunity to do data-augmentation. Indeed, models are first fine-tuned using cross-entropy loss that strongly penalizes generation if it is in any different order than the ground truth order. To avoid the model to overfit to our data and memorize observed triples order, we augment the data by including a few permutations of the graph triples.}}\right)$ task, or to accommodate both directions of generation $\left(\mathcal{M}_{\mathrm{T}+\mathrm{G}</p>
<p>During graph linearization, we encode the subject, predicate, and object positions by using
$&lt;\mathrm{S}\rangle,&lt;\mathrm{P}\rangle,&lt;\mathrm{O}\rangle$ tokens. In practice, we expand the model vocabulary with these special indivisible tokens that are not split during tokenization. No other preprocessing is done on the data for training. We explored masked and span-masked LM fine-tuning to match T5 pretraining (Raffel et al., 2020) which did not lead to any noticeable improvements.</p>
<h3>4.1 Datasets</h3>
<p>WebNLG+ 2020 We report results on WebNLG+ 2020 (v3.0) used in the WebNLG 2020 Challenge (Castro Ferreira et al., 2020b). The Challenge comprises of two tasks: RDF-to-text generation (G2T), and Text-to-RDF semantic parsing (T2G). The Resource Description Framework (RDF) language is used to encode DBpedia and is commonly used in linked data framework. WebNLG+ uses RDF to encode graphs as sets of triples which are associated to one or more lexicalizations of one or more sentences each. Data for English and Russian are provided, but we only worked on the English subset made of 13,211 train, 1,667 dev, 2,155 testA (semantic parsing), and 1,779 testB (data-to-text) samples (triples sets w/ lexicalizations). The data is clustered semantically into 16 categories seen in train and dev sets (Airport, Astronaut, Building, etc.), while 3 categories (Film, Scientist, and Musical-Work) were introduced in test and are unseen, i.e. not present in training; see Castro Ferreira et al. (2020a) for more details. Results are aggregated for all, seen, and unseen categories during evaluation. Note that in the literature, prior works sometimes report 'WebNLG' results on previous dataset version, with completely different performance ranges. We compare all our results to WebNLG+ 2020 (v3.0) numbers reported by Castro Ferreira et al. (2020a) in their Table 6 for G2T, and Table 10 for T2G tasks, using the provided official scoring scripts.
TEKGEN To further study the robustness of our system, we also provide experiments using TEKGEN dataset recently introduced in Agarwal et al. (2021). The graph-sentence alignments are curated using Wikipedia and Wikidata. This serves as a perfect large scale test-bed for both G2T and T2G tasks. Unfortunately, this dataset lacks in entity/relation/object boundaries, which makes it difficult to evaluate systems for T2G tasks. In order to address this issue, we further process the triple-text (with no triple boundaries) to create list of triples using Wikidata properties lookup, via Wikidata</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG G2T</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Team/model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NLTK</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Amazon AI (Shanghai) (Guo et al., 2020a)</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.690</td>
</tr>
<tr>
<td style="text-align: left;">OSU Neural NLG (Li et al., 2020)</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.688</td>
</tr>
<tr>
<td style="text-align: left;">FBConvAI (Yang et al., 2020)</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.686</td>
</tr>
<tr>
<td style="text-align: left;">bt5 (Agarwal et al., 2020)</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.679</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.CE t5-large</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.694</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.RL t5-large</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 6}$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.CE.ES t5-base (early CE)</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.675</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.RL.ES t5-base (early CE)</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.686</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.CE.best t5-base (best CE)</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.677</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.RL.best t5-base (best CE)</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.681</td>
</tr>
</tbody>
</table>
<p>Table 1: G2T Best results on WebNLG 2020 Challenge (v3.0) dataset. The first four rows were the top performers of the Challenge. Results for CE and RL models are presented for our ReGen systems so to show gains from using SCST. Our G2T.RL is the best system overall, fine-tuning a t5-large model using METEOR reward. G2T.RL.ES and G2T.RL.best show the impact of using early stopping (ES) or best CE selection for starting SCST fine-tuning on a t5-base smaller model while using BLEU_NLTK reward.</p>
<p>Query Service. Additionally, we limit the validation set and test set to 5 K and 50 K sentence-triples pairs respectively. Our training split after processing contains 6.3 million sentence-triples pairs. As a contribution to the work, we will present the steps to augment TEKGen dataset with appropriate subject, object and relation boundaries, which enables conventional evaluation of research systems. An example of the processed TEKGen is shown in Fig. 3 in Appendix.
Metrics WebNLG+ 2020 provides automatic metrics to evaluate models. For G2T, we used BLEU, BLEU_NLTK, METEOR, and chrF++ that are provided by the challenge. For T2G, F1, Precision, and Recall scores are utilized and computed for 4 levels of match: Exact, Ent_Type, Partial and Strict as described in Castro Ferreira et al. (2020a), which loosely correspond to different levels of relaxation of how close a match of an entity must be to the ground truth in content and position in a triple. Note that when generating graphs/RDFs, scoring metrics explore all possible permutations of a graph edges. For TEKGen, we use the same metrics as for WebNLG+ 2020.</p>
<h2>5 Results</h2>
<p>For all experiments, PLMs were first exposed to the target datasets (WebNLG+, TEKGEN) by finetuning using $\mathcal{L}<em _SCST="{SCST" _text="\text">{\text {CE }}$ loss. They were then switched to RL training by optimizing the $\mathcal{L}</em>$ loss. Although no exact recipe has been established for}</p>
<p>Seq2Seq RL-training, starting from a good CE model helps RL training performance in practice (Ranzato et al., 2016; Rennie et al., 2017). Therefore, we followed the subsequent simple approach: During fine-tuning, the evaluations are conducted on the validation set. From the CE phase, the best performing model iteration is selected based on the METEOR and F1 score for the G2T and T2G tasks, respectively, to pursue RL fine-tuning. In case of G2T, potential ties in METEOR scores among candidate models, are resolved by using BLEU_NLTK, followed by the chrF++ metric. Note that early stopping selection of CE models led to good performance for t5-base models as well. During the SCST phase, the best model iteration on the validation set is selected and its performance numbers on the test set are reported in our tables.
WebNLG+ 2020 G2T For the WebNLG+ 2020 Challenge, the results of the top four systems for RDF-to-text task can be found in Tab. 1 for all categories (results for seen and unseen categories are given in Tab. 5 in the Appendix), while descriptions the top teams' systems were given in Section 2. We report our G2T results for both t5-large and t5base models as well. For t5-large, ReGen G2T.CE is the best model from CE fine-tuning. ReGen G2T.RL is best model performance for SCST training while using METEOR as reward when starting from G2T.CE model. Tab. 1 shows that our CE model is better than models from all top teams, and the SCST results further improve significantly in</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG T2G <br> Team/model</th>
<th style="text-align: left;">Match</th>
<th style="text-align: left;">F1 $\uparrow$</th>
<th style="text-align: left;">Precision $\uparrow$</th>
<th style="text-align: left;">Recall $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Amazon AI (Shanghai) (Guo et al., 2020a)</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: left;">0.689</td>
<td style="text-align: left;">0.689</td>
<td style="text-align: left;">0.690</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: left;">0.700</td>
<td style="text-align: left;">0.699</td>
<td style="text-align: left;">0.701</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: left;">0.696</td>
<td style="text-align: left;">0.696</td>
<td style="text-align: left;">0.698</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: left;">0.686</td>
<td style="text-align: left;">0.686</td>
<td style="text-align: left;">0.687</td>
</tr>
<tr>
<td style="text-align: left;">bt5 (Agarwal et al., 2020)</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: left;">0.682</td>
<td style="text-align: left;">0.670</td>
<td style="text-align: left;">0.701</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: left;">0.737</td>
<td style="text-align: left;">0.721</td>
<td style="text-align: left;">0.762</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: left;">0.713</td>
<td style="text-align: left;">0.700</td>
<td style="text-align: left;">0.736</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: left;">0.675</td>
<td style="text-align: left;">0.663</td>
<td style="text-align: left;">0.695</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) T2G.CE</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: left;">$\mathbf{0 . 7 2 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 1 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 3 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: left;">$\mathbf{0 . 8 0 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 9 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 3 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: left;">$\mathbf{0 . 7 6 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 5 5}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 8 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: left;">$\mathbf{0 . 7 2 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 1 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 3 5}$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) T2G.RL</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: left;">0.720</td>
<td style="text-align: left;">0.712</td>
<td style="text-align: left;">0.734</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: left;">0.804</td>
<td style="text-align: left;">0.789</td>
<td style="text-align: left;">0.829</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: left;">0.764</td>
<td style="text-align: left;">0.752</td>
<td style="text-align: left;">0.784</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: left;">0.717</td>
<td style="text-align: left;">0.709</td>
<td style="text-align: left;">0.731</td>
</tr>
</tbody>
</table>
<p>Table 2: T2G Best results on WebNLG+ 2020 (v3.0) dataset. The top two teams were the first and second place winner of the Challeneg. Our T2G.CE model improves upon all metrics for all matching schemes, providing a new state-of-the-art results for this Challenge task. T2G.RL models, while still better than previous best results, does not improve upon its CE counterpart.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TEKGEN G2T <br> Model</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ReGen-CE</td>
<td style="text-align: center;">Val</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.400</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.405</td>
</tr>
<tr>
<td style="text-align: left;">ReGen-SCST</td>
<td style="text-align: center;">Val</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.418</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 2}$</td>
</tr>
</tbody>
</table>
<p>Table 3: G2T Results for TeKGen dataset. ReGen-CE establishes a baseline on this dataset. ReGen-SCST consistently improve on the baseline on all metrics, for validation and test sets.
all metrics achieving state-of-the-art results to our knowledge. The gain obtained by SCST alone is quite significant and demonstrates the benefits of RL fine-tuning for this task. We report our best model results in Tab. 1, as well as mean and standard deviation results for multiple random number generator seeds in Tab. 10 in Appendix. When averaging results for few seeded models, sustained gains from SCST are seen for all metrics.</p>
<p>Multiple reward candidates were investigated (BLEU, BLEU_NLTK, METEOR, chrF) as well as some linear combinations of pairs of them, as can be seen in Tab. 7 in Appendix. In Tab. 7, for t5-large, METEOR is consistently the best SCST reward, and improves all the other metrics scores as well. However, for 'smaller' models such as
t5-base, BLEU_NLTK is revealed to be the best reward for improving BLEU performance as expected. Again, SCST brings significant gains across all the metrics in that case. Note that for t5-base model, selecting a METEOR reward improves METEOR results significantly as reported in Tab. 9 in Appendix.</p>
<p>Another interesting fact is that early stopping of CE model G2T.CE.ES (at 5 epochs) leads to the best SCST model G2T.RL.ES for t5-base, while selecting the best CE model G2T.CE.best (at 11 epochs) still showed some gains from SCST model G2T.RL.best. SCST needs a good starting point, but a better CE model that has seen a lot more epochs of our dataset maybe harder for SCST to stir in a better solution in the parameter space.</p>
<p>Moreover, the test split contains unseen categories not present in the validation dataset which render choices based on validation sub-optimal for the test dataset. The best models we report in this work are specialized models $\mathcal{M}_{\mathrm{G}}$. Early in our investigation, hybrid models were the best performing model for G2T reaching 0.547 BLEU, 0.543 BLEU_NLTK and 0.417 METEOR, and first to beat the Challenge winning team. However, when batch size became larger (20-24 samples), the specialized models took the lead and retain it still.</p>
<p>For training, we optimized all our models using AdamW (Loshchilov and Hutter, 2017), variant of the Adam optimizer with default values of $\beta=[0.9,0.999]$ and weight decay of $10^{-2}$. For learning rate, we used $5.10^{-6}$ for all our experiments as it was better than $10^{-5}$ and $10^{-6}$ as seen in Tab. 8 in Appendix. All our models were trained with 20-24 minibatch size on WebNLG. Further details on our experimental setup are provided in the Appendix in Section A.
WebNLG+ 2020 T2G Results for the Text-to-RDF task are reported in Tab. 2 for all categories. Results for our best model on seen and unseen categories are given in Tab. 6 in Appendix. Amazon AI and bt5 are the top performing teams. Again, the proposed ReGen T2G.CE model shows strong results that are better in term of all metrics, for all matching categories. In themselves, these numbers are a de-facto new state-of-the-art for this dataset, as far as we know. SCST model T2G.RL fails to improve on this model though. The exact F1 metric was used as reward, but the model could never pull ahead of the CE model in our experiments. The exact F1 metric may not be a strong enough reward to really capture the dynamics of graph generation properly for WebNLG+ as it is very rigid in its measure (one must have an exact match), although the same reward gave good results on our second dataset TEXGEN. A more sensitive metric could possibly help. We even tried to use n-gram based metrics (like BLEU) but to no avail. We further address this issue at the end on this Section.
TEKGen G2T For the TEXGEN dataset, we present our results on Graph-to-Text generation in Tab. 3. Similar to the experiments in WebNLG+, we pick the best model during the CE fine-tuning based on the METEOR score and proceed with the RL fine-tuning. We observe that the RL fine-tuning step helps boost the test split scores on all metrics. It is worth noting that the scores are slightly under-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">T2G <br> Model</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">$\mathrm{F} 1 \uparrow$</th>
<th style="text-align: left;">$\mathrm{P} \uparrow$</th>
<th style="text-align: left;">$\mathrm{R} \uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Val</td>
<td style="text-align: left;">0.622</td>
<td style="text-align: left;">0.608</td>
<td style="text-align: left;">0.647</td>
</tr>
<tr>
<td style="text-align: left;">ReGen-CE</td>
<td style="text-align: left;">Test</td>
<td style="text-align: left;">0.619</td>
<td style="text-align: left;">0.605</td>
<td style="text-align: left;">0.643</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Val</td>
<td style="text-align: left;">0.615</td>
<td style="text-align: left;">0.600</td>
<td style="text-align: left;">0.640</td>
</tr>
<tr>
<td style="text-align: left;">ReGen-SCST</td>
<td style="text-align: left;">Test</td>
<td style="text-align: left;">$\mathbf{0 . 6 2 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 6 1 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 6 4 7}$</td>
</tr>
</tbody>
</table>
<p>Table 4: T2G TEXGEN Results: ReGen-CE establishes a baseline of the dataset. ReGen-SCST improves results on the test set compared to ReGen-CE.
estimating the potential of our system because of the nature of the sentences in the TEXGEN dataset. Unlike WebNLG+, in a paired text-graph sample in TEXGEN, the linearized graph does not usually cover all the concepts described in the corresponding text. This leads to underestimating when the hypothesis is scored against the reference using n-gram metrics.
TEKGen T2G Results for the Text-to-Graph for TEXGEN are reported in Tab. 4. Once the CE finetuning is done, we continue with the RL fine-tuning using exact F1 as reward. The performance is consistent with what we observe in G2T task for TEXGEN, where SCST step boosts the performance of the model. Since, we reformulate this dataset (refer Section 4.1) to offer as T2G and G2T tasks, our approach is the first attempt in understanding the nature of TEXGEN dataset and our methods provide a baseline for future research. Please note that for both T2G and G2T tasks in TEXGEN, we only start a t5-large PLM.
Summary Results on WebNLG+ 2020 and TEXGEN demonstrated that RL fine-tuning of models leads to significant improvements of results for T2G and G2T, establishing new state-of-the-art results for both tasks. For WebNLG+, T2G was a challenging task for RL fine-tuning. In further work, we plan to address this issue by investigating two points: First, look into a more sensible graphdependent sampling for graph structures, rather than the current multinomial sampling of the best tokens at each generation step. Second, try a different reward schemes where the reward is more attuned to the challenges of graph generation as well as graph structure, allowing for some curriculum learning, or increasing the harshness of rewards gradually during training. Results on TEXGEN showed that RL fine-tuning is a viable option even on large-scale datasets. To enrich this quantitative</p>
<p>study of ReGen, we provide a few qualitative cherry picked results in Tab. 11 and Tab. 12 in Appendix.</p>
<h2>6 Conclusions</h2>
<p>In this paper, we proposed to use RL for improving upon current generation for text-to-graph and graph-to-text tasks for the WebNLG+ 2020 Challenge dataset using pre-trained LMs. We not only defined a novel Seq2Seq training of models in T2G and G2T generation tasks, but we established state-of-the-art results for WebNLG+ for both tasks, significantly improving on the previously published results. We provided extensive analyses of our results and of the steps taken to reach these improvements. We then expanded our approach to large scale training by means of TEXGEN where we demonstrated that RL fine-tuning provides a robust way to improve upon regular model finetuning within a dataset that is orders of magnitude larger than the WebNLG+ starting point. We established gains despite a weaker content overlap in text-graph data pairs for TEXGEN. Along the way, we constructed subject, and relation-object boundaries from TEXGEN sentence-triples pairs that we plan on releasing to benefit the research community.</p>
<p>Future work will focus on developing a variant of SCST that leverages the unique structure of graph by either performing of more sensible graphdependent sampling, or by investigating different reward schemes more attuned to integrating the content and structure of graphs.</p>
<h2>7 Broader Impact Statement</h2>
<p>The techniques proposed in this paper are inherently dependent on the training data and the PLMs used for fine-tuning on this data. The models do benefit from the large amount of data seen by the PLM they are derived from, however it is fair to assume that any detectable bias in the original data or PLMs would most likely be transferred to the text-to-graph and graph-to-text generative models. This is something to keep in mind when building these generative models. Public datasets were used for all experiments. The TEXGEN with recreated boundaries does not change the underlying data and should not add any further noise nor bias to the original data.</p>
<h2>References</h2>
<p>Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. 2021. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3554-3565, Online. Association for Computational Linguistics.</p>
<p>Oshin Agarwal, Mihir Kale, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. 2020. Machine translation aided bilingual data-to-text generation and semantic parsing. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 125-130, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C. Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020a. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 55-76, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina, editors. 2020b. Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). Association for Computational Linguistics, Dublin, Ireland (Virtual).</p>
<p>Yu Chen, Lingfei Wu, and Mohammed J. Zaki. 2020. Reinforcement learning based graph-to-sequence model for natural question generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Pierre Dognin, Igor Melnyk, Inkit Padhi, Cicero Nogueira dos Santos, and Payel Das. 2020. DualTKB: A Dual Learning Bridge between Text and Knowledge Base. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8605-8616, Online. Association for Computational Linguistics.</p>
<p>Qipeng Guo, Zhijing Jin, Ning Dai, Xipeng Qiu, Xiangyang Xue, David Wipf, and Zheng Zhang. 2020a. $i^{2}$ : A plan-and-pretrain approach for knowledge</p>
<p>graph-to-text generation. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 100-106, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, and Zheng Zhang. 2020b. CycleGT: Unsupervised graph-to-text and text-to-graph generation via cycle training. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 7788, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, and David Wipf. 2021. Fork or fail: Cycle-consistent training with many-to-one mappings. In The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pages 1828-1836. PMLR.</p>
<p>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228-231, Prague, Czech Republic. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep reinforcement learning for dialogue generation.</p>
<p>Xintong Li, Aleksandre Maskharashvili, Symon Jory Stevens-Guille, and Michael White. 2020. Leveraging large pretrained models for WebNLG 2020. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 117-124, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Ilya Loshchilov and Frank Hutter. 2017. Fixing weight decay regularization in adam. CoRR, abs/1711.05101.</p>
<p>Ruotian Luo. 2020. A better variant of self-critical sequence training.</p>
<p>San Pa Pa Aung, Win Pa Pa, and Tin Lay Nwe. 2020. Automatic Myanmar image captioning using CNN and LSTM-based language model. In Proceedings of the 1st Joint Workshop on Spoken</p>
<p>Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL), pages 139-143, Marseille, France. European Language Resources association.</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization.</p>
<p>Maja Popović. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pages 612-618, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level training with recurrent neural networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1179-1195. IEEE Computer Society.</p>
<p>Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation.</p>
<p>David Silver. 2015. Lectures on reinforcement learning. URL: https://www.davidsilver.uk/ teaching/.</p>
<p>Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA.</p>
<p>Bowen Tan, Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric Xing. 2019. Connecting the dots between mle and rl for sequence prediction.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,</p>
<p>Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Zixiaofan Yang, Arash Einolghozati, Hakan Inan, Keith Diedrick, Angela Fan, Pinar Donmez, and Sonal Gupta. 2020. Improving text-to-text pretrained models for the graph-to-text task. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 107-116, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Wojciech Zaremba and Ilya Sutskever. 2016. Reinforcement learning neural turing machines - revised.</p>
<h2>A Training Setup</h2>
<p>All our experiments were run using NVIDIA V100 GPUs for training and validation, some trainings were done on A100. We distributed our training to 2-4 GPUs depending on availability. Each training epoch for CE ranged from 30 minutes to 1 hour depending on number of GPUs utilized.</p>
<p>Validation and testing (1,779 and 2,155 samples for testA and testB of WebNLG+ 2020) lasted from 40 minutes to 1 hour depending on machines. Computation was dominated by beam search generation as we used beam search with beam size of 5 and a max sequence length of 192 (since linearized graph sequence can be quite long). We used the official scoring scripts released by WebNLG+ 2020 Challenge to score all our experiments. The evaluation of graph being the most computationally expensive as all possible matching combinations are tested in what looks like a factorial complexity, taking scoring of set of triples larger than 8 from impractical to not feasible.</p>
<p>All our models were built using PyTorch. Total effective batch sizes were set to either 20 or 24 samples for our distributed training. We adjusted the batch size on each worker to ensure consistent global batch size of 20 or 24.</p>
<p>We did some search on learning rates for t5large training and SCST rewards, see discussion and results in Section C.</p>
<p>All our trainings have a seeded random number generator for reproducibility. We also report results on WebNLG+ 2020 G2T tasks for each training setup by showing results for 3 models from different seeds, and provide means and standard deviations of these results in Tab. 10.</p>
<h2>B WebNLG+ 2020 Results per Categories for Best G2T and T2G Models</h2>
<p>In Tab. 5, we are reporting results for all WebNLG+ 2020 categories for our best CE and RL models. While results for unseen categories are much worse than for seen categories, RL fine-tuning manages to improve on both seen and unseen categories.</p>
<p>Tab. 6 provides results for seen, unseen and all categories for our best CE model ReGen T2G.CE which established state-of-the-art results on T2G task of WebNLG+ 2020 Challenge dataset.</p>
<h2>C Ablation Studies</h2>
<p>In Tables 7 and 8 we present ablation studies of different optimized metrics and learning rates for SCST training. As can be seen from Table 7, when METEOR is used as a reward, we get the best performance across all the metrics. We also tried using a combination of multiple rewards with different scaling but did not get any gain over the single metric rewards. In Table 8. we also show the effect of learning rate on SCST performance. Using $l r=5 \cdot 10^{-6}$ gave us the best performance, while higher rates, such as $10^{-4}$, led to unstable training and collapse of SCST.</p>
<h2>D G2T Results t5-base models for SCST with METEOR Reward</h2>
<p>Results for SCST fine-tuning of t5-base models using a METEOR reward are compiled in Tab. 9. Clearly, these models achieve better METEOR results as expected since they are RL optimized on this metric.</p>
<h2>E G2T Results for Models from Multiple Random Seeds</h2>
<p>All our training have a seeded random number generator for reproducibility. We also report the mean and standard deviations for all our G2T models. Each model setup was run 3 times using three independent and distinct seeds, following the same exact process. This is to ensure that our results are not just the product of a lucky system configuration or otherwise advantageous random shuffling of our training dataset. All results are reported in Tab. 10.</p>
<p>The gain reported between CE and RL for our t5large models are clearly still showing after average of all 3 models from distinct random seeds. For t5-base, gains between CE and RL are still present, albeit smaller than for our best systems.</p>
<h2>F Generation Examples for G2T and T2G</h2>
<p>We present some cherry-picked examples for G2T in Tab. 12 and for T2G in Tab. 11 for both WebNLG and TekGen datasets.</p>
<h2>G Processed TekGen Dataset</h2>
<p>In Fig. 3 we show an example of our processing of TEXGen dataset in establishing subject, relation, object boundaries. This enables both training and evaluating systems for T2G and G2T tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG G2T Best Models</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours t5-large ReGen-CE</td>
<td style="text-align: center;">unseen</td>
<td style="text-align: center;">48.76</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.653</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">59.73</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.722</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">55.26</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.694</td>
</tr>
<tr>
<td style="text-align: left;">Ours t5-large ReGen-SCST</td>
<td style="text-align: center;">unseen</td>
<td style="text-align: center;">49.06</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.665</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">61.22</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.734</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">56.25</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.706</td>
</tr>
</tbody>
</table>
<p>Table 5: G2T: Results for seen, unseen, and all categories subsets in WebNLG+ 2020 Challenge Test dataset. As expected, unseen categories much worse results than for seen categories. RL fine-tuning manages to improve on both seen and unseen categories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG T2G <br> ReGen T2G.CE</th>
<th style="text-align: left;">Match</th>
<th style="text-align: center;">F1 $\uparrow$</th>
<th style="text-align: center;">Precision $\uparrow$</th>
<th style="text-align: center;">Recall $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">unseen</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">0.5809</td>
<td style="text-align: center;">0.5662</td>
<td style="text-align: center;">0.6069</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: center;">0.7014</td>
<td style="text-align: center;">0.6741</td>
<td style="text-align: center;">0.7497</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: center;">0.6453</td>
<td style="text-align: center;">0.6241</td>
<td style="text-align: center;">0.6826</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: center;">0.5754</td>
<td style="text-align: center;">0.5608</td>
<td style="text-align: center;">0.6012</td>
</tr>
<tr>
<td style="text-align: left;">seen</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">0.8322</td>
<td style="text-align: center;">0.8286</td>
<td style="text-align: center;">0.8384</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: center;">0.8878</td>
<td style="text-align: center;">0.8811</td>
<td style="text-align: center;">0.8998</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: center;">0.8604</td>
<td style="text-align: center;">0.8553</td>
<td style="text-align: center;">0.8696</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: center;">0.8317</td>
<td style="text-align: center;">0.8282</td>
<td style="text-align: center;">0.8379</td>
</tr>
<tr>
<td style="text-align: left;">all</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">0.7229</td>
<td style="text-align: center;">0.7144</td>
<td style="text-align: center;">0.7376</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: center;">0.8067</td>
<td style="text-align: center;">0.7910</td>
<td style="text-align: center;">0.8345</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: center;">0.7668</td>
<td style="text-align: center;">0.7547</td>
<td style="text-align: center;">0.7882</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: center;">0.7202</td>
<td style="text-align: center;">0.7118</td>
<td style="text-align: center;">0.7349</td>
</tr>
</tbody>
</table>
<p>Table 6: T2G: Results for seen, unseen, and all categories subsets in WebNLG+ 2020 Challenge Test dataset. As expected the performance drops significantly for unseen categories and are the best for seen categories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">SCST Reward</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.698</td>
</tr>
<tr>
<td style="text-align: left;">BLEU NLTK</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.700</td>
</tr>
<tr>
<td style="text-align: left;">METEOR</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 6}$</td>
</tr>
<tr>
<td style="text-align: left;">chrF++</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.701</td>
</tr>
<tr>
<td style="text-align: left;">$1 / 2$-METEOR+ $1 / 2$-BLEU NLTK</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.699</td>
</tr>
<tr>
<td style="text-align: left;">$2 / 3$-METEOR+ $1 / 3$-BLEU NLTK</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.697</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study of metrics used as rewards in SCST for t5-large models. The results shown are on the test split.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning Rate</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$10^{-6}$</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.698</td>
</tr>
<tr>
<td style="text-align: center;">$5 \cdot 10^{-6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 0}$</td>
</tr>
<tr>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.696</td>
</tr>
</tbody>
</table>
<p>Table 8: Ablation study on learning rates in SCST (using BLEU NLTK as the optimized metric)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG G2T</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Team/model</td>
<td style="text-align: center;">NLTK</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.ES.meteor t5-base (early CE)</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.689</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.best.meteor t5-base (best CE)</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.681</td>
</tr>
</tbody>
</table>
<p>Table 9: G2T: Best results for t5-base fine-tuned with SCST using METEOR as reward.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team Name</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ReGen G2T.CE t5-large</td>
<td style="text-align: center;">$0.543 \pm 0.007$</td>
<td style="text-align: center;">$0.540 \pm 0.007$</td>
<td style="text-align: center;">$0.416 \pm 0.002$</td>
<td style="text-align: center;">$0.691 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL t5-large</td>
<td style="text-align: center;">$0.553 \pm 0.007$</td>
<td style="text-align: center;">$0.550 \pm 0.007$</td>
<td style="text-align: center;">$0.422 \pm 0.002$</td>
<td style="text-align: center;">$0.702 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.CE.ES t5-base (early CE)</td>
<td style="text-align: center;">$0.521 \pm 0.004$</td>
<td style="text-align: center;">$0.517 \pm 0.004$</td>
<td style="text-align: center;">$0.404 \pm 0.001$</td>
<td style="text-align: center;">$0.675 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.ES t5-base (early CE)</td>
<td style="text-align: center;">$0.528 \pm 0.007$</td>
<td style="text-align: center;">$0.523 \pm 0.007$</td>
<td style="text-align: center;">$0.408 \pm 0.002$</td>
<td style="text-align: center;">$0.682 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.CE.best t5-base (best CE)</td>
<td style="text-align: center;">$0.524 \pm 0.000$</td>
<td style="text-align: center;">$0.520 \pm 0.001$</td>
<td style="text-align: center;">$0.404 \pm 0.000$</td>
<td style="text-align: center;">$0.670 \pm 0.000$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.best t5-base (best CE)</td>
<td style="text-align: center;">$0.525 \pm 0.007$</td>
<td style="text-align: center;">$0.522 \pm 0.007$</td>
<td style="text-align: center;">$0.407 \pm 0.002$</td>
<td style="text-align: center;">$0.681 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.ES.meteor t5-base (early CE)</td>
<td style="text-align: center;">$0.525 \pm 0.007$</td>
<td style="text-align: center;">$0.521 \pm 0.007$</td>
<td style="text-align: center;">$0.412 \pm 0.002$</td>
<td style="text-align: center;">$0.687 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.best.meteor t5-base (best CE)</td>
<td style="text-align: center;">$0.527 \pm 0.007$</td>
<td style="text-align: center;">$0.524 \pm 0.007$</td>
<td style="text-align: center;">$0.410 \pm 0.002$</td>
<td style="text-align: center;">$0.686 \pm 0.003$</td>
</tr>
</tbody>
</table>
<p>Table 10: Results means and standard deviations (SD), shown as mean $\pm \mathrm{SD}$, for CE and SCST trained models (including our best results model) for a total of 3 different random number generator seeds used in training.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example from the processed TEXGEN dataset. The original dataset lacks KG boundaries, which makes it difficult to evaluate T2G systems efficiently.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Sentence / Graph</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">The Pontiac Rageous began and ended its production in 1997 on an assembly line in Detroit, a city in Michigan.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Pontiac_Rageous $\diamond$ productionStartYear $\diamond 1997 \diamond$ Pontiac_Rageous $\diamond$ assembly $\diamond$ Michigan $\diamond$ Pontiac_Rageous $\diamond$ assembly $\diamond$ Detroit $\diamond$ Pontiac_Rageous $\diamond$ productionEndYear $\diamond 1997 \diamond$ Detroit $\diamond$ type $\diamond$ City_(Michigan)</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Pontiac_Rageous $\diamond$ assembly $\diamond$ Detroit $\diamond$ Pontiac_Rageous $\diamond$ modelYears $\diamond 1997 \diamond$ Pontiac_Rageous $\diamond$ modelYears $\diamond 1997 \diamond$ Detroit $\diamond$ isPartOf $\diamond$ Michigan</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Pontiac_Rageous $\diamond$ assembly $\diamond$ Detroit $\diamond$ Pontiac_Rageous $\diamond$ modelYears $\diamond 1997 \diamond$ Pontiac_Rageous $\diamond$ assembly $\diamond$ Michigan</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">In the United States, where Abraham A, Ribicoff was born, African Americans are one of the ethnic groups. Abraham A. Ribicoff was married to Ruth Ribicoff.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Abraham_A._Ribicoff $\diamond$ spouse $\diamond$ "Ruth Ribicoff" $\diamond$ Abraham_A._Ribicoff $\diamond$ birthPlace $\diamond$ United_States $\diamond$ United_States $\diamond$ ethnicGroup $\diamond$ African_Americans $\diamond$ Abraham_A._Ribicoff $\diamond$ nationality $\diamond$ United_States</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Abraham_A._Ribicoff $\diamond$ birthPlace $\diamond$ United_States $\diamond$ Abraham_A._Ribicoff $\diamond$ spouse $\diamond$ "Ruth Ribicoff" $\diamond$ United_States $\diamond$ ethnicGroup $\diamond$ African_Americans</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Abraham_A._Ribicoff $\diamond$ birthPlace $\diamond$ United_States $\diamond$ Abraham_A._Ribicoff $\diamond$ spouse $\diamond$ "Ruth Ribicoff" $\diamond$ Abraham_A._Ribicoff $\diamond$ nationality $\diamond$ American $\diamond$ United_States $\diamond$ ethnicGroup $\diamond$ African_Americans</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Super Capers, edited by Stacy Katzman, is a 98 minute film starring Michael Rooker and Tom Sizemore.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Super_Capers $\diamond$ editing $\diamond$ Stacy_Katzman $\diamond$ Super_Capers $\diamond$ starring $\diamond$ Michael_Rooker $\diamond$ Super_Capers $\diamond$ starring $\diamond$ Tom_Sizemore $\diamond$ Super_Capers $\diamond$ runtime 198.0</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Super_Capers $\diamond$ starring $\diamond$ Tom_Sizemore $\diamond$ Super_Capers $\diamond$ timeOut $\diamond$ "980.0"(minutes) $\diamond$ Super_Capers $\diamond$ starring $\diamond$ Michael_Rooker $\diamond$ Super_Capers $\diamond$ editor $\diamond$ Stacy_Katzman</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Super_Capers $\diamond$ starring $\diamond$ Tom_Sizemore $\diamond$ Super_Capers $\diamond$ length $\diamond 98.0$ (minutes) $\diamond$ Super_Capers $\diamond$ starring $\diamond$ Michael_Rooker $\diamond$ Super_Capers $\diamond$ editor $\diamond$ Stacy_Katzman</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Doctor George Cary (1611-1680), Professor of Sacred Theology, lord of the manor of Clovelly, Devon, was Dean of Exeter between 1663 and 1680 (amongst other duties responsible for the maintenance and decoration of Exeter Cathedral).</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">George Cary (1611-1680) $\diamond$ position held $\diamond$ Dean of Exeter $\diamond$ start time $\diamond 01$ January 1663 $\diamond$ date of birth $\diamond 001611 \diamond$ date of death $\diamond 001680$</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">George Cary (priest) $\diamond$ date of birth $\diamond 01$ January 1611 $\diamond$ date of death $\diamond 01$ January 1680</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">George Cary (priest) $\diamond$ position held $\diamond$ Dean of Exeter $\diamond$ date of birth $\diamond 01$ January 1611 $\diamond$ date of death $\diamond 01$ January 1680</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Early general elections were held in the Bahamas on 10 April 1968.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">1968 Bahamian general election $\diamond$ point in time $\diamond 10$ April 1968 $\diamond$ country $\diamond$ The Bahamas $\diamond$ applies to jurisdiction $\diamond$ The Bahamas</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">1968 Bahamian general election $\diamond$ point in time $\diamond 10$ April 1968</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">1968 Bahamian general election $\diamond$ point in time $\diamond 10$ April 1968 $\diamond$ country $\diamond$ The Bahamas</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">The school was established on 6 January 1930, by former education minister, CWW Kannangara, who additionally founded two other colleges located in central Ceylon.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Kattankudy Central College $\diamond$ instance of $\diamond$ School</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Government Polytechnic, Colombo $\diamond$ inception $\diamond 001930$</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Government Polytechnic, Colombo $\diamond$ inception $\diamond 001930 \diamond$ instance of $\diamond$ School</td>
</tr>
</tbody>
</table>
<p>Table 11: Few cherry-picked generation for T2G task for WebNLG+ 2020 (top three) and TEXGEN (bottom three). For each source (Text), we show the ground truth (Gold) and system generated hypothesis from the best CE (Hyp-CE) and SCST models (Hyp-SCST). Note that the set of triples in WebNLG+ takes the form $x_{0}=$ $\left[\left(s^{1} \diamond p^{1} \diamond o^{1}\right), \ldots,\left(s^{K} \diamond p^{K} \diamond o^{K}\right)\right]$, whereas the same for TEXGEN is of form $x_{0}=\left[s \diamond\left(p^{1} \diamond o^{1}\right), \ldots,\left(p^{K} \diamond o^{K}\right)\right]$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Graph / Sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">McVeagh_of_the_South_Seas $\diamond$ starring $\diamond$ Harry_Carey_(actor_born_1878) $\diamond$ McVeagh_of_the_South_Seas $\diamond$ writer $\diamond$ Harry_Carey_(actor_born_1878)</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Born in 1878, Harry Carey later grew up to write and star in the movie McVeagh of the South Seas. Harry Carey, born in 1878, wrote and appeared in the movie McVeagh of the South Seas. Harry Carey, who was born in 1878, wrote and starred the film of McVeagh of the South Seas.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">McVeagh of the South Seas was written by Harry Carey, who was born in 1878.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">McVeagh of the South Seas was written by Harry Carey and starred the actor Harry Carey who was born in 1878 .</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Aleksandr_Prudnikov $\diamond$ height $\diamond 185.0$ (centimetres) $\diamond$ Aleksandr_Prudnikov $\diamond$ youthclub $\diamond$</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">FC_Spartak_Moscow $\diamond$ FC_Spartak_Moscow $\diamond$ ground $\diamond$ Otkrytiye_Arena <br> Aleksandr Prudnikov, 185 cm tall played for FC Spartak Moscow's youth team. FC Spartak Moscow is based in the Otkrytiye Arena. Aleksandr Prudnikov who is 185 cm tall is a member of the youth side of FC Spartak Moscow. The home ground of FC Spartak Moscow is Otkrytiye Arena. Aleksandr Prudnikov is 185.0 cm tall and played for the FC Spartak Moscow at the Otkrytiye Arena.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Aleksandr Prudnikov is 185 cm tall and played for FC Spartak Moscow's youth team at the Otkrytiye Arena.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Aleksandr Prudnikov is 185 cm tall and played for the youth team of FC Spartak Moscow whose home ground is the Otkrytiye Arena.</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Baku_Turkish_Martyrs'_Memorial $\diamond$ nativeName $\diamond$ "Türk Şehitleri Anitı" $\diamond$</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Baku_Turkish_Martyrs'_Memorial $\diamond$ location $\diamond$ Azerbaijan <br> The Native name of the Baku Turkish Martyrs' Memorial is "Türk Şehitleri Anıtı" which is located in Azerbaijan. The native name of the Baku Turkish Martyrs' Memorial is Türk Şehitleri Anıtı located in Azerbaijan. The native name for the Baku Turkish Martyrs' Memorial is Türk Şehitleri Anıtı, which is located in Baku, Azerbaijan.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">The native name of the Baku Turkish Martyrs' Memorial in Azerbaijan is Türk Şehitleri Anıtı.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">The Baku Turkish Martyrs' Memorial is located in Azerbaijan and is known locally as Türk Şehitleri Anıtı.</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">John Banister (anatomist) $\diamond$ occupation $\diamond$ Surgeon $\diamond$ date of birth $\diamond 01$ January $1533 \diamond$ date of death $\diamond 01$ January 1610</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">John Banister (1533-1610) was an English anatomist, surgeon and teacher.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">John Banister (1533-1610) was an English surgeon.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">John Banister (1533-1610) was an English surgeon and anatomist.</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">WNPT (TV) $\diamond$ country $\diamond$ United States $\diamond$ instance of $\diamond$ Television station</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">WNPT, virtual channel 8 (VHF digital channel 7), is a PBS member television station licensed to Nashville, Tennessee, United States.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">WNPT, virtual channel 3 (UHF digital channel 15), is a Fox-affiliated television station licensed to Portland, Oregon, United States.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">WNPT, virtual channel 4 (UHF digital channel 16), is a Public Broadcasting Service (PBS) member television station licensed to Portland, Oregon, United States.</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Our Lady of the Presentation Cathedral, Natal $\diamond$ inception $\diamond 21$ November 1988</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Our Lady of the Presentation Cathedral, Natal was inaugurated on November 21, 1988, and is located in the district of Cidade Alta in Natal, capital of the Brazilian state of Rio Grande do Norte.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Our Lady of the Presentation Cathedral, Natal was built in 1988.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Our Lady of the Presentation Cathedral, Natal was consecrated on 21 November 1988.</td>
</tr>
</tbody>
</table>
<p>Table 12: Few cherry-picked generation for G2T task for WebNLG+ 2020 (top three) and TeKGEN (bottom three). For each source (Graph), we show the ground truth (Gold) and system generated hypothesis from the best CE (Hyp-CE) and SCST models (Hyp-SCST). Note that the set of triples in WebNLG+ 2020 takes the form $x_{0}=$ $\left[\left(s^{1} \diamond p^{1} \diamond o^{1}\right), \ldots,\left(s^{K} \diamond p^{K} \diamond o^{K}\right)\right]$, whereas the same for TeKGEN is of form $x_{0}=\left[s \diamond\left(p^{1} \diamond o^{1}\right), \ldots,\left(p^{K} \diamond o^{K}\right)\right]$</p>            </div>
        </div>

    </div>
</body>
</html>
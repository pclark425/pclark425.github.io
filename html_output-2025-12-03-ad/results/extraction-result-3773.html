<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3773 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3773</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3773</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-cccb129527fbe900c6f94dd8a6ad4b408c403152</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cccb129527fbe900c6f94dd8a6ad4b408c403152" target="_blank">Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study</a></p>
                <p><strong>Paper Venue:</strong> Journal of Medical Internet Research</p>
                <p><strong>Paper TL;DR:</strong> Large language models such as GPT-4 have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews.</p>
                <p><strong>Paper Abstract:</strong> Background The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources. Objective This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers. Methods We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts. Results Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. The interrater variability between 2 independent human screeners was κ=0.46, and the prevalence and bias-adjusted κ between our proposed methods and the consensus-based human decisions was κ=0.96. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications. Conclusions Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3773.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3773.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / Chat GPT (OpenAI GPT API)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pretrained Transformer (OpenAI GPT and GPT-4) used via the OpenAI Chat GPT / GPT-4 API</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of large transformer-based language models provided by OpenAI; in this paper they are used via prompt-based API calls to classify titles and abstracts for inclusion/exclusion in clinical systematic/scoping reviews, and to provide natural-language explanations and reflections for decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT / GPT-4 (Chat GPT API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based pretrained language models (GPT family) accessed via OpenAI's Chat GPT / GPT-4 API; the paper does not report architecture details (parameter counts) or task-specific fine-tuning — models are used in a zero-shot / prompt-based manner with plain-language screening instructions and occasional appended prompts requesting reasoning/reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Clinical and biomedical literature (titles and abstracts for systematic and scoping reviews across multiple medical topics such as COVID-19 therapeutics, nonopioid analgesia, Raynaud's, and LLM research in clinical settings).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>24,307 titles/abstracts across 6 real-world review data sets (paper reports screening >24,000 records; specific dataset counts presented in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Decision / screening rules and classification heuristics (operational inclusion/exclusion rules for study selection); provides natural-language rationales but not formalized scientific 'laws'.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompting / zero-shot classification via API: a fixed plain-language instruction prompt containing inclusion and exclusion criteria was concatenated with each title/abstract and sent to the API; additional prompts requested explanations and reflections on incorrect classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to human ground-truth labels created by at least two independent reviewers with conflict resolution; metrics reported included accuracy, macro F1-score, sensitivity (included/excluded), Cohen's κ, and prevalence-and-bias-adjusted κ (PABAK); selected model rationales and reflexive corrections were inspected qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The GPT-based workflow achieved overall accuracy ≈ 0.91, macro F1 ≈ 0.60, sensitivity for included papers ≈ 0.76 and for excluded papers ≈ 0.91; the model could provide natural-language reasoning and, on a subset, revise incorrect initial classifications when prompted to reflect. The system functioned as an effective first-pass screen but missed some included records.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Model showed lower sensitivity for including relevant papers than for excluding irrelevant ones; potential for missing important studies if used fully autonomously; class imbalance affected kappa; reliance on prompt engineering and token-length constraints; no task-specific fine-tuning performed; human oversight recommended; potential costs and runtime (NOA dataset cost ~US$25, runtime ~643 minutes reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3773.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3773.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrialStreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrialStreamer (automated system for extracting clinical trial information)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated system that extracts key elements from clinical trial reports and can infer which interventions work best across sets of full texts indexed on MEDLINE; referenced as an example of systems that extract evidence across many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trialstreamer: a living, automatically updated database of clinical trial reports.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TrialStreamer (system described in reference literature; not an LLM per se)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An automated pipeline/system that ingests trial reports (MEDLINE-indexed), extracts key trial metadata and results, and supports inference of comparative intervention performance; the paper references TrialStreamer as prior work but does not detail its internal model architectures here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Clinical trial literature (MEDLINE-indexed clinical trial reports)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified in this paper (described in referenced TrialStreamer work as operating over MEDLINE-indexed trial reports; the current paper does not provide a numeric corpus size).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Inferred treatment-effect relationships and comparative statements about which interventions perform better (empirical, cross-study inferences rather than formal scientific laws).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Automated information extraction and aggregation across trial reports (the current paper cites TrialStreamer as an example but does not describe TrialStreamer's methods; referenced work uses NLP extraction pipelines and database construction).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated within this paper; TrialStreamer is cited as a prior example with its own evaluation in its original publications (this paper references TrialStreamer as demonstrated to extract key elements and infer intervention effectiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of an AI system that extracts structured elements and can infer comparative effectiveness across many full texts; used in the paper as background motivation for automation of review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The current paper does not report TrialStreamer's limitations; more detailed limitations are in TrialStreamer's original publications (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3773.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3773.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RobotReviewer (automated bias assessment system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated tool designed to extract information from biomedical articles and assess risk-of-bias in clinical trials; referenced as prior work demonstrating automated extraction and quality-assessment capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automating biomedical evidence synthesis: RobotReviewer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RobotReviewer (automated evidence-synthesis / risk-of-bias tool)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A system built to automatically assess risk-of-bias elements from clinical trial reports using NLP and machine-learning classifiers; architecture specifics are in the RobotReviewer literature and are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Clinical trial reports and biomedical literature relevant to bias assessment in trials.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified in this paper (RobotReviewer was introduced earlier and evaluated on sets of trial reports; current paper cites its reported performance).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of methodological heuristics and risk-of-bias judgments (operational rules about study quality rather than general scientific laws).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Automated information extraction and classification using NLP models (RobotReviewer processes full texts to extract bias-related statements and score bias domains).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced evaluation: RobotReviewer was reported to have similar capability to human reviewers for risk-of-bias assessment, differing by ~7% in accuracy (as cited in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as prior evidence that automated systems can perform complex information extraction tasks (risk-of-bias assessment) with performance approaching human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not detailed here; in general, such systems require annotated training data and may differ from humans on nuanced judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3773.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3773.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abstrackr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstrackr (machine learning tool for semi-automated abstract screening)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-automated title/abstract screening tool that learns from human reviewer decisions to prioritize and suggest inclusions/exclusions; referenced as prior work with reported high sensitivity in some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Abstrackr (ML-based abstract screening tool)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A tool using traditional NLP/machine-learning techniques (historically n-grams, supervised learning, and active learning) to learn inclusion/exclusion preferences from reviewers and prioritize records for screening; specific architecture not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Titles and abstracts for systematic reviews across biomedical topics.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Datasets cited in this paper for Abstrackr evaluations ranged from ~5,243 to 47,385 records (the paper cites reported sensitivities on datasets of that scale).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Learned inclusion/exclusion decision rules or heuristics derived from reviewer labels (classification rules for screening).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised learning from reviewer-labeled examples (active learning / prioritization); not a zero-shot LLM approach but an iterative ML-based prioritization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Cited retrospective evaluations reporting sensitivities (examples in paper: Abstrackr reported sensitivities 0.96, 0.79, 0.92, 0.82 on datasets of varying sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported to save reviewer time with high specificity/sensitivity in some datasets, though risk of missing records exists if fully automated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires labelled training examples / reviewer input; possible risk of missing relevant studies if fully automated; uses older NLP approaches compared to modern LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3773.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3773.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rayyan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rayyan (web and mobile app for systematic reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A web/mobile tool that assists semi-automated article filtering by using NLP algorithms to learn reviewer inclusion/exclusion decisions and to prioritize screening; cited for high specificity in practical use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rayyan-a web and mobile app for systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Rayyan (semi-automated screening platform)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A platform that applies NLP-based learning from reviewer decisions to assist with ranking and filtering records during screening; exact algorithmic details are not specified in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Systematic review titles and abstracts across health sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>User-reported evaluations mentioned (e.g., 75% of articles screened retained 98% of relevant articles in one report), dataset sizes vary and are not fixed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Operational screening heuristics learned from reviewer labels (inclusion/exclusion decision rules).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Interactive supervised learning / prioritization and text-mining approaches; the tool learns from reviewer selections rather than performing zero-shot LLM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced user studies and evaluations (example: Rayyan retained 98% of relevant articles after screening 75% of the dataset in one study cited).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported to reduce workload and provide high specificity in pragmatic user evaluations, but may risk missing studies if used without human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on reviewer-provided labels to learn; potential to miss relevant records if fully automated; uses older NLP methods compared to transformer LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3773.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3773.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Research Screener</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Research Screener (machine learning tool to semi-automate abstract screening)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A machine-learning tool developed to semi-automate abstract screening for systematic reviews; cited as an example of automation applied to the screening workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Research screener: a machine learning tool to semi-automate abstract screening for systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Research Screener (ML-based screening tool)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A supervised machine-learning tool designed to prioritize and screen abstracts for systematic reviews; exact modeling details are contained in its original publication and are not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Biomedical systematic review abstracts and titles.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified in this paper (Research Screener has its own evaluation datasets reported in its primary paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Learned screening decision heuristics (inclusion/exclusion rules) rather than explicit scientific laws.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised machine learning trained on reviewer-labeled examples to prioritize records.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced as prior work; this paper does not reproduce its evaluation results but cites it among automation tools.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Included as an example of a tool that can semi-automate screening; purported to save reviewer time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires labeled examples; potential to miss studies; not a generative LLM-based zero-shot method.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3773.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3773.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistillerSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DistillerSR (commercial ML-based prioritization tool for title/abstract screening)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial platform with a machine-learning prioritization feature for screening that has been evaluated for impact on reviewer-relevant outcomes and is compared in the paper to the GPT-based approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DistillerSR (ML prioritization module)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A platform offering machine-learning assisted prioritization and screening for systematic review abstracts; the current paper references published evaluations rather than providing internal technical details.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Systematic review screening (titles/abstracts) in biomedical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified here; referenced comparative analyses used various internal and nonpublic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Screening/prioritization heuristics learned from labeled data (operational rules for selecting studies).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised prioritization algorithms integrated into a review platform (not a zero-shot LLM approach).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Cited evaluations report varied proportions of missed records across tools when compared on nonpublic datasets (DistillerSR exhibited a wide range including high missed proportions in some cases as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Compared qualitatively in the discussion; DistillerSR's performance varied across datasets in cited evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance depends on dataset and configuration; cited evidence includes potential high missed-record proportions in some contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3773.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3773.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RobotAnalyst</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RobotAnalyst (reference prioritization tool for systematic reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool for prioritizing references during screening that was evaluated in user studies and is cited here as part of the landscape of automated screening systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prioritising references for systematic reviews with RobotAnalyst: a user study.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RobotAnalyst (screening prioritization system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A system combining text-mining and prioritization strategies to support systematic review screening; specific modeling details are in its original user-study publication and not described in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Titles and abstracts for systematic reviews in biomedical research.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified in this paper (evaluations cited used nonpublic datasets of various sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Operational inclusion/exclusion prioritization heuristics learned from data and user interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Text-mining and machine-learning prioritization; not an LLM zero-shot method according to the descriptions in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Cited user-study results showing intermediate performance relative to other tools (reported missed proportions in cited comparative work).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported performance fell between other evaluated tools in cited studies; included here as part of comparative discussion of screening automation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subject to dataset-dependent performance; may miss records if used without human oversight; requires configuration and possibly labelled training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automating biomedical evidence synthesis: RobotReviewer. <em>(Rating: 2)</em></li>
                <li>Trialstreamer: a living, automatically updated database of clinical trial reports. <em>(Rating: 2)</em></li>
                <li>Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool. <em>(Rating: 2)</em></li>
                <li>Rayyan-a web and mobile app for systematic reviews. <em>(Rating: 2)</em></li>
                <li>Research screener: a machine learning tool to semi-automate abstract screening for systematic reviews. <em>(Rating: 2)</em></li>
                <li>An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes. <em>(Rating: 2)</em></li>
                <li>Prioritising references for systematic reviews with RobotAnalyst: a user study. <em>(Rating: 2)</em></li>
                <li>A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3773",
    "paper_id": "paper-cccb129527fbe900c6f94dd8a6ad4b408c403152",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [
        {
            "name_short": "GPT-4 / Chat GPT (OpenAI GPT API)",
            "name_full": "Generative Pretrained Transformer (OpenAI GPT and GPT-4) used via the OpenAI Chat GPT / GPT-4 API",
            "brief_description": "A family of large transformer-based language models provided by OpenAI; in this paper they are used via prompt-based API calls to classify titles and abstracts for inclusion/exclusion in clinical systematic/scoping reviews, and to provide natural-language explanations and reflections for decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI GPT / GPT-4 (Chat GPT API)",
            "model_description": "Large transformer-based pretrained language models (GPT family) accessed via OpenAI's Chat GPT / GPT-4 API; the paper does not report architecture details (parameter counts) or task-specific fine-tuning — models are used in a zero-shot / prompt-based manner with plain-language screening instructions and occasional appended prompts requesting reasoning/reflection.",
            "input_domain": "Clinical and biomedical literature (titles and abstracts for systematic and scoping reviews across multiple medical topics such as COVID-19 therapeutics, nonopioid analgesia, Raynaud's, and LLM research in clinical settings).",
            "corpus_size": "24,307 titles/abstracts across 6 real-world review data sets (paper reports screening &gt;24,000 records; specific dataset counts presented in Table 1).",
            "law_type": "Decision / screening rules and classification heuristics (operational inclusion/exclusion rules for study selection); provides natural-language rationales but not formalized scientific 'laws'.",
            "distillation_method": "Prompting / zero-shot classification via API: a fixed plain-language instruction prompt containing inclusion and exclusion criteria was concatenated with each title/abstract and sent to the API; additional prompts requested explanations and reflections on incorrect classifications.",
            "evaluation_method": "Comparison to human ground-truth labels created by at least two independent reviewers with conflict resolution; metrics reported included accuracy, macro F1-score, sensitivity (included/excluded), Cohen's κ, and prevalence-and-bias-adjusted κ (PABAK); selected model rationales and reflexive corrections were inspected qualitatively.",
            "results_summary": "The GPT-based workflow achieved overall accuracy ≈ 0.91, macro F1 ≈ 0.60, sensitivity for included papers ≈ 0.76 and for excluded papers ≈ 0.91; the model could provide natural-language reasoning and, on a subset, revise incorrect initial classifications when prompted to reflect. The system functioned as an effective first-pass screen but missed some included records.",
            "limitations_or_challenges": "Model showed lower sensitivity for including relevant papers than for excluding irrelevant ones; potential for missing important studies if used fully autonomously; class imbalance affected kappa; reliance on prompt engineering and token-length constraints; no task-specific fine-tuning performed; human oversight recommended; potential costs and runtime (NOA dataset cost ~US$25, runtime ~643 minutes reported).",
            "uuid": "e3773.0",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "TrialStreamer",
            "name_full": "TrialStreamer (automated system for extracting clinical trial information)",
            "brief_description": "An automated system that extracts key elements from clinical trial reports and can infer which interventions work best across sets of full texts indexed on MEDLINE; referenced as an example of systems that extract evidence across many papers.",
            "citation_title": "Trialstreamer: a living, automatically updated database of clinical trial reports.",
            "mention_or_use": "mention",
            "model_name": "TrialStreamer (system described in reference literature; not an LLM per se)",
            "model_description": "An automated pipeline/system that ingests trial reports (MEDLINE-indexed), extracts key trial metadata and results, and supports inference of comparative intervention performance; the paper references TrialStreamer as prior work but does not detail its internal model architectures here.",
            "input_domain": "Clinical trial literature (MEDLINE-indexed clinical trial reports)",
            "corpus_size": "Not specified in this paper (described in referenced TrialStreamer work as operating over MEDLINE-indexed trial reports; the current paper does not provide a numeric corpus size).",
            "law_type": "Inferred treatment-effect relationships and comparative statements about which interventions perform better (empirical, cross-study inferences rather than formal scientific laws).",
            "distillation_method": "Automated information extraction and aggregation across trial reports (the current paper cites TrialStreamer as an example but does not describe TrialStreamer's methods; referenced work uses NLP extraction pipelines and database construction).",
            "evaluation_method": "Not evaluated within this paper; TrialStreamer is cited as a prior example with its own evaluation in its original publications (this paper references TrialStreamer as demonstrated to extract key elements and infer intervention effectiveness).",
            "results_summary": "Cited as an example of an AI system that extracts structured elements and can infer comparative effectiveness across many full texts; used in the paper as background motivation for automation of review tasks.",
            "limitations_or_challenges": "The current paper does not report TrialStreamer's limitations; more detailed limitations are in TrialStreamer's original publications (not reproduced here).",
            "uuid": "e3773.1",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RobotReviewer",
            "name_full": "RobotReviewer (automated bias assessment system)",
            "brief_description": "An automated tool designed to extract information from biomedical articles and assess risk-of-bias in clinical trials; referenced as prior work demonstrating automated extraction and quality-assessment capabilities.",
            "citation_title": "Automating biomedical evidence synthesis: RobotReviewer.",
            "mention_or_use": "mention",
            "model_name": "RobotReviewer (automated evidence-synthesis / risk-of-bias tool)",
            "model_description": "A system built to automatically assess risk-of-bias elements from clinical trial reports using NLP and machine-learning classifiers; architecture specifics are in the RobotReviewer literature and are not detailed in this paper.",
            "input_domain": "Clinical trial reports and biomedical literature relevant to bias assessment in trials.",
            "corpus_size": "Not specified in this paper (RobotReviewer was introduced earlier and evaluated on sets of trial reports; current paper cites its reported performance).",
            "law_type": "Extraction of methodological heuristics and risk-of-bias judgments (operational rules about study quality rather than general scientific laws).",
            "distillation_method": "Automated information extraction and classification using NLP models (RobotReviewer processes full texts to extract bias-related statements and score bias domains).",
            "evaluation_method": "Referenced evaluation: RobotReviewer was reported to have similar capability to human reviewers for risk-of-bias assessment, differing by ~7% in accuracy (as cited in this paper).",
            "results_summary": "Cited as prior evidence that automated systems can perform complex information extraction tasks (risk-of-bias assessment) with performance approaching human reviewers.",
            "limitations_or_challenges": "Not detailed here; in general, such systems require annotated training data and may differ from humans on nuanced judgments.",
            "uuid": "e3773.2",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Abstrackr",
            "name_full": "Abstrackr (machine learning tool for semi-automated abstract screening)",
            "brief_description": "A semi-automated title/abstract screening tool that learns from human reviewer decisions to prioritize and suggest inclusions/exclusions; referenced as prior work with reported high sensitivity in some datasets.",
            "citation_title": "Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool.",
            "mention_or_use": "mention",
            "model_name": "Abstrackr (ML-based abstract screening tool)",
            "model_description": "A tool using traditional NLP/machine-learning techniques (historically n-grams, supervised learning, and active learning) to learn inclusion/exclusion preferences from reviewers and prioritize records for screening; specific architecture not detailed in this paper.",
            "input_domain": "Titles and abstracts for systematic reviews across biomedical topics.",
            "corpus_size": "Datasets cited in this paper for Abstrackr evaluations ranged from ~5,243 to 47,385 records (the paper cites reported sensitivities on datasets of that scale).",
            "law_type": "Learned inclusion/exclusion decision rules or heuristics derived from reviewer labels (classification rules for screening).",
            "distillation_method": "Supervised learning from reviewer-labeled examples (active learning / prioritization); not a zero-shot LLM approach but an iterative ML-based prioritization.",
            "evaluation_method": "Cited retrospective evaluations reporting sensitivities (examples in paper: Abstrackr reported sensitivities 0.96, 0.79, 0.92, 0.82 on datasets of varying sizes).",
            "results_summary": "Reported to save reviewer time with high specificity/sensitivity in some datasets, though risk of missing records exists if fully automated.",
            "limitations_or_challenges": "Requires labelled training examples / reviewer input; possible risk of missing relevant studies if fully automated; uses older NLP approaches compared to modern LLMs.",
            "uuid": "e3773.3",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Rayyan",
            "name_full": "Rayyan (web and mobile app for systematic reviews)",
            "brief_description": "A web/mobile tool that assists semi-automated article filtering by using NLP algorithms to learn reviewer inclusion/exclusion decisions and to prioritize screening; cited for high specificity in practical use.",
            "citation_title": "Rayyan-a web and mobile app for systematic reviews.",
            "mention_or_use": "mention",
            "model_name": "Rayyan (semi-automated screening platform)",
            "model_description": "A platform that applies NLP-based learning from reviewer decisions to assist with ranking and filtering records during screening; exact algorithmic details are not specified in the current paper.",
            "input_domain": "Systematic review titles and abstracts across health sciences.",
            "corpus_size": "User-reported evaluations mentioned (e.g., 75% of articles screened retained 98% of relevant articles in one report), dataset sizes vary and are not fixed in this paper.",
            "law_type": "Operational screening heuristics learned from reviewer labels (inclusion/exclusion decision rules).",
            "distillation_method": "Interactive supervised learning / prioritization and text-mining approaches; the tool learns from reviewer selections rather than performing zero-shot LLM prompting.",
            "evaluation_method": "Referenced user studies and evaluations (example: Rayyan retained 98% of relevant articles after screening 75% of the dataset in one study cited).",
            "results_summary": "Reported to reduce workload and provide high specificity in pragmatic user evaluations, but may risk missing studies if used without human oversight.",
            "limitations_or_challenges": "Depends on reviewer-provided labels to learn; potential to miss relevant records if fully automated; uses older NLP methods compared to transformer LLMs.",
            "uuid": "e3773.4",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Research Screener",
            "name_full": "Research Screener (machine learning tool to semi-automate abstract screening)",
            "brief_description": "A machine-learning tool developed to semi-automate abstract screening for systematic reviews; cited as an example of automation applied to the screening workflow.",
            "citation_title": "Research screener: a machine learning tool to semi-automate abstract screening for systematic reviews.",
            "mention_or_use": "mention",
            "model_name": "Research Screener (ML-based screening tool)",
            "model_description": "A supervised machine-learning tool designed to prioritize and screen abstracts for systematic reviews; exact modeling details are contained in its original publication and are not reproduced in this paper.",
            "input_domain": "Biomedical systematic review abstracts and titles.",
            "corpus_size": "Not specified in this paper (Research Screener has its own evaluation datasets reported in its primary paper).",
            "law_type": "Learned screening decision heuristics (inclusion/exclusion rules) rather than explicit scientific laws.",
            "distillation_method": "Supervised machine learning trained on reviewer-labeled examples to prioritize records.",
            "evaluation_method": "Referenced as prior work; this paper does not reproduce its evaluation results but cites it among automation tools.",
            "results_summary": "Included as an example of a tool that can semi-automate screening; purported to save reviewer time.",
            "limitations_or_challenges": "Requires labeled examples; potential to miss studies; not a generative LLM-based zero-shot method.",
            "uuid": "e3773.5",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DistillerSR",
            "name_full": "DistillerSR (commercial ML-based prioritization tool for title/abstract screening)",
            "brief_description": "A commercial platform with a machine-learning prioritization feature for screening that has been evaluated for impact on reviewer-relevant outcomes and is compared in the paper to the GPT-based approach.",
            "citation_title": "An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes.",
            "mention_or_use": "mention",
            "model_name": "DistillerSR (ML prioritization module)",
            "model_description": "A platform offering machine-learning assisted prioritization and screening for systematic review abstracts; the current paper references published evaluations rather than providing internal technical details.",
            "input_domain": "Systematic review screening (titles/abstracts) in biomedical domains.",
            "corpus_size": "Not specified here; referenced comparative analyses used various internal and nonpublic datasets.",
            "law_type": "Screening/prioritization heuristics learned from labeled data (operational rules for selecting studies).",
            "distillation_method": "Supervised prioritization algorithms integrated into a review platform (not a zero-shot LLM approach).",
            "evaluation_method": "Cited evaluations report varied proportions of missed records across tools when compared on nonpublic datasets (DistillerSR exhibited a wide range including high missed proportions in some cases as cited).",
            "results_summary": "Compared qualitatively in the discussion; DistillerSR's performance varied across datasets in cited evaluations.",
            "limitations_or_challenges": "Performance depends on dataset and configuration; cited evidence includes potential high missed-record proportions in some contexts.",
            "uuid": "e3773.6",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RobotAnalyst",
            "name_full": "RobotAnalyst (reference prioritization tool for systematic reviews)",
            "brief_description": "A tool for prioritizing references during screening that was evaluated in user studies and is cited here as part of the landscape of automated screening systems.",
            "citation_title": "Prioritising references for systematic reviews with RobotAnalyst: a user study.",
            "mention_or_use": "mention",
            "model_name": "RobotAnalyst (screening prioritization system)",
            "model_description": "A system combining text-mining and prioritization strategies to support systematic review screening; specific modeling details are in its original user-study publication and not described in detail here.",
            "input_domain": "Titles and abstracts for systematic reviews in biomedical research.",
            "corpus_size": "Not specified in this paper (evaluations cited used nonpublic datasets of various sizes).",
            "law_type": "Operational inclusion/exclusion prioritization heuristics learned from data and user interaction.",
            "distillation_method": "Text-mining and machine-learning prioritization; not an LLM zero-shot method according to the descriptions in this paper.",
            "evaluation_method": "Cited user-study results showing intermediate performance relative to other tools (reported missed proportions in cited comparative work).",
            "results_summary": "Reported performance fell between other evaluated tools in cited studies; included here as part of comparative discussion of screening automation.",
            "limitations_or_challenges": "Subject to dataset-dependent performance; may miss records if used without human oversight; requires configuration and possibly labelled training data.",
            "uuid": "e3773.7",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automating biomedical evidence synthesis: RobotReviewer.",
            "rating": 2
        },
        {
            "paper_title": "Trialstreamer: a living, automatically updated database of clinical trial reports.",
            "rating": 2
        },
        {
            "paper_title": "Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool.",
            "rating": 2
        },
        {
            "paper_title": "Rayyan-a web and mobile app for systematic reviews.",
            "rating": 2
        },
        {
            "paper_title": "Research screener: a machine learning tool to semi-automate abstract screening for systematic reviews.",
            "rating": 2
        },
        {
            "paper_title": "An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes.",
            "rating": 2
        },
        {
            "paper_title": "Prioritising references for systematic reviews with RobotAnalyst: a user study.",
            "rating": 2
        },
        {
            "paper_title": "A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews.",
            "rating": 1
        }
    ],
    "cost": 0.01472225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study</h1>
<p>Eddie Guo ${ }^{1}$; Mehul Gupta ${ }^{1}$, MD; Jiawen Deng ${ }^{2}$, BHSc; Ye-Jean Park ${ }^{2}$; Michael Paget ${ }^{1}$, BFA; Christopher Naugler ${ }^{1}$, MD<br>${ }^{1}$ Cumming School of Medicine, University of Calgary, Calgary, AB, Canada<br>${ }^{2}$ Temerty Faculty of Medicine, University of Toronto, Toronto, AB, Canada</p>
<h2>Corresponding Author:</h2>
<p>Eddie Guo
Cumming School of Medicine
University of Calgary
3330 University Dr NW
Calgary, AB, T2N 1N4
Canada
Phone: 15879880292
Email: eddie.guo@ucalgary.ca</p>
<h4>Abstract</h4>
<p>Background: The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources.</p>
<p>Objective: This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers.
Methods: We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts.
Results: Our results show an accuracy of 0.91 , a macro $F_{1}$-score of 0.60 , a sensitivity of excluded papers of 0.91 , and a sensitivity of included papers of 0.76 . The interrater variability between 2 independent human screeners was $\kappa=0.46$, and the prevalence and bias-adjusted $\kappa$ between our proposed methods and the consensus-based human decisions was $\kappa=0.96$. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications.
Conclusions: Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research.
(J Med Internet Res 2024;26:e48996) doi: $\underline{10.2196 / 48996}$</p>
<h2>KEYWORDS</h2>
<p>abstract screening; Chat GPT; classification; extract; extraction; free text; GPT; GPT-4; language model; large language models; LLM; natural language processing; NLP; nonopiod analgesia; review methodology; review methods; screening; systematic review; systematic; unstructured data</p>
<h2>Introduction</h2>
<p>Knowledge synthesis, the process of integrating and summarizing relevant studies in the literature to gain an improved understanding of a topic, is a key component in identifying knowledge gaps and informing future research endeavors on a topic of interest [1,2]. Systematic and scoping reviews are among the most commonly used and rigorous forms of knowledge synthesis across multiple disciplines [1,2]. Given that the results from systematic and scoping reviews can inform guidelines, protocols, and decision-making processes, particularly for stakeholders in the realms of health care, the quality of the evidence presented by such reviews can significantly impact generated recommendations [3].</p>
<p>The quality of systematic and scoping reviews is highly dependent on the comprehensiveness of the database searches and the subsequent article screening processes. Overlooking relevant articles during these critical steps can lead to bias [4], while including discrepant studies can yield misleading conclusions and increase discordant heterogeneity [5]. Thus, guidelines surrounding the conduct of clinical reviews, such as the Cochrane Handbook [6], recommend that article screening be completed in duplicate by at least 2 independent reviewers.</p>
<p>However, duplicate screening effectively doubles the financial and human resources needed to complete systematic reviews compared to single screening. This is especially problematic for small research groups, review projects with broad inclusion criteria (such as network meta-analyses), or time-constrained review projects (such as reviews relating to COVID-19 during the early stages of the pandemic) [7,8]. Additionally, there is often substantial interrater variability in screening decisions, leading to additional time spent on discussions to resolve disagreements [9]. Due to the time constraints and wasted resources that are often features of duplicate screening, research studies may also include a more tailored, sensitive search strategy that can lead to missing several articles during the retrieval process [10]. Furthermore, although the nuances of each study differ, many systematic reviews may contain thousands of retrieved articles, only to exclude the majority (ie, up to $90 \%$ ) from the title and abstract screening [10,11].</p>
<p>Recent developments in artificial intelligence and machine learning have made it possible to semiautomate or fully automate repetitive steps within the systematic review workflow [12-14]. Prominent examples of such applications include RobotReviewer [15], TrialStreamer [16], Research Screener [7], DistillerSR [17], and Abstrackr [18], which are artificial intelligence models developed to extract information from scientific articles or abstracts to judge study quality and infer treatment effects. More specifically, RobotReviewer (2016) was shown to have similar capabilities to assess the risk of bias assessment as a human reviewer, only differing by around $7 \%$ in accuracy [19]. Similarly, TrialStreamer was a system developed to extract key elements of information from full texts, such as inferring which interventions in a clinical paper worked best, along with comparisons in study outcomes between all relevant extracted full texts of a topic indexed on MEDLINE [20].</p>
<p>While there have been previous attempts at automating the title and abstract screening process, they often involved labor- or computationally-intensive labeling, pretraining, or vectorizations [21]. For instance, Rayyan and Abstrackr are 2 free web tools that provide a semiautomated approach to article filtering by using natural language processing algorithms to learn when and where a reviewer includes or excludes an article and subsequently mimics a similar approach [22,23]. Rayyan also demonstrated high specificity, wherein $98 \%$ of all relevant articles were included after the tool had screened $75 \%$ of all articles to be analyzed in a study [24]. While automation using these tools was found to save time, there was still minimal to substantive risk that there would be missing studies if the tool were fully independent or automated [22,23]. Furthermore, current programs may use previously standard methods, including n-grams, in comparison to more updated techniques, such as the generative pretrained transformer (GPT) model, which is trained with data from a general domain and does not require additional training to learn embeddings that can represent the semantics and contexts of words in relation to other words $[25,26]$.</p>
<p>In this paper, we introduce a novel workflow to screen titles and abstracts for clinical reviews by providing plain language prompts to the publicly available OpenAI GPT application programming interface (API). We aimed to assess GPT models' ability to accurately and efficiently identify relevant titles and abstracts from real-world clinical review data sets, as well as their ability to explain their decisions and reflect on incorrect classifications. We compare the performance of our model against ground truth labeling by 2 independent human reviewers across 6 review papers in the screening of over 24,000 titles and abstracts.</p>
<h2>Methods</h2>
<h2>Overview</h2>
<p>In our study, we obtained a corpus of title and abstract data sets that have already been filtered by a minimum of 2 human reviewers to train our model (Figure 1). Subsequently, we created a Python script that provides the screening criteria for each paper to the OpenAI Chat GPT or GPT-4 API, depending on the input token length. We then passed each paper to the API using a consistent instruction prompt to determine whether a paper should be included or excluded based on the contents of its title and abstract. The overall accuracy (computed by dividing papers selected by both GPT and human reviewers by the total number of papers), sensitivity of both included and excluded papers, and interrater reliability through Cohen $\kappa$ and prevalence-adjusted and bias-adjusted $\kappa$ (PABAK) were computed against the human-reviewed papers:</p>
<p>$$
\text { PABAK }=\frac{k p_{o b s}-1}{k-1}
$$</p>
<p>Where $k$ is the number of categories and $p_{o b s}$ is the proportion of included papers. All data and code are available in Mendeley data sets [27].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h3><strong>Data Collection</strong></h3>
<p>To validate our proposed inclusion and exclusion methodology, we obtained 6 title and abstract screening data sets from different systematic and scoping reviews previously published by the authors of this study, each screened by 2 independent reviewers with conflicts resolved through consensus. These projects cover various medical science topics and vary in size, methodology, and complexity of screening criteria (Table 1 and Table S1 in Multimedia Appendix 1 [28-33]). We obtained the inclusion and exclusion decision from expert reviewers for each title and abstract entry, as well as the criteria provided to the expert reviewers during the screening process. A summary of the review characteristics is presented in Table 2.</p>
<p><strong>Table 1.</strong> Included studies and their characteristics. The first 5 data sets are systematic reviews with meta-analyses. The last study is a scoping review.</p>
<table>
<thead>
<tr>
<th>Study title</th>
<th>Data set name</th>
<th>Included studies (538/24,307), n/N</th>
<th>Study type</th>
<th>Study topic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Efficacy and Safety of Ivermectin for the Treatment of COVID-19: A Systematic Review and Meta-Analysis [29]</td>
<td>IVM^{a}</td>
<td>35/279</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>COVID-19 treatment and antimalarials</td>
</tr>
<tr>
<td>Efficacy and Safety of Selective Serotonin Reuptake Inhibitors in COVID-19 Management: A Systematic Review and Meta-Analysis [30]</td>
<td>SSRI^{b}</td>
<td>29/3989</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>COVID-19 treatment and antidepressants</td>
</tr>
<tr>
<td>Efficacy of Lopinavir-Ritonavir Combination Therapy for the Treatment of Hospitalized COVID-19 Patients: A Meta-Analysis [31]</td>
<td>LPVR^{c}</td>
<td>91/1456</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>COVID-19 treatment and antiretrovirals</td>
</tr>
<tr>
<td>The Use of Acupuncture in Patients With Raynaud's Syndrome: A Systematic Review and Meta-Analysis of Randomized Controlled Trials [32]</td>
<td>RAYNAUDS^{d}</td>
<td>6/942</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>Raynaud syndrome and acupuncture</td>
</tr>
<tr>
<td>Comparative Efficacy of Adjuvant Non-Opioid Analgesia in Adult Cardiac Surgical Patients: A Network Meta-Analysis [33]</td>
<td>NOA^{e}</td>
<td>354/14,771</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>Postoperative pain and analgesics</td>
</tr>
<tr>
<td>Assessing the Research Landscape and Utility of LLMs^{f} in the Clinical Setting: Protocol for a Scoping Review^{g}</td>
<td>LLM</td>
<td>23/2870</td>
<td>Scoping review</td>
<td>Machine learning in clinical medicine</td>
</tr>
</tbody>
</table>
<p>^{a}IVM: ivermectin.</p>
<p>^{b}SSRI: selective serotonin reuptake inhibitor.</p>
<p>^{c}LPVR: lopinavir-ritonavir.</p>
<p>^{d}RAYNAUDS: Raynaud syndrome.</p>
<p>^{e}NOA: nonopioid analgesia.</p>
<p>^{f}LLM: large language model.</p>
<p>^{g}Registered with Open Science Framework [28].</p>
<p>Table 2. Data formatting for the Python script automating screening with the generative pretrained transformer application programming interface. All non-English characters were removed before analysis.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data</th>
<th style="text-align: left;">Columns</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">df_info</td>
<td style="text-align: left;">- Dataset Name (str): name of data set</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">- Inclusion Criteria (str): screening inclusion criteria</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">- Exclusion Criteria (str): screening exclusion criteria</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Dataset $^{\mathrm{a}}$</td>
<td style="text-align: left;">- Title (str): paper title</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">- Abstract (str): paper abstract</td>
</tr>
</tbody>
</table>
<p>${ }^{a}$ The name of the data set must match Dataset Name in df_info.</p>
<h2>App Creation</h2>
<p>Given a data set, df_info, containing information about inclusion and exclusion criteria of the data sets containing titles and
abstracts to be reviewed, the app calls the OpenAI GPT API to classify each paper to be screened as either included or excluded. The app was coded in Python. The prompt given to the GPT API is provided in Textbox 1.</p>
<p>Textbox 1. Prompt given to the generative pretrained transformer application programming interface.
Instructions: You are a researcher rigorously screening titles and abstracts of scientific papers for inclusion or exclusion in a review paper. Use the criteria below to inform your decision. If any exclusion criteria are met or not all inclusion criteria are met, exclude the article. If all inclusion criteria are met, include the article. Only type "included" or "excluded" to indicate your decision. Do not type anything else.</p>
<p>Abstract: {abstract}
Inclusion criteria: {inclusion_criteria}
Exclusion criteria: {exclusion_criteria}</p>
<h2>Decision:</h2>
<p>Where "Decision:" is whether GPT API includes or excludes the article. Thus, the algorithm is as follows:
data_df &lt;- load(df_info)
for each dataset in data_df: for each row in dataset:
prompt &lt;- instructions + title + abstract + inclusion criteria }
+ exclusion criteria decision &lt;- GPT(prompt) row['decision'] &lt;- decision
save(dataset)</p>
<h2>Assessment and Data Analysis</h2>
<p>After the app was run on all data sets included in our analysis, the following metrics were computed: accuracy, macro $F_{1}$-score, sensitivity for decision tags, $\kappa$, and PABAK. A subset of the results was selected for the GPT models to explain their reasoning. The following prompt was appended to the beginning of the original prompt given to the API: "Explain your reasoning for the decision given with the information below." The human and GPT decisions were appended to the end of the prompt. A subset of incorrect results was selected for GPT to reflect on its incorrect answers. The following prompt was appended to the beginning of the original prompt given to the API: "Explain your reasoning for why the decision given was incorrect with
the information below." The human and GPT decisions were appended to the end of the prompt.</p>
<h2>Results</h2>
<p>The overall accuracy of the GPT models was 0.91 , the sensitivity of included papers was 0.76 , and the sensitivity of excluded papers was 0.91 (Table 3 and Figure 2). On the nonopioid analgesia (NOA) data set (354/14,771 included abstracts), the model ran for 643 minutes and 50.8 seconds, with an approximate cost of US $\$ 25$. The data set characteristics are detailed in Table 1, the model performance is in Table 3 and visualized in Figure 2, and the reasoning from GPT is tabulated in Table 4.</p>
<p>Table 3. Performance of generative pretrained transformer (GPT) in screening titles and abstracts against a human reviewer's ground truth. $\kappa$ (human) is the agreement between 2 independent human reviewers. $\kappa$ (screen) is the agreement between GPT and the final papers included and excluded in each data set.</p>
<table>
<thead>
<tr>
<th>Data set</th>
<th>Accuracy</th>
<th>Macro $F_{1}$-score</th>
<th>Sensitivity (included)</th>
<th>Sensitivity (excluded)</th>
<th>$\kappa$ (human)</th>
<th>$\kappa$ (screen)</th>
<th>PABAK $^{\mathrm{a}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>IVM $^{\text {b }}$</td>
<td>0.748</td>
<td>0.610</td>
<td>0.686</td>
<td>0.756</td>
<td>0.72</td>
<td>0.26</td>
<td>0.78</td>
</tr>
<tr>
<td>SSRI $^{\text {c }}$</td>
<td>0.846</td>
<td>0.595</td>
<td>0.966</td>
<td>0.949</td>
<td>0.58</td>
<td>0.21</td>
<td>0.99</td>
</tr>
<tr>
<td>LPVR $^{\text {d }}$</td>
<td>0.949</td>
<td>0.613</td>
<td>0.593</td>
<td>0.862</td>
<td>0.51</td>
<td>0.25</td>
<td>0.88</td>
</tr>
<tr>
<td>RAYNAUDS $^{\text {e }}$</td>
<td>0.965</td>
<td>0.607</td>
<td>0.833</td>
<td>0.966</td>
<td>0.91</td>
<td>0.22</td>
<td>0.99</td>
</tr>
<tr>
<td>NOA $^{\text {f }}$</td>
<td>0.895</td>
<td>0.601</td>
<td>0.782</td>
<td>0.898</td>
<td>0.35</td>
<td>0.23</td>
<td>0.95</td>
</tr>
<tr>
<td>LLM $^{\text {g }}$</td>
<td>0.943</td>
<td>0.594</td>
<td>1.000</td>
<td>0.942</td>
<td>0.69</td>
<td>0.21</td>
<td>0.98</td>
</tr>
<tr>
<td>Total (weighted)</td>
<td>0.907</td>
<td>0.600</td>
<td>0.764</td>
<td>0.910</td>
<td>0.46</td>
<td>0.22</td>
<td>0.96</td>
</tr>
<tr>
<td>Total (macro)</td>
<td>0.891</td>
<td>0.664</td>
<td>0.810</td>
<td>0.900</td>
<td>0.63</td>
<td>0.23</td>
<td>0.93</td>
</tr>
</tbody>
</table>
<p>${ }^{\text {a }}$ PABAK: prevalence-adjusted and bias-adjusted $\kappa$. ${ }^{\mathrm{b}}$ IVM: ivermectin. ${ }^{\text {c }}$ SSRI: selective serotonin reuptake inhibitor. ${ }^{\mathrm{d}}$ LPVR: lopinavir-ritonavir. ${ }^{\text {e }}$ RAYNAUDS: Raynaud syndrome. ${ }^{\mathrm{f}}$ NOA: nonopioid analgesia. ${ }^{\mathrm{g}}$ LLM: large language model.</p>
<p>Figure 2. (A-F) Confusion matrices for the included and excluded papers for all data sets and (G) the overall performance. IVM: ivermectin; LLM: large language model; LPVR: lopinavir-ritonavir; NOA: nonopioid analgesia; RAYNAUDS: Raynaud's syndrome; SSRI: selective serotonin reuptake inhibitor.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Table 4. Decision and reasoning of the generative pretrained transformer (GPT) application programming interface for a subset of 4 example titles and abstracts. Full abstracts are in ivermectin (IVM) data set [27].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Decision and reasoning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">- Title: Use of Ivermectin in the Treatment of COVID-19: A Pilot Trial <br> - Abstract: Objectives: In this randomized open-label trial pilot study we assessed the antiviral effects and safety of various doses of ivermectin in patients with mild clinical symptoms of COVID-19. Method(s): Patients were randomly assigned to receive standard of care (SOC) treatment at hospital admission...</td>
<td style="text-align: center;">- Human: Included <br> - GPT: Included <br> - Reasoning: The study meets all of the inclusion criteria. It is a randomized controlled trial that compares ivermectin against standard of care treatment in adult COVID-19 patients. The study reports outcomes related to time to viral clearance and adverse events. Additionally, the study is published in a peer-reviewed journal. Therefore, the article should be included in the review paper.</td>
</tr>
<tr>
<td style="text-align: center;">- Title: What Does the COVID-19 Pandemic Mean for the Next Decade of Onchocerciasis Control and Elimination? <br> - Abstract: Background: Mass drug administration (MDA) of ivermectin for onchocerciasis has been disrupted by the coronavirus disease 2019 (COVID-19) pandemic. Mathematical modelling can help predict how missed/delayed MDA will affect short-term epidemiological trends and elimination prospects by 2030...</td>
<td style="text-align: center;">- Human: Excluded <br> - GPT: Excluded <br> - Reasoning: The article does not meet any of the inclusion criteria or exclusion criteria listed. It is not a randomized controlled trial or comparative cohort study, it does not compare ivermectin against standard of care or control group/placebo, it does not include adult COVID-19 inpatients and/or outpatients, and it does not report any of the specified outcomes. Additionally, there is no indication that the article is non-peer-reviewed. Therefore, the article is excluded.</td>
</tr>
<tr>
<td style="text-align: center;">- Title: Use of Ivermectin Is Associated With Lower Mortality in Hospitalized Patients With Coronavirus Disease 2019: The Ivermectin in COVID Nineteen Study <br> - Abstract: Background: Ivermectin was shown to inhibit severe acute respiratory syndrome coronavirus 2 replication in vitro, which has led to off-label use, but clinical efficacy has not been described previously. Research Question: Does ivermectin benefit hospitalized coronavirus disease 2019 (COVID-19) patients?...</td>
<td style="text-align: center;">- Human: Included <br> - GPT: Excluded <br> - Reasoning: The article meets all of the inclusion criteria and none of the exclusion criteria, therefore it should be included in the review paper. <br> - Reflection: The given decision of excluding the article is incorrect. The article meets all the inclusion criteria as it is a non-randomized comparative cohort study that compares ivermectin against standard of care or control group/placebo, includes adult COVID-19 inpatients, and reports mortality incidence. The article does not use ivermectin for prophylaxis of COVID-19 and is a peer-reviewed article. Therefore, the correct decision should be "Included."</td>
</tr>
<tr>
<td style="text-align: center;">- Title: What to Expect From Different Drugs Used in the Treatment of COVID-19: A Study on Applications and in Vivo and in Vitro Results <br> - Abstract: The end of 2019 was marked by the emergence of a new type of coronavirus (SARS-CoV-2), which has killed more than 240,000 people...</td>
<td style="text-align: center;">- Human: Excluded <br> - GPT: Included <br> - Reasoning: The study is not a randomized controlled trial or a non-randomized comparative cohort study, which is an inclusion criteria. Therefore, the article does not meet all inclusion criteria and must be excluded. <br> - Reflection: As per the given inclusion and exclusion criteria, the decision given is incorrect. The abstract does not mention the use of ivermectin in any of the studies. Therefore, the article cannot be included based on the inclusion criteria.</td>
</tr>
</tbody>
</table>
<h2>Discussion</h2>
<h2>Overview</h2>
<p>In this study, we assessed the performance of the OpenAI GPT API in the context of clinical review paper inclusion and exclusion criteria selection. We report an overall accuracy of 0.91 and a PABAK of 0.96 , indicating a high level of agreement between the app's decisions and the reference standard. However, the $\kappa$ was low, ranging from 0.21 to 0.26 , largely due to the imbalanced nature of the data sets in this study. The sensitivity of the included papers was 0.76 , suggesting that the app needs improvement to correctly identify relevant papers (Table 3 and Figure 2). The sensitivity of excluded papers was 0.91 , showing promise in excluding irrelevant papers. These results highlight the potential of large language models (LLMs) to support the clinical review process.</p>
<h2>Implications of GPT API's Performance in the Review Process</h2>
<p>GPT's performance has several implications for the efficiency and consistency of clinical review paper inclusion and exclusion criteria selection. By prioritizing the workflow and acting as an
aid rather than a replacement for researchers and reviewers, the GPT and other large language models have the potential to streamline the review process. This enhanced efficiency could save valuable time and effort for researchers and clinicians, allowing them to focus on more complex tasks and in-depth analysis. Further, the API does not require pretraining or seed articles and can provide reasoning for its decision to either include or exclude papers, an aspect traditional natural language processing algorithms lack in automated or semiautomated paper screening (Table 4). Interestingly, upon being asked to explain its reasoning for a subset of incorrect classifications, GPT corrected its initial decision. Ultimately, this increased efficiency, paired with reasoning capabilities, could contribute to the overall quality of clinical reviews, leading to more accurate and reliable conclusions in medical research.</p>
<p>The use of LLMs in the review process could also promote consistency in the selection of relevant papers. By automating certain aspects of the process and acting as an aid to researchers and clinicians, the model can streamline the review process and help reduce the potential for human error and bias, leading to more objective and reliable results [34]. This increased consistency could, in turn, improve the overall quality of the</p>
<p>evidence synthesized in clinical reviews, providing a more robust foundation for medical decision-making and the development of clinical guidelines.</p>
<p>The potential of LLMs as a decision tool becomes particularly valuable when resources are limited. In such situations, LLMs can be used as a first-pass decision aid, streamlining the review process, and allowing human screeners to focus on a smaller, more relevant subset of papers. By automating the initial screening process, LLMs can help reduce the workload for researchers and clinicians, enabling them to allocate their time and effort more efficiently.</p>
<p>In particular, using the GPT API as a first-pass decision aid can also help mitigate the risk of human error and bias in the initial screening phase, promoting a more objective and consistent selection of papers. While the API's sensitivity for including relevant papers may not be perfect, its high specificity for excluding irrelevant papers can still provide valuable support in narrowing down the pool of potentially relevant studies [10]. This can be particularly beneficial in situations where a large number of papers need to be screened and human resources are scarce [35].</p>
<h2>Comparison to Other Tools</h2>
<p>The comparison of our proposed machine learning method to other tools, such as Abstrackr [18], DistillerSR [17], and RobotAnalyst [36], provides evidence of its efficacy and reliability in the context of systematic review processes. On a data set of 24,307 abstracts and titles, our model achieved an accuracy of 0.91 and comparable sensitivity of 0.91 and 0.76 for excluded and included papers, respectively. The significant interrater agreement ( $\kappa=0.96$ ) between our proposed method and consensus-based human decisions, juxtaposed to the lower interrater variability between 2 independent human screeners $(\kappa=0.46)$, emphasizes the model's robustness. In comparison, Abstrackr reported overall sensitivities of $0.96,0.79,0.92$, and 0.82 on data sets ranging from 5243 to 47,385 records. When comparing the proportion of missed records across Abstrackr, DistillerSR, and RobotAnalyst on nonpublic medical title and abstract screening data sets, Abstrackr exhibited the lowest proportions of missed records, namely $28 \%, 5 \%$, and $0 \%$, respectively [37]. Conversely, DistillerSR showed a high proportion of missed records, reaching up to $100 \%$ in the last data set. RobotAnalyst's performance fell between the 2, with missed proportions of $70 \%, 23 \%$, and $100 \%$, respectively. Future work will explore comparative analyses in greater depth and on a broader array of data sets to compare state-of-the-art screening tools.</p>
<h2>Limitations and Challenges in Implementing GPT API in the Review Process</h2>
<p>While the GPT API shows promise in streamlining the review process, it is important to acknowledge its limitations and
challenges. One notable limitation is the disparity between the high specificity of 0.91 for excluding papers and the lower sensitivity of 0.76 for including papers. This discrepancy suggests that while the API effectively excludes irrelevant papers, it may not be as proficient in identifying relevant papers for inclusion. This could lead to the omission of important studies in the review process, potentially affecting the comprehensiveness and quality of the final review. Therefore, the GPT API should not be considered a replacement for human expertise. Instead, it should be viewed as a complementary tool that can enhance the efficiency and consistency of the review process. Human screeners should still be involved in the final decision-making process, particularly in cases where the API's sensitivity for including relevant papers may be insufficient [7]. Another limitation arises in the selection of data sets for screening; 3 of the 6 data sets focused on the efficacy of various drugs for COVID-19, potentially limiting the generalizability of the results from other types of studies. Further work will assess GPT on a greater diversity of studies. By combining the strengths of the GPT API with human expertise, researchers can optimize the review process and ensure the accuracy and comprehensiveness of the final review.</p>
<h2>Future Research and Development</h2>
<p>Several avenues for future research and development include refining the GPT API's performance in the clinical review paper context, incorporating metadata such as study type and year, and exploring few-shot learning approaches. Additionally, training a generator-discriminator model through fine-tuning could improve the API's performance [38]. Expanding the application of the GPT API to other areas of medical research or literature review could also be explored. This would involve large language models for tasks such as identifying and extracting study design information, patient characteristics, and adverse events. As the maximum token length increases with future iterations of the GPT model, screening entire papers may become feasible [39]. Furthermore, exploring the use of LLMs to generate clinical review papers could be a promising research direction.</p>
<h2>Conclusions</h2>
<p>The GPT API shows potential as a valuable tool for improving the efficiency and consistency of clinical review paper inclusion and exclusion criteria selection. While there are limitations and challenges to its implementation, its performance in this study suggests that it could have a broader impact on clinical review paper writing and medical research. Future research and development should focus on refining the API's performance, expanding its applications, and exploring its potential in other aspects of clinical research.</p>
<h1>Acknowledgments</h1>
<p>We would like to acknowledge the following expert reviewers for providing the screening decisions in the review data sets used in this study and for agreeing to make the data sets publicly available: Abhinav Pillai, Mike Paget, Christopher Naugler, Kiyan Heybati, Fangwen Zhou, Myron Moskalyk, Saif Ali, Chi Yi Wong, Wenteng Hou, Umaima Abbas, Qi Kang Zuo, Emma Huang,</p>
<p>Daniel Rayner, Cristian Garcia, Harikrishnaa Ba Ramaraju, Oswin Chang, Zachary Silver, Thanansayan Dhivagaran, Elena Zheng, and Shayan Heybati.</p>
<h1>Authors' Contributions</h1>
<p>EG contributed to conceptualization, methodology, software, formal analysis, investigation, writing the original draft, reviewing, editing, visualization, supervision, and project administration. MG was responsible for conceptualization, methodology, investigation, writing the original draft, reviewing, editing, supervision, and project administration. JD and YJP were involved in methodology, software, formal analysis, investigation, data curation, writing the original draft, and visualization. MP and CN contributed to writing, reviewing, and editing.</p>
<h2>Conflicts of Interest</h2>
<p>None declared.</p>
<h2>Multimedia Appendix 1</h2>
<p>Included studies and their inclusion and exclusion criteria.
[DOCX File , 20 KB-Multimedia Appendix 1]</p>
<h2>References</h2>
<ol>
<li>Sargeant JM, O'Connor AM. Scoping reviews, systematic reviews, and meta-analysis: applications in veterinary medicine. Front Vet Sci. 2020;7:11. [FREE Full text] [doi: 10.3389/fvets.2020.00011] [Medline: 32047759]</li>
<li>Garritty C, Stevens A, Hamel C, Golfam M, Hutton B, Wolfe D. Knowledge synthesis in evidence-based medicine. Semin Nucl Med. 2019;49(2):136-144. [doi: 10.1053/j.semnuc1med.2018.11.006] [Medline: 30819393]</li>
<li>Luchini C, Veronese N, Nottegar A, Shin JI, Gentile G, Granziol U, et al. Assessing the quality of studies in meta-research: review/guidelines on the most important quality assessment tools. Pharm Stat. 2021;20(1):185-195. [doi: 10.1002/pst. 2068] [Medline: 32935459]</li>
<li>Gartlehner G, Affengruber L, Titscher V, Noel-Storr A, Dooley G, Ballarini N, et al. Single-reviewer abstract screening missed 13 percent of relevant studies: a crowd-based, randomized controlled trial. J Clin Epidemiol. 2020;121:20-28. [FREE Full text] [doi: 10.1016/j.jclinepi.2020.01.005] [Medline: 31972274]</li>
<li>Fletcher J. What is heterogeneity and is it important? BMJ. 2007;334(7584):94-96. [FREE Full text] [doi: 10.1136/bmj.39057.406644.68] [Medline: 17218716]</li>
<li>The Cochrane Collaboration; Higgins JPT, Thomas J. Cochrane Handbook for Systematic Reviews of Interventions. London. The Cochrane Collaboration; 2021.</li>
<li>Chai KEK, Lines RLJ, Gucciardi DF, Ng L. Research screener: a machine learning tool to semi-automate abstract screening for systematic reviews. Syst Rev. 2021;10(1):93. [FREE Full text] [doi: 10.1186/s13643-021-01635-3] [Medline: 33795003]</li>
<li>Clark J, McFarlane C, Cleo G, Ramos CI, Marshall S. The impact of systematic review automation tools on methodological quality and time taken to complete systematic review tasks: case study. JMIR Med Educ. 2021;7(2):e24418. [FREE Full text] [doi: $10.2196 / 24418$ ] [Medline: 34057072]</li>
<li>Tuijn S, Janssens F, Robben P, van den Bergh H. Reducing interrater variability and improving health care: a meta-analytical review. J Eval Clin Pract. 2012;18(4):887-895. [doi: 10.1111/j.1365-2753.2011.01705.x] [Medline: 21726359]</li>
<li>Rathbone J, Carter M, Hoffmann T, Glasziou P. Better duplicate detection for systematic reviewers: evaluation of systematic review assistant-deduplication module. Syst Rev. 2015;4(1):6. [FREE Full text] [doi: 10.1186/2046-4053-4-6] [Medline: 25588387]</li>
<li>Polanin JR, Pigott TD, Espelage DL, Grotpeter JK. Best practice guidelines for abstract screening large - evidence systematic reviews and meta - analyses. Res Synth Methods. 2019;10(3):330-342. [FREE Full text] [doi: 10.1002/jrsm. 1354]</li>
<li>Marshall IJ, Wallace BC. Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. Syst Rev. 2019;8(1):163. [FREE Full text] [doi: 10.1186/s13643-019-1074-9] [Medline: 31296265]</li>
<li>Marshall IJ, Trikalinos TA, Soboczenski F, Yun HS, Kell G, Marshall R, et al. In a pilot study, automated real-time systematic review updates were feasible, accurate, and work-saving. J Clin Epidemiol. 2023;153:26-33. [FREE Full text] [doi: 10.1016/j.jclinepi.2022.08.013] [Medline: 36150548]</li>
<li>Blaizot A, Veettil SK, Saidoung P, Moreno-Garcia CF, Wiratunga N, Aceves-Martins M, et al. Using artificial intelligence methods for systematic review in health sciences: a systematic review. Res Synth Methods. 2022;13(3):353-362. [doi: 10.1002/jrsm. 1553] [Medline: 35174972]</li>
<li>
<p>Marshall I, Kuiper J, Banner E, Wallace BC. Automating biomedical evidence synthesis: RobotReviewer. Association for Computational Linguistics; Presented at: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations; July 30-August 4, 2017, 2017;7-12; Vancouver, Canada. URL: https://aclanthology. org/P17-4002.pdf [doi: 10.18653/v1/p17-4002]</p>
</li>
<li>
<p>Marshall IJ, Nye B, Kuiper J, Noel-Storr A, Marshall R, Maclean R, et al. Trialstreamer: a living, automatically updated database of clinical trial reports. J Am Med Inform Assoc. 2020;27(12):1903-1912. [FREE Full text] [doi: 10.1093/jamia/ocaa163] [Medline: 32940710]</p>
</li>
<li>Hamel C, Kelly SE, Thavorn K, Rice DB, Wells GA, Hutton B. An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes. BMC Med Res Methodol. 2020;20(1):256. [FREE Full text] [doi: 10.1186/s12874-020-01129-1] [Medline: 33059590]</li>
<li>Gates A, Johnson C, Hartling L. Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool. Syst Rev. 2018;7(1):45. [FREE Full text] [doi: 10.1186/s13643-018-0707-8] [Medline: 29530097]</li>
<li>Marshall IJ, Kuiper J, Wallace BC. RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials. J Am Med Inform Assoc. 2016;23(1):193-201. [FREE Full text] [doi: 10.1093/jamia/ocv044] [Medline: 26104742]</li>
<li>Nye BE, Nenkova A, Marshall IJ, Wallace BC. Trialstreamer: mapping and browsing medical evidence in real-time. Proc Conf. 2020;2020:63-69. [FREE Full text] [doi: 10.18653/v1/2020.acl-demos.9] [Medline: 34136886]</li>
<li>Moreno-Garcia CF, Jayne C, Elyan E, Aceves-Martins M. A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews. Decis Anal J. 2023;6:100162. [FREE Full text] [doi: 10.1016/j.dajour.2023.100162]</li>
<li>Wallace BC, Trikalinos TA, Lau J, Brodley C, Schmid CH. Semi-automated screening of biomedical citations for systematic reviews. BMC Bioinformatics. 2010;11:55. [FREE Full text] [doi: 10.1186/1471-2105-11-55] [Medline: 20102628]</li>
<li>Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan-a web and mobile app for systematic reviews. Syst Rev. 2016;5(1):210. [FREE Full text] [doi: 10.1186/s13643-016-0384-4] [Medline: 27919275]</li>
<li>Olofsson H, Brolund A, Hellberg C, Silverstein R, Stenström K, Österberg M, et al. Can abstract screening workload be reduced using text mining? User experiences of the tool Rayyan. Res Synth Methods. 2017;8(3):275-280. [doi: 10.1002/irsm. 1237] [Medline: 28374510]</li>
<li>Shree P. The journey of Open AI GPT models. Medium. 2020. URL: https://medium.com/walmartglobaltech/ the-journey-of-open-ai-gpt-models-32d95b7fb7fb2 [accessed 2023-04-19]</li>
<li>O'Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text mining for study identification in systematic reviews: a systematic review of current approaches. Syst Rev. 2015;4(1):5. [FREE Full text] [doi: 10.1186/2046-4053-4-5] [Medline: 25588314]</li>
<li>Guo E, Gupta M, Deng J, Park YJ, Paget M, Naugler C. Automated paper screening for clinical reviews using large language models. Mendeley Data. 2023. URL: https://data.mendeley.com/datasets/np79tmhkh5/1 [accessed 2023-12-15]</li>
<li>Assessing the research landscape and utility of LLMs in the clinical setting: protocol for a scoping review. OSF Registries. URL: https://osf.io/498k6 [accessed 2023-12-15]</li>
<li>Deng J, Zhou F, Ali S, Heybati K, Hou W, Huang E, et al. Efficacy and safety of ivermectin for the treatment of COVID-19: a systematic review and meta-analysis. QJM. 2021;114(10):721-732. [FREE Full text] [doi: 10.1093/qjmed/hcab247] [Medline: 34570241]</li>
<li>Deng J, Rayner D, Ramaraju HB, Abbas U, Garcia C, Heybati K, et al. Efficacy and safety of selective serotonin reuptake inhibitors in COVID-19 management: a systematic review and meta-analysis. Clin Microbiol Infect. 2023;29(5):578-586. [FREE Full text] [doi: 10.1016/j.cmi.2023.01.010] [Medline: 36657488]</li>
<li>Deng J, Zhou F, Hou W, Heybati K, Ali S, Chang O, et al. Efficacy of lopinavir-ritonavir combination therapy for the treatment of hospitalized COVID-19 patients: a meta-analysis. Future Virol. 2021 [FREE Full text] [doi: 10.2217/fvl-2021-0066] [Medline: 35145560]</li>
<li>Zhou F, Huang E, Zheng E, Deng J. The use of acupuncture in patients with Raynaud's syndrome: a systematic review and meta-analysis of randomized controlled trials. Acupunct Med. 2023;41(2):63-72. [FREE Full text] [doi: 10.1177/09645284221076504] [Medline: 35608095]</li>
<li>Heybati K, Zhou F, Lynn MJ, Deng J, Ali S, Hou W, et al. Comparative efficacy of adjuvant nonopioid analgesia in adult cardiac surgical patients: a network meta-analysis. J Cardiothorac Vasc Anesth. 2023;37(7):1169-1178. [doi: 10.1053/j.jvca.2023.03.018] [Medline: 37088644]</li>
<li>Zhang Y, Liang S, Feng Y, Wang Q, Sun F, Chen S, et al. Automation of literature screening using machine learning in medical evidence synthesis: a diagnostic test accuracy systematic review protocol. Syst Rev. 2022;11(1):11. [FREE Full text] [doi: 10.1186/s13643-021-01881-5] [Medline: 35031074]</li>
<li>van de Schoot R, de Bruin J, Schram R, Zahedi P, de Boer J, Weijdema F, et al. An open source machine learning framework for efficient and transparent systematic reviews. Nat Mach Intell. 2021;3(2):125-133. [FREE Full text] [doi: $10.1038 / \mathrm{s} 42256-020-00287-7]$</li>
<li>Przybyła P, Brockmeier AJ, Kontonatsios G, Le Pogam M, McNaught J, von Elm E, et al. Prioritising references for systematic reviews with RobotAnalyst: a user study. Res Synth Methods. 2018;9(3):470-488. [FREE Full text] [doi: 10.1002/irsm. 1311] [Medline: 29956486]</li>
<li>
<p>Gates A, Guitard S, Pillay J, Elliott SA, Dyson MP, Newton AS, et al. Performance and usability of machine learning for screening in systematic reviews: a comparative evaluation of three tools. In: AHRQ Methods for Effective Health Care. Rockville, MD. Agency for Healthcare Research and Quality (US); 2019.</p>
</li>
<li>
<p>Schade M. Fine-tuning a classifier to improve truthfulness. OpenAI. URL: https://help.openai.com/en/articles/ 5528730-fine-tuning-a-classifier-to-improve-truthfulness [accessed 2023-04-20]</p>
</li>
<li>Joshua J. What is the difference between the GPT-4 models? OpenAI. URL: https://help.openai.com/en/articles/ 7127966-what-is-the-difference-between-the-gpt-4-models [accessed 2023-04-19]</li>
</ol>
<h1>Abbreviations</h1>
<p>API: application programming interface
GPT: generative pretrained transformer
LLM: large language model
NOA: nonopioid analgesia
PABAK: prevalence and bias-adjusted kappa</p>
<p>Edited by T de Azevedo Cardoso, G Eysenbach; submitted 14.05.23; peer-reviewed by T Kang, M Chatzimina, I Bojic; comments to author 30.08.23; revised version received 30.08.23; accepted 28.09.23; published 12.01.24</p>
<p>Please cite as:
Guo E, Gupta M, Deng J, Park YJ, Paget M, Naugler C
Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study
J Med Internet Res 2024;26:e48996
URL: https://www.jmir.org/2024/1/e48996
doi: $10.2196 / 48996$
PMID: 38214966
©Eddie Guo, Mehul Gupta, Jiawen Deng, Ye-Jean Park, Michael Paget, Christopher Naugler. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 12.01.2024. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7076 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7076</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7076</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-7c1707db9aafd209aa93db3251e7ebd593d55876</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7c1707db9aafd209aa93db3251e7ebd593d55876" target="_blank">SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database, and demonstrates that it can detect non-factual and factual sentences and rank passages in terms of factuality.</p>
                <p><strong>Paper Abstract:</strong> Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose"SelfCheckGPT", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7076.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7076.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT (zero-resource sampling-based hallucination detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-resource, black-box sampling approach that detects hallucinated vs factual content by drawing multiple stochastic responses from an LLM and measuring informational consistency between the main response and samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method (sampling-based verifier over arbitrary LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a generative model itself â€” a methodology that draws N stochastic samples from a target black-box LLM given a query, then measures consistency between the main response and samples using multiple comparators (BERTScore, QA-based matching, n-gram LMs, NLI, or LLM prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>sampling-based consistency measurement (uses off-the-shelf comparators like NLI classifiers or LLM prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Sample multiple stochastic outputs and measure agreement/consistency; inconsistency indicates likely hallucination (variants include embedding similarity, question-answer consistency, n-gram likelihood, NLI contradiction, and LLM prompt-based support checks).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiBio (GPT-3 generated passages with manual sentence-level factuality labels)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Dataset created by generating Wikipedia-style passages from the WikiBio concepts using GPT-3 (text-davinci-003), then manually annotating each sentence as Accurate / Minor Inaccurate / Major Inaccurate; used for sentence-level hallucination detection and passage-level factuality ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sentence-level hallucination detection; passage-level factuality ranking (aggregate of sentence scores).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sentence-level AUC-PR (detect non-factual/factual sentences); Passage-level Pearson and Spearman correlation with human judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Best variant (Prompt): sentence-level AUC-PR (NonFact) 93.42; passage-level Pearson 78.32, Spearman 78.30 (see per-variant entries for full breakdown).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>SelfCheckGPT variants (especially Prompt and NLI) outperform grey-box token-probability baselines (GPT-3 probabilities) and proxy-LLM probability baselines on both sentence and passage tasks; e.g., passage Pearson 78.32 (Prompt) vs 57.04 (GPT-3 Avg(-logp)).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling-based consistency is an effective zero-resource signal for hallucination detection; SelfCheckGPT (Prompt and NLI variants) substantially outperforms both grey-box uncertainty measures and proxy-LLM approximations on the constructed WikiBio evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Method depends on the ability to draw multiple samples (compute and API cost); the best-performing Prompt variant is computationally heavy; evaluation limited to GPT-3 generated biographical passages (WikiBio concepts) and sentence-level labels; some sentence granularity issues (sentences may mix factual and non-factual atomic facts).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7076.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT-Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT with LLM Prompting (prompt-based verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SelfCheckGPT variant that prompts an LLM to judge whether an assessed sentence is supported by each sampled response, converting Yes/No answers into an inconsistency score averaged across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003) or ChatGPT (gpt-3.5-turbo) used as verifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf autoregressive transformer LLMs prompted in a binary support-check template: 'Context: {} Sentence: {} Is the sentence supported by the context above? Answer Yes or No:'</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (prompt-based reasoning / zero-shot natural language inference)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompted natural language support check (zero-shot); uses the LLM's binary judgment (Yes/No/N/A) across multiple samples to compute inconsistency score.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiBio (GPT-3 generated passages)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See SelfCheckGPT entry; sentence-level labels and passage-level averaging used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sentence-level support detection (is sentence supported by sampled contexts); passage-level factuality ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Sentence-level AUC-PR: NonFact 93.42 | NonFact* 53.19 | Factual 67.09. Passage-level: Pearson 78.32, Spearman 78.30.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Considerably higher than grey-box token-probability baselines (e.g., passage Pearson 78.32 vs 57.04 for GPT-3 Avg(-logp)); also best among SelfCheckGPT variants reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt-based LLM verification is the most effective SelfCheckGPT variant on this dataset; GPT-3 can self-check its own outputs, and ChatGPT performs slightly better as verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High computational and API cost (Prompt variant is 'computationally heavy'); may depend on verifier LLM capability and prompt design; measured only on WikiBio bios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7076.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT-NLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT with Natural Language Inference (NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SelfCheckGPT variant that runs an NLI classifier on premise=sampled passage and hypothesis=sentence-to-assess, using the contradiction logit normalized against entailment to compute inconsistency scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-v3-large (MNLI fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeBERTa-v3-large transformer encoder fine-tuned on MNLI to predict entailment/neutral/contradiction; used here to score contradiction likelihood between a sampled passage and a target sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer encoder (NLI classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MNLI (Multi-Genre Natural Language Inference) for fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Textual entailment/contradiction checking between generated samples and the target sentence; contradiction probability averaged across samples yields inconsistency score.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiBio (GPT-3 generated passages)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See SelfCheckGPT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sentence-level contradiction detection / hallucination detection; passage-level factuality ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Sentence-level AUC-PR: NonFact 92.50 | NonFact* 45.17 | Factual 66.08. Passage-level: Pearson 74.14, Spearman 73.78.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms grey-box probability baselines and many other black-box variants; performance close to Prompt variant while being less costly.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NLI-based contradiction scoring is a strong, practical SelfCheckGPT variant balancing accuracy and compute; it closely approaches Prompt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on NLI classifier quality and domain match (MNLI-trained classifier may mis-handle some generated content); neutral class is ignored by their normalization scheme; may miss fine-grained atomic factual errors within sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7076.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT-n-gram</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT with n-gram language model approximation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SelfCheckGPT variant that trains an n-gram model on the N sampled responses (plus the main response as smoothing) to approximate the generator's token probabilities and uses token log-probabilities to score sentence inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n-gram LM (trained on drawn samples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Count-based n-gram language model trained on the set of sampled responses (and main response for smoothing); can compute Avg or Max negative log-probabilities for sentence scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>n-gram statistical language model (approximation to black-box LM via samples)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Samples drawn from the target LLM for each query (N samples used per assessed response).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Approximate token likelihood via empirical n-gram counts; unusual/rare tokens across samples lead to high Max(-log p) indicating potential hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiBio (GPT-3 generated passages)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See SelfCheckGPT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sentence-level hallucination detection (likelihood-based); passage-level ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Unigram (max) reported: Sentence-level AUC-PR NonFact 85.63 | NonFact* 41.04 | Factual 58.47. Passage-level: Pearson 64.71, Spearman 64.91. (Performance decreases for higher n in max mode.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performs better than many grey-box proxy-LLM approaches and some probability baselines; unigram (max) surprisingly effective and low-cost compared to Prompt/NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Selecting the least-likely token (max over token negative log-probabilities) across samples is an effective and computationally cheaper heuristic; higher n-grams do not improve and often degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires a sufficient number of samples for reliable n-gram counts; performance plateaus slowly with increasing N and degrades for larger n; captures token-level rarity rather than semantic contradiction, so may miss subtle factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7076.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT-QA (MQAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT with Multiple-Choice Question Answering Generation (MQAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SelfCheckGPT variant that generates multiple-choice questions from the target sentence and checks whether answers derived from the main response and other samples agree; uses answerability scoring and a Bayesian soft-counting scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Question generation: T5-Large (SQuAD/RACE fine-tuned); Answering: Longformer (RACE fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage question generation approach (G1 and G2) using T5-Large models fine-tuned on QA datasets to create question+options, and a Longformer-based reader as the answerer/answerability estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>sequence-to-sequence QA generation + transformer-based reader for answering and answerability</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>T5-Large fine-tuned on SQuAD and RACE for generation; Longformer fine-tuned on RACE for answering; answerability model fine-tuned on SQuAD2.0.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate questions about a sentence, answer them using the main response and each sample, count matches/mismatches (soft-counted by answerability), and map counts to a Bayesian inconsistency score.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiBio (GPT-3 generated passages)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See SelfCheckGPT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sentence-level factuality via QA-consistency; passage-level ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Sentence-level AUC-PR: NonFact 84.26 | NonFact* 40.06 | Factual 48.14. Passage-level: Pearson 61.07, Spearman 59.29.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Beats several grey-box and proxy baselines and is competitive with other SelfCheckGPT variants, but lower than NLI and Prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MQAG-style QA consistency is a practical automatic way to measure information agreement between samples, and soft-counting with a Bayesian mapping improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires QA generation and answering components (computational cost); quality depends on generated distractors and answerability estimation; lower than Prompt/NLI in absolute accuracy on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7076.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT with BERTScore (RoBERTa backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SelfCheckGPT variant that measures semantic similarity between the assessed sentence and the most similar sentence from each sample using BERTScore (RoBERTa-Large backbone) and averages corresponding similarity magnitudes to compute inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERTScore (RoBERTa-Large backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embedding-based semantic similarity using pre-trained RoBERTa-Large to compute token-level cosine similarities aggregated into a sentence-level BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>pretrained transformer encoder embeddings for semantic similarity</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>RoBERTa pretraining (not specific to logic); used off-the-shelf</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Embedding similarity across samples: high average similarity implies factual/consistent statements; low similarity implies hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiBio (GPT-3 generated passages)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See SelfCheckGPT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sentence-level semantic consistency for hallucination detection; passage-level ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Sentence-level AUC-PR: NonFact 81.96 | NonFact* 45.96 | Factual 44.23. Passage-level: Pearson 58.18, Spearman 55.90.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms some grey-box baselines in sentence detection and is competitive on passage ranking but is weaker than NLI and Prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embedding-similarity (BERTScore) is an intuitive consistency measure and performs reasonably without requiring token probabilities or extra QA/NLI modules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Semantic similarity can miss factual contradictions expressed with similar wording and may be insensitive to small but crucial factual differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7076.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grey-box uncertainty (GPT-3 probs / entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grey-box uncertainty-based factuality assessment using GPT-3 token probabilities and entropies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grey-box methods that use the LLM's output token-level probabilities (Avg(-log p), Max(-log p)) and token entropy (Avg/Max H) to detect sentences likely to be factual or hallucinated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003) probabilities (grey-box access)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (GPT-3 family) where token-level probability outputs are available; entropy over top-k tokens was computed where available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>autoregressive transformer with token-probability outputs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Use token probability and entropy as intrinsic uncertainty signals: factual tokens have higher probability/lower entropy; hallucinated tokens have flatter distributions/higher entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiBio (GPT-3 generated passages)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See SelfCheckGPT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sentence-level hallucination detection; passage-level ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Avg(-logp) reported: Sentence-level AUC-PR NonFact 83.21 | NonFact* 38.89 | Factual 53.97. Passage-level: Pearson 57.04, Spearman 53.93.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Strong baseline when token probabilities are available, but outperformed by SelfCheckGPT variants (Prompt and NLI).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Token probability and entropy correlate with factuality; factual sentences tend to have higher-probability/low-entropy tokens. However, this requires grey-box access to token probabilities, which is often not available for black-box API LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires access to token-level probabilities (not available for many API-only systems like ChatGPT in some modes); entropy computed only over returned top-5 tokens may be insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7076.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proxy LLM approach</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proxy LLMs to approximate token probabilities of a black-box LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an accessible LLM (e.g., LLaMA, OPT, GPT-NeoX, GPT-J) to approximate token probability distributions of a black-box generator and apply grey-box uncertainty measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (7B, 13B, 30B) and other open LLMs (OPT, GPT-NeoX, GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive transformer models used as proxies; their token distributions are used to compute Avg(-log p) and entropy measures as a stand-in for the true black-box LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 30B (LLaMA variants) and other sizes for OPT/NeoX/J as detailed in appendix</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>autoregressive transformer (proxy models)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Approximate the original LLM's uncertainty by evaluating probabilities with a proxy LLM trained/fitted separately.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiBio (GPT-3 generated passages)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See SelfCheckGPT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sentence-level hallucination detection; passage-level ranking (using proxy probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: LLaMA-30B Avg(-logp): Sentence-level AUC-PR NonFact 75.43 | NonFact* 30.32 | Factual 41.29. Passage-level Pearson 21.72, Spearman 20.20 (worse than original GPT-3 grey-box).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Proxy LLMs performed noticeably worse than using the original LLM's probabilities; some proxy models produce near-random performance depending on model mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Proxy LLMs are an unreliable substitute for token-level uncertainty of the original black-box model due to differing generation patterns and style mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance degrades substantially when proxy model style differs; even large open models (e.g., LLaMA-30B) underperform the true model's grey-box metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7076.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency (Chain-of-Thought mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency decoding for chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding strategy that samples multiple chain-of-thought reasoning traces and aggregates answers (majority vote) to improve complex reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as a related work showing that sampling multiple reasoning traces (self-consistency) improves chain-of-thought outcomes on complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>chain-of-thought prompting + sampling aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Self-consistency: generate multiple reasoning paths (chain-of-thought) and aggregate answers to reduce reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Complex reasoning tasks (background mention in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as prior work demonstrating improvement in chain-of-thought reasoning via sampling/self-consistency; not evaluated within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated here; computational cost and applicability to hallucination detection not explored in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7076.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-evaluation by LLMs (predicting own truthfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting LLMs to evaluate the truth or confidence of their previous outputs (e.g., predict probability the generated response is true), mentioned as a related approach to assessing factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models (mostly) know what they know</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach where an LLM is prompted to self-assess its outputs; referred to as an alternate hallucination detection approach that does not require external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>prompt-based self-assessment</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompt the LLM to evaluate confidence / truthfulness of its own response to detect hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Self-evaluation / truthfulness estimation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior work; the paper uses LLM prompting for support checks (SelfCheckGPT-Prompt) which is related but not identical to the self-evaluation approach described in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not directly evaluated in this paper; self-evaluation effectiveness depends on the LLM and prompt reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7076.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7076.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Internal-state classifier (white-box)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Internal-state hallucination classifier using LLM hidden representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A white-box approach that trains a classifier on LLM internal hidden representations to predict truthfulness of generated sentences, requiring internal access to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The internal state of an llm knows when its lying</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classifier (e.g., multi-layer perceptron) trained on internal activations of a target LLM to predict whether outputs are truthful/hallucinated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>white-box classifier on model internals</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Supervised labels of truthfulness required to train the classifier (per cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Use internal activations as features for a supervised classifier to predict hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Hallucination detection via internal-state classification (white-box)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a white-box alternative â€” effective but impractical when internal states are not accessible (e.g., API-only LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires access to internal model states and labeled training data; not applicable to black-box API settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Language models (mostly) know what they know <em>(Rating: 2)</em></li>
                <li>The internal state of an llm knows when its lying <em>(Rating: 2)</em></li>
                <li>Chatgpt as a factual inconsistency evaluator for abstractive text summarization <em>(Rating: 1)</em></li>
                <li>MQAG: Multiple-choice question answering and generation for assessing information consistency in summarization <em>(Rating: 2)</em></li>
                <li>Llama: Open and efficient foundation language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7076",
    "paper_id": "paper-7c1707db9aafd209aa93db3251e7ebd593d55876",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "SelfCheckGPT",
            "name_full": "SelfCheckGPT (zero-resource sampling-based hallucination detection)",
            "brief_description": "A zero-resource, black-box sampling approach that detects hallucinated vs factual content by drawing multiple stochastic responses from an LLM and measuring informational consistency between the main response and samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "method (sampling-based verifier over arbitrary LLM outputs)",
            "model_description": "Not a generative model itself â€” a methodology that draws N stochastic samples from a target black-box LLM given a query, then measures consistency between the main response and samples using multiple comparators (BERTScore, QA-based matching, n-gram LMs, NLI, or LLM prompting).",
            "model_size": null,
            "architecture_type": "sampling-based consistency measurement (uses off-the-shelf comparators like NLI classifiers or LLM prompts)",
            "training_data": null,
            "reasoning_method": "Sample multiple stochastic outputs and measure agreement/consistency; inconsistency indicates likely hallucination (variants include embedding similarity, question-answer consistency, n-gram likelihood, NLI contradiction, and LLM prompt-based support checks).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "WikiBio (GPT-3 generated passages with manual sentence-level factuality labels)",
            "benchmark_description": "Dataset created by generating Wikipedia-style passages from the WikiBio concepts using GPT-3 (text-davinci-003), then manually annotating each sentence as Accurate / Minor Inaccurate / Major Inaccurate; used for sentence-level hallucination detection and passage-level factuality ranking.",
            "task_type": "Sentence-level hallucination detection; passage-level factuality ranking (aggregate of sentence scores).",
            "performance_metric": "Sentence-level AUC-PR (detect non-factual/factual sentences); Passage-level Pearson and Spearman correlation with human judgements.",
            "performance_value": "Best variant (Prompt): sentence-level AUC-PR (NonFact) 93.42; passage-level Pearson 78.32, Spearman 78.30 (see per-variant entries for full breakdown).",
            "comparison_with_baseline": "SelfCheckGPT variants (especially Prompt and NLI) outperform grey-box token-probability baselines (GPT-3 probabilities) and proxy-LLM probability baselines on both sentence and passage tasks; e.g., passage Pearson 78.32 (Prompt) vs 57.04 (GPT-3 Avg(-logp)).",
            "key_findings": "Sampling-based consistency is an effective zero-resource signal for hallucination detection; SelfCheckGPT (Prompt and NLI variants) substantially outperforms both grey-box uncertainty measures and proxy-LLM approximations on the constructed WikiBio evaluation.",
            "limitations": "Method depends on the ability to draw multiple samples (compute and API cost); the best-performing Prompt variant is computationally heavy; evaluation limited to GPT-3 generated biographical passages (WikiBio concepts) and sentence-level labels; some sentence granularity issues (sentences may mix factual and non-factual atomic facts).",
            "uuid": "e7076.0",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SelfCheckGPT-Prompt",
            "name_full": "SelfCheckGPT with LLM Prompting (prompt-based verifier)",
            "brief_description": "A SelfCheckGPT variant that prompts an LLM to judge whether an assessed sentence is supported by each sampled response, converting Yes/No answers into an inconsistency score averaged across samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003) or ChatGPT (gpt-3.5-turbo) used as verifier",
            "model_description": "Off-the-shelf autoregressive transformer LLMs prompted in a binary support-check template: 'Context: {} Sentence: {} Is the sentence supported by the context above? Answer Yes or No:'",
            "model_size": null,
            "architecture_type": "transformer (prompt-based reasoning / zero-shot natural language inference)",
            "training_data": null,
            "reasoning_method": "Prompted natural language support check (zero-shot); uses the LLM's binary judgment (Yes/No/N/A) across multiple samples to compute inconsistency score.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "WikiBio (GPT-3 generated passages)",
            "benchmark_description": "See SelfCheckGPT entry; sentence-level labels and passage-level averaging used for evaluation.",
            "task_type": "Sentence-level support detection (is sentence supported by sampled contexts); passage-level factuality ranking.",
            "performance_metric": "Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.",
            "performance_value": "Sentence-level AUC-PR: NonFact 93.42 | NonFact* 53.19 | Factual 67.09. Passage-level: Pearson 78.32, Spearman 78.30.",
            "comparison_with_baseline": "Considerably higher than grey-box token-probability baselines (e.g., passage Pearson 78.32 vs 57.04 for GPT-3 Avg(-logp)); also best among SelfCheckGPT variants reported.",
            "key_findings": "Prompt-based LLM verification is the most effective SelfCheckGPT variant on this dataset; GPT-3 can self-check its own outputs, and ChatGPT performs slightly better as verifier.",
            "limitations": "High computational and API cost (Prompt variant is 'computationally heavy'); may depend on verifier LLM capability and prompt design; measured only on WikiBio bios.",
            "uuid": "e7076.1",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SelfCheckGPT-NLI",
            "name_full": "SelfCheckGPT with Natural Language Inference (NLI)",
            "brief_description": "A SelfCheckGPT variant that runs an NLI classifier on premise=sampled passage and hypothesis=sentence-to-assess, using the contradiction logit normalized against entailment to compute inconsistency scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeBERTa-v3-large (MNLI fine-tuned)",
            "model_description": "DeBERTa-v3-large transformer encoder fine-tuned on MNLI to predict entailment/neutral/contradiction; used here to score contradiction likelihood between a sampled passage and a target sentence.",
            "model_size": null,
            "architecture_type": "transformer encoder (NLI classifier)",
            "training_data": "MNLI (Multi-Genre Natural Language Inference) for fine-tuning",
            "reasoning_method": "Textual entailment/contradiction checking between generated samples and the target sentence; contradiction probability averaged across samples yields inconsistency score.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "WikiBio (GPT-3 generated passages)",
            "benchmark_description": "See SelfCheckGPT entry.",
            "task_type": "Sentence-level contradiction detection / hallucination detection; passage-level factuality ranking.",
            "performance_metric": "Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.",
            "performance_value": "Sentence-level AUC-PR: NonFact 92.50 | NonFact* 45.17 | Factual 66.08. Passage-level: Pearson 74.14, Spearman 73.78.",
            "comparison_with_baseline": "Outperforms grey-box probability baselines and many other black-box variants; performance close to Prompt variant while being less costly.",
            "key_findings": "NLI-based contradiction scoring is a strong, practical SelfCheckGPT variant balancing accuracy and compute; it closely approaches Prompt performance.",
            "limitations": "Depends on NLI classifier quality and domain match (MNLI-trained classifier may mis-handle some generated content); neutral class is ignored by their normalization scheme; may miss fine-grained atomic factual errors within sentences.",
            "uuid": "e7076.2",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SelfCheckGPT-n-gram",
            "name_full": "SelfCheckGPT with n-gram language model approximation",
            "brief_description": "A SelfCheckGPT variant that trains an n-gram model on the N sampled responses (plus the main response as smoothing) to approximate the generator's token probabilities and uses token log-probabilities to score sentence inconsistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "n-gram LM (trained on drawn samples)",
            "model_description": "Count-based n-gram language model trained on the set of sampled responses (and main response for smoothing); can compute Avg or Max negative log-probabilities for sentence scoring.",
            "model_size": null,
            "architecture_type": "n-gram statistical language model (approximation to black-box LM via samples)",
            "training_data": "Samples drawn from the target LLM for each query (N samples used per assessed response).",
            "reasoning_method": "Approximate token likelihood via empirical n-gram counts; unusual/rare tokens across samples lead to high Max(-log p) indicating potential hallucination.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "WikiBio (GPT-3 generated passages)",
            "benchmark_description": "See SelfCheckGPT entry.",
            "task_type": "Sentence-level hallucination detection (likelihood-based); passage-level ranking.",
            "performance_metric": "Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.",
            "performance_value": "Unigram (max) reported: Sentence-level AUC-PR NonFact 85.63 | NonFact* 41.04 | Factual 58.47. Passage-level: Pearson 64.71, Spearman 64.91. (Performance decreases for higher n in max mode.)",
            "comparison_with_baseline": "Performs better than many grey-box proxy-LLM approaches and some probability baselines; unigram (max) surprisingly effective and low-cost compared to Prompt/NLI.",
            "key_findings": "Selecting the least-likely token (max over token negative log-probabilities) across samples is an effective and computationally cheaper heuristic; higher n-grams do not improve and often degrade performance.",
            "limitations": "Requires a sufficient number of samples for reliable n-gram counts; performance plateaus slowly with increasing N and degrades for larger n; captures token-level rarity rather than semantic contradiction, so may miss subtle factual errors.",
            "uuid": "e7076.3",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SelfCheckGPT-QA (MQAG)",
            "name_full": "SelfCheckGPT with Multiple-Choice Question Answering Generation (MQAG)",
            "brief_description": "A SelfCheckGPT variant that generates multiple-choice questions from the target sentence and checks whether answers derived from the main response and other samples agree; uses answerability scoring and a Bayesian soft-counting scheme.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Question generation: T5-Large (SQuAD/RACE fine-tuned); Answering: Longformer (RACE fine-tuned)",
            "model_description": "Two-stage question generation approach (G1 and G2) using T5-Large models fine-tuned on QA datasets to create question+options, and a Longformer-based reader as the answerer/answerability estimator.",
            "model_size": null,
            "architecture_type": "sequence-to-sequence QA generation + transformer-based reader for answering and answerability",
            "training_data": "T5-Large fine-tuned on SQuAD and RACE for generation; Longformer fine-tuned on RACE for answering; answerability model fine-tuned on SQuAD2.0.",
            "reasoning_method": "Generate questions about a sentence, answer them using the main response and each sample, count matches/mismatches (soft-counted by answerability), and map counts to a Bayesian inconsistency score.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "WikiBio (GPT-3 generated passages)",
            "benchmark_description": "See SelfCheckGPT entry.",
            "task_type": "Sentence-level factuality via QA-consistency; passage-level ranking.",
            "performance_metric": "Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.",
            "performance_value": "Sentence-level AUC-PR: NonFact 84.26 | NonFact* 40.06 | Factual 48.14. Passage-level: Pearson 61.07, Spearman 59.29.",
            "comparison_with_baseline": "Beats several grey-box and proxy baselines and is competitive with other SelfCheckGPT variants, but lower than NLI and Prompt variants.",
            "key_findings": "MQAG-style QA consistency is a practical automatic way to measure information agreement between samples, and soft-counting with a Bayesian mapping improves performance.",
            "limitations": "Requires QA generation and answering components (computational cost); quality depends on generated distractors and answerability estimation; lower than Prompt/NLI in absolute accuracy on this dataset.",
            "uuid": "e7076.4",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SelfCheckGPT-BERT",
            "name_full": "SelfCheckGPT with BERTScore (RoBERTa backbone)",
            "brief_description": "A SelfCheckGPT variant that measures semantic similarity between the assessed sentence and the most similar sentence from each sample using BERTScore (RoBERTa-Large backbone) and averages corresponding similarity magnitudes to compute inconsistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERTScore (RoBERTa-Large backbone)",
            "model_description": "Embedding-based semantic similarity using pre-trained RoBERTa-Large to compute token-level cosine similarities aggregated into a sentence-level BERTScore.",
            "model_size": null,
            "architecture_type": "pretrained transformer encoder embeddings for semantic similarity",
            "training_data": "RoBERTa pretraining (not specific to logic); used off-the-shelf",
            "reasoning_method": "Embedding similarity across samples: high average similarity implies factual/consistent statements; low similarity implies hallucination.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "WikiBio (GPT-3 generated passages)",
            "benchmark_description": "See SelfCheckGPT entry.",
            "task_type": "Sentence-level semantic consistency for hallucination detection; passage-level ranking.",
            "performance_metric": "Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.",
            "performance_value": "Sentence-level AUC-PR: NonFact 81.96 | NonFact* 45.96 | Factual 44.23. Passage-level: Pearson 58.18, Spearman 55.90.",
            "comparison_with_baseline": "Outperforms some grey-box baselines in sentence detection and is competitive on passage ranking but is weaker than NLI and Prompt variants.",
            "key_findings": "Embedding-similarity (BERTScore) is an intuitive consistency measure and performs reasonably without requiring token probabilities or extra QA/NLI modules.",
            "limitations": "Semantic similarity can miss factual contradictions expressed with similar wording and may be insensitive to small but crucial factual differences.",
            "uuid": "e7076.5",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Grey-box uncertainty (GPT-3 probs / entropy)",
            "name_full": "Grey-box uncertainty-based factuality assessment using GPT-3 token probabilities and entropies",
            "brief_description": "Grey-box methods that use the LLM's output token-level probabilities (Avg(-log p), Max(-log p)) and token entropy (Avg/Max H) to detect sentences likely to be factual or hallucinated.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003) probabilities (grey-box access)",
            "model_description": "Autoregressive transformer (GPT-3 family) where token-level probability outputs are available; entropy over top-k tokens was computed where available.",
            "model_size": null,
            "architecture_type": "autoregressive transformer with token-probability outputs",
            "training_data": null,
            "reasoning_method": "Use token probability and entropy as intrinsic uncertainty signals: factual tokens have higher probability/lower entropy; hallucinated tokens have flatter distributions/higher entropy.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "WikiBio (GPT-3 generated passages)",
            "benchmark_description": "See SelfCheckGPT entry.",
            "task_type": "Sentence-level hallucination detection; passage-level ranking.",
            "performance_metric": "Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.",
            "performance_value": "Avg(-logp) reported: Sentence-level AUC-PR NonFact 83.21 | NonFact* 38.89 | Factual 53.97. Passage-level: Pearson 57.04, Spearman 53.93.",
            "comparison_with_baseline": "Strong baseline when token probabilities are available, but outperformed by SelfCheckGPT variants (Prompt and NLI).",
            "key_findings": "Token probability and entropy correlate with factuality; factual sentences tend to have higher-probability/low-entropy tokens. However, this requires grey-box access to token probabilities, which is often not available for black-box API LLMs.",
            "limitations": "Requires access to token-level probabilities (not available for many API-only systems like ChatGPT in some modes); entropy computed only over returned top-5 tokens may be insufficient.",
            "uuid": "e7076.6",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Proxy LLM approach",
            "name_full": "Proxy LLMs to approximate token probabilities of a black-box LLM",
            "brief_description": "Using an accessible LLM (e.g., LLaMA, OPT, GPT-NeoX, GPT-J) to approximate token probability distributions of a black-box generator and apply grey-box uncertainty measures.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "LLaMA (7B, 13B, 30B) and other open LLMs (OPT, GPT-NeoX, GPT-J)",
            "model_description": "Open-source autoregressive transformer models used as proxies; their token distributions are used to compute Avg(-log p) and entropy measures as a stand-in for the true black-box LLM.",
            "model_size": "7B, 13B, 30B (LLaMA variants) and other sizes for OPT/NeoX/J as detailed in appendix",
            "architecture_type": "autoregressive transformer (proxy models)",
            "training_data": null,
            "reasoning_method": "Approximate the original LLM's uncertainty by evaluating probabilities with a proxy LLM trained/fitted separately.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "WikiBio (GPT-3 generated passages)",
            "benchmark_description": "See SelfCheckGPT entry.",
            "task_type": "Sentence-level hallucination detection; passage-level ranking (using proxy probabilities).",
            "performance_metric": "Sentence-level AUC-PR; Passage-level Pearson and Spearman correlation.",
            "performance_value": "Example: LLaMA-30B Avg(-logp): Sentence-level AUC-PR NonFact 75.43 | NonFact* 30.32 | Factual 41.29. Passage-level Pearson 21.72, Spearman 20.20 (worse than original GPT-3 grey-box).",
            "comparison_with_baseline": "Proxy LLMs performed noticeably worse than using the original LLM's probabilities; some proxy models produce near-random performance depending on model mismatch.",
            "key_findings": "Proxy LLMs are an unreliable substitute for token-level uncertainty of the original black-box model due to differing generation patterns and style mismatch.",
            "limitations": "Performance degrades substantially when proxy model style differs; even large open models (e.g., LLaMA-30B) underperform the true model's grey-box metrics.",
            "uuid": "e7076.7",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Self-consistency (Chain-of-Thought mention)",
            "name_full": "Self-consistency decoding for chain-of-thought reasoning",
            "brief_description": "A decoding strategy that samples multiple chain-of-thought reasoning traces and aggregates answers (majority vote) to improve complex reasoning performance.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Referenced as a related work showing that sampling multiple reasoning traces (self-consistency) improves chain-of-thought outcomes on complex reasoning tasks.",
            "model_size": null,
            "architecture_type": "chain-of-thought prompting + sampling aggregation",
            "training_data": null,
            "reasoning_method": "Self-consistency: generate multiple reasoning paths (chain-of-thought) and aggregate answers to reduce reasoning errors.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Complex reasoning tasks (background mention in paper)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Mentioned as prior work demonstrating improvement in chain-of-thought reasoning via sampling/self-consistency; not evaluated within this paper.",
            "limitations": "Not evaluated here; computational cost and applicability to hallucination detection not explored in this work.",
            "uuid": "e7076.8",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Self-evaluation",
            "name_full": "Self-evaluation by LLMs (predicting own truthfulness)",
            "brief_description": "Prompting LLMs to evaluate the truth or confidence of their previous outputs (e.g., predict probability the generated response is true), mentioned as a related approach to assessing factuality.",
            "citation_title": "Language models (mostly) know what they know",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Approach where an LLM is prompted to self-assess its outputs; referred to as an alternate hallucination detection approach that does not require external knowledge.",
            "model_size": null,
            "architecture_type": "prompt-based self-assessment",
            "training_data": null,
            "reasoning_method": "Prompt the LLM to evaluate confidence / truthfulness of its own response to detect hallucination.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Self-evaluation / truthfulness estimation",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Cited as prior work; the paper uses LLM prompting for support checks (SelfCheckGPT-Prompt) which is related but not identical to the self-evaluation approach described in the cited work.",
            "limitations": "Not directly evaluated in this paper; self-evaluation effectiveness depends on the LLM and prompt reliability.",
            "uuid": "e7076.9",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Internal-state classifier (white-box)",
            "name_full": "Internal-state hallucination classifier using LLM hidden representations",
            "brief_description": "A white-box approach that trains a classifier on LLM internal hidden representations to predict truthfulness of generated sentences, requiring internal access to the model.",
            "citation_title": "The internal state of an llm knows when its lying",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Classifier (e.g., multi-layer perceptron) trained on internal activations of a target LLM to predict whether outputs are truthful/hallucinated.",
            "model_size": null,
            "architecture_type": "white-box classifier on model internals",
            "training_data": "Supervised labels of truthfulness required to train the classifier (per cited work).",
            "reasoning_method": "Use internal activations as features for a supervised classifier to predict hallucination.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Hallucination detection via internal-state classification (white-box)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Mentioned as a white-box alternative â€” effective but impractical when internal states are not accessible (e.g., API-only LLMs).",
            "limitations": "Requires access to internal model states and labeled training data; not applicable to black-box API settings.",
            "uuid": "e7076.10",
            "source_info": {
                "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 2
        },
        {
            "paper_title": "The internal state of an llm knows when its lying",
            "rating": 2
        },
        {
            "paper_title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
            "rating": 1
        },
        {
            "paper_title": "MQAG: Multiple-choice question answering and generation for assessing information consistency in summarization",
            "rating": 2
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models",
            "rating": 1
        }
    ],
    "cost": 0.021924,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</h1>
<p>Potsawee Manakul, Adian Liusie, Mark J. F. Gales<br>ALTA Institute, Department of Engineering, University of Cambridge<br>pm574@cam.ac.uk, al826@cam.ac.uk, mjfg@eng.cam.ac.uk</p>
<h4>Abstract</h4>
<p>Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) are capable of generating fluent and realistic responses to a variety of user prompts. They have been used in many applications such as automatic</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SelfCheckGPT with Prompt. Each LLM-generated sentence is compared against stochastically generated responses with no external database. A comparison method can be, for example, through LLM prompting as shown above.
tools to draft reports, virtual assistants and summarization systems. Despite the convincing and realistic nature of LLM-generated texts, a growing concern with LLMs is their tendency to hallucinate facts. It has been widely observed that models can confidently generate fictitious information, and worryingly there are few, if any, existing approaches to suitably identify LLM hallucinations.</p>
<p>A possible approach of hallucination detection is to leverage existing intrinsic uncertainty metrics to determine the parts of the output sequence that the system is least certain of (Yuan et al., 2021; Fu et al., 2023). However, uncertainty metrics such as token probability or entropy require access to token-level probability distributions, information which may not be available to users for example when systems are accessed through limited external APIs. An alternate approach is to leverage fact-verification approaches, where evidence is retrieved from an external database to assess the veracity of a claim (Thorne et al., 2018; Guo et al., 2022). However, facts can only be assessed relative to the knowledge present in the database. Addition-</p>
<p>ally, hallucinations are observed over a wide range of tasks beyond pure fact verification (Kryscinski et al., 2020; Maynez et al., 2020).</p>
<p>In this paper, we propose SelfCheckGPT, a sampling-based approach that can detect whether responses generated by LLMs are hallucinated or factual. To the best of our knowledge, SelfCheckGPT is the first work to analyze model hallucination of general LLM responses, and is the first zero-resource hallucination detection solution that can be applied to black-box systems. The motivating idea of SelfCheckGPT is that when an LLM has been trained on a given concept, the sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and may contradict one another. By sampling multiple responses from an LLM, one can measure information consistency between the different responses and determine if statements are factual or hallucinated. Since SelfCheckGPT only leverages sampled responses, it has the added benefit that it can be used for black-box models, and it requires no external database. Five variants of SelfCheckGPT for measuring informational consistency are considered: BERTScore, question-answering, $n$-gram, NLI, and LLM prompting. Through analysis of annotated articles generated by GPT-3, we show that SelfCheckGPT is a highly effective hallucination detection method that can even outperform greybox methods, and serves as a strong first baseline for an increasingly important problem of LLMs.</p>
<h2>2 Background and Related Work</h2>
<h3>2.1 Hallucination of Large Language Models</h3>
<p>Hallucination has been studied in text generation tasks, including summarization (Huang et al., 2021) and dialogue generation (Shuster et al., 2021), as well as in a variety of other natural language generation tasks (Ji et al., 2023). Self-consistency decoding has shown to improve chain-of-thought prompting performance on complex reasoning tasks (Wang et al., 2023). Further, Liu et al. (2022) introduce a hallucination detection dataset, however, texts are obtained by perturbing factual texts and thus may not reflect true LLM hallucination.</p>
<p>Recently, Azaria and Mitchell (2023) trained a multi-layer perception classifier where an LLM's hidden representations are used as inputs to predict the truthfulness of a sentence. However, this approach is a white-box approach that uses the internal states of the LLM, which may not be available through API calls, and requires labelled data for supervised training. Another recent approach is self-evaluation (Kadavath et al., 2022), where an LLM is prompted to evaluate its previous prediction, e.g., to predict the probability that its generated response/answer is true.</p>
<h3>2.2 Sequence Level Uncertainty Estimation</h3>
<p>Token probabilities have been used as an indication of model certainty. For example, OpenAI's GPT-3 web interface allows users to display token probabilities (as shown in Figure 2), and further uncertainty estimation approaches based on aleatoric and epistemic uncertainty have been studied for autoregressive generation (Xiao and Wang, 2021; Malinin and Gales, 2021). Additionally, conditional language model scores have been used to evaluate properties of texts (Yuan et al., 2021; Fu et al., 2023). Recently, semantic uncertainty has been proposed to address uncertainty in free-form generation tasks where probabilities are attached to concepts instead of tokens (Kuhn et al., 2023).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of OpenAI's GPT-3 web interface with output token-level probabilities displayed.</p>
<h3>2.3 Fact Verification</h3>
<p>Existing fact-verification approaches follow a multi-stage pipeline of claim detection, evidence retrieval and verdict prediction (Guo et al., 2022; Zhong et al., 2020). Such methods, however, require access to external databases and can have considerable inference costs.</p>
<h2>3 Grey-Box Factuality Assessment</h2>
<p>This section will introduce methods that can be used to determine the factuality of LLM responses in a zero-resource setting when one has full access</p>
<p>to output distributions. ${ }^{2}$ We will use 'factual' to define when statements are grounded in valid information, i.e. when hallucinations are avoided, and 'zero-resource' when no external database is used.</p>
<h3>3.1 Uncertainty-based Assessment</h3>
<p>To understand how the factuality of a generated response can be determined in a zero-resource setting, we consider LLM pre-training. During pretraining, the model is trained with next-word prediction over massive corpora of textual data. This gives the model a strong understanding of language (Jawahar et al., 2019; Raffel et al., 2020), powerful contextual reasoning (Zhang et al., 2020), as well as world knowledge (Liusie et al., 2023). Consider the input "Lionel Messi is a <em>". Since Messi is a world-famous athlete who may have appeared multiple times in pre-training, the LLM is likely to know who Messi is. Therefore given the context, the token "footballer" may be assigned a high probability while other professions such as "carpenter" may be considered improbable. However, for a different input such as "John Smith is a </em>", the system will be unsure of the continuation which may result in a flat probability distribution. During inference, this is likely to lead to a non-factual word being generated.</p>
<p>This insight allows us to understand the connection between uncertainty metrics and factuality. Factual sentences are likely to contain tokens with higher likelihood and lower entropy, while hallucinations are likely to come from positions with flat probability distributions with high uncertainty.</p>
<h2>Token-level Probability</h2>
<p>Given the LLM's response $R$, let $i$ denote the $i$-th sentence in $R, j$ denote the $j$-th token in the $i$-th sentence, $J$ is the number of tokens in the sentence, and $p_{i j}$ be the probability of the word generated by the LLM at the $j$-th token of the $i$-th sentence. Two probability metrics are used:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Avg}(-\log p)=-\frac{1}{J} \sum_{j} \log p_{i j} \
&amp; \operatorname{Max}(-\log p)=\max <em i="i" j="j">{j}\left(-\log p</em>\right)
\end{aligned}
$$</p>
<p>$\operatorname{Max}(-\log p)$ measures the sentence's likelihood by assessing the least likely token in the sentence.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Entropy</h2>
<p>The entropy of the output distribution is:</p>
<p>$$
\mathcal{H}<em _hat_w="\hat{w">{i j}=-\sum</em>)
$$} \in \mathcal{W}} p_{i j}(\hat{w}) \log p_{i j}(\hat{w</p>
<p>where $p_{i j}(\hat{w})$ is the probability of the word $\hat{w}$ being generated at the $j$-th token of the $i$-th sentence, and $\mathcal{W}$ is the set of all possible words in the vocabulary. Similar to the probability-based metrics, two entropy-based metrics are used:</p>
<p>$$
\operatorname{Avg}(\mathcal{H})=\frac{1}{J} \sum_{j} \mathcal{H}<em j="j">{i j} ; \quad \operatorname{Max}(\mathcal{H})=\max </em>\right)
$$}\left(\mathcal{H}_{i j</p>
<h2>4 Black-Box Factuality Assessment</h2>
<p>A drawback of grey-box methods is that they require output token-level probabilities. Though this may seem a reasonable requirement, for massive LLMs only available through limited API calls, such token-level information may not be available (such as with ChatGPT). Therefore, we consider black-box approaches which remain applicable even when only text-based responses are available.</p>
<h2>Proxy LLMs</h2>
<p>A simple approach to approximate the grey-box approaches is by using a proxy LLM, i.e. another LLM that we have full access to, such as LLaMA (Touvron et al., 2023). A proxy LLM can be used to approximate the output token-level probabilities of the black-box LLM generating the text. In the next section, we propose SelfCheckGPT, which is also a black-box approach.</p>
<h2>5 SelfCheckGPT</h2>
<p>SelfCheckGPT is our proposed black-box zeroresource hallucination detection scheme, which operates by comparing multiple sampled responses and measuring consistency.</p>
<p>Notation: Let $R$ refer to an LLM response drawn from a given user query. SelfCheckGPT draws a further $N$ stochastic LLM response samples $\left{S^{1}, S^{2}, \ldots, S^{n}, \ldots, S^{N}\right}$ using the same query, and then measures the consistency between the response and the stochastic samples. We design SelfCheckGPT to predict the hallucination score of the $i$-th sentence, $\mathcal{S}(i)$, such that $\mathcal{S}(i) \in[0.0,1.0]$, where $\mathcal{S}(i) \rightarrow 0.0$ if the $i$-th sentence is grounded in valid information and $\mathcal{S}(i) \rightarrow 1.0$ if the $i$-th sen-</p>
<p>tence is hallucinated. ${ }^{3}$ The following subsections will describe each of the SelfCheckGPT variants.</p>
<h3>5.1 SelfCheckGPT with BERTScore</h3>
<p>Let $\mathcal{B}(.,$.$) denote the BERTScore between two$ sentences. SelfCheckGPT with BERTScore finds the average BERTScore of the $i$-th sentence with the most similar sentence from each drawn sample:</p>
<p>$$
\mathcal{S}<em n="1">{\text {BERT }}(i)=1-\frac{1}{N} \sum</em> \max }^{N<em i="i">{k}\left(\mathcal{B}\left(r</em>\right)\right)
$$}, s_{k}^{n</p>
<p>where $r_{i}$ represents the $i$-th sentence in $R$ and $s_{k}^{n}$ represents the $k$-th sentence in the $n$-th sample $S^{n}$. This way if the information in a sentence appears in many drawn samples, one may assume that the information is factual, whereas if the statement appears in no other sample, it is likely a hallucination. In this work, RoBERTa-Large (Liu et al., 2019) is used as the backbone of BERTScore.</p>
<h3>5.2 SelfCheckGPT with Question Answering</h3>
<p>We also consider using the automatic multiplechoice question answering generation (MQAG) framework (Manakul et al., 2023) to measure consistency for SelfCheckGPT. MQAG assesses consistency by generating multiple-choice questions over the main generated response, which an independent answering system can attempt to answer while conditioned on the other sampled responses. If questions on consistent information are queried, the answering system is expected to predict similar answers. MQAG consists of two stages: question generation $G$ and question answering $A$. For the sentence $r_{i}$ in the response $R$, we draw questions $q$ and options o:</p>
<p>$$
q, \mathbf{o} \sim P_{\mathrm{G}}\left(q, \mathbf{o} \mid r_{i}, R\right)
$$</p>
<p>The answering stage A selects the answers:</p>
<p>$$
\begin{aligned}
a_{R} &amp; =\underset{k}{\operatorname{argmax}}\left[P_{\mathrm{A}}\left(o_{k} \mid q, R, \mathbf{o}\right)\right] \
a_{S^{n}} &amp; =\underset{k}{\operatorname{argmax}}\left[P_{\mathrm{A}}\left(o_{k} \mid q, S^{n}, \mathbf{o}\right)\right]
\end{aligned}
$$</p>
<p>We compare whether $a_{R}$ is equal to $a_{S^{n}}$ for each sample in $\left{S^{1}, \ldots, S^{N}\right}$, yielding #matches $N_{\mathrm{n}}$ and #not-matches $N_{\mathrm{n}}$. A simple inconsistency score for the $i$-th sentence and question $q$ based on the match/not-match counts is defined: $\mathcal{S}_{\mathrm{QA}}(i, q)=$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$\frac{N_{\mathrm{n}}}{N_{\mathrm{n}}+N_{\mathrm{n}}}$. To take into account the answerability of generated questions, we show in Appendix B that we can modify the inconsistency score by applying soft-counting, resulting in:</p>
<p>$$
\mathcal{S}<em 2="2">{\mathrm{QA}}(i, q)=\frac{\gamma</em>
$$}^{N_{\mathrm{n}}^{\prime}}}{\gamma_{1}^{N_{\mathrm{n}}^{\prime}}+\gamma_{2}^{N_{\mathrm{n}}^{\prime}}</p>
<p>where $N_{\mathrm{n}}^{\prime}=$ the effective match count, $N_{\mathrm{n}}^{\prime}=$ the effective mismatch count, with $\gamma_{1}$ and $\gamma_{2}$ defined in Appendix B.1. Ultimately, SelfCheckGPT with QA is the average of inconsistency scores across $q$,</p>
<p>$$
\mathcal{S}<em q="q">{\mathrm{QA}}(i)=\mathbb{E}</em>(i, q)]
$$}[\mathcal{S}_{\mathrm{QA}</p>
<h3>5.3 SelfCheckGPT with n-gram</h3>
<p>Given samples $\left{S^{1}, \ldots, S^{N}\right}$ generated by an LLM, one can use the samples to create a new language model that approximates the LLM. In the limit as $N$ gets sufficiently large, the new language model will converge to the LLM that generated the responses. We can therefore approximate the LLM's token probabilities using the new language model.</p>
<p>In practice, due to time and/or cost constraints, there can only be a limited number of samples $N$. Consequently, we train a simple $n$-gram model using the samples $\left{S^{1}, \ldots, S^{N}\right}$ as well as the main response $R$ (which is assessed), where we note that including $R$ can be considered as a smoothing method where the count of each token in $R$ is increased by 1 . We then compute the average of the log-probabilities of the sentence in response $R$,</p>
<p>$$
\mathcal{S}<em j="j">{n \text {-gram }}^{\text {Avg }}(i)=-\frac{1}{J} \sum</em>
$$} \log \tilde{p}_{i j</p>
<p>where $\tilde{p}_{i j}$ is the probability (of the $j$-th token of the $i$-th sentence) computed using the $n$-gram model. Similar to the grey-box approach, we can also use the maximum of the negative log probabilities,</p>
<p>$$
\mathcal{S}<em j="j">{n \text {-gram }}^{\text {Max }}(i)=\max </em>\right)
$$}\left(-\log \tilde{p}_{i j</p>
<h3>5.4 SelfCheckGPT with NLI</h3>
<p>Natural Language Inference (NLI) determines whether a hypothesis follows a premise, classified into either entailment/neutral/contradiction. NLI measures have been used to measure faithfulness in summarization, where Maynez et al. (2020) use a textual entailment classifier trained on MNLI (Williams et al., 2018) to determine if a summary contradicts a context or not. Inspired by NLI-based</p>
<p>summary assessment, we consider using the NLI contradiction score as a SelfCheckGPT score.</p>
<p>For SelfCheck-NLI, we use DeBERTa-v3-large (He et al., 2023) fine-tuned to MNLI as the NLI model. The input for NLI classifiers is typically the premise concatenated to the hypothesis, which for our methodology is the sampled passage $S^{n}$ concatenated to the sentence to be assessed $r_{i}$. Only the logits associated with the 'entailment' and 'contradiction' classes are considered,</p>
<p>$$
P\left(\operatorname{contradict} \mid r_{i}, S^{n}\right)=\frac{\exp \left(z_{c}\right)}{\exp \left(z_{e}\right)+\exp \left(z_{c}\right)}
$$</p>
<p>where $z_{e}$ and $z_{c}$ are the logits of the 'entailment' and 'contradiction' classes, respectively. This normalization ignores the neutral class and ensures that the probability is bounded between 0.0 and 1.0. The SelfCheckGPT with NLI score for each sample $S^{n}$ is then defined as,</p>
<p>$$
\mathcal{S}<em n="1">{\mathrm{NLI}}(i)=\frac{1}{N} \sum</em>\right)
$$}^{N} P\left(\operatorname{contradict} \mid r_{i}, S^{n</p>
<h3>5.5 SelfCheckGPT with Prompt</h3>
<p>LLMs have recently been shown to be effective in assessing information consistency between a document and its summary in zero-shot settings (Luo et al., 2023). Thus, we query an LLM to assess whether the $i$-th sentence is supported by sample $S^{n}$ (as the context) using the following prompt.</p>
<div class="codehilite"><pre><span></span><code>Context: {}
Sentence: {}
Is the sentence supported by the context above?
Answer Yes or No:
</code></pre></div>

<p>Initial investigation showed that GPT-3 (text-davinci-003) will output either Yes or No $98 \%$ of the time, while any remaining outputs can be set to N/A. The output from prompting when comparing the $i$-th sentence against sample $S^{n}$ is converted to score $x_{i}^{n}$ through the mapping {Yes: 0.0, No: 1.0, N/A: 0.5 }. The final inconsistency score is then calculated as:</p>
<p>$$
\mathcal{S}<em n="1">{\text {Prompt }}(i)=\frac{1}{N} \sum</em>
$$}^{N} x_{i}^{n</p>
<p>SelfCheckGPT-Prompt is illustrated in Figure 1. Note that our initial investigations found that less capable models such as GPT-3 (text-curie-001) or LLaMA failed to effectively perform consistency assessment via such prompting.</p>
<h2>6 Data and Annotation</h2>
<p>As, currently, there are no standard hallucination detection datasets available, we evaluate our hallucination detection approaches by 1) generating synthetic Wikipedia articles using GPT-3 on the individuals/concepts from the WikiBio dataset (Lebret et al., 2016); 2) manually annotating the factuality of the passage at a sentence level; 3) evaluating the system's ability to detect hallucinations.</p>
<p>WikiBio is a dataset where each input contains the first paragraph (along with tabular information) of Wikipedia articles of a specific concept. We rank the WikiBio test set in terms of paragraph length and randomly sample 238 articles from the top $20 \%$ of longest articles (to ensure no very obscure concept is selected). GPT-3 (text-davinci-003) is then used to generate Wikipedia articles on a concept, using the prompt "This is a Wikipedia passage about {concept}". Table 1 provides the statistics of GPT-3 generated passages.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#Passages</th>
<th style="text-align: center;">#Sentences</th>
<th style="text-align: center;">#Tokens/passage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">238</td>
<td style="text-align: center;">1908</td>
<td style="text-align: center;">$184.7 \pm 36.9$</td>
</tr>
</tbody>
</table>
<p>Table 1: The statistics of WikiBio GPT-3 dataset where the number of tokens is based on the OpenAI GPT-2 tokenizer.</p>
<p>We then annotate the sentences of the generated passages using the guidelines shown in Figure 3 such that each sentence is classified as either:</p>
<ul>
<li>Major Inaccurate (Non-Factual, 1): The sentence is entirely hallucinated, i.e. the sentence is unrelated to the topic.</li>
<li>Minor Inaccurate (Non-Factual, 0.5): The sentence consists of some non-factual information, but the sentence is related to the topic.</li>
<li>Accurate (Factual, 0): The information presented in the sentence is accurate.</li>
</ul>
<p>Of the 1908 annotated sentences, 761 (39.9\%) of the sentences were labelled major-inaccurate, 631 (33.1\%) minor-inaccurate, and 516 (27.0\%) accurate. 201 sentences in the dataset had annotations from two different annotators. To obtain a single label for this subset, if both annotators agree, then the agreed label is used. However, if there is disagreement, then the worse-case label is selected (e.g., {minor inaccurate, major inaccurate} is mapped to major inaccurate). The inter-annotator agreement, as measured by Cohen's $\kappa$ (Cohen, 1960), has $\kappa$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Flowchart of our annotation process
values of 0.595 and 0.748 , indicating moderate and substantial agreement (Viera et al., 2005) for the 3-class and 2-class scenarios, respectively. ${ }^{4}$</p>
<p>Furthermore, passage-level scores are obtained by averaging the sentence-level labels in each passage. The distribution of passage-level scores is shown in Figure 4, where we observe a large peak at +1.0 . We refer to the points at this peak as total hallucination, which occurs when the information of the response is unrelated to the real concept and is entirely fabricated by the LLM.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Document factuality scores histogram plot</p>
<h2>7 Experiments</h2>
<p>The generative LLM used to generate passages for our dataset is GPT-3 (text-davinci-003), the state-of-the-art system at the time of creating and annotating the dataset. To obtain the main response, we set the temperature to 0.0 and use standard beam search decoding. For the stochastically generated samples, we set the temperature to 1.0 and generate</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$N=20$ samples. For the proxy LLM approach, we use LLaMA (Touvron et al., 2023), one of the bestperforming open-source LLMs currently available. For SelfCheckGPT-Prompt, we consider both GPT3 (which is the same LLM that is used to generate passages) as well as the newly released ChatGPT (gpt-3.5-turbo). More details about the systems in SelfCheckGPT and results using other proxy LLMs can be found in the appendix.</p>
<h3>7.1 Sentence-level Hallucination Detection</h3>
<p>First, we investigate whether our hallucination detection methods can identify the factuality of sentences. In detecting non-factual sentences, both major-inaccurate labels and minor-inaccurate labels are grouped together into the non-factual class, while the factual class refers to accurate sentences. In addition, we consider a more challenging task of detecting major-inaccurate sentences in passages that are not total hallucination passages, which we refer to as non-factual ${ }^{5}$. Figure 5 and Table 2 show the performance of our approaches, where the following observations can be made:</p>
<p>1) LLM's probabilities $p$ correlate well with factuality. Our results show that probability measures (from the LLM generating the texts) are strong baselines for assessing factuality. Factual sentences can be identified with an AUC-PR of 53.97, significantly better than the random baseline of 27.04 , with the AUC-PR for hallucination detection also increasing from 72.96 to 83.21 . This supports the hypothesis that when the LLMs are uncertain about generated information, generated tokens often have higher uncertainty, paving a promising direction for hallucination detection approaches. Also, the probability $p$ measure performs better than the entropy $\mathcal{H}$ measure of top- 5 tokens.
2) Proxy LLM perform noticeably worse than LLM (GPT-3). The results of proxy LLM (based on LLaMA) show that the entropy $\mathcal{H}$ measures outperform the probability measures. This suggests that using richer uncertainty information can improve factuality/hallucination detection performance, and that previously the entropy of top- 5 tokens is likely to be insufficient. In addition, when using other proxy LLMs such as GPT-NeoX or OPT-30B, the performance is near that of the random baseline. We believe this poor performance occurs as different LLMs have different generating patterns, and so even common tokens may have a</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: PR-Curve of detecting non-factual and factual sentences in the GPT-3 generated WikiBio passages.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Sentence-level (AUC-PR)</th>
<th></th>
<th></th>
<th>Passage-level (Corr.)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>NonFact</td>
<td>NonFact*</td>
<td>Factual</td>
<td>Pearson</td>
<td>Spearman</td>
</tr>
<tr>
<td>Random</td>
<td>72.96</td>
<td>29.72</td>
<td>27.04</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-3 (text-davinci-003)'s probabilities (LLM, grey-box)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Avg(-logp)</td>
<td>83.21</td>
<td>38.89</td>
<td>53.97</td>
<td>57.04</td>
<td>53.93</td>
</tr>
<tr>
<td>Avg(H)â€ </td>
<td>80.73</td>
<td>37.09</td>
<td>52.07</td>
<td>55.52</td>
<td>50.87</td>
</tr>
<tr>
<td>Max(-logp)</td>
<td>87.51</td>
<td>35.88</td>
<td>50.46</td>
<td>57.83</td>
<td>55.69</td>
</tr>
<tr>
<td>Max(H)â€ </td>
<td>85.75</td>
<td>32.43</td>
<td>50.27</td>
<td>52.48</td>
<td>49.55</td>
</tr>
<tr>
<td>LLaMA-30B's probabilities (Proxy LLM, black-box)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Avg(-logp)</td>
<td>75.43</td>
<td>30.32</td>
<td>41.29</td>
<td>21.72</td>
<td>20.20</td>
</tr>
<tr>
<td>Avg(H)</td>
<td>80.80</td>
<td>39.01</td>
<td>42.97</td>
<td>33.80</td>
<td>39.49</td>
</tr>
<tr>
<td>Max(-logp)</td>
<td>74.01</td>
<td>27.14</td>
<td>31.08</td>
<td>-22.83</td>
<td>-22.71</td>
</tr>
<tr>
<td>Max(H)</td>
<td>80.92</td>
<td>37.32</td>
<td>37.90</td>
<td>35.57</td>
<td>38.94</td>
</tr>
<tr>
<td>SelfCheckGPT (black-box)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>w/ BERTScore</td>
<td>81.96</td>
<td>45.96</td>
<td>44.23</td>
<td>58.18</td>
<td>55.90</td>
</tr>
<tr>
<td>w/ QA</td>
<td>84.26</td>
<td>40.06</td>
<td>48.14</td>
<td>61.07</td>
<td>59.29</td>
</tr>
<tr>
<td>w/ Unigram (max)</td>
<td>85.63</td>
<td>41.04</td>
<td>58.47</td>
<td>64.71</td>
<td>64.91</td>
</tr>
<tr>
<td>w/ NLI</td>
<td>92.50</td>
<td>45.17</td>
<td>66.08</td>
<td>74.14</td>
<td>73.78</td>
</tr>
<tr>
<td>w/ Prompt</td>
<td>93.42</td>
<td>53.19</td>
<td>67.09</td>
<td>78.32</td>
<td>78.30</td>
</tr>
</tbody>
</table>
<p>Table 2: AUC-PR for sentence-level detection tasks. Passage-level ranking performances are measured by Pearson correlation coefficient and Spearman's rank correlation coefficient w.r.t. human judgements. The results of other proxy LLMs, in addition to LLaMA, can be found in the appendix. â€ GPT-3 API returns the top-5 tokens' probabilities, which are used to compute entropy.</p>
<p>low probability in situations where the response is dissimilar to the generation style of the proxy LLM. We note that a weighted conditional LM score such as BARTScore (Yuan et al., 2021) could be incorporated in future investigations.</p>
<p>3) SelfCheckGPT outperforms grey-box approaches. It can be seen that SelfCheckGPT-Prompt considerably outperforms the grey-box approaches (including GPT-3's output probabilities) as well as other black-box approaches. Even other variants of SelfCheckGPT, including BERTScore, QA, and n-gram, outperform the grey-box approaches in most setups. Interestingly, despite being the least computationally expensive method, SelfCheckGPT with unigram (max) works well across different setups. Essentially, when assessing a sentence, this method picks up the token with the lowest occurrence given all the samples. This suggests that if a token only appears a few times (or once) within the generated samples (N=20), it is likely non-factual.</p>
<p>4) SelfCheckGPT with n-gram. When inves-tigating the n-gram performance from 1-gram to 5-gram, the results show that simply finding the least likely token/n-gram is more effective than computing the average n-gram score of the sen-tence, details in appendix Table 7. Additionally, as n increases, the performance of SelfCheckGPT with n-gram (max) drops.</p>
<p>5) SelfCheckGPT with NLI. The NLI-based</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Scatter plot of passage-level scores where Y-axis = Method scores, X-axis = Human scores. Correlations are reported in Table 2. The scatter plots of other SelfCheckGPT variants are provided in Figure 10 in the appendix.</p>
<p>method outperforms all black-box and grey-box baselines, and its performance is close to the performance of the Prompt method. As SelfCheckGPT with Prompt can be computationally heavy, SelfCheckGPT with NLI could be the most practical method as it provides a good trade-off between performance and computation.</p>
<h3>7.2 Passage-level Factuality Ranking</h3>
<p>Previous results demonstrate that SelfCheckGPT is an effective approach for predicting sentence-level factuality. An additional consideration is whether SelfCheckGPT can also be used to determine the overall factuality of passages. Passage-level factuality scores are calculated by averaging the sentence-level scores over all sentences.</p>
<p>$$
\mathcal{S}<em i="i">{\text{passage}} = \frac{1}{|R|} \sum</em>
$$} \mathcal{S}(i) \tag{12</p>
<p>where $\mathcal{S}(i)$ is the sentence-level score, and $|R|$ is the number of sentences in the passage. Since human judgement is somewhat subjective, averaging the sentence-level labels would lead to ground truths with less noise. Note that for Avg(- log p) and Avg(H), we compute the average over all tokens in a passage. Whereas for Max(- log p) and Max(H), we first take the maximum operation over tokens at the sentence level, and we then average over all sentences following Equation 12.</p>
<p>Our results in Table 2 and Figure 6 show that all SelfCheckGPT methods correlate far better with human judgements than the other baselines, including the grey-box probability and entropy methods. SelfCheckGPT-Prompt is the best-performing method, achieving the highest Pearson correlation of 78.32. Unsurprisingly, the proxy LLM approach again achieves considerably lower correlations.</p>
<h3>7.3 Ablation Studies</h3>
<h4>External Knowledge (instead of SelfCheck)</h4>
<p>If external knowledge is available, one can measure the informational consistency between the LLM response and the information source. In this experiment, we use the first paragraph of each concept that is available in WikiBio.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Sent-lvl</th>
<th></th>
<th></th>
<th>Passage-lvl</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>NoFac</td>
<td>NoFac*</td>
<td>Fact</td>
<td>Pear.</td>
<td>Spear.</td>
</tr>
<tr>
<td>SelfCk-BERT</td>
<td>81.96</td>
<td>45.96</td>
<td>44.23</td>
<td>58.18</td>
<td>55.90</td>
</tr>
<tr>
<td>WikiBio+BERT</td>
<td>81.32</td>
<td>40.62</td>
<td>49.15</td>
<td>58.71</td>
<td>55.80</td>
</tr>
<tr>
<td>SelfCk-QA</td>
<td>84.26</td>
<td>40.06</td>
<td>48.14</td>
<td>61.07</td>
<td>59.29</td>
</tr>
<tr>
<td>WikiBio+QA</td>
<td>84.18</td>
<td>45.40</td>
<td>52.03</td>
<td>57.26</td>
<td>53.62</td>
</tr>
<tr>
<td>SelfCk-1gm</td>
<td>85.63</td>
<td>41.04</td>
<td>58.47</td>
<td>64.71</td>
<td>64.91</td>
</tr>
<tr>
<td>WikiBio+1gm</td>
<td>80.43</td>
<td>31.47</td>
<td>40.53</td>
<td>28.67</td>
<td>26.70</td>
</tr>
<tr>
<td>SelfCk-NLI</td>
<td>92.50</td>
<td>45.17</td>
<td>66.08</td>
<td>74.14</td>
<td>73.78</td>
</tr>
<tr>
<td>WikiBio+NLI</td>
<td>91.18</td>
<td>48.14</td>
<td>71.61</td>
<td>78.84</td>
<td>80.00</td>
</tr>
<tr>
<td>SelfCk-Prompt</td>
<td>93.42</td>
<td>53.19</td>
<td>67.09</td>
<td>78.32</td>
<td>78.30</td>
</tr>
<tr>
<td>WikiBio+Prompt</td>
<td>93.59</td>
<td>65.26</td>
<td>73.11</td>
<td>85.90</td>
<td>86.11</td>
</tr>
</tbody>
</table>
<p>Table 3: The performance when using SelfCheckGPT samples versus external stored knowledge.</p>
<p>Our findings in Table 3 show the following. First, SelfCheckGPT with BERTScore/QA, using self-samples, can yield comparable or even better performance than when using the reference passage. Second, SelfCheckGPT with n-gram shows a large performance drop when using the WikiBio passages instead of self-samples. This failure is attributed to the fact that the WikiBio reference text alone is not sufficient to train an n-gram model. Third, in contrast, SelfCheckGPT with NLI/Prompt can benefit considerably when access to retrieved information is available. Nevertheless, in practice,</p>
<p>^{6}This method is no longer zero-resource as it requires retrieving relevant knowledge from external data.</p>
<p>it is infeasible to have an external database for every possible use case of LLM generation.</p>
<h2>The Impact of the Number of Samples</h2>
<p>Although sample-based methods are expected to perform better when more samples are drawn, this has higher computational costs. Thus, we investigate performance as the number of samples is varied. Our results in Figure 7 show that the performance of SelfCheckGPT increases smoothly as more samples are used, with diminishing gains as more samples are generated. SelfCheckGPT with $n$-gram requires the highest number of samples before its performance reaches a plateau.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The performance of SelfCheckGPT methods on ranking passages (Spearman's) versus the number of samples.</p>
<h2>The Choice of LLM for SelfCheckGPT-Prompt</h2>
<p>We investigate whether the LLM generating the text can self-check its own text. We conduct this ablation using a reduced set of the samples $(N=4)$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Text-Gen</th>
<th style="text-align: left;">SelfCk-Prompt</th>
<th style="text-align: right;">$N$</th>
<th style="text-align: left;">Pear.</th>
<th style="text-align: left;">Spear.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: right;">20</td>
<td style="text-align: left;">78.32</td>
<td style="text-align: left;">78.30</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: right;">4</td>
<td style="text-align: left;">76.47</td>
<td style="text-align: left;">76.41</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: right;">4</td>
<td style="text-align: left;">73.11</td>
<td style="text-align: left;">74.69</td>
</tr>
<tr>
<td style="text-align: left;">${ }^{\dagger}$ SelfCheck w/ unigram (max)</td>
<td style="text-align: left;">20</td>
<td style="text-align: right;">64.71</td>
<td style="text-align: left;">64.91</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">${ }^{\dagger}$ SelfCheck w/ NLI</td>
<td style="text-align: left;">20</td>
<td style="text-align: right;">74.14</td>
<td style="text-align: left;">73.78</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of GPT-3 (text-davinci-003) and ChatGPT (gpt-3.5.turbo) as the prompt-based text evaluator in SelfCheckGPT-Prompt. ${ }^{\dagger}$ Taken from Table 2 for comparison.</p>
<p>The results in Table 4 show that GPT-3 can selfcheck its own text, and is better than the unigram method even when using only 4 samples. However, ChatGPT shows a slight improvement over GPT-3 in evaluating whether the sentence is supported by the context. More details are in Appendix C.</p>
<h2>8 Conclusions</h2>
<p>This paper is the first work to consider the task of hallucination detection for general large language model responses. We propose SelfCheckGPT, a zero-resource approach that is applicable to any black-box LLM without the need for external resources, and demonstrate the efficacy of our method. SelfCheckGPT outperforms a range of considered grey-box and black-box baseline detection methods at both the sentence and passage levels, and we further release an annotated dataset for GPT-3 hallucination detection with sentencelevel factuality labels.</p>
<h2>Limitations</h2>
<p>In this study, the 238 GPT-3 generated texts were predominantly passages about individuals in the WikiBio dataset. To further investigate the nature of LLM's hallucination, this study could be extended to a wider range of concepts, e.g., to also consider generated texts about locations and objects. Further, this work considers factuality at the sentence level, but we note that a single sentence may consist of both factual and non-factual information. For example, the following work by Min et al. (2023) considers a fine-grained factuality evaluation by decomposing sentences into atomic facts. Finally, SelfCheckGPT with Prompt, which was convincingly the best selfcheck method, is quite computationally heavy. This might lead to impractical computational costs, which could be addressed in future work to be made more efficient.</p>
<h2>Ethics Statement</h2>
<p>As this work addresses the issue of LLM's hallucination, we note that if hallucinated contents are not detected, they could lead to misinformation.</p>
<h2>Acknowledgments</h2>
<p>This work is supported by Cambridge University Press \&amp; Assessment (CUP\&amp;A), a department of The Chancellor, Masters, and Scholars of the University of Cambridge, and the Cambridge Commonwealth, European \&amp; International Trust. We would like to thank the anonymous reviewers for their helpful comments.</p>
<h2>References</h2>
<p>Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734.</p>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.</p>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of BigScience Episode #5 - Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37 - 46.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.</p>
<p>Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics, 10:178-206.</p>
<p>Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations.</p>
<p>Yichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2021. The factual inconsistency problem in abstractive text summarization: A survey.</p>
<p>Ganesh Jawahar, BenoÃ®t Sagot, and DjamÃ© Seddah. 2019. What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651-3657, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea</p>
<p>Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12).</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785794, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>RÃ©mi Lebret, David Grangier, and Michael Auli. 2016. Generating text from structured data with application to the biography domain. CoRR, abs/1603.07771.</p>
<p>Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022. A token-level reference-free hallucination detection benchmark for free-form text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6723-6737, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Adian Liusie, Vatsal Raina, and Mark Gales. 2023. "world knowledge" in multiple choice reading comprehension. In Proceedings of the Sixth Fact Extraction and VERification Workshop (FEVER), pages 49-57, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for abstractive text summarization. arXiv preprint arXiv:2303.15621.</p>
<p>Andrey Malinin and Mark Gales. 2021. Uncertainty estimation in autoregressive structured prediction. In International Conference on Learning Representations.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. MQAG: Multiple-choice question answering and generation for assessing information consistency in summarization. arXiv preprint arXiv:2301.12307.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Vatsal Raina and Mark Gales. 2022. Answer uncertainty and unanswerability in multiple-choice machine reading comprehension. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1020-1034, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784-3803, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. 2018. The Fact Extraction and VERification (FEVER) shared task. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER).</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Anthony J Viera, Joanne M Garrett, et al. 2005. Understanding interobserver agreement: the kappa statistic. Fam med, 37(5):360-363.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Yijun Xiao and William Yang Wang. 2021. On hallucination and predictive uncertainty in conditional language generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2734-2744, Online. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, and Xiang Zhou. 2020. Semantics-aware bert for language understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9628-9635.</p>
<p>Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020. Reasoning over semantic-level graph for fact checking. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6170-6180, Online. Association for Computational Linguistics.</p>
<h2>A Models and Implementation</h2>
<h2>A. 1 Entropy</h2>
<p>The entropy of the output distribution is implemented as follows,</p>
<p>$$
\mathcal{H}<em _tilde_w="\tilde{w">{i j}=2^{-\sum</em>) \log } \in \mathcal{W}} p_{i j}(\tilde{w<em i="i" j="j">{2} p</em>
$$}(\tilde{w})</p>
<p>where $\mathcal{W}$ is the set of all possible words in the vocabulary.</p>
<h2>A. 2 Proxy LLMs</h2>
<p>The proxy LLMs considered are LLaMA-{7B, 13B, 30B} (Touvron et al., 2023), OPT-{125m, 1.3B, 13B, 30B} (Zhang et al., 2022), GPT-J-6B (Wang and Komatsuzaki, 2021) and GPT-NeoX20B (Black et al., 2022).</p>
<h2>A. 3 SelfCheckGPT's Systems</h2>
<p>Question Answering: The generation systems G1 and G2 are T5-Large fine-tuned to SQuAD (Rajpurkar et al., 2016) and RACE (Lai et al., 2017), respectively. The answering system A is Longformer (Beltagy et al., 2020) fine-tuned to the RACE dataset. The answerability system $U$ is also Longformer, but fine-tuned to SQuAD2.0.</p>
<p>LLM for Prompting: We consider two LLMs, GPT-3 (text-davinci-003) and ChatGPT (gpt-3.5turbo) We note that during the data creation and annotation, GPT-3 (text-davinci-003) was the state-of-the-art LLM available; hence, GPT-3 was used as the main LLM generating WikiBio passages.</p>
<h2>B SelfCheckGPT with QA</h2>
<p>Previous work showed that implementing question generation (in Equation 2) with two generators (G1 generates the question and associated answer, and G2 generates distractors) yields higher-quality distractors (Manakul et al., 2023). Thus, a two-stage generation is adopted in this work as follows:</p>
<p>$$
q, a \sim P_{\mathrm{G} 1}\left(q, a \mid r_{i}\right) ; \quad \mathbf{o}<em _mathrm_G="\mathrm{G">{\backslash a} \sim P</em> \mid q, a, R\right)
$$} 2}\left(\mathbf{o}_{\backslash a</p>
<p>where $\mathbf{o}=\left{a, \mathbf{o}<em 1="1">{\backslash a}\right}=\left{o</em>\right}$. In addition, to filter out bad (unanswerable) questions, we define an answerability score (Raina and Gales, 2022):}, \ldots, o_{4</p>
<p>$$
\alpha=P_{\mathrm{0}}(\text { answerable } \mid q, \text { context })
$$</p>
<p>where the context is either the response $R$ or sampled passages $S^{n}$, and $\alpha \rightarrow 0.0$ for unanswerable and $\alpha \rightarrow 1.0$ for answerable. We use $\alpha$ to filter out
unanswerable questions which have $\alpha$ lower than a threshold. Next, we derive how Bayes' theorem can be applied to take into account the number of answerable/unanswerable questions.</p>
<h2>B. 1 SelfCheckGPT-QA with Bayes</h2>
<p>Let $P(\mathrm{~F})$ denote the probability of the $i$-th sentence being non-factual, and $P(\mathrm{~T})$ denote the probability of the $i$-th sentence being factual. For a question $q$, the probability of $i$-th sentence being non-factual given a set of matched answers $L_{\mathrm{n}}$ and a set of not-matched answers $L_{\mathrm{n}}$ is:</p>
<p>$$
\begin{aligned}
&amp; P(\mathrm{~F} \mid L_{\mathrm{n}}, L_{\mathrm{n}}) \
&amp; =\frac{P\left(L_{\mathrm{n}}, L_{\mathrm{n}} \mid \mathrm{F}\right) P(\mathrm{~F})}{P\left(L_{\mathrm{n}}, L_{\mathrm{n}} \mid \mathrm{F}\right) P(\mathrm{~F})+P\left(L_{\mathrm{n}}, L_{\mathrm{n}} \mid \mathrm{T}\right) P(\mathrm{~T})} \
&amp; =\frac{P\left(L_{\mathrm{n}}, L_{\mathrm{n}} \mid \mathrm{F}\right)}{P\left(L_{\mathrm{n}}, L_{\mathrm{n}} \mid \mathrm{F}\right)+P\left(L_{\mathrm{n}}, L_{\mathrm{n}} \mid \mathrm{T}\right)}
\end{aligned}
$$</p>
<p>where we assume the sentence is equally likely to be False or True, i.e. $P(\mathrm{~F})=P(\mathrm{~T})$. The probability of observing $L_{\mathrm{n}}, L_{\mathrm{n}}$ when the sentence is False (non-factual):</p>
<p>$$
\begin{aligned}
&amp; P\left(L_{\mathrm{n}}, L_{\mathrm{n}} \mid \mathrm{F}\right) \
&amp; =\prod_{a \in L_{\mathrm{n}}} P\left(a=a_{R} \mid F\right) \prod_{a^{\prime} \in L_{\mathrm{n}}} P\left(a^{\prime} \neq a_{R} \mid F\right) \
&amp; =\left(1-\beta_{1}\right)^{N_{\mathrm{n}}}\left(\beta_{1}\right)^{N_{\mathrm{n}}}
\end{aligned}
$$</p>
<p>and probability of observing $L_{\mathrm{n}}, L_{\mathrm{n}}$ when the sentence is True (factual):</p>
<p>$$
\begin{aligned}
&amp; P\left(L_{\mathrm{n}}, L_{\mathrm{n}} \mid \mathrm{T}\right) \
&amp; =\prod_{a \in L_{\mathrm{n}}} P\left(a=a_{r} \mid T\right) \prod_{a^{\prime} \in L_{\mathrm{n}}} P\left(a^{\prime} \neq a_{r} \mid T\right) \
&amp; =\left(\beta_{2}\right)^{N_{\mathrm{n}}}\left(1-\beta_{2}\right)^{N_{\mathrm{n}}}
\end{aligned}
$$</p>
<p>where $N_{\mathrm{n}}$ and $N_{\mathrm{n}}$ are the number of matched answers and the number of not-matched answers, respectively. Hence, we can simplify Equation 16:</p>
<p>$$
P\left(\mathrm{~F} \mid L_{\mathrm{n}}, L_{\mathrm{n}}\right)=\frac{\gamma_{2}^{N_{\mathrm{n}}}}{\gamma_{1}^{N_{\mathrm{n}}}+\gamma_{2}^{N_{\mathrm{n}}}}
$$</p>
<p>where $\gamma_{1}=\frac{\beta_{2}}{1-\beta_{1}}$ and $\gamma_{2}=\frac{\beta_{1}}{1-\beta_{2}}$. Lastly, instead of rejecting samples having an answerability score below a threshold, ${ }^{7}$ we find empirically that softcounting (defined below) improves the detection performance. We set both $\beta_{1}$ and $\beta_{2}$ to 0.8 .</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$$
N_{\mathrm{R}}^{\prime}=\sum_{n \text { s.t. } n_{\mathrm{n}} \in L_{\mathrm{R}}} \alpha_{n} ; \quad N_{\mathrm{R}}^{\prime}=\sum_{n \text { s.t. } n_{\mathrm{n}} \in L_{\mathrm{R}}} \alpha_{n}
$$</p>
<p>where $\alpha_{n}=P_{\emptyset}\left(\right.$ answerable $\left|q, S^{n}\right)$. Therefore, the SelfCheckGPT with QA score, $\mathcal{S}_{\mathrm{QA}}$, is:</p>
<p>$$
\mathcal{S}<em _mathrm_R="\mathrm{R">{\mathrm{QA}}=P\left(\mathrm{~F} \mid L</em>
$$}}, L_{\mathrm{R}}\right)=\frac{\gamma_{2}^{N_{\mathrm{R}}^{\prime}}}{\gamma_{1}^{N_{\mathrm{R}}^{\prime}}+\gamma_{2}^{N_{\mathrm{R}}^{\prime}}</p>
<p>In Table 5, we show empically that applying Bayes' theorem and soft counting $\alpha$ (in Equation 20) improves the performance of the SelfCheckGPT with QA method.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Varaint</th>
<th style="text-align: center;">Sentence-lvl</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Passage-lvl</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">NoF</td>
<td style="text-align: center;">NoF*</td>
<td style="text-align: center;">Fact</td>
<td style="text-align: center;">PCC</td>
<td style="text-align: center;">SCC</td>
</tr>
<tr>
<td style="text-align: left;">SimpleCount</td>
<td style="text-align: center;">83.97</td>
<td style="text-align: center;">40.07</td>
<td style="text-align: center;">47.78</td>
<td style="text-align: center;">57.39</td>
<td style="text-align: center;">55.15</td>
</tr>
<tr>
<td style="text-align: left;">+ Bayes</td>
<td style="text-align: center;">83.04</td>
<td style="text-align: center;">38.58</td>
<td style="text-align: center;">47.41</td>
<td style="text-align: center;">56.43</td>
<td style="text-align: center;">55.03</td>
</tr>
<tr>
<td style="text-align: left;">+ Bayes $+\alpha$</td>
<td style="text-align: center;">84.26</td>
<td style="text-align: center;">40.06</td>
<td style="text-align: center;">48.14</td>
<td style="text-align: center;">61.07</td>
<td style="text-align: center;">59.29</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of SelfCheckGPT-QA's variants.</p>
<h2>C SelfCheckGPT with Prompt</h2>
<p>We use the prompt template provided in the main text (in Section 5.5) for both GPT-3 (text-davinci003) and ChatGPT (gpt-3.5-turbo). For ChatGPT, a standard system message "You are a helpful assistant." is used in setting up the system.</p>
<p>At the time of conducting experiments, the API costs per 1,000 tokens are $\$ 0.020$ for GPT-3 and $\$ 0.002$ for ChatGPT. The estimated costs for running the models to answer Yes/No on all 1908 sentences and 20 samples are around $\$ 200$ for GPT-3 and $\$ 20$ for ChatGPT. Given the cost, we conduct the experiments on 4 samples when performing the ablation about LLM choice for SelfCheckGPTPrompt (Section 7.3). Table 6 shows the breakdown of predictions made by GPT-3 and ChatGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">GPT-3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Yes</th>
<th style="text-align: center;">No</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">3179</td>
<td style="text-align: center;">1038</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">367</td>
<td style="text-align: center;">3048</td>
</tr>
</tbody>
</table>
<p>Table 6: Breakdown of predictions made by GPT-3/ChatGPT when prompted to answer Yes(supported)/No(not-supported).</p>
<h2>D Additional Experimental Results</h2>
<p>Here, we provide experimental results that are complementary to those presented in the main paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$n$-gram</th>
<th style="text-align: center;">Sent-lvl AUC-PR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Passage-lvl</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NoFac</td>
<td style="text-align: center;">NoFac*</td>
<td style="text-align: center;">Fact</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Spear.</td>
</tr>
<tr>
<td style="text-align: center;">Avg( $=\log p)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1-gram</td>
<td style="text-align: center;">81.52</td>
<td style="text-align: center;">40.33</td>
<td style="text-align: center;">41.76</td>
<td style="text-align: center;">40.68</td>
<td style="text-align: center;">39.22</td>
</tr>
<tr>
<td style="text-align: center;">2-gram</td>
<td style="text-align: center;">82.94</td>
<td style="text-align: center;">44.38</td>
<td style="text-align: center;">52.81</td>
<td style="text-align: center;">58.84</td>
<td style="text-align: center;">58.11</td>
</tr>
<tr>
<td style="text-align: center;">3-gram</td>
<td style="text-align: center;">83.56</td>
<td style="text-align: center;">44.64</td>
<td style="text-align: center;">53.99</td>
<td style="text-align: center;">62.21</td>
<td style="text-align: center;">63.00</td>
</tr>
<tr>
<td style="text-align: center;">4-gram</td>
<td style="text-align: center;">83.80</td>
<td style="text-align: center;">43.55</td>
<td style="text-align: center;">54.25</td>
<td style="text-align: center;">61.98</td>
<td style="text-align: center;">63.64</td>
</tr>
<tr>
<td style="text-align: center;">5-gram</td>
<td style="text-align: center;">83.45</td>
<td style="text-align: center;">42.31</td>
<td style="text-align: center;">53.98</td>
<td style="text-align: center;">60.68</td>
<td style="text-align: center;">62.96</td>
</tr>
<tr>
<td style="text-align: center;">Max( $=\log p)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1-gram</td>
<td style="text-align: center;">85.63</td>
<td style="text-align: center;">41.04</td>
<td style="text-align: center;">58.47</td>
<td style="text-align: center;">64.71</td>
<td style="text-align: center;">64.91</td>
</tr>
<tr>
<td style="text-align: center;">2-gram</td>
<td style="text-align: center;">85.26</td>
<td style="text-align: center;">39.29</td>
<td style="text-align: center;">58.29</td>
<td style="text-align: center;">62.48</td>
<td style="text-align: center;">66.04</td>
</tr>
<tr>
<td style="text-align: center;">3-gram</td>
<td style="text-align: center;">84.97</td>
<td style="text-align: center;">37.10</td>
<td style="text-align: center;">57.08</td>
<td style="text-align: center;">57.34</td>
<td style="text-align: center;">60.49</td>
</tr>
<tr>
<td style="text-align: center;">4-gram</td>
<td style="text-align: center;">84.49</td>
<td style="text-align: center;">36.37</td>
<td style="text-align: center;">55.96</td>
<td style="text-align: center;">55.77</td>
<td style="text-align: center;">57.25</td>
</tr>
<tr>
<td style="text-align: center;">5-gram</td>
<td style="text-align: center;">84.12</td>
<td style="text-align: center;">36.19</td>
<td style="text-align: center;">54.89</td>
<td style="text-align: center;">54.84</td>
<td style="text-align: center;">55.97</td>
</tr>
</tbody>
</table>
<p>Table 7: The performance using different $n$-gram models in the SelfCheckGPT with $n$-gram method.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The performance of SelfCheckGPT methods on sentence-level non-factual detection (AUC-PR) versus the number of samples. This Figure extends the passage-level results in Figure 7.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Passage-level ranking performance of the Avg $(\mathcal{H})$ method using proxy LLM where the sizes are: LLaMA $={7 \mathrm{~B}$, 13B, 30B $} \mathrm{OPt}={125 \mathrm{~m}, 1.3 \mathrm{~B}, 13 \mathrm{~B}, 30 \mathrm{~B}}$, GPT-J=6B, NeoX=20B. The full results are provided in Table 8.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Scatter plot of passage-level scores where Y-axis $=$ Method scores, X -axis $=$ Human scores. Correlations are reported in Table 2. This figure provides results in addition to Figure 6.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Sentence-level (AUC-PR)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Passage-level (Corr.)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NonFact</td>
<td style="text-align: center;">NonFact*</td>
<td style="text-align: center;">Factual</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Spearman</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">72.96</td>
<td style="text-align: center;">29.72</td>
<td style="text-align: center;">27.04</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Avg( $-\log p)$ Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">75.43</td>
<td style="text-align: center;">30.32</td>
<td style="text-align: center;">41.29</td>
<td style="text-align: center;">21.72</td>
<td style="text-align: center;">20.20</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">74.16</td>
<td style="text-align: center;">30.01</td>
<td style="text-align: center;">37.36</td>
<td style="text-align: center;">13.33</td>
<td style="text-align: center;">12.89</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">71.69</td>
<td style="text-align: center;">27.87</td>
<td style="text-align: center;">31.30</td>
<td style="text-align: center;">$-2.71$</td>
<td style="text-align: center;">$-2.59$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">67.70</td>
<td style="text-align: center;">24.43</td>
<td style="text-align: center;">25.04</td>
<td style="text-align: center;">$-32.07$</td>
<td style="text-align: center;">$-31.45$</td>
</tr>
<tr>
<td style="text-align: center;">NeoX</td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">69.00</td>
<td style="text-align: center;">24.38</td>
<td style="text-align: center;">26.18</td>
<td style="text-align: center;">$-31.79$</td>
<td style="text-align: center;">$-34.15$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">67.46</td>
<td style="text-align: center;">24.39</td>
<td style="text-align: center;">25.20</td>
<td style="text-align: center;">$-33.05$</td>
<td style="text-align: center;">$-32.79$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">67.51</td>
<td style="text-align: center;">24.28</td>
<td style="text-align: center;">24.26</td>
<td style="text-align: center;">$-38.80$</td>
<td style="text-align: center;">$-40.05$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">66.19</td>
<td style="text-align: center;">24.47</td>
<td style="text-align: center;">23.47</td>
<td style="text-align: center;">$-35.20$</td>
<td style="text-align: center;">$-38.95$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">125 m</td>
<td style="text-align: center;">66.63</td>
<td style="text-align: center;">25.31</td>
<td style="text-align: center;">23.07</td>
<td style="text-align: center;">$-30.38$</td>
<td style="text-align: center;">$-37.54$</td>
</tr>
<tr>
<td style="text-align: center;">Avg( $N$ ) Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">80.80</td>
<td style="text-align: center;">39.01</td>
<td style="text-align: center;">42.97</td>
<td style="text-align: center;">33.80</td>
<td style="text-align: center;">39.49</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">80.63</td>
<td style="text-align: center;">38.98</td>
<td style="text-align: center;">40.59</td>
<td style="text-align: center;">29.43</td>
<td style="text-align: center;">33.12</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">78.67</td>
<td style="text-align: center;">37.22</td>
<td style="text-align: center;">33.81</td>
<td style="text-align: center;">19.44</td>
<td style="text-align: center;">21.79</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">77.13</td>
<td style="text-align: center;">33.67</td>
<td style="text-align: center;">29.55</td>
<td style="text-align: center;">$-0.43$</td>
<td style="text-align: center;">3.43</td>
</tr>
<tr>
<td style="text-align: center;">NeoX</td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">77.40</td>
<td style="text-align: center;">32.78</td>
<td style="text-align: center;">30.13</td>
<td style="text-align: center;">5.41</td>
<td style="text-align: center;">7.43</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">76.93</td>
<td style="text-align: center;">33.71</td>
<td style="text-align: center;">29.68</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">1.39</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">76.15</td>
<td style="text-align: center;">33.29</td>
<td style="text-align: center;">28.30</td>
<td style="text-align: center;">$-2.50$</td>
<td style="text-align: center;">$-1.37$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">74.05</td>
<td style="text-align: center;">31.91</td>
<td style="text-align: center;">26.33</td>
<td style="text-align: center;">$-10.59$</td>
<td style="text-align: center;">$-10.00$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">125 m</td>
<td style="text-align: center;">71.51</td>
<td style="text-align: center;">30.88</td>
<td style="text-align: center;">25.36</td>
<td style="text-align: center;">$-14.16$</td>
<td style="text-align: center;">$-13.76$</td>
</tr>
<tr>
<td style="text-align: center;">Max( $-\log p)$ Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">74.01</td>
<td style="text-align: center;">27.14</td>
<td style="text-align: center;">31.08</td>
<td style="text-align: center;">$-22.83$</td>
<td style="text-align: center;">$-22.71$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">71.12</td>
<td style="text-align: center;">26.78</td>
<td style="text-align: center;">28.82</td>
<td style="text-align: center;">$-34.93$</td>
<td style="text-align: center;">$-31.70$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">69.57</td>
<td style="text-align: center;">25.91</td>
<td style="text-align: center;">26.54</td>
<td style="text-align: center;">$-42.57$</td>
<td style="text-align: center;">$-38.24$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">67.32</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">24.32</td>
<td style="text-align: center;">$-49.51$</td>
<td style="text-align: center;">$-45.50$</td>
</tr>
<tr>
<td style="text-align: center;">NeoX</td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">67.51</td>
<td style="text-align: center;">23.88</td>
<td style="text-align: center;">24.82</td>
<td style="text-align: center;">$-47.96$</td>
<td style="text-align: center;">$-44.54$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">67.36</td>
<td style="text-align: center;">24.67</td>
<td style="text-align: center;">24.46</td>
<td style="text-align: center;">$-50.15$</td>
<td style="text-align: center;">$-44.42$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">67.58</td>
<td style="text-align: center;">23.94</td>
<td style="text-align: center;">23.93</td>
<td style="text-align: center;">$-51.23$</td>
<td style="text-align: center;">$-47.68$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">68.16</td>
<td style="text-align: center;">25.85</td>
<td style="text-align: center;">24.66</td>
<td style="text-align: center;">$-45.60$</td>
<td style="text-align: center;">$-42.39$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">125 m</td>
<td style="text-align: center;">69.23</td>
<td style="text-align: center;">27.66</td>
<td style="text-align: center;">24.14</td>
<td style="text-align: center;">$-39.22$</td>
<td style="text-align: center;">$-37.18$</td>
</tr>
<tr>
<td style="text-align: center;">Max( $N$ ) Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">80.92</td>
<td style="text-align: center;">37.32</td>
<td style="text-align: center;">37.90</td>
<td style="text-align: center;">35.57</td>
<td style="text-align: center;">38.94</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">80.98</td>
<td style="text-align: center;">37.94</td>
<td style="text-align: center;">36.01</td>
<td style="text-align: center;">32.07</td>
<td style="text-align: center;">34.01</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">79.65</td>
<td style="text-align: center;">35.57</td>
<td style="text-align: center;">31.32</td>
<td style="text-align: center;">22.10</td>
<td style="text-align: center;">22.53</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">76.58</td>
<td style="text-align: center;">33.44</td>
<td style="text-align: center;">29.31</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">6.41</td>
</tr>
<tr>
<td style="text-align: center;">NeoX</td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">76.98</td>
<td style="text-align: center;">31.96</td>
<td style="text-align: center;">29.13</td>
<td style="text-align: center;">5.97</td>
<td style="text-align: center;">9.31</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">76.26</td>
<td style="text-align: center;">32.81</td>
<td style="text-align: center;">29.25</td>
<td style="text-align: center;">1.42</td>
<td style="text-align: center;">2.82</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">75.30</td>
<td style="text-align: center;">32.51</td>
<td style="text-align: center;">28.13</td>
<td style="text-align: center;">$-2.14$</td>
<td style="text-align: center;">1.41</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">73.79</td>
<td style="text-align: center;">31.42</td>
<td style="text-align: center;">26.38</td>
<td style="text-align: center;">$-9.84$</td>
<td style="text-align: center;">$-9.80$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">125 m</td>
<td style="text-align: center;">71.32</td>
<td style="text-align: center;">31.65</td>
<td style="text-align: center;">25.36</td>
<td style="text-align: center;">$-18.05$</td>
<td style="text-align: center;">$-17.37$</td>
</tr>
</tbody>
</table>
<p>Table 8: AUC-PR for Detecting Non-Factual and Factual Sentences in the GPT-3 generated WikiBio passages. Passage-level PCC and SCC with LLMs used to assess GPT-3 responses. This table is an extension to Table 2.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7} \alpha$ is between 0.0 (unanswerable) and 1.0 (answerable). Standard-counting $N_{\mathrm{n}}$ and $N_{\mathrm{n}}$ can be considered as a special case of soft-counting where $\alpha$ is set to 1.0 if $\alpha$ is greater than the answerability threshold and otherwise $\alpha$ is 0.0 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ There are 206 non-factual ${ }^{\star}$ passages (1632 sentences).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
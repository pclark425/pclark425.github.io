<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8902 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8902</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8902</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-88427d2143d4cff357c3b393ae7580a7b6e19940</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/88427d2143d4cff357c3b393ae7580a7b6e19940" target="_blank">Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules</a></p>
                <p><strong>Paper Venue:</strong> ACS Central Science</p>
                <p><strong>Paper TL;DR:</strong> A method to convert discrete representations of molecules to and from a multidimensional continuous representation that allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds is reported.</p>
                <p><strong>Paper Abstract:</strong> We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in a set of molecules with fewer that nine heavy atoms.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8902.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8902.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE (SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder applied to SMILES-based molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational autoencoder maps SMILES strings to a continuous latent vector (156 dims for QM9, 196 dims for ZINC) and decodes latent points back to SMILES to generate novel molecules; trained jointly with a property predictor in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Variational Autoencoder (VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational autoencoder (probabilistic latent-variable model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>latent dimensionality: 156 (QM9) and 196 (ZINC); encoder/decoder network sizes as reported (see training details)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES strings from QM9 (~108k/134k small molecules) and ZINC (~250k drug-like molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug-like molecule design and small-molecule property optimization (QM9 electronic properties)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Decode random latent vectors sampled from prior, perturb/ interpolate latent vectors of known molecules, or optimize latent vectors via a surrogate model (Gaussian process) then decode; decoder is stochastic (character softmax sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generated molecules largely follow training-distribution statistics but are typically novel: e.g., for ZINC-trained VAE, ~4.5% of generated molecules were found in the source ZINC library and ~7.0% in eMolecules (Table 1); decoding validity for random latent points ~73.9% (ZINC) and ~79.3% (QM9) (Table 4); many decoded molecules are new combinatorial variants not present in training set.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced by joint training with a property predictor (MLP) on the latent vectors and by using a Gaussian process surrogate to optimize latent vectors for an objective (example objective: 5*QED - SAS); latent space becomes organized by property values allowing targeted sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity rate of decoded SMILES, percent overlap with ZINC/eMolecules, property distributions (logP, SAS, QED, NP), property prediction MAE (see Table 2), objective score (percentile of 5*QED - SAS), reconstruction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>VAE yields a smooth continuous latent space of molecules; sampling/decoding yields realistic molecules with property distributions matching training data; joint property training organizes latent space by property; optimization in latent space via GP produces molecules with higher objective values than random search or a GA baseline; decoding validity is substantial but imperfect (e.g., 70–80% for random latent points for ZINC).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to hand-designed mutation rules and a genetic algorithm baseline, VAE-generated molecules follow training-set distributions more closely and generate novel compounds, whereas GA tended to bias toward higher complexity and decreased drug-likeness; property-prediction performance compared to ECFP and graph convolutions: VAE-based predictor achieves comparable but not uniformly superior MAEs (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Decoder frequently produces invalid SMILES (validity ranged widely, 70% to <1% in some settings); latent-space 'dead areas' that decode invalidly; decoder must learn SMILES syntax which is fragile; some generated molecules contain undesirable moieties (unstable or synthetically impractical); property-prediction accuracy limited by model and data; need for graph-based decoders or grammar-based approaches to reduce invalid outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8902.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8902.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq RNN (GRU) SMILES AE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence recurrent neural network autoencoder with GRU decoder for SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence autoencoder using recurrent neural networks (GRU layers in the decoder) to map SMILES strings to and from a continuous representation; trained as the decoder portion of the VAE/autoencoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seq2Seq RNN Autoencoder (GRU decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent neural network sequence-to-sequence (GRU) with teacher forcing</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>decoder GRU hidden dimension: 488 (ZINC model) or 500 (QM9 model); three GRU layers in decoder</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Canonicalized SMILES strings from ZINC and QM9 (padded to fixed max length: ZINC max 120 chars, QM9 max 34 chars)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation of drug-like molecules and small molecules</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Decoder samples characters sequentially from a softmax at each position (stochastic decoding); used to reconstruct/ generate SMILES from latent vectors; teacher forcing used during training to increase generation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Supports stochastic decoding from latent points producing multiple variant SMILES (frequent decoding corresponds to nearest decoded molecule when re-encoded); novelty governed by sampled latent points and training data combinatorics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Combined with joint property prediction and latent-space optimization, enables targeted generation of molecules with desired properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SMILES reconstruction fidelity, percentage of valid SMILES decoded for training/test and random latent points (see Tables 3 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GRU-based decoders produce realistic SMILES and allow stochastic sampling; teacher forcing improved valid-SMILES fraction but risked decoder ignoring latent encoding; decoding stochasticity leads to multiple close variants for a latent point.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Convolutional encoders paired with RNN decoders gave improved performance for encoding (authors experimented with convolutional encoders), but RNN decoders are standard for sequence outputs and used here successfully.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Fragility of SMILES syntax makes decoder prone to invalid outputs; teacher forcing can cause decoder to ignore latent codes; sampling/stochastic decoding adds variability and requires postprocessing/chemical validation (RDKit).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8902.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8902.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conv encoder + RNN decoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional encoder paired with recurrent (GRU) decoder for SMILES autoencoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model variant using 1D convolutional layers as the encoder to exploit translationally-invariant substrings in SMILES (substructures), followed by a fully connected bottleneck and GRU decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Convolutional encoder + GRU decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Convolutional neural network (1D) encoder + recurrent neural network decoder (GRU)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Encoder: three 1D conv layers (filter sizes and kernel counts specified per dataset), followed by FC to latent dim (196 or 156); decoder: three GRU layers (hidden dims 488/500)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Canonical SMILES from ZINC and QM9 (same as VAE experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular representation learning and generation for drug-like and small molecules</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encode SMILES via convolutional encoder to latent vectors, decode via GRU decoder (as part of VAE or AE) to produce SMILES; latent sampling and optimization used to generate novel molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Same as VAE/Seq2Seq descriptions: generated molecules match training distribution and many are novel; see validity and novelty stats in Tables 1, 3, 4.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Convolutional encoder helps capture chemical substrings (cycles, functional groups) improving encoding quality which aids downstream property prediction and latent-space optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reconstruction accuracy, decoding validity rates, property prediction MAE when encoder used as embedding (Table 2 compares 'Encoder' row).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Convolutional encoders improved encoding performance relative to simple RNN encoders, likely because of repeated substrings in SMILES corresponding to chemical substructures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Authors compared convolutional encoders to recurrent encoders and found improved performance; also compared embeddings to ECFP and graph-convolution embeddings in property prediction benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Still bound by SMILES fragility; encoder improvements do not eliminate invalid SMILES decoding; graph-based representations may be preferable in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8902.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8902.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP latent-space optimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian process surrogate model for optimization in continuous latent molecular space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Gaussian process (GP) is trained on latent vectors and known properties to act as a surrogate mapping from latent space to target property; GP is optimized to propose latent vectors expected to maximize an objective which are then decoded to molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gaussian Process (GP) surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Probabilistic nonparametric regression (Gaussian process)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not reported (standard GP trained on 1,000–2,000 diverse latent vectors in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Latent representations of molecules (2,000 molecules chosen to be maximally diverse for primary experiments; also experiments with 1,000 molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Optimization of drug-likeness and synthesizability (example objective: 5*QED - SAS) in molecule design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Train GP on (latent vector -> property) pairs, optimize GP surrogate in latent space (gradient-based) to find latent points with high predicted objective, decode these latent points to SMILES to obtain candidate molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Optimization moves latent representations away from starting low-scoring molecules into regions of higher objective, producing novel molecules along interpolation paths (examples shown in figures); quantitative novelty not tabulated beyond novelty statistics of VAE sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity achieved by selecting objective function combining desired attributes (e.g., QED and SAS) and optimizing GP-predicted objective in latent space; joint training of property predictor further organizes latent space to improve specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Percentile improvement in objective (5*QED - SAS) compared to baselines (random Gaussian search, genetic algorithm); visualization of optimization paths in PCA-compressed latent space; decoded molecule property evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GP-guided optimization in latent space consistently produced molecules with higher objective percentiles than random search and GA baselines; smaller GP training sets (1,000) led to more local minima and less global improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed random Gaussian search and a genetic algorithm baseline on the chosen composite objective; performance sensitive to size/diversity of GP training set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>GP predictive power limited by training set size/diversity causing optimization to fall into local minima; success depends on quality of latent encoding and surrogate accuracy; must decode and chemically validate candidates post-optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8902.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8902.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Genetic Algorithm (GA) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic algorithm / hand-designed mutation baseline for molecule optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traditional discrete optimization approach using hand-designed mutation and crossover rules to generate candidate molecules; used here as a baseline comparator against the VAE + GP optimization approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Genetic Algorithm (GA) with hand-designed mutation rules</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Evolutionary / discrete optimization algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Operates over chemical space / libraries (comparison samples reported in Table 1 drawn from ZINC or QM9-derived experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation and optimization (drug-like molecules and small molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Discrete local search via mutation and crossover of SMILES/graph representations according to hand-specified heuristics; used to produce candidate molecules for property comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>GA produced some novel molecules but was observed to bias toward higher chemical complexity and decreased drug-likeness compared to VAE sampling; percent overlap with ZINC reported in Table 1 (e.g., GA % in ZINC 6.5% in one experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced via mutation heuristics and selection based on objective, but requires manual design of mutation/crossover rules to avoid impractical chemistries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same property metrics as other methods (logP, SAS, QED, percent in ZINC, percent in eMolecules); used as baseline in optimization comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GA baseline generally yielded lower objective percentiles compared to GP optimization over VAE latent space and produced molecules biased toward higher complexity and lower drug-likeness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared unfavorably for the composite 5*QED - SAS objective versus GP+VAE optimization; GAs require manual heuristic design for mutation/crossover while continuous latent-space methods enable gradient-based guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires hand-specified mutation/crossover heuristics; discrete search lacks gradient information making exploration of large chemical spaces inefficient; can produce impractical chemistries without curated rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8902.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8902.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Property predictor (MLP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-layer perceptron jointly trained with autoencoder to predict molecular properties from latent vectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-layer fully connected neural network (1000 neurons per layer with dropout) trained jointly with the autoencoder to predict properties (e.g., logP, QED, SAS, HOMO/LUMO) from latent representations, used to arrange latent space by property and enable property-aware generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-layer Perceptron (MLP) property predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Feedforward neural network (fully connected MLP) with dropout</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Two fully connected layers of 1000 neurons each, dropout rate 0.2</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Latent vectors derived from SMILES encodings of molecules in ZINC and QM9 together with known property labels (logP, QED, SAS for ZINC; HOMO, LUMO, R^2 for QM9)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Property prediction for drug-like and small molecules; used to bias latent space for property-directed generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Joint training with autoencoder (reconstruction + property loss) so latent vectors reflect target properties; gradients / GP surrogate built on latent space enable optimization toward desired property values.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not directly a generative model, but its joint training reorganizes latent space enabling generation of molecules with desired properties; helps produce novel molecules tailored to objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Directly enforces property-awareness by including prediction loss in the autoencoder objective, which produces latent-space gradients and regions associated with high/low property values.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MAE for property prediction compared to baselines (Table 2); visualization of latent space gradients by property (PCA plots).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Joint training improves organization of latent space by property and aids optimization; property prediction MAE competitive with some baselines but not uniformly best (e.g., graph convolutions sometimes outperform VAE-based predictor on certain properties).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to ECFP, Coulomb matrix, and graph-convolution models in Table 2; VAE+MLP prediction comparable for some electronic properties but generally behind best graph-convolution results on some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Predictive accuracy limited by model architecture and available labelled data; joint training must balance reconstruction and prediction losses (annealing schedule used); property coverage depends on available labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Auto-encoding variational bayes <em>(Rating: 2)</em></li>
                <li>Generating Sentences from a Continuous Space <em>(Rating: 2)</em></li>
                <li>Convolutional Networks on Graphs for Learning Molecular Fingerprints <em>(Rating: 2)</em></li>
                <li>Grammar Variational Autoencoder <em>(Rating: 2)</em></li>
                <li>Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models <em>(Rating: 2)</em></li>
                <li>Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry (ORGANIC). <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8902",
    "paper_id": "paper-88427d2143d4cff357c3b393ae7580a7b6e19940",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "VAE (SMILES)",
            "name_full": "Variational Autoencoder applied to SMILES-based molecular generation",
            "brief_description": "A variational autoencoder maps SMILES strings to a continuous latent vector (156 dims for QM9, 196 dims for ZINC) and decodes latent points back to SMILES to generate novel molecules; trained jointly with a property predictor in some experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Variational Autoencoder (VAE)",
            "model_type": "Variational autoencoder (probabilistic latent-variable model)",
            "model_size": "latent dimensionality: 156 (QM9) and 196 (ZINC); encoder/decoder network sizes as reported (see training details)",
            "training_data": "SMILES strings from QM9 (~108k/134k small molecules) and ZINC (~250k drug-like molecules)",
            "application_domain": "Drug-like molecule design and small-molecule property optimization (QM9 electronic properties)",
            "generation_method": "Decode random latent vectors sampled from prior, perturb/ interpolate latent vectors of known molecules, or optimize latent vectors via a surrogate model (Gaussian process) then decode; decoder is stochastic (character softmax sampling)",
            "novelty_of_chemicals": "Generated molecules largely follow training-distribution statistics but are typically novel: e.g., for ZINC-trained VAE, ~4.5% of generated molecules were found in the source ZINC library and ~7.0% in eMolecules (Table 1); decoding validity for random latent points ~73.9% (ZINC) and ~79.3% (QM9) (Table 4); many decoded molecules are new combinatorial variants not present in training set.",
            "application_specificity": "Specificity enforced by joint training with a property predictor (MLP) on the latent vectors and by using a Gaussian process surrogate to optimize latent vectors for an objective (example objective: 5*QED - SAS); latent space becomes organized by property values allowing targeted sampling.",
            "evaluation_metrics": "Validity rate of decoded SMILES, percent overlap with ZINC/eMolecules, property distributions (logP, SAS, QED, NP), property prediction MAE (see Table 2), objective score (percentile of 5*QED - SAS), reconstruction fidelity.",
            "results_summary": "VAE yields a smooth continuous latent space of molecules; sampling/decoding yields realistic molecules with property distributions matching training data; joint property training organizes latent space by property; optimization in latent space via GP produces molecules with higher objective values than random search or a GA baseline; decoding validity is substantial but imperfect (e.g., 70–80% for random latent points for ZINC).",
            "comparison_to_other_methods": "Compared to hand-designed mutation rules and a genetic algorithm baseline, VAE-generated molecules follow training-set distributions more closely and generate novel compounds, whereas GA tended to bias toward higher complexity and decreased drug-likeness; property-prediction performance compared to ECFP and graph convolutions: VAE-based predictor achieves comparable but not uniformly superior MAEs (Table 2).",
            "limitations_and_challenges": "Decoder frequently produces invalid SMILES (validity ranged widely, 70% to &lt;1% in some settings); latent-space 'dead areas' that decode invalidly; decoder must learn SMILES syntax which is fragile; some generated molecules contain undesirable moieties (unstable or synthetically impractical); property-prediction accuracy limited by model and data; need for graph-based decoders or grammar-based approaches to reduce invalid outputs.",
            "uuid": "e8902.0",
            "source_info": {
                "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "Seq2Seq RNN (GRU) SMILES AE",
            "name_full": "Sequence-to-sequence recurrent neural network autoencoder with GRU decoder for SMILES",
            "brief_description": "A sequence-to-sequence autoencoder using recurrent neural networks (GRU layers in the decoder) to map SMILES strings to and from a continuous representation; trained as the decoder portion of the VAE/autoencoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Seq2Seq RNN Autoencoder (GRU decoder)",
            "model_type": "Recurrent neural network sequence-to-sequence (GRU) with teacher forcing",
            "model_size": "decoder GRU hidden dimension: 488 (ZINC model) or 500 (QM9 model); three GRU layers in decoder",
            "training_data": "Canonicalized SMILES strings from ZINC and QM9 (padded to fixed max length: ZINC max 120 chars, QM9 max 34 chars)",
            "application_domain": "Molecular generation of drug-like molecules and small molecules",
            "generation_method": "Decoder samples characters sequentially from a softmax at each position (stochastic decoding); used to reconstruct/ generate SMILES from latent vectors; teacher forcing used during training to increase generation accuracy.",
            "novelty_of_chemicals": "Supports stochastic decoding from latent points producing multiple variant SMILES (frequent decoding corresponds to nearest decoded molecule when re-encoded); novelty governed by sampled latent points and training data combinatorics.",
            "application_specificity": "Combined with joint property prediction and latent-space optimization, enables targeted generation of molecules with desired properties.",
            "evaluation_metrics": "SMILES reconstruction fidelity, percentage of valid SMILES decoded for training/test and random latent points (see Tables 3 and 4).",
            "results_summary": "GRU-based decoders produce realistic SMILES and allow stochastic sampling; teacher forcing improved valid-SMILES fraction but risked decoder ignoring latent encoding; decoding stochasticity leads to multiple close variants for a latent point.",
            "comparison_to_other_methods": "Convolutional encoders paired with RNN decoders gave improved performance for encoding (authors experimented with convolutional encoders), but RNN decoders are standard for sequence outputs and used here successfully.",
            "limitations_and_challenges": "Fragility of SMILES syntax makes decoder prone to invalid outputs; teacher forcing can cause decoder to ignore latent codes; sampling/stochastic decoding adds variability and requires postprocessing/chemical validation (RDKit).",
            "uuid": "e8902.1",
            "source_info": {
                "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "Conv encoder + RNN decoder",
            "name_full": "Convolutional encoder paired with recurrent (GRU) decoder for SMILES autoencoding",
            "brief_description": "A model variant using 1D convolutional layers as the encoder to exploit translationally-invariant substrings in SMILES (substructures), followed by a fully connected bottleneck and GRU decoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Convolutional encoder + GRU decoder",
            "model_type": "Convolutional neural network (1D) encoder + recurrent neural network decoder (GRU)",
            "model_size": "Encoder: three 1D conv layers (filter sizes and kernel counts specified per dataset), followed by FC to latent dim (196 or 156); decoder: three GRU layers (hidden dims 488/500)",
            "training_data": "Canonical SMILES from ZINC and QM9 (same as VAE experiments)",
            "application_domain": "Molecular representation learning and generation for drug-like and small molecules",
            "generation_method": "Encode SMILES via convolutional encoder to latent vectors, decode via GRU decoder (as part of VAE or AE) to produce SMILES; latent sampling and optimization used to generate novel molecules.",
            "novelty_of_chemicals": "Same as VAE/Seq2Seq descriptions: generated molecules match training distribution and many are novel; see validity and novelty stats in Tables 1, 3, 4.",
            "application_specificity": "Convolutional encoder helps capture chemical substrings (cycles, functional groups) improving encoding quality which aids downstream property prediction and latent-space optimization.",
            "evaluation_metrics": "Reconstruction accuracy, decoding validity rates, property prediction MAE when encoder used as embedding (Table 2 compares 'Encoder' row).",
            "results_summary": "Convolutional encoders improved encoding performance relative to simple RNN encoders, likely because of repeated substrings in SMILES corresponding to chemical substructures.",
            "comparison_to_other_methods": "Authors compared convolutional encoders to recurrent encoders and found improved performance; also compared embeddings to ECFP and graph-convolution embeddings in property prediction benchmarks.",
            "limitations_and_challenges": "Still bound by SMILES fragility; encoder improvements do not eliminate invalid SMILES decoding; graph-based representations may be preferable in future work.",
            "uuid": "e8902.2",
            "source_info": {
                "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "GP latent-space optimization",
            "name_full": "Gaussian process surrogate model for optimization in continuous latent molecular space",
            "brief_description": "A Gaussian process (GP) is trained on latent vectors and known properties to act as a surrogate mapping from latent space to target property; GP is optimized to propose latent vectors expected to maximize an objective which are then decoded to molecules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gaussian Process (GP) surrogate",
            "model_type": "Probabilistic nonparametric regression (Gaussian process)",
            "model_size": "Not reported (standard GP trained on 1,000–2,000 diverse latent vectors in experiments)",
            "training_data": "Latent representations of molecules (2,000 molecules chosen to be maximally diverse for primary experiments; also experiments with 1,000 molecules)",
            "application_domain": "Optimization of drug-likeness and synthesizability (example objective: 5*QED - SAS) in molecule design",
            "generation_method": "Train GP on (latent vector -&gt; property) pairs, optimize GP surrogate in latent space (gradient-based) to find latent points with high predicted objective, decode these latent points to SMILES to obtain candidate molecules.",
            "novelty_of_chemicals": "Optimization moves latent representations away from starting low-scoring molecules into regions of higher objective, producing novel molecules along interpolation paths (examples shown in figures); quantitative novelty not tabulated beyond novelty statistics of VAE sampling.",
            "application_specificity": "Specificity achieved by selecting objective function combining desired attributes (e.g., QED and SAS) and optimizing GP-predicted objective in latent space; joint training of property predictor further organizes latent space to improve specificity.",
            "evaluation_metrics": "Percentile improvement in objective (5*QED - SAS) compared to baselines (random Gaussian search, genetic algorithm); visualization of optimization paths in PCA-compressed latent space; decoded molecule property evaluations.",
            "results_summary": "GP-guided optimization in latent space consistently produced molecules with higher objective percentiles than random search and GA baselines; smaller GP training sets (1,000) led to more local minima and less global improvement.",
            "comparison_to_other_methods": "Outperformed random Gaussian search and a genetic algorithm baseline on the chosen composite objective; performance sensitive to size/diversity of GP training set.",
            "limitations_and_challenges": "GP predictive power limited by training set size/diversity causing optimization to fall into local minima; success depends on quality of latent encoding and surrogate accuracy; must decode and chemically validate candidates post-optimization.",
            "uuid": "e8902.3",
            "source_info": {
                "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "Genetic Algorithm (GA) baseline",
            "name_full": "Genetic algorithm / hand-designed mutation baseline for molecule optimization",
            "brief_description": "A traditional discrete optimization approach using hand-designed mutation and crossover rules to generate candidate molecules; used here as a baseline comparator against the VAE + GP optimization approach.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Genetic Algorithm (GA) with hand-designed mutation rules",
            "model_type": "Evolutionary / discrete optimization algorithm",
            "model_size": "Not applicable",
            "training_data": "Operates over chemical space / libraries (comparison samples reported in Table 1 drawn from ZINC or QM9-derived experiments)",
            "application_domain": "Molecule generation and optimization (drug-like molecules and small molecules)",
            "generation_method": "Discrete local search via mutation and crossover of SMILES/graph representations according to hand-specified heuristics; used to produce candidate molecules for property comparison.",
            "novelty_of_chemicals": "GA produced some novel molecules but was observed to bias toward higher chemical complexity and decreased drug-likeness compared to VAE sampling; percent overlap with ZINC reported in Table 1 (e.g., GA % in ZINC 6.5% in one experiment).",
            "application_specificity": "Specificity enforced via mutation heuristics and selection based on objective, but requires manual design of mutation/crossover rules to avoid impractical chemistries.",
            "evaluation_metrics": "Same property metrics as other methods (logP, SAS, QED, percent in ZINC, percent in eMolecules); used as baseline in optimization comparisons.",
            "results_summary": "GA baseline generally yielded lower objective percentiles compared to GP optimization over VAE latent space and produced molecules biased toward higher complexity and lower drug-likeness.",
            "comparison_to_other_methods": "Compared unfavorably for the composite 5*QED - SAS objective versus GP+VAE optimization; GAs require manual heuristic design for mutation/crossover while continuous latent-space methods enable gradient-based guidance.",
            "limitations_and_challenges": "Requires hand-specified mutation/crossover heuristics; discrete search lacks gradient information making exploration of large chemical spaces inefficient; can produce impractical chemistries without curated rules.",
            "uuid": "e8902.4",
            "source_info": {
                "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "Property predictor (MLP)",
            "name_full": "Multi-layer perceptron jointly trained with autoencoder to predict molecular properties from latent vectors",
            "brief_description": "A two-layer fully connected neural network (1000 neurons per layer with dropout) trained jointly with the autoencoder to predict properties (e.g., logP, QED, SAS, HOMO/LUMO) from latent representations, used to arrange latent space by property and enable property-aware generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multi-layer Perceptron (MLP) property predictor",
            "model_type": "Feedforward neural network (fully connected MLP) with dropout",
            "model_size": "Two fully connected layers of 1000 neurons each, dropout rate 0.2",
            "training_data": "Latent vectors derived from SMILES encodings of molecules in ZINC and QM9 together with known property labels (logP, QED, SAS for ZINC; HOMO, LUMO, R^2 for QM9)",
            "application_domain": "Property prediction for drug-like and small molecules; used to bias latent space for property-directed generation",
            "generation_method": "Joint training with autoencoder (reconstruction + property loss) so latent vectors reflect target properties; gradients / GP surrogate built on latent space enable optimization toward desired property values.",
            "novelty_of_chemicals": "Not directly a generative model, but its joint training reorganizes latent space enabling generation of molecules with desired properties; helps produce novel molecules tailored to objectives.",
            "application_specificity": "Directly enforces property-awareness by including prediction loss in the autoencoder objective, which produces latent-space gradients and regions associated with high/low property values.",
            "evaluation_metrics": "MAE for property prediction compared to baselines (Table 2); visualization of latent space gradients by property (PCA plots).",
            "results_summary": "Joint training improves organization of latent space by property and aids optimization; property prediction MAE competitive with some baselines but not uniformly best (e.g., graph convolutions sometimes outperform VAE-based predictor on certain properties).",
            "comparison_to_other_methods": "Compared to ECFP, Coulomb matrix, and graph-convolution models in Table 2; VAE+MLP prediction comparable for some electronic properties but generally behind best graph-convolution results on some metrics.",
            "limitations_and_challenges": "Predictive accuracy limited by model architecture and available labelled data; joint training must balance reconstruction and prediction losses (annealing schedule used); property coverage depends on available labels.",
            "uuid": "e8902.5",
            "source_info": {
                "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
                "publication_date_yy_mm": "2016-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Auto-encoding variational bayes",
            "rating": 2
        },
        {
            "paper_title": "Generating Sentences from a Continuous Space",
            "rating": 2
        },
        {
            "paper_title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints",
            "rating": 2
        },
        {
            "paper_title": "Grammar Variational Autoencoder",
            "rating": 2
        },
        {
            "paper_title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
            "rating": 2
        },
        {
            "paper_title": "Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry (ORGANIC).",
            "rating": 2
        }
    ],
    "cost": 0.01466125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automatic Chemical Design Using a Data-Driven</h1>
<h2>Continuous Representation of Molecules</h2>
<p>Rafael Gómez-Bombarelli, ${ }^{\dagger, #}$ Jennifer N. Wei, ${ }^{\ddagger, #}$ David Duvenaud, ${ }^{\text {, }}$, # José Miguel Hernández-Lobato, ${ }^{\S, #}$ Benjamín Sánchez-Lengeling, ${ }^{\ddagger}$ Dennis Sheberla, ${ }^{\ddagger}$ Jorge Aguilera-Iparraguirre, ${ }^{\dagger}$ Timothy D. Hirzel, ${ }^{\dagger}$ Ryan P. Adams, ${ }^{\S}$ and Alán Aspuru-Guzik<em _35_Equal="#Equal" contribution="contribution">, ${ }^{\ddagger}, \perp$<br>$\dagger$ Kyulux North America Inc.<br>$\ddagger$ Department of Chemistry and Chemical Biology, Harvard University, Cambridge MA 02138, USA<br>IDepartment of Computer Science, University of Toronto<br>§Department of Engineering, University of Cambridge Trumpington Street, Cambridge CB2 1PZ, UK<br>$|$ Google Brain and Princeton University<br>$\perp$ Canadian Institute for Advanced Research (CIFAR), Biologically-Inspired Solar Energy Program.<br>\section</em><br>E-mail: aspuru@chemistry.harvard.edu</p>
<h4>Abstract</h4>
<p>We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds.</p>
<p>A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule.</p>
<p>Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules.</p>
<p>Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in the set of molecules with fewer that nine heavy atoms.</p>
<h1>Introduction</h1>
<p>The goal of drug and material design is to identify novel molecules that have certain desirable properties. We view this as an optimization problem, in which we are searching for the molecules that maximize our quantitative desiderata. However, optimization in molecular space is extremely challenging, because the search space is large, discrete, and unstructured. Making and testing new compounds is costly and time consuming, and the number of potential candidates is overwhelming. Only about $10^{8}$ substances have ever been synthesized, ${ }^{1}$ whereas the range of potential drug-like molecules is estimated to be between $10^{23}$ and $10^{60} .^{2}$</p>
<p>Virtual screening can be used to speed up this search. ${ }^{3-6}$ Virtual libraries containing thousands to hundreds of millions of candidates can be assayed with first-principles simulations or statistical predictions based on learned proxy models, and only the most promising leads are selected and tested experimentally.</p>
<p>However, even when accurate simulations are available, ${ }^{7}$ computational molecular design</p>
<p>is limited by the search strategy used to explore chemical space. Current methods either exhaustively search through a fixed library, ${ }^{8,9}$ or use discrete local search methods such as genetic algorithms ${ }^{10-15}$ or similar discrete interpolation techniques. ${ }^{16-18}$ Although these techniques have led to useful new molecules, these approaches still face large challenges. Fixed libraries are monolithic, costly to fully explore, and require hand-crafted rules to avoid impractical chemistries. The genetic generation of compounds requires the manual specification of heuristics for mutation and crossover rules. Discrete optimization methods have difficulty effectively searching large areas of chemical space because since it is not possible guide the search with gradients.</p>
<p>A molecular representation method that is continuous, data-driven, and can easily be converted into a machine-readable molecule has several advantages. First, hand-specified mutation rules are unnecessary, as new compounds can be generated automatically by modifying the vector representation and then decoding. Second, if we develop a differentiable model that maps from molecular representations to desirable properties, we can enable the use of gradient-based optimization to make larger jumps in chemical space. Gradient-based optimization can be combined with Bayesian optimization methods to select compounds that are likely to be informative about the global optimum. Third, a data-driven representation can leverage large sets of unlabeled chemical compounds to automatically build an even larger implicit library, and then use the smaller set of labeled examples to build a regression model from the continuous representation to the desired properties. This lets us take advantage of large chemical databases containing millions of molecules, even when many properties are unknown for most compounds.</p>
<p>Recent advances in machine learning have resulted in powerful probabilistic generative models that, after being trained on real examples, are able to produce realistic synthetic samples. Such models usually also produce low-dimensional continuous representations of the data being modeled, allowing interpolation or analogical reasoning for natural images, ${ }^{19}$ text, ${ }^{20}$ speech, and music. ${ }^{21}$ We apply such generative models to chemical design, using a</p>
<p>pair of deep networks trained as an autoencoder to convert molecules represented as SMILES strings into a continuous vector representation. In principle, this method of converting from a molecular representation to a continuous vector representation could be applied to any molecular representation, including chemical fingerprints, ${ }^{22}$ convolutional neural networks on graphs, ${ }^{23}$ similar graph-convolutions, ${ }^{24}$ and Coulomb matrices. ${ }^{25}$ We chose to use SMILES representation because it can be readily converted into a molecule.</p>
<p>Using this new continuous vector-valued representation, we experiment with the use of continuous optimization to produce novel compounds. We trained the autoencoder jointly on a property prediction task; we added a multilayer perceptron that predicts property values from the continuous representation generated by the encoder and included the regression error in our loss function. We examined the effects this joint training had on the latent space.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a). A diagram of the proposed autoencoder for molecular design, including the joint property prediction model. Starting from a discrete molecular representation, such as a SMILES string, the encoder network converts each molecule into a vector in the latent space, which is effectively a continuous molecular representation. Given a point in the latent space, the decoder network produces a corresponding SMILES string. Another network estimates the value of target properties associated with each molecule. (b) Gradient-based optimization in continuous latent space. After training a surrogate model $f(z)$ to predict the properties of molecules based on their latent representation $z$, we can optimize $f(z)$ with respect to $z$ to find new latent representations expected to have high values of desired properties. These new latent representations can then be decoded into SMILES strings, at which point their properties can be tested empirically.</p>
<h1>Representation and Autoencoder Framework</h1>
<h2>Initial representation of molecules</h2>
<p>Before building an encoder that produces a continuous latent representation, we must choose which discrete molecular representation to use for the input and output. To leverage the power of recent advances in sequence-to-sequence autoencoders for modeling text, ${ }^{20}$ we used the SMILES ${ }^{26}$ representation, a commonly-used text encoding for organic molecules. We also tested $\mathrm{InChI}^{27}$ as an alternative string representation, but found it to perform substantially worse than SMILES, presumably due to a more complex syntax that includes counting and arithmetic.</p>
<h2>Training an autoencoder</h2>
<p>Starting from a large library of string-based representations of molecules, we trained a pair of deep neural networks: an encoder network to convert each string into a fixed-dimensional vector, and a decoder network to convert vectors back into strings (Figure 1a). Such encoderdecoder pairs are known as autoencoders. The autoencoder is trained to minimize error in reproducing the original string, i.e., it attempts to learn the identity function. Key to the design of the autoencoder is mapping through an information bottleneck. This bottleneck - here the fixed-length continuous vector - induces the network to learn a compressed representation that captures the most statistically salient information in the data. We call the vector-encoded molecule the latent representation of the molecule.</p>
<p>The character-by-character nature of the SMILES representation and the fragility of its internal syntax (opening and closing cycles and branches, allowed valences, etc.) can result in the output of invalid molecules from the decoder. Multiple factors contribute to the proportion of valid SMILES output from the decoder, including atom count and training set density. In generating new SMILES strings, the percentage of valid SMILES output ranged from $70 \%$ to less than $1 \%$. We employed the open source cheminformatics suite RDKit ${ }^{28}$</p>
<p>and Marvin to validate the chemical structures of output molecules and discard invalid ones. While it would be more efficient to have the autoencoder generate only valid strings, this post-processing step is lightweight and allows for greater flexibility in the autoencoder to learn the architecture of the SMILES.</p>
<p>To enable molecular design, the chemical structures encoded in the continuous representation of the autoencoder need to be correlated with the target properties that we are seeking to optimize. Therefore, we added a model to the autoencoder that predicts the properties from the latent space representation. This autoencoder was then trained jointly on the reconstruction task and a property prediction task; an additional multi-layer perceptron (MLP) was used to predict the property from the latent vector of the encoded molecule. To propose promising new candidate molecules, we can start from the latent vector of an encoded molecule and then move in the direction most likely to improve the desired attribute. The resulting new candidate vectors can then be decoded into corresponding molecules. (Figure 1b)</p>
<h1>Using variational autoencoders to produce a latent representation.</h1>
<p>For unconstrained optimization in the latent space to work, points in the latent space must decode into valid SMILES strings that capture the chemical nature of the training data. However, the original training objective of the autoencoder does not enforce this constraint, as we chose to handle invalid SMILES in a post processing step. As a result, the latent space learned by the autoencoder may contain large "dead areas", which decode to invalid SMILES strings.</p>
<p>To ensure that points in the latent space correspond to valid realistic molecules, we modified our autoencoder and its objective into a variational autoencoder (VAE). ${ }^{29}$ VAEs were developed as a principled approximate-inference method for latent-variable models, in which each datum has a corresponding, but unknown, latent representation. VAEs generalize autoencoders, adding stochasticity to the encoder which combined with a penalty term</p>
<p>encourages all areas of the latent space to correspond to a valid decoding. The intuition is that adding noise to the encoded molecules forces the decoder to learn how to decode a wider variety of latent points and find more robust representations. In addition, since two different molecules can have their encodings stochastically brought close in the latent space, but still need to decode to different molecular graphs, this constraint encourages the encodings to spread out over the entire latent space to avoid overlap. Variational autoencoders with recurrent neural network encoding/decoding were proposed by Bowman et al. in the context of written English sentences and we followed their approach closely. ${ }^{20}$</p>
<p>Two autoencoder system were trained; one with 108,000 molecules from the QM9 dataset of molecules with fewer than 9 heavy atoms ${ }^{30}$ and another with 250,000 drug-like commercially available molecules extracted at random from the ZINC database. ${ }^{31}$</p>
<p>We performed random optimization over hyperparameters specifying the deep autoencoder architecture and training, such as the choice between a recurrent or convolutional encoder, the number of hidden layers, layer sizes, regularization and learning rates. The latent space representations for the QM9 and ZINC datasets had 156 dimensions and 196 dimensions respectively.</p>
<h1>Results and discussion</h1>
<p>Representation of molecules in latent space Firstly, we analyze the fidelity of the autoencoder and the ability of the latent space to capture structural molecular features. Figure 2a) shows a kernel density estimate of each dimension when encoding a set of 5000 randomly selected ZINC molecules from outside the training set. Whereas each individual dimension shows a slightly different mean and standard deviation, all follow normal distribution as enforced by the variational regularizer.</p>
<p>The variational autoencoder is a doubly-probabilistic model. In addition to the added noise to the encoder, which can be turned off by simply sampling the mean of the encoding</p>
<p>distribution, the decoder also samples a string from of the probability distribution over characters in each position generated by the final layer. This implies that decoding a single point in the latent space back to a string representation is stochastic. Figure 2b) shows the probability of decoding the latent representation of a sample FDA-approved drug molecule into several different molecules. For most latent points, a prominent molecule is decoded and many other slight variations appear with lower frequencies. When these resulting SMILES are re-encoded into the latent space, the most frequent decoding also tends to be the one with the lowest Euclidean distance to the original point, indicating the latent space is indeed capturing features relevant to molecules.</p>
<p>Figure 2c) shows some molecules in the latent space that are close to ibuprofen. These structures become less similar with increasing distance in the latent space. When the distance approaches the average distance of molecules in the training set, the changes are more pronounced, eventually resembling random molecules likely to be sampled from the training set. Figure 5d) shows the distribution of distances in latent space between 50,000 random points from our ZINC training set.</p>
<p>A continuous latent space allows interpolation of molecules by following the shortest Euclidean path between their latent representations. When exploring high dimensional spaces, it is important to note that Euclidean distance might not map directly to notions of similarity of molecules. ${ }^{32}$ In high dimensional spaces, most of the mass of independent normally distributed random variables is not near the mean, but in an increasingly distant annulus around it. ${ }^{33}$ Interpolating linearly between two points might pass by an area of low probability, to keep the sampling on the areas of high probability we utilize spherical interpolation ${ }^{34}$ (slerp). With slerp, the path between two points is a circular arc lying on the on the surface of a N-dimensional sphere. Figure 2d) shows the spherical interpolation between two random drug molecules, showing smooth transitions in between. Figure 7 shows the difference between linear and spherical interpolation.</p>
<p>Table 1 compares the distribution of chemical properties in the training sets with a)</p>
<p>Table 1: Comparison of molecule generation results using genetic algorithm (GA) and variational autoencoder (VAE) without joint property prediction. a) Describes the source of the molecules, Data refers to the entire dataset; b) The dataset used, either ZINC or QM9, c) Number of samples used fro comparison; d) water-octanal partition coefficient (logP); ${ }^{35}$ e) synthetic accessibility score (SAS); ${ }^{36}$ f) Qualitative Estimate of Drug-likeness (QED); ${ }^{37}$ g) percentage of generated molecules found in ZINC; h) percentage of generated molecules founds in E-molecules (emol) database ${ }^{38}$</p>
<table>
<thead>
<tr>
<th>Source $^{a}$</th>
<th>Dataset ${ }^{b}$ Samples $^{c}$</th>
<th></th>
<th>$\log \mathrm{P}^{d}$</th>
<th>SAS $^{e}$</th>
<th>QED $^{f}$</th>
<th>\% in ZINC $^{g}$</th>
<th>$\begin{aligned} &amp; \text { \% in } \ &amp; \text { emol }^{h} \end{aligned}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data</td>
<td>ZINC</td>
<td>249 k</td>
<td>2.46</td>
<td>3.05 (0.83)</td>
<td>0.73</td>
<td>100</td>
<td>12.9</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$(1.43)$</td>
<td></td>
<td>$(0.14)$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GA</td>
<td>ZINC</td>
<td>5303</td>
<td>2.84</td>
<td>3.80 (1.01)</td>
<td>$-0.82$</td>
<td>6.5</td>
<td>4.8</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$(1.86)$</td>
<td></td>
<td>$(0.71)$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VAE</td>
<td>ZINC</td>
<td>8728</td>
<td>2.67</td>
<td>3.18 (0.86)</td>
<td>$-0.96$</td>
<td>4.5</td>
<td>7.0</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$(1.46)$</td>
<td></td>
<td>$(0.75)$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Data</td>
<td>QM9</td>
<td>134 k</td>
<td>0.31</td>
<td>4.24 (0.91)</td>
<td>0.99</td>
<td>0.0</td>
<td>8.6</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$(1.00)$</td>
<td></td>
<td>$(1.20)$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GA</td>
<td>QM9</td>
<td>5470</td>
<td>0.96</td>
<td>4.47 (1.01)</td>
<td>0.68</td>
<td>0.018</td>
<td>3.8</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$(1.53)$</td>
<td></td>
<td>$(0.97)$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VAE</td>
<td>QM9</td>
<td>2839</td>
<td>0.30</td>
<td>4.34 (0.98)</td>
<td>0.47</td>
<td>0.0</td>
<td>8.9</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$(0.97)$</td>
<td></td>
<td>$(0.08)$</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>random molecules generated with a list of hand-designed rules ${ }^{10-15}$ and b) with molecules decoded from sampling random points in the latent space of an VAE trained only for the reconstruction task. We compare the water-octanol partition coefficient (logP), the synthetic accessibility score (SAS), the natural-product score (NP) and drug-likeness (QED). Despite the fact that the VAE is trained purely on the SMILES strings independently of chemical properties, it is able to generate realistic-looking molecules whose features follow the intrinsic distribution of the training data. The two rightmost columns in Table 1 report the fraction of molecules that belong to the the 17 million drug-like compounds from which the training set was selected and how often they can be found in a library of existing organic compounds. In the case of drug-like molecules, the VAE generates molecules that follow the property distribution of the training data, but are new, since the combinatorial space is extremely large the training set is an arbitrary sub-sample. The hand-selected mutations are less able to generate new compounds while at the same time biasing the properties of the set to higher chemical complexity and decreased drug-likeness. In the case of the QM9 dataset, since the</p>
<p>combinatorial space is smaller, the training set has more coverage and the VAE generates essentially the same population statistics as the training data.</p>
<p>Property prediction of molecules The interest in discovering new molecules and chemicals is most often in relation to maximizing some desirable property. For this reason, we extended the the purely generative model to also predict property values from the latent representation. We trained a multi-layer perceptron jointly with the autoencoder to predict properties from the latent representation of each molecule.</p>
<p>Table 2: MAE prediction error for properties using various methods on the ZINC and QM9 datasets. a) Baseline, mean prediction; b) As implemented in Deepchem benchmark (MoleculeNet), ${ }^{39}$ ECFPcircular fingerprints, CM-coulomb matrix, GC-graph convolutions; c) 1-hot-encoding of SMILES used as input to property predictor; d) The network trained without decoder loss; e) full variational autoencoder network trained for individual properties.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Database/Property</th>
<th style="text-align: center;">Mean $^{a}$</th>
<th style="text-align: center;">ECFP $^{b}$</th>
<th style="text-align: center;">$\mathrm{CM}^{b}$</th>
<th style="text-align: center;">GC $^{b}$</th>
<th style="text-align: center;">1-hot SMILES $^{c}$</th>
<th style="text-align: center;">Encoder $^{d}$</th>
<th style="text-align: center;">VAE $^{e}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ZINC250k/logP</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: center;">ZINC250k/QED</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">0.054</td>
</tr>
<tr>
<td style="text-align: center;">QM9/HOMO, eV</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: center;">QM9/LUMO, eV</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: center;">QM9/Gap, eV</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.21</td>
</tr>
</tbody>
</table>
<p>With joint training for property prediction, the distribution of molecules in the latent space is organized by property values. Figure 3 shows the mapping of true property values to the latent space representation of molecules, compressed into two dimensions using PCA. The latent space generated by autoencoders jointly trained with the property prediction task shows in the distribution of molecules a gradient by property values; molecules with high values are located in one region, and molecules with low values in another. Autoencoders that were trained without the property prediction task do not show a discernible pattern with respect to property values in the resulting latent representation distribution.</p>
<p>While the primary purpose of adding property prediction was to organize the latent space, it is interesting to observe how the property predictor model compares with other standard models for property prediction. Table 2 compares the performance of commonly used molecular embeddings and models to the VAE. Our VAE model shows that property prediction</p>
<p>performance for electronic properties (i.e., orbital energies) are similar to graph convolutions for some properties; prediction accuracy could be improved with further hyperparameter optimization.</p>
<p>Optimization of molecules via properties We next optimized molecules in the latent space from the autoencoder which was jointly trained for property prediction. We used a Gaussian process model ${ }^{40}$ to predict target properties for molecules given the latent space representation of the molecules as an input. The 2,000 molecules used for training the Gaussian process were selected to be maximally diverse. Using this model, we optimized in the latent space to find a molecule that maximized our objective. As a baseline, we compared our optimization results against molecules found using a random Gaussian search and molecules optimized via a genetic algorithm.</p>
<p>The objective we chose to optimize was $5 \times$ QED - SAS, where QED is the Quantitative Estimation of Drug-likeness (QED), ${ }^{37}$ and SAS is the Synthetic Accessibility score. ${ }^{36}$ This objective represents a rough estimate of finding the most drug-like molecule that is also easy to synthesize. To provide the greatest challenge for our optimizer, we started with molecules from the ZINC dataset that were in the bottom $10 \%$ percentile of our objective.</p>
<p>From Figure 4a) we can see that the optimization with the Gaussian process model on the latent space representation consistently results in molecules with a higher percentile score than the two baseline search methods. Figure 4b) shows the path of one optimization from the starting molecule to the final molecule in the two-dimensional PCA representation, the final molecule ending up in the region of high objective value. Figure 4c) shows molecules decoded along this optimization path using a Gaussian interpolation.</p>
<p>Performing this optimization on a Gaussian process (GP) model trained with 1,000 molecules leads to a slightly wider range of molecules as shown in Figure 4a). Since the training set is smaller, the predictive power of the GP is lower which when optimizing in latent space, and as a result optimizes to several local minima instead of a global optimization.</p>
<p>In cases where it is difficult to define an objective that completely describes the desirable traits of the molecule, it may be better to use this localized optimization approach to reach a larger diversity of potential molecules.</p>
<h1>Conclusion</h1>
<p>We propose a new family of methods for exploring chemical space based on continuous encodings of molecules. These methods eliminate the need to hand-build libraries of compounds and allow a new type of directed gradient-based search through chemical space. We observed high fidelity in reconstruction, the ability to capture characteristic features of a molecular training set into the generative model, good predictive power when training jointly an autoencoder and a predictor, and the ability to perform model-based optimization of molecules in the smoothed latent space.</p>
<p>There are several avenues for further improvement of this approach to molecular design. In this work, we used a text-based molecular encoding, but using a graph-based autoencoder would have several advantages. Forcing the decoder to produce valid SMILES strings makes the learning problem unnecessarily hard since the decoder must also implicitly learn which strings are valid SMILES. An autoencoder that directly outputs molecular graphs is appealing since it could explicitly address issues of graph isomorphism and the problem of strings that do not correspond to valid molecular graphs. Building an encoder which takes in molecular graphs is straightforward through the use of off-the-shelf molecular fingerprinting methods, such as ECFP ${ }^{22}$ or a continuously-parameterized variant of ECFP such as neural molecular fingerprints. ${ }^{23}$ However, building a neural network which can output arbitrary graphs is an open problem. Further extensions of this work to use a explicitly defined grammar for SMILES instead of forcing the model to learn one ${ }^{41}$ or to actively learn valid sequences ${ }^{42}$ are underway, as also is the application of adversarial networks for this task. ${ }^{43,44}$</p>
<p>The autoencoder sometimes produced molecules that are formally valid as graphs but</p>
<p>contain moieties that are not desirable because of stability or synthetic constraints. Examples are acid chlorides, anhydrides, cyclopentadienes, aziridines, enamines, hemiaminals, enol ethers, cyclobutadiene, and cycloheptatriene. One option is to train the autoencoder with to predict properties related to steric constraints of other structural constraints. In general, the objective function to be optimized needs to capture as many desirable traits as possible and balance them to ensure that the optimizer focuses on genuinely desirable compounds.</p>
<p>The results reported in this work, and its application with carefully composed objective functions, have the potential to create new avenues for molecular design.</p>
<h1>Methods</h1>
<p>Autoencoder architecture Strings of characters can be encoded into vectors using recurrent neural networks (RNNs). An encoder RNN can be paired with a decoder RNN to perform sequence-to-sequence learning. ${ }^{45}$ We also experimented with convolutional networks for string encoding ${ }^{46}$ and observed improved performance. This is explained by the presence of repetitive, translationally-invariant substrings that correspond to chemical substructures, e.g., cycles and functional groups.</p>
<p>Our SMILES-based text encoding used a subset of 35 different characters for ZINC and 22 different characters for QM9. For ease of computation, we encoded strings up to a maximum length of 120 characters for ZINC and 34 characters for QM9, although in principle there is no hard limit to string length. Shorter strings were padded with spaces to this same length. We used only canonicalized SMILES for training to avoid dealing with equivalent SMILES representations. The structure of the VAE deep network was as follows: For the autoencoder used for the ZINC dataset, the encoder used three 1D convolutional layers of filter sizes 9, 9,10 and $9,9,11$ convolution kernels, respectively, followed by one fully-connected layer of width 196. The decoder fed into three layers of gated recurrent unit (GRU) networks ${ }^{47}$ with hidden dimension of 488 . For the model used for the QM9 dataset, the encoder used three</p>
<p>1D convolutional layers of filter sizes $2,2,1$ and $5,5,4$ convolution kernels, respectively, followed by one fully-connected layer of width 156. The three recurrent neural network layers each had a hidden dimension of 500 neurons.</p>
<p>The last layer of the RNN decoder defines a probability distribution over all possible characters at each position in the SMILES string. This means that the writeout operation is stochastic, and the same point in latent space may decode into to different SMILES strings, depending on the random seed used to sample characters. The output GRU layer had one additional input, corresponding to the character sampled from the softmax output of the previous time step and was trained using teacher forcing. ${ }^{48}$ This increased the accuracy of generated SMILES strings, which resulted in higher fractions of valid SMILES strings for latent points outside the training data, but also made training more difficult, since the decoder showed a tendency to ignore the (variational) encoding and rely solely on the input sequence. The variational loss was annealed according to sigmoid schedule after 29 epochs, running for a total 120 epochs.</p>
<p>For property prediction, two fully connected layers of 1000 neurons were used to predict properties from the latent representation, with a dropout rate of 0.2 . For the algorithm trained on the ZINC dataset, the objective properties include logP, QED, SAS. For the algorithm trained on the QM9 dataset, the objective properties include HOMO energies, LUMO energies, and the electronic spatial extent $\left(\mathrm{R}^{2}\right)$. The property prediction loss was annealed in at the same time as the variational loss. We used the Keras ${ }^{49}$ and TensorFlow ${ }^{50}$ packages to build and train this model and the rdkit package for cheminformatics. ${ }^{28}$</p>
<h1>Acknowledgement</h1>
<p>This work was supported financially by the Samsung Advanced Institute of Technology. The authors acknowledge the use of the Harvard FAS Odyssey Cluster and support from FAS Research Computing. JNW acknowledges support from the National Science Foundation</p>
<p>Graduate Research Fellowship Program under Grant No. DGE-1144152. JMHL acknowledges support from the Rafael del Pino Foundation. RPA acknowledges support from the Alfred P. Sloan Foundation and NSF IIS-1421780. AAG acknowledges support from The Department of Energy, Office of Basic Energy Sciences under award de-sc0015959. We thank Dr. Anders Frøseth for his generous support of this work.</p>
<h1>References</h1>
<p>(1) Kim, S.; Thiessen, P. A.; Bolton, E. E.; Chen, J.; Fu, G.; Gindulyte, A.; Han, L.; He, J.; He, S.; Shoemaker, B. A.; Wang, J.; Yu, B.; Zhang, J.; Bryant, S. H. PubChem Substance and Compound databases. Nucleic Acids Res. 2016, 44, D1202-D1213.
(2) Polishchuk, P. G.; Madzhidov, T. I.; Varnek, A. Estimation of the size of drug-like chemical space based on GDB-17 data. J. Comput.-Aided Mol. Des. 2013, 27, 675-679.
(3) Shoichet, B. K. Virtual screening of chemical libraries. Nature 2004, 432, 862-5.
(4) Scior, T.; Bender, A.; Tresadern, G.; Medina-Franco, J. L.; Martinez-Mayorga, K.; Langer, T.; Cuanalo-Contreras, K.; Agrafiotis, D. K. Recognizing Pitfalls in Virtual Screening: A Critical Review. J. Chem. Inf. Model. 2012, 52, 867-881.
(5) Cheng, T.; Li, Q.; Zhou, Z.; Wang, Y.; Bryant, S. H. Structure-Based Virtual Screening for Drug Discovery: a Problem-Centric Review. AAPS J. 2012, 14, 133-141.
(6) Pyzer-Knapp, E. O.; Suh, C.; Gómez-Bombarelli, R.; Aguilera-Iparraguirre, J.; AspuruGuzik, A. What Is High-Throughput Virtual Screening? A Perspective from Organic Materials Discovery. Annu. Rev. Mater. Res. 2015, 45, 195-216.
(7) Schneider, G. Virtual screening: an endless staircase? Nat. Rev. Drug Discov. 2010, 9, $273-276$.</p>
<p>(8) Hachmann, J.; Olivares-Amaya, R.; Atahan-Evrenk, S.; Amador-Bedolla, C.; SánchezCarrera, R. S.; Gold-Parker, A.; Vogt, L.; Brockway, A. M.; Aspuru-Guzik, A. The Harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid. J. Phys. Chem. Lett. 2011, 2, 2241-2251.
(9) Gómez-Bombarelli, R. et al. Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach. 2016, 15, 1120-1127.
(10) Virshup, A. M.; Contreras-García, J.; Wipf, P.; Yang, W.; Beratan, D. N. Stochastic Voyages into Uncharted Chemical Space Produce a Representative Library of All Possible Drug-Like Compounds. J. Am. Chem. Soc. 2013, 135, 7296-7303.
(11) Rupakheti, C.; Virshup, A.; Yang, W.; Beratan, D. N. Strategy To Discover Diverse Optimal Molecules in the Small Molecule Universe. J. Chem. Inf. Model. 2015, 55, $529-537$.
(12) Reymond, J.-L. The Chemical Space Project. Acc. Chem. Res. 2015, 48, 722-730.
(13) Reymond, J.-L.; van Deursen, R.; Blum, L. C.; Ruddigkeit, L. Chemical space as a source for new drugs. Med. Chem. Commun. 2010, 1, 30.
(14) Kanal, I. Y.; Owens, S. G.; Bechtel, J. S.; Hutchison, G. R. Efficient Computational Screening of Organic Polymer Photovoltaics. J. Phys. Chem. Lett. 2013, 4, 1613-1623.
(15) O'Boyle, N. M.; Campbell, C. M.; Hutchison, G. R. Computational Design and Selection of Optimal Organic Photovoltaic Materials. J. Phys. Chem. C 2011, 115, 16200-16210.
(16) van Deursen, R.; Reymond, J.-L. Chemical Space Travel. ChemMedChem 2007, 2, $636-640$.
(17) Wang, M.; Hu, X.; Beratan, D. N.; Yang, W. Designing molecules by optimizing potentials. J. Am. Chem. Soc. 2006, 128, 3228-3232.</p>
<p>(18) Balamurugan, D.; Yang, W.; Beratan, D. N. Exploring chemical space with discrete, gradient, and hybrid optimization methods. J. Chem. Phys. 2008, 129, 174105.
(19) Radford, A.; Metz, L.; Chintala, S. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. 2015,
(20) Bowman, S. R.; Vilnis, L.; Vinyals, O.; Dai, A. M.; Jozefowicz, R.; Bengio, S. Generating Sentences from a Continuous Space. 2015,
(21) van den Oord, A.; Dieleman, S.; Zen, H.; Simonyan, K.; Vinyals, O.; Graves, A.; Kalchbrenner, N.; Senior, A.; Kavukcuoglu, K. WaveNet: A Generative Model for Raw Audio. 2016,
(22) Rogers, D.; Hahn, M. Extended-Connectivity Fingerprints. J. Chem. Inf. Model. 2010, $50,742-754$.
(23) Duvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Gómez-Bombarell, R.; Hirzel, T.; Aspuru-Guzik, A.; Adams, R. P. Convolutional Networks on Graphs for Learning Molecular Fingerprints. Advances in Neural Information Processing Systems. 2015; pp $2215-2223$.
(24) Kearnes, S.; McCloskey, K.; Berndl, M.; Pande, V.; Riley, P. Molecular Graph Convolutions: Moving Beyond Fingerprints. 2016,
(25) Rupp, M.; Tkatchenko, A.; Müller, K.-R.; von Lilienfeld, O. A. Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning. Phys. Rev. Lett. 2012, 108 .
(26) Weininger, D. SMILES a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Model. 1988, 28, 31-36.
(27) Heller, S.; McNaught, A.; Stein, S.; Tchekhovskoi, D.; Pletnev, I. InChI - the worldwide chemical structure identifier standard. J. Cheminf. 2013, 5, 7.</p>
<p>(28) RDKit: Open-source cheminformatics. http://www.rdkit.org, [Online; accessed 11-April-2017].
(29) Kingma, D. P.; Welling, M. Auto-encoding variational bayes. 2013,
(30) Ramakrishnan, R.; Dral, P. O.; Rupp, M.; Von Lilienfeld, O. A. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data 2014, 1, 140022.
(31) Irwin, J. J.; Sterling, T.; Mysinger, M. M.; Bolstad, E. S.; Coleman, R. G. ZINC: A Free Tool to Discover Chemistry for Biology. J. Chem. Inf. Model. 2012, 52, 1757-1768.
(32) Aggarwal, C. C.; Hinneburg, A.; Keim, D. A. Database Theory - ICDT 2001: 8th International Conference London, UK, January 4-6, 2001 Proceedings; Springer, Berlin, Heidelberg, 2001; pp 420-434.
(33) Domingos, P.; Pedro, A few useful things to know about machine learning. Communications of the ACM 2012, 55, 78 .
(34) White, T. Sampling Generative Networks. 2016,
(35) Wildman, S. A.; Crippen, G. M. Prediction of Physicochemical Parameters by Atomic Contributions. J. Chem. Inf. Comput. Sci. 1999, 39, 868-873.
(36) Ertl, P.; Schuffenhauer, A. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. J. Cheminf. 2009, $1,1-11$.
(37) Bickerton, G. R.; Paolini, G. V.; Besnard, J.; Muresan, S.; Hopkins, A. L. Quantifying the chemical beauty of drugs. Nat. Chem. 2012, 4, 90-98.
(38) E-molecules. https://www.emolecules.com/info/plus/download-database, [Online; accessed 22-July-2017].</p>
<p>(39) Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse, C.; Pappu, A. S.; Leswing, K.; Pande, V. MoleculeNet: A Benchmark for Molecular Machine Learning. arXiv:1703.00564 2017,
(40) Rasmussen, C. E.; Williams, C. K. Gaussian processes for machine learning; MIT press Cambridge, 2006; Vol. 1.
(41) Kusner, M. J.; Paige, B.; Hernández-Lobato, J. M. Grammar Variational Autoencoder. arXiv:1703.01925 2017,
(42) Janz, D.; van der Westhuizen, J.; Hernández-Lobato, J. M. Actively Learning what makes a Discrete Sequence Valid. 2017,
(43) Guimaraes, G. L.; Sanchez-Lengeling, B.; Farias, P. L. C.; Aspuru-Guzik, A. ObjectiveReinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models. arXiv:1705.10843 2017,
(44) Sanchez-Lengeling, B.; Outeiral, C.; Guimaraes, G. L.; Aspuru-Guzik, A. Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry (ORGANIC). 2017,
(45) Sutskever, I.; Vinyals, O.; Le, Q. V. Sequence to sequence learning with neural networks. Advances in neural information processing systems. 2014; pp 3104-3112.
(46) Kalchbrenner, N.; Grefenstette, E.; Blunsom, P. A Convolutional Neural Network for Modelling Sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics 2014,
(47) Chung, J.; Gülçehre, Ç.; Cho, K.; Bengio, Y. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. CoRR 2014, abs/1412.3555.
(48) Williams, R. J.; Zipser, D. A Learning Algorithm for Continually Running Fully Recurrent Neural Networks. Neural Comput. 1989, 1, 270-280.</p>
<p>(49) Chollet, F. keras. https://github.com/fchollet/keras, 2015.
(50) Abadi, M. et al. TensorFlow: A system for large-scale machine learning. 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). 2016; pp $265-283$.</p>
<h1>Supplementary Materials</h1>
<p>The code and full training data sets will be made available at https://github.com/ aspuru-guzik-group/chemical_vae</p>
<p>Table 3: Percentage of successfully decoding of latent representation after 1000 attempts for 1000 molecules from the traning set, 1000 validation molecules randomly chosen from ZINC and a 1000 validation molecules randomly chosen from eMolecules. Both VAEs perform very well for training data, and they are well transferable within molecules of the same class outside the training data, as evidence by the good validation performance of the ZINC VAE and the underperformance of the QM9 VAE against real-life small molecules.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">ZINC</th>
<th style="text-align: center;">QM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Training set</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">99.6</td>
</tr>
<tr>
<td style="text-align: center;">Test set</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">99.4</td>
</tr>
<tr>
<td style="text-align: center;">ZINC</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;">eMolecules</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">8.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Percentage of 5000 randomly-selected latent points that decode to valid molecules after 1000 attempts</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">ZINC</th>
<th style="text-align: center;">QM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Decoding probability</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">79.3</td>
</tr>
</tbody>
</table>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8807 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8807</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8807</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-b4812702a7c1c47e2af34d8752b2103505089fc2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b4812702a7c1c47e2af34d8752b2103505089fc2" target="_blank">A Graph-to-Sequence Model for AMR-to-Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics, and shows superior results to existing methods in the literature.</p>
                <p><strong>Paper Abstract:</strong> The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8807.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8807.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq-linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence with AMR linearization (Konstas et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence encoder-decoder model that takes a serialized (linearized) AMR graph as an input sequence (depth-first traversal) and decodes text with an attention-based LSTM decoder; used as a strong baseline for AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / serialization (depth-first traversal)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graph is serialized into a bracketed token sequence via depth-first traversal (Konstas et al. formulation). Nodes and edge labels become tokens in a long input sequence; token representations combine word embeddings and optionally a character-LSTM embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) rooted directed semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal serialization to a sequence of tokens (Konstas et al. linearization); each token is embedded (word + optional char-LSTM) and fed to a bidirectional LSTM encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (text generation from semantic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dev BLEU: Seq2seq 18.8 (dev, Table 1); Seq2seq+copy 19.9; Seq2seq+charLSTM+copy 20.6 (dev). Test (with additional Gigaword silver data): Seq2seq+charLSTM+copy -- 27.4 BLEU (200K), 31.7 BLEU (2M) (Table 2). Encoding time reported (dev encoding part): 35.4 s (Seq2seq).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly in this paper to Graph2Seq: the graph encoder outperforms the sequence encoder (Graph2Seq gives higher BLEU and faster encoding). Seq2Seq performance suffers when graph serialization produces long distances between related nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages mature seq2seq machinery (bidirectional LSTM encoder, attention, copy); simple to implement by linearizing graphs; benefits from standard sequence modeling techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization can break graph locality — closely related nodes (parents/children/siblings) can be far apart in the serialized sequence, making it hard for an RNN to recover original graph connections; long serialized sequences for large graphs worsen learning; less parallelizable (sequential encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails on cases where graph locality is crucial (examples in case study): nodes directly connected in AMR become distant in serialization leading to generation errors (e.g., producing incorrect part-of-speech or missing modifiers). Single-direction encoding (only forward or backward LSTM) causes large performance drops (BLEU 11.8/12.7 when only forward/backward used).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8807.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8807.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph2Seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-sequence model with graph-state LSTM (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph encoder that uses LSTM-based node states and recurrent graph-state transitions to directly encode AMR graphs (without serialization), combined with an attention-based LSTM decoder and an optional copy mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-state LSTM (Graph2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each AMR node has a hidden state and cell; in each recurrent transition step, a node aggregates representations of incoming/outgoing labeled edges and summed hidden states of neighboring nodes (incoming/outgoing), and updates its LSTM gates in parallel. Multiple transition iterations allow non-local information propagation. Edge representations include edge label embeddings and source node embeddings (and optional char-LSTM of source node). Final node states are used as attention memory for an LSTM decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) rooted directed labeled graphs (can handle cyclic / reentrancies)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No serialization; representation stays as graph. Encoder performs T recurrent graph-state LSTM transitions (T tuned; paper uses 9) where messages (edge representations + summed neighbor states) are used to update node LSTM states in parallel. Decoder attends over final node states and generates text; copy mechanism optionally used to copy tokens from graph nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (text generation from semantic graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dev BLEU: Graph2seq 20.4; Graph2seq+copy 22.2; Graph2seq+charLSTM+copy 22.8 (Table 1). Dev encoding times: Graph2seq 11.2 s (much faster than seq2seq encoder). Test BLEU (AMR corpus only): Graph2seq+charLSTM+copy 23.3 (Table 2), outperforming MSeq2seq+Anon by 1.3 BLEU. With additional Gigaword silver data: Graph2seq+charLSTM+copy -- 28.2 BLEU (200K), 33.0 BLEU (2M). Ablation: using only 1 graph transition iteration yields BLEU 14.1; 5 iterations BLEU 21.5; 9 iterations BLEU 22.8 (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms the sequence-to-sequence baseline consistently (1.6 dev BLEU over Seq2seq without copy; 2.3 BLEU gap when both use copy). Performs better than anonymized multi-layer seq2seq (MSeq2seq+Anon) on same AMR-only data and on scaled silver data. More efficient (parallel node updates) vs sequential LSTM encoder (significantly lower encoding time). Compared qualitatively to Graph Convolutional Networks (GCN): GCN uses linear transformations while Graph LSTM uses gated LSTM updates and keeps cell vectors, suggesting higher modeling power; empirical GCN comparison left to future work.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models graph structure without losing locality; allows non-local propagation via multiple recurrent steps; parallelizable node updates (faster encoding); integrates edge labels and directionality explicitly; benefits attention and copy (better node representations yield better copying). Demonstrated better BLEU and efficiency compared to seq2seq linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires choosing number of recurrent transition steps (too few steps yield poor performance; too many may be unnecessary); training/updating multiple node states and cell vectors increases model complexity; hyperparameters like max neighbors (paper sets at most 10) constrain the receptive field per iteration. Empirical comparisons to other graph encoders (e.g., GCN) were not performed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If only one transition iteration is used, performance is very poor (BLEU 14.1). Using only incoming or only outgoing edges during state transitions causes large performance drops (nodes lack ancestor/descendant/sibling information). The paper also notes that full-graph propagation may not be necessary for all graphs (graph diameters vary) and that excessive iterations were not exhaustively optimized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8807.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8807.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy-mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copy mechanism / pointer-generator (Gu et al., 2016; Gulcehre et al., 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism integrated into the attention-based decoder to allow generating words either from a fixed vocabulary or by copying tokens directly from the input via the attention distribution, mitigating data sparsity for open-class tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Incorporating copying mechanism in sequence-to-sequence learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Copy/pointer mechanism over graph nodes</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The decoder computes a generation probability θ_t (sigmoid over decoder state, context and input embedding) to interpolate between P_vocab (softmax over vocabulary) and P_attn (distribution over input graph nodes aggregated by attention; probabilities of nodes with identical concepts are summed). Final distribution is P_final = θ_t P_vocab + (1-θ_t) P_attn.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (open-class tokens appear as node concepts, e.g., names, dates, numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not a graph-to-sequence conversion per se; operates in the decoder, using attention over node representations (from either serialized or graph encoder) to copy node tokens into output when appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (also cited as effective for dialogue, summarization, question generation in background literature).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dev BLEU improvements: Seq2seq -> Seq2seq+copy: 18.8 -> 19.9; Seq2seq+charLSTM+copy: 20.6. Graph2seq -> Graph2seq+copy: 20.4 -> 22.2 (Table 1). Copy mechanism yields substantial BLEU gains on both encoder types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to anonymization: copy mechanism achieves comparable or better performance when integrated with Graph2Seq; has practical advantages (see advantages). The paper contrasts copy (learned, automatic) with anonymization (rule-based placeholders).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Automatically learns what to copy, avoids manual rule crafting, easier to adapt to new domains/languages, effective at handling rare open-class tokens (dates, numbers, named entities) that often appear in the input graph, and improves BLEU scores when used with both seq2seq and graph2seq.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Biases outputs towards copying input tokens; requires that tokens to be copied appear in the input graph/surface form. Not a full replacement for anonymization in all pipelines (some prior systems rely on anonymized preprocessing).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure case beyond dependence on presence of correct surface tokens in input; copying may not perfectly handle formatting (e.g., hyphenation) as seen in case studies where punctuation/hyphens differ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8807.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8807.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anonymization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anonymization of open-class AMR subgraphs (Konstas et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing approach that replaces subgraphs representing dates, numbers, and named entities with predefined placeholders before decoding, and recovers surface tokens after decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Anonymization / placeholder substitution</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR subgraphs (e.g., name, quantity structures) are identified and replaced with placeholder tokens (e.g., person_name_0, num_0) during training and decoding; after generation placeholders are mapped back to the original surface tokens using heuristic rules.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (subgraphs for dates, numbers, named entities)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Preprocess AMR graphs by replacing matched subgraph patterns with placeholders prior to linearization/encoding; postprocess generated text by reversing placeholders to recover actual tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Graph2seq+Anon dev BLEU: 22.1 (Table 1). MSeq2seq+Anon test BLEU: 22.0 (AMR-only). Comparable dev/test performance to copy-based models in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Anonymization gives comparable gains as copy when used with Graph2Seq (Graph2seq+Anon vs Graph2seq+copy produce similar dev BLEU). However, the paper emphasizes copy mechanism's practical advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces data sparsity for open-class items by canonicalizing surface forms; can produce strong results when rules cover common subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires significant manual work to define placeholders and heuristic mapping rules; brittle to domain/language shifts; mapping rules may be costly to craft and maintain.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Coverage depends on hand-crafted rules; may fail on subgraph patterns not covered by rules or on languages/domains outside the rule set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8807.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8807.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree2Str</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-to-string conversion by splitting re-entrances (Flanigan et al., 2016b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts AMR graphs to tree structures by splitting graph re-entrancies, then uses a tree transducer to generate strings from the resulting trees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation from abstract meaning representation using tree transducers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-to-tree conversion (split re-entrances) then tree transduction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs are transformed into trees by splitting nodes that have multiple parents (re-entrancies), producing a tree suitable for tree-to-string transducers which generate surface text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Identify re-entrancies (nodes with multiple incoming edges) and duplicate/split them to yield a tree; apply a tree transducer to convert the tree to string output.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test BLEU (Table 2): Tree2Str 23.0 BLEU (on the test set compared in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared in Table 2. Tree2Str is a competitive non-neural/statistical approach but is outperformed by Graph2Seq+charLSTM+copy when large silver data is used (Graph2Seq with 2M silver data achieves 33.0).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Uses principled, linguistically motivated tree transduction; avoids handling full graph structure in generator by converting to tree.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Splitting re-entrances duplicates content and may lose the explicit re-entrancy semantics; tree conversion is a lossy transformation relative to the original graph.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Potential semantic distortion from splitting re-entrancies; not explicitly detailed in this paper beyond being a contrasting approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8807.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8807.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PBMT-linearized</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phrase-Based Machine Translation on linearized AMR (Pourdamghani et al., 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A phrase-based statistical machine translation approach treating linearized AMR graphs as source sequences to be translated into English sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating English from abstract meaning representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization (breadth-first in Pourdamghani) + phrase-based MT</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs are serialized (in Pourdamghani et al. they used breadth-first traversal in prior work) into sequences that are input to a phrase-based statistical MT system trained to 'translate' these sequences into natural language sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (serialized)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linearize graph (breadth-first traversal as used in related work) and feed sequence to a phrase-based MT system (Moses) for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test BLEU (Table 2): PBMT 26.9 BLEU (on test set reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>PBMT is a strong statistical baseline that outperforms some neural models trained only on limited gold data, but neural Graph2Seq can outperform PBMT when scaled silver data is available.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Well-understood statistical approach; can be competitive, especially with limited gold training data.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on serialization (same shortcomings as seq2seq linearization); less flexible to neural end-to-end feature learning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper; general limitations include dependence on quality of linearization and phrase-based models' inability to exploit graph structure directly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8807.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8807.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SNRG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synchronous Node Replacement Grammar (Song et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grammar-based approach that parses AMR graphs using synchronous node replacement grammar to jointly parse and generate text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMR-to-text generation with synchronous node replacement grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Grammar-based graph-to-string via synchronous node replacement grammar</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Synchronous node replacement grammar formalism is applied to AMR graphs to produce target strings while performing graph rewriting/parsing operations; allows joint handling of graph structure and output generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use synchronous node replacement grammar rules to rewrite graph fragments into string fragments, effectively mapping subgraphs to surface strings in a grammar-driven manner.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test BLEU (Table 2): SNRG 25.6 BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared in Table 2; SNRG outperforms some neural baselines trained on limited gold data but is outperformed by Graph2Seq with large silver data (2M).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicit grammar captures structure-to-string mappings; can model complex graph-to-text correspondences.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Grammar extraction and coverage can be complex; may require considerable engineering and suffer when encountering unseen subgraph patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specifically detailed in this paper; subject to grammar coverage and generalization limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8807.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8807.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN-encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Networks for graph encoding (Marcheggiani and Titov / Kipf and Welling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph convolutional encoders that update node representations via (linear) transformations and neighborhood aggregation; cited as prior work for encoding syntactic/semantic graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Encoding sentences with graph convolutional networks for semantic role labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Convolutional Network (GCN) encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Node states are updated by linear transformations of neighboring node features (convolution-like operations) and aggregation; GCNs typically do not use gated recurrent cells or explicit cell memory per node.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Syntactic/semantic graphs (applicable to AMR in principle)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply GCN layers to the input graph to compute node embeddings used by downstream decoders or classifiers; each layer aggregates neighboring node features with learnable linear transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Cited applications include semantic role labeling and syntax-aware NMT; suggested applicability to AMR-to-text encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not empirically evaluated in this paper for AMR-to-text; referenced as prior work. The paper contrasts qualitative modeling differences but leaves empirical GCN comparison to future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper states GCN uses linear transformations while their Graph LSTM uses gated LSTM updates and maintains cell vectors, implying potentially greater modeling power for the graph LSTM. No direct empirical comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simplicity and efficiency; effective neighborhood aggregation in a convolutional manner; has been successfully applied to related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Less expressive than gated recurrent units in capturing long-range dependencies and remembering history (no per-node cell vector), according to authors' qualitative argument.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not evaluated on AMR-to-text in this paper; empirical failure cases not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8807.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8807.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAG-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAG LSTM for graph-structured encodings (Peng et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM extension that models DAG-structured inputs (e.g., syntactic/discourse graphs) with sequential update rules; related to but distinct from the parallel graph-state LSTM in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cross-sentence n-ary relation extraction with graph LSTMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>DAG-LSTM (sequence-ordered updates over DAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>DAG LSTM updates node states following the sentence order for each node, with sequential nature; typically requires splitting graphs into DAGs and processing them with ordered state updates.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>DAGs derived from syntactic/discourse graphs (used for n-ary relation extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Split input graph into DAGs, update node LSTM states along DAG order(s); used to produce node representations for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Relation extraction (cross-sentence n-ary relation extraction) and other graph-based NLP tasks; referenced as related work to graph LSTM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated for AMR-to-text in this paper; cited as related literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors state DAG-LSTM is sequential and requires splitting graphs into DAGs, whereas their graph-state LSTM updates node states in parallel and does not require graph splitting. No empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages LSTM gating for graph-structured inputs and has been shown effective for relation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Sequential update order and requirement to split graphs into DAGs limits direct applicability to arbitrary graphs (e.g., cyclic AMRs); less parallelizable than the graph-state LSTM proposed here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper; limitations are described conceptually (need to split graphs, sequential nature).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Sequence Model for AMR-to-Text Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation. <em>(Rating: 2)</em></li>
                <li>Generation from abstract meaning representation using tree transducers. <em>(Rating: 2)</em></li>
                <li>Generating English from abstract meaning representations. <em>(Rating: 2)</em></li>
                <li>AMR-to-text generation with synchronous node replacement grammar. <em>(Rating: 2)</em></li>
                <li>Encoding sentences with graph convolutional networks for semantic role labeling. <em>(Rating: 2)</em></li>
                <li>Semisupervised classification with graph convolutional networks. <em>(Rating: 1)</em></li>
                <li>Incorporating copying mechanism in sequence-to-sequence learning. <em>(Rating: 2)</em></li>
                <li>Pointing the unknown words. <em>(Rating: 1)</em></li>
                <li>Cross-sentence n-ary relation extraction with graph LSTMs. <em>(Rating: 2)</em></li>
                <li>AMR-to-text generation with synchronous node replacement grammar. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8807",
    "paper_id": "paper-b4812702a7c1c47e2af34d8752b2103505089fc2",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Seq2Seq-linearization",
            "name_full": "Sequence-to-sequence with AMR linearization (Konstas et al., 2017)",
            "brief_description": "A sequence-to-sequence encoder-decoder model that takes a serialized (linearized) AMR graph as an input sequence (depth-first traversal) and decodes text with an attention-based LSTM decoder; used as a strong baseline for AMR-to-text generation.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation.",
            "mention_or_use": "use",
            "representation_name": "Linearization / serialization (depth-first traversal)",
            "representation_description": "AMR graph is serialized into a bracketed token sequence via depth-first traversal (Konstas et al. formulation). Nodes and edge labels become tokens in a long input sequence; token representations combine word embeddings and optionally a character-LSTM embedding.",
            "graph_type": "Abstract Meaning Representation (AMR) rooted directed semantic graphs",
            "conversion_method": "Depth-first traversal serialization to a sequence of tokens (Konstas et al. linearization); each token is embedded (word + optional char-LSTM) and fed to a bidirectional LSTM encoder.",
            "downstream_task": "AMR-to-text generation (text generation from semantic graphs)",
            "performance_metrics": "Dev BLEU: Seq2seq 18.8 (dev, Table 1); Seq2seq+copy 19.9; Seq2seq+charLSTM+copy 20.6 (dev). Test (with additional Gigaword silver data): Seq2seq+charLSTM+copy -- 27.4 BLEU (200K), 31.7 BLEU (2M) (Table 2). Encoding time reported (dev encoding part): 35.4 s (Seq2seq).",
            "comparison_to_others": "Compared directly in this paper to Graph2Seq: the graph encoder outperforms the sequence encoder (Graph2Seq gives higher BLEU and faster encoding). Seq2Seq performance suffers when graph serialization produces long distances between related nodes.",
            "advantages": "Leverages mature seq2seq machinery (bidirectional LSTM encoder, attention, copy); simple to implement by linearizing graphs; benefits from standard sequence modeling techniques.",
            "disadvantages": "Linearization can break graph locality — closely related nodes (parents/children/siblings) can be far apart in the serialized sequence, making it hard for an RNN to recover original graph connections; long serialized sequences for large graphs worsen learning; less parallelizable (sequential encoder).",
            "failure_cases": "Fails on cases where graph locality is crucial (examples in case study): nodes directly connected in AMR become distant in serialization leading to generation errors (e.g., producing incorrect part-of-speech or missing modifiers). Single-direction encoding (only forward or backward LSTM) causes large performance drops (BLEU 11.8/12.7 when only forward/backward used).",
            "uuid": "e8807.0",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Graph2Seq",
            "name_full": "Graph-to-sequence model with graph-state LSTM (this paper)",
            "brief_description": "A graph encoder that uses LSTM-based node states and recurrent graph-state transitions to directly encode AMR graphs (without serialization), combined with an attention-based LSTM decoder and an optional copy mechanism.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph-state LSTM (Graph2Seq)",
            "representation_description": "Each AMR node has a hidden state and cell; in each recurrent transition step, a node aggregates representations of incoming/outgoing labeled edges and summed hidden states of neighboring nodes (incoming/outgoing), and updates its LSTM gates in parallel. Multiple transition iterations allow non-local information propagation. Edge representations include edge label embeddings and source node embeddings (and optional char-LSTM of source node). Final node states are used as attention memory for an LSTM decoder.",
            "graph_type": "Abstract Meaning Representation (AMR) rooted directed labeled graphs (can handle cyclic / reentrancies)",
            "conversion_method": "No serialization; representation stays as graph. Encoder performs T recurrent graph-state LSTM transitions (T tuned; paper uses 9) where messages (edge representations + summed neighbor states) are used to update node LSTM states in parallel. Decoder attends over final node states and generates text; copy mechanism optionally used to copy tokens from graph nodes.",
            "downstream_task": "AMR-to-text generation (text generation from semantic graphs).",
            "performance_metrics": "Dev BLEU: Graph2seq 20.4; Graph2seq+copy 22.2; Graph2seq+charLSTM+copy 22.8 (Table 1). Dev encoding times: Graph2seq 11.2 s (much faster than seq2seq encoder). Test BLEU (AMR corpus only): Graph2seq+charLSTM+copy 23.3 (Table 2), outperforming MSeq2seq+Anon by 1.3 BLEU. With additional Gigaword silver data: Graph2seq+charLSTM+copy -- 28.2 BLEU (200K), 33.0 BLEU (2M). Ablation: using only 1 graph transition iteration yields BLEU 14.1; 5 iterations BLEU 21.5; 9 iterations BLEU 22.8 (Figure 3).",
            "comparison_to_others": "Outperforms the sequence-to-sequence baseline consistently (1.6 dev BLEU over Seq2seq without copy; 2.3 BLEU gap when both use copy). Performs better than anonymized multi-layer seq2seq (MSeq2seq+Anon) on same AMR-only data and on scaled silver data. More efficient (parallel node updates) vs sequential LSTM encoder (significantly lower encoding time). Compared qualitatively to Graph Convolutional Networks (GCN): GCN uses linear transformations while Graph LSTM uses gated LSTM updates and keeps cell vectors, suggesting higher modeling power; empirical GCN comparison left to future work.",
            "advantages": "Directly models graph structure without losing locality; allows non-local propagation via multiple recurrent steps; parallelizable node updates (faster encoding); integrates edge labels and directionality explicitly; benefits attention and copy (better node representations yield better copying). Demonstrated better BLEU and efficiency compared to seq2seq linearization.",
            "disadvantages": "Requires choosing number of recurrent transition steps (too few steps yield poor performance; too many may be unnecessary); training/updating multiple node states and cell vectors increases model complexity; hyperparameters like max neighbors (paper sets at most 10) constrain the receptive field per iteration. Empirical comparisons to other graph encoders (e.g., GCN) were not performed in this work.",
            "failure_cases": "If only one transition iteration is used, performance is very poor (BLEU 14.1). Using only incoming or only outgoing edges during state transitions causes large performance drops (nodes lack ancestor/descendant/sibling information). The paper also notes that full-graph propagation may not be necessary for all graphs (graph diameters vary) and that excessive iterations were not exhaustively optimized.",
            "uuid": "e8807.1",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Copy-mechanism",
            "name_full": "Copy mechanism / pointer-generator (Gu et al., 2016; Gulcehre et al., 2016)",
            "brief_description": "A mechanism integrated into the attention-based decoder to allow generating words either from a fixed vocabulary or by copying tokens directly from the input via the attention distribution, mitigating data sparsity for open-class tokens.",
            "citation_title": "Incorporating copying mechanism in sequence-to-sequence learning.",
            "mention_or_use": "use",
            "representation_name": "Copy/pointer mechanism over graph nodes",
            "representation_description": "The decoder computes a generation probability θ_t (sigmoid over decoder state, context and input embedding) to interpolate between P_vocab (softmax over vocabulary) and P_attn (distribution over input graph nodes aggregated by attention; probabilities of nodes with identical concepts are summed). Final distribution is P_final = θ_t P_vocab + (1-θ_t) P_attn.",
            "graph_type": "AMR graphs (open-class tokens appear as node concepts, e.g., names, dates, numbers)",
            "conversion_method": "Not a graph-to-sequence conversion per se; operates in the decoder, using attention over node representations (from either serialized or graph encoder) to copy node tokens into output when appropriate.",
            "downstream_task": "AMR-to-text generation (also cited as effective for dialogue, summarization, question generation in background literature).",
            "performance_metrics": "Dev BLEU improvements: Seq2seq -&gt; Seq2seq+copy: 18.8 -&gt; 19.9; Seq2seq+charLSTM+copy: 20.6. Graph2seq -&gt; Graph2seq+copy: 20.4 -&gt; 22.2 (Table 1). Copy mechanism yields substantial BLEU gains on both encoder types.",
            "comparison_to_others": "Compared to anonymization: copy mechanism achieves comparable or better performance when integrated with Graph2Seq; has practical advantages (see advantages). The paper contrasts copy (learned, automatic) with anonymization (rule-based placeholders).",
            "advantages": "Automatically learns what to copy, avoids manual rule crafting, easier to adapt to new domains/languages, effective at handling rare open-class tokens (dates, numbers, named entities) that often appear in the input graph, and improves BLEU scores when used with both seq2seq and graph2seq.",
            "disadvantages": "Biases outputs towards copying input tokens; requires that tokens to be copied appear in the input graph/surface form. Not a full replacement for anonymization in all pipelines (some prior systems rely on anonymized preprocessing).",
            "failure_cases": "No explicit failure case beyond dependence on presence of correct surface tokens in input; copying may not perfectly handle formatting (e.g., hyphenation) as seen in case studies where punctuation/hyphens differ.",
            "uuid": "e8807.2",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Anonymization",
            "name_full": "Anonymization of open-class AMR subgraphs (Konstas et al., 2017)",
            "brief_description": "A preprocessing approach that replaces subgraphs representing dates, numbers, and named entities with predefined placeholders before decoding, and recovers surface tokens after decoding.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation.",
            "mention_or_use": "use",
            "representation_name": "Anonymization / placeholder substitution",
            "representation_description": "AMR subgraphs (e.g., name, quantity structures) are identified and replaced with placeholder tokens (e.g., person_name_0, num_0) during training and decoding; after generation placeholders are mapped back to the original surface tokens using heuristic rules.",
            "graph_type": "AMR graphs (subgraphs for dates, numbers, named entities)",
            "conversion_method": "Preprocess AMR graphs by replacing matched subgraph patterns with placeholders prior to linearization/encoding; postprocess generated text by reversing placeholders to recover actual tokens.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Graph2seq+Anon dev BLEU: 22.1 (Table 1). MSeq2seq+Anon test BLEU: 22.0 (AMR-only). Comparable dev/test performance to copy-based models in some settings.",
            "comparison_to_others": "Anonymization gives comparable gains as copy when used with Graph2Seq (Graph2seq+Anon vs Graph2seq+copy produce similar dev BLEU). However, the paper emphasizes copy mechanism's practical advantages.",
            "advantages": "Reduces data sparsity for open-class items by canonicalizing surface forms; can produce strong results when rules cover common subgraphs.",
            "disadvantages": "Requires significant manual work to define placeholders and heuristic mapping rules; brittle to domain/language shifts; mapping rules may be costly to craft and maintain.",
            "failure_cases": "Coverage depends on hand-crafted rules; may fail on subgraph patterns not covered by rules or on languages/domains outside the rule set.",
            "uuid": "e8807.3",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Tree2Str",
            "name_full": "Tree-to-string conversion by splitting re-entrances (Flanigan et al., 2016b)",
            "brief_description": "A method that converts AMR graphs to tree structures by splitting graph re-entrancies, then uses a tree transducer to generate strings from the resulting trees.",
            "citation_title": "Generation from abstract meaning representation using tree transducers.",
            "mention_or_use": "mention",
            "representation_name": "Graph-to-tree conversion (split re-entrances) then tree transduction",
            "representation_description": "AMR graphs are transformed into trees by splitting nodes that have multiple parents (re-entrancies), producing a tree suitable for tree-to-string transducers which generate surface text.",
            "graph_type": "Abstract Meaning Representation (AMR) graphs",
            "conversion_method": "Identify re-entrancies (nodes with multiple incoming edges) and duplicate/split them to yield a tree; apply a tree transducer to convert the tree to string output.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Test BLEU (Table 2): Tree2Str 23.0 BLEU (on the test set compared in paper).",
            "comparison_to_others": "Compared in Table 2. Tree2Str is a competitive non-neural/statistical approach but is outperformed by Graph2Seq+charLSTM+copy when large silver data is used (Graph2Seq with 2M silver data achieves 33.0).",
            "advantages": "Uses principled, linguistically motivated tree transduction; avoids handling full graph structure in generator by converting to tree.",
            "disadvantages": "Splitting re-entrances duplicates content and may lose the explicit re-entrancy semantics; tree conversion is a lossy transformation relative to the original graph.",
            "failure_cases": "Potential semantic distortion from splitting re-entrancies; not explicitly detailed in this paper beyond being a contrasting approach.",
            "uuid": "e8807.4",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "PBMT-linearized",
            "name_full": "Phrase-Based Machine Translation on linearized AMR (Pourdamghani et al., 2016)",
            "brief_description": "A phrase-based statistical machine translation approach treating linearized AMR graphs as source sequences to be translated into English sentences.",
            "citation_title": "Generating English from abstract meaning representations.",
            "mention_or_use": "mention",
            "representation_name": "Linearization (breadth-first in Pourdamghani) + phrase-based MT",
            "representation_description": "AMR graphs are serialized (in Pourdamghani et al. they used breadth-first traversal in prior work) into sequences that are input to a phrase-based statistical MT system trained to 'translate' these sequences into natural language sentences.",
            "graph_type": "AMR graphs (serialized)",
            "conversion_method": "Linearize graph (breadth-first traversal as used in related work) and feed sequence to a phrase-based MT system (Moses) for generation.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Test BLEU (Table 2): PBMT 26.9 BLEU (on test set reported in Table 2).",
            "comparison_to_others": "PBMT is a strong statistical baseline that outperforms some neural models trained only on limited gold data, but neural Graph2Seq can outperform PBMT when scaled silver data is available.",
            "advantages": "Well-understood statistical approach; can be competitive, especially with limited gold training data.",
            "disadvantages": "Relies on serialization (same shortcomings as seq2seq linearization); less flexible to neural end-to-end feature learning.",
            "failure_cases": "Not detailed in this paper; general limitations include dependence on quality of linearization and phrase-based models' inability to exploit graph structure directly.",
            "uuid": "e8807.5",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "SNRG",
            "name_full": "Synchronous Node Replacement Grammar (Song et al., 2017)",
            "brief_description": "A grammar-based approach that parses AMR graphs using synchronous node replacement grammar to jointly parse and generate text.",
            "citation_title": "AMR-to-text generation with synchronous node replacement grammar.",
            "mention_or_use": "mention",
            "representation_name": "Grammar-based graph-to-string via synchronous node replacement grammar",
            "representation_description": "Synchronous node replacement grammar formalism is applied to AMR graphs to produce target strings while performing graph rewriting/parsing operations; allows joint handling of graph structure and output generation.",
            "graph_type": "AMR graphs",
            "conversion_method": "Use synchronous node replacement grammar rules to rewrite graph fragments into string fragments, effectively mapping subgraphs to surface strings in a grammar-driven manner.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Test BLEU (Table 2): SNRG 25.6 BLEU.",
            "comparison_to_others": "Compared in Table 2; SNRG outperforms some neural baselines trained on limited gold data but is outperformed by Graph2Seq with large silver data (2M).",
            "advantages": "Explicit grammar captures structure-to-string mappings; can model complex graph-to-text correspondences.",
            "disadvantages": "Grammar extraction and coverage can be complex; may require considerable engineering and suffer when encountering unseen subgraph patterns.",
            "failure_cases": "Not specifically detailed in this paper; subject to grammar coverage and generalization limitations.",
            "uuid": "e8807.6",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "GCN-encoding",
            "name_full": "Graph Convolutional Networks for graph encoding (Marcheggiani and Titov / Kipf and Welling)",
            "brief_description": "Graph convolutional encoders that update node representations via (linear) transformations and neighborhood aggregation; cited as prior work for encoding syntactic/semantic graphs.",
            "citation_title": "Encoding sentences with graph convolutional networks for semantic role labeling.",
            "mention_or_use": "mention",
            "representation_name": "Graph Convolutional Network (GCN) encoding",
            "representation_description": "Node states are updated by linear transformations of neighboring node features (convolution-like operations) and aggregation; GCNs typically do not use gated recurrent cells or explicit cell memory per node.",
            "graph_type": "Syntactic/semantic graphs (applicable to AMR in principle)",
            "conversion_method": "Apply GCN layers to the input graph to compute node embeddings used by downstream decoders or classifiers; each layer aggregates neighboring node features with learnable linear transforms.",
            "downstream_task": "Cited applications include semantic role labeling and syntax-aware NMT; suggested applicability to AMR-to-text encoding.",
            "performance_metrics": "Not empirically evaluated in this paper for AMR-to-text; referenced as prior work. The paper contrasts qualitative modeling differences but leaves empirical GCN comparison to future work.",
            "comparison_to_others": "Paper states GCN uses linear transformations while their Graph LSTM uses gated LSTM updates and maintains cell vectors, implying potentially greater modeling power for the graph LSTM. No direct empirical comparison provided here.",
            "advantages": "Simplicity and efficiency; effective neighborhood aggregation in a convolutional manner; has been successfully applied to related tasks.",
            "disadvantages": "Less expressive than gated recurrent units in capturing long-range dependencies and remembering history (no per-node cell vector), according to authors' qualitative argument.",
            "failure_cases": "Not evaluated on AMR-to-text in this paper; empirical failure cases not provided here.",
            "uuid": "e8807.7",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "DAG-LSTM",
            "name_full": "DAG LSTM for graph-structured encodings (Peng et al., 2017)",
            "brief_description": "An LSTM extension that models DAG-structured inputs (e.g., syntactic/discourse graphs) with sequential update rules; related to but distinct from the parallel graph-state LSTM in this paper.",
            "citation_title": "Cross-sentence n-ary relation extraction with graph LSTMs.",
            "mention_or_use": "mention",
            "representation_name": "DAG-LSTM (sequence-ordered updates over DAGs)",
            "representation_description": "DAG LSTM updates node states following the sentence order for each node, with sequential nature; typically requires splitting graphs into DAGs and processing them with ordered state updates.",
            "graph_type": "DAGs derived from syntactic/discourse graphs (used for n-ary relation extraction)",
            "conversion_method": "Split input graph into DAGs, update node LSTM states along DAG order(s); used to produce node representations for downstream tasks.",
            "downstream_task": "Relation extraction (cross-sentence n-ary relation extraction) and other graph-based NLP tasks; referenced as related work to graph LSTM approaches.",
            "performance_metrics": "Not evaluated for AMR-to-text in this paper; cited as related literature.",
            "comparison_to_others": "Authors state DAG-LSTM is sequential and requires splitting graphs into DAGs, whereas their graph-state LSTM updates node states in parallel and does not require graph splitting. No empirical comparison in this paper.",
            "advantages": "Leverages LSTM gating for graph-structured inputs and has been shown effective for relation extraction.",
            "disadvantages": "Sequential update order and requirement to split graphs into DAGs limits direct applicability to arbitrary graphs (e.g., cyclic AMRs); less parallelizable than the graph-state LSTM proposed here.",
            "failure_cases": "Not detailed in this paper; limitations are described conceptually (need to split graphs, sequential nature).",
            "uuid": "e8807.8",
            "source_info": {
                "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
                "publication_date_yy_mm": "2018-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation.",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Generation from abstract meaning representation using tree transducers.",
            "rating": 2,
            "sanitized_title": "generation_from_abstract_meaning_representation_using_tree_transducers"
        },
        {
            "paper_title": "Generating English from abstract meaning representations.",
            "rating": 2,
            "sanitized_title": "generating_english_from_abstract_meaning_representations"
        },
        {
            "paper_title": "AMR-to-text generation with synchronous node replacement grammar.",
            "rating": 2,
            "sanitized_title": "amrtotext_generation_with_synchronous_node_replacement_grammar"
        },
        {
            "paper_title": "Encoding sentences with graph convolutional networks for semantic role labeling.",
            "rating": 2,
            "sanitized_title": "encoding_sentences_with_graph_convolutional_networks_for_semantic_role_labeling"
        },
        {
            "paper_title": "Semisupervised classification with graph convolutional networks.",
            "rating": 1,
            "sanitized_title": "semisupervised_classification_with_graph_convolutional_networks"
        },
        {
            "paper_title": "Incorporating copying mechanism in sequence-to-sequence learning.",
            "rating": 2,
            "sanitized_title": "incorporating_copying_mechanism_in_sequencetosequence_learning"
        },
        {
            "paper_title": "Pointing the unknown words.",
            "rating": 1,
            "sanitized_title": "pointing_the_unknown_words"
        },
        {
            "paper_title": "Cross-sentence n-ary relation extraction with graph LSTMs.",
            "rating": 2,
            "sanitized_title": "crosssentence_nary_relation_extraction_with_graph_lstms"
        },
        {
            "paper_title": "AMR-to-text generation with synchronous node replacement grammar.",
            "rating": 2,
            "sanitized_title": "amrtotext_generation_with_synchronous_node_replacement_grammar"
        }
    ],
    "cost": 0.017704249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Graph-to-Sequence Model for AMR-to-Text Generation</h1>
<p>Linfeng Song ${ }^{1}$, Yue Zhang ${ }^{3}$, Zhiguo Wang ${ }^{2}$ and Daniel Gildea ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, University of Rochester, Rochester, NY 14627<br>${ }^{2}$ IBM T.J. Watson Research Center, Yorktown Heights, NY 10598<br>${ }^{3}$ Singapore University of Technology and Design</p>
<h4>Abstract</h4>
<p>The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although it is able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.</p>
<h2>1 Introduction</h2>
<p>Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as "describe-01" and "person") represent the concepts, and edges (such as ":ARG0" and ":name") represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015).</p>
<p>The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of AMR graph meaning "Ryan's description of himself: a genius."
be far from literal. For example, shown in Figure 1, "Ryan" is represented as "(p / person :name (n / name :op1 "Ryan"))", and "description of" is represented as "(d / describe-01 :ARG1 )".</p>
<p>While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a linear recurrent neural network to automatically induce their original connections from bracketed string forms.</p>
<p>To address this issue, we introduce a novel graph-to-sequence model, where a graph-state LSTM is used to encode AMR structures directly.</p>
<p>To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input.</p>
<p>Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our model is consistently better than Konstas et al. (2017) using the same amount of gigaword data, showing the effectiveness of our model on large-scale training set.</p>
<p>We release our code and models at https: //github.com/freesunshine0316/ neural-graph-to-seq-mp.</p>
<h2>2 Baseline: a seq-to-seq model</h2>
<p>Our baseline is a sequence-to-sequence model, which follows the encoder-decoder framework of Konstas et al. (2017).</p>
<h3>2.1 Input representation</h3>
<p>Given an AMR graph $G=(V, E)$, where $V$ and $E$ denote the sets of nodes and edges, respectively, we use the depth-first traversal of Konstas et al. (2017) to linearize it to obtain a sequence of tokens $v_{1}, \ldots, v_{N}$, where $N$ is the number of tokens. For example, the AMR graph in Figure 1 is serialized as "describe :arg0 ( person :name ( name :op1 ryan ) ) :arg1 person :arg2 genius". We can see that the distance between "describe" and "genius", which are directly connected in the original AMR, becomes 14 in the serialization result.</p>
<p>A simple way to calculate the representation for each token $v_{j}$ is using its word embedding $e_{j}$ :</p>
<p>$$
x_{j}=W_{1} e_{j}+b_{1}
$$</p>
<p>where $W_{1}$ and $b_{1}$ are model parameters for compressing the input vector size.</p>
<p>To alleviate the data sparsity problem and obtain better word representation as the input, we also adopt a forward LSTM over the characters of the token, and concatenate the last hidden state $h_{j}^{c}$ with the word embedding:</p>
<p>$$
x_{j}=W_{1}\left(\left[e_{j} ; h_{j}^{c}\right]\right)+b_{1}
$$</p>
<h3>2.2 Encoder</h3>
<p>The encoder is a bi-directional LSTM applied on the linearized graph by depth-first traversal, as in Konstas et al. (2017). At each step $j$, the current states $\overleftarrow{h_{j}}$ and $\overrightarrow{h_{j}}$ are generated given the previous states $\overleftarrow{h_{j+1}}$ and $h_{j}{ }^{\rightarrow}$ and the current input $x_{j}$ :</p>
<p>$$
\begin{aligned}
&amp; \overleftarrow{h_{j}}=\operatorname{LSTM}\left(\overleftarrow{h_{j+1}}, x_{j}\right) \
&amp; \overrightarrow{h_{j}}=\operatorname{LSTM}\left(h_{j}{ }^{\rightarrow}, x_{j}\right)
\end{aligned}
$$</p>
<h3>2.3 Decoder</h3>
<p>We use an attention-based LSTM decoder (Bahdanau et al., 2015), where the attention memory $(A)$ is the concatenation of the attention vectors among all input words. Each attention vector $a_{j}$ is the concatenation of the encoder states of an input token in both directions ( $\overleftarrow{h_{j}}$ and $\overrightarrow{h_{j}}$ ) and its input vector $\left(x_{j}\right)$ :</p>
<p>$$
\begin{aligned}
a_{j} &amp; =\left[\overleftarrow{h_{j}} ; \overrightarrow{h_{j}} ; x_{j}\right] \
A &amp; =\left[a_{1} ; a_{2} ; \ldots ; a_{N}\right]
\end{aligned}
$$</p>
<p>where $N$ is the number of input tokens.
The decoder yields an output sequence $w_{1}, w_{2}, \ldots, w_{M}$ by calculating a sequence of hidden states $s_{1}, s_{2} \ldots, s_{M}$ recurrently. While generating the $t$-th word, the decoder considers five factors: (1) the attention memory $A$; (2) the previous hidden state of the LSTM model $s_{t}$; (3) the embedding of the current input (previously generated word) $e_{t}$; (4) the previous context vector $\mu_{t}$ 1, which is calculated with attention from $A$; and (5) the previous coverage vector $\gamma_{t}$ 1, which is the accumulation of all attention distributions so far (Tu et al., 2016). When $t=1$, we initialize $\mu_{0}$ and $\gamma_{0}$ as zero vectors, set $e_{1}$ to the embedding of the start token " $&lt;\mathrm{s}&gt;$ ", and $s_{0}$ as the average of all encoder states.</p>
<p>For each time-step $t$, the decoder feeds the concatenation of the embedding of the current input $e_{t}$ and the previous context vector $\mu_{t} 1$ into the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Graph state LSTM.</p>
<p>LSTM model to update its hidden state. Then the attention probability $\alpha_{t, i}$ on the attention vector $a_{i} \in A$ for the time-step is calculated as:</p>
<p>$$
\begin{aligned}
&amp; \epsilon_{t, i}=v_{2}^{T} \tanh \left(W_{a} a_{i}+W_{s} s_{t}+W_{\gamma} \gamma_{t} 1+b_{2}\right) \
&amp; \alpha_{t, i}=\frac{\exp \left(\epsilon_{t, i}\right)}{\sum_{j=1}^{N} \exp \left(\epsilon_{t, j}\right)}
\end{aligned}
$$</p>
<p>where $W_{a}, W_{s}, W_{\gamma}, v_{2}$ and $b_{2}$ are model parameters. The coverage vector $\gamma_{t}$ is updated by $\gamma_{t}=\gamma_{t} 1+\alpha_{t}$, and the new context vector $\mu_{t}$ is calculated via $\mu_{t}=\sum_{i=1}^{N} \alpha_{t, i} a_{i}$.</p>
<p>The output probability distribution over a vocabulary at the current state is calculated by:</p>
<p>$$
P_{\text {vocab }}=\operatorname{softmax}\left(V_{3}\left[s_{t}, \mu_{t}\right]+b_{3}\right)
$$</p>
<p>where $V_{3}$ and $b_{3}$ are learnable parameters, and the number of rows in $V_{3}$ represents the number of words in the vocabulary.</p>
<h2>3 The graph-to-sequence model</h2>
<p>Unlike the baseline sequence-to-sequence model, we leverage a recurrent graph encoder to represent each input AMR, which directly models the graph structure without serialization.</p>
<h3>3.1 The graph encoder</h3>
<p>Figure 2 shows the overall structure of our graph encoder. Formally, given a graph $G=(V, E)$, we use a hidden state vector $h^{j}$ to represent each node $v_{j} \in V$. The state of the graph can thus be represented as:</p>
<p>$$
g=\left.\left{h^{j}\right}\right|<em j="j">{v</em>
$$} \in V</p>
<p>In order to capture non-local interaction between nodes, we allow information exchange between nodes through a sequence of state transitions, leading to a sequence of states $g_{0}, g_{1}, \ldots, g_{t}, \ldots$, where $g_{t}=\left.\left{h_{t}^{j}\right}\right|<em j="j">{v</em>$ is a hyperparameter of the model.} \in V}$. The initial state $g_{0}$ consists of a set of initial node states $h_{0}^{j}=h_{0}$, where $h_{0</p>
<p>State transition A recurrent neural network is used to model the state transition process. In particular, the transition from $g_{t} 1$ to $g_{t}$ consists of a hidden state transition for each node, as shown in Figure 2. At each state transition step $t$, we allow direct communication between a node and all nodes that are directly connected to the node. To avoid gradient diminishing or bursting, LSTM (Hochreiter and Schmidhuber, 1997) is adopted, where a cell $c_{t}^{j}$ is taken to record memory for $h_{t}^{j}$. We use an input gate $i_{t}^{j}$, an output gate $o_{t}^{j}$ and a forget gate $f_{t}^{j}$ to control information flow from the inputs and to the output $h_{t}^{j}$.</p>
<p>The inputs include representations of edges that are connected to $v_{j}$, where $v_{j}$ can be either the source or the target of the edge. We define each edge as a triple $(i, j, l)$, where $i$ and $j$ are indices of the source and target nodes, respectively, and $l$ is the edge label. $x_{i, j}^{l}$ is the representation of edge $(i, j, l)$, detailed in Section 3.3. The inputs for $v_{j}$ are distinguished by incoming and outgoing edges, before being summed up:</p>
<p>$$
\begin{aligned}
&amp; x_{j}^{i}=\sum_{(i, j, l) \in E_{i n}(j)} x_{i, j}^{l} \
&amp; x_{j}^{o}=\sum_{(j, k, l) \in E_{\text {out }}(j)} x_{j, k}^{l}
\end{aligned}
$$</p>
<p>where $E_{\text {in }}(j)$ and $E_{\text {out }}(j)$ denote the sets of incoming and outgoing edges of $v_{j}$, respectively.</p>
<p>In addition to edge inputs, a cell also takes the hidden states of its incoming nodes and outgoing nodes during a state transition. In particular, the states of all incoming nodes and outgoing nodes are summed up before being passed to the cell and gate nodes:</p>
<p>$$
\begin{aligned}
&amp; h_{j}^{i}=\sum_{(i, j, l) \in E_{i n}(j)} h_{t}^{i} 1 \
&amp; h_{j}^{o}=\sum_{(j, k, l) \in E_{\text {out }}(j)} h_{t}^{k} 1
\end{aligned}
$$</p>
<p>Based on the above definitions of $x_{j}^{i}, x_{j}^{o}, h_{j}^{i}$ and $h_{j}^{o}$, the state transition from $g_{t} 1$ to $g_{t}$, as repre-</p>
<p>sented by $h_{t}^{j}$, can be defined as:</p>
<p>$$
\begin{aligned}
i_{t}^{j} &amp; =\sigma\left(W_{i} x_{j}^{i}+\hat{W}<em j="j">{i} x</em>}^{o}+U_{i} h_{j}^{i}+\hat{U<em j="j">{i} h</em>\right) \
o_{t}^{j} &amp; =\sigma\left(W_{o} x_{j}^{i}+\hat{W}}^{o}+b_{i<em j="j">{o} x</em>}^{o}+U_{o} h_{j}^{i}+\hat{U<em j="j">{o} h</em>\right) \
f_{t}^{j} &amp; =\sigma\left(W_{f} x_{j}^{i}+\hat{W}}^{o}+b_{o<em j="j">{f} x</em>}^{o}+U_{f} h_{j}^{i}+\hat{U<em j="j">{f} h</em>\right) \
u_{t}^{j} &amp; =\sigma\left(W_{u} x_{j}^{i}+\hat{W}}^{o}+b_{f<em j="j">{u} x</em>}^{o}+U_{u} h_{j}^{i}+\hat{U<em j="j">{u} h</em>\right) \
c_{t}^{j} &amp; =f_{t}^{j} \odot c_{t}^{j}{ }}^{o}+b_{u<em t="t">{1}+i</em> \
h_{t}^{j} &amp; =o_{t}^{j} \odot \tanh \left(c_{t}^{j}\right)
\end{aligned}
$$}^{j} \odot u_{t}^{j</p>
<p>where $i_{t}^{j}, o_{t}^{j}$ and $f_{t}^{j}$ are the input, output and forget gates mentioned earlier. $W_{x}, \hat{W}<em x="x">{x}, U</em>}, \hat{U<em x="x">{x}, b</em>$, where $x \in{i, o, f, u}$, are model parameters.</p>
<h3>3.2 Recurrent steps</h3>
<p>Using the above state transition mechanism, information from each node propagates to all its neighboring nodes after each step. Therefore, for the worst case where the input graph is a chain of nodes, the maximum number of steps necessary for information from one arbitrary node to reach another is equal to the size of the graph. We experiment with different transition steps to study the effectiveness of global encoding.</p>
<p>Note that unlike the sequence LSTM encoder, our graph encoder allows parallelization in nodestate updates, and thus can be highly efficient using a GPU. It is general and can be potentially applied to other tasks, including sequences, syntactic trees and cyclic structures.</p>
<h3>3.3 Input Representation</h3>
<p>Different from sequences, the edges of an AMR graph contain labels, which represent relations between the nodes they connect, and are thus important for modeling the graphs. Similar with Section 2 , we adopt two different ways for calculating the representation for each edge $(i, j, l)$ :</p>
<p>$$
\begin{aligned}
x_{i, j}^{l} &amp; =W_{4}\left(\left[e_{l} ; e_{i}\right]\right)+b_{4} \
x_{i, j}^{l} &amp; =W_{4}\left(\left[e_{l} ; e_{i} ; h_{i}^{c}\right]\right)+b_{4}
\end{aligned}
$$</p>
<p>where $e_{l}$ and $e_{i}$ are the embeddings of edge label $l$ and source node $v_{i}, h_{i}^{c}$ denotes the last hidden state of the character LSTM over $v_{i}$, and $W_{4}$ and $b_{4}$ are trainable parameters. The equations correspond to Equations 1 and 2 in Section 2.1, respectively.</p>
<h3>3.4 Decoder</h3>
<p>We adopt the attention-based LSTM decoder as described in Section 2.3. Since our graph encoder
generates a sequence of graph states, only the last graph state is adopted in the decoder. In particular, we make the following changes to the decoder. First, each attention vector becomes $a_{j}=$ $\left[h_{T}^{j} ; x_{j}\right]$, where $h_{T}^{j}$ is the last state for node $v_{j}$. Second, the decoder initial state $s_{1}$ is the average of the last states of all nodes.</p>
<h3>3.5 Integrating the copy mechanism</h3>
<p>Open-class tokens, such as dates, numbers and named entities, account for a large portion in the AMR corpus. Most appear only a few times, resulting in a data sparsity problem. To address this issue, Konstas et al. (2017) adopt anonymization for dealing with the data sparsity problem. In particular, they first replace the subgraphs that represent dates, numbers and named entities (such as "(q / quantity :quant 3)" and "(p / person :name (n / name :op1 "Ryan"))") with predefined placeholders (such as "num_0" and "person_name_0") before decoding, and then recover the corresponding surface tokens (such as " 3 " and "Ryan") after decoding. This method involves hand-crafted rules, which can be costly.</p>
<p>Copy We find that most of the open-class tokens in a graph also appear in the corresponding sentence, and thus adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016) to solve this problem. The mechanism works on top of an attention-based RNN decoder by integrating the attention distribution into the final vocabulary distribution. The final probability distribution is defined as the interpolation between two probability distributions:</p>
<p>$$
P_{\text {final }}=\theta_{t} P_{\text {vocab }}+\left(\begin{array}{ll}
1 &amp; \theta_{t}
\end{array}\right) P_{\text {attn }}
$$</p>
<p>where $\theta_{t}$ is a switch for controlling generating a word from the vocabulary or directly copying it from the input graph. $P_{\text {vocab }}$ is the probability distribution of directly generating the word, as defined in Equation 5, and $P_{\text {attn }}$ is calculated based on the attention distribution $\alpha_{t}$ by summing the probabilities of the graph nodes that contain identical concept. Intuitively, $\theta_{t}$ is relevant to the current decoder input $e_{t}$ and state $s_{t}$, and the context vector $\mu_{t}$. Therefore, we define it as:</p>
<p>$$
\theta_{t}=\sigma\left(w_{\mu}^{T} \mu_{t}+w_{s}^{T} s_{t}+w_{e}^{T} e_{t}+b_{5}\right)
$$</p>
<p>where vectors $w_{\mu}, w_{s}, w_{e}$ and scalar $b_{5}$ are model parameters. The copy mechanism favors gener-</p>
<p>ating words that appear in the input. For AMR-to-text generation, it facilitates the generation of dates, numbers, and named entities that appear in AMR graphs.</p>
<p>Copying vs anonymization Both copying and anonymization alleviate the data sparsity problem by handling the open-class tokens. However, the copy mechanism has the following advantages over anonymization: (1) anonymization requires significant manual work to define the placeholders and heuristic rules both from subgraphs to placeholders and from placeholders to the surface tokens, (2) the copy mechanism automatically learns what to copy, while anonymization relies on hard rules to cover all types of the open-class tokens, and (3) the copy mechanism is easier to adapt to new domains and languages than anonymization.</p>
<h2>4 Training and decoding</h2>
<p>We train our models using the cross-entropy loss over each gold-standard output sequence $W^{<em>}=$ $w_{1}^{</em>}, \ldots, w_{t}^{<em>}, \ldots, w_{M}^{</em>}$ :</p>
<p>$$
l=\sum_{t=1}^{M} \log p\left(w_{t}^{<em>} \mid w_{t}^{</em>}{ }<em 1="1">{1}, \ldots, w</em>, X ; \theta\right)
$$}^{*</p>
<p>where $X$ is the input graph, and $\theta$ is the model parameters. Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used as the optimizer, and the model that yields the best devset performance is selected to evaluate on the test set. Dropout with rate 0.1 is used during training. Beam search with beam size to 5 is used for decoding. Both training and decoding use Tesla K80 GPUs.</p>
<h2>5 Experiments</h2>
<h3>5.1 Data</h3>
<p>We use a standard AMR corpus (LDC2015E86) as our experimental dataset, which contains 16,833 instances for training, 1368 for development and 1371 for test. Each instance contains a sentence and an AMR graph.</p>
<p>Following Konstas et al. (2017), we supplement the gold data with large-scale automatic data. We take Gigaword as the external data to sample raw sentences, and train our model on both the sampled data and LDC2015E86. We adopt Konstas et al. (2017)'s strategy for sampling sentences from Gigaword, and choose JAMR (Flanigan et al., 2016a) to parse selected sentences into</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2seq</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">35.4 s</td>
</tr>
<tr>
<td style="text-align: left;">Seq2seq+copy</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">37.4 s</td>
</tr>
<tr>
<td style="text-align: left;">Seq2seq+charLSTM+copy</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">39.7 s</td>
</tr>
<tr>
<td style="text-align: left;">Graph2seq</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">11.2 s</td>
</tr>
<tr>
<td style="text-align: left;">Graph2seq+copy</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">11.1 s</td>
</tr>
<tr>
<td style="text-align: left;">Graph2seq+Anon</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">9.2 s</td>
</tr>
<tr>
<td style="text-align: left;">Graph2seq+charLSTM+copy</td>
<td style="text-align: center;">$\mathbf{2 2 . 8}$</td>
<td style="text-align: center;">16.3 s</td>
</tr>
</tbody>
</table>
<p>Table 1: Dev BLEU scores and decoding times.</p>
<p>AMRs, as the AMR parser of Konstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data.</p>
<h3>5.2 Settings</h3>
<p>We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002).</p>
<p>For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or linearized token.</p>
<h3>5.3 Development experiments</h3>
<p>As shown in Table 1, we compare our model with a set of baselines on the AMR devset to demonstrate how the graph encoder and the copy mechanism can be useful when training instances are not sufficient. Seq2seq is the sequence-to-sequence baseline described in Section 2. Seq2seq+copy extends Seq2seq with the copy mechanism, and Seq2seq+charLSTM+copy further extends Seq2seq+copy with character LSTM. Graph2seq is our graph-to-sequence model, Graph2seq+copy extends Graph2seq with the copy mechanism, and Graph2seq+charLSTM+copy further extends</p>
<p>Graph2seq + copy with the character LSTM. We also try Graph2seq + Anon, which applies our graph-to-sequence model on the anonymized data from Konstas et al. (2017).</p>
<p>The graph encoder As can be seen from Table 1, the performance of Graph2seq is 1.6 BLEU points higher than Seq2seq, which shows that our graph encoder is effective when applied alone. Adding the copy mechanism (Graph2seq + copy vs Seq2seq + copy), the gap becomes 2.3. This shows that the graph encoder learns better node representations compared to the sequence encoder, which allows attention and copying to function better.</p>
<p>Applying the graph encoder together with the copy mechanism gives a gain of 3.4 BLEU points over the baseline (Graph2seq + copy vs Seq2seq). The graph encoder is consistently better than the sequence encoder no matter whether character LSTMs are used.</p>
<p>We also list the encoding part of decoding times on the devset, as the decoders of the seq2seq and the graph2seq models are similar, so the time differences reflect efficiencies of the encoders. Our graph encoder gives consistently better efficiency compared with the sequence encoder, showing the advantage of parallelization.</p>
<p>The copy mechanism Table 1 shows that the copy mechanism is effective on both the graph-to-sequence and the sequence-to-sequence models. Anonymization gives comparable overall performance gains on our graph-to-sequence model as the copy mechanism (comparing Graph2seq + Anon with Graph2seq + copy). However, the copy mechanism has several advantages over anonymization as discussed in Section 3.5.</p>
<p>Character LSTM Character LSTM helps to increase the performances of both systems by roughly 0.6 BLEU points. This is largely because it further alleviates the data sparsity problem by handling unseen words, which may share common substrings with in-vocabulary words.</p>
<h3>5.4 Effectiveness on graph state transitions</h3>
<p>We report a set of development experiments for understanding the graph LSTM encoder.</p>
<p>Number of iterations We analyze the influence of the number of state transitions to the model performance on the devset. Figure 3 shows the BLEU scores of different state transition numbers,
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Dev BLEU scores against transition steps for the graph encoder.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Percentage of Dev AMRs with different diameters.
when both incoming and outgoing edges are taken for calculating the next state (as shown in Figure 2). The system is Graph2seq + charLSTM + copy. Executing only 1 iteration results in a poor BLEU score of 14.1. In this case the state for each node only contains information about immediately adjacent nodes. The performance goes up dramatically to 21.5 when increasing the iteration number to 5 . In this case, the state for each node contains information of all nodes within a distance of 5 . The performance further goes up to 22.8 when increasing the iteration number from 5 to 9 , where all nodes with a distance of less than 10 are incorporated in the state for each node.</p>
<p>Graph diameter We analyze the percentage of the AMR graphs in the devset with different graph diameters and show the cumulative distribution in Figure 4. The diameter of an AMR graph is defined as the longest distance between two AMR nodes. ${ }^{1}$ Even though the diameters for less than $80 \%$ of the AMR graphs are less or equal than 10 , our development experiments show that it is not necessary to incorporate the whole-graph information for each node. Further increasing state transition number may lead to additional improvement.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PBMT</td>
<td style="text-align: center;">26.9</td>
</tr>
<tr>
<td style="text-align: left;">SNRG</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: left;">Tree2Str</td>
<td style="text-align: center;">23.0</td>
</tr>
<tr>
<td style="text-align: left;">MSeq2seq+Anon</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: left;">Graph2seq+copy</td>
<td style="text-align: center;">22.7</td>
</tr>
<tr>
<td style="text-align: left;">Graph2seq+charLSTM+copy</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr>
<td style="text-align: left;">MSeq2seq+Anon (200K)</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: left;">MSeq2seq+Anon (2M)</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: left;">Seq2seq+charLSTM+copy (200K)</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: left;">Seq2seq+charLSTM+copy (2M)</td>
<td style="text-align: center;">31.7</td>
</tr>
<tr>
<td style="text-align: left;">Graph2seq+charLSTM+copy (200K)</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: left;">Graph2seq+charLSTM+copy (2M)</td>
<td style="text-align: center;">$\mathbf{3 3 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Test results. "(200K)", "(2M)" and "(20M)" represent training with the corresponding number of additional sentences from Gigaword.</p>
<p>We do not perform exhaustive search for finding the optimal state transition number.</p>
<p>Incoming and outgoing edges As shown in Figure 3, we analyze the efficiency of state transition when only incoming or outgoing edges are used. From the results, we can see that there is a huge drop when state transition is performed only with incoming or outgoing edges. Using edges of one direction, the node states only contain information of ancestors or descendants. On the other hand, node states contain information of ancestors, descendants, and siblings if edges of both directions are used. From the results, we can conclude that not only the ancestors and descendants, but also the siblings are important for modeling the AMR graphs. This is similar to observations on syntactic parsing tasks (McDonald et al., 2005), where sibling features are adopted.</p>
<p>We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014).</p>
<h3>5.5 Results</h3>
<p>Table 2 compares our final results with existing work. MSeq2seq+Anon (Konstas et al., 2017) is an attentional multi-layer sequence-to-sequence
model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-based model for machine translation (Koehn et al., 2003) on the input of linearized AMR graph, $S N R G$ (Song et al., 2017) uses synchronous node replacement grammar for parsing the AMR graph while generating the text, and Tree2Str (Flanigan et al., 2016b) converts AMR graphs into trees by splitting the re-entrances before using a tree transducer to generate the results.</p>
<p>Graph2seq+charLSTM+copy achieves a BLEU score of 23.3 , which is 1.3 points better than MSeq2seq+Anon trained on the same AMR corpus. In addition, our model without character LSTM is still 0.7 BLEU points higher than MSeq2seq+Anon. Note that MSeq2seq+Anon relies on anonymization, which requires additional manual work for defining mapping rules, thus limiting its usability on other languages and domains. The neural models tend to underperform statistical models when trained on limited (16K) gold data, but performs better with scaled silver data (Konstas et al., 2017).</p>
<p>Following Konstas et al. (2017), we also evaluate our model using both the AMR corpus and sampled sentences from Gigaword. Using additional 200 K or 2 M gigaword sentences, Graph2seq+charLSTM+copy achieves BLEU scores of 28.2 and 33.0 , respectively, which are 0.8 and 0.7 BLEU points better than MSeq2seq+Anon using the same amount of data, respectively. The BLEU scores are 5.3 and 10.1 points better than the result when it is only trained with the AMR corpus, respectively. This shows that our model can benefit from scaled data with automatically generated AMR graphs, and it is more effective than MSeq2seq+Anon using the same amount of data. Using 2 M gigaword data, our model is better than all existing methods. Konstas et al. (2017) also experimented with 20M external data, obtaining a BLEU of 33.8 . We did not try this setting due to hardware limitations. The Seq2seq+charLSTM+copy baseline trained on the large-scale data is close to MSeq2seq+Anon using the same amount of training data, yet is much worse than our model.</p>
<h3>5.6 Case study</h3>
<p>We conduct case studies for better understanding the model performances. Table 3 shows example outputs of sequence-to-sequence ( $S 2 S$ ), graph-to-</p>
<p>sequence ( $G 2 S$ ) and graph-to-sequence with copy mechanism ( $G 2 S+C P$ ). Ref denotes the reference output sentence, and Lin shows the serialization results of input AMRs. The best hyperparameter configuration is chosen for each model.</p>
<p>For the first example, $S 2 S$ fails to recognize the concept "a / account" as a noun and loses the concept "o / old" (both are underlined). The fact that "a / account" is a noun is implied by "a / account :mod (o / old)" in the original AMR graph. Though directly connected in the original graph, their distance in the serialization result (the input of $S 2 S$ ) is 26 , which may be why $S 2 S$ makes these mistakes. In contrast, G2S handles "a / account" and "o / old" correctly. In addition, the copy mechanism helps to copy "look-over" from the input, which rarely appears in the training set. In this case, $G 2 S+C P$ is incorrect only on hyphens and literal reference to "anti-japanese war", although the meaning is fully understandable.</p>
<p>For the second case, both $G 2 S$ and $G 2 S+C P$ correctly generate the noun "agreement" for "a / agree" in the input AMR, while $S 2 S$ fails to. The fact that "a / agree" represents a noun can be determined by the original graph segment "p / provide :ARG0 (a / agree)", which indicates that "a / agree" is the subject of "p / provide". In the serialization output, the two nodes are close to each other. Nevertheless, $S 2 S$ still failed to capture this structural relation, which reflects the fact that a sequence encoder is not designed to explicitly model hierarchical information encoded in the serialized graph. In the training instances, serialized nodes that are close to each other can originate from neighboring graph nodes, or distant graph nodes, which prevents the decoder from confidently deciding the correct relation between them. In contrast, $G 2 S$ sends the node "p / provide" simultaneously with relation "ARG0" when calculating hidden states for "a / agree", which facilitates the yielding of "the agreement provides".</p>
<h2>6 Related work</h2>
<p>Among early statistical methods for AMR-to-text generation, Flanigan et al. (2016b) convert input graphs to trees by splitting re-entrances, and then translate the trees into sentences with a tree-tostring transducer. Song et al. (2017) use a synchronous node replacement grammar to parse input AMRs and generate sentences at the same time. Pourdamghani et al. (2016) linearize input</p>
<div class="codehilite"><pre><span></span><code>(p / possible-01 :polarity -
    :ARG1 (1/ look-over-06
        :ARG0 (w / we)
        :ARG1 (a / account-01
            :ARG1 (w2 / war-01
                :ARG1 (c2 / country :wiki &quot;Japan&quot;
                :name (n2 / name :op1 &quot;Japan&quot;))
            :time (p2 / previous)
                :ARG1-of (c / call-01
                :mod (s / so)))
            :mod (o / old))))
</code></pre></div>

<p>Lin: possible :polarity - :arg1 ( look-over :arg0 we :arg1 ( account :arg1 ( war :arg1 ( country :wiki japan :name ( name :op1 japan ) ) :time previous :arg1-of ( call :mod so ) ) :mod old ) )
Ref: we can n't look over the old accounts of the previous so-called anti-japanese war .
S2S: we can n't be able to account the past drawn out of japan 's entire war .
G2S: we can n't be able to do old accounts of the previous and so called japan war.
G2S+CP: we can n't look-over the old accounts of the previous so called war on japan.</p>
<div class="codehilite"><pre><span></span><code><span class="ss">(</span><span class="nv">p</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">provide</span><span class="o">-</span><span class="mi">01</span>
<span class="w">    </span>:<span class="nv">ARG0</span><span class="w"> </span><span class="ss">(</span><span class="nv">a</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">agree</span><span class="o">-</span><span class="mi">01</span><span class="ss">)</span>
<span class="w">    </span>:<span class="nv">ARG1</span><span class="w"> </span><span class="ss">(</span><span class="nv">a2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">and</span>
<span class="w">        </span>:<span class="nv">op1</span><span class="w"> </span><span class="ss">(</span><span class="nv">s</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">staff</span>
<span class="w">            </span>:<span class="nv">prep</span><span class="o">-</span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">c</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">center</span>
<span class="w">                </span>:<span class="nv">mod</span><span class="w"> </span><span class="ss">(</span><span class="nv">r</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">research</span><span class="o">-</span><span class="mi">01</span><span class="ss">)))</span>
<span class="w">            </span>:<span class="nv">op2</span><span class="w"> </span><span class="ss">(</span><span class="nv">f</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">fund</span><span class="o">-</span><span class="mi">01</span>
<span class="w">                </span>:<span class="nv">prep</span><span class="o">-</span><span class="k">for</span><span class="w"> </span><span class="nv">c</span><span class="ss">)))</span>
</code></pre></div>

<p>Lin: provide :arg0 agree :arg1 ( and :op1 ( staff :prep-for ( center :mod research ) ) :op2 ( fund :prep-for center ) )
Ref: the agreement will provide staff and funding for the research center .
S2S: agreed to provide research and institutes in the center .
G2S: the agreement provides the staff of research centers and funding .
G2S+CP: the agreement provides the staff of the research center and the funding .</p>
<p>Table 3: Example system outputs.
graphs by breadth-first traversal, and then use a phrase-based machine translation system ${ }^{2}$ to generate results by translating linearized sequences.</p>
<p>Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to future work. In this work our main goal is to show that graph LSTM encoding of AMR is superior compared with sequence LSTM.</p>
<p>Closest to our work, Peng et al. (2017) modeled syntactic and discourse structures using DAG LSTM, which can be viewed as extensions to tree LSTMs (Tai et al., 2015). The state update follows the sentence order for each node, and has sequential nature. Our state update is in parallel. In addition, Peng et al. (2017) split input graphs into separate DAGs before their method can be used. To our knowledge, we are the first to apply an LSTM structure to encode AMR graphs.</p>
<p>The recurrent information exchange mechanism in our state transition process is remotely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective.</p>
<p>In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation.</p>
<h2>7 Conclusion</h2>
<p>We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance.</p>
<p>Acknowledgments We thank the anonymized reviewers for the insightful comments, and the Center for Integrated Research Computing (CIRC) of University of Rochester for providing computation resources.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR).</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186.</p>
<p>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders for syntax-aware neural machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-17), pages 1957-1967, Copenhagen, Denmark.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016a. CMU at semeval-2016 task 8: Graph-based AMR parsing with infinite ramp loss. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1202-1206, San Diego, California.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016b. Generation from abstract meaning representation using tree transducers. In Proceedings of the 2016 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-16), pages 731-739.</p>
<p>Normunds Gruzitis, Didzis Gosko, and Guntis Barzdins. 2017. RIGOTRIO at SemEval-2017 Task 9: Combining Machine Learning and Grammar Engineering for AMR Parsing and Generation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 924-928, Vancouver, Canada.</p>
<p>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16), pages 1631-1640, Berlin, Germany.</p>
<p>Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16), pages 140-149, Berlin, Germany.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, $9(8): 1735-1780$.</p>
<p>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-based machine translation with hyperedge replacement grammars. In Proceedings of</p>
<p>the International Conference on Computational Linguistics (COLING-12), pages 1359-1376.</p>
<p>Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR).</p>
<p>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), pages 48-54.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17), pages 146-157, Vancouver, Canada.</p>
<p>Gerasimos Lampouras and Andreas Vlachos. 2017. Sheffield at semeval-2017 task 9: Transition-based language generation from amr. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 586-591, Vancouver, Canada.</p>
<p>Xiang Li, Thien Huu Nguyen, Kai Cao, and Ralph Grishman. 2015. Improving event detection with abstract meaning representation. In Proceedings of the First Workshop on Computing News Storylines, pages 11-15, Beijing, China.</p>
<p>Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for semantic role labeling. In Conference on Empirical Methods in Natural Language Processing (EMNLP17), pages 1506-1515, Copenhagen, Denmark.</p>
<p>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 91-98, Ann Arbor, Michigan.</p>
<p>Simon Mille, Roberto Carlini, Alicia Burga, and Leo Wanner. 2017. Forge at semeval-2017 task 9: Deep sentence generation based on a sequence of graph transducers. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval2017), pages 920-923, Vancouver, Canada.</p>
<p>Arindam Mitra and Chitta Baral. 2015. Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning. In Proceedings of the National Conference on Artificial Intelligence (AAAI-16).</p>
<p>Kevin P Murphy, Yair Weiss, and Michael I Jordan. 1999. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 467-475. Morgan Kaufmann Publishers Inc.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pages 311-318.</p>
<p>Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for Computational Linguistics, 5:101-115.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing (EMNLP14), pages 1532-1543.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating English from abstract meaning representations. In International Conference on Natural Language Generation (INLG-16), pages 21-25, Edinburgh, UK.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17), pages 1073-1083, Vancouver, Canada.</p>
<p>Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. AMR-to-text generation with synchronous node replacement grammar. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17), pages 7-13, Vancouver, Canada.</p>
<p>Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, and Daniel Gildea. 2018. Leveraging context information for natural question generation. In Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18), New Orleans.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-15), pages 1556-1566, Beijing, China.</p>
<p>Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, and Masaaki Nagata. 2016. Neural headline generation on abstract meaning representation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-16), pages 1054-1059, Austin, Texas.</p>
<p>Aleš Tamchyna, Chris Quirk, and Michel Galley. 2015. A discriminative model for semantics-to-string translation. In Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation (S2MT 2015), pages 30-36, Beijing, China.</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16), pages 76-85, Berlin, Germany.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ http://www.statmt.org/moses/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
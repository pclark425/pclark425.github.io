<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9208 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9208</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9208</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-258865880</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.15121v6.pdf" target="_blank">Beyond Individual Input for Deep Anomaly Detection on Tabular Data</a></p>
                <p><strong>Paper Abstract:</strong> Anomaly detection is vital in many domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train an NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features to generate an anomaly score. To the best of our knowledge, this is the first work to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. Through extensive experiments on 31 benchmark tabular datasets, we demonstrate that our method achieves state-of-the-art performance, outperforming existing methods by 2.4% and 1.2% in terms of F1-score and AUROC, respectively. Our ablation study further proves that modeling both types of dependencies is crucial for anomaly detection on tabular data.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9208.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9208.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NPT-AD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-Parametric Transformer for Anomaly Detection (NPT-AD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-based anomaly detection method that adapts Non-Parametric Transformers (NPTs) to tabular data by masking features and using NPT's joint feature-feature and sample-sample attention to reconstruct masked entries; anomaly scores are aggregated reconstruction losses computed in a non-parametric inference setup that includes the whole training set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Non-Parametric Transformer (NPT) adapted as NPT-AD</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer with alternating Attention Between Datapoints (ABD) and Attention Between Attributes (ABA); multi-head self-attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tabular data with mixed numerical and categorical features (masked-feature reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>General tabular benchmarks covering multiple domains (e.g., fraud detection, campaign/backdoor datasets, medical, census/benchmark ODDS datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Point anomalies/outliers (samples whose feature values deviate from normal training distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train NPT with stochastic/deterministic masking to reconstruct masked features of normal samples (reconstruction loss: squared error for numeric, cross-entropy for categorical). At inference, assemble validation samples (with masks) together with the full unmasked training set as input to the NPT (non-parametric use of training set) and compute per-mask reconstruction losses; aggregate losses across mask bank to produce an anomaly score (mean or max).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>GOAD, DROCC, NeuTraL-AD, internal contrastive method (Shenkar & Wolf 2022), vanilla Transformer (mask reconstruction), Mask-KNN (KNN imputation-based reconstruction), Isolation Forest, KNN, COPOD, PIDForest, RRCF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1-score, AUROC (reported averaged across datasets and runs), mean rank across datasets, standard deviation over runs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On a benchmark of 31 tabular datasets, NPT-AD achieved state-of-the-art average performance: reported improvement of ~+2.4 percentage points in mean F1-score and ~+1.2 percentage points in mean AUROC versus prior best methods; ablation numbers: NPT-AD mean F1 ≈ 68.8, Transformer mean F1 ≈ 57.4; AUROC (ablation) NPT-AD ≈ 89.8, Transformer ≈ 83.0, Mask-KNN ≈ 84.5 (these values reported in the paper's ablation tables). NPT-AD also shows lower variance across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Consistently outperforms vanilla transformer and KNN-based reconstruction (Mask-KNN) in both F1 and AUROC; ranks best on average vs. deep and classical baselines; ablations show combining feature-feature (ABA) and sample-sample (ABD) attention is crucial — datasets favoring sample-sample or feature-feature dependencies are both improved by NPT-AD.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher computational cost and memory use (non-parametric inference uses training set and mask bank scales with number of features) making it expensive for large d and large n; inference runtime and mask count grow rapidly with d and chosen max masked features r (r increases masks combinatorially); sensitivity to training-set contamination: performance degrades as contamination increases (F1/AUROC stable up to small contamination — paper: F1 remains near max for contamination <2% and AUROC <5%, but degrades markedly beyond), and contamination has a double effect because anomalies can be learned during training and included in non-parametric inference.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Demonstrates that explicitly combining sample-sample and feature-feature attention (via NPT) yields substantial gains for anomaly detection on tabular data; using the training set non-parametrically during inference stabilizes anomaly scores across runs; masked-feature reconstruction (analogue of MLM for tabular data) is an effective self-supervised pretext for AD when both types of dependencies are modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Individual Input for Deep Anomaly Detection on Tabular Data', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9208.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9208.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (vanilla, mask-recon)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Transformer trained for masked-feature reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard transformer (Vaswani et al.) architecture applied to tabular masked-feature reconstruction (no explicit cross-sample attention) used as an ablation/baseline to isolate the contribution of sample-sample attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (Vaswani et al. style) trained for mask reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer with multi-head self-attention applied per-sample (Attention Between Attributes only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tabular data with mixed numerical and categorical features (masked-feature reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Same benchmark domains as NPT-AD (various tabular datasets including fraud, campaign, medical, ODDS datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Point anomalies/outliers</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a standard transformer to reconstruct masked features of each sample using only intra-sample (feature-feature) attention; compute reconstruction loss per mask as anomaly score; does not attend across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>NPT-AD (ABD+ABA), Mask-KNN, GOAD, NeuTraL-AD, DROCC, Isolation Forest, KNN, COPOD, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1-score, AUROC, mean rank</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported ablation/benchmark numbers: mean F1 ≈ 57.4 and mean AUROC ≈ 83.0 in the ablation shown; lower than NPT-AD (F1 ≈ 68.8, AUROC ≈ 89.8). Performance varies by dataset: on some datasets transformer outperforms Mask-KNN, while on others it lags.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Performs worse than NPT-AD overall; in ablations, transformer outperforms Mask-KNN on datasets where feature-feature dependencies dominate, but is outperformed by NPT-AD which combines both dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lacks ability to model sample-sample dependencies so fails on datasets where cross-sample relations are important; still sensitive to masking design and per-feature embeddings. Does not benefit from non-parametric use of training set during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Isolating only feature-feature attention (vanilla transformer) clarifies that sample-sample attention provides complementary and often crucial information for tabular anomaly detection; some datasets are dominated by feature-feature signals where transformer suffices, but combining both yields better robustness across heterogeneous datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Individual Input for Deep Anomaly Detection on Tabular Data', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9208.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9208.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Masked Language Modeling (MLM) / stochastic masking (Devlin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Language Modeling / stochastic masking (as used in BERT and related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Stochastic masking objective (masking input elements with probability p_mask and predicting them from context) is used analogously in this paper as a reconstruction pretext for tabular anomaly detection; the paper links MLM to mutual information maximization for representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-style Masked Language Modeling (concept referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based masked prediction objective</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Originally applied to text (token sequences); here the concept is applied to tabular feature vectors (masked-feature reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Language (originally); applied conceptually to tabular datasets in this work</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Not directly used for anomalies in original BERT; in this work the analogous masked reconstruction is used to detect anomalies as high reconstruction loss</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Paper uses stochastic masking (Bernoulli feature masking) during training — conceptually analogous to MLM — to maximize mutual information between observed and masked parts and train a reconstruction objective; for AD, deterministic mask bank is used at inference to compute reconstruction-based anomaly scores.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported for BERT itself here; the masked-reconstruction approach is evaluated via F1 and AUROC when implemented with Transformer/NPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>The paper does not report results for BERT on AD; it cites the theoretical link (Kong et al. 2020) that stochastic masking maximizes mutual information and motivates the reconstruction objective used by NPT-AD.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>MLM-style masking is the conceptual foundation for the reconstruction objective and is compared empirically in this work when implemented with different model families (vanilla transformer, NPT, Mask-KNN).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper notes that choice of masking probability p_mask and maximum number of simultaneous masked features r crucially affect difficulty of reconstruction and runtime; too-large masks make reconstruction too hard and blur separation between normal and anomalous reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Adapting the MLM/stochastic-masking idea to tabular AD is effective when combined with models that also exploit sample-sample relations; theoretical links to mutual information justify the masking objective as a self-supervised signal for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Individual Input for Deep Anomaly Detection on Tabular Data', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-attention between datapoints: Going beyond individual input-output pairs in deep learning <em>(Rating: 2)</em></li>
                <li>BERT: pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Neural transformation learning for deep anomaly detection beyond images <em>(Rating: 2)</em></li>
                <li>Anomaly detection for tabular data with internal contrastive learning <em>(Rating: 2)</em></li>
                <li>Classification-based anomaly detection for general data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9208",
    "paper_id": "paper-258865880",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "NPT-AD",
            "name_full": "Non-Parametric Transformer for Anomaly Detection (NPT-AD)",
            "brief_description": "A reconstruction-based anomaly detection method that adapts Non-Parametric Transformers (NPTs) to tabular data by masking features and using NPT's joint feature-feature and sample-sample attention to reconstruct masked entries; anomaly scores are aggregated reconstruction losses computed in a non-parametric inference setup that includes the whole training set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Non-Parametric Transformer (NPT) adapted as NPT-AD",
            "model_type": "Transformer with alternating Attention Between Datapoints (ABD) and Attention Between Attributes (ABA); multi-head self-attention",
            "model_size": null,
            "data_type": "Tabular data with mixed numerical and categorical features (masked-feature reconstruction)",
            "data_domain": "General tabular benchmarks covering multiple domains (e.g., fraud detection, campaign/backdoor datasets, medical, census/benchmark ODDS datasets)",
            "anomaly_type": "Point anomalies/outliers (samples whose feature values deviate from normal training distribution)",
            "method_description": "Train NPT with stochastic/deterministic masking to reconstruct masked features of normal samples (reconstruction loss: squared error for numeric, cross-entropy for categorical). At inference, assemble validation samples (with masks) together with the full unmasked training set as input to the NPT (non-parametric use of training set) and compute per-mask reconstruction losses; aggregate losses across mask bank to produce an anomaly score (mean or max).",
            "baseline_methods": "GOAD, DROCC, NeuTraL-AD, internal contrastive method (Shenkar & Wolf 2022), vanilla Transformer (mask reconstruction), Mask-KNN (KNN imputation-based reconstruction), Isolation Forest, KNN, COPOD, PIDForest, RRCF",
            "performance_metrics": "F1-score, AUROC (reported averaged across datasets and runs), mean rank across datasets, standard deviation over runs",
            "performance_results": "On a benchmark of 31 tabular datasets, NPT-AD achieved state-of-the-art average performance: reported improvement of ~+2.4 percentage points in mean F1-score and ~+1.2 percentage points in mean AUROC versus prior best methods; ablation numbers: NPT-AD mean F1 ≈ 68.8, Transformer mean F1 ≈ 57.4; AUROC (ablation) NPT-AD ≈ 89.8, Transformer ≈ 83.0, Mask-KNN ≈ 84.5 (these values reported in the paper's ablation tables). NPT-AD also shows lower variance across runs.",
            "comparison_to_baseline": "Consistently outperforms vanilla transformer and KNN-based reconstruction (Mask-KNN) in both F1 and AUROC; ranks best on average vs. deep and classical baselines; ablations show combining feature-feature (ABA) and sample-sample (ABD) attention is crucial — datasets favoring sample-sample or feature-feature dependencies are both improved by NPT-AD.",
            "limitations_or_failure_cases": "Higher computational cost and memory use (non-parametric inference uses training set and mask bank scales with number of features) making it expensive for large d and large n; inference runtime and mask count grow rapidly with d and chosen max masked features r (r increases masks combinatorially); sensitivity to training-set contamination: performance degrades as contamination increases (F1/AUROC stable up to small contamination — paper: F1 remains near max for contamination &lt;2% and AUROC &lt;5%, but degrades markedly beyond), and contamination has a double effect because anomalies can be learned during training and included in non-parametric inference.",
            "unique_insights": "Demonstrates that explicitly combining sample-sample and feature-feature attention (via NPT) yields substantial gains for anomaly detection on tabular data; using the training set non-parametrically during inference stabilizes anomaly scores across runs; masked-feature reconstruction (analogue of MLM for tabular data) is an effective self-supervised pretext for AD when both types of dependencies are modeled.",
            "uuid": "e9208.0",
            "source_info": {
                "paper_title": "Beyond Individual Input for Deep Anomaly Detection on Tabular Data",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Transformer (vanilla, mask-recon)",
            "name_full": "Vanilla Transformer trained for masked-feature reconstruction",
            "brief_description": "Standard transformer (Vaswani et al.) architecture applied to tabular masked-feature reconstruction (no explicit cross-sample attention) used as an ablation/baseline to isolate the contribution of sample-sample attention.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Transformer (Vaswani et al. style) trained for mask reconstruction",
            "model_type": "Transformer with multi-head self-attention applied per-sample (Attention Between Attributes only)",
            "model_size": null,
            "data_type": "Tabular data with mixed numerical and categorical features (masked-feature reconstruction)",
            "data_domain": "Same benchmark domains as NPT-AD (various tabular datasets including fraud, campaign, medical, ODDS datasets)",
            "anomaly_type": "Point anomalies/outliers",
            "method_description": "Train a standard transformer to reconstruct masked features of each sample using only intra-sample (feature-feature) attention; compute reconstruction loss per mask as anomaly score; does not attend across samples.",
            "baseline_methods": "NPT-AD (ABD+ABA), Mask-KNN, GOAD, NeuTraL-AD, DROCC, Isolation Forest, KNN, COPOD, etc.",
            "performance_metrics": "F1-score, AUROC, mean rank",
            "performance_results": "Reported ablation/benchmark numbers: mean F1 ≈ 57.4 and mean AUROC ≈ 83.0 in the ablation shown; lower than NPT-AD (F1 ≈ 68.8, AUROC ≈ 89.8). Performance varies by dataset: on some datasets transformer outperforms Mask-KNN, while on others it lags.",
            "comparison_to_baseline": "Performs worse than NPT-AD overall; in ablations, transformer outperforms Mask-KNN on datasets where feature-feature dependencies dominate, but is outperformed by NPT-AD which combines both dependencies.",
            "limitations_or_failure_cases": "Lacks ability to model sample-sample dependencies so fails on datasets where cross-sample relations are important; still sensitive to masking design and per-feature embeddings. Does not benefit from non-parametric use of training set during inference.",
            "unique_insights": "Isolating only feature-feature attention (vanilla transformer) clarifies that sample-sample attention provides complementary and often crucial information for tabular anomaly detection; some datasets are dominated by feature-feature signals where transformer suffices, but combining both yields better robustness across heterogeneous datasets.",
            "uuid": "e9208.1",
            "source_info": {
                "paper_title": "Beyond Individual Input for Deep Anomaly Detection on Tabular Data",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Masked Language Modeling (MLM) / stochastic masking (Devlin et al.)",
            "name_full": "Masked Language Modeling / stochastic masking (as used in BERT and related work)",
            "brief_description": "Stochastic masking objective (masking input elements with probability p_mask and predicting them from context) is used analogously in this paper as a reconstruction pretext for tabular anomaly detection; the paper links MLM to mutual information maximization for representation learning.",
            "citation_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "mention",
            "model_name": "BERT-style Masked Language Modeling (concept referenced)",
            "model_type": "Transformer-based masked prediction objective",
            "model_size": null,
            "data_type": "Originally applied to text (token sequences); here the concept is applied to tabular feature vectors (masked-feature reconstruction)",
            "data_domain": "Language (originally); applied conceptually to tabular datasets in this work",
            "anomaly_type": "Not directly used for anomalies in original BERT; in this work the analogous masked reconstruction is used to detect anomalies as high reconstruction loss",
            "method_description": "Paper uses stochastic masking (Bernoulli feature masking) during training — conceptually analogous to MLM — to maximize mutual information between observed and masked parts and train a reconstruction objective; for AD, deterministic mask bank is used at inference to compute reconstruction-based anomaly scores.",
            "baseline_methods": "",
            "performance_metrics": "Not reported for BERT itself here; the masked-reconstruction approach is evaluated via F1 and AUROC when implemented with Transformer/NPT variants.",
            "performance_results": "The paper does not report results for BERT on AD; it cites the theoretical link (Kong et al. 2020) that stochastic masking maximizes mutual information and motivates the reconstruction objective used by NPT-AD.",
            "comparison_to_baseline": "MLM-style masking is the conceptual foundation for the reconstruction objective and is compared empirically in this work when implemented with different model families (vanilla transformer, NPT, Mask-KNN).",
            "limitations_or_failure_cases": "The paper notes that choice of masking probability p_mask and maximum number of simultaneous masked features r crucially affect difficulty of reconstruction and runtime; too-large masks make reconstruction too hard and blur separation between normal and anomalous reconstructions.",
            "unique_insights": "Adapting the MLM/stochastic-masking idea to tabular AD is effective when combined with models that also exploit sample-sample relations; theoretical links to mutual information justify the masking objective as a self-supervised signal for anomaly detection.",
            "uuid": "e9208.2",
            "source_info": {
                "paper_title": "Beyond Individual Input for Deep Anomaly Detection on Tabular Data",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-attention between datapoints: Going beyond individual input-output pairs in deep learning",
            "rating": 2,
            "sanitized_title": "selfattention_between_datapoints_going_beyond_individual_inputoutput_pairs_in_deep_learning"
        },
        {
            "paper_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "rating": 2,
            "sanitized_title": "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding"
        },
        {
            "paper_title": "Neural transformation learning for deep anomaly detection beyond images",
            "rating": 2,
            "sanitized_title": "neural_transformation_learning_for_deep_anomaly_detection_beyond_images"
        },
        {
            "paper_title": "Anomaly detection for tabular data with internal contrastive learning",
            "rating": 2,
            "sanitized_title": "anomaly_detection_for_tabular_data_with_internal_contrastive_learning"
        },
        {
            "paper_title": "Classification-based anomaly detection for general data",
            "rating": 1,
            "sanitized_title": "classificationbased_anomaly_detection_for_general_data"
        }
    ],
    "cost": 0.014306,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Individual Input for Deep Anomaly Detection on Tabular Data
2 May 2024</p>
<p>Hugo Thimonier 
Laboratoire Interdisciplinaire des Sciences du Numérique
Université Paris-Saclay
CNRS, Centrale-Supélec
91190Gif-sur-YvetteFrance</p>
<p>Fabrice Popineau 
Laboratoire Interdisciplinaire des Sciences du Numérique
Université Paris-Saclay
CNRS, Centrale-Supélec
91190Gif-sur-YvetteFrance</p>
<p>Arpad Rimmel 
Laboratoire Interdisciplinaire des Sciences du Numérique
Université Paris-Saclay
CNRS, Centrale-Supélec
91190Gif-sur-YvetteFrance</p>
<p>Bich-Liên Doân 
Laboratoire Interdisciplinaire des Sciences du Numérique
Université Paris-Saclay
CNRS, Centrale-Supélec
91190Gif-sur-YvetteFrance</p>
<p>Hugo Thi 
Laboratoire Interdisciplinaire des Sciences du Numérique
Université Paris-Saclay
CNRS, Centrale-Supélec
91190Gif-sur-YvetteFrance</p>
<p>Beyond Individual Input for Deep Anomaly Detection on Tabular Data
2 May 202410F6E79B9AF10E958604127F18B1B28BarXiv:2305.15121v6[cs.LG]
Anomaly detection is vital in many domains, such as finance, healthcare, and cybersecurity.In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies.In a reconstruction-based framework, we train an NPT to reconstruct masked features of normal samples.In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features to generate an anomaly score.To the best of our knowledge, this is the first work to successfully combine feature-feature and samplesample dependencies for anomaly detection on tabular datasets.Through extensive experiments on 31 benchmark tabular datasets, we demonstrate that our method achieves state-of-the-art performance, outperforming existing methods by 2.4% and 1.2% in terms of F1-score and AUROC, respectively.Our ablation study further proves that modeling both types of dependencies is crucial for anomaly detection on tabular data.</p>
<p>Introduction</p>
<p>Anomaly detection is a critical task that aims to identify samples that deviate from a pre-defined notion of normality within a dataset.Traditional approaches to anomaly detection characterize the normal 1 distribution almost exclusively using samples considered as normal, and flag data points as anomalies based on their deviation from this distribution.Anomaly detection (AD) is especially useful for applications involving imbalanced datasets, where standard supervised methods may fail to achieve satisfactory performance (Yanmin et al., 2011).Those applications include fraud detection (Hilal et al., 2022), intrusion detection in cybersecurity (Malaiya et al., 2018) or astronomy (Reyes &amp; Estévez, 2020).</p>
<p>Anomaly detection encompasses both unsupervised and semi-supervised methods.In most real-world scenarios, labeled datasets that differentiate normal samples from anomalies are unavailable or costly to obtain.To address this, efficient anomaly detection methods must be robust to dataset contamination, where the training set is predominantly composed of normal samples but also includes anomalies.However, when labeled data is available, one can consider a semi-supervised approach to create a training set consisting solely of normal samples, thereby indirectly incorporating label information into the anomaly detection model.</p>
<p>Many general AD methods tend to work well on tasks that involve unstructured data (e.g., natural language processing or computer vision) such as (Schölkopf et al., 1999;Tax &amp; Duin, 2004;Liu et al., 2008;Ruff et al., 2018;Kim et al., 2020;Liznerski et al., 2021).However, recent work (Bergman &amp; Hoshen, 2020;Qiu et al., 2021;Shenkar &amp; Wolf, 2022) has revealed that the best-performing methods for tabular data involve models tailored to consider the particular structure of this data type.AD methods for structured data typically identify anomalies by using either feature-feature or sample-sample dependencies.For instance, in (Shenkar &amp; Wolf, 2022), the authors assume a class-dependent relationship between a subset of variables in a sample's feature vector and the rest of its variables.The authors thus propose a contrastive learning framework to detect anomalies based on this assumption.Another recent method (Thimonier et al., 2022) identifies anomalies in tabular datasets by focusing on sample-sample dependencies by measuring the influence of training normal samples on the validation samples.Both approaches have demonstrated competitive results for anomaly detection in tabular datasets.</p>
<p>Recent work on supervised deep learning methods for tabular data (Shavitt &amp; Segal, 2018;Arik &amp; Pfister, 2021;Gorishniy et al., 2023;Somepalli et al., 2021;Kossen et al., 2021) has also highlighted the importance of considering the particular structure of tabular data.In particular, in (Kossen et al., 2021;Somepalli et al., 2021;Gorishniy et al., 2023), the authors emphasize the significance of considering both feature-feature and sample-sample dependencies for supervised regression and classification problems on tabular data.Based on the latter observation, we hypothesize that feature-feature relations and sample-sample dependencies are class-dependent, and both dependencies should be used conjointly to identify anomalies.In particular, since interactions between samples are learned exclusively using normal samples in the anomaly detection setup, they should be especially discriminative in identifying anomalies during inference.</p>
<p>To test this hypothesis, we employ Non-Parametric Transformers (NPT) (Kossen et al., 2021), first proposed for supervised tasks on tabular datasets.We show that NPTs are particularly relevant for flagging anomalies, in line with recent work (Han et al., 2022) demonstrating the effectiveness of new deep learning architectures for anomaly detection on tabular data.We experiment on an extensive benchmark of tabular datasets to demonstrate the capacity of our approach to detect anomalies and compare our performances to existing AD methods.We obtain state-of-the-art results when it comes to detection accuracy.We also test the robustness of our approach to dataset contamination and give evidence that it can serve for unsupervised anomaly detection when the training set contamination is not too severe.Finally, our ablation study, conducted with reconstructionbased approaches similar to our proposed method but utilizing K-nearest neighbors (KNN) imputation and a vanilla transformer (Vaswani et al., 2017), provides evidence that considering both types of dependencies can be crucial to detect anomalies on specific datasets accurately.</p>
<p>The present work offers the following contributions:</p>
<p>• We propose the first AD method for tabular data relying on masked feature reconstruction.</p>
<p>• We put forward the first deep AD method to successfully combine feature-feature and sample-sample dependencies.</p>
<p>• Our method shows state-of-the-art anomaly detection capacity on an extensive benchmark of 31 tabular datasets.</p>
<p>• We provide strong evidence of the crucial role of considering both dependencies for anomaly detection on tabular data.</p>
<p>Related works</p>
<p>Anomaly detection approaches can be categorized into four main types: density estimation, one-class classification, reconstruction-based, and self-supervised.</p>
<p>Density Estimation</p>
<p>The most straightforward approach to detecting samples that do not belong to a distribution is to estimate the distribution directly and to measure the likelihood of a sample under the estimated distribution.Several approaches found in the literature have considered using non-parametric density estimation methods to estimate the density of the normal distribution, such as KDE (Parzen, 1962), GMM (Roberts &amp; Tarassenko, 1994), or Copula as in COPOD (Li et al., 2020).Other approaches also focused on local density estimation to detect outliers, such as Local Outlier Factor (LOF) (Breunig et al., 2000).In inference, one flags the samples that lie in low-probability regions under the estimated distribution as anomalies.</p>
<p>Reconstruction Based Methods Other methods have consisted in learning to reconstruct samples that belong to the normal distribution.In this framework, the models' incapacity to reconstruct a sample correctly serves as a proxy to measure anomaly.A high reconstruction error would indicate that a sample does not belong to the estimated normal distribution.Those approaches can involve PCA (Hawkins, 1974) or neural networks such as diverse types of autoencoders (Principi et al., 2017;Chen &amp; Konukoglu, 2018;Kim et al., 2020), or GANs (Schlegl et al., 2017).</p>
<p>One-Class Classification</p>
<p>The term one-class classification was coined in (Moya &amp; Hush, 1996) and describes identifying anomalies without directly estimating the normal density.One-class classification (OCC) involves discriminative models that directly estimate a decision boundary.</p>
<p>For instance, in kernel-based approaches (Schölkopf et al., 1999;Tax &amp; Duin, 2004), authors propose to characterize the support of the normal samples in a Hilbert space and to flag as anomalies the samples that would lie outside of the estimated support.Similarly, recent work has extended their approach by replacing kernels with deep neural networks (Ruff et al., 2018).In (Goyal et al., 2020), authors proposed DROCC that involves generating, in the course of training, synthetic anomalous samples in order to learn a classifier on top of the one-class representation.Other OCC approaches have relied on tree-based models such as isolation forest (IForest) (Liu et al., 2008), extended isolation forest (Hariri et al., 2021), RRCF (Guha et al., 2016) and PIDForest (Gopalan et al., 2019).</p>
<p>Self-Supervised Approaches Recent methods have also considered self-supervision as a means to identify anomalies.In GOAD (Bergman &amp; Hoshen, 2020), authors apply several affine transformations to each sample and train a classifier to identify from the transformed samples which transformation was applied.The classifier only learns to discriminate between transformations using normal transformed samples: assuming this problem is class-dependent, the classifier should fail to identify transformation applied to anomalies.In NeuTraL-AD (Qiu et al., 2021), authors propose a contrastive framework in which samples are transformed using neural mappings and are embedded in a latent semantic space using an encoder.The objective is to learn transformations so that transformed samples still share similarities with their untransformed counterpart while different transformations are easily distinguishable.The contrastive loss then serves as the anomaly score in inference.Similarly, (Shenkar &amp; Wolf, 2022) also propose a contrastive framework in which they identify samples as anomalies based on their inter-feature relations.Other self-supervised approaches, such as (Sohn et al., 2021;Reiss &amp; Hoshen, 2023), have focused on representation learning to foster the performance of one-class classification models.</p>
<p>Attention Mechanisms First introduced in (Vaswani et al., 2017), the concept of attention has become ubiquitous in the machine learning literature.Scholars have successfully applied transformers on a broad range of tasks, including computer vision, e.g.image generation with the Image Transformer (Parmar et al., 2018) or image classification with the Vision Transformer (ViT) (Dosovitskiy et al., 2021), natural language processing e.g.Masked Language Models (MLM) such as BERT (Devlin et al., 2019), and classification tasks on structured datasets (Somepalli et al., 2021;Kossen et al., 2021).</p>
<p>Deep Learning for Tabular Data Despite the effectiveness of deep learning models for numerous tasks involving unstructured data, non-deep models remain the prevalent choice for machine learning tasks such as classification and regression on tabular data (Grinsztajn et al., 2022;Shwartz-Ziv &amp; Armon, 2021).However, in recent years, scholars have shown that one could successfully resort to deep learning methods for various tasks on tabular datasets.For instance, in (Shavitt &amp; Segal, 2018;Jeffares et al., 2023), authors discuss how regularization is crucial in training a deep learning model tailored for tabular data.Hence, they propose a new regularization loss to accommodate the variability between features.Similarly, (Kadra et al., 2021) shows that correctly selecting a combination of regularization techniques can suffice for a Multi-Layer Perceptron (MLP) to compete with GBDT.Finally, (Somepalli et al., 2021;Kossen et al., 2021) propose deep learning models based on attention mechanisms that rely on feature-feature, feature-label, sample-sample, and sample-label attention.</p>
<p>Both models achieve competitive results on several baseline datasets and emphasize sample-sample interaction's role in classifying samples correctly.</p>
<p>Method</p>
<p>In this section, we discuss the learning objective used to optimize the parameters of our model, then we briefly present the mechanisms involved in Non-Parametric Transformers (Kossen et al., 2021), and finally, we present NPT-AD, our method to derive an anomaly score.</p>
<p>Learning Objective</p>
<p>Reconstruction
(x)),(1)
where d(x, ϕ θ (x)) measures how well the model reconstructs sample x.The latter is often set to be a distance measure such as the Euclidean distance.</p>
<p>The AD method proposed in (Shenkar &amp; Wolf, 2022) employs a masking strategy that maximizes the mutual information between each sample and its masked-out part by minimizing a contrastive loss.Recently, (Kong et al., 2020) demonstrated how stochastic masking (Devlin et al., 2019) also maximizes mutual information, thereby establishing a link between the method of (Shenkar &amp; Wolf, 2022) and stochastic masking.In stochastic masking, each entry in a sample vector x ∈ R d is masked with probability p mask , and the objective task is to predict the masked-out features from the unmasked features.Formally, let m ∈ R d be a binary vector taking value 1 when the corresponding entry in x is masked, 0 otherwise.Let x m , x o ∈ R d represent respectively the masked and unmasked entries of sample x defined as
x m = m ⊙ x x o = (1 d − m) ⊙ x,(2)
where 1 d is the d-dimensional unit vector.</p>
<p>In this framework, the objective in eq. 1 is modified to
min θ∈Θ x∈Dtrain d(x m , ϕ θ (x o )),(3)
where ϕ θ (x o ) denotes the reconstructed masked features of sample x by the model.</p>
<p>Our proposed approach leverages the entire dataset in a nonparametric manner to reconstruct masked features.This method considers feature-feature interactions and also captures relationships between samples to optimize the recon- In step (a), mask j is applied to each validation sample.We construct a matrix X composed of the masked validation samples and the whole unmasked training set.In step (b), we feed X to the Non-Parametric Transformer (NPT), which tries to reconstruct the masked features for each validation sample.On top of the learned feature-feature interactions, NPT will use the unmasked training samples to reconstruct the mask features.In step (c), we compute the reconstruction error that we later aggregate in the NPT-AD score.struction objective.Let X ∈ R n×d denote the dataset matrix, consisting of n training samples with d features.We introduce the matrix equivalents of m, x m , and x o , denoted as M, X M , and X O , respectively, all in R n×d .The reconstruction objective described in eq. 3 can then be reformulated as
min θ∈Θ x∈Dtrain d x m , ϕ θ x o | X O .(4)</p>
<p>Non-parametric transformer (NPT)</p>
<p>We resort to Non-Parametric Transformer (NPT) (Kossen et al., 2021) as the core model for our approach, denoted as ϕ θ in section 3.1.NPT involves both attention between features and attention between samples, thus allowing the ability to capture feature-feature and sample-sample dependencies.More precisely, two mechanisms involved in NPTs allow anomalies to be identified: Attention Between Datapoints (ABD) and Attention Between Attributes (ABA).</p>
<p>Both attention mechanisms rely on multi-head self-attention (MHSA), which was first introduced in the natural-language processing literature (Bahdanau et al., 2014;Devlin et al., 2019;Vaswani et al., 2017).We discuss MHSA more thoroughly in appendix A and only detail in this section the two mechanisms put forward in (Kossen et al., 2021).</p>
<p>As an input, NPT receives both the dataset and a masking matrix (X, M) ∈ R n×d × R n×d .Before feeding the input to the NPT, we pass each of the n data samples through a linear embedding layer to obtain an e-dimensional embedding for each feature.Thus, as an input, NPT receives a representation H 0 ∈ R n×d×e .A sequence of MHSA layers is applied to the input, alternating between ABA and ABD.</p>
<p>The model then outputs a prediction for masked features while keeping unmasked features unchanged X ∈ R n×d .</p>
<p>Attention Between Datapoints (ABD) It is the key feature that differentiates NPT from standard transformer models.This mechanism captures pairwise relations between data samples.Consider as an input to the ABD layer the previous layer representation
H (ℓ) ∈ R n×d×e flattened to R n×h where h = d • e.
Then, NPT applies MHSA, as seen in equation 13 in appendix A, between the data samples flattened representations
{H (ℓ) i ∈ R 1×h |i ∈ 1, . . . , n}. ABD(H (ℓ) ) = MHSA(H (ℓ) ) = H (ℓ+1) ∈ R n×h (5)
After applying ABD, the data representation is reshaped to its original dimension in R n×d×e .</p>
<p>Attention Between Attributes (ABA) As already discussed, NPT alternates between ABD and ABA layers.ABA layers should help learn per data sample representation for the inter-sample representations.In contrast with ABD, ABA consists in applying MHSA independently to each row in
H (ℓ) , i.e. to each data sample's intermediate representa- tion H (ℓ) i ∈ R d×e , i ∈ {1, . . . , n}. ABA(H (ℓ) ) = stack axis=n MHSA(H (ℓ) 1 ), . . . , MHSA(H (ℓ) n ) (6)</p>
<p>Anomaly score</p>
<p>We directly derive the anomaly score from the loss optimized during training.The loss corresponds to the squared difference between the reconstructed feature and its actual value for numerical features.Meanwhile, for categorical features, we use the cross-entropy loss function.The anomaly score relies on our model's capacity to reconstruct masked features correctly and assumes that the model should better reconstruct normal samples.Two reasons support this assumption.First, relations between features are classdependent , having observed only normal samples in the training phase, the model should be unable to fetch the learned feature-feature interactions to reconstruct anomalies properly.Second, sample-sample interactions seen by the model only correspond to interactions between normal samples, making it difficult to successfully exploit interactions between normal samples and anomalies.</p>
<p>As detailed in Figure 1, we consider a mask bank composed of m d-dimensional deterministic mask vectors that designate which of the d features of each validation sample will be hidden.We set the maximum number of features to be masked simultaneously r, and construct m = r k=1 d k masks.Each mask is applied to each validation sample z ∈ D val to obtain m different masked samples {z (1) , . . ., z (m) } of the original sample z.We use the whole unmasked training set2 D train to predict the masked features of each sample for each of the m masked vectors and construct the anomaly score for a validation sample z as
NPT-AD(z; D train ) = 1 m m k=1 L(z (k) ; D train ),(7)
where L(z (k) ; D train ) designates the loss for the sample z with mask k.We also considered other forms of aggregation, such as the maximum loss over all masks.</p>
<p>Experiments</p>
<p>Datasets We experiment on an extensive benchmark of tabular datasets following previous work (Shenkar &amp; Wolf, 2022).The benchmark is comprised of two datasets widely used in the anomaly detection literature, namely Arrhythmia and Thyroid, a second group of datasets, the "Multidimensional point datasets", obtained from the Outlier Detection DataSets (ODDS)3 containing 28 datasets.We omit the datasets Heart and Yeast following the literature and also omit the KDD dataset since it presents a certain number of limitations (Silva et al., 2020).Instead, we include three real-world datasets from (Han et al., 2022) that display relatively similar characteristics to KDD in terms of dimensions: fraud, campaign, and backdoor.See appendix C for more detail on the datasets' characteristics.</p>
<p>Experimental settings Per the literature (Zong et al., 2018;Bergman &amp; Hoshen, 2020), we construct the training set with a random subsample of the normal samples representing 50% of the normal samples, we concatenate the 50% remaining with the entire set of anomalies to constitute the validation set.Following previous work, (Bergman &amp; Hoshen, 2020), the decision threshold for the NPT-AD score is chosen such that the number of predicted anomalies is equal to the number of existing anomalies.We report the results in tables 4, 5 6 and 7 in appendix D.1.Most metrics are obtained from (Shenkar &amp; Wolf, 2022), apart from NeuTraL-AD (Qiu et al., 2021), which we trained using their official code made available online, the transformer model, and the experiments on the fraud, campaign, and backdoor datasets.</p>
<p>Per the literature, we evaluate the different methods using the F1-score (↑) and AUROC (↑) metrics.We compare our method to recent deep methods, namely GOAD (Bergman &amp; Hoshen, 2020), DROCC (Goyal et al., 2020), NeuTraL-AD (Qiu et al., 2021), the contrastive approach proposed in (Shenkar &amp; Wolf, 2022) and a transformer model (Vaswani et al., 2017) trained in a similar way to NPT-AD.We also compare to classical non-deep methods such as Isolation Forest (Liu et al., 2008), KNN (Ramaswamy et al., 2000), RRCF (Guha et al., 2016), COPOD (Li et al., 2020) and PIDForest (Gopalan et al., 2019).We refer the reader to (Shenkar &amp; Wolf, 2022) for implementation details of nondeep models.</p>
<p>Notice that for DROCC (Goyal et al., 2020), GOAD (Bergman &amp; Hoshen, 2020), and NeuTraL-AD (Qiu et al., 2021), we report in tables 4 and 5 the architecture that obtained the highest mean F1-score.The metrics obtained for the other architectures are detailed in tables 8, 9, and 10 in appendix D.1.We included each architecture of each approach to compute the mean rank as shown in 2. Following the literature, we report the average metrics over 20 runs.</p>
<p>Our model was trained for each dataset on 4 or 8 Nvidia GPUs V100 16Go/32Go, depending on the dataset dimension.Note that the model can also be trained on a single GPU for small and medium datasets.</p>
<p>For each dataset, we considered the same NPT architecture composed of 4 layers alternating between Attention Between Datapoints and Attention Between Attributes and 4 attention heads.Per (Kossen et al., 2021), we consider a Rowwise feed-forward (rFF) network with one hidden layer, 4x expansion factor, GeLU activation, and also include dropout with p = 0.1 for both attention weights and hidden layers.We used LAMB (You et al., 2020) with β = (0.9, 0.999) as the optimizer and also included a Lookahead (Zhang et al., 2019) wrapper with slow update rate α = 0.5 and k = 6 steps between updates.Similarly, following (Kossen et al., 2021), we consider a flat-then-anneal learning rate schedule: flat at the base learning rate for 70% of steps and then anneals following a cosine schedule to 0 by the end of the training phase, and set gradient clipping at 1.We chose r in accordance with the masking probability p mask used during training and the total number of features d.We hypothesized that a too-high value of r for a low p mask would pollute the anomaly score with reconstructions too challenging for the model, leading to high reconstruction error for both normal samples and anomalies.Moreover, the hardest reconstructions, i.e. those with a high number of masked features, would constitute a too high share of the total masks.Indeed, for a fixed d, d k as a function of k is non-decreasing for k ≤ d/2 and has an exponential growth rate.Furthermore, raising the value of the parameter r can lead to a substantial augmentation in the number of masks m, consequently inducing a significant upsurge in the inference runtime.</p>
<p>We detail in appendix C the varying hyperparameters used for each dataset in our experiments.Notice that for most datasets, the hyperparameters remain unchanged.Variations of the hyperparameters are motivated by a swifter convergence of the training loss or computational costs for larger datasets.Each experiment can be replicated using the code made available on GitHub 4 .</p>
<p>Results</p>
<p>As seen in figure 2 and tables 4 and 5, our model surpasses existing methods on most datasets by a significant margin regarding the F1-score.Moreover, our approach displays the highest mean F1-score and lowest mean rank over all datasets out of the 18 tested approaches.The method of Shenkar &amp; Wolf (2022) ranks as the second highest in terms of average F1-score and the second highest mean rank over all datasets, while the simple KNN-based method (Ramaswamy et al., 2000) ranks as the third best approach in terms of average F1-score and rank.Also, our approach displays a smaller variance than competing methods except for COPOD (Li et al., 2020), which performs significantly worse than our approach regarding the F1-score and AU-ROC.The smaller variance could originate from the fact that 4 https://github.com/hugothimonier/NPT-AD/our model uses, in a non-parametric fashion, the training set in the inference phase.This contributes to flattening the variations in the anomaly score attributed to discrepancies in the model's weights between runs.We also display in tables 6 and 7 in appendix D.1 the AUROC for the same experiments and observe that we obtain the highest mean AUROC and the lowest mean rank while also displaying a smaller variance than other tested approaches.</p>
<p>Discussion</p>
<p>Training set contamination</p>
<p>Real-life anomaly detection applications often involve contaminated training sets; anomaly detection models must, therefore, be robust to small levels of dataset contamination.We experimented using a synthetic dataset to evaluate how much NPT-AD suffers from dataset contamination compared to recent deep AD methods.We constructed a synthetic dataset using two perfectly separable distributions for normal and anomaly samples.Our training set contained 900 normal samples, and we kept aside 100 anomaly samples that we could add to the training set.We considered 11 different training sets with contamination shares ranging from 0% to 10% with a 1% step while keeping the validation set constant with a fixed composition of 10% anomalies and 90% normal samples.We display the results of this experiment in Figure 3 in which we show how the performance of NPT-AD varies when the contamination share increases in comparison with a vanilla transformer (Vaswani et al., 2017) for mask reconstruction, NeuTraL-AD (Qiu et al., 2021), GOAD (Bergman &amp; Hoshen, 2020) and the internal contrastive approach of (Shenkar &amp; Wolf, 2022).Our experimental results show that, as expected, the performance of NPT-AD deteriorates as the proportion of anomalies in the training set increases.For contamination shares lower than 2% (resp.5%), the F1-score (resp.AUROC) remains close to its maximum value of 100%.However, the F1-score and AUROC deteriorate significantly for higher contamination levels while displaying a higher standard deviation.When anomalies constitute 10% of the training set, our approach achieves an average F1-score slightly lower than 50% and an average AUROC of 87%.We observe that NPT-AD suffers less from dataset contamination than (Shenkar &amp; Wolf, 2022) for both F1-score and AUROC.We also notice that the method of Shenkar &amp; Wolf ( 2022) is particularly sensible to dataset contamination regarding the F1-score compared with NeuTraL-AD, GOAD, the transformer approach, and NPT-AD even for low contamination shares.</p>
<p>Finally, we observe that the transformer approach suffers significantly less from training set contamination than NPT-AD, especially for high contamination shares.What could account for such a difference is the fact that the contamination effect is double for NPT-AD.First, during training, the model acquires the ability to reconstruct not only normal samples but also anomalies, thereby compromising the reconstruction error as a good proxy for anomalousness.Second, in inference, the model relies on the unmasked training set to reconstruct samples: normal samples may attend to anomaly samples, which may hamper the ability of the model to reconstruct them properly.Conversely, anomalies can attend to other anomalies, which helps reduce the reconstruction error for this class.</p>
<p>Sample-sample dependencies ablation study</p>
<p>Limitations and Conclusion</p>
<p>Limitations As with most non-parametric models, NPT-AD displays a higher complexity than parametric approaches.NPT-AD can scale well for datasets with a reasonable number of features d; however, for large values of d, our approach involves a high computational cost in terms of memory and time.This cost originates from the complexity of NPT itself and how the anomaly score is derived.In table 14 in appendix D.3, we observe that NPT-AD displays longer runtime for datasets with large values of d when n is also high, e.g.Mnist or backdoor.Two factors can account for this.First, the number of reconstructions highly depends on d, which increases the inference runtime; secondly, due to the feature embeddings, the dimension of the model also increases rapidly with d.</p>
<p>Conclusion</p>
<p>In this work, we have proposed a novel deep anomaly detection method designed explicitly for tabular datasets.To the best of our knowledge, our approach is the first to successfully utilize both feature-feature and samplesample dependencies to identify anomalies.Our experiments on an extensive benchmark demonstrate the effectiveness of our approach, outperforming existing state-of-the-art methods in terms of F1-score and AUROC.Our experiments further demonstrate the robustness of our method to a small training set contamination.Finally, this work emphasizes the importance of leveraging both dependencies to effectively detect anomalies on tabular datasets.</p>
<p>Future Work Our work invites further research on the optimal way to combine sample-sample and feature-feature dependencies for anomaly detection on tabular data.In the current work, we relied on NPTs, which combine both dependencies internally.However, this restrains pretext tasks used for self-supervised AD to tasks that NPTs can perform.Other forms of external retrieval methods may be interesting to investigate to be able to augment existing AD methods.</p>
<p>A. Multi-Head Self-Attention</p>
<p>Scaled dot-product attention as first proposed in (Vaswani et al., 2017) describes a mapping between queries
Q i ∈ R 1×h k , keys K i ∈ R 1×h k and values V i ∈ R 1×hv
to an output.The output is computed as a weighted sum of the values, where each weight is obtained by measuring the compatibility between queries and keys.Take Q ∈ R n×h k , K ∈ R m×h k and V ∈ R m×hv the corresponding matrices in which queries, keys, and values are stacked.Scaled dot-product attention is computed as
Attention(Q, K, V) = softmax QK ⊤ √ h k V (8)
where, for convenience, one often sets
h k = h v = h.
To foster the ability of a model to produce diverse and powerful representations of data samples, one often includes several dot-product attention mechanisms.Multi-head dot-product attention then describes the concatenation of k independent attention heads:
MultiHead(Q, K, V) = concat axis=k (O 1 , . . . , O k )W O (9)
where,
O j = Attention(QW Q j , KW K j , VW V j )(10)
where the embedding matrices W Q j , W K j , W V j ∈ R h×h/k are learned for each attention head j ∈ {1, . . ., k} and W O ∈ R h×h serves to mix the h attention heads outputs.NPTs only include multi-head self -attention mechanisms which consist in multi-head dot-product attention where queries, keys, and values are identical:
MHSelfAtt(H) = MultiHead(Q = H, K = H, V = H)(11)
As described in (Kossen et al., 2021), NPT follows transformer best practices to improve performances and involves a residual branch as well as layer normalization (LN) (Ba et al., 2016) before MHSelfAtt(.).</p>
<p>Res(H) = HW res + MHSelfAtt (LN(H))</p>
<p>where W res ∈ R h×h are learned weights.Layer normalization is also added after the residual branch as well as a row-wised feed-forward network (rFF):
MHSA(H) = Res(H) + rFF (LN (Res (H))) ∈ R n×h (13)</p>
<p>B. Training Pipeline</p>
<p>Let x ∈ X ⊆ R d be a sample with d features, which can be either numerical or categorical.Let e designate the hidden dimension of the transformer.The training pipeline consists of the following steps:</p>
<p>Masking We sample from a Bernouilli distribution with probability p mask whether each of the d features is masked.
mask = (m 1 , . . . , m d ),
where m j ∼ B(1, p mask ) ∀j ∈ [1, ..., d] and m j = 1 if feature j is masked.</p>
<p>Encoding For numerical features, we normalize to obtain 0 mean and unit variance, while we use one-hot encoding for categorical features.At this point, each feature j for j ∈ [1, 2, ..., d] has an e j -dimensional representation, encoded(x j ) ∈ R ej , where e j = 1 for numerical features and for categorical features e j corresponds to its cardinality.We then mask each feature according to the sampled mask vector and concatenate each feature representation with the corresponding mask indicator function.Hence, each feature j has an (e j + 1)-dimensional representation
((1 − m j ) • encoded(x j ), m j ) ∈ R ej +1 ,
where x j is the j-th features of sample x.</p>
<p>In-Embedding We pass each of the features encoded representations of sample x through learned linear layers</p>
<p>Linear(e j + 1, e).We also learn e-dimensional index and feature-type embeddings as proposed in (Kossen et al., 2021).Both are added to the embedded representation of sample x.The obtained embedded representation is thus of dimension d × e e x = (e 1</p>
<p>x , e 2 x , . . ., e d x ) ∈ R d×e , and e j</p>
<p>x ∈ R e is the embedded representation of feature j for sample x.</p>
<p>NPT Encoder We concatenate the embedded representations of n samples and construct a matrix X, X = (e x1 , . . ., e xn ) ⊤ ∈ R n×d×e that is fed to the NPT.Let H ∈ R n×d×e denote the output of the NPT,
H = NPT(X) ∈ R n×d×e
Out-Embedding The output of the NPT, H ∈ R n×d×e is then used to compute an estimation of the original d-dimensional vector for each of the n samples.To obtain the estimated feature j for the n samples, we take the j-th n × e-dimensional representation which is output by the NPT encoder and pass it through a learned linear layer Linear(e, e j ), where e j is 1 for numerical features and the cardinality for categorical features.In other words, for each element j in the second dimension of H, H :,j: , we compute Z = H :,j:
W j out ∈ R n×ej ,
where W j out ∈ R e×ej , where e j = 1 for numerical features and for categorical features e j corresponds to its cardinality.For categorical feature, we take the arg max along the second dimension of Z.</p>
<p>C. Datasets characteristics and experimental settings</p>
<p>In table 2, we display the main characteristics of the datasets involved in our experiments, while in 3 we display the hyperparameters of our experiments.Hyperparameter selection was done so as to obtain the fastest training loss convergence.The pipeline consisted of the following:</p>
<p>• Hidden dimension e: We started from smaller to larger models to achieve convergence.</p>
<p>• Batch size: We maximized batch size to fit into memory.Larger batch sizes are beneficial to our approach since a larger number of samples in the same batch increases the samples to which each sample can attend and fosters better learning of sample-sample dependencies.</p>
<p>• Learning rate (lr) was selected to achieve the fastest loss convergence for each architecture.</p>
<p>• Masking probability p mask : we started from 0.25 and reduced with a step 0.05 until loss convergence.</p>
<p>• Maximum number of features masked simultaneously r: it was chosen in accordance with the chosen p mask and the overall number of feature d.We also considered inference time when setting r as setting r higher than 1 for a dataset with many features would make inference time explode.Take the Speech dataset with d = 400; going from r = 1 to r = 2 would make the number of masks go from 400 to 82200.</p>
<p>D. Additional experiments D.1. Additional results</p>
<p>In this section, we display the metrics for each of the experiments we performed.This includes the F1-score for all tested approaches in tables 4 and 5, the AUROC for the approaches for which it is relevant to compute it; displayed in tables 6 and 7, the F1-score for each architecture discussed in the original papers of NeuTraL-AD (Qiu et al., 2021) in table 10, GOAD (Bergman &amp; Hoshen, 2020) in table 9, and DROCC (Goyal et al., 2020) in table 8.For each of these tables, we highlight in bold the highest metric in the table.To further investigate the impact of combining feature-feature and sample-sample dependencies, we rely on reconstructionbased strategy which makes use of the KNN-Imputer strategy and compare to the vanilla transformer and NPT-AD.</p>
<p>K-Nearest Neighbor Imputation Take a dataset D = {x i } n i=1 where x i ∈ R d and for which some samples might display missing values in the feature vector.K-nearest neighbor imputation for a sample z ∈ D consists in identifying the k nearest neighbors of sample z given a distance measure d : R d × R d → R, where k is a hyperparameter that must be discretionary chosen.This distance measure only takes into account the non-missing features of sample z.Let I designate the index of the non-missing features and z [I] the corresponding features of sample z, then the k-nearest neighbors of sample z are identified through evaluating the distance d(z [I] , x x i .(
)14
Other imputation methods include weighting each sample in K(z) by its inverse distance to z, denoted ω</p>
<p>[I]</p>
<p>(z,x) = 1/d(z [I] , x</p>
<p>[I] j ).This gives ẑi = 1</p>
<p>x ω</p>
<p>[I] (z,x) x∈K(z)
ω [I] (z,x) x i .(15)
This approach leverage only sample-sample dependencies, while the vanilla transformer only leverage feature-feature dependencies and NPT-AD combines these two types as illustrated in figure 4.</p>
<p>Mask-KNN Anomaly Score Consider a training set D train = {x i } ntrain i=1 , x i ∈ R d comprised of only normal samples and a validation set D val = {x i } n val i=1 for which we wish to predict the label.In a reconstruction-based approach we construct an anomaly score based on how masked samples are well-reconstructed using KNN imputation as described in the previous paragraph.First, we construct a mask bank comprised of m masks, where m = r j=1 d j and r designates the maximum number of features masked simultaneously.The mask bank is comprised of all possible combinations of j masked features for j ≤ r.Each mask corresponds to a d-dimensional vector composed of 0 and 1, where 1's indicate that the corresponding features will be masked.Let us denote as ẑ(ℓ) the reconstructed sample z for mask ℓ, take d : R d × R d → R a distance measure, e.g. the ℓ 2 -norm, then the anomaly score for sample z is given as
Mask-KNN(z) = m ℓ=1 d(z, ẑ(ℓ) )(16)
We give the pseudo-code of this method in alg. 1.</p>
<p>Table 12.Necessary dependencies to take into account for AD by mask-reconstruction.Given the obtained hierarchy between NPT-AD, Mask-KNN and the transformer on each dataset, we display the necessary dependencies to identify efficiently anomalies.</p>
<p>Dependencies feature-feature sample-sample
Wine ✓ ✓ Lympho ✓ × Glass × ✓ Vertebral × ✓ Wbc ✓ ✓ Ecoli ✓ × Ionosph. ✓ ✓ Arrhyth. ✓ ✓ Breastw ✓ ✓ Pima ✓ ✓ Vowels × ✓ Letter ✓ ✓ Cardio ✓ ✓ Seismic × ✓ Musk ✓ ✓ Speech ✓ ✓ Thyroid ✓ ✓ Abalone ✓ ✓ Optdigits × ✓ Satimage ✓ ✓ Satellite × ✓ Pendigits × ✓ Annthyr. ✓ ✓ Mnist × ✓ Mammo. × ✓ Shuttle ✓ ✓ Mulcross ✓ ✓ Forest ✓ × Campaign ✓ ✓ Fraud ✓ × Backdoor ✓ ×
Figure 1 .
1
Figure1.NPT-AD inference pipeline.In step (a), mask j is applied to each validation sample.We construct a matrix X composed of the masked validation samples and the whole unmasked training set.In step (b), we feed X to the Non-Parametric Transformer (NPT), which tries to reconstruct the masked features for each validation sample.On top of the learned feature-feature interactions, NPT will use the unmasked training samples to reconstruct the mask features.In step (c), we compute the reconstruction error that we later aggregate in the NPT-AD score.</p>
<p>Figure 2 .
2
Figure 2.For each of the 31 datasets on which models were evaluated, we compute the average F1-score over 20 runs for 20 different seeds.We report on figure 2(a) the average F1-score over all datasets for each tested model.We report on figure 2(b) the average rank over the 31 datasets.For both figures, the model displayed on the far left is the worst-performing model for the chosen metric, and on the far right is the best-performing model.We also highlight the metric of the best-performing model in bold.See tables 4 and 5 in appendix D.1 for the details of the obtained metrics.</p>
<p>Figure 3 .
3
Figure 3. Training set contamination impact on the F1-score and AUROC.Each model was trained 5 times for each contamination share.The architecture used for NPT-AD is the same as for all experiments (see section 4).The NPT and Transformer were trained for 100 epochs with batch size equal to the dataset size, with learning rate 0.01, optimizer LAMB(You et al., 2020) with β = (0.9, 0.999), per-feature embedding dimension 16, r set to 1, and masking probability p mask = 0.15.NeuTraL-AD and GOAD were trained with hyperparameters as for the thyroid dataset in the original papers and (Shenkar &amp; Wolf, 2022) with its default parameters in their implementation.</p>
<p>Figure 4 .
4
Figure 4.While Mask-KNN only relies on sample-sample dependencies and the vanilla transformer only attends to feature-feature dependencies, NPT-AD combines both for anomaly detection.</p>
<p>each x j ∈ D and ordering them to find the k smallest.Let K(z) designate the k nearest neighbors of sample z, Ī the missing values of z, then ∀i ∈</p>
<p>Table 1 .
1
Ablation study.Comparison in AD performance between the vanilla transformer, Mask-KNN, and NPT-AD..2 and summarized in tables 1, 11 and 13.We observe that, indeed, NPT-AD outperforms the vanilla transformer and Mask-KNN based on both AUROC and F1-score as shown in table 1, emphasizing that combining both types of dependencies boosts anomaly detection performance.
Transformer Mask-KNN NPT-ADF157.457.568.8AUROC83.084.589.8To further explore the combined impact of sample-sample and feature-feature dependencies, we introduce areconstruction-based technique similar to NPT-AD. We putforward Mask-KNN that relies on KNN imputation to re-construct masked features (see alg. 1 in appendix D.2). Wealso compare it to the vanilla transformer model trained in a
framework similar to NPT-AD.Mask-KNN (resp.thetransformer)can be seen as approximately equivalent to NPT-AD without considering the feature-feature dependencies (resp.thesample-sampledependencies) as illustrated in figure4in appendix D.2.Our experiment is detailed in appendix DAs displayed in table 11, our experiments show that we do observe this performance behavior.For instance, on datasets where Mask-KNN significantly outperforms the transformer by a sizable gap, e.g.glass, vertebral, or vowels, NPT-AD also outperforms the transformer significantly.Similarly, on datasets where the transformer outperforms Mask-KNN, e.g.lymphography, WBC or Forest Cover, NPT-AD also performs significantly better than Mask-KNN.This performance relation can help identify which datasets require which type of dependency to flag anomalies using reconstruction-based methods correctly.We propose such classification in table 12 in appendix D.2.</p>
<p>Table 2 .
2
Datasets characteristics
DatasetndOutliersWine1291310 (7.7%)Lympho148186 (4.1%)Glass21499 (4.2%)Vertebral240630 (12.5%)WBC2783021 (5.6%)Ecoli33679 (2.6%)Ionosphere35133126 (36%)Arrhythmia45227466 (15%)BreastW6839239 (35%)Pima7688268 (35%)Vowels14561250 (3.4%)Letter Recognition160032100 (6.25%)Cardio183121176 (9.6%)Seismic258411170 (6.5%)Musk306216697 (3.2%)Speech368640061 (1.65%)Thyroid3772693 (2.5%)Abalone4177929 (0.69%)Optdigits521664150 (3%)Satimage-258033671 (1.2%)Satellite6435362036 (32%)Pendigits687016156 (2.27%)Annthyroid72006534 (7.42%)Mnist7603100700 (9.2%)Mammography111836260 (2.32%)Shuttle4909793511 (7%)Mulcross262144426214 (10%)ForestCover286048 102747 (0.9%)Campaign4118862 4640 (11.3%)Fraud284807 29492 (0.17%)Backdoor95329 196 2329 (2.44%)</p>
<p>Table 3 .
3
Datasets hyperparameters.When the batch size is −1 it refers to a full pass over the training set before an update of the weights.
Datasetepoch batch sizelrp train maskrmeWine1000−10.0010.151138Lympho100−10.010.154 3078 16Glass1000−10.010.154 255 16Vertebral2000−10.0010.15168WBC100−10.010.153 4525 16Ecoli100−10.010.1536316Ionosphere100−10.0010.152 561 16Arrhythmia100−10.010.151 274 16BreastW500−10.010.153 129 16Pima500−10.010.154 162 16Vowels1000−10.010.1527816Letter Recognition 1000−10.010.1513216Cardio100−10.010.152 231 16Seismic100−10.010.152 276 16Musk100−10.010.152 166 16Speech10005120.0010.151 4008Thyroid5000−10.010.122116Abalone1000−10.00010.154 162 16Optdigits500−10.010.216416Satimage-2100−10.010.213616Satellite100−10.010.213616Pendigits1000−10.010.252 136 16Annthyroid400−10.010.151616Mnist1000−10.0010.151 100 32Mammography200−10.010.2545616Shuttle10040960.010.253 129 64Mulcross10040960.0010.1521016ForestCover10040960.010.1525516Campaign10040960.0010.1516216Fraud10040960.0010.212932Backdoor10002560.0010.21 196 32</p>
<p>Table 4 .
4
Anomaly detection F1-score (↑) for deep models.We perform 5% T-test to test whether the difference between the highest metrics for each dataset is statistically significant.
MethodDROCCGOADNeuTraL. Inter.Cont. TransformerNPT-AD(abalone)(thyroid)(arrhy.)Wine63.0±20.0 67.0±9.478.2±4.590.0±6.323.5±7.972.5±7.7Lympho65.0±5.0 68.3±13.0 20.0±18.786.7±6.088.3±7.694.2±7.9Glass14.5±11.1 12.7±3.99.0±4.427.2±10.614.4±6.126.2±10.9Vertebral9.3±6.116.3±9.63.8±1.226.0±7.712.3±5.220.3±4.8Wbc9.0±6.266.2±2.960.9±5.667.6±3.666.4±3.267.3±1.7EcoliN/A61.4±31.77.0±7.170.0±7.875.0±9.977.7±0.1Ionosphere 76.9±2.883.4±2.690.6±2.493.2±1.388.1±2.892.7±0.6Arrhyth.37.1±6.852.0±2.359.5±2.661.8±1.859.8±2.260.4±1.4Breastw93.0±3.796.0±0.691.8±1.396.1±0.796.7±0.395.7±0.3Pima66.0±4.166.0±3.160.3±1.459.1±2.265.6±2.068.8±0.6Vowels66.2±8.831.1±4.210.0±6.290.8±1.628.7±8.088.7±1.6Letter55.6±3.620.7±1.75.7±0.862.8±2.441.5±6.271.4±1.9Cardio49.8±3.278.6±2.545.5±4.371.0±2.468.8±2.878.1±0.1Seismic19.1±0.924.1±1.011.8±4.320.7±1.919.1±5.726.2±0.7Musk99.4±1.5100±0.099.0±0.0100±0.0100±0.0100±0.0Speech4.3±2.04.8±2.34.7±1.45.2±1.26.8±1.99.3±0.8Thyroid72.7±3.172.5±2.869.4±1.476.8±1.255.5±4.877.0±0.6Abalone17.9±1.357.6±2.253.2±4.068.7±2.342.5±7.859.7±0.1Optdig.30.5±5.20.3±0.316.2±7.3 66.3±10.161.1±4.762.0±2.7Satim.4.8±1.690.7±0.792.3±1.992.4±0.789.0±4.194.8±0.8Satellite52.2±1.564.2±0.871.6±0.673.2±1.665.6±3.374.6±0.7Pendig.11.0±2.640.1±5.069.8±8.782.3±4.535.4±10.992.5±1.3Annthyr.64.2±3.350.3±6.344.1±2.345.4±1.829.9±1.557.7±0.6MnistN/A66.9±1.384.8±0.585.9±0.056.7±5.771.8±0.3Mammo.32.6±2.133.7±6.119.2±2.429.4±1.417.4±2.243.6±0.5ShuttleN/A73.5±5.197.9±0.298.4±0.185.3±9.898.2±0.3Mullcr.N/A99.7±0.8 96.3±10.5100±0.0100±0.0100±0.0ForestN/A0.1±0.251.6±8.244.0±4.121.3±3.158.0±10Camp.N/A16.2±1.842.1±1.746.8±1.447.0±1.949.8±0.3FraudN/A53.1±10.2 24.3±7.857.9±2.854.3±5.258.1±3.2Backd.N/A12.7±2.984.4±1.886.6±0.185.8±0.684.1±0.1mean32.751.050.867.256.268.8mean std3.44.44.02.94.32.0mean rk11.58.49.63.77.53.1</p>
<p>Table 5 .
5
Anomaly detection F1-score (↑) for non-deep models.We perform 5% T-test to test whether the difference between the highest metrics for each dataset is statistically significant.
MethodCOPODIForestKNNPIDForestRRCFNPT-ADWine60.0±4.5 64.0±12.8 94.0±4.950.0±6.4 69.0±11.4 72.5±7.7Lympho85.0±5.071.7±7.6 80.0±11.7 70.0±0.0 36.7±18.0 94.2±7.9Glass11.1±0.011.1±0.011.1±9.78.9±6.015.6±13.3 26.2±10.9Vertebral1.7±1.713.0±3.810.0±4.512.0±5.28.0±4.820.3±4.8Wbc71.4±0.070.0±3.763.8±2.365.7±3.754.8±6.167.3±1.7Ecoli25.6±11.2 58.9±22.2 77.8±3.3 25.6±11.2 28.9±11.3 77.7±0.1Ionosphere70.8±1.880.8±2.188.6±1.667.1±3.972.0±1.892.7±0.6Arrhythmia 58.2±1.460.9±3.361.8±2.222.7±2.550.6±3.360.4±1.4Breastw96.4±0.697.2±0.596.0±0.770.6±7.663.0±1.895.7±0.3Pima62.3±1.169.6±1.265.3±1.065.9±2.955.4±1.768.8±0.6Vowels4.8±1.025.8±4.764.4±3.723.2±3.218.0±4.688.7±1.6Letter12.9±0.715.6±3.345.0±2.614.2±2.317.4±2.271.4±1.9Cardio65.0±1.473.5±4.167.6±0.943.0±2.543.9±2.778.1±0.1Seismic29.2±1.373.9±1.530.6±1.429.2±1.624.1±3.226.2±0.7Musk49.6±1.2 52.0±15.3 100±0.035.4±0.038.4±6.5100±0.0Speech3.3±0.04.9±1.95.1±1.02.0±1.93.9±2.89.3±0.8Thyroid30.8±0.578.9±2.757.3±1.372.0±3.231.9±4.777.0±0.6Abalone50.3±6.453.4±1.743.4±4.858.6±1.636.9±6.459.7±0.1Optdigit3.0±0.315.8±4.390.0±1.2 22.5±16.81.3±0.762.0±2.7Satimage77.9±0.986.5±1.793.8±1.235.5±0.447.9±3.494.8±0.8Satellite56.7±0.269.6±0.576.3±0.446.9±3.755.4±1.374.6±0.7Pendigit34.9±0.652.1±6.491.0±1.444.6±5.316.3±2.692.5±1.3Annthyroid 31.5±0.557.3±1.337.8±0.665.4±2.732.1±0.857.7±0.6Mnist38.5±0.451.2±2.569.4±0.932.6±5.733.5±1.771.8±0.3Mammo.53.4±0.939.0±3.338.8±1.528.1±4.327.1±1.943.6±0.5Shuttle96.0±0.096.4±0.897.3±0.270.7±1.032.0±2.298.2±0.3Mullcr.66.0±0.199.1±0.5100±0.067.4±2.1100±0.0100±0.0Forest18.2±0.211.1±1.692.1±0.38.1±2.89.9±1.558.0±10Campaign49.5±0.142.4±1.041.6±0.442.4±0.236.6±0.149.8±0.3Fraud44.7±0.930.3±3.760.5±1.541.0±0.917.1±0.458.1±3.2Backdoor13.4±0.43.8±1.288.5±0.13.4±0.224.5±0.184.1±0.1mean44.252.665.839.837.068.8mean std1.53, 92.23.64.02.0mean rk10.57.65.211.512.63.1</p>
<p>Table 6 .
6
Anomaly detection AUROC(↑).We perform 5% T-test to test whether the differences between the highest metrics for each dataset are statistically significant.
MethodDROCCDROCCDROCCGOADGOADGOADInternal Cont. NPT-AD(Thyroid)(Arrhyth.)(Abalone)(Thyroid)(kddrev)(kdd)(Ours)Wine53.5±2260.1±32 90.9±8.295.2±1.997.3±1.7 86.3±9.599.5±0.696.6±0.5Lympho6.4±5.258.6±3083.7±1294.8±5.679.7±1159.9±1599.5±0.399.9±0.1Glass63.5±9.1 55.5±22 75.4±8.962.2±1485.5±782.1±6.388.1±5.082.8±2.4Vertebral55.0±5.1 58.0±15 41.2±1047.0±1352.2±3.9 49.4±4.251.1±3.254.6±3.9WBC6.8±1.841.3±2535.4±1395.4±0.766.1±12 86.6±2.995.4±1.196.3±0.3EcoliN/AN/AN/A82.7±8.487.2±3.3 84.7±6.886.5±1.288.7±1.6Ionosph.19.6±5.8 83.5±5.6 80.0±2.892.4±1.396.3±1.1 96.5±1.198.1±0.497.4±1.7Arrhyth.53.2±7.0 52.7±8.6 51.2±8.180.0±1.973.3±5.1 64.3±8.881.7±0.680.1±0.0Breastw7.7±8.664.4±33 96.6±3.398.7±0.880.8±9.5 97.7±0.899.1±0.398.6±0.1Pima36.2±4.6 54.9±11 69.1±4.968.7±3.959.3±2.2 63.2±2.359.4±2.873.4±0.4Vowels79.4±9.5 72.0±12 95.3±2.181.0±2.498.5±0.3 97.6±0.599.7±0.199.3±0.1Letter77.6±3.3 73.3±5.4 90.0±1.260.9±0.789.9±0.5 87.6±0.992.8±0.996.1±0.2Cardio84.3±4.0 73.8±12 73.5±3.2 94.8±1.7 81.3±4.5 84.6±3.092.7±0.894.7±0.2Seismic58.2±2.8 60.3±4.5 56.7±1.3 69.5±1.5 67.2±1.2 67.9±1.262.9±1.069.8±0.3Musk2.3±5.1100±0.0 100±0.0100±0.0100±0.0 100±0.0100±0.0100±0.0Speech51.2±5.6 50.5±4.0 52.6±3.447.1±1.3 65.3±3.2 54.1±4.458.9±2.754.3±0.3Thyroid95.6±0.9 96.1±2.5 98.1±0.394.5±1.577.1±8.8 89.2±3.098.5±0.197.8±0.1Abalone82.4±1452.9±26 70.6±9.789.2±0.946.0±3.7 54.3±7.894.3±0.691.6±1.2Optdig.84.2±4.6 89.0±4.6 89.5±2.166.9±3.395.7±0.5 93.1±1.997.5±1.597.5±0.3Satimage19.1±1.4 87.5±8.8 11.5±1.299.1±0.186.5±7.1 93.2±1.799.8±0.199.9±0.0Satellite64.6±8.9 73.1±1.3 50.2±2.269.1±0.876.3±1.0 78.2±0.980.6±1.780.3±0.9Pendigits58.9±7.6 50.8±15 76.6±5.487.5±3.989.2±2.9 85.1±3.499.5±0.199.9±0.0Annthyr.92.9±2.3 86.5±3.6 93.4±1.3 76.1±6.589.6±4.9 93.2±0.980.5±1.386.7±0.6MnistN/AN/AN/A90.9±0.489.4±0.7 87.7±1.098.2±0.094.4±0.1Mammo.81.0±1.3 85.0±2.1 82.0±1.566.3±6.457.2±1.9 54.5±2.381.1±2.088.6±0.3MullcrossN/AN/AN/A100±0N/A51.3±16100±0100±0ShuttleN/AN/AN/A88.4±5.5N/A99.9±0.0100±099.8±0.1ForestN/AN/AN/A15.9±6.6N/A76.0±5.396.2±0.695.8±7.9CampaignN/AN/AN/A38.4±2.0N/A50.9±2.573.7±1.579.1±0.5FraudN/AN/AN/A83.5±2.7N/A86.3±0.895.2±0.595.7±0.1BackdoorN/AN/AN/A61.3±10.2N/A88.9±1.192.6±0.695.2±0.1mean39.850.953.777.364.178.888.889.8mean std4.59.13.43.53.23.81.00.8mean rk10.310.08.77.47.77.22.92.5</p>
<p>Table 7 .
7
Anomaly detection AUROC(↑).We perform 5% T-test to test whether the differences between the highest metrics for each dataset are statistically significant.
MethodNeuTraL-AD NeuTraL-AD NeuTraL-AD NeuTraL-AD COPOD Transformer(thyroid)(arrhythmia)(kddrev)(kdd)Wine82.9±13.195.4±1.986.0±10.385.2±12.987.5±1.761.4±6.7Lympho90.7±5.180.9±7.593.1±4.183.1±9.999.4±0.499.5±0.4Glass62.2±3.062.9±1.262.5±4.665.1±2.963.7±3.361.2±7.0Vertebral32.8±3.525.8±4.551.9±14.954.4±16.332.6±1.244.8±5.2Wbc80.4±5.092.6±2.262.4±8.232.5±11.696.3±0.595.0±1.1Ecoli43.2±9.353.8±9.952.5±10.649.9±10.981.0±1.284.8±1.6Iono.87.9±2.995.7±1.792.9±0.987.2±2.780.3±2.195.4±1.9Arrhyt.76.0±1.580.2±1.677.9±1.876.4±2.780.5±1.381.7±1.1Breastw86.0±2.296.2±1.195.9±1.691.3±5.199.4±0.299.6±0.1Pima55.1±1.960.6±1.257.3±2.157.3±3.765.2±0.767.2±2.4Vowels72.3±3.169.7±3.862.2±6.156.1±8.049.6±1.078.4±9.2Letter40.4±3.935.4±0.836.1±2.937.4±4.250.1±0.880.5±4.8Cardio74.6±2.774.3±3.147.7±4.228.9±6.392.2±0.393.5±1.3Seismic42.7±4.239.9±5.540.6±9.441.0±13.170.8±0.458.2±7.9Musk99.5±0.299.4±0.298.2±1.091.9±3.694.5±0.2100±0.0Speech46.7±1.648.0±1.652.7±3.254.3±2.349.1±0.547.2±0.7Thyroid97.5±0.297.1±0.295.8±1.679.4±10.494.1±0.293.8±1.2Abalone92.9±1.092.7±0.880.8±2.473.9±4.686.3±0.388.3±2.0Optdigits72.3±7.182.6±5.484.1±4.378.4±7.268.0±0.496.4±4.7Satimage99.6±0.499.8±0.199.8±0.199.7±0.197.4±0.199.7±0.1Satellite80.7±0.481.0±0.476.8±0.674.0±1.663.5±0.273.8±2.5Pendigits88.6±5.798.6±0.897.5±0.993.0±3.190.4±0.293.8±2.6Annthyr.87.7±4.280.2±2.964.9±1.663.7±3.077.4±0.465.4±1.4Mnist97.5±0.397.8±0.293.4±0.789.9±1.177.2±0.287.4±3.2Mammo.67.6±3.373.8±2.565.4±3.369.6±3.390.5±0.177.6±1.0Mullcross98.5±2.199.5±1.599.6±0.599.2±1.593.2±0.0100±0.0Shuttle99.8±0.399.9±0.1100±0.099.9±0.199.4±0.097.2±2.2Forestc.96.8±1.596.7±1.184.4±7.082.6±6.288.4±0.095.1±0.8Campaign75.4±4.870.0±2.176.5±0.776.4±0.578.3±0.175.3±2.1Fraud81.6±7.484.8±4.793.1±0.693.3±0.494.7±0.194.7±0.4Backdoor91.6±5.892.5±7.592.9±0.492.9±0.378.9±0.195.1±0.2mean77.579.376.672.879.783.4mean std3.52.53.65.10.62.4mean rank8.27.17.88.77.56.1</p>
<p>Table 8 .
8
(Goyal et al., 2020)2020): F1-score (↑) between architecture.The mean rank was computed including all architectures of all models.
MethodDROCCDROCCDROCC(thyroid)(arrhyth.)(abalone)Wine20.0±19.0 32.0±35.4 63.0±20.0Lympho0.0±0.038.3±23.665.0±5.0Glass22.2±17.2 13.3±12.0 14.5±11.1Vertebral25.7±5.427.0±15.99.3±6.1Wbc0.0±0.018.6±16.09.0±6.2EcoliN/AN/AN/AIonosph.29.9±6.876.3±6.476.9±2.8Arrhyth.38.8±6.237.9±8.037.1±6.8Breastw15.3±7.763.8±29.393.0±3.7Pima40.6±3.355.2±8.066.0±4.1Vowels33.0±16.4 20.4±15.066.2±8.8Letter39.0±4.831.3±6.555.6±3.6Cardio62.6±6.153.3±12.949.8±3.2Seismic17.7±2.517.9±2.719.1±0.9Musk1.3±3.399.7±0.999.4±1.5Speech3.4±2.42.1±1.94.3±2.0Thyroid68.4±3.269.7±5.772.7±3.1Abalone44.3±17.6 11.6±10.517.9±1.3Optdigits18.4±5.426.5±12.630.5±5.2Satimage210.2±2.533.7±19.64.8±1.6Satellite61.3±6.368.1±0.752.2±1.5Pendigits7.9±2.910.6±7.911.0±2.6Annthyr.63.8±4.755.6±5.264.2±3.3MnistN/AN/AN/AMammo.34.1±2.231.5±6.232.6±2.1ShuttleN/AN/AN/AMullcrossN/AN/AN/AForestN/AN/AN/ACampaignN/AN/AN/AFraudN/AN/AN/ABackdoorN/AN/AN/Amean21.228.932.7mean std4.78.53.4mean rank12.611.910.8</p>
<p>Table 9 .
9
(Bergman &amp; Hoshen, 20202020): F1-score (↑) between architecture.The mean rank was computed including all architectures of all models.</p>
<p>Table 10 .
10
(Qiu et al., 2021)al., 2021): F1-score (↑) between architecture.The mean rank was computed including all architectures of all models.
MethodNeuTraL-AD NeuTraL-AD NeuTraL-AD NeuTraL-AD(thyroid)(arrhythmia)(kddrev)(kdd)Wine51.4±26.278.2±4.562.3±26.962.3±28.9Lympho46.7±17.920±18.754.2±15.734.2±15.3Glass7.5±6.29.0±4.49.5±5.913.0±7.8Vertebral9.2±3.43.8±1.223.3±9.813.0±7.8Wbc40.7±1060.9±5.621.1±1013.0±7.8Ecoli4.0±5.87.0±7.16.5±11.18.0±12.5Ionosph.79.2±2.890.6±2.486.9±1.479.4±4.0Arrhyth.54.9±3.459.5±2.657.7±1.657.2±3.1Breastw80.2±2.091.8±1.389.6±2.985.6±5.6Pima55.4±1.760.3±1.457.2±1.956.8±2.5Vowels13.2±6.310±6.25.0±3.83.9±3.4Letter4.9±1.75.7±0.83.6±1.24.8±2.8Cardio46.9±3.945.5±4.314.7±5.03.8±2.7Seismic12.8±1.311.8±4.38.7±4.410.7±7.9Musk98.9±099.0±079.3±11.243.4±16.5Speech5.3±1.84.7±1.44.3±2.46.1±2.7Thyroid75.6±2.369.4±1.461.4±8.426.6±17.0Abalone60.8±4.253.2±4.045.6±8.647.1±8.3Optdigits11.9±7.016.2±7.318.5±9.617.1±9.4Satimage285.8±9.092.3±1.992.4±1.491.3±0.9Satellite72.7±0.371.6±0.670.4±0.666.9±2.1Pendigits32.4±14.369.8±8.758.4±8.942.2±13.2Annthyr.53.5±5.144.1±2.333.2±2.129.1±4.3Mnist82.8±0.984.8±0.568.8±2.560.8±2.8Mammo.11.3±1.719.2±2.420.8±3.719.1±4.9Shuttle97.1±0.497.9±0.297.6±0.197.5±0.1Mullcross88.9±12.296.3±10.596.2±3.692.9±9.4Forest64.6±9.951.6±8.212.2±11.48.7±7.6Campaign52.0±4.142.1±1.751.6±0.151.2±0.8Fraud24.7±7.824.3±7.861.0±5.255.9±4.2Backdoor84.9±5.084.4±1.887.3±0.286.8±0.3mean48.750.847.041.7mean std5, 84.05.97.0mean rank9.89.09.410.7</p>
<p>Table 13 .
13
Mean rank (F1-score) for the experiments conducted, without Mask-KNN and with Mask-KNN
Methodmean rank mean rank (w/ Mask-KNN) diff.DROCC (abalone)11.512.4+0.9GOAD (thyroid)8.49.1+0.7NeuTraL-AD (arrhythmia)9.610.3+0.7Internal Cont.3.74.0+0.3COPOD10.511.0+0.5IForest7.68.1+0.5KNN5.25.6+0.4PIDForest11.512.2+0.7RRCF12.613.4+0.8Transformer7.58.0+0.5Mask-KNNN/A6.6N/ANPT-AD3.13.4+0.3</p>
<p>Table 14 .
14
Runtime in seconds of NPT-AD for the training and inference phase.The training runtime corresponds to the average training time of the model over the 20 runs with the parameters displayed in table 3. The inference runtime corresponds to the average runtime over the 20 runs to compute NPT-AD as shown in equation 7.
DatasettraininferenceWine6368Lympho10283Glass766Vertebral1282Wbc10479Ecoli1123Ionosph.1276Arrhyth.100223Breastw76Pima3718Vowels6263Letter10515Cardio1097Seismic9189Musk56168Speech6264Thyroid2532Abalone5550Optdigits127152Satimage21317Satellite1323Pendigits7847Annthyr.225Mnist478153Mammo.1624Shuttle16115Mullcross4344Forest73409Campaign52251Fraud141362Backdoor183961992
For large datasets, we resort to a random subsample of the training set for computational reasons.
http://odds.cs.stonybrook.edu/
Acknowledgment This work was granted access to the HPC resources of IDRIS under the allocation 2023-101424 made by GENCI.This research publication is supported by the Chair "Artificial intelligence applied to credit card fraud detection and automated trading" led by CentraleSupelec and sponsored by the LUSIS company.The authors would also like to thank Gabriel Kasmi for his helpful advice and feedback.Implementation For simplicity we set r to 2 for all experiments, except for large dataset (n &gt; 200, 000) for which r was set to 1 for computational reasons.We set k, the number of neighbors, to 5 as for the vanilla KNN implementation.When present, categorical features were encoded using one-hot encoding.Except for large datasets (n &gt; 200, 000) with many features, d, such as ForestCover, Fraud and Backdoor, we set B as the entire training set.Otherwise, we take a random subsample of size b = 10, 000.We use the imputation strategy described in equation 15 to reconstruct the masked sampled.We report the results of this experiment in table 11 and compare the performance of Mask-KNN to KNN, the internal contrastive approach of (Shenkar &amp; Wolf, 2022) and NPT-AD.We run the algorithm 20 times for each dataset, except for ForestCover, Fraud and Backdoor, for which report an average over 10 runs for computational reasons.The mean rank, provided in table 11, was computed, including each architecture of each approach.For completeness, we also include a table containing the mean rank of all approaches including MasK-KNN in table13.ResultsWe observe that Mask-KNN obtains satisfactory results on a significant share of the tested datasets, e.g.pendigits, satellite; while also displaying poor performance on some datasets such as forest or backdoor in comparison with NPT-AD.Several factors can account for this.First, NPTs automatically select the number of relevant samples on which to rely to reconstruct the masked features, thus making this approach much more flexible than Mask-KNN, which has a fixed number of neighbors.Second, NPT-AD relies on attention mechanisms to learn the weights attributed to relevant samples while Mask-KNN relies on the ℓ 2 -distance.Although the ℓ 2 -distance offers a precise measure of similarity based on geometric distance, the attention mechanism can capture much more complex relations between samples.Finally, NPT-AD not only relies on sample-sample dependencies to reconstruct the mask features, but it also attends to feature-feature dependencies.The strong performance of NPT-AD on datasets where Mask-KNN also performs well serves as evidence supporting the fact that NPT-AD effectively captures sample-sample dependencies.Moreover, NPT-AD outperforms Mask-KNN on most datasets where the approach of (Shenkar &amp; Wolf, 2022) performs well, highlighting the crucial role of featurefeature dependencies on specific datasets.The results displayed in table 11 show that NPT-AD manages to capture both feature-feature and sample-sample dependencies to reconstruct samples when sample-sample dependencies are not sufficient.
Thirty-Third Conference on Innovative Applications of Artificial Intelligence. S Ö Arik, T Pfister, 10.1609/aaai.v35i8.16826The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI PressFebruary 2-9, 202120212021Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021</p>
<p>Neural machine translation by jointly learning to align and translate. J L Ba, J R Kiros, G E Hinton, D Bahdanau, K Cho, Y Bengio, 2016. 2014Layer normalization</p>
<p>Classification-based anomaly detection for general data. L Bergman, Y Hoshen, International Conference on Learning Representations. 2020</p>
<p>Identifying density-based local outliers. M M Breunig, H.-P Kriegel, R T Ng, J Sander, Lof, 10.1145/335191.335388SIGMOD Rec. 0163-5808292may 2000</p>
<p>Unsupervised detection of lesions in brain MRI using constrained adversarial autoencoders. X Chen, E Konukoglu, Medical Imaging with Deep Learning. 2018</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational LinguisticsJune 2-7, 2019. 20191</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, International Conference on Learning Representations. 2021</p>
<p>Pidforest: Anomaly detection and certification via partial identification. P Gopalan, V Sharan, U Wieder, Neural Information Processing Systems. 2019</p>
<p>Y Gorishniy, I Rubachev, N Kartashev, D Shlenskii, A Kotelnikov, A Babenko, Tabr, Tabular deep learning meets nearest neighbors in 2023. 2023</p>
<p>Deep robust one-class classification. S Goyal, A Raghunathan, M Jain, H V Simhadri, P Jain, Drocc, Proceedings of the 37th International Conference on Machine Learning. H D Iii, A Singh, the 37th International Conference on Machine LearningPMLRJul 2020119</p>
<p>Why do tree-based models still outperform deep learning on typical tabular data?. L Grinsztajn, E Oyallon, G Varoquaux, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022</p>
<p>Robust random cut forest based anomaly detection on streams. S Guha, N Mishra, G Roy, O Schrijvers, International Conference on Machine Learning. 2016</p>
<p>AD-Bench: Anomaly detection benchmark. S Han, X Hu, H Huang, M Jiang, Y Zhao, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022</p>
<p>Extended isolation forest. S Hariri, M C Kind, R J Brunner, 10.1109/TKDE.2019.2947676IEEE Transactions on Knowledge and Data Engineering. 3342021</p>
<p>The detection of errors in multivariate data using principal components. D M Hawkins, Journal of the American Statistical Association. 01621459693461974</p>
<p>Financial fraud: A review of anomaly detection techniques and recent advances. W Hilal, S A Gadsden, J Yawney, 10.1016/j.eswa.2021.116429.URLhttps://www.sciencedirect.com/science/article/pii/S09574174210171642022193116429Expert Systems with Applications</p>
<p>TANGOS: Regularizing tabular neural networks through gradient orthogonalization and specialization. A Jeffares, T Liu, J Crabbé, F Imrie, M Van Der Schaar, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Welltuned simple nets excel on tabular datasets. A Kadra, M Lindauer, F Hutter, J Grabocka, Thirty-Fifth Conference on Neural Information Processing Systems. 2021</p>
<p>Novelty detection with reconstruction along projection pathway. K H Kim, S Shim, Y Lim, J Jeon, J Choi, B Kim, A S Yoon, Rapp, International Conference on Learning Representations. 2020</p>
<p>A mutual information maximization perspective of language representation learning. L Kong, C De Masson D'autume, L Yu, W Ling, Z Dai, D Yogatama, International Conference on Learning Representations. 2020</p>
<p>Self-attention between datapoints: Going beyond individual input-output pairs in deep learning. J Kossen, N Band, C Lyle, A Gomez, T Rainforth, Y Gal, Advances in Neural Information Processing Systems. A Beygelzimer, Y Dauphin, P Liang, J W Vaughan, 2021</p>
<p>COPOD: Copula-based outlier detection. Z Li, Y Zhao, N Botta, C Ionescu, X Hu, 10.1109/icdm50108.2020.001352020 IEEE International Conference on Data Mining (ICDM). IEEEnov 2020</p>
<p>Isolation forest. F T Liu, K M Ting, Z.-H Zhou, 10.1109/ICDM.2008.172008 Eighth IEEE International Conference on Data Mining. 2008</p>
<p>Explainable deep one-class classification. P Liznerski, L Ruff, R A Vandermeulen, B J Franks, M Kloft, K R Muller, International Conference on Learning Representations. 2021</p>
<p>An empirical evaluation of deep learning for network anomaly detection. R K Malaiya, D Kwon, J Kim, S C Suh, H Kim, I Kim, 10.1109/ICCNC.2018.83902782018 International Conference on Computing, Networking and Communications (ICNC). 2018</p>
<p>Network constraints and multi-objective optimization for one-class classification. M M Moya, D R Hush, 10.1016/0893-6080(95)00120-4https:// doi.org/10.1016/0893-6080(95)00120-4Neural Netw. 0893-608093apr 1996</p>
<p>Image transformer. N J Parmar, A Vaswani, J Uszkoreit, L Kaiser, N Shazeer, A Ku, D Tran, International Conference on Machine Learning (ICML). 2018</p>
<p>On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics. E Parzen, 10.1214/aoms/1177704472196233</p>
<p>Acoustic novelty detection with adversarial autoencoders. E Principi, F Vesperini, S Squartini, F Piazza, 10.1109/IJCNN.2017.79662732017 International Joint Conference on Neural Networks (IJCNN). 2017</p>
<p>Neural transformation learning for deep anomaly detection beyond images. C Qiu, T Pfrommer, M Kloft, S Mandt, M Rudolph, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. M Meila, T Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>Efficient algorithms for mining outliers from large data sets. S Ramaswamy, R Rastogi, K Shim, 10.1145/342009.335437Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data. W Chen, J F Naughton, P A Bernstein, the 2000 ACM SIGMOD International Conference on Management of DataDallas, Texas, USAACMMay 16-18, 2000. 2000</p>
<p>Mean-shifted contrastive loss for anomaly detection. T Reiss, Y Hoshen, 10.1609/aaai.v37i2.25309Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence. the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial IntelligenceAAAI Press2023AAAI'23/IAAI'23/EAAI'23</p>
<p>Transformation based deep anomaly detection in astronomical images. E Reyes, P A Estévez, 10.1109/IJCNN48605.2020.92069972020 International Joint Conference on Neural Networks (IJCNN). 2020</p>
<p>A probabilistic resource allocating network for novelty detection. S Roberts, L Tarassenko, L Ruff, R Vandermeulen, N Goernitz, L Deecke, S A Siddiqui, A Binder, E Müller, M Kloft, 10.1162/neco.1994.6.2.270Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR1994. 10-15 Jul 20186Neural Computation</p>
<p>Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. T Schlegl, P Seeböck, S M Waldstein, U Schmidt-Erfurth, G Langs, M Niethammer, M Styner, S Aylward, H Zhu, I Oguz, Information Processing in Medical Imaging. D Shen, Yap; ChamSpringer International Publishing2017</p>
<p>Support vector method for novelty detection. B Schölkopf, R Williamson, A Smola, J Shawe-Taylor, J Platt, Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99. the 12th International Conference on Neural Information Processing Systems, NIPS'99Cambridge, MA, USAMIT Press1999</p>
<p>Anomaly detection for tabular data with internal contrastive learning. I Shavitt, E Segal, International Conference on Learning Representations. S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, 2018. 202231Advances in Neural Information Processing Systems</p>
<p>Tabular data: Deep learning is not all you need. R Shwartz-Ziv, A Armon, 8th ICML Workshop on Automated Machine Learning (AutoML). 2021</p>
<p>Attackers are not stealthy: Statistical analysis of the well-known and infamous kdd network security dataset. J V V Silva, M A Lopez, D M Mattos, 10.1109/CIoT50422.2020.92442892020 4th Conference on Cloud and Internet of Things (CIoT). 2020</p>
<p>Learning and evaluating representations for deep one-class classification. K Sohn, C.-L Li, J Yoon, M Jin, T Pfister, International Conference on Learning Representations. 2021</p>
<p>SAINT: improved neural networks for tabular data via row attention and contrastive pre-training. G Somepalli, M Goldblum, A Schwarzschild, C B Bruss, T Goldstein, CoRR, abs/2106.013422021</p>
<p>Support vector data description. Machine Learning. D Tax, R Duin, 10.1023/B:MACH.0000008084.60811.49542004</p>
<p>Measuring influence for anomaly detection. H Thimonier, F Popineau, A Rimmel, B.-L Doan, F Daniel, Tracinad, 10.1109/IJCNN55064.2022.98920582022 International Joint Conference on Neural Networks (IJCNN). 2022</p>
<p>Classification of imbalanced data: a review. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, Garnett , 10.1142/S0218001409007326Advances in Neural Information Processing Systems. S Yanmin, A Wong, M Kamel, 2017302011Curran Associates</p>
<p>Large batch optimization for deep learning: Training bert in 76 minutes. Y You, J Li, S Reddi, J Hseu, S Kumar, S Bhojanapalli, X Song, J Demmel, K Keutzer, C.-J Hsieh, International Conference on Learning Representations. 2020</p>
<p>We perform 5% T-test to test whether the difference between the highest metrics for each dataset is statistically significant. Apart from this table, Mask-KNN was not included in the computation of the mean rank. The mean rank for the F1-score of all approaches including Mask. M Zhang, J Lucas, J Ba, G E Hinton, B Zong, Q Song, M R Min, W Cheng, C Lumezanu, D Cho, H Chen, International Conference on Learning Representations. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc2019. 201832Deep autoencoding gaussian mixture model for unsupervised anomaly detection. KNN is displayed in table 13. Method Transformer NPT-AD Mask-KNN</p>            </div>
        </div>

    </div>
</body>
</html>
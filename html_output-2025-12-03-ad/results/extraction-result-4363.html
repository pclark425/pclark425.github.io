<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4363 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4363</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4363</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-280081994</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.07155v1.pdf" target="_blank">Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics</a></p>
                <p><strong>Paper Abstract:</strong> We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this purpose.The RAG configurations are manually evaluated by a human expert, that is, a total of 945 generated answers were assessed. We find that currently the best RAG agent configuration is with OpenAI embedding and generative model, yielding 91.4\% accuracy. Using our human evaluation results we calibrate LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human evaluation. These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics (e.g., cmbagent presented in a companion paper) and provide us with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system publicly available for further use by the astrophysics community.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4363.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4363.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciRag</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciRag RAG Implementation Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular pipeline introduced in this paper that integrates document preprocessing (including OCR), semantic/hybrid retrieval backends, and multi-provider LLM generation to systematically deploy and benchmark Retrieval-Augmented Generation agents for scientific discovery in cosmology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciRag</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Multi-stage pipeline that preprocesses documents (OCR optional), chunks and indexes a document corpus (here: 5 papers), supports semantic-only and hybrid retrieval (ChromaDB, file search tools, similarity thresholds), retrieves top-k chunks (k=20 typical), and conditions deterministic LLM generation (temperature=0.01, top-k sampling) to answer domain questions. SciRag orchestrates multiple backends (OpenAI file search + embeddings, VertexAI embeddings, Gemini embeddings, PaperQA2) and evaluation hooks (human expert + LLM-as-a-Judge), enabling controlled comparison of retrieval strategies (keyword+semantic reranking, query rewriting, parallel searches) and generation models for extracting factual numerical quantities and relationships from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4.1 (generation), text-embedding-3-large (OpenAI embeddings), gemini-2.5-flash-preview-05-20 (generation), text-embedding-005 (VertexAI), gemini/text-embedding-001 (Gemini embeddings) — multiple models supported by the pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics / cosmology</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5 papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>cosmological parameters (numerical values), simulation parameter dependencies, observational constraints and parameter relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>textual answers containing numeric values and citations; some assistant prompts require JSON with fields ("answer","sources") for structured output</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>human expert evaluation on 105 QA pairs (945 generated answers) and calibration/validation against LLM-as-a-Judge evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy across SciRag agent configs: OpenAI embedding+GPT-4.1: 91.4% (best); OpenAI family: 89.5-91.4%; VertexAI: 86.7%; HybridOAIGem: 85.7%; HybridGemGem: 84.8%; PaperQA2: 81.9%; Gemini baseline: 16.2%; Perplexity: 17.1% (all on CosmoPaperQA benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Non-RAG baselines (Gemini, Perplexity) performed poorly (≈16–17% accuracy) vs RAG configurations (≈82–91%); hybrid architectures approached commercial RAG accuracy (~84.8–85.7%) with much lower cost</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>limited corpus size (5 papers) may overestimate retrieval performance vs real-world corpora; explicit question references to source papers can give retrieval cues; LLM hallucination and knowledge cutoffs remain issues; summarization steps (in some systems) can dilute numeric facts; judge models show systematic biases (OpenAI conservative, Gemini optimistic).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4363.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4363.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2 RAG Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An academic retrieval-augmented generative agent that performs retrieval, summarization, and generation with GPT-4.1; used here as a benchmark academic RAG tool and adapted (Modified PaperQA2) with domain prompts for cosmology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrievalaugmented generative agent for scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Standard academic RAG pipeline using GPT-4.1 across search, summarization, and generation stages; processes OCR-enhanced documents with semantic-only search, evidence retrieval k=30 (default) and maximum 5 citations per response. A modified variant reduces k (to k=10) and applies specialized astronomical prompts and citation protocols to focus responses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>applied here to astrophysics/cosmology (originally designed for scientific literature generally / biology benchmarks cited)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5 papers (in this evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>extraction of factual parameters and synthesis across literature (numerical parameter values, reported constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>summaries and concise textual answers with up to 5 cited evidence snippets; Modified version returns similar JSON/concise outputs per prompt constraints</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>human expert evaluation on CosmoPaperQA and LLM-as-a-Judge calibration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy: 81.9% on CosmoPaperQA (PaperQA2); Modified PaperQA2 variations reported in comparisons (e.g., differing judge scores vs human)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperformed compared to top commercial RAGs (OpenAI 89.5–91.4%, VertexAI 86.7%) by ≈4.8–9.5 percentage points, attributed to summarization potentially diluting factual details</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>summarization pipelines can omit or dilute precise numeric facts needed for expert-level QA; tuning retrieval k and prompts required for domain focus</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4363.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4363.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pathfinder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pathfinder (semantic framework for literature review and knowledge discovery in astronomy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific retrieval framework that applies query expansion, reranking, and domain weighting to improve retrieval effectiveness for literature review and knowledge discovery in astronomy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>pathfinder</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Implements query expansion, reranking, and domain-specific weighting schemes to prioritize relevant passages for downstream LLM synthesis; designed to improve the retrieval stage that feeds generative models tasked with extracting scientific facts and relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>supports extraction of quantitative facts (numerical parameters, constraints) by improving retrieval relevance</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>retrieved document chunks/ ranked passages that serve as input to LLM generation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>described as improving retrieval in related work; no experimental metrics provided within this paper</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4363.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4363.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMaaJ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge (calibrated evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based evaluator system calibrated against human expert judgments to provide scalable, proxy evaluations of generated scientific answers; introduced and calibrated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-as-a-Judge (LLMaaJ)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Binary accuracy evaluation prompt (0 or 100) that directs judge models to assess whether a generated answer captures the core factual content of a validated ideal answer; calibrated on 945 human expert labels (9 agents × 105 QA) and implemented with chain-of-thought prompting to improve concordance. Two judge models were used to probe bias: OpenAI o3 mini (conservative scoring) and Gemini gemini-2.5-pro-preview-06-05 (tends to overrate).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>OpenAI o3 mini; Gemini gemini-2.5-pro-preview-06-05</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics / cosmology evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>used to evaluate 105 QA pairs derived from 5 papers; calibrated against 945 human-evaluated responses</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>numerical evaluation scores (binary mapped to 0 or 100, then scaled 0–100 for comparisons) with rationale</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>direct comparison to human expert evaluations; concordance reported (Pearson r > 0.99 for ranking), and systematic biases quantified (OpenAI judge conservative by 2–8% vs humans; Gemini judge overestimates by 5–15 points)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Judge-specific patterns: OpenAI judge gave lower absolute scores (safe lower bounds), Gemini judge higher; ranking preserved across human and AI judges (high Pearson correlation). Exact per-agent ranges reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against human expert labels (the ground truth for evaluation); served as a proxy to scale evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM evaluators can be biased toward responses from models similar to themselves; calibration requires a substantial human-labeled seed and careful prompt engineering; binary scoring prohibits partial credit.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4363.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4363.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Assistant (file search RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Assistant with file search tool (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial RAG configuration used in this study combining OpenAI embeddings and generation with a file search tool that applies query rewriting, parallel searches, keyword+semantic search, and result reranking to produce high-accuracy scientific answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>OpenAI Assistant (file-search RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses OpenAI text-embedding-3-large for vector embeddings and GPT-4.1 for generation; employs OpenAI file search tooling that automatically rewrites queries, performs parallel semantic/keyword searches, applies reranking, and uses configurable retrieval thresholds (e.g., similarity threshold = 0.5). Deterministic generation settings (temperature=0.01) and retrieval of top-k chunks feed the generator to produce concise, citation-backed answers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4.1 (generation); text-embedding-3-large (embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics / cosmology</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5 papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>cosmological parameter values, observational constraints and related quantitative facts</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>textual answers with numeric values and citations; in some prompts JSON with 'answer' and 'sources' required</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>human expert evaluation and LLMaaJ calibration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy: 91.4% for the best OpenAI configuration on CosmoPaperQA (105 QA pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed hybrid and academic RAGs in accuracy (e.g., VertexAI 86.7%, PaperQA2 81.9%) at higher operational cost</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>higher operational cost compared to hybrid alternatives; performance may degrade on much larger, noisier corpora and relies on multi-faceted file-search tooling</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4363.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4363.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HybridOAIGem / HybridGemGem architectures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-platform hybrid RAG architectures that combine local vector stores (ChromaDB) with different embedding and generation model pairings to trade off cost and performance for literature-derived QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>HybridOAIGem / HybridGemGem</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Hybrid setups using ChromaDB for chunk storage and semantic-only search: HybridGemGem uses Gemini text-embedding-001 for embeddings with gemini-2.5-flash-preview-05-20 for generation; HybridOAIGem uses OpenAI text-embedding-3-large for embeddings with Gemini generation. Retrieval is semantic-only (no OpenAI file-search reranker) with top-k chunk retrieval feeding generation; designed for cost-efficiency while retaining high accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gemini-2.5-flash-preview-05-20 (generation); text-embedding-001 (Gemini embeddings); text-embedding-3-large (OpenAI embeddings for HybridOAIGem)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics / cosmology</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5 papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>numerical cosmological parameters and simulation parameter settings/relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>textual answers with citations drawn from retrieved chunks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>human expert evaluation on CosmoPaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy: HybridOAIGem 85.7%; HybridGemGem 84.8% on CosmoPaperQA. Reported per-query cost: HybridOAIGem ≈ $0.003182, HybridGemGem ≈ $0.003806 (paper reports these hybrid cost figures as much lower than OpenAI systems).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Competitive with commercial RAGs (within ≈4–6 percentage points) while reducing operational cost significantly (paper reports ~93% cost reduction vs OpenAI configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Semantic-only retrieval underperforms the multi-faceted OpenAI file-search approach; may benefit from hybrid sparse-dense retrieval, query decomposition, or reranking to close gap</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4363.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4363.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity Assistant (web-search retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A web-search-enabled baseline that relies on real-time web retrieval (no local vector store) and a sonar-reasoning-pro model; included to evaluate the limits of web search vs RAG in extracting precise scientific facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Perplexity Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Relies entirely on web retrieval using sonar-reasoning-pro for reasoning over retrieved web content; no local vector storage used. In this study Perplexity was constrained to answer using only the specified 5 papers (per prompt), but its architecture represents search-first, non-RAG approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>sonar-reasoning-pro</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics / cosmology (evaluation constrained to 5 papers)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5 papers (retrieval constrained in prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>factual retrieval from web/document sources (numeric facts)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>very concise textual answers with mandatory JSON/markdown formatting in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy: 17.1% on CosmoPaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performed comparably poorly to non-RAG Gemini baseline (≈16%) and far below RAG systems (≈82–91%)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Unfiltered web search and no local RAG integration insufficient for expert-level, precise scientific QA; constrained formatting and sentence limits in prompts further restrict detailed numeric reporting</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4363.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4363.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini Assistant (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini Assistant (non-RAG baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that uses Google's Gemini model without RAG infrastructure (no external retrieval), relying on pre-trained knowledge and in-context information about listed papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Gemini Assistant (non-RAG baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Direct utilization of the Gemini family model (gemini-2.5-flash-preview-05-20) to answer cosmology questions from pre-trained model knowledge and in-context prompts listing key papers; lacks explicit retrieval over a vector index or document reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gemini-2.5-flash-preview-05-20</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics / cosmology</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5 papers (listed in context/in-prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>pre-trained model outputs of numeric facts (parameters) and qualitative relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>textual answers (sometimes constrained by prompt to JSON/markdown)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy: 16.2% on CosmoPaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Very low performance compared to RAG-enabled agents (≈16% vs ≈82–91%) demonstrating need for external retrieval to extract precise literature facts</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No document retrieval causes hallucinations and inability to access paper-specific numeric details; poor factual accuracy for specialist queries</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paperqa: Retrievalaugmented generative agent for scientific research <em>(Rating: 2)</em></li>
                <li>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy <em>(Rating: 2)</em></li>
                <li>LitQA2: A scientific literature question answering dataset <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4363",
    "paper_id": "paper-280081994",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "SciRag",
            "name_full": "SciRag RAG Implementation Pipeline",
            "brief_description": "A modular pipeline introduced in this paper that integrates document preprocessing (including OCR), semantic/hybrid retrieval backends, and multi-provider LLM generation to systematically deploy and benchmark Retrieval-Augmented Generation agents for scientific discovery in cosmology.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SciRag",
            "method_description": "Multi-stage pipeline that preprocesses documents (OCR optional), chunks and indexes a document corpus (here: 5 papers), supports semantic-only and hybrid retrieval (ChromaDB, file search tools, similarity thresholds), retrieves top-k chunks (k=20 typical), and conditions deterministic LLM generation (temperature=0.01, top-k sampling) to answer domain questions. SciRag orchestrates multiple backends (OpenAI file search + embeddings, VertexAI embeddings, Gemini embeddings, PaperQA2) and evaluation hooks (human expert + LLM-as-a-Judge), enabling controlled comparison of retrieval strategies (keyword+semantic reranking, query rewriting, parallel searches) and generation models for extracting factual numerical quantities and relationships from papers.",
            "llm_model_used": "GPT-4.1 (generation), text-embedding-3-large (OpenAI embeddings), gemini-2.5-flash-preview-05-20 (generation), text-embedding-005 (VertexAI), gemini/text-embedding-001 (Gemini embeddings) — multiple models supported by the pipeline",
            "scientific_domain": "astrophysics / cosmology",
            "number_of_papers": "5 papers",
            "type_of_quantitative_law": "cosmological parameters (numerical values), simulation parameter dependencies, observational constraints and parameter relationships",
            "extraction_output_format": "textual answers containing numeric values and citations; some assistant prompts require JSON with fields (\"answer\",\"sources\") for structured output",
            "validation_method": "human expert evaluation on 105 QA pairs (945 generated answers) and calibration/validation against LLM-as-a-Judge evaluators",
            "performance_metrics": "Human-evaluated accuracy across SciRag agent configs: OpenAI embedding+GPT-4.1: 91.4% (best); OpenAI family: 89.5-91.4%; VertexAI: 86.7%; HybridOAIGem: 85.7%; HybridGemGem: 84.8%; PaperQA2: 81.9%; Gemini baseline: 16.2%; Perplexity: 17.1% (all on CosmoPaperQA benchmark)",
            "baseline_comparison": "Non-RAG baselines (Gemini, Perplexity) performed poorly (≈16–17% accuracy) vs RAG configurations (≈82–91%); hybrid architectures approached commercial RAG accuracy (~84.8–85.7%) with much lower cost",
            "challenges_limitations": "limited corpus size (5 papers) may overestimate retrieval performance vs real-world corpora; explicit question references to source papers can give retrieval cues; LLM hallucination and knowledge cutoffs remain issues; summarization steps (in some systems) can dilute numeric facts; judge models show systematic biases (OpenAI conservative, Gemini optimistic).",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4363.0",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2 RAG Agent",
            "brief_description": "An academic retrieval-augmented generative agent that performs retrieval, summarization, and generation with GPT-4.1; used here as a benchmark academic RAG tool and adapted (Modified PaperQA2) with domain prompts for cosmology.",
            "citation_title": "Paperqa: Retrievalaugmented generative agent for scientific research",
            "mention_or_use": "use",
            "method_name": "PaperQA2",
            "method_description": "Standard academic RAG pipeline using GPT-4.1 across search, summarization, and generation stages; processes OCR-enhanced documents with semantic-only search, evidence retrieval k=30 (default) and maximum 5 citations per response. A modified variant reduces k (to k=10) and applies specialized astronomical prompts and citation protocols to focus responses.",
            "llm_model_used": "GPT-4.1",
            "scientific_domain": "applied here to astrophysics/cosmology (originally designed for scientific literature generally / biology benchmarks cited)",
            "number_of_papers": "5 papers (in this evaluation)",
            "type_of_quantitative_law": "extraction of factual parameters and synthesis across literature (numerical parameter values, reported constraints)",
            "extraction_output_format": "summaries and concise textual answers with up to 5 cited evidence snippets; Modified version returns similar JSON/concise outputs per prompt constraints",
            "validation_method": "human expert evaluation on CosmoPaperQA and LLM-as-a-Judge calibration",
            "performance_metrics": "Human-evaluated accuracy: 81.9% on CosmoPaperQA (PaperQA2); Modified PaperQA2 variations reported in comparisons (e.g., differing judge scores vs human)",
            "baseline_comparison": "Underperformed compared to top commercial RAGs (OpenAI 89.5–91.4%, VertexAI 86.7%) by ≈4.8–9.5 percentage points, attributed to summarization potentially diluting factual details",
            "challenges_limitations": "summarization pipelines can omit or dilute precise numeric facts needed for expert-level QA; tuning retrieval k and prompts required for domain focus",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4363.1",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "pathfinder",
            "name_full": "pathfinder (semantic framework for literature review and knowledge discovery in astronomy)",
            "brief_description": "A domain-specific retrieval framework that applies query expansion, reranking, and domain weighting to improve retrieval effectiveness for literature review and knowledge discovery in astronomy.",
            "citation_title": "pathfinder: A semantic framework for literature review and knowledge discovery in astronomy",
            "mention_or_use": "mention",
            "method_name": "pathfinder",
            "method_description": "Implements query expansion, reranking, and domain-specific weighting schemes to prioritize relevant passages for downstream LLM synthesis; designed to improve the retrieval stage that feeds generative models tasked with extracting scientific facts and relationships.",
            "llm_model_used": null,
            "scientific_domain": "astronomy",
            "number_of_papers": null,
            "type_of_quantitative_law": "supports extraction of quantitative facts (numerical parameters, constraints) by improving retrieval relevance",
            "extraction_output_format": "retrieved document chunks/ ranked passages that serve as input to LLM generation",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "described as improving retrieval in related work; no experimental metrics provided within this paper",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4363.2",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "LLMaaJ",
            "name_full": "LLM-as-a-Judge (calibrated evaluators)",
            "brief_description": "An LLM-based evaluator system calibrated against human expert judgments to provide scalable, proxy evaluations of generated scientific answers; introduced and calibrated in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LLM-as-a-Judge (LLMaaJ)",
            "method_description": "Binary accuracy evaluation prompt (0 or 100) that directs judge models to assess whether a generated answer captures the core factual content of a validated ideal answer; calibrated on 945 human expert labels (9 agents × 105 QA) and implemented with chain-of-thought prompting to improve concordance. Two judge models were used to probe bias: OpenAI o3 mini (conservative scoring) and Gemini gemini-2.5-pro-preview-06-05 (tends to overrate).",
            "llm_model_used": "OpenAI o3 mini; Gemini gemini-2.5-pro-preview-06-05",
            "scientific_domain": "astrophysics / cosmology evaluation",
            "number_of_papers": "used to evaluate 105 QA pairs derived from 5 papers; calibrated against 945 human-evaluated responses",
            "type_of_quantitative_law": null,
            "extraction_output_format": "numerical evaluation scores (binary mapped to 0 or 100, then scaled 0–100 for comparisons) with rationale",
            "validation_method": "direct comparison to human expert evaluations; concordance reported (Pearson r &gt; 0.99 for ranking), and systematic biases quantified (OpenAI judge conservative by 2–8% vs humans; Gemini judge overestimates by 5–15 points)",
            "performance_metrics": "Judge-specific patterns: OpenAI judge gave lower absolute scores (safe lower bounds), Gemini judge higher; ranking preserved across human and AI judges (high Pearson correlation). Exact per-agent ranges reported in paper.",
            "baseline_comparison": "Compared against human expert labels (the ground truth for evaluation); served as a proxy to scale evaluation.",
            "challenges_limitations": "LLM evaluators can be biased toward responses from models similar to themselves; calibration requires a substantial human-labeled seed and careful prompt engineering; binary scoring prohibits partial credit.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4363.3",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "OpenAI Assistant (file search RAG)",
            "name_full": "OpenAI Assistant with file search tool (RAG)",
            "brief_description": "A commercial RAG configuration used in this study combining OpenAI embeddings and generation with a file search tool that applies query rewriting, parallel searches, keyword+semantic search, and result reranking to produce high-accuracy scientific answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "OpenAI Assistant (file-search RAG)",
            "method_description": "Uses OpenAI text-embedding-3-large for vector embeddings and GPT-4.1 for generation; employs OpenAI file search tooling that automatically rewrites queries, performs parallel semantic/keyword searches, applies reranking, and uses configurable retrieval thresholds (e.g., similarity threshold = 0.5). Deterministic generation settings (temperature=0.01) and retrieval of top-k chunks feed the generator to produce concise, citation-backed answers.",
            "llm_model_used": "GPT-4.1 (generation); text-embedding-3-large (embeddings)",
            "scientific_domain": "astrophysics / cosmology",
            "number_of_papers": "5 papers",
            "type_of_quantitative_law": "cosmological parameter values, observational constraints and related quantitative facts",
            "extraction_output_format": "textual answers with numeric values and citations; in some prompts JSON with 'answer' and 'sources' required",
            "validation_method": "human expert evaluation and LLMaaJ calibration",
            "performance_metrics": "Human-evaluated accuracy: 91.4% for the best OpenAI configuration on CosmoPaperQA (105 QA pairs)",
            "baseline_comparison": "Outperformed hybrid and academic RAGs in accuracy (e.g., VertexAI 86.7%, PaperQA2 81.9%) at higher operational cost",
            "challenges_limitations": "higher operational cost compared to hybrid alternatives; performance may degrade on much larger, noisier corpora and relies on multi-faceted file-search tooling",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4363.4",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Hybrid RAG",
            "name_full": "HybridOAIGem / HybridGemGem architectures",
            "brief_description": "Cross-platform hybrid RAG architectures that combine local vector stores (ChromaDB) with different embedding and generation model pairings to trade off cost and performance for literature-derived QA.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "HybridOAIGem / HybridGemGem",
            "method_description": "Hybrid setups using ChromaDB for chunk storage and semantic-only search: HybridGemGem uses Gemini text-embedding-001 for embeddings with gemini-2.5-flash-preview-05-20 for generation; HybridOAIGem uses OpenAI text-embedding-3-large for embeddings with Gemini generation. Retrieval is semantic-only (no OpenAI file-search reranker) with top-k chunk retrieval feeding generation; designed for cost-efficiency while retaining high accuracy.",
            "llm_model_used": "gemini-2.5-flash-preview-05-20 (generation); text-embedding-001 (Gemini embeddings); text-embedding-3-large (OpenAI embeddings for HybridOAIGem)",
            "scientific_domain": "astrophysics / cosmology",
            "number_of_papers": "5 papers",
            "type_of_quantitative_law": "numerical cosmological parameters and simulation parameter settings/relationships",
            "extraction_output_format": "textual answers with citations drawn from retrieved chunks",
            "validation_method": "human expert evaluation on CosmoPaperQA",
            "performance_metrics": "Human-evaluated accuracy: HybridOAIGem 85.7%; HybridGemGem 84.8% on CosmoPaperQA. Reported per-query cost: HybridOAIGem ≈ $0.003182, HybridGemGem ≈ $0.003806 (paper reports these hybrid cost figures as much lower than OpenAI systems).",
            "baseline_comparison": "Competitive with commercial RAGs (within ≈4–6 percentage points) while reducing operational cost significantly (paper reports ~93% cost reduction vs OpenAI configurations).",
            "challenges_limitations": "Semantic-only retrieval underperforms the multi-faceted OpenAI file-search approach; may benefit from hybrid sparse-dense retrieval, query decomposition, or reranking to close gap",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4363.5",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Perplexity Assistant",
            "name_full": "Perplexity Assistant (web-search retrieval)",
            "brief_description": "A web-search-enabled baseline that relies on real-time web retrieval (no local vector store) and a sonar-reasoning-pro model; included to evaluate the limits of web search vs RAG in extracting precise scientific facts.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Perplexity Assistant",
            "method_description": "Relies entirely on web retrieval using sonar-reasoning-pro for reasoning over retrieved web content; no local vector storage used. In this study Perplexity was constrained to answer using only the specified 5 papers (per prompt), but its architecture represents search-first, non-RAG approaches.",
            "llm_model_used": "sonar-reasoning-pro",
            "scientific_domain": "astrophysics / cosmology (evaluation constrained to 5 papers)",
            "number_of_papers": "5 papers (retrieval constrained in prompts)",
            "type_of_quantitative_law": "factual retrieval from web/document sources (numeric facts)",
            "extraction_output_format": "very concise textual answers with mandatory JSON/markdown formatting in prompts",
            "validation_method": "human expert evaluation",
            "performance_metrics": "Human-evaluated accuracy: 17.1% on CosmoPaperQA",
            "baseline_comparison": "Performed comparably poorly to non-RAG Gemini baseline (≈16%) and far below RAG systems (≈82–91%)",
            "challenges_limitations": "Unfiltered web search and no local RAG integration insufficient for expert-level, precise scientific QA; constrained formatting and sentence limits in prompts further restrict detailed numeric reporting",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4363.6",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Gemini Assistant (baseline)",
            "name_full": "Gemini Assistant (non-RAG baseline)",
            "brief_description": "A baseline approach that uses Google's Gemini model without RAG infrastructure (no external retrieval), relying on pre-trained knowledge and in-context information about listed papers.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Gemini Assistant (non-RAG baseline)",
            "method_description": "Direct utilization of the Gemini family model (gemini-2.5-flash-preview-05-20) to answer cosmology questions from pre-trained model knowledge and in-context prompts listing key papers; lacks explicit retrieval over a vector index or document reranking.",
            "llm_model_used": "gemini-2.5-flash-preview-05-20",
            "scientific_domain": "astrophysics / cosmology",
            "number_of_papers": "5 papers (listed in context/in-prompts)",
            "type_of_quantitative_law": "pre-trained model outputs of numeric facts (parameters) and qualitative relationships",
            "extraction_output_format": "textual answers (sometimes constrained by prompt to JSON/markdown)",
            "validation_method": "human expert evaluation",
            "performance_metrics": "Human-evaluated accuracy: 16.2% on CosmoPaperQA",
            "baseline_comparison": "Very low performance compared to RAG-enabled agents (≈16% vs ≈82–91%) demonstrating need for external retrieval to extract precise literature facts",
            "challenges_limitations": "No document retrieval causes hallucinations and inability to access paper-specific numeric details; poor factual accuracy for specialist queries",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4363.7",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paperqa: Retrievalaugmented generative agent for scientific research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "pathfinder: A semantic framework for literature review and knowledge discovery in astronomy",
            "rating": 2,
            "sanitized_title": "pathfinder_a_semantic_framework_for_literature_review_and_knowledge_discovery_in_astronomy"
        },
        {
            "paper_title": "LitQA2: A scientific literature question answering dataset",
            "rating": 2,
            "sanitized_title": "litqa2_a_scientific_literature_question_answering_dataset"
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge",
            "rating": 2,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        }
    ],
    "cost": 0.020303799999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics
9 Jul 2025</p>
<p>Xueqing Xu 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Boris Bolliet 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Adrian Dimitrov 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Andrew Laverick 
Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Francisco Villaescusa-Navarro 
Center for Computational Astrophysics
Flatiron Institute
New YorkNYUSA</p>
<p>Department of Astrophysical Sciences
Prince-ton University
PrincetonNJUSA</p>
<p>Licong Xu 
Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Institute of Astronomy
Uni-versity of Cambridge
CambridgeUnited Kingdom</p>
<p>Í Ñigo Zubeldia 
Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Institute of Astronomy
Uni-versity of Cambridge
CambridgeUnited Kingdom</p>
<p>Boris Bolliet</p>
<p>Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics
9 Jul 2025712A7A925B42A9FA722DBEAE48F31A3FarXiv:2507.07155v1[astro-ph.IM]
We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this purpose. 1The RAG configurations are manually evaluated by a human expert, that is, a total of 945 generated answers were assessed.We find that currently the best RAG agent configuration is with OpenAI embedding and generative model, yielding 91.4% accuracy.Using our human evaluation results we calibrate LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human evaluation.These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics (e.g., cmbagent 2 presented in a companion paper) and provide us with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs.We make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system publicly available for further use by the astrophysics community. 3</p>
<p>Introduction</p>
<p>The rapid advancements of Large Language Models (LLMs) (Liu et al., 2024;Bai et al., 2025) have opened a new era in automated scientific discovery, where AI systems can conduct independent research and generate scientific insights (Lu et al., 2024).In cosmology, automated discovery systems are required to synthesize knowledge across collections of scientific literature, computational models, and observational datasets.The successful implementations require AI infrastructure capable of interacting with the knowledge ecosystem utilized by domain experts, and a specialized computational framework that constitutes the methodological foundation.In this work, we focus on the knowledge integration aspect of automated scientific discovery, specifically targeting the information overload in modern astronomy.</p>
<p>While LLMs have demonstrated impressive capabilities in scientific text analysis (Zhang et al., 2024), their deployment in critical research scenarios remains constricted (Fouesneau et al., 2024), by hallucination (Huang et al., 2025) and knowledge cut-off (Cheng et al., 2024).Retrieval-Augmented Generation (RAG) has emerged as a powerful tool to enhance LLMs' performance with external knowledge (Lewis et al., 2021) to meet scientific accuracy standards.The efficacy of this approach has been demonstrated in biology, where PaperQA2 RAG Agents (Lála et al., 2023;Skarlinski et al., 2024) achieve superhuman performance on LitQA2 (Futurehouse, 2024), a benchmark designed to evaluate knowledge synthesis in real research scenarios.Despite these successes in biology, systematic evaluation of RAG agents in astronomy remains limited by the lack of standardized benchmarks.As annotated by Bowman et al. (Bowman et al., 2015), developing human-annotated benchmarks for doctoral-level scientific research domains remains economically prohibitive.Consequently, evaluation of RAG agents in astronomy is constrained by the absence of authentic evaluation datasets that capture the complexity of real research scenarios.</p>
<p>To address these challenges, we introduce CosmoPaperQA, a high-quality benchmark dataset including 105 expert-curated question-answer pairs derived from five highly-cited cosmological literature.Unlike synthetic benchmarks, Cos-moPaperQA captures authentic research scenarios by extracting questions directly from research papers.</p>
<p>To facilitate a comprehensive and reproducible evaluation of CosmoPaperQA, we develop SciRag, a modular framework designed for systematic integration and benchmarking of multiple RAG Agents for scientific discovery.Our implementation enables evaluation across commercial APIs (Ope-nAI Assistant, VertexAI Assistant), hybrid architectures (ChromaDB with several embedding models), specialized academic tools (PaperQA2), and search-enhanced systems (Perplexity), providing empirical guidance for optimal RAG configuration selection in scientific contexts.</p>
<p>Our systematic evaluation across SciRag implementations reveals significant performance differences across four configuration categories, with commercial solutions (OpenAI Assistant: 89.5-91.4%,VertexAI Assistant: 86.7%) achieving the highest accuracy on CosmoPaperQA.Hybrid architectures (HybridOAIGem: 85.7%, HybridGemGem: 84.8%) show competitive performance while significantly reducing operational costs.Academic tools PaperQA2 (81.9%) show solid performance but lag behind commercial and hybrid SciRag Agents, while baseline approaches (Gemini Assistant: 16.2%, Perplexity Assistant: 17.1%) prove insufficient for expert-level scientific inquiry.</p>
<p>We present four primary contributions that collectively advance the state of RAG evaluation in cosmology:</p>
<p>Benchmark Development: We introduce CosmoPaperQA, a comprehensive benchmark dataset containing 105 expertvalidated question-answer pairs.Implementation Pipeline: We develop SciRag, a modular framework that enables systematic deployment and reproducible comparison of diverse RAG solutions.</p>
<p>Multi-System RAG Performance Analysis: We conduct a systematic evaluation of nine distinct RAG implementations utilizing high-performing LLMs and embedding models, revealing significant performance variations across different system architectures and cost-efficiency trade-offs for scientific applications.</p>
<p>Calibrated AI Judge Evaluation:</p>
<p>We introduce a LLMas-a-Judge (LLMaaJ) system that matches human expert assessment in astronomy, enabling scalable performance evaluation while maintaining the quality standards required for scientific applications.</p>
<p>Related Work</p>
<p>RAG Agents in Cosmology</p>
<p>Recent work has demonstrated the significant potential of LLMs in astronomical research contexts.Ciucȃ et al. (Ciucȃ et al., 2023) showed that through in-context learning and adversarial prompting, LLMs can synthesize diverse astronomical information into coherent and innovative hypotheses, while Shao et al. (Shao et al., 2024) demonstrated their effectiveness in extracting specialized knowledge entities from astrophysics journals using carefully designed prompting strategies.These capabilities have motivated the development of specialized RAG frameworks for astronomy, such as the pathfinder system by Iyer et al. (Iyer et al., 2024), which implements query expansion, reranking, and domain-specific weighting schemes to enhance retrieval performance in scientific applications.</p>
<p>However, the growing deployment of RAG systems in astronomy has highlighted the critical need for systematic evaluation methodologies.Wu et al. (Wu et al., 2024) addressed this challenge by proposing a dynamic evaluation framework using a Slack-based chatbot that retrieves information from arXiv astro-ph papers, emphasizing the importance of real-world user interactions over static benchmarks.While their approach provides valuable insights into user behavior and system usability, it relies on user feedback and reaction data rather than systematic performance assessment against validated ground-truth, highlighting a complementary need for standardized benchamrks that can provide consistent, reproducible evaluation metrics across different RAG implementations.</p>
<p>Benchmarks and Evaluation in Cosmology</p>
<p>Existing evaluation falls into two categories, each with some limitations:</p>
<p>Astronomy-Specific Knowledge Benchmarks: AstroM-Lab 1 (Ting et al., 2024) provides the first comprehensive astronomy-specific evaluation with 4425 AI-generated multiple-choice questions from Annual Review articles.While demonstrating significant performance variations between models with specialized astronomical knowledge, its multiple-choice format and automated question generation limit evaluation to content mastery rather than scientific inquiry workflows.Similarly, Astro-QA (Li et al., 2025) provides a structured evaluation with 3082 questions spanning diverse astronomical topics, demonstrating the application of LLMaaJ evaluation in astronomical contexts.However, its synthetic questions limit its ability to assess the complex, open-ended reasoning required for an authentic scientific research workflow.</p>
<p>General Scientific Evaluation: Broader scientific benchmarks like LitQA2 (Futurehouse, 2024) (Zhong et al., 2025), ScisummNet (Yasunaga et al., 2019) are designed for other scientific domains and may not capture astronomy-specific challenges such as mathematical reasoning about cosmological models, and interpretation of observational constraints.</p>
<p>Methodology</p>
<p>To enable AI systems to interact effectively with domain experts' knowledge bases in astrophysics, we present a comprehensive framework consisting of four integrated components designed to systematically evaluate RAG Agents.</p>
<p>CosmoPaperQA: Benchmark for Authentic Research Scenarios</p>
<p>To address the evaluation challenges identified in the previous section, we manually construct CosmoPaperQA.</p>
<p>We systematically selected five highly influential papers spanning critical areas of modern cosmology: the Planck 2018 cosmological parameters (Aghanim et al., 2020), CAMELS machine learning simulations (Villaescusa-Navarro et al., 2021;2022), local Hubble constant measurements (Riess et al., 2016), and recent Atacama Cosmology Telescope constraints (Calabrese et al., 2025).This curation ensures comprehensive coverage of observational, theoretical, and computational aspects of modern cosmological research.</p>
<p>A team of expert cosmologists generated 105 questionanswer pairs through a rigorous protocol designed to mir-ror research inquiries.The questions in our dataset span multiple complexity levels: (1) factual retrieval requiring specific parameter extraction, (2) synthetic reasoning requiring integration across multiple evidence sources, and (3) analytical interpretation requiring deep domain knowledge.</p>
<p>Each pair underwent expert validation to ensure scientific accuracy and representativeness of real research scenarios, distinguishing our benchmark from synthetic alternatives that lack authentic complexity.</p>
<p>Hence, CosmoPaperQA is designed for the following evaluations: zero-shot learning, answering without prior training on specific question types; open-ended questions, mirroring research scenarios; and multi-source knowledge synthesis, requiring integration across observational, theoretical, and computational domains.</p>
<p>SciRag: RAG Implementation Pipeline</p>
<p>Our preprocessing pipeline addresses the requirements of astronomical literature through multi-stage processing.Optical character recognition (OCR) integration using Mistral's advanced capabilities (Mistral AI, 2025)  All RAG systems perform retrieval over the complete corpus of 5 papers, regardless of which paper a specific question was derived from.This design tests the system's ability to identify and retrieve relevant information from the correct source paper among multiple cosmological documents.</p>
<p>We evaluate nine RAG implementations spanning commercial APIs (OpenAI, VertexAI), hybrid architectures (Chro-maDB with OpenAI/Gemini embeddings), academic tools (PaperQA2), and search-enhanced systems (Perplexity).All systems use temperature=0.01 and top-k=20 for consistent evaluation.Detailed analysis is in Appendix A.</p>
<p>Dual Evaluation Framework: Human Expert and Calibrated AI Assessment</p>
<p>To evaluate the quality of RAG Agents' responses in cosmological research contexts, we compare generated answers against expert-validated ground-truth responses to determine whether core factual claims in generated responses align with ground-truth.</p>
<p>While a single domain expert would be the optimal evaluator for this evaluation task, human-expert evaluation faces critical scalability limitations that make it impractical to evaluate across multiple RAG Agents.To address this scalability challenge, we implement a calibrated LLMaaJ system for automated response evaluation.However, we maintain scientific rigor by conducting parallel human expert evaluations on our benchmark results to validate the AI judges' performance and ensure assessment quality.Detailed evaluation setup is in Appendix B. After obtaining the scores, we scaled them to 0-100 for comparison between different system configurations.</p>
<p>Results</p>
<p>Human Evaluated Results</p>
<p>From the expert-evaluated results, we observe that the topperforming ones (OpenAIPDF, OpenAI, VertexAI) are all commercial RAGs, achieving 86.7-91.4% accuracy.Both hybrid implementations (HybridOAIGem: 85.7% , Hy-bridGemGem: 84.8% ) achieve performance competitive with commercial RAGs.PaperQA2 (81.90%) demonstrates solid performance but lags by 4.8-9.5 % compared to top performers.The poor performance of Perplexity Assistant (17.1%) and Gemini Assistant (16.2%) shows that unfiltered web search and non-RAG integration are insufficient for expert-level scientific inquiry, reinforcing the essential role of RAG Agents in scientific knowledge synthesis for autonomous scientific discovery.These clear performance distinctions between different system architectures validate CosmoPaperQA as an effective benchmark for distinguish-ing RAG agents' capabilities in authentic scientific research scenarios.</p>
<p>AI Evaluated Results</p>
<p>Evaluation Concordance: Both OpenAI and Gemini judges preserve the performance ranking observed in human evaluation.The performance gaps are preserved: baseline systems achieve 11.4-18.1% (OpenAI judge) and 16.2-31.4%(Gemini judge), while top-performing agents reach 80.0-84.8%(OpenAI judge) and 88.6-91.4% (Gemini judge).</p>
<p>Judge-Specific Patterns: The OpenAI judge demonstrates conservative scoring, consistently rating systems 2-8% lower than human experts across all categories.In contrast, the Gemini judge exhibits systematic overrating, scoring systems 5-15 percentage points higher than human evaluation (e.g., Gemini Baseline: 27.6% vs Human: 16.2%, Modified PaperQA2: 81.9% vs Human: 73.3%).This overrating pattern suggests that Gemini judge may be overly optimistic in assessing scientific accuracy.</p>
<p>For researchers seeking robust performance estimates, the OpenAI judge's conservative scoring provides a safer lower bound for system capabilities, while Gemini's optimistic scoring may overestimate real-world performance.Despite these systematic biases, the consistent ranking order across all three evaluation methods (Pearson r &gt; 0.99) demonstrates the robustness of our assessment framework.Ver-texAI demonstrates superior cost-efficiency while maintaining strong performance, while OpenAI achieves highest accuracy at a greater operational cost.Detailed cost analysis is provided in Appendix E.</p>
<p>Discussion and Future Work</p>
<p>While CosmoPaperQA represents a first step in systematic astronomical RAG evaluation, several design choices warrant discussion.Many questions explicitly reference their source papers (e.g., Cosmology From One Galaxy?questions mention the paper title, others reference Planck 2018 or ACT DR6).This was intentionally adopted to ensure clear answer provenance and facilitate rigorous evaluation.However, researchers typically formulate queries around scientific concepts without specifying source documents, and our explicit references may systematically improve RAG performance by providing retrieval cues.</p>
<p>Additionally, our five-paper corpus, while enabling expert evaluation, is more constrained than typical research contexts where systems must search thousands of papers or use web search, likely leading to degraded retrieval performance due to increased noise and irrelevant content.Future iterations should incorporate naturalistic question formulations and progressively larger document collections to test sys-tems' ability to identify relevant sources without explicit guidance and understand how accuracy scales with corpus size.</p>
<p>Our results also reveal important insights into retrieval mechanisms that drive performance differences.OpenAI Assistants (89.5-91.4%)use OpenAI's file search tool, which combines automatic query rewriting, parallel searches, keyword and semantic search, and result reranking.This multi-faceted approach outperforms simple semantic-only retrieval used in hybrid systems (84.8-85.7%).Future work should evaluate domain-specific retrieval enhancements such as hybrid sparse-dense methods, contextual chunk expansion, query decomposition strategies, and multi-hop reasoning approaches to further optimize RAG performance for scientific applications.</p>
<p>The calibrated LLMaaJ evaluators developed in this work enable the next phase of our research: building AI questioner systems that can automatically generate domain-specific questions.Our current dataset of 945 human-evaluated responses provides a valuable training foundation for developing such automated question generation capabilities, potentially scaling evaluation to much larger document corpora.</p>
<p>The evaluation framework could be extended to other scientific domains such as chemistry, biology, or materials science to demonstrate generalizability.Despite these limitations, our framework provides a foundation for more comprehensive astronomical RAG benchmarks.</p>
<p>Conclusion</p>
<p>We have evaluated 9 agent configurations on 105 Cosmology Question-Answer (QA) pairs that were built specifically for this purpose, based on 5 carefully selected papers.The papers were selected for their impact on the field and the quality of the presentation of their results, and their relevance to the autonomous discovery systems that we are building, e.g., cmbagent, presented in a companion paper.</p>
<p>The 9 agent configurations were manually evaluated by a human expert with more than 10 years of experience in the field, that is, a total of 945 generated answers were assessed.We find that currently the best RAG agent configuration uses OpenAI embedding and generative models, achieving 91.4% accuracy.VertexAI (86.7%) and hybrid architectures (84.8-85.7%)demonstrate competitive performance.These configurations outperform academic tools uch as PaperQA2 (81.9%), (Lála et al., 2023;Skarlinski et al., 2024), which we attribute to the summarization steps in such systems that may dilute specific factual information critical for our evaluation tasks.Notably, online tools like Perplexity perform poorly (17.1%), showing essentially no advantage over frontier LLMs without RAG (16.2%), indicating that unfiltered web search is insufficient for expert-level scientific inquiry.</p>
<p>Using our human evaluation results, we are able to calibrate evaluator agents which can be used as robust proxy for human evaluation.These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics and provide us with AI evaluators that can be scaled to much larger evaluation datasets.By themselves, our 945 manually evaluated QA pairs constitute a precious dataset that can serve for the calibration of future AI evaluator agents.</p>
<p>Impact Statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>A. Detailed System Configurations</p>
<p>All generation agents are configured with a temperature of 0.01 for consistent, deterministic responses, and top-k=20 (retrieving the 20 most similar document chunks per query) excluding Gemini Assistant, PaperQA2 (both versions) and Perplexity Assistant.The implementation provides both semantic search and hybrid retrieval capabilities across different backends, with specific configurations optimized for each system's strengths.Here are the configurations that we use for each assistant.</p>
<p>OpenAI Assistant: Direct implementation of OpenAI vector stores with file search tool (providing automatic query rewriting, parallel searches, keyword+semantic search, and result reranking) with text-embedding-3-large (OpenAI, 2023) for embeddings and GPT-4.1 for generation, with configurable retrieval parameters (similarity threshold=0.5).</p>
<p>OpenAIPDF Assistant: Direct PDF processing implementation without OCR preprocessing, enabling comparison of raw PDF handling versus OCR-enhanced document processing.Identical configuration to OpenAI Assistant, but operates on unprocessed PDF documents.</p>
<p>VertexAI Assistant: Google Cloud implementation using Google's text-embedding-005 for embeddings and gemini-2.5flash-preview-05-20(Google DeepMind, 2025) for generation.Creates RAG corpora through Vertex AI infrastructure with automatic document ingestion from Google Cloud Storage buckets.Supports semantic search with configurable similarity thresholds (0.5).</p>
<p>Gemini Assistant: Direct integration with Google's Gemini model gemini-2.5-flash-preview-05-20for baseline comparison without specialized RAG infrastructure.</p>
<p>HybridGemGem Assistant: Dual-Gemini implementation using Gemini's text-embedding-001 for embedding, leading embedding model on MTEB (Muennighoff et al., 2023) 4 with ChromaDB storage and gemini-2.5-flash-preview-05-20for generation.Supports ChromaDB backends with semantic-only search.</p>
<p>HybridOAIGem Assistant: Cross-platform architecture identical to HybridGemGem but specifically configured with OpenAI embeddings (text-embedding-3-large) and gemini-2.5-flash-preview-05-20,enabling comparison of embeddinggeneration combinations.</p>
<p>PaperQA2: Standard academic RAG implementation utilizing GPT-4.1 across all components (search, summarization, retrieval), evidence retrieval k=30, maximum 5 citations per response (optimal settings from original work).Processes OCR-enhanced documents with semantic-only search.</p>
<p>Modified PaperQA2: Domain-adapted version with identical technical configuration but specialized astronomical prompts and cosmological citation protocols.Uses evidence retrieval k=10 (reduced from standard k=30) for more focused responses.</p>
<p>Perplexity Assistant: Web-search enabled system using sonar-reasoning-pro model with real-time access to current literature.No local vector storage -relies entirely on web retrieval.</p>
<p>This diverse implementation suite enables comprehensive comparison across commercial, academic, and hybrid approaches, providing empirical guidance for selecting optimal RAG configurations for autonomous scientific discovery workflows.</p>
<p>B. Evaluation Setup</p>
<p>A domain expert is provided (1) a question query, (2) an ideal solution validated by experts, and (3) an RAG Agent-generated response.Then, evaluation is based on</p>
<p>Correct (1): Generated responses demonstrate factual accuracy, and capture essential scientific understanding equivalent to the ideal answer.</p>
<p>Incorrect (0): Generated responses contain errors, contradict established scientific knowledge, or fail to include all the core concepts of ideal answers.</p>
<p>After obtaining the scores, we scaled them to 0-100 for comparison between different system configurations.</p>
<p>The cosmologist who evaluated the response is a domain expert with a PhD-level degree currently working as a researcher in astronomy, astrophysics, or physics.Together with this cosmologist, we designed the evaluation criteria and pipeline to ensure alignment with authentic research standards.In total, our expert evaluated 945 responses (9 systems × 105 questions) generated by RAG Agents.</p>
<p>We explored LLM-as-a-Judge (LLMaaJ) (Gu et al., 2025;Zheng et al., 2023), an AI-based evaluation system calibrated for scientific research queries, using a binary scoring protocol aligned with human expert methodology.Our prompting experiments in Appendix D revealed that chain-of-thought, which asks models to formulate their underlying reasoning process, typically enhances evaluation accuracy and improves concordance with field expert judgments.</p>
<p>To investigate the bias of the pipeline specifically, as LLM evaluators may prefer responses generated by themselves (Dai et al., 2024), we used two LLM-as-a-Judge settings.Given that majority of generation systems utilize either OpenAI or Gemini-based agents, with the exception of the Perplexity Agent, we used the OpenAI o3 mini and Gemini gemini-2.5-propreview-06-05,reasoning models for evaluation.</p>
<p>Research Papers Document</p>
<p>Preprocessing</p>
<p>SciRag</p>
<p>CosmoPaperQA</p>
<p>Retrieval Generation</p>
<p>Retrieved Chunks</p>
<p>Factual Retrieval</p>
<p>How large is the impact of beam window functions on the 2018 spectra in the baseline Plik likelihood?</p>
<p>Synthetic Reasoning</p>
<p>What parameters and initial conditions are varied in the simulations that are run in CAMELS and how are they varied for each simulation?</p>
<p>Analytical Interpretation</p>
<p>Are the neural networks or the symbolic regression equations better at modelling the evolution of cosmological quantities with the redshift in the CAMELS results?</p>
<p>C. RAG Prompts</p>
<p>Our modified PaperQA2 prompt priorities conciseness and domain specificity for efficient human evaluation.</p>
<p>Perplexity Assistants Prompt</p>
<p>You are a scientific literature search agent specializing in cosmology.</p>
<p>We perform retrieval on the following set of papers: {paper list} Your task is to answer questions using ONLY information from these specific papers.CRITICAL: Your answer section must contain no more than 3 sentences total.Count your sentences carefully.You must search your knowledge base calling your tool.The sources must be from the retrieval only.Your response must be in JSON format with exactly these fields:</p>
<p>-"answer": Your 1-3 sentence response with citations -"sources": Array of citation numbers used (e.g., ["1", "2"]) Gemini Assistant's approach to leveraging pre-trained knowledge of specific cosmological papers without requiring external retrieval mechanisms.</p>
<p>Gemini Assistant Prompt</p>
<p>You are a scientific literature agent specializing in cosmology.</p>
<p>You have access to the following key cosmology papers in your knowledge base: {paper list} Your task is to answer cosmology questions using your knowledge of these papers and general cosmology knowledge.</p>
<p>D. CoT Prompts</p>
<p>AI judges are given the following prompt:</p>
<p>Judge Prompt</p>
<p>You are an expert scientific evaluator assessing the quality of scientific responses against reference answers.</p>
<p>Your task is to evaluate responses using one critical criterion: ACCURACY (0-100): CRITICAL: Use ONLY these two scores for accuracy:</p>
<p>-100: The answer contains the core correct factual content, concepts, and conclusions from the ideal answer -0: The answer is fundamentally wrong or contradicts the ideal answer This is a BINARY evaluation -either the answer is essentially correct (100) or fundamentally incorrect (0).No partial credit or intermediate scores allowed.</p>
<p>EVALUATION GUIDELINES:</p>
<p>-Focus ONLY on whether the main scientific concepts and conclusions are correct -Check that the core factual claims from the ideal answer are present in the generated answer -Verify the overall conceptual direction and main conclusions align -Additional correct information beyond the ideal answer is acceptable -Only award 0 if the answer contradicts the ideal answer or gets the main concepts wrong -Award 100 if the answer captures the essential correct scientific understanding Provide your evaluation with the numerical score and detailed rationale explaining why you chose 100 or 0.""" Please evaluate this system's response against the ideal answer: QUESTION: {question} GENERATED ANSWER: {generated answer} IDEAL ANSWER: {ideal answer} Evaluate based on: Accuracy (0-100): How factually correct is the answer compared to the ideal?Use the evaluate response function to provide your structured evaluation with detailed rationale.</p>
<p>E. Cost Performance Analysis</p>
<p>Cost considerations are critical for scientific research deployment, where institutions face budget constraints and researchers require sustainable access to AI-powered literature analysis tools.While our evaluation represents a controlled academic setting, understanding cost-performance trade-offs enables informed decisions for scaling RAG systems across research groups, institutions, and broader scientific communities.</p>
<p>Figure 2 .
2
Figure 2. SciRag System Architecture and CosmoPaperQA Benchmark Overview.Our framework integrates document preprocessing, retrieval mechanisms, and multi-provider generation to enable systematic evaluation of RAG Agents on astronomical literature.</p>
<p>, ChemRAG-Toolkit
100OpenAI Judge Gemini Judge Human Eval (Reference)81.983.781.980.085.784.883.790.585.778.185.786.780.088.689.584.890.591.47573.371.465.7Accuracy (%)5031.427.62518.116.217.111.40Gemini (Baseline)PerplexityModified PaperQA2PaperQA2HybridGemGem HybridOAIGemVertexAIOpenAIOpenAIPDFFigure 1. Performance comparison of SciRag Agents across three evaluation methods. Vertical dashed lines separate different configu-
ration categories: baseline systems (Gemini, Perplexity), academic RAG tools (Modified PaperQA2, PaperQA2), hybrid architectures (HybridGemGem, HybridOAIGem, VertexAI), and commercial solutions (OpenAI, OpenAIPDF).The first two entries (Gemini Baseline and Perplexity) do not perform RAG but simply rely on pre-trained LLM knowledge and, for Perplexity, built-in retrieval tools.</p>
<p>Do not use any other sources or general knowledge beyond what these papers contain.
Instructions:1. Search for information relevant to the question within the specified papers2. Provide a CONCISE answer in EXACTLY 1-3 sentences. Do not exceed 3 sentences under any circumstances.3. Add numerical references [1], [2], [3], etc. corresponding to the paper numbers listed above4. If the papers don't contain sufficient information, state this clearly in 1-2 sentences maximum5. Focus ONLY on the most important quantitative results or key findings6. Be precise, direct, and avoid any unnecessary elaboration or context</p>
<p>You are a retrieval agent.You must add precise source from where you got the answer.Your answer should be in markdown format with the following structure: <strong>Answer</strong>:{answer} <strong>Sources</strong>:{sources} You must search your knowledge base calling your tool.The sources must be from the retrieval only.You must report the source names in the sources field, if possible, the page number, equation number, table number, section number, etc.
OpenAI/Vertex Assistants PromptInstructions: 1. Answer the question based on your knowledge of cosmology and the listed papers2. Provide a CONCISE answer in EXACTLY 1-2 sentences maximum3. Add numerical references [1], [2], [3], etc. when citing the specific papers listed above4. Focus ONLY on the most important quantitative results or key findings5. Be precise, direct, and avoid any unnecessary elaborationPaper reference guide:[1] -Planck 2018 cosmological parameters[2] -CAMELS machine learning cosmology simulations[3] -Single galaxy cosmology analysis[4] -Local Hubble constant measurement (Riess et al.)[5] -Atacama Cosmology Telescope DR6 resultsCRITICAL: Your answer must be no more than 2 sentences total. Count your sentences carefully.Your response must be in JSON format with exactly these fields:-"answer": Your 1-2 sentence response with citations-"sources": Array of paper citations [1]-[5] that are relevant to your answerOpenAI/VertexAI assistants use a tool-based retrieval approach with markdown formatting, emphasising precise source andknowledge integration.
 Retrieved on 30-05-2025 <br />
AcknowledgmentsThe work of BB was partially funded by an unrestricted gift from Google, the Cambridge Centre for Data-Driven Discovery Accelerate Programme and the Infosys-Cambridge AI Centre.We are very grateful to the referees and panel of the ICML 2025 ML4ASTRO workshop for reviewing and accepting our work.Author ContributionsXX led the work and wrote the paper.BB led the work, supervised XX and AD, and provided the human evaluation for all the 945 answers.AD created the CosmoPaperQA benchmark dataset.AL, FVN, LX and IZ provided crucial input at various stages of this work.Modified PaperQA2 PromptProvide a concise answer in 1-2 sentences maximum.Context (with relevance scores):{context} Question: {question} Write a concise answer based on the context, focusing on astronomical facts and concepts.If the context provides insufficient information, reply {CANNOT ANSWER PHRASE}.Write in the style of a scientific astronomy reference, with precise and factual statements.The context comes from a variety of sources and is only a summary, so there may be inaccuracies or ambiguities.{prior answer prompt} Answer (maximum one sentence):In contrast, the original prompt emphasizes comprehensive information synthesis, mandatory citation and Wikipedia-style formatting.PaperQA2 PromptAnswer the question below with the context.Write in the style of a Wikipedia article, with concise sentences and coherent paragraphs.The context comes from a variety of sources and is only a summary, so there may inaccuracies or ambiguities.If quotes are present and relevant, use them in the answer.This answer will go directly onto Wikipedia, so do not add any extraneous information.{prior answer prompt} Answer ({answer length}):The Hybrid SciRag assistant adopt a structured approach, requiring a JSON format return for consistent response parsing.Hybrid Assistants PromptYou are a helpful assistant.Answer based on the provided context.You must respond in valid JSON format with the following structure: { "answer": "your detailed answer here", "sources": ["source1", "source2", "source3"]} The sources must be from the <strong>Context</strong> material provided.Include source names, page numbers, equation numbers, table numbers, section numbers when available.Ensure your response is valid JSON only.The Perplexity assistant uses web search to specific papers while utilizing its real-time retrieval capabilities.For our 105-question evaluation, total costs ranged from $0.037 (VertexAI) to $5.12 (GPT-4.1 based systems), representing a 137× cost difference.The cost differences reflect underlying model pricing structures: GPT-4.1 costs $0.002 per 1K input tokens and $0.008 per 1K output tokens, while Gemini 2.5 Flash charges $0.00015 per 1K input tokens and $0.0006 per 1K output tokens.For a typical research corpus of 1,000 papers with 10,000 queries, projected costs would range from $35.7 (VertexAI) to $4,880 (OpenAI systems).Hybrid approaches (HybridOAIGem: $0.003182, HybridGemGem: $0.003806) provide compelling cost-performance balance, achieving 84.8-85.7%accuracy while reducing costs by 93% compared to OpenAI systems.This positions them as practical solutions for resource-constrained research environments requiring both high accuracy and operational sustainability.Figure3synthesizes these trade-offs across performance, cost efficiency, and overall value.While OpenAI systems achieve highest accuracy (89.5-91.4%),their poor cost efficiency limits practical deployment scalability.Conversely, VertexAI maximizes value by combining strong performance with exceptional cost efficiency, making it suitable for widespread institutional adoption.
Planck2018 results: Vi. cosmological parameters. N Aghanim, 10.1051/0004-6361/201833910Astronomy and Astrophysics. 1432-0746641A6September 2020</p>
<p>S Bai, Qwen2.5-vl technical report. 2025</p>
<p>A large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalSeptember 2015Association for Computational Linguistics</p>
<p>The atacama cosmology telescope: Dr6 constraints on extended cosmological models. E Calabrese, 2025</p>
<p>Dated data: Tracing knowledge cutoffs in large language models. J Cheng, M Marone, O Weller, D Lawrie, D Khashabi, B V Durme, 2024</p>
<p>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. I Ciucȃ, Y.-S Ting, S Kruk, K Iyer, 2023</p>
<p>Neural retrievers are biased towards llm-generated content. S Dai, Y Zhou, L Pang, W Liu, X Hu, Y Liu, X Zhang, G Wang, J Xu, 10.1145/3637528.3671882Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24New York, NY, USA20249798400704901Association for Computing Machinery</p>
<p>What is the role of large language models in the evolution of astronomy research?. M Fouesneau, 2024</p>
<p>Litqa2: A scientific literature question answering dataset. Futurehouse, 2024</p>
<p>. Google Deepmind, Gemini, </p>
<p>J Gu, X Jiang, Z Shi, H Tan, X Zhai, C Xu, W Li, Y Shen, S Ma, H Liu, S Wang, K Zhang, Y Wang, W Gao, L Ni, J Guo, A survey on llm-as-a-judge. 2025</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, T Liu, 10.1145/3703155ACM Transactions on Information Systems. 1558-2868432January 2025</p>
<p>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy. K G Iyer, 10.3847/1538-4365/ad7c43The Astrophysical Journal Supplement Series. 1538-4365275238November 2024</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Tau Yih, T Rocktäschel, S Riedel, D Kiela, 2021</p>
<p>An astronomical question answering dataset for evaluating large language models. J Li, F Zhao, P Chen, 10.1038/s41597-025-04613-9Scientific Data. 122025</p>
<p>A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, C Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, 2024</p>
<p>Paperqa: Retrievalaugmented generative agent for scientific research. J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, 2023</p>
<p>Mistral ocr: Introducing the world's best document understanding api. A I Mistral, 2025. June 6, 2025</p>
<p>MTEB: Massive text embedding benchmark. N Muennighoff, N Tazi, L Magne, N Reimers, Proceedings of the 17th Conference of the European Chapter. A Vlachos, I Augenstein, the 17th Conference of the European Chapterthe Association for Computational Linguistics</p>
<p>Association for Computational Linguistics. Croatia Dubrovnik, doi: 10.18653May 2023</p>
<p>URL. </p>
<p>New embedding models and api updates. Openai, 2023</p>
<p>A 2.4. A G Riess, L M Macri, S L Hoffmann, D Scolnic, S Casertano, A V Filippenko, B E Tucker, M J Reid, D O Jones, J M Silverman, R Chornock, P Challis, W Yuan, P J Brown, R J Foley, 10.3847/0004-637X/826/1/56The Astrophysical Journal. 1538-4357826156July 2016</p>
<p>Astronomical knowledge entity extraction in astrophysics journal articles via large language models. W Shao, P Ji, D Fan, Y Hu, X Yan, C Cui, L Mi, L Chen, R Zhang, 2024</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. M D Skarlinski, S Cox, J M Laurent, J D Braza, M Hinks, M J Hammerling, M Ponnapati, S G Rodriques, A D White, 2024</p>
<p>Astromlab 1: Who wins astronomy jeopardy!?. Y.-S Ting, 2024</p>
<p>The camels project: Cosmology and astrophysics with machine-learning simulations. F Villaescusa-Navarro, 10.3847/1538-4357/abf7baThe Astrophysical Journal. 1538-4357915171July 2021</p>
<p>Cosmology with one galaxy?. F Villaescusa-Navarro, 10.3847/1538-4357/ac5d3fThe Astrophysical Journal. 1538-43579292132April 2022</p>
<p>Designing an evaluation framework for large language models in astronomy research. J F Wu, 2024</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. M Yasunaga, J Kasai, R Zhang, A R Fabbri, I Li, D Friedman, D R Radev, 2019</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, 2023</p>
<p>Benchmarking retrieval-augmented generation for chemistry. X Zhong, B Jin, S Ouyang, Y Shen, Q Jin, Y Fang, Z Lu, J Han, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
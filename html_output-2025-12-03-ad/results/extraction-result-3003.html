<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3003 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3003</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3003</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-257427208</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.acl-industry.4.pdf" target="_blank">MathPrompter: Mathematical Reasoning using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3003.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3003.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MathPrompter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MathPrompter: Mathematical Reasoning using Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-and-verification pipeline that elicits algebraic expressions and Python functions from an LLM (Zero-shot-CoT), evaluates those symbolic outputs with an external interpreter (Python eval) on randomized mappings, and reports consensus answers to improve accuracy and apparent confidence on arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 DaVinci (text-davinci-002, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM (GPT-3 'DaVinci' family) used via the OpenAI completion API; the paper reports experiments using models in the 175B class (text-davinci-002 style).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step math word problems (MultiArith dataset): chained operations in arithmetic word problems (add/subtract/multiply combinations; variable substitution).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Behavioral mechanism: the model is prompted to produce explicit symbolic intermediate representations (algebraic templates and Python functions); the pipeline relies on generating surface-form symbolic computations which are then executed by an external deterministic interpreter (Python eval) and aggregated by consensus — i.e., arithmetic is performed by external computation of generated symbolic outputs rather than by assuming a reliable internal arithmetic algorithm of the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: large improvement in MultiArith accuracy when using the pipeline (reported 92.5% for MathPrompter vs 78.7% Zero-shot-CoT baseline); qualitative examples where the algebraic/Python outputs reveal intermediate steps and allow detection/correction of numeric mistakes; consensus across multiple random-evaluation runs (N ~ 5) increased stability of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors report cases where both algebraic and Pythonic outputs agree yet produce an incorrect numeric result (failure despite consensus), and note occasional correct-seeming reasoning traces with incorrect final computation; no internal representation probing or mechanistic analysis was performed, so the claim that external eval + symbolic elicitation is the effective mechanism is behavioral rather than mechanistically proven.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting interventions + external tool use: Zero-shot chain-of-thought prompting; transformation of problem to algebraic template; prompts to generate Algebraic expression and Python function; compute verification by evaluating generated expressions with Python's eval(); repeated sampling and consensus (self-consistency style).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantially improved accuracy on MultiArith (reported increase to 92.5% from 78.7% for Zero-shot-CoT baseline); reduced token usage for concise algebraic/Python outputs compared to verbose CoT; increased reliability/confidence via consensus across multiple generations and external deterministic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported MultiArith accuracies in paper: Zero-shot 17.7%; Zero-shot (PaLM 540B) 25.5%; Zero-shot-CoT 78.7%; Zero-shot-CoT (PaLM 540B) 66.1%; Zero-shot-CoT + self-consistency (PaLM 540B) 89.0%; MathPrompter (Zero-shot-CoT, 175B) 92.5%. Few-shot-CoT variants reported up to ~93.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Cases where algebraic and Pythonic outputs both agree but yield an incorrect numeric answer; single-step arithmetic computation errors despite correct intermediate reasoning; commonsense mistakes; unnecessary extra reasoning steps in baseline CoT; approach is not guaranteed correct even with multiple prompts and consensus (costly to run multiple trials).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors explicitly analogize the pipeline to human student practices (multi-verification, cross-checking) and use a symbolic/external calculator (Python eval) as a ground-truth compute step; no in-depth cognitive or representational comparison to human internal arithmetic algorithms is provided beyond this analogy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathPrompter: Mathematical Reasoning using Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3003.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3003.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step natural-language reasoning from LLMs in a zero-shot manner (e.g., by prefacing a question with 'Let's think step-by-step'), improving multi-step reasoning performance compared to vanilla zero-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to large autoregressive transformer LLMs (examples in paper: text-davinci-002 / 175B and PaLM 540B reported in baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique applied to large pre-trained autoregressive transformer LLMs; the referenced work and baselines report results for models in the 175B and 540B parameter ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (MultiArith) and other chainable reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Elicits explicit chain-of-thought (natural-language intermediate steps) from the model; arithmetic success is attributed to the model generating intermediate reasoning traces that lead to correct numerical answers rather than performing exact symbolic arithmetic internally.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large empirical gains in accuracy on MultiArith reported (paper reports Zero-shot-CoT improving accuracy from 17.7% to 78.7% for a 175B model), and illustrative reasoning traces showing staged calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors and examples note that CoT can produce long or incorrect reasoning traces and still yield wrong computations; performance varies with model family/size (e.g., reported PaLM 540B Zero-shot-CoT numbers differ), indicating the method is not uniformly reliable; no internal mechanistic proof is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting: adding an instruction to generate step-by-step reasoning (e.g., 'Let's think step-by-step') and then eliciting a final numeric answer.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Dramatic accuracy improvement over plain zero-shot prompting on multi-step arithmetic tasks, but remaining failure modes include arithmetic computation errors and verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in this paper's comparisons: Zero-shot-CoT (175B) 78.7% accuracy on MultiArith; PaLM 540B Zero-shot-CoT reported 66.1% in table; combining Zero-shot-CoT with self-consistency reported up to 89.0% (PaLM 540B in table).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Incorrect final numeric answers despite plausible step-by-step chains; occasional commonsense mistakes and extraneous steps; variability across model families/sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors compare CoT to human stepwise reasoning but point out CoT lacks checks on validity of intermediate steps; no direct claim that CoT uncovers algorithmic internal arithmetic comparable to symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathPrompter: Mathematical Reasoning using Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3003.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3003.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 DaVinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 DaVinci (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The 175B-parameter GPT-3 'DaVinci' completion engine used as the LLM backend in experiments; a large autoregressive transformer trained for next-token prediction and used here to generate algebraic expressions, Python code, and chain-of-thought answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 DaVinci / text-davinci-002 (≈175B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer large language model pre-trained with next-token prediction on massive corpora; used via OpenAI completions (Brown et al., 2020 style GPT-3 family).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (MultiArith dataset) involving chained operations, variable mapping, and numeric evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Model produces surface-form symbolic/math and code-like strings when prompted; arithmetic performance in this work is achieved by eliciting these explicit symbolic outputs and delegating exact numeric computation to an external interpreter, implying the LLM is primarily used as a generator of symbolic descriptions rather than a reliable internal precise numeric calculator.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral evidence: when prompted to output algebraic expressions and Python functions, the generated forms can be deterministically evaluated to produce correct answers, markedly improving accuracy; experiments show improved metrics when these prompts and external evaluation are used together (MathPrompter results).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The paper documents arithmetic errors and examples where correct-looking reasoning leads to incorrect final numbers; no internal probes or neuron-level analyses are provided to show that the model internally encodes arithmetic algorithms—thus claims about internal arithmetic mechanisms are not directly evidenced.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting (Zero-shot-CoT), algebraic-template prompts, Python-function generation, external compute via Python eval, multiple-sample consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>With MathPrompter-style prompting + eval consensus, the GPT-3 DaVinci instance achieved reported MultiArith accuracy of 92.5% (175B), improving substantially over vanilla Zero-shot and over Zero-shot-CoT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used in MathPrompter: 92.5% accuracy on MultiArith (reported); baseline comparisons include Zero-shot 17.7% and Zero-shot-CoT 78.7% (all numbers reported in the paper's table).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Final numeric errors despite correct intermediate expressions; agreement across generated symbolic outputs does not guarantee correctness; some commonsense or multi-step reasoning errors persist; failure cases remain even after multiple runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors explicitly use symbolic evaluation (Python eval) as a verification tool and compare the prompting+verification workflow to human verification strategies (cross-checking, multi-verification); no deeper cognitive or mechanistic equivalence to human arithmetic processing is claimed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathPrompter: Mathematical Reasoning using Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Solving general arithmetic word problems <em>(Rating: 1)</em></li>
                <li>Mawps: A math word problem repository <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3003",
    "paper_id": "paper-257427208",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "MathPrompter",
            "name_full": "MathPrompter: Mathematical Reasoning using Large Language Models",
            "brief_description": "A prompting-and-verification pipeline that elicits algebraic expressions and Python functions from an LLM (Zero-shot-CoT), evaluates those symbolic outputs with an external interpreter (Python eval) on randomized mappings, and reports consensus answers to improve accuracy and apparent confidence on arithmetic word problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 DaVinci (text-davinci-002, 175B)",
            "model_description": "Autoregressive transformer LLM (GPT-3 'DaVinci' family) used via the OpenAI completion API; the paper reports experiments using models in the 175B class (text-davinci-002 style).",
            "arithmetic_task_type": "Multi-step math word problems (MultiArith dataset): chained operations in arithmetic word problems (add/subtract/multiply combinations; variable substitution).",
            "reported_mechanism": "Behavioral mechanism: the model is prompted to produce explicit symbolic intermediate representations (algebraic templates and Python functions); the pipeline relies on generating surface-form symbolic computations which are then executed by an external deterministic interpreter (Python eval) and aggregated by consensus — i.e., arithmetic is performed by external computation of generated symbolic outputs rather than by assuming a reliable internal arithmetic algorithm of the LLM.",
            "evidence_for_mechanism": "Empirical: large improvement in MultiArith accuracy when using the pipeline (reported 92.5% for MathPrompter vs 78.7% Zero-shot-CoT baseline); qualitative examples where the algebraic/Python outputs reveal intermediate steps and allow detection/correction of numeric mistakes; consensus across multiple random-evaluation runs (N ~ 5) increased stability of outputs.",
            "evidence_against_mechanism": "Authors report cases where both algebraic and Pythonic outputs agree yet produce an incorrect numeric result (failure despite consensus), and note occasional correct-seeming reasoning traces with incorrect final computation; no internal representation probing or mechanistic analysis was performed, so the claim that external eval + symbolic elicitation is the effective mechanism is behavioral rather than mechanistically proven.",
            "intervention_type": "Prompting interventions + external tool use: Zero-shot chain-of-thought prompting; transformation of problem to algebraic template; prompts to generate Algebraic expression and Python function; compute verification by evaluating generated expressions with Python's eval(); repeated sampling and consensus (self-consistency style).",
            "effect_of_intervention": "Substantially improved accuracy on MultiArith (reported increase to 92.5% from 78.7% for Zero-shot-CoT baseline); reduced token usage for concise algebraic/Python outputs compared to verbose CoT; increased reliability/confidence via consensus across multiple generations and external deterministic evaluation.",
            "performance_metrics": "Reported MultiArith accuracies in paper: Zero-shot 17.7%; Zero-shot (PaLM 540B) 25.5%; Zero-shot-CoT 78.7%; Zero-shot-CoT (PaLM 540B) 66.1%; Zero-shot-CoT + self-consistency (PaLM 540B) 89.0%; MathPrompter (Zero-shot-CoT, 175B) 92.5%. Few-shot-CoT variants reported up to ~93.0%.",
            "notable_failure_modes": "Cases where algebraic and Pythonic outputs both agree but yield an incorrect numeric answer; single-step arithmetic computation errors despite correct intermediate reasoning; commonsense mistakes; unnecessary extra reasoning steps in baseline CoT; approach is not guaranteed correct even with multiple prompts and consensus (costly to run multiple trials).",
            "comparison_to_humans_or_symbolic": "Authors explicitly analogize the pipeline to human student practices (multi-verification, cross-checking) and use a symbolic/external calculator (Python eval) as a ground-truth compute step; no in-depth cognitive or representational comparison to human internal arithmetic algorithms is provided beyond this analogy.",
            "uuid": "e3003.0",
            "source_info": {
                "paper_title": "MathPrompter: Mathematical Reasoning using Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Zero-shot-CoT",
            "name_full": "Zero-shot Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step natural-language reasoning from LLMs in a zero-shot manner (e.g., by prefacing a question with 'Let's think step-by-step'), improving multi-step reasoning performance compared to vanilla zero-shot prompts.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "model_name": "Applied to large autoregressive transformer LLMs (examples in paper: text-davinci-002 / 175B and PaLM 540B reported in baselines).",
            "model_description": "Technique applied to large pre-trained autoregressive transformer LLMs; the referenced work and baselines report results for models in the 175B and 540B parameter ranges.",
            "arithmetic_task_type": "Multi-step arithmetic word problems (MultiArith) and other chainable reasoning tasks.",
            "reported_mechanism": "Elicits explicit chain-of-thought (natural-language intermediate steps) from the model; arithmetic success is attributed to the model generating intermediate reasoning traces that lead to correct numerical answers rather than performing exact symbolic arithmetic internally.",
            "evidence_for_mechanism": "Large empirical gains in accuracy on MultiArith reported (paper reports Zero-shot-CoT improving accuracy from 17.7% to 78.7% for a 175B model), and illustrative reasoning traces showing staged calculations.",
            "evidence_against_mechanism": "Authors and examples note that CoT can produce long or incorrect reasoning traces and still yield wrong computations; performance varies with model family/size (e.g., reported PaLM 540B Zero-shot-CoT numbers differ), indicating the method is not uniformly reliable; no internal mechanistic proof is provided.",
            "intervention_type": "Prompting: adding an instruction to generate step-by-step reasoning (e.g., 'Let's think step-by-step') and then eliciting a final numeric answer.",
            "effect_of_intervention": "Dramatic accuracy improvement over plain zero-shot prompting on multi-step arithmetic tasks, but remaining failure modes include arithmetic computation errors and verbosity.",
            "performance_metrics": "Reported in this paper's comparisons: Zero-shot-CoT (175B) 78.7% accuracy on MultiArith; PaLM 540B Zero-shot-CoT reported 66.1% in table; combining Zero-shot-CoT with self-consistency reported up to 89.0% (PaLM 540B in table).",
            "notable_failure_modes": "Incorrect final numeric answers despite plausible step-by-step chains; occasional commonsense mistakes and extraneous steps; variability across model families/sizes.",
            "comparison_to_humans_or_symbolic": "Authors compare CoT to human stepwise reasoning but point out CoT lacks checks on validity of intermediate steps; no direct claim that CoT uncovers algorithmic internal arithmetic comparable to symbolic computation.",
            "uuid": "e3003.1",
            "source_info": {
                "paper_title": "MathPrompter: Mathematical Reasoning using Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3 DaVinci",
            "name_full": "GPT-3 DaVinci (text-davinci-002)",
            "brief_description": "The 175B-parameter GPT-3 'DaVinci' completion engine used as the LLM backend in experiments; a large autoregressive transformer trained for next-token prediction and used here to generate algebraic expressions, Python code, and chain-of-thought answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 DaVinci / text-davinci-002 (≈175B parameters)",
            "model_description": "Autoregressive transformer large language model pre-trained with next-token prediction on massive corpora; used via OpenAI completions (Brown et al., 2020 style GPT-3 family).",
            "arithmetic_task_type": "Multi-step arithmetic word problems (MultiArith dataset) involving chained operations, variable mapping, and numeric evaluation.",
            "reported_mechanism": "Model produces surface-form symbolic/math and code-like strings when prompted; arithmetic performance in this work is achieved by eliciting these explicit symbolic outputs and delegating exact numeric computation to an external interpreter, implying the LLM is primarily used as a generator of symbolic descriptions rather than a reliable internal precise numeric calculator.",
            "evidence_for_mechanism": "Behavioral evidence: when prompted to output algebraic expressions and Python functions, the generated forms can be deterministically evaluated to produce correct answers, markedly improving accuracy; experiments show improved metrics when these prompts and external evaluation are used together (MathPrompter results).",
            "evidence_against_mechanism": "The paper documents arithmetic errors and examples where correct-looking reasoning leads to incorrect final numbers; no internal probes or neuron-level analyses are provided to show that the model internally encodes arithmetic algorithms—thus claims about internal arithmetic mechanisms are not directly evidenced.",
            "intervention_type": "Prompting (Zero-shot-CoT), algebraic-template prompts, Python-function generation, external compute via Python eval, multiple-sample consensus.",
            "effect_of_intervention": "With MathPrompter-style prompting + eval consensus, the GPT-3 DaVinci instance achieved reported MultiArith accuracy of 92.5% (175B), improving substantially over vanilla Zero-shot and over Zero-shot-CoT alone.",
            "performance_metrics": "When used in MathPrompter: 92.5% accuracy on MultiArith (reported); baseline comparisons include Zero-shot 17.7% and Zero-shot-CoT 78.7% (all numbers reported in the paper's table).",
            "notable_failure_modes": "Final numeric errors despite correct intermediate expressions; agreement across generated symbolic outputs does not guarantee correctness; some commonsense or multi-step reasoning errors persist; failure cases remain even after multiple runs.",
            "comparison_to_humans_or_symbolic": "Authors explicitly use symbolic evaluation (Python eval) as a verification tool and compare the prompting+verification workflow to human verification strategies (cross-checking, multi-verification); no deeper cognitive or mechanistic equivalence to human arithmetic processing is claimed.",
            "uuid": "e3003.2",
            "source_info": {
                "paper_title": "MathPrompter: Mathematical Reasoning using Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Solving general arithmetic word problems",
            "rating": 1,
            "sanitized_title": "solving_general_arithmetic_word_problems"
        },
        {
            "paper_title": "Mawps: A math word problem repository",
            "rating": 1,
            "sanitized_title": "mawps_a_math_word_problem_repository"
        }
    ],
    "cost": 0.0144815,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MathPrompter: Mathematical Reasoning using Large Language Models
July 10-12, 2023</p>
<p>Shima Imani 
Microsoft Research
RedmondUSA</p>
<p>Liang Du 
Microsoft Research
RedmondUSA</p>
<p>Harsh Shrivastava 
Microsoft Research
RedmondUSA</p>
<p>MathPrompter: Mathematical Reasoning using Large Language Models</p>
<p>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
the 61st Annual Meeting of the Association for Computational Linguistics5July 10-12, 2023
Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose 'MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chainof-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the MultiArith dataset (78.7% → 92.5%) evaluated using 175B parameter GPT-based LLM.</p>
<p>Introduction</p>
<p>Recent advancements in natural language processing (NLP) can be attributed to massive scaling of Large Language Models (LLMs) (Vaswani et al., 2017;Devlin et al., 2018;Raffel et al., 2020;Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022;Thoppilan et al., 2022). A very interesting recent discovery that the LLMs are naturally good (in-context) Zero-shot or few-shot learners turned out to be very useful (Brown et al., 2020;Liu et al., 2021Liu et al., , 2023. This led to the development of 'prompting' technique, where the user provides a small context for solving the task at-hand to the LLM. This conditioning of the models on a few examples is termed as few-shot prompting, while providing instructions to solve a task is known as Zero-shot prompting. Extensive research efforts are being poured into designing these prompts, either manually (Schick and Schütze, 2020;Reynolds and McDonell, 2021) or automatically (Shin et al., 2020;Gao et al., 2020). Although quite successful for single-step system-I tasks (Stanovich and West, 2000;Liu et al., 2023), the prompting techniques were inadequate in their performance on system-II tasks where multi-step reasoning is required (Rae et al., 2021). As humans, we tend to break down a problem and attempt to solve them step-by-step. Extending this intuition to LLMs led to the development of 'chain-of-thought' (CoT) prompting technique . The use of CoT has led to improved performance on a range of NLP tasks (Talmor et al., 2018;Gao et al., 2020;Patel et al., 2021;Cobbe et al., 2021;Geva et al., 2021;Chowdhery et al., 2022;Srivastava et al., 2022) In this work, we investigate Zero-shot-CoT methods for solving mathematical reasoning tasks. To the best of our knowledge, we found the recent work by (Kojima et al., 2022) that proposed a Zeroshot-CoT technique to be the state-of-the-art where they demonstrated a remarkable accuracy improvement on the 'MultiArith' (Roy and Roth, 2016) data (17.7% → 78.7%). Now, we identify two key aspects that lacks in the previous CoT prompting based SOTA, namely (1) Although, the chainof-thought followed by the model improved the results, but there is no check on the validity of the steps followed by the chain-of-thought prompting and (2) The confidence in the predictions of LLMs are often not provided. In order to address these gap to some extent, we derive inspiration from how we humans solve a math question by breaking it down to a simpler multi-step procedure and make use of multiple ways to validate our approach at each step. Specifically, given a question Q, (I) Generating Algebraic template: We first gen-</p>
<p>Python prompt</p>
<p>Write a python function that returns the answer. </p>
<p>Algebraic prompt</p>
<p>Method</p>
<p>Since the LLMs are generative models, it becomes very tricky to ensure that the generated answers are accurate, especially for mathematical reasoning tasks. We take clues from the process followed by students to solve arithmetic problems. We narrowed down a few steps that students take in order to verify their solutions, namely</p>
<p>• Compliance with known results: By comparing the solution to a known result, one can assess its accuracy and make necessary adjustments. This is particularly useful when the question is a standard problem with a well-established solution.</p>
<p>• Multi-verification: By approaching a problem from multiple perspectives and comparing the results helps to confirm the validity of the solution and ensure that it is both sound and accurate.</p>
<p>• Cross-checking: The process of solving a problem is just as necessary as the final answer. Verifying the correctness of the intermediate steps of the process provide a clear understanding of the thought process behind the solution.</p>
<p>• Compute verification: Utilizing a calculator or computer to perform arithmetic calculations can assist in verifying the accuracy of the final answer.</p>
<p>MathPrompter</p>
<p>Our proposed method, MathPrompter, is an attempt to transfer some of this thought process to the LLM answer generation process. Fig. 1 provides a high-level overview of steps followed by Math-Prompter to solve a mathematical reasoning problem. We use the state-of-the-art GPT-3 DaVinci completion engine (Brown et al., 2020) for the question-answering tasks. We use the following question 'Q' from the Mul-tiArith dataset to demonstrate the problem solving process followed by MathPrompter.</p>
<p>Q: At a restaurant, each adult meal costs $5 and kids eat free. If a group of 15 people came in and 8 were kids, how much would it cost for the group to eat? (I) Generating Algebraic template: We begin by transforming the question into its Algebraic form by replacing the numeric entries with variables using a key-value mapping. In this particular instance, the modified question 'Qt' becomes:</p>
<p>Qt: at a restaurant, each adult meal costs A and kids eat free. if a group of B people came in and C were kids, how much would it cost for the group to eat? Mapping: {A:5, B:15, C:8} (II) Math-prompts: We build up on the intuition provided by the multi-verification and crosschecking thought processes mentioned above. We generate analytical solutions of Qt using two different approaches, Algebraic way and Pythonic way. We give the following prompts to the LLM to generate additional context for Qt Algebraic prompt: Write a mathematical equation and generate the answer format starting with 'Answer =' Python prompt: Write a Python function that returns the answer.</p>
<p>The LLM model in response to the above prompts generated the following output expressions</p>
<h1>Algebraic expression output Answer = A*(B-C) # Python expression output def total_price(A, B, C): return A * (B-C)</h1>
<p>The above generated analytical solutions gives the user a hint into the 'intermediate thought process' of the LLM. Incorporating additional prompts will improve the accuracy and consistency of the results. This will, in turn, enhance the Math-Prompter's ability to generate more precise and effective solutions.</p>
<p>(III) Compute verification: We evaluate the expressions generated in the previous step using multiple randomized key-value mappings of the input variables in Qt. To evaluate the expressions, we used the Python's eval() method. We compare the outputs to see if we can find a consensus among the answers. This also provides us with a higher level of confidence that the answers are correct and reliable. Once the expressions agree on their outputs, we use the values of the variables in the input Q to compute the final answer, as below Algebraic-answer = 35 Pythonic-answer = 35</p>
<p>(IV) Statistical significance: In order to ensure that consensus is reached among various expressions' output, in our experiments, we repeat the steps (II) &amp; (III) for N ∼ 5 times and report the most frequent value observed for the answer.</p>
<p>Experiment</p>
<p>Dataset</p>
<p>We evaluate MathPrompter on Multi-Arith dataset (Roy and Roth, 2016), which is a subset of the Math World Problem Repository (Koncel-Kedziorski et al., 2016). This dataset is a collection of mathematical problems that are specifically designed to test the ability of machine learning models to perform complex arithmetic operations and reasoning. These problems demand the application of multiple arithmetic operations and logical reasoning to be sucessfully solved.</p>
<p>Baseline</p>
<p>One of the popular baselines is the standard Zeroshot model by (Brown et al., 2020). Their train their models in a way that it is able to recognize and classify new objects or classes that it has never seen before during training. This was achieved by utilizing the semantic relationships between classes.</p>
<p>We also compared against the state-of-the-art Zero-shot-CoT prompting model by (Kojima et al., 2022). This is a very recent approach that addresses the limitations of the standard Zero-shot learning by incorporating a 'context of the task' using CoT to improve the performance. Briefly, their method follows this procedure. Given a question Q, the authors use the prompt 'Lets think step-by-step' followed by Q to generate a response Z. Then, they use the prompt 'The answer (Arabic numericals) is' followed by Z to get their final result.  (Kojima et al., 2022). They used Textdavinci-002 (175B) model along with the same 8 examples as described in  for Few-shot and Few-shot-CoT settings.)   (Kojima et al., 2022) technique falls short, and where these can be remedied with MathPrompter , which was designed to address these issues. For example, the generated answers sometimes have one step of error, which can be avoided by running the model multiple times and reporting the consensus results.</p>
<p>Results</p>
<p>Accuracy comparisons</p>
<p>Example comparisons</p>
<p>Additionally, the reasoning steps in (Kojima et al., 2022) can be excessively lengthy, but the Pythonic or Algebraic methods can address this by typically requiring fewer tokens. Furthermore, the reasoning steps may be correct, but the final computation is incorrect. MathPrompter address problem by using the Python's eval() method function.</p>
<p>In many cases, the MathPrompter generates correct intermediate and final answers. However, there are a few cases, such as the last question in Table 2, where both the Algebraic and Pythonic outputs are in agreement, yet erroneous. We plan to address these issues by incorporating additional methods to further enhance the performance of MathPrompter .</p>
<p>Conclusions &amp; Discussions</p>
<p>We introduced MathPrompter, a novel approach that improves LLM performance on mathematical reasoning problems. It also addresses an important concern of building the user trust to some extent in the LLM predictions. We translated our intuition on how students solve arithmetic problems to a LLM model by utilizing the Zero-shot chain-of-thought prompting technique. MathPrompter incorporates ideas like cross-checking the intermediate steps and solving the same math problem using multiple approaches in its design. We empirically show that our model is comparable to SOTA Few-shot-CoT models as well as the larger Zero-shot-CoT models that have 540B parameters. In future, we plan to further evaluate performance on additional datasets and explore incorporating additional prompts into That means Jerry will need to make 1 trip to the second table. In total, Jerry will make 3 trips. Therefore, the answer (arabic numerals) is 3. ✗ </p>
<p>Limitation</p>
<p>One of the limitations of our work is that while we are running the MathPrompter multiple times in different ways to increase the accuracy of our results, this does not always guarantee the correctness of the output. Both Algebraic and Pythonic expressions have the potential to produce the incorrect results, even if the prompt outputs match each other. This is the fail case as shown in the last row of Table 2. Increasing the number of prompts will mitigate this issue. We are currently investigating techniques that can address this issue in a more principled manner.</p>
<p>bought 14 boxes of chocolate candy and gave 5 to his little brother. If each box has 6 pieces inside it, how many pieces did Kaleb still have? A: Let's think step by step. Kaleb bought 14 boxes of chocolate candy. This means that he had 84 pieces of chocolate candy. He gave 5 boxes to his little brother, which means that he gave his little brother 30 pieces of chocolate candy. Kaleb still had 54 pieces of chocolate candy. Therefore, the answer (arabic numerals) is 54 ✓.</p>
<p>1 :
1Accuracy on MultiArith dataset. MathPrompter outperforms all the Zero-shot &amp; Zero-shot-CoT baselines. We emphasize that our model's performance is comparable to 540B parameter models as well as the SOTA Few-shot-CoT approaches. (If not mentioned explicitly, the models in each row consists of 175B parameters. Results are borrowed fromModel </p>
<p>Accuracy </p>
<p>Zero-shot 
17.7 
Zero-shot (PaLM 540B) 
25.5 
Zero-shot-CoT 
78.7 
Zero-shot-CoT (PaLM 540B) 
66.1 
Zero-shot-CoT + self consistency (PaLM 540B) 
89.0 
Zero-shot-CoT (MathPrompter) 
92.5 </p>
<p>Few-Shot (2 samples) 
33.7 
Few-Shot (8 samples) 
33.8 
Few-Shot-CoT (2 samples) 
84.8 
Few-Shot-CoT (4 samples) 
90.5 
Few-Shot-CoT (8 samples) 
93.0 
Zero-Plus-Few-Shot-CoT (8 samples) 
92.8 </p>
<p>Table </p>
<p>Table 1
1compares the performance of the Math-Prompter against the baseline models. The results of few-shot &amp; zero-shot learning based approaches are shown. Furthermore, we add the results for models with different number of parameters to get better highlight the significance of our approach.Since, MathPrompter is a Zero-shot-CoT (175B 
parameters) method, we choose the state-of-the-art 
Zero-shot-CoT (175B parameters) model by (Ko-
jima et al., 2022) and a Zero-shot(175B parameters) 
by (Brown et al., 2020) for fair comparison. We 
report an accuracy of 92.5% which is a huge im-
provement to the other SOTA models with 78.7% 
and 17.7% accuracy, respectively. </p>
<p>Table 2
2presents a sample set of questions and 
their respective outputs, intermediate steps, and 
final answers generated by both MathPrompterand 
the current state-of-the-art model (Kojima et al., 
2022). For simplicity, only one output of Math-
Prompter for each question is shown for both the 
Algebraic and Pythonic outputs. 
The table highlights areas where </p>
<p>Table 2 :
2Example-based comparison between current state of art Zero-shot-CoT and MathPrompter .A (OTHERS; MAXIMUM LIMIT OF TEXT LENGTH): Let's think step by step. Each chocolate bar in a box costs $3. If a box had 9 bars total, then Wendy sold all but 3 bars. How much money would she have made? We can solve this problem using algebra. Let's start by creating a variable to represent the number of chocolate bars Wendy sold. We'll call this variable "x". If Wendy sold "x" chocolate bars, then she would have 9 -x chocolate bars left in the box. We know that each chocolate bar costs $3, so the total cost of the chocolate bars Wendy sold Therefore, the answer (arabic numerals) is: Wendy would have made $12. ✗ Q: Jerry was helping the cafeteria workers pick up lunch trays, but he could only carry 8 trays at a time. If he had to pick up 9 trays from one table and 7 trays from another, how many trips will he make?A (COMMONSENSE MISTAKE): Let's think step by step. Jerry can carry 8 trays at a time. There are 9 trays on the first table. That means Jerry will need to make 2 trips to the first table. There are 7 trays on the second table.Question 
SOTA Zero-shot-CoT 
MathPrompter </p>
<p>Q: At the fair Adam bought 13 
tickets. After riding the ferris 
wheel he had 4 tickets left. If 
each ticket cost 9 dollars, how 
much money did Adam spend 
riding the ferris wheel? </p>
<p>A (ONE UNNECESSARY STEP ERROR): Let's 
think step by step. Adam bought 13 tickets. That 
means he spent 13 * 9 = 117 dollars on tickets. 
After riding the ferris wheel, he had 4 tickets left. 
That means he used 9 tickets to ride the ferris wheel. 
That means he spent 9 * 9 = 81 dollars on riding the 
ferris wheel. So the answer is 117 -81 = 36 dollars. 
Therefore, the answer (arabic numerals) is 36. ✗ </p>
<p>Mapping = {A:13, B:4, C:9} </p>
<h1>Algabraic answer</h1>
<p>Answer = A*(B-C) </p>
<h1>Python code</h1>
<p>def total_price(A, B, C): 
return A * (B-C) </p>
<p>Answer = 81 ✓ </p>
<p>Q: Each chocolate bar in a box 
cost $3. If a box had 9 bars 
total and Wendy sold all but 3 
bars, how much money would 
she have made? </p>
<p>Mapping = {A:3, B:9, C:3} </p>
<h1>Algabraic answer</h1>
<p>Answer = A x (B -C) </p>
<h1>Python code</h1>
<p>def money_made(A, B, C): 
return (B-C)*A </p>
<p>Answer = 18 ✓ </p>
<p>Training verifiers to solve math word problems. Nakano, arXiv:2110.14168arXiv preprintNakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, arXiv:2012.15723arXiv preprintTianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 9Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346- 361.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.11916arXiv preprintTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large lan- guage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Mawps: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies. the 2016 conference of the north american chapter of the association for computational linguistics: human language technologiesRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1152-1157.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3? arXiv preprint. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 559Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35.</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191arXiv preprintArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191.</p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprintJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, et al. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 211Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. Laria Reynolds, Kyle Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Laria Reynolds and Kyle McDonell. 2021. Prompt pro- gramming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Comput- ing Systems, pages 1-7.</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, arXiv:1608.01413arXiv preprintSubhro Roy and Dan Roth. 2016. Solving gen- eral arithmetic word problems. arXiv preprint arXiv:1608.01413.</p>
<p>It's not just size that matters: Small language models are also few-shot learners. Timo Schick, Hinrich Schütze, arXiv:2009.07118arXiv preprintTimo Schick and Hinrich Schütze. 2020. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, I V Robert L Logan, Eric Wallace, Sameer Singh, arXiv:2010.15980arXiv preprintTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, R Adam, Adam Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615arXiv preprintAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>E Keith, Richard F Stanovich, West, individual differences in reasoning: Implications for the rationality debate? Behavioural and Brain Science. 24Keith E Stanovich and Richard F West. 2000. 24. indi- vidual differences in reasoning: Implications for the rationality debate? Behavioural and Brain Science, 23(5):665-726.</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937arXiv preprintAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowl- edge. arXiv preprint arXiv:1811.00937.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applica- tions. arXiv preprint arXiv:2201.08239.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.11171arXiv preprintXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency im- proves chain of thought reasoning in language mod- els. arXiv preprint arXiv:2203.11171.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>            </div>
        </div>

    </div>
</body>
</html>
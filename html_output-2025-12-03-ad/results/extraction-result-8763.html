<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8763 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8763</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8763</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-277244351</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.16814v2.pdf" target="_blank">Understanding Bias Reinforcement in LLM Agents Debate</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent De-bate (MAD) has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce MetaNIM Arena , a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD’s limitations, we propose DReaMAD ( D iverse Rea soning via M ulti-A gent D ebate with Refined Prompt), a novel framework that (1) re-fines LLMs’ strategic prior knowledge to improve reasoning quality and (2) promotes diverse view-points within a single model by systematically modifying prompts, reducing bias. Empirical re-sults show that DReaMAD significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8763.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8763.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DReaMAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diverse Reasoning via Multi-Agent Debate with Refined Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper that combines strategic prior-knowledge elicitation and perspective diversification to produce diverse agent prompts, structured multi-agent debate, and a post-debate refinement step to improve strategic decision-making and mitigate bias reinforcement in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini, GPT-4o, GEMINI-1.5-pro, GEMINI-1.5-flash, GPT-o3-mini (evaluated across multiple backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various proprietary large language models (GPT and GEMINI families) used by the authors; the paper does not provide parameter counts or training details for these specific variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>DReaMAD (Strategic Prior Knowledge Elicitation + Perspective Diversification + Debate + Post-debate refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-stage method: (1) Strategic Prior Knowledge Elicitation (SPKE) prompts the model to reinterpret the problem and produce high-level strategies (temperature 0.1 used for consistency); (2) Perspective Diversification produces distinct self-generated prompts for multiple agents (temperature 0.7 to encourage diversity). Agents then engage in a structured multi-agent debate and a final post-debate refinement revisits conclusions to improve decision quality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MetaNIM Arena (impartial combinatorial games: NIM, Fibonacci, Kayles, Chomp, Corner Queen) and additional math/Commonsense benchmarks (AIME, AMC, CommonsenseQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Adversarial strategic decision-making benchmark (MetaNIM Arena) with mathematically provable optimal moves via Sprague-Grundy theory; also evaluated on math reasoning (AIME/AMC) and CommonsenseQA for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>+12.0% accuracy gain over standard prompting on the MetaNIM Arena dataset (paper claim); +20.8% higher win rate than MAD in the simulator (paper claim). Example reported numbers: DReaMAD (−) accuracies in Table 3 (averages vary by model; e.g., GPT-4o-mini DReaMAD (−) average 0.57 accuracy), and simulator win rates in Table 4 (e.g., DReaMAD win rates reach up to 0.98 for some settings with GPT-4o-mini and aggregate improvements across variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline methods reported include Standard Prompting, ReAct, Chain-of-Thought, Self-Refinement, Self-Consistency, and MAD; relative to these, DReaMAD shows the improvements cited above (see Table 3 and Table 4). Example baselines: standard prompting and ReAct average accuracies in Table 3 (vary by model; e.g., GPT-4o-mini ReAct average 0.40 vs DReaMAD (−) 0.57).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering based modules: structured prompting to elicit prior strategic knowledge (SPKE), automated self-generation of diverse prompts per agent (perspective diversification), multi-agent debate rounds, and a post-debate refinement prompt; no additional external memory or retraining required.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: reported +12.0% accuracy over standard prompting on MetaNIM Arena and +20.8% higher win rate than MAD in simulator; Table 3 shows DReaMAD (−) yields higher optimal-action accuracy across multiple models/tasks; Table 4 shows higher win rates for DReaMAD compared to other self-correction methods. Qualitative: reduced bias amplification in debate examples (Figures 4–6) and improved convergence toward optimal moves in previously biased states.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>DReaMAD struggles in environments like Chomp where a general, concise prior strategy is difficult to define and where effective behavior requires exploratory play rather than direct elicitation of prior knowledge; inference cost includes multiple prompt steps (though authors argue test-time scaling is cheaper than fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct comparisons in the paper show DReaMAD outperforms ReAct, CoT, Self-Refinement, Self-Consistency, and standard MAD in both accuracy and win-rate metrics on MetaNIM Arena and on selected math/Commonsense tasks; DReaMAD (−) (no debate) also often outperforms CoT and ReAct, indicating SPKE contributes substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablation DReaMAD (−) (SPKE + perspective diversification but excluding debate) still yields substantial gains (reported in Table 3). The paper also applies SPKE to Self-Refinement and Self-Consistency and reports that SPKE alone consistently improves their performance (Section E.2), though exact per-method numeric deltas are not fully tabulated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Bias Reinforcement in LLM Agents Debate', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8763.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8763.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A debate-style iterative method where multiple instances/agents (often the same model) critique and counter one another's outputs, intended to surface errors and improve final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini, GPT-4o, GEMINI-1.5-pro, GEMINI-1.5-flash (used as debating agents in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multiple instances of the same LLM family used as debating agents; model internals/training not specified beyond family names.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Instantiate multiple agents (often the same model) to generate responses and critiques in multiple rounds of debate; in experiments authors ran up to three rounds of debate (process terminates early if consensus reached). Agents generate multiple responses per round (e.g., 20 responses per agent in some analyses) and distributions are compared before/after debate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MetaNIM Arena (NIM, Fibonacci) and other strategic game scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Adversarial strategic games where debate is used to refine move selection; evaluated both on static dataset examples and through an interactive simulator measuring win rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Varied by model and task; in many cases MAD amplifies pre-existing biases and does not increase optimal-action frequency. Paper reports average reinforcement increases after MAD of +9.17% for GPT models and +12.29% for GEMINI models in Fibonacci (Table 2). In simulator experiments (Table 4), MAD often yields lower win rates than DReaMAD (paper claims DReaMAD achieves +20.8% higher win rate than MAD).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (pre-debate) initial action distributions are reported; in experiments, the initial (pre-debate) frequency of optimal actions was sometimes higher than post-debate frequencies (e.g., a curated set with 80% optimal responses fell to predominance of biased responses after debate in Figure 3 experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Iterative multi-agent exchanges implemented via prompt-engineered debates among identical model instances, sampling multiple responses per agent at controlled temperature; no external verifier used in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>The paper presents evidence that MAD does not reliably improve answers in strategic settings; quantitative analysis in NIM and Fibonacci shows MAD tends to increase frequency of initially dominant (biased) actions rather than correcting them (Figures 2 and 3, Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Bias reinforcement: MAD frequently amplifies pre-existing, strongly-consistent biases (both 'good' and 'wrong' biases) rather than correcting them; lack of perspective diversity when multiple agents are identical leads to homogenized reasoning; curated high-quality inputs can be overwhelmed by dominant biased trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>MAD is compared to Self-Refinement, Self-Consistency, ReAct, CoT, and DReaMAD; DReaMAD (with prompt diversification and SPKE) outperforms MAD in accuracy and win rate. The paper cites proposals to combine different models for debate to boost diversity (Chen et al. 2023b) but focuses on single-model diversification.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Authors compare (+)MAD (identical prompts for both agents) vs DReaMAD (distinct self-generated prompts) and find perspective diversification significantly improves performance (Figure 6 left). They also show that providing optimal curated responses in debate does not prevent bias amplification under MAD (Figure 3 experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Bias Reinforcement in LLM Agents Debate', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8763.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8763.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refinement (iterative self-critique and revise)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-correction method where an LLM critiques its own output and generates revisions over multiple refinement steps to improve answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-4o-mini, GPT-o3-mini, GEMINI family (used in comparisons/experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same family LLMs as above; paper does not report parameter counts or training specifics for each model variant.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative self-review: the model generates an answer, critiques it, and revises; the authors follow Madaan et al. (2024) methodology and apply three iterative refinement steps in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MetaNIM Arena (games), math benchmarks (AIME/AMC), CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Strategic game decision tasks and standard reasoning benchmarks used to measure whether iterative self-refinement improves solution quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Mixed or degraded results in some settings. Example: on CommonsenseQA (Table 6) Self-Refinement yields 49.2% for GPT-4o and 61.5% for GPT-4o-mini (worse than ReAct and Self-Consistency in those reported figures). In simulator win-rate comparisons (Table 4), Self-Refinement gives moderate/variable win rates but generally underperforms DReaMAD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to ReAct/CoT and standard prompting baselines; e.g., ReAct on CommonsenseQA yields 83.6% (GPT-4o) while Self-Refinement reports 49.2% (GPT-4o) in Table 6, indicating worse performance in that case.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered iterative refinement (self-critique and revise) executed at inference time; in experiments authors used three refinement iterations as per Madaan et al. (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>The paper reports that Self-Refinement can improve some models/tasks in prior work, but in their experiments Self-Refinement sometimes degrades performance (citing Huang et al., 2024). They also show SPKE (strategic prior knowledge elicitation) can boost Self-Refinement when applied, indicating structured guidance aids iterative methods (E.2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors cite literature and report empirical cases where self-refinement degrades performance because models struggle to judge correctness of their own reasoning; in this paper, Self-Refinement often underperforms ReAct or Self-Consistency on CommonsenseQA and some strategic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to ReAct, CoT, Self-Consistency, MAD, and DReaMAD; Self-Refinement underperforms DReaMAD and shows inconsistent gains relative to ReAct/CoT/self-consistency in the reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Applying the SPKE module (from DReaMAD) to Self-Refinement improves its performance (Section E.2 and Table 3/appendix mention), although precise numeric ablation deltas are not exhaustively tabulated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Bias Reinforcement in LLM Agents Debate', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8763.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8763.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (sampling + majority/ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that samples multiple chain-of-thought traces or outputs and aggregates them (e.g., by majority vote) to increase answer robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-4o-mini, GEMINI-1.5-flash (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLM families used elsewhere in the paper; specific model training/size not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Ensemble many sampled outputs (multiple chain-of-thought traces) and take the most frequent final answer; does not perform iterative critique/revision but aggregates across stochastic samples to reduce randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MetaNIM Arena (games), math and commonsense benchmarks (AIME/AMC, CommonsenseQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same suite of strategic decision and reasoning tasks where sampling-based consensus is evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Varied; on CommonsenseQA (Table 6) Self-Consistency matches ReAct performance for GPT-4o (83.6%) and GPT-4o-mini (78.7%), showing good performance in that task. In game settings, Self-Consistency sometimes underperforms DReaMAD and does not overcome debate-based bias reinforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines like ReAct/CoT/Standard prompting are reported; Self-Consistency sometimes equals or slightly improves over ReAct (e.g., in CommonsenseQA reported numbers) but does not always yield improvements in strategic game accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompting + sampling: multiple outputs are sampled at inference-time and aggregated; the method is implemented via prompt-engineering and altered sampling (temperature) but not iterative revision.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative results show Self-Consistency matches or slightly improves over ReAct in some benchmarks (CommonsenseQA examples), but the paper emphasizes conceptual limitations: it reduces randomness rather than providing corrective feedback and thus can converge on frequent but incorrect answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-Consistency lacks a critical feedback mechanism and converges toward the most frequent answer rather than necessarily the correct one; cannot iteratively correct reasoning errors because it does not perform critique-and-revise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with ReAct, CoT, Self-Refinement, MAD, and DReaMAD; Self-Consistency performs well in some benchmarks but is outperformed by DReaMAD on strategic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Authors apply SPKE to Self-Consistency and find SPKE improves performance (Section E.2), but no detailed per-setting ablation tables are fully enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Bias Reinforcement in LLM Agents Debate', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8763.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8763.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed feedback-loop method where an agent records its past actions and outcomes as trajectories and uses them as verbal memory to self-improve over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Cited as a feedback-loop method in related work: agents use verbalized memory of past interactions and reinforcement-style updates to adjust future behavior; specifics are cited to Shinn et al. (2023) but are not implemented in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Mentioned as an external feedback-loop technique (verbal memory + reinforcement), not applied in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Not evaluated in this paper; cited in related work mentioning limitations of feedback-loop methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper groups Reflexion with other feedback-loop methods that the authors claim 'struggle to reliably correct biases or foster diverse reasoning' (related-work summary), but no empirical Reflexion results are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside STaR and SCoRe as examples of feedback-loop methods that have limitations in bias mitigation; no direct comparison data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Bias Reinforcement in LLM Agents Debate', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8763.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8763.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STaR (Self-Taught Reasoner / training-with-review style methods cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of feedback-loop or reinforcement-style methods for improving LLM outputs over time via generated 'teacher' data or review/critique loops; cited as prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>STaR (and related feedback-loop methods)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Cited as an external method (Zelikman et al. 2022 among others) employing self-generated data or reinforcement to teach models to correct outputs; not applied in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Related-work class: external feedback (self-generated training data or reinforcement signals); not used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper mentions STaR in a list of feedback-loop methods that still 'struggle to reliably correct biases or foster diverse reasoning' but provides no empirical STaR results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Grouped with other feedback-loop approaches as having limited ability to mitigate bias in iterative reasoning contexts per the paper's literature summary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited as part of prior art contrasted with DReaMAD; no quantitative comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Bias Reinforcement in LLM Agents Debate', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8763.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8763.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoRe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCoRe (Training/verifier and score-based feedback methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed feedback/learning method (cited) that trains verifiers or scoring modules to improve LLM outputs; referenced in related work as having limitations with bias mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SCoRe</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Mentioned as part of feedback-loop approaches (Kumar et al., 2024) that attempt to train verifiers or use reinforcement to correct models; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External verifier/training feedback approaches referenced in related work; not used in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>No direct evidence reported here; cited among methods that have struggled to reliably correct biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that feedback-loop methods such as SCoRe 'struggle to reliably correct biases or foster diverse reasoning' (literature summary) but provides no empirical SCoRe results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned in comparison to DReaMAD conceptually, but no empirical head-to-head data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Bias Reinforcement in LLM Agents Debate', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8763.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8763.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strategic Prior Knowledge Elicitation (SPKE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured prompting module within DReaMAD that reinterprets the problem and elicits high-level strategic rules prior to reasoning, used to reduce strongly-consistent bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini, GPT-4o, GEMINI models (as applied within DReaMAD and applied to other self-correction methods in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as an inference-time prompting module to the LLMs used in experiments; model internals not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Strategic Prior Knowledge Elicitation (SPKE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt module that asks the model to reinterpret the game/problem, extract key elements (game definition, winning condition, move constraints), and formulate generalized winning strategies and endgame tactics before producing an action; uses low sampling temperature (0.1) to increase strategic consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MetaNIM Arena (strategic games) and as an augmentation to Self-Refinement/Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to improve strategic reasoning in adversarial games and to enhance other iterative/self-correction methods by providing structured prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>SPKE applied alone (DReaMAD (−)) yields substantial gains in accuracy (Table 3 shows DReaMAD (−) outperforms ReAct and CoT across models and games; e.g., GPT-4o-mini DReaMAD (−) average 0.57 vs ReAct average 0.40). The paper states SPKE consistently improves performance when applied to Self-Refinement and Self-Consistency (Section E.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>When SPKE is not used, baseline Self-Refinement and Self-Consistency perform worse on many tasks (see Table 3/Section E.2), though specific baseline numbers depend on the model/task.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Structured prompt engineering that elicits and codifies prior domain/strategic knowledge before action selection; implemented as additional inference-time prompt steps (no external training).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements in Table 3 for DReaMAD (−) vs ReAct/CoT, and statements in Section E.2 that SPKE improves Self-Refinement/Self-Consistency; qualitative examples show SPKE helps preserve correct reasoning when introduced alongside curated optimal responses (Section 4.1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SPKE helps when a clear strategic prior exists (NIM, Fibonacci, Kayles) but is less effective in games like Chomp that lack concise generalized winning heuristics and require exploratory play.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>When added to other methods (Self-Refinement, Self-Consistency) SPKE improves their performance, suggesting that structured prior elicitation can be complementary to iterative self-correction techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>DReaMAD (−) ablation (SPKE + perspective diversification but without multi-agent debate) yields strong gains (Table 3). SPKE applied to Self-Refinement and Self-Consistency also shows improvements per Section E.2, though numerical ablation breakdowns are limited in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Bias Reinforcement in LLM Agents Debate', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Universal self-consistency for large language model generation <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Training language models to self-correct via reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8763",
    "paper_id": "paper-277244351",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "DReaMAD",
            "name_full": "Diverse Reasoning via Multi-Agent Debate with Refined Prompt",
            "brief_description": "A framework introduced in this paper that combines strategic prior-knowledge elicitation and perspective diversification to produce diverse agent prompts, structured multi-agent debate, and a post-debate refinement step to improve strategic decision-making and mitigate bias reinforcement in LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini, GPT-4o, GEMINI-1.5-pro, GEMINI-1.5-flash, GPT-o3-mini (evaluated across multiple backbones)",
            "model_description": "Various proprietary large language models (GPT and GEMINI families) used by the authors; the paper does not provide parameter counts or training details for these specific variants.",
            "reflection_method_name": "DReaMAD (Strategic Prior Knowledge Elicitation + Perspective Diversification + Debate + Post-debate refinement)",
            "reflection_method_description": "Two-stage method: (1) Strategic Prior Knowledge Elicitation (SPKE) prompts the model to reinterpret the problem and produce high-level strategies (temperature 0.1 used for consistency); (2) Perspective Diversification produces distinct self-generated prompts for multiple agents (temperature 0.7 to encourage diversity). Agents then engage in a structured multi-agent debate and a final post-debate refinement revisits conclusions to improve decision quality.",
            "task_name": "MetaNIM Arena (impartial combinatorial games: NIM, Fibonacci, Kayles, Chomp, Corner Queen) and additional math/Commonsense benchmarks (AIME, AMC, CommonsenseQA)",
            "task_description": "Adversarial strategic decision-making benchmark (MetaNIM Arena) with mathematically provable optimal moves via Sprague-Grundy theory; also evaluated on math reasoning (AIME/AMC) and CommonsenseQA for generalization.",
            "performance_with_reflection": "+12.0% accuracy gain over standard prompting on the MetaNIM Arena dataset (paper claim); +20.8% higher win rate than MAD in the simulator (paper claim). Example reported numbers: DReaMAD (−) accuracies in Table 3 (averages vary by model; e.g., GPT-4o-mini DReaMAD (−) average 0.57 accuracy), and simulator win rates in Table 4 (e.g., DReaMAD win rates reach up to 0.98 for some settings with GPT-4o-mini and aggregate improvements across variants).",
            "performance_without_reflection": "Baseline methods reported include Standard Prompting, ReAct, Chain-of-Thought, Self-Refinement, Self-Consistency, and MAD; relative to these, DReaMAD shows the improvements cited above (see Table 3 and Table 4). Example baselines: standard prompting and ReAct average accuracies in Table 3 (vary by model; e.g., GPT-4o-mini ReAct average 0.40 vs DReaMAD (−) 0.57).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineering based modules: structured prompting to elicit prior strategic knowledge (SPKE), automated self-generation of diverse prompts per agent (perspective diversification), multi-agent debate rounds, and a post-debate refinement prompt; no additional external memory or retraining required.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: reported +12.0% accuracy over standard prompting on MetaNIM Arena and +20.8% higher win rate than MAD in simulator; Table 3 shows DReaMAD (−) yields higher optimal-action accuracy across multiple models/tasks; Table 4 shows higher win rates for DReaMAD compared to other self-correction methods. Qualitative: reduced bias amplification in debate examples (Figures 4–6) and improved convergence toward optimal moves in previously biased states.",
            "limitations_or_failure_cases": "DReaMAD struggles in environments like Chomp where a general, concise prior strategy is difficult to define and where effective behavior requires exploratory play rather than direct elicitation of prior knowledge; inference cost includes multiple prompt steps (though authors argue test-time scaling is cheaper than fine-tuning).",
            "comparison_to_other_methods": "Direct comparisons in the paper show DReaMAD outperforms ReAct, CoT, Self-Refinement, Self-Consistency, and standard MAD in both accuracy and win-rate metrics on MetaNIM Arena and on selected math/Commonsense tasks; DReaMAD (−) (no debate) also often outperforms CoT and ReAct, indicating SPKE contributes substantially.",
            "ablation_study_results": "Ablation DReaMAD (−) (SPKE + perspective diversification but excluding debate) still yields substantial gains (reported in Table 3). The paper also applies SPKE to Self-Refinement and Self-Consistency and reports that SPKE alone consistently improves their performance (Section E.2), though exact per-method numeric deltas are not fully tabulated in the main text.",
            "uuid": "e8763.0",
            "source_info": {
                "paper_title": "Understanding Bias Reinforcement in LLM Agents Debate",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Multi-Agent Debate (MAD)",
            "name_full": "Multi-Agent Debate",
            "brief_description": "A debate-style iterative method where multiple instances/agents (often the same model) critique and counter one another's outputs, intended to surface errors and improve final answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini, GPT-4o, GEMINI-1.5-pro, GEMINI-1.5-flash (used as debating agents in experiments)",
            "model_description": "Multiple instances of the same LLM family used as debating agents; model internals/training not specified beyond family names.",
            "reflection_method_name": "Multi-Agent Debate (MAD)",
            "reflection_method_description": "Instantiate multiple agents (often the same model) to generate responses and critiques in multiple rounds of debate; in experiments authors ran up to three rounds of debate (process terminates early if consensus reached). Agents generate multiple responses per round (e.g., 20 responses per agent in some analyses) and distributions are compared before/after debate.",
            "task_name": "MetaNIM Arena (NIM, Fibonacci) and other strategic game scenarios",
            "task_description": "Adversarial strategic games where debate is used to refine move selection; evaluated both on static dataset examples and through an interactive simulator measuring win rates.",
            "performance_with_reflection": "Varied by model and task; in many cases MAD amplifies pre-existing biases and does not increase optimal-action frequency. Paper reports average reinforcement increases after MAD of +9.17% for GPT models and +12.29% for GEMINI models in Fibonacci (Table 2). In simulator experiments (Table 4), MAD often yields lower win rates than DReaMAD (paper claims DReaMAD achieves +20.8% higher win rate than MAD).",
            "performance_without_reflection": "Baseline (pre-debate) initial action distributions are reported; in experiments, the initial (pre-debate) frequency of optimal actions was sometimes higher than post-debate frequencies (e.g., a curated set with 80% optimal responses fell to predominance of biased responses after debate in Figure 3 experiment).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Iterative multi-agent exchanges implemented via prompt-engineered debates among identical model instances, sampling multiple responses per agent at controlled temperature; no external verifier used in their experiments.",
            "number_of_iterations": 3,
            "evidence_for_improvement": "The paper presents evidence that MAD does not reliably improve answers in strategic settings; quantitative analysis in NIM and Fibonacci shows MAD tends to increase frequency of initially dominant (biased) actions rather than correcting them (Figures 2 and 3, Table 2).",
            "limitations_or_failure_cases": "Bias reinforcement: MAD frequently amplifies pre-existing, strongly-consistent biases (both 'good' and 'wrong' biases) rather than correcting them; lack of perspective diversity when multiple agents are identical leads to homogenized reasoning; curated high-quality inputs can be overwhelmed by dominant biased trajectories.",
            "comparison_to_other_methods": "MAD is compared to Self-Refinement, Self-Consistency, ReAct, CoT, and DReaMAD; DReaMAD (with prompt diversification and SPKE) outperforms MAD in accuracy and win rate. The paper cites proposals to combine different models for debate to boost diversity (Chen et al. 2023b) but focuses on single-model diversification.",
            "ablation_study_results": "Authors compare (+)MAD (identical prompts for both agents) vs DReaMAD (distinct self-generated prompts) and find perspective diversification significantly improves performance (Figure 6 left). They also show that providing optimal curated responses in debate does not prevent bias amplification under MAD (Figure 3 experiment).",
            "uuid": "e8763.1",
            "source_info": {
                "paper_title": "Understanding Bias Reinforcement in LLM Agents Debate",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Refinement",
            "name_full": "Self-Refinement (iterative self-critique and revise)",
            "brief_description": "An iterative self-correction method where an LLM critiques its own output and generates revisions over multiple refinement steps to improve answer quality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o, GPT-4o-mini, GPT-o3-mini, GEMINI family (used in comparisons/experiments)",
            "model_description": "Same family LLMs as above; paper does not report parameter counts or training specifics for each model variant.",
            "reflection_method_name": "Self-Refinement",
            "reflection_method_description": "Iterative self-review: the model generates an answer, critiques it, and revises; the authors follow Madaan et al. (2024) methodology and apply three iterative refinement steps in their experiments.",
            "task_name": "MetaNIM Arena (games), math benchmarks (AIME/AMC), CommonsenseQA",
            "task_description": "Strategic game decision tasks and standard reasoning benchmarks used to measure whether iterative self-refinement improves solution quality.",
            "performance_with_reflection": "Mixed or degraded results in some settings. Example: on CommonsenseQA (Table 6) Self-Refinement yields 49.2% for GPT-4o and 61.5% for GPT-4o-mini (worse than ReAct and Self-Consistency in those reported figures). In simulator win-rate comparisons (Table 4), Self-Refinement gives moderate/variable win rates but generally underperforms DReaMAD.",
            "performance_without_reflection": "Compared to ReAct/CoT and standard prompting baselines; e.g., ReAct on CommonsenseQA yields 83.6% (GPT-4o) while Self-Refinement reports 49.2% (GPT-4o) in Table 6, indicating worse performance in that case.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered iterative refinement (self-critique and revise) executed at inference time; in experiments authors used three refinement iterations as per Madaan et al. (2024).",
            "number_of_iterations": 3,
            "evidence_for_improvement": "The paper reports that Self-Refinement can improve some models/tasks in prior work, but in their experiments Self-Refinement sometimes degrades performance (citing Huang et al., 2024). They also show SPKE (strategic prior knowledge elicitation) can boost Self-Refinement when applied, indicating structured guidance aids iterative methods (E.2).",
            "limitations_or_failure_cases": "Authors cite literature and report empirical cases where self-refinement degrades performance because models struggle to judge correctness of their own reasoning; in this paper, Self-Refinement often underperforms ReAct or Self-Consistency on CommonsenseQA and some strategic tasks.",
            "comparison_to_other_methods": "Compared directly to ReAct, CoT, Self-Consistency, MAD, and DReaMAD; Self-Refinement underperforms DReaMAD and shows inconsistent gains relative to ReAct/CoT/self-consistency in the reported tables.",
            "ablation_study_results": "Applying the SPKE module (from DReaMAD) to Self-Refinement improves its performance (Section E.2 and Table 3/appendix mention), although precise numeric ablation deltas are not exhaustively tabulated in the main text.",
            "uuid": "e8763.2",
            "source_info": {
                "paper_title": "Understanding Bias Reinforcement in LLM Agents Debate",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (sampling + majority/ensemble)",
            "brief_description": "An approach that samples multiple chain-of-thought traces or outputs and aggregates them (e.g., by majority vote) to increase answer robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o, GPT-4o-mini, GEMINI-1.5-flash (evaluated)",
            "model_description": "Same LLM families used elsewhere in the paper; specific model training/size not provided in the paper.",
            "reflection_method_name": "Self-Consistency",
            "reflection_method_description": "Ensemble many sampled outputs (multiple chain-of-thought traces) and take the most frequent final answer; does not perform iterative critique/revision but aggregates across stochastic samples to reduce randomness.",
            "task_name": "MetaNIM Arena (games), math and commonsense benchmarks (AIME/AMC, CommonsenseQA)",
            "task_description": "Same suite of strategic decision and reasoning tasks where sampling-based consensus is evaluated.",
            "performance_with_reflection": "Varied; on CommonsenseQA (Table 6) Self-Consistency matches ReAct performance for GPT-4o (83.6%) and GPT-4o-mini (78.7%), showing good performance in that task. In game settings, Self-Consistency sometimes underperforms DReaMAD and does not overcome debate-based bias reinforcement.",
            "performance_without_reflection": "Baselines like ReAct/CoT/Standard prompting are reported; Self-Consistency sometimes equals or slightly improves over ReAct (e.g., in CommonsenseQA reported numbers) but does not always yield improvements in strategic game accuracy.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompting + sampling: multiple outputs are sampled at inference-time and aggregated; the method is implemented via prompt-engineering and altered sampling (temperature) but not iterative revision.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative results show Self-Consistency matches or slightly improves over ReAct in some benchmarks (CommonsenseQA examples), but the paper emphasizes conceptual limitations: it reduces randomness rather than providing corrective feedback and thus can converge on frequent but incorrect answers.",
            "limitations_or_failure_cases": "Self-Consistency lacks a critical feedback mechanism and converges toward the most frequent answer rather than necessarily the correct one; cannot iteratively correct reasoning errors because it does not perform critique-and-revise.",
            "comparison_to_other_methods": "Compared with ReAct, CoT, Self-Refinement, MAD, and DReaMAD; Self-Consistency performs well in some benchmarks but is outperformed by DReaMAD on strategic tasks.",
            "ablation_study_results": "Authors apply SPKE to Self-Consistency and find SPKE improves performance (Section E.2), but no detailed per-setting ablation tables are fully enumerated in main text.",
            "uuid": "e8763.3",
            "source_info": {
                "paper_title": "Understanding Bias Reinforcement in LLM Agents Debate",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (language agents with verbal reinforcement learning)",
            "brief_description": "A previously proposed feedback-loop method where an agent records its past actions and outcomes as trajectories and uses them as verbal memory to self-improve over time.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reflexion",
            "reflection_method_description": "Cited as a feedback-loop method in related work: agents use verbalized memory of past interactions and reinforcement-style updates to adjust future behavior; specifics are cited to Shinn et al. (2023) but are not implemented in this paper's experiments.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Mentioned as an external feedback-loop technique (verbal memory + reinforcement), not applied in the paper's experiments.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Not evaluated in this paper; cited in related work mentioning limitations of feedback-loop methods.",
            "limitations_or_failure_cases": "Paper groups Reflexion with other feedback-loop methods that the authors claim 'struggle to reliably correct biases or foster diverse reasoning' (related-work summary), but no empirical Reflexion results are reported here.",
            "comparison_to_other_methods": "Mentioned alongside STaR and SCoRe as examples of feedback-loop methods that have limitations in bias mitigation; no direct comparison data in this paper.",
            "ablation_study_results": null,
            "uuid": "e8763.4",
            "source_info": {
                "paper_title": "Understanding Bias Reinforcement in LLM Agents Debate",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "STaR",
            "name_full": "STaR (Self-Taught Reasoner / training-with-review style methods cited)",
            "brief_description": "A class of feedback-loop or reinforcement-style methods for improving LLM outputs over time via generated 'teacher' data or review/critique loops; cited as prior work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "STaR (and related feedback-loop methods)",
            "reflection_method_description": "Cited as an external method (Zelikman et al. 2022 among others) employing self-generated data or reinforcement to teach models to correct outputs; not applied in experiments in this paper.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Related-work class: external feedback (self-generated training data or reinforcement signals); not used in this paper's experiments.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper mentions STaR in a list of feedback-loop methods that still 'struggle to reliably correct biases or foster diverse reasoning' but provides no empirical STaR results.",
            "limitations_or_failure_cases": "Grouped with other feedback-loop approaches as having limited ability to mitigate bias in iterative reasoning contexts per the paper's literature summary.",
            "comparison_to_other_methods": "Cited as part of prior art contrasted with DReaMAD; no quantitative comparisons in this paper.",
            "ablation_study_results": null,
            "uuid": "e8763.5",
            "source_info": {
                "paper_title": "Understanding Bias Reinforcement in LLM Agents Debate",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SCoRe",
            "name_full": "SCoRe (Training/verifier and score-based feedback methods)",
            "brief_description": "A recently proposed feedback/learning method (cited) that trains verifiers or scoring modules to improve LLM outputs; referenced in related work as having limitations with bias mitigation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "SCoRe",
            "reflection_method_description": "Mentioned as part of feedback-loop approaches (Kumar et al., 2024) that attempt to train verifiers or use reinforcement to correct models; not evaluated in this paper.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "External verifier/training feedback approaches referenced in related work; not used in the paper's experiments.",
            "number_of_iterations": null,
            "evidence_for_improvement": "No direct evidence reported here; cited among methods that have struggled to reliably correct biases.",
            "limitations_or_failure_cases": "Paper notes that feedback-loop methods such as SCoRe 'struggle to reliably correct biases or foster diverse reasoning' (literature summary) but provides no empirical SCoRe results.",
            "comparison_to_other_methods": "Mentioned in comparison to DReaMAD conceptually, but no empirical head-to-head data in this paper.",
            "ablation_study_results": null,
            "uuid": "e8763.6",
            "source_info": {
                "paper_title": "Understanding Bias Reinforcement in LLM Agents Debate",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SPKE",
            "name_full": "Strategic Prior Knowledge Elicitation (SPKE)",
            "brief_description": "A structured prompting module within DReaMAD that reinterprets the problem and elicits high-level strategic rules prior to reasoning, used to reduce strongly-consistent bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini, GPT-4o, GEMINI models (as applied within DReaMAD and applied to other self-correction methods in ablations)",
            "model_description": "Applied as an inference-time prompting module to the LLMs used in experiments; model internals not specified in the paper.",
            "reflection_method_name": "Strategic Prior Knowledge Elicitation (SPKE)",
            "reflection_method_description": "Prompt module that asks the model to reinterpret the game/problem, extract key elements (game definition, winning condition, move constraints), and formulate generalized winning strategies and endgame tactics before producing an action; uses low sampling temperature (0.1) to increase strategic consistency.",
            "task_name": "MetaNIM Arena (strategic games) and as an augmentation to Self-Refinement/Self-Consistency",
            "task_description": "Used to improve strategic reasoning in adversarial games and to enhance other iterative/self-correction methods by providing structured prior knowledge.",
            "performance_with_reflection": "SPKE applied alone (DReaMAD (−)) yields substantial gains in accuracy (Table 3 shows DReaMAD (−) outperforms ReAct and CoT across models and games; e.g., GPT-4o-mini DReaMAD (−) average 0.57 vs ReAct average 0.40). The paper states SPKE consistently improves performance when applied to Self-Refinement and Self-Consistency (Section E.2).",
            "performance_without_reflection": "When SPKE is not used, baseline Self-Refinement and Self-Consistency perform worse on many tasks (see Table 3/Section E.2), though specific baseline numbers depend on the model/task.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Structured prompt engineering that elicits and codifies prior domain/strategic knowledge before action selection; implemented as additional inference-time prompt steps (no external training).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative improvements in Table 3 for DReaMAD (−) vs ReAct/CoT, and statements in Section E.2 that SPKE improves Self-Refinement/Self-Consistency; qualitative examples show SPKE helps preserve correct reasoning when introduced alongside curated optimal responses (Section 4.1).",
            "limitations_or_failure_cases": "SPKE helps when a clear strategic prior exists (NIM, Fibonacci, Kayles) but is less effective in games like Chomp that lack concise generalized winning heuristics and require exploratory play.",
            "comparison_to_other_methods": "When added to other methods (Self-Refinement, Self-Consistency) SPKE improves their performance, suggesting that structured prior elicitation can be complementary to iterative self-correction techniques.",
            "ablation_study_results": "DReaMAD (−) ablation (SPKE + perspective diversification but without multi-agent debate) yields strong gains (Table 3). SPKE applied to Self-Refinement and Self-Consistency also shows improvements per Section E.2, though numerical ablation breakdowns are limited in the main text.",
            "uuid": "e8763.7",
            "source_info": {
                "paper_title": "Understanding Bias Reinforcement in LLM Agents Debate",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Universal self-consistency for large language model generation",
            "rating": 2,
            "sanitized_title": "universal_selfconsistency_for_large_language_model_generation"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Training language models to self-correct via reinforcement learning",
            "rating": 1,
            "sanitized_title": "training_language_models_to_selfcorrect_via_reinforcement_learning"
        }
    ],
    "cost": 0.022045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Understanding Bias Reinforcement in LLM Agents Debate
10 Jun 2025</p>
<p>Jihwan Oh 
KAIST AI
SeoulRepublic of Korea</p>
<p>Minchan Jeong 
KAIST AI
SeoulRepublic of Korea</p>
<p>Jongwoo Ko <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#111;&#110;&#103;&#119;&#111;&#111;&#46;&#107;&#111;&#64;&#107;&#97;&#105;&#115;&#116;&#46;&#97;&#99;&#46;&#107;&#114;">&#106;&#111;&#110;&#103;&#119;&#111;&#111;&#46;&#107;&#111;&#64;&#107;&#97;&#105;&#115;&#116;&#46;&#97;&#99;&#46;&#107;&#114;</a> 
KAIST AI
SeoulRepublic of Korea</p>
<p>Se-Young Yun <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#121;&#117;&#110;&#115;&#101;&#121;&#111;&#117;&#110;&#103;&#64;&#107;&#97;&#105;&#115;&#116;&#46;&#97;&#99;&#46;&#107;&#114;">&#121;&#117;&#110;&#115;&#101;&#121;&#111;&#117;&#110;&#103;&#64;&#107;&#97;&#105;&#115;&#116;&#46;&#97;&#99;&#46;&#107;&#114;</a>. 
KAIST AI
SeoulRepublic of Korea</p>
<p>Se-Young Yun</p>
<p>Understanding Bias Reinforcement in LLM Agents Debate
10 Jun 20256AFB71B42E7F1FD981E9428284695ADEarXiv:2503.16814v3[cs.LG]
Large Language Models (LLMs) solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging.While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms.Multi-Agent Debate (MAD) has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness.To systematically evaluate these issues, we introduce MetaNIM Arena, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions.To overcome MAD's limitations, we propose DReaMAD (Diverse Reasoning via Multi-Agent Debate with Refined Prompt), a novel framework that (1) refines LLMs' strategic prior knowledge to improve reasoning quality and (2) promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias.Empirical results show that DReaMAD significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLMbased decision-making.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated remarkable problem-solving capabilities across a wide range of tasks by leveraging knowledge acquired from vast datasets Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025.Copyright 2025 by the author(s).(Achiam et al., 2023;The;Team et al., 2023;Dubey et al., 2024).These models can address complex decision-making problems using training-free methods such as prompt engineering (White et al., 2023;Chen et al., 2023a;Schulhoff et al., 2024b) and in-context learning (Brown et al., 2020;Dong et al., 2022;Wei et al., 2022;Mavromatis et al., 2023;Pan, 2023), which provide guidance for effective reasoning.However, these approaches do not explicitly guarantee the correctness of the generated responses.</p>
<p>To address this, recent research has explored self-correction mechanisms that allow LLMs to refine their own outputs without external feedback.Self-consistency (Wang et al., 2022;Chen et al., 2023c) enhances reliability by ensembling multiple responses.Self-refinement (Wan et al., 2023;Shinn et al., 2023;Madaan et al., 2024) enables LLMs to iteratively critique and revise their outputs.However, self-consistency lacks a critical feedback mechanism, meaning it does not iteratively refine responses but merely reduces the model's inherent randomness by converging on the most frequently generated answer.Further recent studies (Huang et al., 2024) suggest that self-refinement can degrade performance, as models often struggle to assess the correctness of their own reasoning.</p>
<p>Recently, inspired by the Society of Mind philosophy, Multi-Agent Debate (MAD; Chan et al. 2023;Du et al. 2023;Liang et al. 2023) has emerged as a promising alternative.However, its success has been limited to static problemsolving and lacks assessments for adversarial strategic reasoning (Cobbe et al., 2021;Edwards, 1994;He et al., 2020).Additionally, current evaluations do not account for dynamic decision-making in interactive environments, where an agent's choices influence and adapt to an opponent's actions.This limitation hinders LLMs from retrieving and applying strategic knowledge beyond the given context.</p>
<p>To overcome the above limitation, we introduce MetaNIM Arena, a framework for evaluating LLMs in adversarial strategic decision-making.It allows us to assess their ability to adapt dynamically and ensures robust reasoning under mathematically rigorous conditions.Through MetaNIM Arena, we systematically analyze two fundamental limitations of MAD:</p>
<p>(1) Bias Reinforcement: In strategic reasoning tasks, LLMs tend to rely on immediate context rather than retrieving broader strategic knowledge, leading to distorted reasoning instead of correct inference.Debatebased frameworks further amplify this issue by reinforcing the model's inherent biases rather than mitigating them ( §4.1-4.2).</p>
<p>(2) Lack of Perspective Diversity: Although MAD uses a debate structure, it relies on multiple instances of the same model.This limits the diversity of perspectives introduced in the reasoning process, reducing its ability to challenge inherent biases ( §4.3).</p>
<p>Rooted in the Learning from Multiple Approaches framework (Council et al., 2005;Cleaves, 2008), research shows that engaging with multiple problem-solving representations enhances comprehension and mitigates biases.Building on this insight, we propose DReaMAD (Diverse Reasoning via Multi-Agent Debate with Refined Prompt).DReaMAD addresses the limitations of MAD by (1) refining LLMs' domain-specific knowledge to guide more accurate strategic reasoning, and (2) systematically modifying prompts to foster diverse perspectives.A detailed comparison with existing self-correction methods is presented in Table 1.Our key contributions are as follows:</p>
<p>• We introduce MetaNIM Arena, a benchmark designed to evaluate LLMs in adversarial strategic decision-making, where mathematical rigor enables precise assessment of reasoning quality and strategic adaptability.</p>
<p>• We identify the bias reinforcement in Multi-Agent Debate, showing that MAD strengthens both correct and incorrect reasoning rather than inherently improving it.</p>
<p>• We propose DReaMAD, a novel framework that refines strategic prior knowledge and enhances reasoning diversity through structured self-prompt refinement and perspective diversification, achieving a +12.0%accuracy gain over standard prompting on MetaNIM Arena dataset and a +20.8% higher win rate than MAD in the simulator.</p>
<p>Preliminary</p>
<p>Bias in LLMs</p>
<p>Large Language Models (LLMs) can exhibit biases that lead to unfair or skewed outcomes, arising from training data, model architectures, learning objectives, or deployment conditions (Guo et al., 2024).Such biases manifest both intrinsically, for instance in word embeddings (Bolukbasi et al., 2016), and extrinsically, reflecting real-world disparities (Goldfarb-Tarrant et al., 2021).Moreover, biases can emerge dynamically during interactive reasoning, where current reinforcement mechanisms-like self-consistency and self-refinement-often fail to mitigate them (Huang et al., 2024;Shin et al., 2024).Indeed, recent research shows that iterative interactions can reinforce existing biases instead of diversifying reasoning (Ganguli et al., 2023).</p>
<p>Prompt Engineering and Self-Correction in LLMs</p>
<p>Prompt engineering shapes model outputs without retraining, potentially improving generalization and reducing bias (Brown et al., 2020;Reynolds &amp; McDonell, 2021;Zhao et al., 2024;Schulhoff et al., 2024a;Shin et al., 2024).However, fully eliminating biases in complex reasoning remains challenging (Jiang et al., 2022;Lu et al., 2022).</p>
<p>Meanwhile, self-correction mechanisms in LLMs refine responses without external supervision (Ganguli et al., 2023;Liu et al., 2024;Kamoi et al., 2024).Self-consistency, for instance, ensembles multiple outputs but converges on frequent rather than correct answers (Wang et al., 2022;Chen et al., 2023c), and self-refinement can reinforce rather than fix biases (Wan et al., 2023;Shinn et al., 2023;Madaan et al., 2024;Huang et al., 2024).Feedback-loop methods such as STaR (Zelikman et al., 2022), Reflexion (Shinn et al., 2023), and SCoRe (Kumar et al., 2024) also struggle to reliably correct biases or foster diverse reasoning (Guo et al., 2024).</p>
<p>Multi-Agent Debate in LLMs</p>
<p>Multi-Agent Debate (MAD) enables LLM agents to critique each other, enhancing reasoning on complex tasks (Liang et al., 2023;Du et al., 2023).ChatEval (Chan et al., 2023), a multi-agent evaluation system, simulates human judgment to assess model output quality.Optimizations include taskspecific strategies for improving debate effectiveness (Smit et al., 2024) and ACC-Debate, an actor-critic framework that trains models to specialize in debates, achieving benchmark gains (Estornell et al., 2024).While these enhancements improve performance, studies reveal a key limitation: static evaluations focus on assessing predefined problems, whereas real-world decision-making often involves dynamic, interactive environments where biases can evolve.Understanding how biases shift in these settings is crucial for developing robust strategies that extend beyond conventional static benchmarks.</p>
<p>MetaNIM Arena</p>
<p>Overview</p>
<p>We introduce MetaNIM Arena, illustrated in Figure 7.It features six impartial combinatorial games, meaning both players share identical moves at each state, all information is fully observable, and each game terminates in a finite number of moves.By merging combinatorial game theory with adversarial play, MetaNIM Arena serves as a benchmark for debate-based strategic reasoning in LLMs.</p>
<p>Dataset and Simulator.Key game situations are systematically collected into a dataset, each accompanied by an optimal action, enabling LLMs to be tested for decision-making accuracy.These results appear in Table 3, with details in Appendix A.3.Separately, MetaNIM Arena can function as a simulator: the model encounters an adaptive opponent, so each trajectory depends on both the agent's and the opponent's actions.Here, binary win/loss outcomes allows evaluation by win rate.We demonstrate this approach in Table 4, with further explanation in Appendix A.4.We refer the Appendix B for details of opponent modeling.</p>
<p>Why MetaNIM Arena?</p>
<p>MetaNIM Arena provides a rigorous environment for adversarial strategic reasoning in LLMs.Rather than isolated problem-solving or factual recall, our framework uses impartial combinatorial games, where each position's Grundy number defines the provably correct move.We will discuss further theoretical details in the following Section 3.4.This design offers:</p>
<ol>
<li>Adversarial Strategic Reasoning: Each scenario includes an opponent whose actions shape outcomes.Models must anticipate adversarial moves across multiple turns, going beyond static QA or single-step predictions.This approach tests latent strategic knowledge in an interactive, step-by-step context.</li>
</ol>
<p>Clear Optimality Criterion: By the Sprague-Grundy</p>
<p>Theorem, these games admit an optimal strategy.MetaNIM Arena thus measures how closely a model's reasoning aligns with that strategy, instead of relying on approximate metrics like BLEU or perplexity.We also assert that MetaNIM Arena naturally supports reinforcement learning framework.By providing a binary win/loss signal and structuring gameplay as a Markov Decision Process (MDP), it enables iterative strategy refinement-an advantage often absent in static benchmarks.</p>
<p>Game Variants</p>
<p>Here, we introduce four settings of MetaNIM Arena.Detailed explanation and theoretically determined winning strategies for these games are provided in Appendix A.</p>
<p>• NIM: Agents take turns removing 1 to N (typically 3) objects from a set of heaps where the player who takes the last object wins.Success requires maintaining specific heap configurations to control the game's outcome.</p>
<p>• Fibonacci: A variation of NIM where an agent's maximum removal is constrained by the opponent's previous move.Each turn, an agent may remove between 1 and 2× the opponent's last action.This rule introduces dynamic strategy adjustments, balancing immediate gains with long-term positioning.</p>
<p>• Kayles: Played with a or two row(s) of pins, where agents take turns knocking down one or two adjacent pins.</p>
<p>The player unable to make a move loses.The challenge lies in evaluating pin configurations and predicting the opponent's responses to optimize each turn.</p>
<p>• Chomp: Played on a quadrangle grid, agents take turns consuming a "block" of chocolate along with all blocks below and to the right.The player forced to eat the topright (or top-left) "poisoned" block loses.</p>
<p>• Corner Queen: On a rectangular board, two players take turns moving a queen left, down, or diagonally down-left.The first to reach the bottom-left corner wins.Strategy lies in limiting the opponent's options while advancing.The Grundy Number framework, along with the Sprague-Grundy Theorem, guarantees the existence of a winning strategy and provides a concrete method to determine it.In the MetaNIM Arena dataset, each state has a mathematically provable optimal move, allowing an LLM's decisions to be evaluated against the theoretical optimal strategy-a key advantage for unbiased assessment.Definition 3.1 (Grundy Number).For a finite impartial combinatorial game under normal play (where the last player to make a valid move wins), the Grundy number (or Nimber) G(S) is recursively defined as follows.If S is a terminal state with no valid moves, set G(S) = 0. Otherwise,
G(S) = mex{ G(S ′ ) | S ′ is reachable from S } .
Here, mex(X) is the smallest nonnegative integer not in X.</p>
<p>Note that the Grundy number is well-defined for every impartial combinatorial game, since the game's state space forms a DAG.In many combinatorial games, direct enumeration of all possible move sequences is computationally infeasible.However, with Sprague-Grundy Theorem, we can easily calculate Grundy Numbers on complex games.See Appendix A.1.1 for further discussions.</p>
<p>Optimal Strategy.When G(S) ̸ = 0, there is at least one move to a successor S ′ with G(S ′ ) = 0, forcing the opponent into a losing position.Conversely, if G(S) = 0, all successor states have G(S ′ ) ̸ = 0.Because the game DAG is finite and acyclic, repeatedly applying "move to G = 0" (or avoiding it) ensures a forced result under optimal play.See Appendix A.1.2 for more details, including the misère variant where taking the last object loses.</p>
<p>Understanding Bias Reinforcement in Debate Process</p>
<p>To quantitatively analyze bias in the debate process, we define strong consistency and bias reinforcement as below.This phenomenon naturally emerges in strategic decisionmaking contexts.As shown in Figure 1, when presented with a specific game state, the model repeatedly generates the same reasoning pattern, highlighted in orange.Rather than effectively utilizing strategic prior knowledge, the model fixates on a single line of reasoning, limiting adaptability and decision quality.</p>
<p>Definition 4.2 (Bias Reinforcement).Bias reinforcement in the context of large language models refers to the phenomenon where iterative reasoning processes-such as multi-agent debates-amplify pre-existing model biases instead of mitigating them.Rather than converging toward a more accurate or optimal reasoning outcome, the debate process reinforces strongly consistent, yet potentially suboptimal or distorted, reasoning patterns.</p>
<p>In the rest of this section, we analyze bias reinforcement and lack of diversity in the debate process, using GPT-4o-mini and GEMINI-1.5-profor the NIM game, and then extend the evaluation to Fibonacci with those two plus GPT-4o and GEMINI-1.5-flash.</p>
<p>Bias Reinforcement in MAD</p>
<p>In NIM: (Figure 2) We find that MAD amplifies models' pre-existing biases rather than refining their reasoning.To investigate this, we first identify game states where each model exhibits strong consistency, i.e., consistently selecting the same action across multiple trials.For each such state, we conduct multi-agent debates using two identical agents instantiated from the same model.</p>
<p>Each agent generates 20 responses (40 per state) at a fixed temperature of 0.7, maintained throughout the debate.Initial action distributions (light red) are compared against post-debate distributions after three rounds (blue).If MAD functioned as a self-correction mechanism, we would expect the distribution to shift toward the optimal action.However, we found the opposite: regardless of whether the initial reasoning was correct, the debate process consistently amplifies pre-existing biases rather than mitigating them.For instance, in Figure 2 (top-left), GPT-4o-mini initially selects a suboptimal action (Action 3) 82.5% of the time.</p>
<p>After the debate, this frequency increases to 90.0%, while the proportion of optimal responses drops further.Rather than correcting errors, the debate reinforces strongly consistent-but incorrect-responses.This pattern persists across model families.The GEMINI models (bottom row of Figure 2) exhibit similar behavior, regardless of whether the initial bias aligns with optimal play ("good bias") or not ("wrong bias").In both cases, MAD strengthens the dominant trajectory without introducing new strategic insight.</p>
<p>Figure 1 provides a detailed view: two LLM agents receive the same input and begin debating.Despite initial divergence, they converge quickly-after the first round-on a shared line of reasoning.Crucially, this convergence occurs even when the initial consensus is incorrect, illustrating that MAD often serves as an amplifier of bias rather than a correction mechanism.</p>
<p>In Fibonacci: (Table 2) Extending our NIM analysis, we evaluate MAD's effect in the Fibonacci game, a more complex setting with move constraints and dynamic interactions.</p>
<p>As in NIM, We identify states exhibiting strong consistency and categorize them into two groups: those where consistent responses align with the optimal strategy, and those where they do not.We then apply MAD to examine how response distributions evolve post-debate.</p>
<p>Consistent with the NIM results, MAD reinforces the model's initial biases in Fibonacci as well.As shown in Table 2, the frequency of initially consistent actions increases after debate, regardless of whether those actions are optimal.On average, reinforcement rises by 9.17% in GPT models and 12.29% in GEMINI models, indicating a systematic amplification of dominant reasoning patterns across architectures.</p>
<p>Optimal Inputs Do Not Reduce Bias in MAD</p>
<p>To further assess the bias reinforcement in MAD, we conduct an experiment on the NIM game (5 items remaining) using GPT-4o-mini, illustrated in Figure 3.We prepare two sets of 20 responses: one exhibiting strong consistency toward a biased action (Action 2), and another consisting of optimal responses, where 80% of actions correspond to the game-theoretic best move (Action 1).We then introduced these responses into a multi-agent debate setting using GPT-4o-mini, pairing each strongly consistent response with an optimal response, and observed how the model's reasoning evolves over multiple rounds of debate (denoted the game situation in detail in Appendix E).</p>
<p>Surprisingly, the debate fails to leverage the high-quality input.Initially, the curated dataset contained 80% optimal responses, yet after a single round of debate, the model predominantly aligned with the biased responses.As debate rounds proceed, this effect intensifies: the influence of the optimal input diminishes, while the initially frequent-but suboptimal-choice becomes dominant.These results demonstrate that MAD is not only ineffective at correcting bias, but can systematically aligns with its preexisting consistency patterns, reinforcing suboptimal but frequent choices.</p>
<p>In contrast, applying DReaMAD with the curated optimal responses preserves correct reasoning and mitigating bias reinforcement.This underscores the need for mechanisms that introduce diverse perspectives beyond internal debate, which we further discuss in §5.</p>
<p>Lack of Reasoning Diversity in MAD</p>
<p>While MAD is designed to refine reasoning through agent interaction, its effectiveness is fundamentally constrained by a lack of genuine diversity ( §2.3).To address this limitation, Chen et al. (2023b) propose a multi-model debate framework that combines outputs from different models to encourage diverse reasoning.In contrast, our approach focuses on single-model self-correction.</p>
<p>Interestingly, we observe that even within a single model, small variations in prompts can induce markedly different reasoning paths (Appendix D).For example, including or omitting the word Fibonacci leads to distinct strategies in the same task.This suggests that diversity in reasoning can be enhanced by strategically modifying prompts within the same model, providing a practical alternative to multi-model debate frameworks.</p>
<p>DReaMAD: Diverse Reasoning via MAD</p>
<p>To address the limitations of Multi-Agent Debate (MAD) and improve strategic decision-making in Large Language Models (LLMs), we introduce DReaMAD (Diverse Reasoning via Multi-Agent Debate with Refined Prompt).</p>
<p>Our framework refines prior knowledge and ensures diverse perspectives by extending MAD in two main stages:</p>
<ol>
<li>
<p>Strategic Prior Knowledge Elicitation: The model redefines the problem, extracts key strategic insights, and formulates a high-level strategy before reasoning.</p>
</li>
<li>
<p>Perspective Diversification: Multiple agents are instantiated with self-generated distinct viewpoints to engage in dialectical reasoning.</p>
</li>
</ol>
<p>After these stages, the agents conduct a structured multiagent debate.A final post-debate refinement step then revisits their conclusions to improve reasoning quality.The complete workflow is illustrated in Figure 4 and the de-tailed prompt formulation used in these two modules is well documented in the Appendix C.</p>
<p>Strategic Prior Knowledge Elicitation</p>
<p>To address strongly consistent bias, DReaMAD integrates a structured module that ensures systematic extraction and refinement of the LLM's internal strategic knowledge before the debate.First, the model is prompted to reinterpret the given problem, leading to a more organized understanding of the strategic context (Game Situation Reinterpretation).</p>
<p>Next, it formulates a set of high-level strategies that can be applied to the scenario at hand (General Strategy Formulation), preventing the model from settling too early on potentially flawed reasoning.As shown in Table 3, this module enhances the model's performance in tasks that require strategic prior knowledge (Figure 4-1).We set the temperature hyperparameter to be 0.1 for strategic consistency.</p>
<p>Perspective Diversification</p>
<p>Building on MAD, DReaMAD mitigates argument homogenization by ensuring each agent adopts a distinct viewpoint prior to the debate.This approach is inspired by the Learning from Multiple Approaches concept in education theory (Council et al., 2005;Cleaves, 2008), which suggests that individuals improve their problem-solving skills by exploring multiple representations of the same problem.Analogously, this module ensures that each agent is given differentiated initial prompts that guide reasoning along distinct strategic trajectories.By self-customizing initial prompts for each model instance, DReaMAD encourages unique strategic perspectives and reduces the risk of bias reinforcement (Figure 4-2) as demonstrated in Figure 5 and Figure 6 (left).In Figure 5, although each agent receives the same initial prompt, they independently generate different optimal prompts, leading to distinct distributions of reasoning.This process reduces each agent's bias and fosters a more robust debate.Remarkably, even in a state with very In this example, we illustrate how the debate process converges to an optimal outcome using DReaMAD.We begin with the same current state shown in Figure 1, employing self-generated prompts for each LLM agent.</p>
<p>strong consistency, the discussion converges well toward the correct reasoning direction.Here, we set temperature to be 0.7 for diversity.</p>
<p>Experiments</p>
<p>We utilized the benchmark MetaNIM Arena as both our dataset and simulator, as it provides a controlled environment for evaluating reasoning under grounded strategic tasks.Our investigation focuses on three key questions:</p>
<p>(1) Does our approach improve reasoning quality compared to existing prompting techniques?(2) Does our approach prove its strategic reasoning quality under adversarial decision-making environments?(3) Does generating diverse prompts contribute to better decision-making within the debate framework?</p>
<p>To address these questions, we compare DReaMAD with standard prompting methods including ReAct (Yao et al., Our method builds on the MAD framework by Du et al. (2023), augmenting it with structured self-prompt refinement and perspective diversification.For self-refinement, we follow the methodology of Madaan et al. (2024), applying three iterative refinement steps.Similarly, for MAD, we conducted up to three rounds of debate, following Du et al. (2023), with the process terminating early if a consensus is reached before the final round.</p>
<p>We also investigate whether the observed improvements generalize across different LLM architectures, including both GPT and GEMINI models.This experiment isolates the effect of strategic prior knowledge elicitation ( §5.1), allowing us to assess whether our method enhances decision-making without relying on debate dynamics.</p>
<p>Setup.To evaluate the effectiveness of our approach in improving reasoning capabilities, we compare DReaMAD without the debate process against ReAct and zero-shot CoT prompting across multiple models in the MetaNIM Arena dataset ( §A.3).For showing versatility of DReaMAD, we conduct experiments on four variants of LLMs as shown in Table 3.</p>
<p>Results.Table 3 demonstrates that DReaMAD consistently outperforms both ReAct and CoT prompting across all models and tasks.These results highlight the impact of our method in reinforcing structured strategic reasoning, even without the iterative correction process of debate.Notably, our approach leads to substantial improvements in NIM, Fibonacci, and Kayles, which are environments where long-term strategic planning plays a crucial role.Since defining a general winning strategy in Chomp is non-trivial, applying prior knowledge is challenging and results in less effectiveness compared to other games.Furthermore, we observe that models with inherently weaker reasoning abilities benefit the most from strategic prior knowledge elicitation (e.g., GPT-4o-mini with +17%p, GEMINI-1.5-flashwith +12%p on average).</p>
<p>DReaMAD in Adversarial Strategic</p>
<p>Decision-Making Setup.We applied DReaMAD to GPT-4o-mini and GEMINI-1.5-flashand compared it with selfcorrection methods, including standard-prompt, ReAct, selfrefinement, self-consistency, and MAD.To demonstrate its effectiveness, we used GPT-4o as the opponent model due to its superior performance.In this experiments, we utilized MetaNIM Arena simulator ( §A.4) to maximize the effect of generating diverse prompts.We aimed to validate our hypothesis in a simulator that requires strategic decisionmaking within complex dynamics.We ran 50 independent episodes and average the win-rate.4, DReaMAD consistently outperforms other self-correction methods across various strategic environments, demonstrating a significant improvement in winning rates.This result suggests that our approach enables LLM agents to effectively adapt to complex dynamics, particularly in adversarial decision-making scenarios where strategic reasoning is crucial.However, we observe that DReaMAD struggles in the Chomp game, which aligns with our hypothesis that Chomp lacks a well-defined generalized winning strategy.Unlike other tested environments, Chomp requires more exploratory play rather than direct reasoning from prior knowledge, highlighting a limitation of our method in environments where strategic heuristics are less structured.</p>
<p>Results. As shown in Table</p>
<p>Does Generating Diverse Prompts Improve</p>
<p>Performance?We assess whether prompt diversity improves decision quality within the MAD framework.To this end, we compare two settings: (1) identical prompts generated via the Strategic Prior Knowledge Elicitation module for both agents ((+)MAD), and (2) distinct, self-generated prompts per agent, as in DReaMAD (Figure 6, left).Experiments were conducted on four variants of the MetaNIM Arena simulator, NIM (Normal and Misère) and Fibonacci (Normal and Misère), over 50 episodes each ( §A.4).Our results show that incorporating diverse prompts within the MAD framework significantly enhances performance, validating the effectiveness of our Perspective Diversification module.</p>
<p>We also examine the effect of sampling temperature on prompt diversity in the Fibonacci task.Within both the Strategic Prior Knowledge Elicitation and Perspective Diversification modules, we vary the temperature from 0.0 to 1.0.As shown in Figure 6 right, higher temperature (further diversity) correlates with increased optimal action accuracy, indicating that greater diversity in generated prompts contributes to improved reasoning performance.</p>
<p>Generalization to Math and Commonsense Reasoning</p>
<p>While our primary benchmark focuses on structured games, we further evaluate DReaMAD on NLP tasks to test its broader applicability.Specifically, we consider algebra and number theory problems from AIME 2024 and AMC 2023, as well as CommonsenseQA (Talmor et al., 2018), which require symbolic reasoning and multi-step inference.These tasks are representative of domains where chainof-thought reasoning is essential, making them suitable for evaluating generalization.We also examine whether DReaMAD benefits reasoning-specialized models, such as GPT-o3-mini, in a similar manner as general-purpose LMs.Experiments are conducted using standard accuracy metrics across model-task pairs.Results in Table 5 and Table 6 show that DReaMAD not only boosts accuracy across heterogeneous tasks but also yields clear gains on reasoningoriented models such as GPT-o3-mini, underscoring the method's robustness well beyond the structured-game domain.</p>
<p>Conclusions</p>
<p>Our study shows that Multi-Agent Debate (MAD) often reinforces biases instead of reducing them, leading to suboptimal reasoning.Through our experiments with the MetaNIM Arena, we have observed that models persist in biased reasoning even when presented with superior alternatives.</p>
<p>While our current strategy focuses on strategic games, the principles of structured self-refinement and diversified reasoning could be valuable for a wider range of NLP tasks.These include complex activities such as multi-step reasoning in question answering, legal analysis, and scientific inference.Future work will explore how these techniques enhance decision-making beyond structured games.</p>
<p>Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa: A question answering challenge targeting commonsense knowledge.arXiv preprint arXiv:1811.00937,2018.</p>
<p>Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et  Success: Goal Achieved 13:</p>
<p>end if 14: end while A.1.Theoretical Background on Combinatorial Impartial Games Each game in MetaNIM Arena is a combinatorial impartial game.We begin by outlining the relevant theoretical foundation, starting with the Sprague-Grundy theorem.</p>
<p>A.1.1. SPRAGUE-GRUNDY THEOREM</p>
<p>The Sprague-Grundy theorem provides a fundamental method for analyzing impartial combinatorial games by decomposing complex games into simpler, independent subgames.As discussed in Section 3.4, every impartial combinatorial game can be represented as a directed acyclic graph (DAG).However, directly computing Grundy numbers recursively from terminal states is often impractical.</p>
<p>We summarize key results from Sprague (1935) and Grundy (1939).According to the theorem, the optimal strategy for playing multiple impartial games simultaneously (in parallel), or a single complex game viewed as multiple independent subgames, is equivalent to playing a single game of Nim with multiple heaps.This equivalence arises from the concept of the disjunctive sum of DAGs.
Definition A.1 (Disjunctive Sum of DAGs). Let G 1 = (X 1 , F 1 ), G 2 = (X 2 , F 2 ), . . . , G n = (X n , F n ) be DAGs representing n impartial combinatorial games. The disjunctive sum of G 1 , . . . , G n is a DAG G = (X, F ) defined as follows: 1. The vertex set X is the Cartesian product X 1 × X 2 × • • • × X n .
2. The edge set F consists of edges connecting (x 1 , . . ., x n ) to (y 1 , . . ., y n ) if and only if exactly one pair (x i , y i ) is in F i , and x j = y j for all j ̸ = i.</p>
<p>Note.In a disjunctive sum of DAGs, each player chooses exactly one subgame to play during their turn and moves within that subgame.The entire game ends when all subgames reach terminal positions.</p>
<p>Theorem A.2 (Sprague-Grundy (Sprague, 1935;Grundy, 1939)).A position S is losing if and only if its Grundy number G(S) = 0; otherwise, if G(S) ̸ = 0, it is winning.Furthermore, if a position S decomposes into independent subpositions S 1 , . . ., S k via the disjunctive sum of DAGs, then
G(S) (2) = G(S 1 ) (2) ⊕ G(S 2 ) (2) ⊕ • • • ⊕ G(S k ) (2) ,
where ⊕ denotes bitwise XOR.</p>
<p>This result implies that Grundy numbers for complex games, such as Kayles or Chomp, can be efficiently computed by decomposing them into simpler subgames and combining the Grundy numbers using bitwise XOR.</p>
<p>For example, consider a variant of the game Kayles played on two separate rows of pins, each forming an independent subgame.Suppose we computed the Grundy numbers separately for these rows, obtaining Grundy numbers 7 for the first row and 4 for the second row.By the Sprague-Grundy theorem, the combined Grundy number of the position is given by:
7 (2) ⊕ 4 (2) = 111 (2) ⊕ 100 (2) = 011 (2) = 3.
Thus, even though the original game involves two distinct rows of pins, the strategic analysis reduces precisely to analyzing a Nim heap of size 3. Since a Nim heap of size 3 is nonzero, this indicates a winning position for the player about to move.</p>
<p>A.1.2. BASIC DISCUSSIONS ON THE OPTIMAL STRATEGY</p>
<p>Why G(S) = 0 Implies Losing.Recall that G(S) is defined as:
G(S) = mex G(S ′ ) S ′ is reachable from S ,
where mex(X) is the smallest nonnegative integer not in the set X. Thus,
G(S) = 0 ⇐⇒ 0 / ∈ G(S ′ ) : S ′ is reachable from S .
Concretely, if G(S) = 0, then no valid move leads to a successor S ′ with G(S ′ ) = 0.In other words, from S, the player to move cannot transition the game into a G(•) = 0 state.Because a state G(S ′ ) = 0 corresponds to a losing position for the player who faces it, the mover in state S has no way to force the opponent into a losing position on the next turn.Hence, S is losing for the player to move.</p>
<p>Why G(S) ̸ = 0 Implies Winning (Opposite viewpoint).By the same logic, if G(S) ̸ = 0, then the definition of mex guarantees 0 does appear among the Grundy values G(S ′ ) of the successors.Thus, there exists some child state S ′ for which G(S ′ ) = 0. Consequently, the current mover can place the opponent directly into a losing position (i.e. a position with Grundy number 0).Recursively iterating this argument along the Directed Acyclic Graph of states ensures that the current mover, if playing optimally, keeps forcing the opponent into G(•) = 0 states until the game ends.Therefore, S must be a winning state.</p>
<p>Misère Variant.Misère play reverses the normal condition: taking the last object loses rather than wins.Although standard Sprague-Grundy analysis still applies to most states, a special exception arises when all heaps (or subpositions) are size 1, such as in misère Nim.In that endgame scenario, the usual strategy must switch to avoid forcing the final move, ensuring the player leaves the opponent to pick the last object.</p>
<p>A.2. Game Variants in MetaNIM Arena</p>
<p>A.2.1. NIM</p>
<p>Nim is a mathematical strategy game where two players alternate turns removing objects from distinct heaps/piles.The classic version follows these rules:</p>
<p>• Heaps: The game starts with k heaps containing n 1 , n 2 , . . ., n k objects respectively</p>
<p>• Moves: On their turn, a player must remove at least 1 to previously fixed number of objects from exactly one heap</p>
<p>• Objective: The player who takes the last remaining object wins (normal play convention)</p>
<p>Mathematical Strategy</p>
<p>The game can be analyzed using binary representations through the concept of Nimbers (Grundy numbers).For any position, the key is to calculate the binary XOR (exclusive OR) sum of all heap sizes:
Nim-sum = n 1 ⊕ n 2 ⊕ • • • ⊕ n k
A position is losing if the Nim-sum equals 0. The winning strategy consists of always moving to a position with Nim-sum 0.</p>
<p>For the single-heap variant (as in our current game), this simplifies to maintaining modular arithmetic conditions.</p>
<p>Example Consider a game with heaps [3, 4, 5]:
3 = 011 2 4 = 100 2 5 = 101 2 Nim-sum = 011 2 ⊕ 100 2 ⊕ 101 2 = 010 2 = 2 ̸ = 0
The first player can win by removing 2 objects from the 5-object heap to make the new Nim-sum 0.</p>
<p>Variants Several Nim variants exist, including:</p>
<p>• Single-heap Nim (as in our current game)</p>
<p>• Misère Nim (player taking last object loses)</p>
<p>• Multi-heap Nim with different removal constraints</p>
<p>The fundamental mathematical principles of combinatorial game theory apply to all variants.</p>
<p>A.2.2. FIBONACCI</p>
<p>The Fibonacci Game, also known as Fibonacci Nim, is a combinatorial number game where players alternate removing items from a pile, with move constraints based on the Fibonacci sequence.The rules are:</p>
<p>• Initial Move: First player takes 1 ≤ k &lt; n items from a pile of n items</p>
<p>• Subsequent Moves: Each player must take between 1 and twice the number of items taken by their opponent in the previous move</p>
<p>• Objective: The player who takes the last item wins</p>
<p>Mathematical Strategy</p>
<p>The game is governed by Fibonacci numbers (F 1 = 1, F 2 = 2, F n = F n−1 + F n−2 ) and Zeckendorf's Theorem, which states that every positive integer can be uniquely expressed as a sum of non-consecutive Fibonacci numbers.</p>
<p>• Losing Positions: Pile sizes equal to Fibonacci numbers (F n )</p>
<p>• Winning Strategy: Reduce the pile to the largest Fibonacci number smaller than the current size For a pile of size m, its Zeckendorf representation is:
m = F k1 + F k2 + • • • + F kr (|k i − k j | ≥ 2)
The optimal first move is to remove the smallest Fibonacci number in this decomposition.</p>
<p>Example For a starting pile of m = 20:</p>
<p>Zeckendorf: 20 = 13 + 5 + 2 (F 7 = 13, F 5 = 5, F 3 = 2)</p>
<p>First move = Remove smallest term 2</p>
<p>New pile = 18 = 13 + 5</p>
<p>Now the opponent faces a position composed purely of Fibonacci numbers.Any move they make (1 ≤ x ≤ 4) can be countered by reducing the pile to the next Fibonacci number.</p>
<p>Key Properties</p>
<p>• If m is a Fibonacci number, the first player will lose against perfect play</p>
<p>• The number of moves in a game is always ≤ the index of the largest Fibonacci number ≤ m</p>
<p>• The golden ratio ϕ = 1+</p>
<h1>Game Role:</h1>
<p>You are {agent['name']}, a participant in a simple Fibonacci game.</p>
<h1>Objective:</h1>
<p>Your goal is to win the game by taking all remaining stones on your turn, leaving no stones for your opponent.The person who takes the last stones wins.</p>
<h1>Game Rule:</h1>
<p>1.There is a single pile of stones.2. Players take turns one after another.</p>
<ol>
<li>The first player can take any number of stones, but not all the stones in the first move.4. On subsequent turns, the number of stones a player can take must be at least 1 and at most twice the number of stones the previous player took. 5.The player who takes the last stone wins the game.</li>
</ol>
<h1>Current State:</h1>
<p>There are {remaining items} stones remaining in the pile.</p>
<h1>Task:</h1>
<p>You are the first player.Based on the current state of the game, decide how many items you will take (between 1 and {remaining items -1}) on this turn.</p>
<h1>Objective:</h1>
<p>Move the queen so that you are the first to place it on the lower-left corner square.</p>
<h1>Game Rule:</h1>
<ol>
<li>Board size: {board height}×{board width}.Standard Prompting (Table 15) Standard prompting provides a direct task description, outlining the game rules, current state, and the required decision.The model is expected to generate only an action to determine the best move.This method is cheap and efficient but often leads to suboptimal decisions, as the model may fail to make proper reasoning before selecting the action.</li>
</ol>
<p>ReAct Prompting (Table 16) ReAct prompting provides a direct task description, outlining the game rules, current state, and the required decision.The model is expected to generate an action with proper explicit reasoning steps to determine the best move.This method is efficient but often leads to suboptimal decisions, as the model may fail to retrieve and apply deeper strategic reasoning.</p>
<p>Chain-of-Thought (CoT) Prompting (Table 17) CoT prompting extends the standard prompt by explicitly instructing the model to think step-by-step before making a decision.By guiding the model through an explicit reasoning process, CoT enables it to break down the problem, consider strategic implications, and refine its choices before committing to an action.This often leads to improved decision-making, particularly in multi-step strategic environments where deeper reasoning is required.</p>
<p>Key Difference and Impact</p>
<p>As illustrated in Table 17, the only modification in the CoT prompt is the addition of a simple directive: "Let's think step-by-step.What is the best move for you?"This small change significantly alters the model's reasoning trajectory, encouraging more structured and strategic decision-making.Our experimental results (detailed in §3) confirm that CoT prompting leads to a measurable improvement in decision accuracy, particularly in complex scenarios where retrieving and applying prior knowledge is essential.</p>
<p>By leveraging CoT, we can enhance the model's ability to explain its decisions, mitigate biases, and adapt more effectively to adversarial settings.However, as we further discuss in Experiment Section, CoT has limitations to leverage the strategic reasoining well in our proposed environment, necessitating additional mechanisms to further enhance strategic reasoning.</p>
<h1>Objective:</h1>
<p>Your goal is to win the game by taking all remaining items on your turn, leaving no items for your opponent.The person who takes the last item wins.</p>
<h1>Game Rule:</h1>
<p>There is a single pile of items.You can take between 1 and {max take} items on your turn.</p>
<h1>Current State:</h1>
<p>There are {remaining items} items remaining in the pile.</p>
<h1>Task:</h1>
<p>Based on the current state of the game, decide how many items you will take (between 1 and {max take}) on this turn.</p>
<p>Output Format:</p>
<p>The output should be a Markdown code snippet with the following scheme, including leading and trailing triple backticks with "json" and: ''' { action: integer // This is an action you take.Only integer between 1 and 3. } ''' Prompting Strategy for Opponent Modeling Anything can act as an opponent in the MetaNIM Arena simulator, but we model OpenAI's GPT-4o, the most powerful LLM model currently available, as the opponent and apply the ReAct prompting method.</p>
<h1>Objective:</h1>
<p>Your goal is to win the game by taking all remaining items on your turn, leaving no items for your opponent.The person who takes the last item wins.</p>
<h1>Game Rule:</h1>
<p>There is a single pile of items.You can take between 1 and {max take} items on your turn.</p>
<h1>Current State:</h1>
<p>There are {remaining items} items remaining in the pile.</p>
<h1>Task:</h1>
<p>Based on the current state of the game, decide how many items you will take (between 1 and {max take}) on this turn.</p>
<p>Output Format:</p>
<p>The output should be a Markdown code snippet with the following scheme, including leading and trailing triple backticks with "json" and: ''' { reasoning: string // This is the reason for the action action: integer // This is an action you take based on the reasoning.Only integer between 1 and 3. } '''</p>
<h1>Objective:</h1>
<p>Your goal is to win the game by taking all remaining items on your turn, leaving no items for your opponent.The person who takes the last item wins.</p>
<h1>Game Rule:</h1>
<p>There is a single pile of items.You can take between 1 and {max take} items on your turn.</p>
<h1>Current State:</h1>
<p>There are {remaining items} items remaining in the pile.</p>
<h1>Task:</h1>
<p>Based on the current state of the game, decide how many items you will take (between 1 and {max take}) on this turn.</p>
<p>Output Format:</p>
<p>The output should be a Markdown code snippet with the following scheme, including leading and trailing triple backticks with "json" and: ''' { reasoning: string // This is the reason for the action action: integer // This is an action you take based on the reasoning.Only integer between 1 and 3. } ''' Let's think step-by-step.What is the best move for you?Despite the remarkable problem-solving capabilities of Large Language Models (LLMs), their reasoning is highly sensitive to subtle changes in prompt phrasing.As demonstrated in Figure 8, even a single word variation in the prompt can significantly alter the reasoning process and final decision-making.This phenomenon underscores a critical limitation in LLM-based strategic reasoning: models do not inherently generalize optimal strategies but instead rely on heuristic cues embedded within the prompt.D.1.1.IMPACT OF WORD CHOICE ON STRATEGIC REASONING Figure 8 compares LLM responses when the word Fibonacci is explicitly mentioned versus when it is omitted in an identical game scenario.In the presence of the keyword Fibonacci, the model aligns its reasoning with Fibonacci-based strategy, leveraging number sequences to maintain control over the game.Conversely, when the term is absent, the model defaults to an alternative heuristic, such as maintaining a multiple of three or even resorting to a trivial greedy strategy.For instance, in the first decision step, when instructed with Fibonacci, the model identifies 13 as the closest Fibonacci number and takes 7 stones, ensuring an advantageous future state.Without the keyword, however, the model applies a modulo-based heuristic, taking only 2 stones to leave a multiple of three.Similarly, in the second decision step, the Fibonacci-aware model deliberately leaves 8 stones in the pile-another Fibonacci number-while the other instance simply takes all remaining stones without strategic foresight.</p>
<p>D.1.2. IMPLICATIONS FOR ROBUST PROMPTING</p>
<p>This stark contrast highlights the fundamental issue that LLMs do not inherently retrieve the most effective strategic reasoning but are instead disproportionately influenced by linguistic cues.The reliance on explicit terminology for optimal reasoning raises concerns about robustness, as different wordings of the same task can lead to dramatically different problem-solving approaches.This suggests that ensuring reliable strategic reasoning in LLMs requires more than just fine-tuned prompts; it necessitates methods that encourage models to autonomously retrieve and apply domain knowledge without over-reliance on explicit wording cues.</p>
<p>These observations motivate our approach in DReaMAD, where we systematically refine LLMs' strategic reasoning by structuring prior knowledge retrieval and diversifying input perspectives.By mitigating the sensitivity to prompt variations, our method enhances the robustness and consistency of LLM decision-making across different strategic environments.</p>
<p>Figure 1 .
1
Figure 1.An example demonstrating how the debate process converges to a biased outcome.We observed that bias reinforcement occurs in the first debate.Blue text indicates the correct reasoning and orange text indicates the strong consistent (biased) reasoning.The second debate is omitted, as its procedure replicates the first and third; all debates use GPT-4o-mini as the debating agent.</p>
<p>Figure 2 .
2
Figure 2. Bias reinforcement in NIM game by MAD.We compared initial action distribution and action distribution after 3 rounds of debates.MAD amplifies a model's biases, making debates favor consistent but potentially incorrect responses.</p>
<p>Figure 3 .
3
Figure 3. Decline in optimal actions over debate rounds, demonstrating the convergence toward consistently biased reasoning.</p>
<p>Figure 4 .
4
Figure 4. DReaMAD framework.DReaMAD improves LLM reasoning by combining Strategic Prior Knowledge Elicitation and Perspective Diversification.In the first stage, the model reinterprets the problem and formulates high-level strategies to reduce bias.In the second stage, multiple agents adopt distinct viewpoints, engage in structured debate, and refine their conclusions to enhance decision-making.</p>
<p>Figure 5 .
5
Figure5.In this example, we illustrate how the debate process converges to an optimal outcome using DReaMAD.We begin with the same current state shown in Figure1, employing self-generated prompts for each LLM agent.</p>
<p>2023), Chain-of-Thought (CoT), Self-Consistency (Wang et al., 2022), Self-Refinement (Madaan et al., 2024), and Multi-Agent Debate (MAD (Du et al., 2023), MAD2 (Liang et al., 2023)).The details of standard and CoT prompts are provided in Appendix B.</p>
<p>Figure 6 .
6
Figure 6.Effect of perspective diversification.Left: Average win rate of (+)MAD and DReaMAD on NIM and Fibonacci (Normal and Misère variants), aggregated over 50 simulations per setting.Right: Accuracy on the Fibonacci benchmark with GPT-4o across different sampling temperatures (15 runs each, 95% CI).Higher temperatures yield greater prompt diversity, leading to improved accuracy.</p>
<p>Nim (last player to move loses)• Multi-pile Fibonacci games• Constrained Fibonacci sequences (e.g., Tribonacci variants)    This game demonstrates deep connections between combinatorial game theory, number theory, and the Fibonacci sequence.A.2.3.KAYLESKayles is an impartial combinatorial game played with a linear arrangement of pins where players alternate knocking down pins under specific adjacency rules.First analyzed in 1929 by Dudeney and later studied by Conway and Berlekamp, it demonstrates complex mathematical patterns.Basic Rules• Initial Setup: A row of n identical pins • Moves: On each turn, a player must either:-Knock down 1 pin -Knock down 2 adjacent pins • Objective: Last player to make a valid move wins (normal play convention)</p>
<ol>
<li>Coordinates use zero-based indices [row, col].Row 0 is the top row; Col 0 is the leftmost column.Valid ranges: row ∈ [0, board height − 1], col ∈ [0, board width − 1]. 3. From the current position [r, c] the queen may move to <strong>one</strong> of: (a) left: [r, c ′ ] with c ′ &lt; c; (b) down: [r ′ , c] with r ′ &gt; r; (c) left-down diagonal: [r + d, c − d] with d &gt; 0. 4. The game ends when the queen reaches [row = board height-1, col = 0].#Current State: Current position: [row = {r}, col = {c}].#Task: Based on the current state, decide the next move [row, col].B.2.Prompts for basic reasoningB.2.1.STANDARD, REACT &amp; COT PROMPTSIn our evaluation of LLMs within the MetaNIM Arena, we compare two key prompting techniques: Standard Prompting and Chain-of-Thought (CoT) Prompting.The distinction between these approaches significantly impacts the model's reasoning and decision-making process.</li>
</ol>
<p>Figure 8.According the word Fibonacci usage, the reasoning and the performance differs.</p>
<p>Table 1 .
1
Feature comparison between self-correction methods.In contrast to Multi-Agent Debate, DReaMAD encourages diverse viewpoints by varying prompts and enhancing debate robustness through automated knowledge structuring.
MethodsSingle Model Usage Multiple Instances Rethinking Process Self-Feedback Diversity in reasoning Perspective ShiftsSelf-Consistency✓✓✗✗✗Self-Refinement✓✗✓✓✗✗Multi-Agent Debate✓✓✓✗✗DReaMAD (Ours)✓✓✓✗✓✓: means diversity from LLM's randomness, controlled by temperature hyperparameter.</p>
<p>Table 2 .
2
Bias reinforcement across models: showing that even after the debate concludes, strongly consistent actions continue to exhibit strong consistency, reinforcing biased action distributions in the Fibonacci game.A wrong bias occurs when the model's biased response deviates from the optimal action, while a good bias refers to cases where the biased response aligns with the optimal action.(a, b) indicate the state where the remaining items are a, and player can take the items maximum to b.
GPT-4oWrong BiasGood BiasGPT-4o-miniWrong BiasGood Bias(20, 19) (12, 4) (7, 4) (15, 10) (16, 8) (7, 7)(18, 4) (12, 6) (10, 4) (15, 10) (7, 2) (15, 2)Standard0.7000.6750.6000.5250.7250.850Standard0.7000.8750.6000.9750.9500.975+ After MAD0.9000.7500.7000.7500.7500.900+ After MAD0.8500.9500.7501.0000.9501.000GEMINI-1.5-proWrong BiasGood BiasGEMINI-1.5-flashWrong BiasGood Bias(20, 19) (12, 4) (4, 4) (15, 10) (10, 4) (16, 8)(12, 6) (12, 4) (7, 4) (15, 4) (20, 19) (7, 7)Standard0.6500.6000.6000.8000.6250.675Standard0.8000.7000.5250.7500.5000.750+ After MAD0.7000.6500.8000.8000.8000.700+ After MAD0.8501.0000.7500.7500.6501.0003.4. Combinatorial Games: Theory and StrategyAll MetaNIM Arena games are impartial combinatorialgames, forming a Directed Acyclic Graph (DAG) wherevertices represent game states and edges denote valid moves.</p>
<p>Table 3 .
3
Effect of Strategic Prior Knowledge Elicitation module.DReaMAD(−)indicates our method except multi-agent debate process.We can fully evaluate reasoning ability between different prompting methods.The metric accuracy of selecting optimal action is used.The best results are highlighted in bold.
LLM ModelsPrompting MethodsNIMFibonacciChompKaylesAverageReAct0.95 ± 0.040.33 ± 0.040.18 ± 0.070.19 ± 0.080.41GPT-4o+ CoT-Prompting0.96 ± 0.040.43 ± 0.110.28 ± 0.090.20 ± 0.100.47DReaMAD (−)0.98 ± 0.040.44 ± 0.090.23 ± 0.100.23 ± 0.120.47ReAct0.75 ± 0.050.33 ± 0.040.40 ± 0.070.12 ± 0.060.40GPT-4o-mini+ CoT-Prompting0.84 ± 0.080.36 ± 0.060.61 ± 0.050.02 ± 0.030.46DReaMAD (−)1.00 ± 0.000.49 ± 0.170.62 ± 0.100.18 ± 0.110.57ReAct0.82 ± 0.060.42 ± 0.040.19 ± 0.080.57 ± 0.040.50GEMINI-1.5-pro+ CoT-Prompting0.88 ± 0.050.47 ± 0.110.22 ± 0.110.59 ± 0.040.54DReaMAD (−)0.97 ± 0.040.53 ± 0.070.24 ± 0.050.72 ± 0.120.62ReAct0.94 ± 0.020.35 ± 0.040.05 ± 0.030.01 ± 0.020.34GEMINI-1.5-flash+ CoT-Prompting0.93 ± 0.020.33 ± 0.070.09 ± 0.040.0 ± 0.000.34DReaMAD (−)0.97 ± 0.040.45 ± 0.060.05 ± 0.000.42 ± 0.060.46ReAct0.870.360.210.22-Average+ CoT-Prompting0.900.400.300.20-DReaMAD (−)0.980.480.290.39-</p>
<p>Table 4 .
4
Winning rate comparison across different models and different self-correction methods.This is an result based on MetaNIM Arena simulator.The best results are highlighted in bold.
LLM ModelsPrompting MethodsNIM Normal Misère Normal Misère Single 2 Rows Rectangular Square Fibonacci Kayles ChompC.Queen NormalStandard Prompting0.320.540.160.100.540.560.780.180.46+ ReAct0.100.680.160.760.500.300.420.120.46+ Self-Refinement0.140.660.180.360.500.460.460.160.42GEMINI-1.5-flash+ Self-Consistency0.040.280.280.860.300.240.740.00.34+ MAD0.060.300.120.780.540.200.740.140.58+ MAD20.260.260.080.060.440.360.680.100.58+ DReaMAD0.380.840.160.940.580.620.600.220.74Standard Prompting0.380.540.220.280.460.480.460.380.12+ ReAct0.220.680.200.340.400.260.580.340.28+ Self-Refinement0.220.700.180.500.460.520.520.440.24GPT-4o-mini+ Self-Consistency0.140.520.340.460.320.200.540.260.30+ MAD0.280.620.220.640.420.280.520.560.44+ MAD20.340.200.180.180.500.340.440.900.14+ DReaMAD0.980.740.540.720.680.840.640.220.76
6.1.Does DReaMAD Improve Reasoning Quality?</p>
<p>Table 5 .
5
Accuracy (%) on math-reasoning benchmarks.Columns group the five algorithms for each backbone model.Bold indicates the best algorithm for a given (dataset, model) pair.
DatasetGPT-o3-miniGPT-4oReAct Self-Refinement Self-Consistency MAD DReaMAD ReAct Self-Refinement Self-Consistency MAD DReaMADAIME 202476.773.386.773.390.00.03.310.03.310.0AMC 202397.510010010010060.052.552.560.062.5</p>
<p>Table 6 .
6
Accuracy (%) on CommonsenseQA dataset.Bold indicates the best performance.We abbreviate Self-Refinement as Self-Refine.and Self-Consistency as Self-Consist.
ModelReAct Self-Refine. Self-Consist. MAD DReaMADGPT-4o83.649.283.682.084.4GPT-4o-mini78.761.578.775.479.5</p>
<p>Table 9 .
9
MetaNIM Arena Simulator: Kayles, Chomp, and Corner Queen
B. Prompts DesignB.1. Game Prompts</p>
<p>Table 10 .
10
NIM game basic input prompt.Your goal is to win the game by taking all remaining items on your turn, leaving no items for your opponent.The person who takes the last item wins.#Game Rule: There is a single pile of items.You can take between 1 and {max take} items on your turn.#Current State: There are {remaining items} items remaining in the pile.#Task: Based on the current state of the game, decide how many items you will take (between 1 and {max take}) on this turn.</p>
<h1>Game Role:You are {agent['name']}, a participant in a game of Nim variants.#Objective:</h1>
<p>Table 11 .
11
Fibonacci game basic input prompt.</p>
<p>Table 14 .
14
Corner-Queen game basic input prompt.You are {agent['name']}, a participant in a Corner-Queen game.</p>
<h1>Game Role:</h1>
<p>Table 15 .
15
Standard Prompt in NIM #Game Role: You are {agent['name']}, a participant in a game of Nim variants.</p>
<p>Table 16 .
16
ReAct Prompt in NIM #Game Role: You are {agent['name']}, a participant in a game of Nim variants.</p>
<p>Table 17 .
17
CoT Prompt in NIM #Game Role: You are {agent['name']}, a participant in a game of Nim variants.</p>
<p>AcknowledgmentsThis work was supported by Center for Applied Research in Artificial Intelligence (CARAI) grant funded by DefenseAcquisition Program Administration (DAPA) and Agency for Defense Development (ADD) (UD230017TD).Impact StatementThis work advances Machine Learning by enhancing LLMs' strategic reasoning through our approach, addressing bias reinforcement and lack of perspective diversity in Multi-Agent Debate (MAD).Our method improves decision-making in adversarial settings, with potential applications in automated negotiations, economics, and multi-agent systems.However, stronger AI-driven strategies could be misused in manipulative or deceptive contexts.To ensure ethical deployment, future research should focus on integrating fairness constraints and transparency mechanisms in AI decision-making.Mathematical AnalysisThe game is analyzed using Grundy numbers and the Sprague-Grundy theorem.Positions split into independent segments after moves create disjunctive game components.• Let G(n) be the Grundy number for a row of n pins • Recursive Grundy number calculation:A row of 4 pins has Grundy number 1, making it a winning position.Strategic Principles• Split long rows into independent segments with XOR-sum 0• Mirror opponent's moves in symmetric positions • Avoid leaving isolated single pinsVariants• Circular Kayles (pins arranged in a circle)• Multi-row Kayles • k-Kayles (allow knocking down up to k adjacent pins)• Misère Kayles (last player to move loses) Computational Complexity Kayles is:• PSPACE-complete for general positions • Solved in linear time for standard single-row play • Used in complexity theory to study impartial games This analysis demonstrates how simple rule sets can generate complex mathematical structures.The complete Grundy number sequence for Kayles was only fully determined through extensive computational analysis.A.2.4. CHOMPChomp is an impartial combinatorial game first formulated by David Gale in 1974.Played on a rectangular grid representing a chocolate bar, it features unique topological constraints and demonstrates fundamental principles of partially ordered sets (posets).Basic Rules• Initial Setup: An m × n rectangular grid of "chocolate squares"• Special Square: The lower-left square (position (1,1)) is poisoned • Moves: On each turn, a player must:-Select any remaining square -Remove ("chomp") all squares above and/or to the right of the selected square • Objective: Avoid taking the poisoned square -last player to make a valid move wins (normal play convention) Mathematical Analysis Chomp is particularly significant in combinatorial game theory because:• It is a partisan game with inherent asymmetry • The starting position is a poset under component-wise ordering• A winning strategy exists for the first player (proven by strategy-stealing argument), though explicit strategies are unknown for most grid sizes Key Theorem(Gale, 1974)Theorem A.3.For any initial grid size m × n where m, n ≥ 2, the first player has a winning strategy.Example: 2 × 3 GridFirst player wins by taking the (2,3) square:• If Player 2 takes (1,3), Player 1 takes (2,2)• If Player 2 takes (2,2), Player 1 takes (1,2)• All paths eventually force Player 2 to take the poisonStrategic Principles• Maintain control of the antidiagonal• Force symmetry when possible• Reduce the game to independent subgames• Avoid leaving isolated columnsComputational Complexity• General Chomp is PSPACE-complete• Solved in polynomial time for:-2 × n grids -Square grids up to 5 × 5• Number of winning positions grows exponentially with grid sizeVariants• 3D Chomp (cuboidal grids)• Circular Chomp• Hypergraph Chomp• Misère Chomp (taking poison square wins)• Numerical Chomp (played on factor lattices)Significance Chomp demonstrates fundamental connections between:• Combinatorial game theory • Computational complexity• Algebraic geometry (via Gröbner basis interpretations)Despite its simple rules, Chomp remains unsolved for general grid sizes, making it an active research area in computational combinatorics.A.2.5.CORNER QUEEN Corner Queen is a two-player combinatorial impartial game played on a rectangular grid, where a single queen starts at an arbitrary position and players alternate moving it toward the bottom-left corner.The player who moves the queen to the target wins.Basic Rules• Initial Setup: A queen is placed on an m × n grid at position (x, y)• Moves: On each turn, a player moves the queen in one of the following directions:for any k ∈ Z &gt;0 such that the move stays within the board• Objective: The player who moves the queen to position (0, 0) wins Mathematical Analysis Corner Queen is mathematically equivalent to Wythoff's Game, a well-studied impartial game in combinatorial game theory.Each game state (x, y) corresponds to a position on the board where x, y ∈ N and x ≤ y (without loss of generality).• The game's Grundy function G(x, y) satisfies analyzed via Beatty sequences for Wythoff's Game• The losing positions (also called P -positions) are given by pairs of the form:is the golden ratio • Examples: The first few P -positions are (1, 2), (3, 5), (4, 7), (6, 10), (8, 13), and (9, 15).Optimal Strategy• A position is losing if and only if it lies on the Wythoff pairs described above• The winning strategy is to move the queen to the nearest P -position • These positions are sparse and non-periodic, but can be computed efficiently using Beatty sequencesA.3. Our Dataset used in ExperimentsWe construct simple dataset based on the MetaNIM Arena.This dataset doesn't require any opponent model because samples in this dataset is focusing on the specific scene in each game.A.4. Our Simulator used in ExperimentsWe build simulator based on the MetaNIM Arena.This simulator requires any opponent available to receive prompt and make an output as an action.Here, we utilize the gpt-4o model as an opponent.#Game Role:You are {agent['name']}, a participant in a game of Kayles.#Objective:Your goal is to win the game by leaving your opponent with no valid moves.The player who takes the last pin(s) wins.#Game Rule:1.There is a single row of pins.2. On your turn, you can remove:• 1 pin,• 2 adjacent pins.3. You cannot remove non-adjacent pins or pins that have already been removed.#Current State:The row of pins is represented as a binary string:-'1' means the pin is still there.-'0' means the pin has already been removed.Current state: {remaining pins} #Task:Based on the current state of the game, decide which pin(s) you will take on this turn.Table13.Chomp game basic input prompt.#Game Role:You are {agent['name']}, a participant in a game of Chomp.#Objective:Your goal is to force your opponent to take the top-left corner of the grid (position (0, 0)).#Game Rule:1.The game is played on a square grid.2. On your turn, you select a position (row, col).3. All positions to the right and below the selected position are removed.4. The player forced to select (0, 0) loses.#Current State:The grid is represented as a binary matrix, where '1' means the position is still available, and '0' means it is removed: {remaining grid}#Task:Based on the current state of the grid, decide which position (row, col) you will select.C. DReaMAD (−) : Structured Prompt Optimization without DebateWhile the full DReaMAD framework integrates multi-agent debate to refine strategic reasoning, its core prompting methodology-excluding debate-remains a powerful mechanism for enhancing decision-making.This streamlined version, DReaMAD (−) , focuses on three key stages to systematically extract and refine strategic knowledge, improving reasoning diversity and mitigating bias.We present the DReaMAD prompt as in Table18.1. Game Situation Reinterpretation The first step involves extracting fundamental game principles from the standard prompt.The model is tasked with identifying key elements, such as:• Game Definition: The nature of the game and its mechanics.• Winning Condition: The criteria for victory.• Move Constraints: The permissible actions per turn.This step ensures that the model builds a structured understanding of the strategic environment before making decisions.General Strategy FormulationAfter extracting the core game elements, the model derives a generalized winning strategy applicable to various game states.It generates:• State Evaluation: How to assess the game state at any given turn.• Winning Strategy: The optimal decision-making framework for victory.• Endgame Tactics: Best strategies in near-win scenarios.This formulation helps structure the model's reasoning beyond the immediate game context, fostering more strategic foresight.Perspective DiversificationFinally, the model refines the original prompt using the extracted strategic knowledge.This process introduces structured variations to the initial prompt to encourage diverse reasoning, rather than reinforcing a singular bias.The self-refined prompt:• Guides decision-making explicitly.• Prioritizes winning strategies.• Encourages logical, step-by-step reasoning.This structured refinement ensures that LLMs adopt distinct strategic viewpoints even without external debate, improving their adaptability and robustness in adversarial environments.By systematically structuring knowledge retrieval and refining prompts, DReaMAD (−) enhances strategic reasoning as illustrated in Figure10, this approach strengthens the model's ability to retrieve and apply prior knowledge effectively, offering a scalable solution for improving LLM-based decision-making.Submission and Formatting Instructions for ICML 2025 While DReaMAD requires a single model, its inference involves additional prompt steps (e.g., prior knowledge elicitation) and a debate process.However, we believe this test-time scaling method is much more efficient than train-time scaling.As we utilized language models through an API, it was challenging to perform a precise quantitative comparison (e.g., GPU usage time) between our test-time scaling approach and traditional model training.Therefore, we compared the costs using dollar amounts, specifically contrasting the API cost per single game using our method versus the costs incurred when OpenAI fine-tuning models on constructed datasets for NIM-N games.The NIM-N dataset is constructed by mixing data from three different variants of the NIM game.In all variants, the game starts with 31 stones remaining; however, the rules differ in terms of the maximum number of stones that can be removed per turn-3, 4, or 5, respectively.For each variant, eight distinct game states were sampled and the corresponding optimal action was used as the label, yielding a total of 24 training examples.Additionally, testing was conducted on the aforementioned three scenarios (each 50 games) by using the GPT-4o model as the opponent.The win rate was measured for each scenario, and the average win rate across these variants was reported.The results of this comparison are illustrated in the table above.When fine-tuning a model using API-based fine-tuning (GPT-4o-mini), the performance gradually improved with additional training epochs, achieving win-rates of 0.253, 0.300, 0.420, and 0.460 at 1, 2, 3, and 4 epochs respectively, with corresponding API costs of $0.013, $0.023, $0.033, and $0.043 (Here, the cost of constructing dataset is not included).In contrast, our proposed method, DReaMAD, achieved significantly higher performance (0.966) with substantially lower API costs ($0.0098).These results strongly suggest that our approach not only outperforms traditional fine-tuning methods but also is far more cost-efficient.All the price is calculated by the pricing policy: https://openai.com/api/pricing/E.2. Applicability of Diverse Amplication on Self-Reflection and Self-ConsistencyWe show whether other self-correction methods-such as Self-Refinement and Self-Consistency-can benefit from structured guidance that enhances reasoning diversity.In DReaMAD, this is operationalized through the Strategic Prior Knowledge Elicitation (SPKE) module, which prompts the model to reinterpret the problem and formulate general strategies before engaging in debate.To isolate SPKE's impact, we evaluate DReaMAD, which includes SPKE but excludes debate (see Table3).We further apply SPKE to Self-Refinement and Self-Consistency and compare them to their vanilla versions.The results show that SPKE alone consistently improves performance across settings.E.3. Experiments of Figure 3 setupFor Figure3, we explain the situation used in this experiment.Figure9illustrates a Nim game scenario with a pile of 5 items remaining.On the left (Current State), we present the basic setting and the task: the player (Agent 1) must decide how many items to take given the rules of the Nim variant.Below it (Strong Consistency), we see a single-agent reasoning process where the agent internally evaluates the outcome of different moves and arrives at a conclusion (taking 2 items, leaving 3 to the opponent).On the right (Multi-Agent Debate), we show a contrasting approach in which two agents (Agent 1 and Agent 2) engage in a debate.Each agent proposes a move and justifies why it would be advantageous.For example, Agent 1 reasons that taking 2 items leaves the opponent with a position that is favorable for Agent 1 (which is wrong reasoning), while Agent 2 counters by proposing to take 1 item for a different strategic benefit (correct reasoning).GPT-4o-miniMethod NIM-N NIM-M Fib-N Fib-M Self-Refinement 0.22 0.70 0.18 0.50 Self-Refinement + DReaMAD (−)  0  (−) to Self-Refinement and Self-Consistency method.Bold numbers mark the best score in each column.DReaMAD (−) indicates ablations where debate is not applied during inference.Figure9.The situation and debate process of the experiments in Figure3
The claude 3 model family: Opus, sonnet, haiku. </p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Man is to computer programmer as woman is to homemaker? debiasing word embeddings. T Bolukbasi, K.-W Chang, J Zou, V Saligrama, A Kalai, Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16. the 30th International Conference on Neural Information Processing Systems, NIPS'16Red Hook, NY, USACurran Associates Inc2016ISBN 9781510838819</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, arXiv:2308.07201Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Unleashing the potential of prompt engineering in large language models: A comprehensive review. B Chen, Z Zhang, N Langrené, S Zhu, arXiv:2310.147352023aarXiv preprint</p>
<p>J C Chen, .-Y Saha, S Bansal, M Reconcile, arXiv:2309.13007Round-table conference improves reasoning via consensus among diverse llms. 2023barXiv preprint</p>
<p>Universal self-consistency for large language model generation. X Chen, R Aksitov, U Alon, J Ren, K Xiao, P Yin, S Prakash, C Sutton, X Wang, D Zhou, arXiv:2311.173112023carXiv preprint</p>
<p>W P Cleaves, Promoting mathematics accessibility through multiple representations jigsaws. Mathematics Teaching in the Middle School. 200813</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>How students learn. N R Council, S Donovan, J Bransford, 2005National Academies PressWashington, DC</p>
<p>A survey on in-context learning. Q Dong, L Li, D Dai, C Zheng, J Ma, R Li, H Xia, J Xu, Z Wu, T Liu, arXiv:2301.002342022arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Portable game notation specification and implementation guide. S J Edwards, April, 4:2011, 1994</p>
<p>Acc-debate: An actor-critic approach to multi-agent debate. A Estornell, J.-F Ton, Y Yao, Y Liu, 2024</p>
<p>. D Ganguli, A Askell, N Schiefer, T I Liao, K Lukošiūtė, A Chen, A Goldie, A Mirhoseini, C Olsson, D Hernandez, D Drain, D Li, E Tran-Johnson, E Perez, J Kernion, J Kerr, J Mueller, J Landau, K Ndousse, K Nguyen, L Lovitt, M Sellitto, N Elhage, N Mercado, N Dassarma, O Rausch, R Lasenby, R Larson, S Ringer, S Kundu, S Kadavath, S Johnston, S Kravec, S E Showk, T Lanham, T Telleen-Lawton, T Henighan, T Hume, Y Bai, Z Hatfield-Dodds, B Mann, D Amodei, N Joseph, S Mccandlish, T Brown, C Olah, J Clark, S R Bowman, 2023and Kaplan, J. The capacity for moral self-correction in large language models</p>
<p>Intrinsic bias metrics do not correlate with application bias. S Goldfarb-Tarrant, R Marchant, R Muñoz Sánchez, M Pandya, A Lopez, 10.18653/v1/2021.acl-long.150Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. C Zong, F Xia, W Li, R Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>. P M Grundy, Mathematics, Eureka, 19392</p>
<p>Y Guo, M Guo, J Su, Z Yang, M Zhu, H Li, M Qiu, S S Liu, Bias in large language models: Origin, evaluation, and mitigation. 2024</p>
<p>The box is in the pen: Evaluating commonsense reasoning in neural machine translation. J He, T Wang, D Xiong, Q Liu, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Large language models cannot self-correct reasoning yet. J Huang, X Chen, S Mishra, H S Zheng, A W Yu, X Song, D Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>. L Jiang, J D Hwang, C Bhagavatula, R L Bras, J Liang, J Dodge, K Sakaguchi, M Forbes, J Borchardt, S Gabriel, Y Tsvetkov, O Etzioni, M Sap, R Rini, Y Choi, 2022Can machines learn morality? the delphi experiment</p>
<p>When can LLMs actually correct their own mistakes? a critical survey of self-correction of LLMs. R Kamoi, Y Zhang, N Zhang, J Han, R Zhang, 10.1162/tacla007132024Transactions of the Association for Computational Linguistics12</p>
<p>Training language models to self-correct via reinforcement learning. A Kumar, V Zhuang, R Agarwal, Y Su, J D Co-Reyes, A Singh, K Baumli, S Iqbal, C Bishop, R Roelofs, L M Zhang, K Mckinney, D Shrivastava, C Paduraru, G Tucker, D Precup, F Behbahani, A Faust, 2024</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. T Liang, Z He, W Jiao, X Wang, Y Wang, R Wang, Y Yang, S Shi, Z Tu, arXiv:2305.191182023arXiv preprint</p>
<p>On the intrinsic self-correction capability of llms: Uncertainty and latent concept. G Liu, H Mao, B Cao, Z Xue, X Zhang, R Wang, J Tang, K Johnson, 2024</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Y Lu, M Bartolo, A Moore, S Riedel, P Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. S Muresan, P Nakov, A Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Self-refine: Iterative refinement with selffeedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Which examples to annotate for in-context learning? towards effective and efficient selection. C Mavromatis, B Srinivasan, Z Shen, J Zhang, H Rangwala, C Faloutsos, G Karypis, arXiv:2310.200462023arXiv preprint</p>
<p>What in-context learning "learns" in-context: Disentangling task recognition and task learning. J Pan, Master's thesis. 2023Princeton University</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. L Reynolds, K Mcdonell, 10.1145/3411763.3451760Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, CHI EA '21. New York, NY, USA2021Association for Computing Machinery. ISBN 9781450380959</p>
<p>. S Schulhoff, M Ilie, N Balepur, K Kahadze, A Liu, C Si, Y Li, A Gupta, H Han, S Schulhoff, P S Dulepet, S Vidyadhara, D Ki, S Agrawal, C Pham, G Kroiz, F Li, H Tao, A Srivastava, H D Costa, S Gupta, M L Rogers, I Goncearenco, G Sarli, I Galynker, D Peskoff, M Carpuat, J White, S Anadkat, A Hoyle, Resnik , 2024aP. The prompt report: A systematic survey of prompting techniques</p>
<p>S Schulhoff, M Ilie, N Balepur, K Kahadze, A Liu, C Si, Y Li, A Gupta, H Han, S Schulhoff, arXiv:2406.06608The prompt report: A systematic survey of prompting techniques. 2024barXiv preprint</p>
<p>Can prompt modifiers control bias? a comparative analysis of text-to-image generative models. P W Shin, J J Ahn, W Yin, J Sampson, V Narayanan, 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, E Berman, A Gopinath, K Narasimhan, S Yao, arXiv:2303.113662023arXiv preprint</p>
<p>Should we be going mad? a look at multi-agent debate strategies for llms. A Smit, N Grinsztajn, P Duckworth, T D Barrett, A Pretorius, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024ICML'24. JMLR.org</p>
<p>Über mathematische kampfspiele. R P Sprague, Tôhoku Mathematical Journal. 411935</p>            </div>
        </div>

    </div>
</body>
</html>
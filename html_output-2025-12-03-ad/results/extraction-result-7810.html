<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7810 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7810</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7810</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-521ccc898395a2818fced22b4cf371b0e5121f94</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/521ccc898395a2818fced22b4cf371b0e5121f94" target="_blank">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model, and results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size.</p>
                <p><strong>Paper Abstract:</strong> The common practice for training commonsense models has gone from–human–to–corpus–to–machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from–machine–to–corpus–to–machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically–as text–in addition to the neural model. We distill only one aspect–the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model’s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7810",
    "paper_id": "paper-521ccc898395a2818fced22b4cf371b0e5121f94",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0053037499999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</h1>
<p>Peter West ${ }^{\dagger \ddagger <em>}$ Chandra Bhagavatula ${ }^{\ddagger}$ Jack Hessel ${ }^{\ddagger}$ Jena D. Hwang ${ }^{\ddagger}$ Liwei Jiang ${ }^{\dagger \ddagger}$ Ronan Le Bras ${ }^{\ddagger}$ Ximing Lu ${ }^{\dagger \ddagger}$ Sean Welleck ${ }^{\dagger \ddagger}$ Yejin Choi ${ }^{\dagger \ddagger </em>}$<br>${ }^{\dagger}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{\ddagger}$ Allen Institute for Artificial Intelligence</p>
<h4>Abstract</h4>
<p>The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-tomachine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the resulting neural model. We distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type of model, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill highquality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Prior works have suggested that pre-trained language models possess limited understanding of commonsense knowledge (Merrill et al., 2021; Talmor et al., 2021; Davis and Marcus, 2017) despite</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Symbolic knowledge distillation extracts the commonsense from the large, general language model GPT-3, into 2 forms: a large commonsense knowledge graph $\mathrm{ATOMIC}^{\mathbf{1 0 x}}$, and a compact commonsense model COMET $_{\text {III }}^{\text {III }}$. The quality of this knowledge can be controlled and improved by adding a critic model, making GPT-3 a stronger teacher.
otherwise stellar performance on leaderboards. As a result, symbolic commonsense knowledge graphs (Speer et al., 2017; Sap et al., 2019; Hwang et al., 2021) and corresponding neural representations (Bosselut et al., 2019; Hwang et al., 2021; Zhang et al., 2020b) have supplemented past models with commonsense capabilities. This has enabled diverse downstream applications, including interactive learning through a conversational interface (Arabshahi et al., 2021), persona- and affect-aware conversation models (Kearns et al., 2020), figurative language understanding (Chakrabarty et al., 2020, 2021), story telling (Ammanabrolu et al., 2021a) and fantasy games (Ammanabrolu et al., 2021b).</p>
<p>The common practice for commonsense knowledge graph construction sees humans spell out as many pieces of knowledge as possible. This pipeline goes from-human-to-corpus-to-machine, with commonsense models trained from human-</p>
<p>authored knowledge graphs. Yet, high-quality, human-authored knowledge is expensive to scale, limiting coverage; this motivates an alternative: from-machine-to-corpus-to-machine. Prior efforts toward automatic commonsense knowledge graphs have resulted in considerably lower quality than human-written data (Hwang et al., 2021; Zhang et al., 2020b), which in turn leads to less reliable neural models (Hwang et al., 2021). Broad literature consistently shows machine-authored knowledge graphs underperform human-authored graphs (Etzioni et al., 2011; Mitchell et al., 2015; Bollacker et al., 2008).</p>
<p>In this work, we propose Symbolic knowledge distillation, a new conceptual framework towards high-quality automatic knowledge graphs for commonsense, leveraging state-of-the-art models and novel methodology. Most prior art for automatic knowledge graph construction extracts knowledge from raw text (Bhakthavatsalam et al., 2020; Zhang et al., 2020a; Zhou et al., 2020; Zhang et al., 2020b; Li et al., 2020). In contrast, our approach is motivated by knowledge distillation (Hinton et al., 2015) wherein a larger teacher model transfers knowledge to a compact student model (§2.1). Our method differs from prior knowledge distillation in key ways: we distill a symbolic knowledge graph (i.e., generated text) in addition to a neural model, and we distill only a selective aspect of the teacher model. This selectively allows the student model to be of a different type (commonsense model), compared to the teacher (general language model), enriching the scope of distillation. An added benefit is that knowledge distilled as text is human readable: it can be understood and evaluated.</p>
<p>A general language model-GPT-3 in our case-is an imperfect commonsense teacher on its own, and the ability to evaluate distilled knowledge is useful in improving it. We empirically demonstrate that, by training a separate critic model to judge symbolic generation quality, a more precise teacher can be defined. Knowledge from this critical teacher is higher quality-even exceeding human-authored knowledge. Yet even before training a critic, our study makes the unexpected finding that the student model surpasses the commonsense of GPT-3, our knowledge source.</p>
<p>To test symbolic knowledge distillation against the human-to-corpus-to-machine paradigm, we compare with $\operatorname{Atomic}<em 10="10" _mathrm_L="\mathrm{~L">{20}^{20}$ (Hwang et al., 2021), which is a human-authored commonsense knowl-
edge graph. We find that $\operatorname{AtOMIC}^{\mathbf{1 0 x}}$, our machinegenerated corpus, exceeds the human generated corpus in scale, accuracy, and diversity with respect to 7 commonsense inference types that we focus on in this study. The resulting commonsense model, $\operatorname{Comet}</em>$, but is also smaller, more efficient, and produces commonsense at a higher accuracy than its own teacher-GPT-3.}}^{\text {DIS }}$, not only surpasses the humantrained equivalent $\operatorname{Comet}_{20}^{20</p>
<p>Symbolic knowledge distillation offers a promising new role for general language models, as commonsense knowledge sources, and humans, as small-scale evaluators to train critic models rather than authors of commonsense knowledge. Our work demonstrates that humans and LMs can be effective collaborators for curating commonsense knowledge graphs and training efficient and performant commonsense models.</p>
<h2>2 Overview and Key Findings</h2>
<p>Throughout our work, we describe the machine-to-corpus-to-machine methodology of symbolic knowledge distillation. We first go machine-tocorpus (§3), by decoding from GPT-3, then improve our knowledge with a specialized critic model (§4), and finally distill this knowledge into an efficient commonsense model (§5), going corpus-to-machine. Throughout this process, we evaluate against a human knowledge source, comparing our automatic knowledge graph ATOMIC ${ }^{\mathbf{1 0 x}}$ and commonsense model $\operatorname{Comet}<em 20="20">{10}^{\text {DIS }}$ to the humanauthored ATOMIC ${ }</em>$ (Hwang et al., 2021).}^{20}$ and resulting model $\operatorname{Comet}_{20}^{20</p>
<h3>2.1 Symbolic Knowledge Distillation</h3>
<p>Our proposed methodology parallels knowledge distillation (Hinton et al., 2015), a method for compressing a large or complicated teacher distribution $P_{t}$ into a smaller/simpler student distribution $P_{s}$. Key to knowledge distillation ${ }^{2}$ is the notion of minimizing the cross-entropy between $P_{t}$ and $P_{s}$ :</p>
<p>$$
H\left(P_{t}, P_{s}\right)=-\sum_{y \in Y} P_{t}(y) \log P_{s}(y)
$$</p>
<p>Knowledge is transferred to the student by encouraging it to match teacher predictions. Hinton et al. (2015) apply this to conditional classification: for</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">X starts running</th>
<th style="text-align: center;">xEffect <br> so, $x$</th>
<th style="text-align: center;">gets in shape</th>
<th style="text-align: center;">X sings a song</th>
<th style="text-align: center;">HinderedBy <br> but not if</th>
<th style="text-align: center;">X can't remember <br> the lyrics</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">X and Y engage in <br> an argument</td>
<td style="text-align: center;">xWant <br> so, $X$ wants</td>
<td style="text-align: center;">to avoid Y</td>
<td style="text-align: center;">X is not well <br> liked</td>
<td style="text-align: center;">xReact <br> so, $X$ feels</td>
<td style="text-align: center;">lonely</td>
</tr>
<tr>
<td style="text-align: center;">X learns to type <br> fast</td>
<td style="text-align: center;">xNeed <br> $X$ needed</td>
<td style="text-align: center;">to have taken <br> typing lessons</td>
<td style="text-align: center;">X takes care of <br> a monkey</td>
<td style="text-align: center;">xAttr <br> $X$ is seen as</td>
<td style="text-align: center;">kind</td>
</tr>
<tr>
<td style="text-align: center;">X steals his <br> grandfather's sword</td>
<td style="text-align: center;">xEffect <br> so, $X$</td>
<td style="text-align: center;">is punished by <br> his grandfather</td>
<td style="text-align: center;">X butts in</td>
<td style="text-align: center;">HinderedBy <br> but not if</td>
<td style="text-align: center;">X is too shy to <br> speak up</td>
</tr>
<tr>
<td style="text-align: center;">X takes up new <br> employment</td>
<td style="text-align: center;">xIntent <br> because $X$ wants</td>
<td style="text-align: center;">to be self <br> sufficient</td>
<td style="text-align: center;">X waits for the <br> storm to break</td>
<td style="text-align: center;">xEffect <br> so, $X$</td>
<td style="text-align: center;">is safe from the <br> storm</td>
</tr>
</tbody>
</table>
<p>Figure 2: Example automatically generated ATOMIC triples from our ATOMIC ${ }^{10 x}$ commonsense knowledge graph. Each example includes a generated event, relation (with natural language interpretation), and generated inference.
each training input, $P_{t}$ and $P_{s}$ are model predictions over label set $Y$. Typically $Y$ is a tractable set, over which this sum can reasonably be calculated.</p>
<p>For distilling the knowledge of generative models, we can think of an unconditional language model (LM e.g. GPT-3) as $P_{t}$. This makes $Y$ the set of all strings, over which LMs define probability. Unfortunately $Y$ is an exponential set, intractable to sum over in Eq 1. Kim and Rush (2016) address this problem by simply taking the mode of $P_{t}$ over $Y$, truncating most of the teacher distribution to the most likely sequence and discarding information.</p>
<p>Instead, we consider a sampling-based interpretation of the same objective:</p>
<p>$$
H\left(P_{t}, P_{s}\right)=\underset{y \sim P_{t}(y)}{\mathbb{E}}\left[-\log P_{s}(y)\right]
$$</p>
<p>which exactly equals the cross-entropy of Eq 1, at the limit under pure sampling from $P_{t} .^{3}$</p>
<p>Yet distilling all knowledge from the teacher may not be desirable-our work is specifically focused on distlling commonsense knowledge from GPT3. The ideal teacher $P_{t}$ is a commonsense expert, but GPT-3 can approximate such a teacher, off-theshelf, via prompting. This ability to select information is one explicit benefit of the sampling-based interpretation of Eq 2: while Eq 1 uses continuous logits over existing data, sampling gives discrete control over transferred information, by selecting which samples are elicited and used. For the general language model GPT-3, We encourage domain/quality with prompting, and sample truncation (Holtzman et al., 2020). We call this the loose teacher $P_{t}^{L}$-knowledge is generated and transferred from GPT-3, but without critical assessment of correctness (§3).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In fact, sampling knowledge in Eq 2 offers even more control, as generations can be individually interpreted and judged. Given an indicator function $A(x)$ for which knowledge $x$ is correct, we can define a stronger teacher model. Using a Product of Experts (Hinton, 2002) between the loose teacher $P_{t}^{L}$ and and the critic $A(x)$, we define a critical teacher:</p>
<p>$$
P_{t}(x) \propto P_{t}^{L}(x \mid p) \cdot A(x)
$$</p>
<p>In practice, $A(x)$ is a textual classifier learned on human judgements, 1 for knowledge predicted to be correct and 0 otherwise. Thus, the critic gives control over the correctness and confidence of the knowledge that is transferred (§4).</p>
<h3>2.2 Key Findings</h3>
<p>Applying symbolic knowledge distillation in practice results in promising and surprising findings:</p>
<ol>
<li>Learning symbolic knowledge from language models can be framed as a symbolic extension to knowledge distillation. In $\S 2.1$, we describe learning commonsense as a symbolic extension to knowledge distillation, with GPT-3 a knowledge source. We elaborate on this process with positive results in $\S 3,4$, and 5 .</li>
<li>Symbolic knowledge distillation constructs a high quality knowledge graph at scale. Our method naturally yields a machine-generated commonsense knowledge graph, which can achieve impressive quality (§4), beyond that of humanauthored data. An effective critic which filters incorrect generated knowledge is key.</li>
<li>A critical teacher results in a higher quality student. In $\S 4$, we show that making the teacher more critical results in higher quality knowledge, even as it reduces the scale of knowledge transferred. This demonstrates that quality matters, not</li>
</ol>
<p>just quantity, as higher quality knowledge results in a higher quality commonsense model in $\S 5$ despite smaller scale data.
4. Critical teacher or not, a student can outperform the knowledge source. In $\S 5$, we show the unexpected result that all student models exceed the quality of GPT-3, the knowledge source.
5. Machines can win over humans for automatic knowledge graph construction. In $\S 4$ and $\S 5$, we show that machine generated knowledge and the resulting commonsense model can outperform their equivalents that use a human knowledge source. Our symbolic knowledge exceeds humans at scale, quality, and diversity. The resulting commonsense model achieves the most accurate commonsense KG completions.</p>
<h2>3 Machine-to-Corpus Verbalization</h2>
<p>Symbolic knowledge distillation begins by going machine-to-corpus, i.e. generating many commonsense facts, which results in a commonsense knowledge graph. $\S 2.1$ frames this as sampling to estimate the knowledge distillation objective-a student commonsense model learns from the generations of a teacher (GPT-3).</p>
<p>We start with a loose teacher, transferring knowledge by prompted generation with truncated sampling alone-this is in contrast to the critical teacher (§4) which explicitly judges and filters the generated samples. The loose teacher uses few-shot prompting as in Brown et al. (2020). We use a few-shot template:</p>
<div class="codehilite"><pre><span></span><code>&lt;TASK-PROMPT&gt;
&lt;EX \(_{1}-\) INP&gt;&lt;EX \(_{1}-\) OUT \(&gt;\)
. . 
&lt;EX \(_{N-1}-\) INP&gt;&lt;EX \(_{N-1}-\) OUT \(&gt;\)
&lt;EX \(_{N}-\) INP \(&gt;\)
</code></pre></div>

<p>where &lt;EX $<em i="i">{i}$-INP $/&lt;\mathrm{EX}</em>-1$ ). We find important aspects for producing high-quality commonsense knowledge:}$-OUT $&gt;$ are humanauthored, natural language Atomic entries, and <TASK-PROMPT> is a description of the problem. Given such a prompt, GPT-3 generates the missing piece, output <EX $_{N}$-OUT> for input $<E X_{N}-$ INP $>$, following the pattern of earlier examples ( 1 to $\mathrm{N</p>
<ul>
<li>Examples should be numbered. e.g. $<E X_{5}-$ INP $>$ might begin with "5)" to indicate it is the 5 th example.</li>
<li>The format of <EX $_{i}$-INP> and <EX $_{i}$-OUT> should linguistically imply the relationship between them. See below for examples.</li>
<li><TASK-PROMPT> can be used to give extra specification to complicated problems.</li>
</ul>
<h3>3.1 Data: ATOMIC</h3>
<p>We demonstrate symbolic knowledge distillation on the Atomic if-then resource (Sap et al., 2019). This follows an event-relation-inference (triple) format. The corpus links events (e.g. $X$ attacks $Y$ ) to relations, e.g. HinderedBy which describes what might hinder an event. For a relation/event, the goal is to generate a resulting inference, e.g. $X$ attacks $Y$ HinderedBy $X$ is restrained.</p>
<p>Of the 23 relations from the most recent versionATOMIC $_{20}^{20}$-we limit our investigation to 7 relations that correspond to causal commonsense knowledge: xAttr (how X is perceived after event), xReact (how X reacts in response to event), xEffect (what X does after event), xIntent (X's intent in event), xWant (what X wants after event), xNeed (what X needed for event to take place) and HinderedBy. We describe how verbalization is applied to Atomic data in 2 steps: generating underlying events (heads), then full examples (inference given event).</p>
<h3>3.2 Event Generation</h3>
<p>Events are context-free premises in Atomic involving PersonX (and sometimes a second PersonY) in various scenarios. These events form heads in knowledge graph triples. We generate events by filling in the elements of our template:</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">Event</span><span class="p">:</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="n">overcomes</span><span class="w"> </span><span class="n">evil</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="kr">go</span><span class="n">od</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">Event</span><span class="p">:</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">learn</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">Y</span>
<span class="mf">...</span>
<span class="mf">10.</span><span class="w"> </span><span class="n">Event</span><span class="p">:</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="n">looks</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">flowers</span>
</code></pre></div>

<ol>
<li></li>
</ol>
<p>The format is simple, as events are generated unconditionally. We use 100 high-quality events from the $\mathrm{Atomic}_{20}^{20}$ corpus for our prompt, selected to avoid grammatical or logical errors, and minimize semantic overlap. We randomly sample 10 of these seed events for each generation batch, resulting in randomized prompts. We use nucleus sampling $(p=0.9)$ (Holtzman et al., 2020), and presence/frequency penalties of 0.5 from the GPT3 interface. We generate 165 K unique events using the 175B-parameter Davinci model ${ }^{4}$ from Brown</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>et al. (2020) (human-authored ATOMIC ${ }_{20}^{20}$ contains only 6.2 K events).</p>
<h3>3.3 Inference Generation</h3>
<p>Generating ATOMIC inferences requires reasoning about events and relations together. We design verbalization templates fo reach relation, with iterative design and small-scale verification by the authors ${ }^{5}$ e.g. we prompt the $\mathbf{x N e e d}$ relation as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">What</span><span class="w"> </span><span class="nv">needs</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">true</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">this</span>
<span class="nv">event</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">take</span><span class="w"> </span><span class="nv">place</span>?
...
<span class="nv">Event</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">i</span><span class="o">&gt;</span>:<span class="w"> </span><span class="nv">X</span><span class="w"> </span><span class="nv">goes</span><span class="w"> </span><span class="nv">jogging</span>
<span class="nv">Prerequisites</span>:<span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">to</span>
<span class="nv">happen</span>,<span class="w"> </span><span class="nv">X</span><span class="w"> </span><span class="nv">needed</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">wear</span><span class="w"> </span><span class="nv">running</span>
<span class="nv">shoes</span>
...
<span class="nv">Event</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">N</span><span class="o">&gt;</span>:<span class="w"> </span><span class="nv">X</span><span class="w"> </span><span class="nv">looks</span><span class="w"> </span><span class="nv">at</span><span class="w"> </span><span class="nv">flowers</span>
<span class="nv">Prerequisites</span>:<span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">to</span>
<span class="nv">happen</span>,
</code></pre></div>

<p>The language of this template implies the relationspecific task, both "Prerequisites:" and beginning with "for this to happen" suggest the $\mathbf{x N e e d}$ relation. As well, we include an xNeed-specific <TASK-PROMPT>. We use 10 few-shot examples for each prompt. ${ }^{6}$</p>
<p>For each event/relation ( $165 \mathrm{~K} \mathrm{X} 7$ ) we generate 10 inferences with the Curie GPT-3 model ${ }^{7}$ and earlier hyperparameters. Removing duplicate and degenerate (e.g. fewer than 3 characters) generations yields 6.46M ATOMIC-style data triples (examples in Figure 2). We call this ATOMIC ${ }^{\mathbf{1 0 x}}$, as it contains an order of magnitude more triples than ATOMIC ${ }_{20}^{20}$ for the 7 relations we study.</p>
<h3>3.4 Evaluating a Generated Commonsense Knowledge Graph</h3>
<p>Machine generation enables a large scale of unique generations at a much lower cost than humanauthored knowledge (Table 1), but what kind of examples are produced by GPT-3, and how does it differ from knowledge produced by humans? In this section, we conduct an in-depth analysis to answer these questions.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Number of unique triples with the given relation, $[(\cdot$, relation,$\cdot)$. The estimated cost for ATOMIC ${ }^{\mathbf{1 0 x}}$ comes at a fraction of a conservative estimation for ATOMIC ${ }_{20}^{20}$ crowdsourcing costs.</p>
<h2>Lexical Differences: Diversity and Uniqueness</h2>
<p>Recent work finds that machine generations can be repetitive and lack diversity (Welleck et al., 2020; Holtzman et al., 2020); one way generated knowledge may differ from human-authored is less creative word choice, diversity, or more repetition.</p>
<p>To test this, we begin with lexical diversity (i.e. unique words used, Table 2). While there is variation by relation, the diveristy of ATOMIC ${ }^{\mathbf{1 0 x}}$ actually exceeds ATOMIC ${ }_{20}^{20}$ here, 5.2 M unique words to 1.5 M . In addition, it contains significantly more strictly unique generated inferences (Table 2, unique tails).</p>
<p>BLEU Soft Uniqueness. Exact match (above) fails to capture the notion of similar text. Following the intuition of self-BLEU (Zhu et al., 2018), we define soft uniqueness to describe diversity of generations in a corpus. An inference $x$ is softlyunique if:</p>
<p>$$
B L E U_{2}(C, x)&lt;0.5
$$</p>
<p>where $C$ is the set of inferences for a given input (in our case, event + relation), and 0.5 is an empirical threshold. To find soft-uniqueness of a corpus, we iteratively remove examples until all are softly unique, i.e. low mutual lexical overlap; higher diversity means more such examples (thus a larger softly unique corpus is preferable). Softly-unique corpus sizes are given in Table 4 ("Size (div)"). ATOMIC ${ }^{\mathbf{1 0 x}}$ has a smaller fraction of softly-unique examples than ATOMIC ${ }<em 20="20">{20}^{20}$, yet it contains many more such examples. ATOMIC ${ }^{\mathbf{1 0 x}}$ contains 4.38 M such examples (full size 6.5 M ) vs. ATOMIC ${ }</em>$, which has 560 K (full size 600 K ).}^{20</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: right;">Unique</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Unique</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Length</td>
<td style="text-align: right;">Tokens (K)</td>
<td style="text-align: right;">Tails (K)</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{A}_{20}^{20}$</td>
<td style="text-align: right;">$\mathbf{A}^{\mathbf{1 0 x}}$</td>
<td style="text-align: right;">$\mathbf{A}_{20}^{20}$</td>
<td style="text-align: right;">$\mathbf{A}^{\mathbf{1 0 x}}$</td>
</tr>
<tr>
<td style="text-align: left;">xWant</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: right;">5.16</td>
<td style="text-align: right;">322</td>
<td style="text-align: right;">784</td>
</tr>
<tr>
<td style="text-align: left;">xAttr</td>
<td style="text-align: center;">1.42</td>
<td style="text-align: right;">2.73</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">21</td>
</tr>
<tr>
<td style="text-align: left;">xEffect</td>
<td style="text-align: center;">3.92</td>
<td style="text-align: right;">4.66</td>
<td style="text-align: right;">216</td>
<td style="text-align: right;">864</td>
</tr>
<tr>
<td style="text-align: left;">xIntent</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: right;">5.92</td>
<td style="text-align: right;">136</td>
<td style="text-align: right;">800</td>
</tr>
<tr>
<td style="text-align: left;">xNeed</td>
<td style="text-align: center;">4.51</td>
<td style="text-align: right;">5.97</td>
<td style="text-align: right;">289</td>
<td style="text-align: right;">1378</td>
</tr>
<tr>
<td style="text-align: left;">xReact</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: right;">1.77</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;">HinderedBy</td>
<td style="text-align: center;">7.93</td>
<td style="text-align: right;">7.49</td>
<td style="text-align: right;">522</td>
<td style="text-align: right;">1775</td>
</tr>
<tr>
<td style="text-align: left;">Events</td>
<td style="text-align: center;">5.20</td>
<td style="text-align: right;">$\mathbf{5 . 3 2}$</td>
<td style="text-align: right;">109</td>
<td style="text-align: right;">$\mathbf{8 8 1}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Average length, total unique tokens and total unique examples (in K, i.e. 1000s) by relation type and in events (bottom row) from $\operatorname{Atomic}<em 20="20">{20}^{20}\left(\mathrm{~A}</em>\right)$.}^{20}\right)$ and $\operatorname{Atomic}^{\mathbf{1 0 x}}\left(\mathrm{A}^{\mathbf{1 0 x}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entropy</th>
<th style="text-align: center;">Cross Entropy</th>
<th style="text-align: center;">KL Divergence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$H\left(D_{1}\right)=1.27$</td>
<td style="text-align: center;">$H\left(D_{1}, D_{2}\right)=9.31$</td>
<td style="text-align: center;">$D_{K L}(D 1 | D 2)=8.04$</td>
</tr>
<tr>
<td style="text-align: center;">$H\left(D_{2}\right)=7.80$</td>
<td style="text-align: center;">$H\left(D_{2}, D_{1}\right)=41.48$</td>
<td style="text-align: center;">$D_{K L}\left(D_{2} | D_{1}\right)=33.68$</td>
</tr>
</tbody>
</table>
<p>Table 3: Entropy, cross-entropy, and divergence of $\operatorname{Atomic}<em 1="1">{20}^{20}\left(D</em>\right)$.}\right)$ and $\operatorname{Atomic}^{\mathbf{1 0 x}}\left(D_{2</p>
<p>Model-based Diversity Measurement. Lexical notions of diversity reward differences in surface form, which may not always reflect diversity of information, only format. Thus, we next study information-theoretic measures for diversity. Intuitively, diverse information should be less predictable, or higher entropy. With GPT-2 XL models finetuned on $\operatorname{Atomic}<em 20="20">{20}^{20}$ and $\operatorname{Atomic}^{\mathbf{1 0 x}}$ (§5) we estimate entropy-roughly, how difficult it is for a model to capture the corpus information (Table 3). This is 4 times higher for $\operatorname{Atomic}^{\mathbf{1 0 x}}$, suggesting more content from a modeling perspective. We also estimate cross-entropy-how well a model trained on one corpus describes the other. From $\operatorname{Atomic}^{\mathbf{1 0 x}}$ to $\operatorname{Atomic}</em>}^{20}$, this is 9.31 , only 2 points higher than its entropy suggesting $\operatorname{Atomic<em 20="20">{20}^{20}$ is describable with information from $\operatorname{Atomic}^{\mathbf{1 0 x}}$. In reverse, this is 41.48 suggesting much of $\operatorname{Atomic}^{\mathbf{1 0 x}}$ is not captured by $\operatorname{Atomic}</em>$.}^{20}$ $\operatorname{AtOMIC}^{\mathbf{1 0 x}}$ is surprising given only information from $\operatorname{AtOMIC}_{20}^{20</p>
<p>Human Evaluation of Quality. Perhaps most importantly, we study the quality of knowledge in each corpus. We conduct human evaluation with Amazon Mechanical Turk. 3 annotators rate each triple resulting in "accepted", "rejected" or "no judgement". We evaluate 3000 examples ${ }^{8}$ from</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Attributes of $\operatorname{AtOMIC}^{\mathbf{1 0 x}}$ and $\operatorname{AtOMIC}^{\mathbf{1 0 x}}$ (row 2) including the critic model ( $\S 4$, rows $3-6$ ) with various filtering cutoffs. Accept and Reject are by majority human vote unless any mark N/A. Size is in unique examples ${ }^{9}$. The highest precision corpus is $\operatorname{AtOMIC}^{\mathbf{1 0 x}}$ with (critic ${ }<em 20="20">{\text {high }}$ ), but multiple versions surpass $\operatorname{AtOMIC}</em>$. We also include alternate models (GPTJ and T5-11B) as the loose teacher.}^{20</p>
<p>Atomic ${ }^{\mathbf{1 0 x}}$, and 1000 from $\operatorname{Atomic}_{20}^{20}$ (Table 4). We find Fleiss' kappa (Fleiss, 1971) of 40.8 indicating moderate agreement (Landis and Koch, 1977), and $90.5 \%$ accuracy agreement. We require workers meet an Amazon Mechanical Turk qualification for annotation quality based on past commonsense evaluations. We compensate workers $\$ 0.17$ per task, which we estimate require 30 seconds. Further details and task template are in appendix §A.</p>
<p>For the loose teacher, consider the top row of $\operatorname{AtOMIC}^{\mathbf{1 0 x}}$ in Table 4 (other rows add the critic §4). $\operatorname{AtOMIC}^{\mathbf{1 0 x}}$ exceeds $\operatorname{AtOMIC}_{20}^{20}$ in scale, but is somewhat less acceptable by human raters-by roughly 8 percentage points. Yet, the larger scale of $\operatorname{AtOMIC}^{\mathbf{1 0 x}}$ implies a significantly higher number of accurate examples. Increasing the proportion of these is the main objective of the critic (§4).</p>
<p>How do Knowledge Sources Compare? To understand the robustness of our approach, we assess other language models as the knowledge source (i.e. loose teacher): GPT-J (Wang and Komatsuzaki, 2021) and T5-11B adapted for language modelling (Lester et al., 2021). We substitute both for GPT-3 as in §3.2,3.3, generating a small-scale corpus to evaluate. We conduct human evaluation on 1000 examples as above (Table 4). Both models attain roughly $72 \%$ accuracy, 6 points below GPT-3 (78.5). This suggests strong potential, but higher quality from GPT-3. We explore this further in Appendix B.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>4 Making the Teacher More Critical</h2>
<p>Symbolic knowledge distillation requires a strong teacher model to maximize the quality of the generated knowledge graph and resulting student model (§5). While the loose teacher (GPT-3 alone) results in a viable commonsense knowledge graph, evaluation shows this isn't a perfect commonsense teacher. Thus, we multiply in a critic model, to filter lower-quality knowledge, correcting the teacher (§2.1). With modest supervision (a small-scale human evaluation) we train a classifier to predict and discriminate unacceptable examples. We multiply this with the loose teacher $\S 3$, creating a critical teacher product of experts. In practice this means filtering ATOMIC ${ }^{\mathbf{1 0 x}}$ to create new corpora that are higher quality, yet still larger scale than humanauthored ATOMIC ${ }_{20}^{20}$.</p>
<p>Training a knowledge critic We gather a training set of correct vs. incorrect human judgments on a randomly-sampled set of 10 K entries of ATOMIC ${ }^{\mathbf{1 0 x}}$, as in $\S 3.4$ but with one annotation per example. We take a (random) train/dev/test split of $8 \mathrm{k} / 1 \mathrm{k} / 1 \mathrm{k}$. While this step requires human annotation, humans take on the role of high-level supervisors here-critiquing a small number of generations rather than authoring the entire knowledge graph as in previous work. Indeed, the cost/complexity of this step is similar to a typical human evaluation, making it far cheaper/easier than eliciting humanauthored knowledge in past work.</p>
<p>We train binary classifiers (critics) for human acceptability using RoBERTa-Large (Liu et al., 2019). We find pretraining on MNLI results in the best model in terms of precision and recall, and we suggest this technique for future studies. We give more detail in Appendix C, including baselines. Our best model vastly improves the accuracy of ATOMIC ${ }^{\mathbf{1 0 x}}$ (Table 4), demonstrating that a small amount of human supervision can consistently help to correct GPT-3's mistakes.</p>
<p>Size-accuracy trade-off Using our critic to filter knowledge results in a natural trade-off between size and accuracy. We test several cutoffs for ATOMIC ${ }^{\mathbf{1 0 x}}$, i.e. confidence at which the critic rejects examples. We report humanmeasured accuracy (Accept/Reject column Table 4) following §3.4. We compare the loose teacher (unfiltered) to critical teachers. Discarding $20 \%$ of instances that the critic judges as least acceptable (reducing corpus size from 6.5 M to</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Random</th>
<th style="text-align: center;">Inf</th>
<th style="text-align: center;">Event</th>
<th style="text-align: center;">EMAP</th>
<th style="text-align: center;">Full</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">$\mathbf{9 4 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Average Precision for ablated critic models. The critic not only filters awkward phrasings which can be identified by either the event (Event) or inference (Inf) in isolation (EMAP only identifies these), but also logical misalignments, which require modeling interactions between event/inference, i.e. the full critic (Full).
5.1M), ATOMIC ${ }^{\mathbf{1 0 x}}$ 's accuracy rises $78.5 \rightarrow 88.4$; human-authored ATOMIC ${ }<em 20="20">{20}^{20}$ contains 600 K entries at $86.8 \%$ accuracy. Reducing to total size to 2.5 M examples ( $38 \%$ of full size), we attain $96.4 \%$ accuracy, nearly 10 points above ATOMIC ${ }</em>$ while still 4 X larger.}^{20</p>
<p>What gets filtered out? We qualitatively identify two types of filtered triples: 1) logical misalignments, events/inferences joined in an inconsistent manner. Recognizing these requires understanding events-inference interactions, e.g., $X$ cannot find his shirt as a result $X$ is wearing a shirt; 2) awkward phrasings, in which events/inferences are individually incoherent e.g. PersonX has a fire in the bath-resulting triples are invalid as the event is implausible.</p>
<p>To understand what is filtered, we ablate the critic (Table 5): our full model is compared to a random predictor, event-only model, and inferenceonly model. We also compare to an EMAP (Hessel and Lee, 2020) version, i.e. an ensemble of event and inference-only, without interactions between event/inference (needed for logical misalignments).</p>
<p>We find GPT-3 produces both independent awkwardly-phrased events/inferences (filtered by X-only models) and logical misalignments. The classifier, trained on validated knowledge triples, helps in both cases. The EMAP of our full model (identifies only awkward phrasings) achieves $87 \%$ AP, and our full model (which additionally identifies logical misalignments) improves to $94 \%$ AP.</p>
<p>Does filtering hurt diversity? One concern is that the critic may keep only similar "safe" examples, lacking novelty. We repeat our diversity analysis (§3.4) for critical corpora (Table 4, "Size (div)", higher=better). As we filter, we surprisingly observe proportionally more diverse examples: full ATOMIC ${ }^{\mathbf{1 0 x}}$ has a diverse subset $68 \%$ of its size; rising to $80 \%$ with the most extreme filtering. One possibility is that GPT-3 gravitates towards com-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">CKG Completion</th>
<th style="text-align: center;">Train Corpus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Accept</td>
<td style="text-align: center;">Reject</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">GPT2-XL zero-shot</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">4.6</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: left;">COMET $_{20}^{20}$</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: left;">COMET $_{\text {TIL }}^{\text {DIS }}$</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: left;">+ critic $_{\text {low }}$</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: left;">+critic $_{\text {high }}$</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">2.3</td>
</tr>
</tbody>
</table>
<p>Table 6: Model performance on knowledge base completion, measured by human judgement. Inferences are generated on held-out events from $\mathrm{Atomic}<em _TIL="{TIL" _text="\text">{20}^{20}$. Models besides GPT-3 use GPT-2 XL architecture. COMET ${ }</em>$ ) achieves the highest acceptance rate overall- 87.5 .
mon sentence structures for inconsistent knowledge. These would be recognizable to the critic, and removing them would increase both quality and diversity. This surprising result warrants further study.}}^{\text {DIS }}$ with a strong critic (+critic $_{\text {high }</p>
<h2>5 Corpus-to-Machine: Distillation</h2>
<p>The final step of symbolic knowledge distillation trains a compact model on the generated natural language knowledge graph. Our base model is GPT2-XL trained on all of $\mathrm{Atomic}^{10 \mathrm{x}}$ : we denote this model by $\operatorname{COMET}<em _low="{low" _text="\text">{\text {TIL }}^{\text {DIS }}$. We additionally train the model on critical versions of $\mathrm{Atomic}^{10 \mathrm{x}}$-crit $</em>$ on the $96.4 \%$ accuracy corpus. Models are trained for 1 epoch, with default parameters using the Huggingface Transformers library (Wolf et al., 2019).}}$ denotes training on the corpus achieving $91.5 \%$ accuracy, and crit $_{\text {high }</p>
<h3>5.1 Evaluating a Symbolically Distilled Model</h3>
<p>Evaluation follows past work (Hwang et al., 2021; Bosselut et al., 2019; Sap et al., 2019) testing the ability of models to do knowledge base completion, i.e. generating inferences for test events, specifically from the $\mathrm{Atomic}<em 20="20">{20}^{20}$ test set. We use human evaluation ${ }^{10}$ following Section 3.4, on 1000 inputs (event + relation), with results in Table 6. We compare to the GPT2-XL-based COMET ${ }</em>}^{20}$ model trained on human-generated $\mathrm{Atomic<em _TIL="{TIL" _text="\text">{20}^{20}$, and GPT3 using the same generation method as $\S 3$-in effect, comparing the student $\operatorname{COMET}</em>$ to the loose teacher GPT-3. We omit the critical teacher (GPT$3+$ critic), which is not assured to produce an in-}}^{\text {DIS }</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ference for each input, as the critic may reject all tails for some inputs. We also compare to zero-shot GPT2-XL (Radford et al., 2019) using the same methodology (Table 6).</p>
<p>How does COMET ${ }<em _TIL="{TIL" _text="\text">{\text {TIL }}^{\text {DIS }}$ compare to GPT-3? In knowledge distillation, the student model often deteriorates in performance (Hinton et al., 2015; Kim and Rush, 2016) compared to its teacher. Comparing our base teacher-GPT-3-to the simplest version of $\operatorname{COMET}</em>}}^{\text {DIS }}$ (top-row $\operatorname{COMET<em _TIL="{TIL" _text="\text">{\text {TIL }}^{\text {DIS }}$ of Table 6) surprisingly shows the student surpasses GPT-3, the model that generates its training data ${ }^{11}$. We posit that the superior performance of $\operatorname{COMET}</em>$ on one commonsense domain while GPT-3 covers a more general domain. We leave further study of this effect for future work.}}^{\text {DIS }}$ may have to do with mistakes of GPT-3 being filtered by verbalization and training of GPT-2, and possibly the focus of $\operatorname{COMET}_{\text {TIL }}^{\text {DIS }</p>
<p>How does COMET ${ }<em _TIL="{TIL" _text="\text">{\text {TIL }}^{\text {DIS }}$ compare to human knowledge? While COMET ${ }</em>}}^{\text {DIS }}$ without the critic is slightly outperformed by $\operatorname{COMET<em _TIL="{TIL" _text="\text">{20}^{20}$ in terms of accuracy, this reverses with the critic. For both cutoffs tested, $\operatorname{COMET}</em>$, with more filtering resulting in a wider gap.}}^{\text {DIS }}$ surpasses $\operatorname{COMET}_{20}^{20</p>
<p>Usefulness of COMET ${ }<em _TIL="{TIL" _text="\text">{\text {TIL }}^{\text {DIS }}$ For on-demand inference, where a single high quality inference for some input event/relation is required, $\operatorname{COMET}</em>$ by 5 points and GPT-3 by over 10. The critical teacher (GPT-3 + critic) yields a more accurate corpus, but may filter all inferences for an input, giving no output.}}^{\text {DIS }}$ is the best available model: the most performant version surpasses $\operatorname{COMET}_{20}^{20</p>
<p>Limits and Future Work The success of symbolic knowledge distillation is a first stepdemonstrating superior performance to human authoring on the commonsense relations tested here. No aspect of our approach is specific to these relations, yet further work is needed to explore the feasibility of generation for other aspects of commonsense and knowledge, beyond these relations, to concepts like physical or temporal commonsense.</p>
<h2>6 Related Work</h2>
<p>Commonsense Knowledge Graphs (CKG) CKGs provide knowledge for commonsense reasoning. Some are manually constructed, e.g.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Atomic (Sap et al., 2019; Hwang et al., 2021). ConceptNet (Speer et al., 2017) contains taxonomy and physical commonsense, authored by humans or compiled from such sources. Some CKGs are automatically constructed: TransOMCS (Zhang et al., 2020a) extracts 18.48M tuples from syntactic parses and CausalBank (Li et al., 2020) extracts 314M cause-effect pairs by pattern-matching. In contrast, we generate commonsense.</p>
<p>Extracting Knowledge from LMs Past work uses models for automatic knowledge graph completion (Bosselut et al., 2019; Hwang et al., 2021; Li et al., 2020). Yet, models are trained on existing resources; ATOMIC ${ }^{\mathbf{1 0 s}}$ is generated without these. Other works mine factual/commonsense knowledge directly from off-the-shelf LMs (Petroni et al., 2019; Davison et al., 2019; Xiong et al., 2020), but not resulting in the quality at scale of ATOMIC ${ }^{\mathbf{1 0 s}}$.</p>
<p>Knowledge Distillation Other works use knowledge distillation (Hinton et al., 2015) for generation. (Sanh et al., 2019) follow a label smoothing formulation, while Kim and Rush (2016) follow a similar formulation to us ( $\$ 2.1$ ), use the mode of the teacher distribution rather than sampling. Our work is unique in distilling specific information (commonsense) from a general language model.</p>
<p>Data Generation While manual dataset creation is expensive and complex (Schwartz et al., 2017; Agrawal et al., 2018; Tsuchiya, 2018; Bras et al., 2020),crowdsourcing is the most popular method for goal-oriented, high quality/coverage datasets.</p>
<p>Past automatic data mainly use extractive approaches, e.g. syntactic parsing (Zhang et al., 2020a) or pattern matching (Li et al., 2020) from unstructured text (Lehmann et al., 2015; Buck et al., 2014). These scale, but are noisy and limited in format-ATOMIC knowledge will not appear simply in natural text. Some works explore automatic data synthesis/expansion by finetuning LMs on existing labeled data (Anaby-Tavor et al., 2020; Papanikolaou and Pierleoni, 2020; Kumar et al., 2020; Yang et al., 2020), but are limited by data quality.</p>
<h2>7 Conclusions</h2>
<p>We introduce symbolic knowledge distillation, a machine-to-corpus-to-machine pipeline for commonsense that does not require human-authored knowledge-instead, using machine generation. Knowledge is transferred from a large, general model to a compact commonsense model, through
a commonsense corpus-yielding a commonsense knowledge graph and model. Our resulting symbolic knowledge graph has greater scale, diversity, and quality than human authoring. symbolic knowledge distillation offers an alternative to humanauthored knowledge in commonsense research.</p>
<h2>Acknowledgments</h2>
<p>This work was funded in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) (funding reference number 401233309), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI.</p>
<h2>Ethical Considerations</h2>
<p>One aspect of our work with the potential for ethical pitfalls is large-scale generation from pretrained language models, in constructing ATOMIC ${ }^{\mathbf{1 0 s}}$. Recent work (Bender et al., 2021) has highlighted the risks of models trained on massive text resources, as GPT-3 (Brown et al., 2020) is, which we use for generation. Indeed, open generations from pretrained language models can often contain harmful, biased, or offensive aspects. We argue here that this risk is largely mitigated in our work, mainly due to the narrow and constrained nature of our generations. The goal of our work is characterising simple and generic anonymous situations, specifically in terms of commonsense causes and effects. We ensure generations are focused on these topics through careful prompting, which we found to be quite effective at keeping these generations ontopic. As such, the potential for harmful generation is very low; indeed, in a manual inspection of 100 generated examples, we found none that were significant harmful, besides one that contained adult content.</p>
<p>A related concern is the potential for large models and training sets to make automated oppression or exploitation possible, for instance in surveillance or generating fake news. As above, we argue that the generic, commonsense nature of our data and models makes this concern less relevant here. Our data does not contain any information directly related to these harmful domains (e.g. social media or fake news generation). While our data may assist machines in understanding basic situations, this is unlikely to be useful for harmful models given the simplicity of our data and still-flawed commonsense capabilities of even the most advanced</p>
<p>models.
Finally, we note that we ensure fair and generous compensation for all human evaluators we hire through Amazon Mechanical Turk. Based on our estimates of time required per task, we ensure that the effective pay rate is at least $\$ 15$ per hour.</p>
<h2>References</h2>
<p>Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018. Don't just assume; look and answer: Overcoming priors for visual question answering. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49714980.</p>
<p>Prithviraj Ammanabrolu, Wesley Cheung, William Broniec, and Mark O. Riedl. 2021a. Automated storytelling via causal, commonsense plot ordering. In AAAI.</p>
<p>Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur D. Szlam, Tim Rocktaschel, and Jason Weston. 2021b. How to motivate your dragon: Teaching goaldriven agents to speak and act in fantasy worlds. In NAACL.</p>
<p>Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2020. Do not have enough data? deep learning to the rescue! Proceedings of the AAAI Conference on Artificial Intelligence, 34:7383-7390.</p>
<p>Forough Arabshahi, Jennifer Lee, Antoine Bosselut, Yejin Choi, and Tom. Mitchell. 2021. Conversational multi-hop reasoning with neural commonsense knowledge and symbolic logic rules. In EMNLP.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, page 610-623, New York, NY, USA. Association for Computing Machinery.</p>
<p>Sumithra Bhakthavatsalam, Chloe Anastasiades, and Peter E. Clark. 2020. Genericskb: A knowledge base of generic statements. ArXiv, abs/2005.00660.</p>
<p>Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD Conference.</p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, A. Çelikyilmaz, and Yejin Choi. 2019. Comet: Commonsense transformers for automatic knowledge graph construction. In $A C L$.</p>
<p>Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew E. Peters, Ashish</p>
<p>Sabharwal, and Yejin Choi. 2020. Adversarial filters of dataset biases. In ICML.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Christian Buck, Kenneth Heafield, and Bas Van Ooyen. 2014. N-gram counts and language models from the common crawl. In LREC, volume 2, page 4. Citeseer.</p>
<p>Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, and Nanyun Peng. 2020. R'3: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In $A C L$.</p>
<p>Tuhin Chakrabarty, Xurui Zhang, Smaranda Muresan, and Nanyun Peng. 2021. MERMAID: Metaphor generation with symbolism and discriminative decoding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4250-4261, Online. Association for Computational Linguistics.</p>
<p>Ernest Davis and Gary Marcus. 2017. Causal generative models are just a start. Behavioral and Brain Sciences, 40.</p>
<p>Joe Davison, Joshua Feldman, and Alexander M Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173-1178.</p>
<p>Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, et al. 2011. Open information extraction: The second generation. In Twenty-Second International Joint Conference on Artificial Intelligence.</p>
<p>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.</p>
<p>Jack Hessel and Lillian Lee. 2020. Does my multimodal model learn cross-modal interactions? it's harder to tell than you might think! In EMNLP.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop.</p>
<p>Geoffrey E Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771-1800.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2021. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. AAAI.</p>
<p>William R. Kearns, Neha Kaura, Myra Divina, Cuong Viet Vo, Dong Si, Teresa M. Ward, and Weichao Yuwen. 2020. A wizard-of-oz interface and persona-based methodology for collecting health counseling dialog. Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems.</p>
<p>Yoon Kim and Alexander M. Rush. 2016. Sequencelevel knowledge distillation. In EMNLP.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.</p>
<p>Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. Data augmentation using pre-trained transformer models. In Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18-26, Suzhou, China. Association for Computational Linguistics.</p>
<p>J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159-174.</p>
<p>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, D. Kontokostas, Pablo N. Mendes, Sebastian Hellmann, M. Morsey, Patrick van Kleef, S. Auer, and C. Bizer. 2015. Dbpedia - a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6:167-195.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In EMNLP.</p>
<p>Zhongyang Li, Xiao Ding, Ting Liu, J. Edward Hu, and Benjamin Van Durme. 2020. Guided generation of cause and effect. In Proceedings of the TwentyNinth International Joint Conference on Artificial Intelligence, IJCAI-20.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>William Merrill, Yoav Goldberg, Roy Schwartz, and Noah A. Smith. 2021. Provable limitations of acquiring meaning from ungrounded form: What will future language models understand? Transactions of the Association for Computational Linguistics, 9:10471060.</p>
<p>Tom Michael Mitchell, William W. Cohen, Estevam R. Hruschka, Partha P. Talukdar, Bo Yang, Justin Betteridge, Andrew Carlson, Bhavana Dalvi, Matt Gardner, Bryan Kisiel, Jayant Krishnamurthy, N. Lao, Kathryn Mazaitis, Thahir Mohamed, Ndapandula Nakashole, Emmanouil Antonios Platanios, Alan Ritter, Mehdi Samadi, Burr Settles, Richard C. Wang, D. Wijaya, Abhinav Gupta, Xinlei Chen, Abulhair Saparov, Malcolm Greaves, and Joel Welling. 2015. Never-ending learning. Communications of the ACM, 61:103 - 115.</p>
<p>Yannis Papanikolaou and A. Pierleoni. 2020. Dare: Data augmented relation extraction with gpt-2. ArXiv, abs/2004.13845.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108.</p>
<p>Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3027-3035.</p>
<p>Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, and Noah A. Smith. 2017. The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 15-25, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.</p>
<p>Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021. Commonsenseqa 2.0: Exposing the limits of ai through gamification.</p>
<p>Masatoshi Tsuchiya. 2018. Performance impact caused by hidden bias of training data for recognizing textual entailment. In Proceedings of the Eleventh International Conference on Language Resources and</p>
<p>Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In International Conference on Learning Representations.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.</p>
<p>Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2020. Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. In International Conference on Learning Representations.</p>
<p>Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. 2020. Generative data augmentation for commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1008-1025, Online. Association for Computational Linguistics.</p>
<p>Hongming Zhang, Daniel Khashabi, Y. Song, and D. Roth. 2020a. TransOMCS: From linguistic graphs to commonsense knowledge. In IJCAI.</p>
<p>Hongming Zhang, Xin Liu, Haojie Pan, Y. Song, and C. Leung. 2020b. Aser: A large-scale eventuality knowledge graph. Proceedings of The Web Conference 2020.</p>
<p>Ben Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth. 2020. Temporal common sense acquisition with minimal supervision. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7579-7589, Online. Association for Computational Linguistics.</p>
<p>Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval, pages 1097-1100.</p>
<h2>A Human Evaluation Details</h2>
<p>We conduct human evaluations on Amazon Mechanical Turk using the template of Figures 4,5. Workers are presented with ATOMIC-style triples, replacing relations with natural language templates (e.g. HinderedBy becomes "can be hindered by"). 3 annotators rate each triple, with options for acceptability: "always/often", "sometimes/likely", "farfetched/never", "invalid", or "too unfamiliar to judge". The first two are considered "accepted", the second two "rejected" and the final is "no judgement". For reporting acceptance rates, and training a critic model, we only distinguish between "accepted" and not "accepted".</p>
<p>Workers are compensated $\$ 0.17$ per task (i.e. completing all questions in the evaluation template Figures 4,5). We estimate an upper bound of 30s to complete a single task, which gives an hourly rate of $\$ 20.4$. Workers are selected based on an Amazon Mechanical Turk qualification, specifically filtering for workers with high accuracy on past knowledge base triple evaluations. We follow the same setup for all evaluations, besides number of annotators. This setup is shown to result in consistent and reliable annotations, with an inter-annotator agreement given by Fleiss' kappa (Fleiss, 1971) of 40.8 when evaluating with 3 annotators, in $\S 3.4$.</p>
<h2>B Using Alternate Models as Knowledge Sources</h2>
<p>One natural question that arises from the strong performance of symbolic knowledge distillation is whether other sources of knowledge (i.e. language models) would similarly benefit from this method. In this section, we particularly measure the capacity of other language models to serve as the "loose teacher" which generated the base knowledge of the resulting corpus.</p>
<p>We expand our study beyond GPT-3 here (the model used in our work), to include 2 contemporary large language models, GPT-J (Wang and Komatsuzaki, 2021) and T5-11B (Lester et al., 2021) finetuned for language modelling. For knowledge generation (verbalization) we follow the same procedure as $\S 3$ along with simple adjustments to improve quality. We are investigating the effect of the critic on knowledge precision here, so we also include ATOMIC $_{20}^{20}$ to probe the usefulness of automatic filtering for human-authored knowledge.</p>
<p>For each knowledge source, we follow the human evaluation setup in $\S 3.4$ to obtain quality an-
notations of 2000 examples, with 1 annotation per example. This follows a similar setup to $\S 4$-indeed, we are replicating the earlier critic experiments but at a smaller scale (2000 annotations vs. 10000) to allow for more knowledge sources. For each knowledge source, we randomly split into sizes of 1400/300/300 for train, dev, and test sets. We follow $\S 4$ to train a critic model for each knowledge source.</p>
<p>We plot different thresholds (\% of corpus filtered) against the resulting precision (proportion of corpus that is judged to be "valid" knowledge) in Figure 3, and give numbers at various sizes in Table 7. One striking aspect is that a critic model can raise the precision of any of these knowledge sources to approximately $90 \%$ while retaining $30 \%$ of the original corpus size. While this discards a significant portion of the original generated knowledge, it raises the exciting prospect of using more cost-effective models at a large scale to generate strong commonsense corpora like ATOMIC ${ }^{\mathbf{1 0 x}}$. GPT-J and T5-11B can both be run locally by researchers, unlike GPT-3 which uses a pay-pergeneration API. Thus, one can imagine producing a large and high-quality corpus like ATOMIC ${ }^{\mathbf{1 0 x}}$ at a lower cost by instead generating a larger volume of knowledge from such an accessible model, and simply filtering to a greater extent.</p>
<p>Another interesting aspect is how the various knowledge sources diverge. Under little to no critical filtering (i.e. corpus size $=1.0$ ), the precision of various knowledge sources is widely spread. Before applying a critic, quality of knowledge source is very important. Indeed, precision is ordered by cost of generation: human ATOMIC $_{20}^{20}$ has the highest precision while being the most expensive, followed by GPT-3 (used here) which is pay-pergeneration, and finally the two publicly available models. Another point of divergence is for extreme filtering (at approximately $20 \%$ of the original corpus size. All knowledge sources but GPT-3 plateau at approximately $90 \%$ accuracy, while GPT-3 rises towards $100 \%$. Indeed, this supports our use of GPT-3 in this work, as a high-quality automatic knowledge source.</p>
<h2>C Critic Model</h2>
<p>We train binary classifiers (critics) for human acceptability using RoBERTa-Large (Liu et al., 2019), fine-tuning all parameters, along with a 2-layer MLP on the [CLF] representation. We conduct</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Precision at Corpus Size (\%)</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Knowledge Source</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">90</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">70</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">10</td>
</tr>
<tr>
<td style="text-align: left;">ATOMIC $_{20}^{20}$</td>
<td style="text-align: right;">84.0</td>
<td style="text-align: right;">86.3</td>
<td style="text-align: right;">87.9</td>
<td style="text-align: right;">89.0</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">88.7</td>
<td style="text-align: right;">91.7</td>
<td style="text-align: right;">90.0</td>
<td style="text-align: right;">90.0</td>
<td style="text-align: right;">90.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-J</td>
<td style="text-align: right;">71.7</td>
<td style="text-align: right;">76.7</td>
<td style="text-align: right;">81.7</td>
<td style="text-align: right;">83.8</td>
<td style="text-align: right;">86.7</td>
<td style="text-align: right;">88.0</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">87.8</td>
<td style="text-align: right;">93.3</td>
<td style="text-align: right;">90.0</td>
</tr>
<tr>
<td style="text-align: left;">T5-11B</td>
<td style="text-align: right;">64.7</td>
<td style="text-align: right;">66.7</td>
<td style="text-align: right;">70.8</td>
<td style="text-align: right;">74.8</td>
<td style="text-align: right;">79.4</td>
<td style="text-align: right;">84.7</td>
<td style="text-align: right;">89.2</td>
<td style="text-align: right;">92.2</td>
<td style="text-align: right;">91.7</td>
<td style="text-align: right;">93.3</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 curie</td>
<td style="text-align: right;">79.3</td>
<td style="text-align: right;">81.5</td>
<td style="text-align: right;">85.0</td>
<td style="text-align: right;">86.2</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">90.7</td>
<td style="text-align: right;">91.7</td>
<td style="text-align: right;">90.0</td>
<td style="text-align: right;">98.3</td>
<td style="text-align: right;">100.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Knowledge precision at various corpus sizes (from 100\% to 10\%) based on filtering by the critic model. Precision is calculated by human annotation of valid or invalid knowledge. We consider 4 knowledge sources, as described in Appendix B. This corresponds to the data plotted in Figure 3.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Precision resulting from the critic step from $\S 4$, with various thresholds. We include corpora generated by GPT-3 (ATOMIC ${ }^{\mathbf{1 0 x}}$ ), GPT-J, T5-11B, and humans (ATOMIC ${ }_{20}^{20}$ ). Without filtering (corpus size $=$ 1.0), different corpora have a variety of precisions. As more examples are filtered by the critic, precision rises significantly demonstrating the strong value of the critic step.
a small grid search on the validation set finding batch size 128, dropout .1, and Adam (Kingma and $\mathrm{Ba}, 2015$ ) learning rate $5 \mathrm{e}-6$ to be effective. We use early stopping and decay learning rate on validation performance plateauing, to maximize $R @ 80 \%$ on the validation set. We find RoBERTa pretrained on MNLI (Williams et al., 2018) effective, outperforming other options. As well, we substitute randomly-sampled names in for person designations "X"/"Y". We include as a baseline an unsupervised filtration metric inspired by (Davison et al., 2019): they propose a model estimate of PMI to score mined commonsense triples. In our case, we use Negative Log-Likelihood (NLL) and token-mean-NLL from GPT-3 itself.</p>
<p>The validation precision/recall of our best performing model, the baselines, and the in-optimal hyperparameter configurations are given in Figure 6. Once fixing our model, we applied it to the test set (also in Fig 6), verifying that it generalizes to ATOMIC ${ }^{\mathbf{1 0 x}}$ entries. Overall, our trained critic model is more effective than the baselines in identifying high and low quality teacher generations at all levels of precision and recall. This result demonstrates that a small amount of human supervision can consistently help to correct GPT-3's mistakes.</p>
<h2>D ATOMIC ${ }^{\mathbf{1 0 x}}$ Generation Prompts</h2>
<p>We include example prompts for all generations we do, from Table 8 to 15 . Note that elements of generation prompts are randomized for each batch. For event generation, the few-shot examples and order are randomly sampled from a seed set of 100 high-quality examples from ATOMIC ${ }_{20}^{20}$ in each batch. For inference generation, the natural names used for PersonX and PersonY are randomly sampled from a small predefined set of names.</p>
<p>Instructions (click to expand/collapse)
WARNING: This HIT may contain adult content. Worker discretion is advised.)
Thanks for participating in this HIT!
If the data is good, it's good. If bad, then bad. Please annotate as you see not worrying about how many of each label you find yourself assigning! If you understand the words but the Phrases or the complete assertation makes poor sense, please mark as INVALID. Thank you!</p>
<h1>You will evaluate how often assertions are true. Each assertion is comprised of 3 parts: Phrase A, Relation, Phrase B</h1>
<p>Phrase A, Phrase B Short phrases. May describe objects, object properties, events, actions, etc. Relation How A relates to B.</p>
<h2>For each assertion, determine how true it is:</h2>
<p>always/often Always or quite often true.
sometimes/likely Sometimes is true or true for some people. -or- Likely true.
farfetched/never False or farfetched, at best. -or- Unlikely to be true.
invalid. This assertion makes no sense (i.e., "what does this even mean?!").
too unfamiliar to judge Cannot make a fair evaluation. Unfamiliar with one or both of the phrase.</p>
<h2>If you see "nothing in particular" for Phrase B, assess Phrase B in context:</h2>
<ul>
<li>Sometimes certain actions can simply be responded to by doing nothing!</li>
<li>Other times, doing nothing in particular is simply a weird or unlikely reaction to something.</li>
<li>See examples under tricky relations tagged with nothing in particular example.</li>
</ul>
<p>Please report any prejudiced or inappropriate language:</p>
<ul>
<li>Profane or offensive content (NSTW, R-rated material etc)</li>
<li>Prejudiced assumptions or derogatory language that villainizes people.</li>
</ul>
<p>HOWEVER, please note, not all negative content is derogatory especially if Phrase B is intrinsically what Phrase A means. For example:
criminals are characterized by committing crime to OK.
$\sim$ This isn't necessarily villianizing people since "criminal" means "a person who has commited a crime". homeless are characterized by being lazy is prejudiced.
$\sim$ There are many reason a person is rendered homeless. This is a gratuitous prejudice about homelessness.</p>
<ul>
<li>Material that people may find disturbing, off-putting, or improper</li>
</ul>
<p>A couple NOTES:</p>
<ul>
<li>Please be forgiving of spelling or grammatical errors</li>
<li>If the terms are too obscure or you don't know the truth of the fact at the top of your head, it is okay to mark is "too unfamiliar to judge". If you can answer (e.g., based on likelihood), please provide a response.</li>
</ul>
<p>Tricky Relations (click to expand/collapse)</p>
<p>Examples (click to expand/collapse)</p>
<p>Figure 4: Page 1 of template used for human evaluation.</p>
<p>1) PersonX approaches PersonY's aunt, as a result, PersonX feels, awkward</p>
<h1>How often does the assertion hold true?</h1>
<p>always/often sometimes/likely farfetched/never invalid too unfamiliar to judge
$\square$ This fact is true but outdated
$\square$ I would count this as an inappropriate, prejudiced or offensive material
2) PersonX asked PersonY out on a date, can be hindered by, PersonX is still dating Sarah</p>
<h2>How often does the assertion hold true?</h2>
<p>always/often sometimes/likely farfetched/never invalid too unfamiliar to judge
$\square$ This fact is true but outdated
$\square$ I would count this as an inappropriate, prejudiced or offensive material
3) PersonX fails to go home, as a result, PersonX, is grounded</p>
<h2>How often does the assertion hold true?</h2>
<p>always/often sometimes/likely farfetched/never invalid too unfamiliar to judge
$\square$ This fact is true but outdated
$\square$ I would count this as an inappropriate, prejudiced or offensive material
4) PersonX makes her own clothes, as a result, PersonX feels, artistic</p>
<h2>How often does the assertion hold true?</h2>
<p>always/often sometimes/likely farfetched/never invalid too unfamiliar to judge
$\square$ This fact is true but outdated
$\square$ I would count this as an inappropriate, prejudiced or offensive material
5) PersonX notices PersonY's response, can be hindered by, PersonX is distracted by the music</p>
<h2>How often does the assertion hold true?</h2>
<p>always/often sometimes/likely farfetched/never invalid too unfamiliar to judge
$\square$ This fact is true but outdated
$\square$ I would count this as an inappropriate, prejudiced or offensive material
(Optional) Please let us know if anything was unclear, if you experienced any issues, or if you have any other fedback for us.
$\square$</p>
<p>| 1. Event: PersonX unwraps PersonY's hands |
| :-- | :-- |
| 2. Event: PersonX overcomes evil with good |
| 3. Event: PersonX is fed up with the present situation |
| 4. Event: PersonX breaks PersonX's back |
| 5. Event: PersonX calls no one |
| 6. Event: PersonX never gets angry |
| 7. Event: PersonX does not learn from PersonY |
| 8. Event: PersonX refuses to touch PersonY's hands |
| 9. Event: PersonX looks at flowers |
| 10. Event: PersonX unloads an atomic bomb |
| 11. Event: |</p>
<p>Table 8: Prompt for head generation.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: Precision vs. recall of our critic model on the human labelled validation set. The best trained models are labelled, and other hyper-parameter settings are shown as faded lines. We also include generation negative log-likelihood (nll) and token-wise mean nll as cutoff measures-these perform much worse than the supervised model.</p>
<p>Next, how are people seen in each situation? Examples:</p>
<p>Situation 1: Devin bullies Jean.</p>
<p>Devin is seen as dominant.</p>
<p>Situation 2: Jamie moves to another city.</p>
<p>Jamie is seen as adventurous.</p>
<p>Situation 3: Sydney changes Ryan's mind.</p>
<p>Sydney is seen as influential.</p>
<p>Situation 4: Lindsay writes a story.</p>
<p>Lindsay is seen as creative.</p>
<p>Situation 5: Rowan covers Pat's expenses.</p>
<p>Rowan is seen as wealthy.</p>
<p>Situation 6: Lee takes time off.</p>
<p>Lee is seen as carefree.</p>
<p>Situation 7: Riley advises Noel.</p>
<p>Riley is seen as informed.</p>
<p>Situation 8: Adrian bursts into tears.</p>
<p>Adrian is seen as depressed.</p>
<p>Situation 9: Hunter deals with problems.</p>
<p>Hunter is seen as responsible.</p>
<p>Situation 10: Sam follows Charlie.</p>
<p>Sam is seen as suspicious.</p>
<p>Situation 11: Alex makes Chris wait.</p>
<p>Alex is seen as</p>
<p>Table 9: Prompt for generating xAttr.</p>
<p>Next, what do situations make people do? Examples:</p>
<p>Situation 1: Devin gets a divorce.</p>
<p>As a result, Devin dates someone new.</p>
<p>Situation 2: Jamie lifts weights.</p>
<p>As a result, Jamie has sore muscles.</p>
<p>Situation 3: Sydney takes Ryan to a bar.</p>
<p>As a result, Sydney gets drunk.</p>
<p>Situation 4: Lindsay decides to hire a tutor.</p>
<p>As a result, Lindsay gets better grades.</p>
<p>Situation 5: Rowan buys Pat drinks.</p>
<p>As a result, Rowan is thanked by Pat.</p>
<p>Situation 6: Lee hears bad news.</p>
<p>As a result, Lee begins to cry.</p>
<p>Situation 7: Riley buys a chocolate bar.</p>
<p>As a result, Riley gets change.</p>
<p>Situation 8: Adrian does a lot of work.</p>
<p>As a result, Adrian gets mental fatigue.</p>
<p>Situation 9: Hunter attends a concert.</p>
<p>As a result, Hunter hears a new song.</p>
<p>Situation 10: Sam gets the job done.</p>
<p>As a result, Sam gets more responsibilities.</p>
<p>Situation 11: Alex makes Chris wait.</p>
<p>As a result, Alex</p>
<p>Table 10: Prompt for generating xEffect.</p>
<p>For each situation, describe the intent. Examples:</p>
<p>Situation 1: Devin gets the newspaper.
Devin intends to read the newspaper.
Situation 2: Jamie works all night.
Jamie intends to meet a deadline.
Situation 3: Sydney destroys Ryan.
Sydney intends to punish Ryan.
Situation 4: Lindsay clears her mind.
Lindsay intends to be ready for a new task.
Situation 5: Rowan wants to start a business.
Rowan intends to be self sufficient.
Situation 6: Lee ensures Ali's safety.
Lee intends to be helpful.
Situation 7: Riley buys lottery tickets.
Riley intends to become rich.
Situation 8: Alex makes Chris wait.
Alex intends</p>
<p>Table 11: Prompt for generating xIntent.</p>
<p>Next, we will discuss what people need for certain situations. Examples:</p>
<ol>
<li>Before Devin makes many new friends, Devin has to spend time with people.</li>
<li>Before Jamie gets a date, Jamie has to ask someone out.</li>
<li>Before Sydney changes Ryan's mind, Sydney has to think of an argument.</li>
<li>Before Lindsay gets a job offer, Lindsay has to apply.</li>
<li>Before Rowan takes a quick nap, Rowan has to lie down.</li>
<li>Before Lee tries to kiss Ali, Lee has to approach Ali.</li>
<li>Before Riley rides Noel's skateboard, Riley has to borrow it.</li>
<li>Before Adrian eats the food, Adrian has to prepare a meal.</li>
<li>Before Hunter watches Netflix, Hunter has to turn on the TV.</li>
<li>Before Sam has a baby shower, Sam has to invite some friends.</li>
<li>Before Alex makes Chris wait, Alex has</li>
</ol>
<p>Table 12: Prompt for generating xNeed.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ We find Fleiss' kappa (Fleiss, 1971) of 47.1 for acceptance, indicating moderate agreement. (Landis and Koch, 1977), and accuracy agreement of $88.7 \%$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{11}$ The slight difference in acceptability for GPT-3 from Table 4 is likely due to variance in raters between rounds of evaluation, and a different distribution of events-Table 4 uses generated events while Table 6 uses events from $\mathrm{Atomic}_{20}^{20}$.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
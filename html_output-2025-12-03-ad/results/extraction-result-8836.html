<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8836 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8836</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8836</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-01a35c75721650182fce1c3ea39ab0911211fdbd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/01a35c75721650182fce1c3ea39ab0911211fdbd" target="_blank">Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining</a></p>
                <p><strong>Paper Venue:</strong> Mathematics</p>
                <p><strong>Paper TL;DR:</strong> This review systematically review the combination and application techniques of LLMs and GNNs and presents a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM(GdL), LLM-driving-GNN(LdG), and GNN-LLM-co-driving(GLcd).</p>
                <p><strong>Paper Abstract:</strong> Graph mining is an important area in data mining and machine learning that involves extracting valuable information from graph-structured data. In recent years, significant progress has been made in this field through the development of graph neural networks (GNNs). However, GNNs are still deficient in generalizing to diverse graph data. Aiming to this issue, large language models (LLMs) could provide new solutions for graph mining tasks with their superior semantic understanding. In this review, we systematically review the combination and application techniques of LLMs and GNNs and present a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM(GdL), LLM-driving-GNN(LdG), and GNN-LLM-co-driving(GLcd). Within this framework, we reveal the capabilities of LLMs in enhancing graph feature extraction as well as improving the effectiveness of downstream tasks such as node classification, link prediction, and community detection. Although LLMs have demonstrated their great potential in handling graph-structured data, their high computational requirements and complexity remain challenges. Future research needs to continue to explore how to efficiently fuse LLMs and GNNs to achieve more powerful graph learning and reasoning capabilities and provide new impetus for the development of graph mining techniques.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8836.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8836.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TalkLikeAGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Talk Like a Graph: Encoding Graphs for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flattening/encoding approach that studies how to convert graphs into text descriptions so LLMs can reason about graph structure; introduces and evaluates generic 'flattening functions' for graph->text conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>flattening / graph-to-text encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts graph G and node/edge attributes into sequential textual descriptions (Describe = f(G, Attr)) via general flattening functions so LLMs can accept graph information as natural language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / text-attributed graphs (TAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Generic 'flattening functions' that serialize graph structure and attributes into text; the paper frames f(G, Attr) abstractly and studies different text encodings for nodes and edges.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>graph reasoning and simple graph-structure tasks (e.g., node classification, link prediction, graph reasoning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No single numeric metric reported in this survey for this method; survey cites that choosing suitable encoding can significantly improve LLM reasoning but provides no unified numbers for TalkLikeAGraph itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented as the first comprehensive investigation of graph->text encoding; compared qualitatively in the survey to other specific encodings like GraphML-like conversion, syntax-tree traversal (GraphText), and node-edge lists.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple and intuitive; enables direct use of LLMs for graph tasks without graph-specific models; flexible to different graph attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Potential information loss due to linearization; can produce very long sequences limited by LLM input length; choice of flattening function strongly affects performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported to struggle on complex graphs and multi-hop structural reasoning when naive flattening loses topology; survey notes LLMs perform well on simple graph reasoning but insufficiently on complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8836.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts graphs into a graph syntax tree and traverses it to produce natural language graph prompts, enabling LLMs to treat graph reasoning as text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtext: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph syntax tree traversal / natural-language prompts</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructs a graph syntax tree from graph data and generates textual prompts by traversing the tree, outputting natural-language descriptions that preserve hierarchical and relational structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Build syntax tree from graph, perform traversal (explicitly described) to produce structured natural language statements describing nodes, edges and neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>graph reasoning posed as text generation; node classification and other reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey does not list a numeric metric for GraphText specifically; it is discussed qualitatively as an approach enabling LLMs to perform graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively to flattening and GraphML-like serializations; presented as a more structured natural-language encoding than simple node-edge lists.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves hierarchical/structural relationships through syntax-tree organization; yields prompts more amenable to LLM reasoning than naive lists.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Conversion is more complex; may still produce long text sequences and can be sensitive to traversal order/design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May lose global structural context for very large graphs due to truncation and prompt-length limits; complexity of conversion may hinder scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8836.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GraphML-like serialization approach that translates graphs into a Graph Description Language (e.g., GraphML-like) and pairs that with prompts so LLMs (like GPT-4) can process graph-structured inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Description Language (GDL) / GraphML-like serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph topology and attributes into a machine-readable Graph Description Language (similar to GraphML) and includes that encoding in LLM prompts to allow the LLM to parse and answer graph queries.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs; benchmark graphs used for empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize nodes, edges, attributes into GDL/GraphML-like markup text embedded in prompts along with user queries.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>graph understanding and reasoning benchmarks (e.g., empirical evaluation tasks measuring LLM ability to answer graph queries)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports general findings from GPT4Graph that LLMs can understand simple graphs but struggle on complex tasks; no single aggregated numeric metric from the survey entry.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Positioned against simpler natural-language lists and tree-based encodings; uses a more structured, machine-like serialization expected to reduce ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly represents structure and attributes in a standardized markup yielding less ambiguity than free-form natural language; amenable to programmatic parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Still subject to LLM input length limits; the machine-like markup may be less amenable to LLMs trained mostly on natural language and may require careful prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>LLMs exhibit limited capability on complex graph reasoning even with GDL encodings; performance degrades with graph complexity and multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8836.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node-Edge List (NL) Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language node-edge list (digital organized list of nodes and edges)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A straightforward representation that records graph data directly in natural language as organized lists of nodes and edges, used as a baseline graph->text conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models on graphs: Performance insights and comparative analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>node-edge list (natural-language serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Describes the graph as enumerated node entries and edge entries in natural language (e.g., 'Node A connected to Node B, Node C; Node A attributes: ...'), forming a sequence usable as LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / synthetic graphs used for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Convert adjacency and attribute information into a linearized textual list of node descriptions and explicit edge statements.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>graph reasoning and evaluation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey cites evaluations showing LLMs can handle simple tasks with this encoding but struggles with complex structure; no single numeric metric provided for node-edge lists in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared as the simplest encoding; generally outperformed by more structured encodings (e.g., syntax-tree traversal or GraphML-like) on complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Very simple to implement and interpret; transparent mapping from graph to text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>High risk of losing structural context for large graphs; can produce very long sequences; sensitive to ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performs poorly on multi-hop and global-structure reasoning tasks due to truncation and loss of compact structural representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8836.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT: Graph Instruction Tuning for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction tuning and CoT augmentation approach for LLMs that includes a lightweight graph-text projector aligning graph structure representations to text embedding space, improving LLM graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>instruction-tuned graph->text + graph-text projector</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses chain-of-thought (CoT) style instructions and trains a light graph-text projector to align graph structural representations to LLM token embedding space so the LLM can seamlessly process graph inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs and textual graphs (instruction datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Create instruction-tuning datasets containing prompts augmented with graph-related instructions and call sequences; use a learned projector to map graph features into the text embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>graph reasoning, node classification (as evaluated), and other instruction-based graph tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In survey Table 2 GraphGPT-7B-v1.5-stage2 attains 75.11% accuracy on ogbn-arxiv node classification (as reported in table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Reported alongside other instruction-tuning methods (e.g., InstructGLM); shows comparable performance but specific relative claims are qualitative in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Improves LLM reasoning via instruction tuning and explicit alignment; leverages CoT to boost multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires extra instruction-tuning data and a learned projector component; additional training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Noted that LLMs still have limitations on complex graph reasoning despite instruction tuning; projector alignment may be insufficient for very large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8836.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-ToolFormer: Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses LLMs (ChatGPT) to produce prompts augmented with API calls to graph inference tools and fine-tunes causal LLMs to automatically call external graph tools during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>prompt augmentation with tool-use API calls</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of fully serializing graphs into text, Graph-ToolFormer augments prompts with explicit API call tokens that instruct the model to invoke external graph reasoning tools (e.g., shortest-path, connectivity) during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs requiring algorithmic graph inference (e.g., path finding, centrality)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Create a large dataset of prompts containing graph reasoning API calls (annotated/expanded by ChatGPT), fine-tune causal LLMs to insert API calls appropriately in outputs to delegate computation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>complex graph reasoning tasks requiring precise computation and multi-step logic</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey describes that tool-augmented LLMs better address limitations like precise computation and multi-step logic; no single numeric metric provided in the survey entry.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Offloads precise graph computation to specialized tools instead of pure serialization; presented as addressing shortcomings of pure text encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Mitigates LLM weaknesses in precise computation and topological reasoning; leverages external graph tools for accurate results.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires availability and integration of external graph tools and extra engineering; depends on correct tool selection and call placement.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If tool calls are not invoked correctly or tools are unavailable, performance degrades; tool integration increases system complexity and potential failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8836.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoleculeSTM / InstructMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructMol: Multi-modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical/molecular alignment approach that aligns molecular graph structures with natural language via contrastive training and a lightweight alignment projector, enabling LLMs to process molecular graphs as text-aligned embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>contrastive structure-text alignment + alignment projector</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes molecular graphs with a graph encoder, aligns graph and textual representations through contrastive training, and uses a lightweight projector to map graph features to LLM word embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graphs (chemical structures)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Learn joint embeddings via contrastive pretraining on molecule-text pairs, then use a lightweight projector to align graph embeddings into the LLM embedding space for downstream LLM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>molecule retrieval, text-based molecular editing, zero-shot molecular property prediction and molecular-text retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey references MoleculeSTM (and related datasets like PubChemSTM) and states it improves retrieval/editing tasks; no explicit numeric metric quoted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to pure-text encodings and naive flattening, alignment via contrastive training provides better cross-modal retrieval and edit capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables cross-modal retrieval and editing; leverages structure-specific encoders for chemical detail while allowing language-model interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires paired molecule-text datasets and contrastive pretraining; extra model components (projector) add complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May fail when textual descriptions are low-quality or inconsistent with structural features; alignment may not generalize across very different molecular classes without broad pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8836.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DGTL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach that uses a frozen LLM to encode text attributes and custom disentangled GNN layers to capture neighborhood structure, ultimately converting graph neighborhood information into text-finetunable features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Disentangled representation learning with large language models for text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>frozen-LLM text encoding + disentangled GNN -> textual-friendly features</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses a frozen LLM to encode node textual attributes, then applies disentangled GNN layers to capture graph neighborhood information and produces features suitable for fine-tuning the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs (TAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Encode raw textual node information with frozen LLM, disentangle neighborhood effects via custom GNN layers, and use the resulting features to fine-tune/facilitate LLM understanding of graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification and other TAG tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey mentions DGTL as a method but does not provide numeric performance values in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented as an approach that blends frozen LLM text encodings with specialized GNN layers; contrasted with pure text flattening and pure GNN approaches for TAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves strengths of powerful frozen LLM encodings while explicitly modeling structural neighborhood with disentangled GNNs; reduces need to fine-tune full LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds GNN complexity and extra training components; not fully end-to-end LLM-only.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May be limited if frozen LLM encodings lack alignment with structural signals or if disentangling fails on heterogeneous neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8836.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuseGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction tuning strategy that generates compact graph descriptions using neighbor nodes and random walks and creates task-based chain-of-thought instruction sets to fine-tune LLMs for graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>compact neighborhood/random-walk-based graph descriptions + CoT instruction sets</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graphs into compact textual descriptions by sampling neighbor nodes and random walks and pairs these with task-specific chain-of-thought instructions for instruction-tuning LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / TAGs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Generate compact descriptions based on neighborhood sampling and random walks, then assemble task-based CoT instruction packs for fine-tuning LLMs; dynamically allocate instruction packages across tasks/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification, link prediction, graph-to-text generation, and other graph mining tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states MuseGraph improves LLM performance with dynamic instruction allocation but does not report specific numeric metrics in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Contrasted with naive flattening and with other instruction-tuning approaches; emphasizes compactness and task-adaptive instruction allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces prompt length via compact descriptions; tailors instruction units to datasets/tasks improving generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Sampling/random-walk summarization may omit important global structure; design of instruction packs adds complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May underperform on tasks requiring full global structure (e.g., long-range multi-hop reasoning) due to compacting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8836.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIMLET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tunes LLMs with graph-text instruction data so the LLM directly outputs predictive labels from compact graph descriptions, focusing on molecular graphs and zero-shot molecule tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>instruction-finetuned compact graph descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Fine-tunes LLMs to accept compact graph descriptions and directly output predictive labels, using instruction-based datasets crafted for molecular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize molecular neighborhood information into compact descriptions and fine-tune LLMs via instruction datasets to produce labels directly.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>molecular property prediction (zero-shot), molecule classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes GIMLET fine-tunes LLMs to output predictive labels directly, improving predictive accuracy in molecular zero-shot tasks but provides no numeric figures in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared conceptually with other fine-tuning approaches like MuseGraph and InstructGLM; positioned as molecule-focused.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables direct LLM label output without intermediate parsing; targets zero-shot capability for molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires curated instruction-label datasets; domain-specific (molecules) so less general.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May fail when molecular descriptions are insufficiently informative or when molecule classes are out-of-distribution for the instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8836.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGLM (Language is all a graph needs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that designs highly scalable rule-based natural language prompts describing graph structures and fine-tunes LLMs with these instructions so LLMs can perform graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language is all a graph needs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>rule-based natural language prompting for graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Creates scalable, rule-based natural language prompt templates to describe graph topology and tasks, and fine-tunes LLMs on these prompts enabling the models to understand and process such descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs and text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply rule-based templates to convert graph elements into natural language instructions/prompts; use these to fine-tune LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification and other graph tasks; InstructGLM reported on ogbn-arxiv node classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In survey Table 2, InstructGLM (Llama-7b) reports 75.70% on ogbn-arxiv node classification (as listed).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared in Table 2 to models like OFA, GraphGPT and GraphAdapter; performance is competitive though slightly below best reported in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Highly scalable prompting rules; enables LLM fine-tuning without complex graph encoders; works across tasks via prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Rule-based prompts may not capture complex structure; limited by prompt design and LLM capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May degrade on very large or highly structured graphs due to prompt length and rule limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8836.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphTranslator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that produces graph-text alignment data via a 'Producer' mechanism, enabling LLMs to predict graph data from language instructions by aligning graph representations to LLM space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtranslator: Aligning graph model to large language model for open-ended tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>producer-based graph-text alignment</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses a Producer mechanism to generate aligned graph-text pairs for training, teaching LLMs to predict/understand graph data given natural language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>heterogeneous graphs / general graphs for open-ended tasks</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Generate alignment datasets (graph, textual instruction pairs) via Producer; train alignment to allow LLMs to map language instructions to graph predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>open-ended graph tasks, graph prediction from language</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey mentions GraphTranslator as enabling LLM prediction of graph data; no numeric metrics provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Positioned as an explicit alignment-data generator compared to ad-hoc flattening; complements instruction-tuning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Creates direct graph-text supervision enabling LLMs to learn graph semantics from language; useful for open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires generation of high-quality aligned data; Producer quality crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If Producer produces low-quality alignments, LLM predictions will be unreliable; may not scale without automation of high-quality pairing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8836.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8836.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphLLM: Boosting Graph Reasoning Ability of Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end approach that integrates a graph transformer with an LLM by generating graph-enhanced prefixes injected into LLM attention layers, thereby enabling structural information to inform language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphllm: Boosting graph reasoning ability of large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph transformer -> graph-enhanced prefix injection</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses a graph Transformer to learn graph structure and aggregate node representations; produces graph representations that are injected as prefixes into each LLM attention layer to condition LLM reasoning on structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Textual transformer encoder-decoder extracts node descriptions, graph Transformer learns structure, aggregated graph representations become prefixes added to LLM attention at each layer.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>graph inference, question answering over graphs, node classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes GraphLLM integrates graph transformer prefixes to improve graph inference but provides no explicit numeric metrics in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Distinguished from pure graph->text flattening by creating learned structural prefix injection; compared conceptually with other co-driving architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Deep integration preserves structural signals across LLM layers; leverages complementary strengths of graph transformers and LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increased model complexity and compute; requires joint training of graph transformer and LLM prefix layers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May be computationally heavy; prefix injection may not fully capture large-scale global graph properties if prefix dimension or scope limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt <em>(Rating: 2)</em></li>
                <li>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery <em>(Rating: 1)</em></li>
                <li>Disentangled representation learning with large language models for text-attributed graphs <em>(Rating: 1)</em></li>
                <li>Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining <em>(Rating: 1)</em></li>
                <li>Language is all a graph needs <em>(Rating: 1)</em></li>
                <li>Graphllm: Boosting graph reasoning ability of large language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8836",
    "paper_id": "paper-01a35c75721650182fce1c3ea39ab0911211fdbd",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "TalkLikeAGraph",
            "name_full": "Talk Like a Graph: Encoding Graphs for Large Language Models",
            "brief_description": "A flattening/encoding approach that studies how to convert graphs into text descriptions so LLMs can reason about graph structure; introduces and evaluates generic 'flattening functions' for graph-&gt;text conversion.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models",
            "mention_or_use": "mention",
            "representation_name": "flattening / graph-to-text encoding",
            "representation_description": "Converts graph G and node/edge attributes into sequential textual descriptions (Describe = f(G, Attr)) via general flattening functions so LLMs can accept graph information as natural language prompts.",
            "graph_type": "general graphs / text-attributed graphs (TAGs)",
            "conversion_method": "Generic 'flattening functions' that serialize graph structure and attributes into text; the paper frames f(G, Attr) abstractly and studies different text encodings for nodes and edges.",
            "downstream_task": "graph reasoning and simple graph-structure tasks (e.g., node classification, link prediction, graph reasoning benchmarks)",
            "performance_metrics": "No single numeric metric reported in this survey for this method; survey cites that choosing suitable encoding can significantly improve LLM reasoning but provides no unified numbers for TalkLikeAGraph itself.",
            "comparison_to_others": "Presented as the first comprehensive investigation of graph-&gt;text encoding; compared qualitatively in the survey to other specific encodings like GraphML-like conversion, syntax-tree traversal (GraphText), and node-edge lists.",
            "advantages": "Simple and intuitive; enables direct use of LLMs for graph tasks without graph-specific models; flexible to different graph attributes.",
            "disadvantages": "Potential information loss due to linearization; can produce very long sequences limited by LLM input length; choice of flattening function strongly affects performance.",
            "failure_cases": "Reported to struggle on complex graphs and multi-hop structural reasoning when naive flattening loses topology; survey notes LLMs perform well on simple graph reasoning but insufficiently on complex problems.",
            "uuid": "e8836.0",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GraphText",
            "name_full": "GraphText: Graph reasoning in text space",
            "brief_description": "A method that converts graphs into a graph syntax tree and traverses it to produce natural language graph prompts, enabling LLMs to treat graph reasoning as text generation.",
            "citation_title": "Graphtext: Graph reasoning in text space",
            "mention_or_use": "mention",
            "representation_name": "graph syntax tree traversal / natural-language prompts",
            "representation_description": "Constructs a graph syntax tree from graph data and generates textual prompts by traversing the tree, outputting natural-language descriptions that preserve hierarchical and relational structure.",
            "graph_type": "general graphs / text-attributed graphs",
            "conversion_method": "Build syntax tree from graph, perform traversal (explicitly described) to produce structured natural language statements describing nodes, edges and neighborhoods.",
            "downstream_task": "graph reasoning posed as text generation; node classification and other reasoning tasks",
            "performance_metrics": "Survey does not list a numeric metric for GraphText specifically; it is discussed qualitatively as an approach enabling LLMs to perform graph reasoning.",
            "comparison_to_others": "Compared qualitatively to flattening and GraphML-like serializations; presented as a more structured natural-language encoding than simple node-edge lists.",
            "advantages": "Preserves hierarchical/structural relationships through syntax-tree organization; yields prompts more amenable to LLM reasoning than naive lists.",
            "disadvantages": "Conversion is more complex; may still produce long text sequences and can be sensitive to traversal order/design choices.",
            "failure_cases": "May lose global structural context for very large graphs due to truncation and prompt-length limits; complexity of conversion may hinder scaling.",
            "uuid": "e8836.1",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT4Graph",
            "name_full": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "brief_description": "A GraphML-like serialization approach that translates graphs into a Graph Description Language (e.g., GraphML-like) and pairs that with prompts so LLMs (like GPT-4) can process graph-structured inputs.",
            "citation_title": "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
            "mention_or_use": "mention",
            "representation_name": "Graph Description Language (GDL) / GraphML-like serialization",
            "representation_description": "Encodes graph topology and attributes into a machine-readable Graph Description Language (similar to GraphML) and includes that encoding in LLM prompts to allow the LLM to parse and answer graph queries.",
            "graph_type": "general graphs; benchmark graphs used for empirical evaluation",
            "conversion_method": "Serialize nodes, edges, attributes into GDL/GraphML-like markup text embedded in prompts along with user queries.",
            "downstream_task": "graph understanding and reasoning benchmarks (e.g., empirical evaluation tasks measuring LLM ability to answer graph queries)",
            "performance_metrics": "Survey reports general findings from GPT4Graph that LLMs can understand simple graphs but struggle on complex tasks; no single aggregated numeric metric from the survey entry.",
            "comparison_to_others": "Positioned against simpler natural-language lists and tree-based encodings; uses a more structured, machine-like serialization expected to reduce ambiguity.",
            "advantages": "Explicitly represents structure and attributes in a standardized markup yielding less ambiguity than free-form natural language; amenable to programmatic parsing.",
            "disadvantages": "Still subject to LLM input length limits; the machine-like markup may be less amenable to LLMs trained mostly on natural language and may require careful prompting.",
            "failure_cases": "LLMs exhibit limited capability on complex graph reasoning even with GDL encodings; performance degrades with graph complexity and multi-hop reasoning.",
            "uuid": "e8836.2",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Node-Edge List (NL) Encoding",
            "name_full": "Natural-language node-edge list (digital organized list of nodes and edges)",
            "brief_description": "A straightforward representation that records graph data directly in natural language as organized lists of nodes and edges, used as a baseline graph-&gt;text conversion.",
            "citation_title": "Evaluating large language models on graphs: Performance insights and comparative analysis",
            "mention_or_use": "mention",
            "representation_name": "node-edge list (natural-language serialization)",
            "representation_description": "Describes the graph as enumerated node entries and edge entries in natural language (e.g., 'Node A connected to Node B, Node C; Node A attributes: ...'), forming a sequence usable as LLM input.",
            "graph_type": "general graphs / synthetic graphs used for evaluation",
            "conversion_method": "Convert adjacency and attribute information into a linearized textual list of node descriptions and explicit edge statements.",
            "downstream_task": "graph reasoning and evaluation benchmarks",
            "performance_metrics": "Survey cites evaluations showing LLMs can handle simple tasks with this encoding but struggles with complex structure; no single numeric metric provided for node-edge lists in the survey.",
            "comparison_to_others": "Compared as the simplest encoding; generally outperformed by more structured encodings (e.g., syntax-tree traversal or GraphML-like) on complex reasoning.",
            "advantages": "Very simple to implement and interpret; transparent mapping from graph to text.",
            "disadvantages": "High risk of losing structural context for large graphs; can produce very long sequences; sensitive to ordering.",
            "failure_cases": "Performs poorly on multi-hop and global-structure reasoning tasks due to truncation and loss of compact structural representation.",
            "uuid": "e8836.3",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT: Graph Instruction Tuning for Large Language Models",
            "brief_description": "Instruction tuning and CoT augmentation approach for LLMs that includes a lightweight graph-text projector aligning graph structure representations to text embedding space, improving LLM graph reasoning.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models",
            "mention_or_use": "mention",
            "representation_name": "instruction-tuned graph-&gt;text + graph-text projector",
            "representation_description": "Uses chain-of-thought (CoT) style instructions and trains a light graph-text projector to align graph structural representations to LLM token embedding space so the LLM can seamlessly process graph inputs.",
            "graph_type": "general graphs and textual graphs (instruction datasets)",
            "conversion_method": "Create instruction-tuning datasets containing prompts augmented with graph-related instructions and call sequences; use a learned projector to map graph features into the text embedding space.",
            "downstream_task": "graph reasoning, node classification (as evaluated), and other instruction-based graph tasks",
            "performance_metrics": "In survey Table 2 GraphGPT-7B-v1.5-stage2 attains 75.11% accuracy on ogbn-arxiv node classification (as reported in table).",
            "comparison_to_others": "Reported alongside other instruction-tuning methods (e.g., InstructGLM); shows comparable performance but specific relative claims are qualitative in the survey.",
            "advantages": "Improves LLM reasoning via instruction tuning and explicit alignment; leverages CoT to boost multi-step reasoning.",
            "disadvantages": "Requires extra instruction-tuning data and a learned projector component; additional training complexity.",
            "failure_cases": "Noted that LLMs still have limitations on complex graph reasoning despite instruction tuning; projector alignment may be insufficient for very large graphs.",
            "uuid": "e8836.4",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Graph-ToolFormer",
            "name_full": "Graph-ToolFormer: Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
            "brief_description": "A framework that uses LLMs (ChatGPT) to produce prompts augmented with API calls to graph inference tools and fine-tunes causal LLMs to automatically call external graph tools during generation.",
            "citation_title": "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt",
            "mention_or_use": "mention",
            "representation_name": "prompt augmentation with tool-use API calls",
            "representation_description": "Instead of fully serializing graphs into text, Graph-ToolFormer augments prompts with explicit API call tokens that instruct the model to invoke external graph reasoning tools (e.g., shortest-path, connectivity) during generation.",
            "graph_type": "general graphs requiring algorithmic graph inference (e.g., path finding, centrality)",
            "conversion_method": "Create a large dataset of prompts containing graph reasoning API calls (annotated/expanded by ChatGPT), fine-tune causal LLMs to insert API calls appropriately in outputs to delegate computation.",
            "downstream_task": "complex graph reasoning tasks requiring precise computation and multi-step logic",
            "performance_metrics": "Survey describes that tool-augmented LLMs better address limitations like precise computation and multi-step logic; no single numeric metric provided in the survey entry.",
            "comparison_to_others": "Offloads precise graph computation to specialized tools instead of pure serialization; presented as addressing shortcomings of pure text encodings.",
            "advantages": "Mitigates LLM weaknesses in precise computation and topological reasoning; leverages external graph tools for accurate results.",
            "disadvantages": "Requires availability and integration of external graph tools and extra engineering; depends on correct tool selection and call placement.",
            "failure_cases": "If tool calls are not invoked correctly or tools are unavailable, performance degrades; tool integration increases system complexity and potential failure modes.",
            "uuid": "e8836.5",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MoleculeSTM / InstructMol",
            "name_full": "InstructMol: Multi-modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
            "brief_description": "A chemical/molecular alignment approach that aligns molecular graph structures with natural language via contrastive training and a lightweight alignment projector, enabling LLMs to process molecular graphs as text-aligned embeddings.",
            "citation_title": "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "mention_or_use": "mention",
            "representation_name": "contrastive structure-text alignment + alignment projector",
            "representation_description": "Encodes molecular graphs with a graph encoder, aligns graph and textual representations through contrastive training, and uses a lightweight projector to map graph features to LLM word embedding space.",
            "graph_type": "molecular graphs (chemical structures)",
            "conversion_method": "Learn joint embeddings via contrastive pretraining on molecule-text pairs, then use a lightweight projector to align graph embeddings into the LLM embedding space for downstream LLM tasks.",
            "downstream_task": "molecule retrieval, text-based molecular editing, zero-shot molecular property prediction and molecular-text retrieval",
            "performance_metrics": "Survey references MoleculeSTM (and related datasets like PubChemSTM) and states it improves retrieval/editing tasks; no explicit numeric metric quoted in the survey.",
            "comparison_to_others": "Compared to pure-text encodings and naive flattening, alignment via contrastive training provides better cross-modal retrieval and edit capabilities.",
            "advantages": "Enables cross-modal retrieval and editing; leverages structure-specific encoders for chemical detail while allowing language-model interaction.",
            "disadvantages": "Requires paired molecule-text datasets and contrastive pretraining; extra model components (projector) add complexity.",
            "failure_cases": "May fail when textual descriptions are low-quality or inconsistent with structural features; alignment may not generalize across very different molecular classes without broad pretraining.",
            "uuid": "e8836.6",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DGTL",
            "name_full": "Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs",
            "brief_description": "A hybrid approach that uses a frozen LLM to encode text attributes and custom disentangled GNN layers to capture neighborhood structure, ultimately converting graph neighborhood information into text-finetunable features.",
            "citation_title": "Disentangled representation learning with large language models for text-attributed graphs",
            "mention_or_use": "mention",
            "representation_name": "frozen-LLM text encoding + disentangled GNN -&gt; textual-friendly features",
            "representation_description": "Uses a frozen LLM to encode node textual attributes, then applies disentangled GNN layers to capture graph neighborhood information and produces features suitable for fine-tuning the LLM.",
            "graph_type": "text-attributed graphs (TAGs)",
            "conversion_method": "Encode raw textual node information with frozen LLM, disentangle neighborhood effects via custom GNN layers, and use the resulting features to fine-tune/facilitate LLM understanding of graph structure.",
            "downstream_task": "node classification and other TAG tasks",
            "performance_metrics": "Survey mentions DGTL as a method but does not provide numeric performance values in the main text.",
            "comparison_to_others": "Presented as an approach that blends frozen LLM text encodings with specialized GNN layers; contrasted with pure text flattening and pure GNN approaches for TAGs.",
            "advantages": "Preserves strengths of powerful frozen LLM encodings while explicitly modeling structural neighborhood with disentangled GNNs; reduces need to fine-tune full LLM.",
            "disadvantages": "Adds GNN complexity and extra training components; not fully end-to-end LLM-only.",
            "failure_cases": "May be limited if frozen LLM encodings lack alignment with structural signals or if disentangling fails on heterogeneous neighborhoods.",
            "uuid": "e8836.7",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MuseGraph",
            "name_full": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining",
            "brief_description": "Instruction tuning strategy that generates compact graph descriptions using neighbor nodes and random walks and creates task-based chain-of-thought instruction sets to fine-tune LLMs for graph tasks.",
            "citation_title": "Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining",
            "mention_or_use": "mention",
            "representation_name": "compact neighborhood/random-walk-based graph descriptions + CoT instruction sets",
            "representation_description": "Encodes graphs into compact textual descriptions by sampling neighbor nodes and random walks and pairs these with task-specific chain-of-thought instructions for instruction-tuning LLMs.",
            "graph_type": "general graphs / TAGs",
            "conversion_method": "Generate compact descriptions based on neighborhood sampling and random walks, then assemble task-based CoT instruction packs for fine-tuning LLMs; dynamically allocate instruction packages across tasks/datasets.",
            "downstream_task": "node classification, link prediction, graph-to-text generation, and other graph mining tasks",
            "performance_metrics": "Survey states MuseGraph improves LLM performance with dynamic instruction allocation but does not report specific numeric metrics in the main text.",
            "comparison_to_others": "Contrasted with naive flattening and with other instruction-tuning approaches; emphasizes compactness and task-adaptive instruction allocation.",
            "advantages": "Reduces prompt length via compact descriptions; tailors instruction units to datasets/tasks improving generalization.",
            "disadvantages": "Sampling/random-walk summarization may omit important global structure; design of instruction packs adds complexity.",
            "failure_cases": "May underperform on tasks requiring full global structure (e.g., long-range multi-hop reasoning) due to compacting strategies.",
            "uuid": "e8836.8",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GIMLET",
            "name_full": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning",
            "brief_description": "Fine-tunes LLMs with graph-text instruction data so the LLM directly outputs predictive labels from compact graph descriptions, focusing on molecular graphs and zero-shot molecule tasks.",
            "citation_title": "Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning",
            "mention_or_use": "mention",
            "representation_name": "instruction-finetuned compact graph descriptions",
            "representation_description": "Fine-tunes LLMs to accept compact graph descriptions and directly output predictive labels, using instruction-based datasets crafted for molecular tasks.",
            "graph_type": "molecular graphs",
            "conversion_method": "Serialize molecular neighborhood information into compact descriptions and fine-tune LLMs via instruction datasets to produce labels directly.",
            "downstream_task": "molecular property prediction (zero-shot), molecule classification",
            "performance_metrics": "Survey notes GIMLET fine-tunes LLMs to output predictive labels directly, improving predictive accuracy in molecular zero-shot tasks but provides no numeric figures in the survey.",
            "comparison_to_others": "Compared conceptually with other fine-tuning approaches like MuseGraph and InstructGLM; positioned as molecule-focused.",
            "advantages": "Enables direct LLM label output without intermediate parsing; targets zero-shot capability for molecules.",
            "disadvantages": "Requires curated instruction-label datasets; domain-specific (molecules) so less general.",
            "failure_cases": "May fail when molecular descriptions are insufficiently informative or when molecule classes are out-of-distribution for the instruction data.",
            "uuid": "e8836.9",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "InstructGLM",
            "name_full": "InstructGLM (Language is all a graph needs)",
            "brief_description": "An approach that designs highly scalable rule-based natural language prompts describing graph structures and fine-tunes LLMs with these instructions so LLMs can perform graph tasks.",
            "citation_title": "Language is all a graph needs",
            "mention_or_use": "mention",
            "representation_name": "rule-based natural language prompting for graphs",
            "representation_description": "Creates scalable, rule-based natural language prompt templates to describe graph topology and tasks, and fine-tunes LLMs on these prompts enabling the models to understand and process such descriptions.",
            "graph_type": "general graphs and text-attributed graphs",
            "conversion_method": "Apply rule-based templates to convert graph elements into natural language instructions/prompts; use these to fine-tune LLMs.",
            "downstream_task": "node classification and other graph tasks; InstructGLM reported on ogbn-arxiv node classification",
            "performance_metrics": "In survey Table 2, InstructGLM (Llama-7b) reports 75.70% on ogbn-arxiv node classification (as listed).",
            "comparison_to_others": "Compared in Table 2 to models like OFA, GraphGPT and GraphAdapter; performance is competitive though slightly below best reported in the table.",
            "advantages": "Highly scalable prompting rules; enables LLM fine-tuning without complex graph encoders; works across tasks via prompt templates.",
            "disadvantages": "Rule-based prompts may not capture complex structure; limited by prompt design and LLM capacity.",
            "failure_cases": "May degrade on very large or highly structured graphs due to prompt length and rule limitations.",
            "uuid": "e8836.10",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GraphTranslator",
            "name_full": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
            "brief_description": "A method that produces graph-text alignment data via a 'Producer' mechanism, enabling LLMs to predict graph data from language instructions by aligning graph representations to LLM space.",
            "citation_title": "Graphtranslator: Aligning graph model to large language model for open-ended tasks",
            "mention_or_use": "mention",
            "representation_name": "producer-based graph-text alignment",
            "representation_description": "Uses a Producer mechanism to generate aligned graph-text pairs for training, teaching LLMs to predict/understand graph data given natural language instructions.",
            "graph_type": "heterogeneous graphs / general graphs for open-ended tasks",
            "conversion_method": "Generate alignment datasets (graph, textual instruction pairs) via Producer; train alignment to allow LLMs to map language instructions to graph predictions.",
            "downstream_task": "open-ended graph tasks, graph prediction from language",
            "performance_metrics": "Survey mentions GraphTranslator as enabling LLM prediction of graph data; no numeric metrics provided in the survey text.",
            "comparison_to_others": "Positioned as an explicit alignment-data generator compared to ad-hoc flattening; complements instruction-tuning approaches.",
            "advantages": "Creates direct graph-text supervision enabling LLMs to learn graph semantics from language; useful for open-ended tasks.",
            "disadvantages": "Requires generation of high-quality aligned data; Producer quality crucial.",
            "failure_cases": "If Producer produces low-quality alignments, LLM predictions will be unreliable; may not scale without automation of high-quality pairing.",
            "uuid": "e8836.11",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GraphLLM",
            "name_full": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
            "brief_description": "An end-to-end approach that integrates a graph transformer with an LLM by generating graph-enhanced prefixes injected into LLM attention layers, thereby enabling structural information to inform language modeling.",
            "citation_title": "Graphllm: Boosting graph reasoning ability of large language model",
            "mention_or_use": "mention",
            "representation_name": "graph transformer -&gt; graph-enhanced prefix injection",
            "representation_description": "Uses a graph Transformer to learn graph structure and aggregate node representations; produces graph representations that are injected as prefixes into each LLM attention layer to condition LLM reasoning on structure.",
            "graph_type": "general graphs / text-attributed graphs",
            "conversion_method": "Textual transformer encoder-decoder extracts node descriptions, graph Transformer learns structure, aggregated graph representations become prefixes added to LLM attention at each layer.",
            "downstream_task": "graph inference, question answering over graphs, node classification",
            "performance_metrics": "Survey notes GraphLLM integrates graph transformer prefixes to improve graph inference but provides no explicit numeric metrics in the main text.",
            "comparison_to_others": "Distinguished from pure graph-&gt;text flattening by creating learned structural prefix injection; compared conceptually with other co-driving architectures.",
            "advantages": "Deep integration preserves structural signals across LLM layers; leverages complementary strengths of graph transformers and LLMs.",
            "disadvantages": "Increased model complexity and compute; requires joint training of graph transformer and LLM prefix layers.",
            "failure_cases": "May be computationally heavy; prefix injection may not fully capture large-scale global graph properties if prefix dimension or scope limited.",
            "uuid": "e8836.12",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt",
            "rating": 2,
            "sanitized_title": "graphtoolformer_to_empower_llms_with_graph_reasoning_ability_via_prompt_augmented_by_chatgpt"
        },
        {
            "paper_title": "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "rating": 1,
            "sanitized_title": "instructmol_multimodal_integration_for_building_a_versatile_and_reliable_molecular_assistant_in_drug_discovery"
        },
        {
            "paper_title": "Disentangled representation learning with large language models for text-attributed graphs",
            "rating": 1,
            "sanitized_title": "disentangled_representation_learning_with_large_language_models_for_textattributed_graphs"
        },
        {
            "paper_title": "Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining",
            "rating": 1,
            "sanitized_title": "musegraph_graphoriented_instruction_tuning_of_large_language_models_for_generic_graph_mining"
        },
        {
            "paper_title": "Language is all a graph needs",
            "rating": 1,
            "sanitized_title": "language_is_all_a_graph_needs"
        },
        {
            "paper_title": "Graphllm: Boosting graph reasoning ability of large language model",
            "rating": 1,
            "sanitized_title": "graphllm_boosting_graph_reasoning_ability_of_large_language_model"
        }
    ],
    "cost": 0.01967675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining</h1>
<p>Yuxin You<br>School of Computer Science and Engineering<br>University of Electronic Science and Technology of China<br>Chengdu 611731, China<br>202321081209@std.uestc.edu.cn</p>
<p>Zhen Liu<br>School of Computer Science and Engineering<br>University of Electronic Science and Technology of China<br>Chengdu 611731, China<br>quake@uestc.edu.cn</p>
<h2>Xiangchao Wen</h2>
<p>School of Computer Science and Engineering
University of Electronic Science and Technology of China
Chengdu 611731, China
cha0s101cs@gmail.com</p>
<h2>Yongtao Zhang</h2>
<p>School of Computer Science and Engineering
University of Electronic Science and Technology of China
Chengdu 611731, China
202222080730@std.uestc.edu.cn</p>
<h2>Wei Ai</h2>
<p>54th Research Institute of CETC Shijiazhuang 050081, China
aiwei2001@sina.com</p>
<h2>Abstract</h2>
<p>Graph mining is an important area in data mining and machine learning that involves extracting valuable information from graph-structured data. In recent years, significant progress has been made in this field through the development of graph neural networks (GNNs). However, GNNs are still deficient in generalizing to diverse graph data. Aiming to this issue, Large Language Models (LLMs) could provide new solutions for graph mining tasks with their superior semantic understanding. In this review, we systematically review the combination and application techniques of LLMs and GNNs and present a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving. Within this framework, we reveal the capabilities of LLMs in enhancing graph feature extraction as well as improving the effectiveness of downstream tasks such as node classification, link prediction, and community detection. Although LLMs have demonstrated their great potential in handling graphstructured data, their high computational requirements and complexity remain challenges. Future research needs to continue to explore how to efficiently fuse LLMs and GNNs to achieve more powerful graph learning and reasoning capabilities and provide new impetus for the development of graph mining techniques.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Based on the relationship between LLMs and GNNs, we categorize the application scenarios of LLMs in graph mining into three groups: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving. These models are capable of handling a variety of datasets and achieving success in various downstream tasks.</p>
<h1>1 Introduction</h1>
<p>Graphs are data structures used to describe relationships between objects, and they are widely used in many domains, such as social networks [1], computer networks, and molecular structures. Some of these graphs contain hundreds of millions of node information, but the vast majority are redundant and irrelevant. What graph mining investigates is how to extract relevant or valuable knowledge and information from scaled graph data by using graph mining models.
Over the past decade, graph mining techniques have evolved, yielding many significant results and significantly contributing to the development of the field. Early research was inspired by the word vector technique [2], and Perozzi et al. [3] firstly proposed the random walk-based graph embedding method DeepWalk; Node2Vec [4] proposed by Grover and Leskovec et al. generates node embeddings through biased random walk strategies for tasks such as node classification and link prediction, and Graph2Vec [5], on the other hand, embeds the entire graph into the vector space for graph classification tasks. For more efficient graph representation learning, the Graph Neural Networks (GNNs) [6]proposed by Kipf and Welling pioneered a new research paradigm. GNNs efficiently capture and collect structural information and dependencies in graph-structured data through information propagation and aggregation mechanisms, which enable the model to make precise predictions in complex graph structures. In recent years, various GNN architectures have been developed in information propagation and aggregation methods. For example, Graph Convolutional Network (GCN) [7] processes graph data through spectral convolution, which is widely used in node classification and graph classification tasks; Graph Attention Network (GAT) [8] introduces an attention mechanism to adaptively assign the importance of different neighboring nodes, which enhances the expressive power of the model; GraphSAGE [9] can efficiently process large-scale graph data by sampling and aggregating features from neighboring nodes. In addition, for heterogeneous graph analysis, the Heterogeneous Graph Attention Network (HAN) [10] proposed by Wang et al. aggregates information and learns between different types of nodes and edges through the attention mechanism, while Zhang et al.'s Heterogeneous Graph Neural Network (HetGNN) [11] learns embeddings by sampling various types of nodes and edges, which enables it to manage complex heterogeneous graph structures. These results not only demonstrate the prospect of wide application of graph mining techniques but also promote the development of related research fields.
In 2022, the emergence of Large Language Models (LLMs), represented by ChatGPT [12], revolutionized the field of Natural Language Processing (NLP) as well as the domain of artificial intelligence research. The large language model is pre-trained on a large amount of text, which can learn rich language expressions and huge amount of real-world</p>
<p>knowledge, and has excellent semantic understanding capabilities [13]. For example, BERT [14] uses a bidirectional attention mechanism to capture the contextual information of the text, which enables the model to perform on various NLP tasks such as question answering, named entity recognition, and sentence classification. GPT-3 [15], with 175 billion parameters, is pre-trained by an autoregressive approach and is capable of performing a wide range of tasks from text generation and translation to dialog systems. Its strength lies in its ability to perform new tasks without fine-tuning and with a small number of examples or hints, demonstrating impressive zero-shot and few-shot learning capabilities. These LLMs can be applied to various downstream tasks with little additional training. Although large language models were initially developed for natural language processing, researchers have also been exploring the application of LLMs on multimodal data in recent years. For instance, the DALL-E [16] model that combines image and text can generate corresponding images based on the text by performing self-supervised learning on paired image-text corpora. These multimodal models demonstrate outstanding potential for processing and understanding different data types.
Given the disruptive potential of large models, many researchers in graph mining have focused on them in the last two years, expecting them to bring new developments to the field of graph mining. Zhang et al. [17] discusses the challenges and opportunities presented by the combination of graphs and LLMs and showcases the potential of these models across various application domains. Chen et al. [18] explores the potential of utilizing LLMs in graph learning tasks and investigates two possible approaches: LLMs-as-Enhancers and LLMs-as-Predictors. Liu et al. [19] introduced the concept of Graph Foundation Models (GFMs) and provided a taxonomy and review of existing work related to GFMs. These studies suggest that integrating LLMs with graph neural networks can enhance various downstream graph mining tasks. The reason is that, although GNNs excel at capturing structural information, they have limitations in terms of expressiveness and generalizability[20], and their semantically constrained embeddings often fail to characterize the node features for complex node information fully. On the contrary, LLMs are good at processing complex texts but are often deficient in structural information processing. Combining the strengths of both can significantly improve the accuracy of graph mining.
To the best of our knowledge, existing reviews on the application of large models in graph mining mainly focus on categorizing LLMs simply as enhancers or predictors [18]. However, such a categorization approach is only a simple and superficial combination of LLMs and GNNs considered as independent models. It fails to delve into the profound fusion potential of LLMs and GNNs in the graph mining domain. Therefore, we propose a new classification framework, as shown in Figure 1, based on the main driving components in graph mining models, which are categorized into three groups: GNN-driving-LLMs, LLM-driving-GNNs, and GNN-LLM-co-driving. In this new classification framework, the GNN-driving-LLM mode emphasizes the GNN as the central task processing module, and the LLM plays an assisting role in specific tasks or scenarios, such as natural language interpretation or feature extraction. On the contrary, the LLM-driving-GNN mode places the LLM at the core and the GNN as an auxiliary tool for processing and guiding the graph-structured data to enhance the model's performance in complex graph data. In the GNN-LLM-co-driving mode, on the other hand, GNN and LLM work closely together to form a kind of interdependent joint model that collaboratively solves the corresponding graph mining tasks. Such a classification not only helps to fully understand the deep integration of LLMs and GNNs but also provides new ideas for future research directions.</p>
<h1>2 preliminary</h1>
<h3>2.1 Graph Mining</h3>
<p>Graph mining tasks are important in data knowledge discovery, aiming to extract valuable information from graphstructured data. Graph-structured data consists of nodes and edges, where nodes represent entities and edges represent relationships between represented entities. It is widely applied in fields such as social network analysis [21], recommender systems [22], chemistry [23] and knowledge graphs [24].
Common graph mining tasks include node classification, link prediction, graph clustering, graph matching, community detection, frequent subgraph mining, etc. Node classification utilizes labeled nodes as training data and predicts the classes of unlabeled nodes in the graph, such as node embeddings based on random walks [4], Graph Convolutional Networks (GCN) [7]. Link prediction is used to predict potential future edges, with important applications in recommender systems (e.g., friend recommendation, item recommendation). Common link prediction algorithms include neighborhood-based algorithms [25], path-dependent models [26], deep learning methods [27], and so on. Graph clustering [28] and graph matching [29] are respectively concerned with the grouping of nodes and subgraph correspondence between different graphs. Graph clustering groups graph nodes such that nodes within the same group are more tightly connected, while graph matching finds corresponding subgraphs between different graphs, which is commonly used in chemical molecular structure comparison and pattern recognition. Community detection [30] identifies subsets or associations in the graph, where nodes within a community are closely connected but have relatively few connections to nodes outside the community; commonly used algorithms such as the Girvan-Newman algorithm</p>
<p>[31], Louvain's method [32] and so on. Frequent subgraph mining [33] spots frequently occurring subgraphs from a set of graphs, which can be used in chemoinformatics to discover common molecular structures. By analyzing a large number of molecular diagrams, frequent subgraph mining can reveal which molecular fragments appear repeatedly across multiple compounds, providing valuable insights for fields such as bioinformatics and chemoinformatics [34].</p>
<h1>2.2 Large Language Model</h1>
<p>A large language model consists of a neural network with many parameters (usually billions of weights or more). In recent years, thanks to the introduction of the Transformer architecture and its ability to be pre-trained on large-scale text data, it has made significant progress in natural language processing.
The transformer model [35] proposed by Vaswani et al. in 2017 is based on the attention mechanism, which can greatly improve training efficiency and performance. Its fundamental unit is the multi-head self-attention mechanism, which can be expressed by the following formula:</p>
<p>$$
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>Where $Q, K$, and $V$ denote the Query, Key, and Value matrices, respectively, and $d_{k}$ is the dimension of the key vector. The multiple head mechanism further computes several different attention values and combines them:</p>
<p>$$
\operatorname{MultiHead}(Q, K, V)=\operatorname{Concat}\left(\operatorname{head}<em 2="2">{1}, \operatorname{head}</em>
$$}, \ldots, \operatorname{head}_{h}\right) W^{O</p>
<p>The types and applications of LLMs are constantly evolving with the development of architectures and training methods. Current LLMs can be classified into the categories of autoregressive models, masked language models, encoder-decoder models, contrastive learning models, multimodal models, and others. Autoregressive models generate text by predicting the next word in a sequence, and such models can generate coherent text and perform very well in generative tasks such as GPT [36], GPT-2 [37], GPT-3 [15] and GPT-4 [38].On the other hand, Masked language models typically mask the words in a given utterance to train the model to predict these masked words and thus learn the contextual representation. Representative models of masked language models include BERT [14], RoBERTa [39] and XLNet [40]. The encoder-decoder models achieve a high degree of parallelization and dramatically improve computational efficiency by encoding the input text into a contextual representation and decoding it to generate the target text. Classic models of this type include BART [41], and T5 [42].Contrastive learning models, such as SimCLR [43], train the model to differentiate between positive and negative samples by constructing pairs of positive and negative samples so that the model can capture relevant features and similarities in the data. Finally, multimodal models are able to process information from multiple modalities (including images, videos, and texts) to enhance the performance of the model in multimodal tasks, such as CLIP [44] and DALL-E [16]. The continuous development of large language models not only enhances the ability of computers in natural language processing but also provides new ideas and methods for solving broader and more complex data analysis and processing tasks.</p>
<h2>3 Techniques of the LLMs combined with GNNs</h2>
<p>Regarding techniques of the LLMs combined with GNNs, as shown in Figure 2, we propose a novel taxonomy including GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.</p>
<h3>3.1 GNN-driving-LLM</h3>
<p>In graph mining, GNNs are a class of deep learning models specialized in processing graph data. By combining the structural information and node features of the graph, they are able to effectively learn the representation of nodes, edges, and the overall graph in a graph. Text-attributed graphs (TAGs), in which the attributes of nodes exist in the form of text, are ubiquitous in the research of graph machine learning, such as product networks [91], social networks [1, 92] and citation networks [93, 94], where textual attributes provide key semantic information for the graph mining task. Therefore, when dealing with such graphs, the structure of the graph, the textual information, and their interrelationships must be considered simultaneously.
Traditionally, the processing of node text attributes often relies on shallow embedding methods, such as the classic Cora dataset [95], which only provides embedding features based on bag-of-words models. This approach is coarse in semantic understanding, leading to the limited performance of GNNs in processing textual attribute graphs. However, with the development of LLMs, their powerful text processing and semantic understanding capabilities provide a new solution to this problem. LLMs can extract richer semantic features from the textual attributes of nodes and generate additional auxiliary information, such as attribute interpretations and pseudo-labels, which provide more resounding</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The proposed taxonomy on LLM-GNN-combined techniques.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: GNN-driving-LLM: <strong>a)</strong> LLMs generate additional information for the textual attributes of the nodes. These features extracted by LLMs are fed into the next layer of the language model to generate enhanced node embeddings, which are finally processed by GNNs for downstream tasks; <strong>b)</strong> the textual embeddings exported by LLMs can be directly used as the initial node embeddings for GNNs.</p>
<p>Semantic support for node embedding in graph mining tasks. In previous research [53, 54], techniques combining language model (LM) and GNNs have been investigated to use LMs for encoding and providing them as node features to GNNs, and the introduction of extensive language modeling further enhances the effectiveness of this approach. In the GNN-driving-LLM model, GNNs are still used as the central information processing unit, but the text attributes are deeply parsed and embedded by incorporating LLMs, which makes the GNNs more accurate in capturing the semantic information of the nodes and thus improves the performance of the model in the downstream tasks. As shown in Figure 3(a), LLMs generate additional information for the textual attributes of the nodes. Next, these features extracted by LLMs are fed into the next layer of the language model to generate enhanced node embeddings. A typical example is the TAPE [47] model, which first predicts the node text (such as paper titles and abstracts) in LLMs and generates the corresponding interpretations. Through task-specific prompts, LLMs generate categorization predictions and explanation texts. The interpreted text generated by LLMs is then fed into a smaller language model, processed through fine-tuning and transformed into node features (including original text features<strong>h</strong><em expl="expl">{orig}, explanation features<strong>h</strong></em> and prediction features<strong>h</strong><em TAPE="TAPE">{pred}). The combination of these three classes of features as <strong>h</strong></em> is used for downstream</p>
<p>GNNs training. Finally, the GNN models are trained on the generated features to achieve the node classification task. The following formula can describe the process:</p>
<p>$\mathbf{s}=\operatorname{LLM}(x,p), \quad \mathbf{h}=\operatorname{LM}(\mathbf{s}, p) \in \mathbb{R}^{N \times d}, \quad \hat{\mathbf{y}}=\operatorname{GNN}(\mathbf{h}, \mathbf{s}, A) \in \mathbb{R}^{N \times C}, \quad$ (3)</p>
<p>where the LLMs accept a raw text $x=\left(x_{1}, x_{2}, \ldots, x_{q}\right)$ as input and generates a sequence of interpreted text $s=$ $\left(s_{1}, s_{2}, \ldots, s_{m}\right)$ as output, with $p$ is the cue for the LLMs. $\mathbf{h} \in \mathbb{R}^{N \times d}$ is the output of LMs, which is the node embedding matrix augmented by LLMs, and $A \in \mathbb{R}^{N \times N}$ is the adjacency matrix of the graph. Several studies have shown that this strategy of combining LLMs and GNNs has significant advantages in practical applications. For example, LLMRec [45] is the first work on graph enhancement using LLMs by augmenting user-item interaction edges, item node attributes, and user node profiles, which effectively solves the problems of data sparsity and low-quality auxiliary information in recommender systems. Similarly, RLMRec [46] also utilizes LLMs to enhance representation learning in existing recommender systems, which uses LLMs to respectively process textual information of items, as well as user interaction information and textual attributes of related items, to generate item profiles and user profiles. By maximizing mutual information, RLMRec aligns the semantic representations from LLMs with collaborative relationship representations, significantly improving the performance of the recommendation system. PRODIGY [48] is a framework for pre-training context learners on prompt graphs. It leverages the powerful zero-sample capability of LLMs to encode textual information of graph nodes, enabling context learning on graphs. Inspired by the remarkable effectiveness of prompt learning in NLP, ALL-in-one [49] proposes a new multitask prompting approach that unifies graph prompts and language prompts formats by prompt tokens, token structures, and insertion patterns. It enables the NLP prompting concepts to be seamlessly introduced into the graph domain, thus unifying the formats of graph prompting and language prompting.
When using embeddable or open-source LLMs, the text embeddings they generate can be accessed directly. In this case, the LLMs first extract textual features for each node, which are then fed into the GNNs as initial node embeddings. Subsequently, the GNNs combine the graph structure information with these augmented node embeddings through their message passing and feature aggregation mechanisms, as shown in Figure 3(b). In general, the process can be described by the following equation:</p>
<p>$$
\mathbf{h}=\operatorname{LLM}(\mathbf{s}), \quad \hat{\mathbf{y}}=\operatorname{GNN}(\mathbf{h}, A)
$$</p>
<p>This approach significantly improves the performance of graph mining tasks, especially in downstream tasks such as node classification, link prediction, and graph classification. OFA [50] embeds textual descriptions of graph datasets from different domains into the same feature space, thus becoming the first cross-domain generalized graph classification model. GaLM [51] pretrains on large-scale heterogeneous graphs and incorporates structural information into the fine-tuning stage of LLM to generate higher quality node embeddings, thus improving performance for downstream applications in different graph patterns. LEADING [52] is an end-to-end fine-tuning algorithm that significantly improves LLMs' computational and data efficiency in processing TAGs by reducing coding redundancy and propagation redundancy. GraphEdit [55] enhances LLMs' reasoning ability on node relationships in graph data through instruction tuning to solve the noise and sparsity problems in graph structures. Specifically, GraphEdit first uses LLMs to generate semantic embeddings of nodes and filter candidate edges by a lightweight edge predictor; then, in combination with the original graph structure, it uses the inference ability of LLMs to optimize edge additions and deletions and generates the improved graph structure. Eventually, the optimized graph structure is used for GNNs training to support downstream tasks such as node classification, thus realizing denoising and global dependency mining of the graph structure.
With their exceptional ability to process text sequences, LLMs perform excellently in handling TAGs. They can extract deep semantic information from textual attributes, providing richer feature representations than traditional methods. In addition, rather than traditional GNNs that need to design different architectures for different datasets, combining LLMs and GNNs can process data from different domains by unifying the feature space and cross-domain embedding methods. This approach, which combines the diversity of language models and the ability to understand the structure of graph neural networks, offers flexibility in dealing with complex attributes and structures.
Beyond that, LLMs can also improve the performance of downstream tasks through data augmentation. For example, LLM-GNN[57] achieves efficient and low-cost label-free node classification through LLM-based zero-shot labeling and GNN-based extended learning. LLM4NG [56] represents a typical application of graph generation learning. It utilizes a large language model to generate new nodes and integrates these nodes into the original graph structure via an edge predictor to generate a new graph structure. This approach significantly improves model performance in few-shot scenarios, demonstrating the effectiveness of augmenting model learning capabilities by generating samples. Similarly, OpenGraph [58] uses LLMs for generating synthetic graph data (e.g., nodes and edges) as well as augmenting the pre-training data. It also employs a unified graph tokenizer and an efficient graph Transformer, achieving excellent generalization on multi-domain graph data in zero-shot learning.
Despite the fact that LLMs can enhance model performance, they are extremely demanding of computational resources, especially when processing large amounts of textual data, requiring significant computational resources or frequent</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: LLM-driving-GNN: Models use spreading functions or GNNs to transform graph data into sequence text so that LLMs can directly understand the graph data and accomplish downstream tasks.</p>
<p>API query calls to the LLMs, which comes with a high cost. Moreover, although LLMs provide rich semantic information, their decision-making process is often opaque and lacks interpretability. Therefore, how to efficiently fuse text embedding with graph structure information in the framework of GNNs remains a challenging issue that requires further research.</p>
<h1>3.2 LLM-driving-GNN</h1>
<p>In some graph mining tasks, LLMs, with their powerful zero-shot learning capabilities, can directly perform prediction, classification, or reasoning. The computational power of LLMs and the advantages of deep learning algorithms enable them to perform well in these tasks. However, since LLMs can only accept sequential text as input, an additional step is required for graph data: transforming the graph data, which has been defined in various ways in terms of structure and features, into sequential text directly fed into LLMs. In recent years, many studies have explored the applicability of LLMs for tasks downstream of graph structures, yielding some preliminary results [96, 97, 98]. These results suggest that LLMs have achieved initial success in processing implicit graph structures. Wang et al.[63] proposed a synthetic benchmark, NLGraph, for evaluating the performance of LLMs in the task of reasoning about graph structures. It was found that the capability of LLMs has been demonstrated in simple graph reasoning tasks but is still insufficient when dealing with complex problems.Additionally, several studies have explored the capability of LLMs in processing graph-structured data and their effectiveness in extracting and utilizing graph structural information [99, 100]. These studies have opened new avenues for exploiting the capabilities of LLMs in graph applications and further exploring the incorporation of structured data into LLMs frameworks. A growing number of researchers are committed to exploring how to better apply LLMs to downstream tasks containing graph structures and further improve their performance.</p>
<p>Typically, to enable LLMs to understand graph data, researchers use specific methods to convert graph data into textual descriptions, which are then used as inputs to LLMs and finally extract predictions from the output of the model. Among these methods, flattening functions can directly convert graphs into textual descriptions, which is more convenient and intuitive, while GNNs have demonstrated excellent ability in understanding graph structures through information propagation and aggregation among nodes. Therefore, researchers have also explored using GNNs to transform graph data with different structures and features into sequential text to fully leverage the structural information in the graph data for prediction. As shown in Figure 4, the method of encoding graph structure data into text for use in LLMs was first comprehensively investigated by Fatemi et al. [60], and can be described by the following equation:</p>
<p>$$
\text { Describe }=f(G, \text { Attr }), \quad \mathbf{A}=\operatorname{LLM}(\text { Describe }, p)
$$</p>
<p>where $f$ represents the flattening function or GNNs, which inputs the graph $G$ and the text attributes Attr on each node or edge of the graph to get the text description, i.e., Describe, to be fed to the LLMs. $p$ denotes the prompts, and $\mathbf{A}$ is the answers given by the LLMs, from which the predicted labels of downstream tasks can be extracted in a specific way. Research has shown that choosing a suitable graph encoding method can significantly improve the performance of LLMs in graph reasoning tasks. Thus, one of the current research priorities is to explore suitable graph encoding methods. For example, GPT4Graph [61] converts graph data into a Graph Description Language (GDL) like GraphML [101]. It generates prompts in conjunction with user queries so that the large language model can understand and process the graph-structured data. GraphText [59] constructs a graph syntax tree from graph data and generates graph prompts through traversal of the tree, expressing them in natural language so that LLMs can treat graph reasoning as a text generation task. An alternative approach is to record the graphical data directly in natural language, i.e., to describe the graphical data with a digitally organized list of nodes and edges [64]. GraphGPT [65] also utilizes Chain-of-Thought (CoT) [102] to augment the model's reasoning capabilities. However, in terms of transforming graph data, GraphGPT trains a lightweight graph-text projector that is able to align representations between text and graph structure, allowing the model to switch seamlessly during processing. Similarly, MoleculeSTM [66] uses a graph</p>
<p>encoder as a molecular graph structure encoder. The method enables large language models to align molecular structures to natural language by aligning molecular graphs and textual representations through contrastive training and then employing a lightweight alignment projector to map graph features into the word embedding space. DGTL [62] encodes the raw textual information in TAGs using a frozen LLM and then captures the graph neighborhood information in TAGs by a set of custom disentangled graph neural network layers. Finally, the features learned from these disentangled layers are used to fine-tune the LLMs to help the model better understand the complex graph structure information in the TAGs, thereby improving the final prediction ability of the LLMs. On the other hand, GraphTranslator [67] proposes a mechanism called Producer for creating graph-text alignment data, which enables a large language model to predict graph data based on language instructions. HiGPT [68] adapts to diverse heterogeneous graph learning tasks without downstream fine-tuning through a heterogeneous graph instruction-tuning paradigm.</p>
<p>Another way to enhance the capability of LLMs on graph-structured data is by fine-tuning LLMs to enhance their graph mining capabilities. Several researchers have explored this area and made notable progress. GIMLET [69] fine-tunes LLMs to output predictive labels directly, thus providing accurate predictions without additional parsing steps. MuseGraph [70] generates compact graph descriptions using neighbor nodes and random walks and creates task-based CoT instruction sets to fine-tune the large language model. The method dynamically allocates instruction packages between tasks and datasets to ensure the effectiveness and generalization of the training process. Eventually, the graph structure data is converted into a format suitable for LLMs, which allows the fine-tuned model to fit downstream tasks such as node classification, link prediction, and graph-to-text generation. InstructGLM [71] designed a series of rule-based, highly scalable natural language prompts for describing graph structures and performing graph tasks, and fine-tunes large language models with these instructions, enabling them to understand and process these descriptions to perform graph tasks. GraphLLM [72] adopts an end-to-end approach that integrates a graph learning module (graph transformer) with LLMs. Specifically, the approach uses a textual Transformer encoder-decoder to extract the necessary information from the node descriptions, learns the graph structure through the graph Transformer, and generates overall graph representations by aggregating the node representations. Ultimately, these graph representations are used to generate graph-enhanced prefixes injected in each LLM attention layer. This approach allows the LLM to work synergistically with the graph Transformer to incorporate structural information critical for graph inference and thus improve performance on graph reasoning tasks.</p>
<p>Unlike the above approaches, the Graph-ToolFormer framework [73] uses API calls to invoke external graph inference tools to complete reasoning tasks. First, ChatGPT is utilized to annotate and expand the manually written graph reasoning task prompts to generate a large dataset of prompts containing graph reasoning API calls. Then, the generated dataset is used to fine-tune pre-trained causal LLMs (e.g., GPT-J [103] and LLaMA [104]) and teach them how to use external graph inference tools in the generated output. Finally, the fine-tuned Graph-ToolFormer models are able to automatically add the corresponding graph reasoning API calls to the output statements when they receive input queries and questions. Through the above steps, the Graph-ToolFormer framework realizes the ability to empower existing LLMs to handle complex graph reasoning tasks, effectively addressing the current limitations of LLMs in handling precise computation, multi-step logical reasoning, spatial and topological awareness, etc. LLMs can also be used to generate new GNN architectures. Wang et al. [74] proposed a novel graph neural network architecture search method, GPT4GNAS, which guides GPT-4 to understand the search space and search strategies of GNAS by designing a new type of prompt. These prompts are iteratively run to generate new GNN architectures, and the evaluation results are used as feedback to optimize the generated architectures further. ChatRule [75] utilizes LLMs to mine logical rules as well as learn and represent graph structures by combining semantic and structural information from Knowledge Graphs (KGs), helping encode and process graph structures. These rules can be regarded as new graph structures to construct new knowledge graphs by capturing the generative rules and patterns of graphs. Meanwhile, GNP [76] extracts valuable knowledge from knowledge graphs, and through graph neural network encoding, cross-modal pooling, and self-supervised learning, it significantly improves the performance of LLMs in common-sense reasoning and biomedical reasoning tasks.</p>
<p>In conclusion, when LLMs dominate downstream tasks such as prediction, classification, and reasoning, they demonstrate significant advantages over traditional GNNs, especially in zero-sample learning and processing textual attributes. LLMs can utilize their powerful text generation and comprehension capabilities to predict and classify the graph data directly without the complex structural processing required by GNNs. However, since every coin has two sides, each approach requires careful trade-offs of the advantages and disadvantages between LLMs and GNNs. Converting graph data into textual descriptions can simplify the processing flow and enable LLMs to utilize their textual processing capabilities for reasoning directly. Nevertheless, due to the input length limitation, this approach may result in the loss of graph structure information, and the text conversion process is so complex that it is unsuitable for processing complex graph data. On the other hand, Combining GNNs can fully utilize the information in the graph structure and enhance the processing capability of the model. However, this integration method increases the complexity of the system. Effective integration of GNNs with LLMs requires careful design and tuning to ensure that both can effectively</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: a) Text encoding and graph aggregation are iteratively executed through hierarchical Graph Neural Networks (GNN) and Transformers (TRM); b) Data from these two different modalities are projected into an aligned semantic embedding space, where attention mechanisms are used to learn the correlation rules between molecular substructures and textual keywords.
work together to handle complex graph data. To sum up, how to efficiently combine graph structural information with LLMs for more powerful graph learning and reasoning is the core of research in this area.</p>
<h1>3.3 GNN-LLM-co-driving</h1>
<p>In GNN-driving-LLM, LLMs act as the preprocessors of graph data, primarily converting the textual attributes of the graph into rich feature representations through their powerful text comprehension and generation capabilities and then passing these representations to GNNs for further structural processing. In this setup, LLMs play a role in information extraction and feature enhancement. In contrast, in LLM-driving-GNN, GNNs are mainly responsible for processing graph structural information and utilizing this structural information to enhance the prediction, classification, reasoning, and generation capabilities of LLMs.
GNN-LLM-co-driving synthesizes the strengths of GNNs and LLMs, leveraging their collaboration to solve complex tasks. Different from GNN-driving-LLM and LLM-driving-GNN, the co-driving strategy emphasizes the deep interaction and complementarity between GNNs and LLMs. In this architecture, GNNs and LLMs co-drive the learning process of the model, alternating and complementing each other. GNNs, with their advantage in graph structural information, help LLMs generate semantically deeper features under complex structures; at the same time, LLMs provide GNNs with more accurate node and edge representations through their strong ability in text sequence processing. This bidirectional interaction not only improves the model's comprehensive processing capability on graph structures and textual information but also enhances the robustness and generalization of the overall model, which can better cope with diverse tasks and datasets.</p>
<p>Typical co-driving models are the GraphFormers framework proposed by Yang et al. [77], as shown in figure 5 a). It creates a unified architecture capable of processing textual and graph structural information simultaneously by embedding GNNs into each transformer layer. The graph is first aggregated in each layer through GNN, bringing neighborhood information to the central node. Then, the enhanced node features are processed by the transformer to generate richer node representations. This allows the text encoding and graph aggregation processes to alternate in the same workflow, allowing nodes to exchange information at each layer to enhance the node representations with the information from neighboring nodes. PATTON [78] adopts the GraphFormers framework and designs two pretraining strategies-Network Contextualized Masked Language Modeling (NMLM) and Masked Node Prediction (MNP)based on it. By jointly optimizing the two objective functions NMLM and MNP to carry out the pretraining process, this approach enhances the model's semantic understanding at both the vocabulary and document levels, enabling the pre-trained language model to sufficiently understand and represent the complex semantic information in rich textual networks. Zhao et al. [79] propose the GLEM model, which combines GNNs and LLMs and alternately updates LLMs and GNNs to generate pseudo-labels with each other through a Variational Expectation-Maximization (EM) framework. By integrating textual semantics and graph structural information, GLEM can effectively perform node representation learning on large-scale text-attributed graphs and improve node classification performance. GREASELM [80] uses a modality interaction mechanism to facilitate bidirectional information transfer between each layer of the LM and the</p>
<p>GNN, deeply integrating language context with knowledge graph representations to realize joint reasoning to answer complex questions.</p>
<p>Unlike the aforementioned architectures that combine GNN and LLM models, Text2Mol [81] employs two independent encoders for textual and molecular representations, respectively. By projecting the data from these two modalities into an aligned semantic embedding space, the model achieves a cross-modal search for retrieving molecules from natural language descriptions. To enhance the interpretability of the model, the Text2Mol framework introduces a cross-modal attention model based on the Transformer decoder. The model utilizes the output of SciBERT [105] as the source sequence and the node representations generated by the GCN model as the target sequence, learning the association rules between molecule substructures and text keywords through an attention mechanism. The architecture is shown in figure 5 b). Similarly, methods such as MoleculeSTM [82], CLAMP [83], ConGraT [84], G2P2 [85], GRENADE [86] also employ independent GNN encoders and LLMs encoders to process molecular and textual data separately, and then map the embedded representations to a shared joint representation space for contrastive learning. However, these models differ in implementation details or in the specific application scenarios they are applicable to. MoleculeSTM focuses on solving new challenges in drug design, such as structure-text retrieval and text-based molecular editing, and constructs PubChemSTM, a multimodal dataset containing a large number of chemical structure-text pairs. CLAMP, on the other hand, is primarily dedicated to zero-sample bio-activity prediction. Pretraining on large-scale chemical databases (such as PubChem) containing molecular structures, text descriptions, and bioactivity measurement data significantly enhances the generalization ability of activity prediction models. ConGraT connects an adapter module after each of the text encoder and graph node encoders, respectively, which consists of two fully connected layers to generate text embeddings and graph node embeddings of the same dimension. G2P2 and GRENADE take a further step by employing contrastive learning strategies. G2P2 enhances the granularity of contrastive learning by jointly training its graph encoder and text encoder with three graph-based contrast strategies (text-node interaction, text-summary interaction, and node-summary interaction) during the pretraining phase to jointly train the graph encoder and text encoder. This allows for the alignment of graph node and text representations in a bimodal embedding space, enabling better capture of fine-grained semantic information in the text while leveraging graph structures to enhance classification model performance. On the other hand, GRENADE jointly optimizes the pre-trained language model encoder and the graph neural network encoder through two self-supervised learning algorithms including graph-centered contrastive learning [106] and graph-centered knowledge alignment.
In order to achieve efficient node classification on textual graphs, GraD [87] employs the concept of knowledge distillation. The core idea is to transfer the graph structure information from the GNN teacher model to the graph-free student model through the distillation process. The student model does not need to use the graph structure during reasoning, thus significantly improving the reasoning efficiency and realizing efficient and accurate node classification. In order to introduce between models with different coupling strengths and flexibility, GraD also proposes three different optimization strategies, namely GraD-Joint, GraD-Alt, and GraD-JKD. Similarly, the THLM [88] framework combines BERT with the heterogeneous graph neural network R-HGNN [107], and through topology-aware pretraining tasks and text augmentation strategies, it pretrains on Text-Attributed Heterogeneous Graphs (TAHGs). After the pretraining phase, the THLM framework retains only the language model for downstream tasks and no longer relies on the auxiliary HGNN, ensuring efficiency and flexibility in processing downstream tasks. Some studies focus on combining LLMs and GNNs in the context of TAGs to reduce training complexity and memory consumption while maintaining the model's expressiveness. GraphAdapter [89] combines GNNs and LLMs specifically for processing TAGs. It firstly uses GNNs for each node in the TAG to model its structural information, then integrates the structural information with context-hidden states of LLMs, and finally transforms the original task into a next-word prediction task by adding task-specific prompts. Its lightweight design, residual connections, and task-related prompts enable the method to exhibit high performance in various downstream tasks, validating its effectiveness in TAG modeling. ENGINE [90] through a tunable bypass structure-G-Ladder-combining LLMs and GNNs for efficient fine-tuning and reasoning on TAGs. The framework significantly reduces memory and computational costs while preserving structural information through the introduced lightweight G-Ladder structure that adds tunable parameters next to each layer of LLMs. To further improve efficiency, a caching mechanism is also introduced to precompute the node embeddings during the training process, and dynamic early stop is used to accelerate model inference during the reasoning process.
Compared with GNN-driving-LLM and LLM-driving-GNN, the GNN-LLM-co-driving mode emphasizes the deep interaction and complementarity between the GNNs and LLMs. In this mode, GNN and LLM alternate and enhance each other in the learning process, thereby demonstrating higher robustness and generalization in the integrated processing of graph structure and text information. This co-driving strategy can solve complex tasks efficiently by integrating the graph structure processing capability of GNNs and the text comprehension capability of LLMs.</p>
<p>Table 1: A summary of LLMs used in the field of graph mining, highlighting the model architecture, which includes the LLM model, the GNN model, whether the parameters of the LLM are fine-tuned, the predictor of the architecture, the types of datasets, and the downstream task types. In the "Task" column, "Node" denotes node-level tasks, "Link" denotes edge-level tasks, and "Graph" denotes graph-level tasks.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Architecture</th>
<th></th>
<th></th>
<th>Graph Data</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The citation network uses directed edges to represent the citation relationships between articles. Green, blue, and red nodes represent literature categorized as "GNN-driving-LLM", "LLM-driving-GNN" and "GNN-LLM-codriving" respectively. The size of the nodes reflects the number of citations of each paper.</p>
<p>Table 2: Summary of experimental setups on selected models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">LLMs</th>
<th style="text-align: center;">GNNs</th>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">ratio*</th>
<th style="text-align: center;">Code</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Peak Classification</td>
<td style="text-align: center;">TAPE-GNN- $0 \times 45 \mathrm{E}[47]$</td>
<td style="text-align: center;">DeBERTa-base</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">$4 \times$ Nvidia RTX AN880 24GB GPUs</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">77.50 $\pm 0.12$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GLEM-G [79]</td>
<td style="text-align: center;">DeBERTa-base</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">$4 \times$ Nvidia RTX AN880 24GB GPUs</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">76.57 $\pm 0.29$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OFA [50]</td>
<td style="text-align: center;">llama2-13b</td>
<td style="text-align: center;">R-GCN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">77.51 $\pm 0.17$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">InstructGLM [71]</td>
<td style="text-align: center;">Llama-7b</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$4 \times 40 \mathrm{G}$ Nvidia A100 GPUs</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">75.70 $\pm 0.12$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphGPT-7B-v1.5-stage2 [65]</td>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">Graph Transformer</td>
<td style="text-align: center;">$4 \times 40 \mathrm{G}$ Nvidia A100 GPUs</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">75.11</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRENADE [86]</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">RevGAT-KD</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">76.21 $\pm 0.17$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraDBERT [87]</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">GraphSAGE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">75.05 $\pm 0.11$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphAdapter [89]</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">GraphSAGE</td>
<td style="text-align: center;">Nvidia A800 80GB GPU</td>
<td style="text-align: center;">Ogbn-arxiv</td>
<td style="text-align: center;">77.07 $\pm 0.15$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ENGINE [90]</td>
<td style="text-align: center;">LLaMA2-7B</td>
<td style="text-align: center;">GCN,SAGE,GAT</td>
<td style="text-align: center;">$6 \times$ Nvidia RTX 3090 GPUs</td>
<td style="text-align: center;">Ogbn-arxiv</td>
<td style="text-align: center;">76.02 $\pm 0.29$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PATTON [78]</td>
<td style="text-align: center;">BERT, SciBERT</td>
<td style="text-align: center;">GraphFormers</td>
<td style="text-align: center;">$4 \times$ Nvidia A6000 GPUs</td>
<td style="text-align: center;">Amazon-Sports</td>
<td style="text-align: center;">78.60 $\pm 0.15$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">THEM [88]</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">R-HGNN</td>
<td style="text-align: center;">$4 \times$ Nvidia RTX 3090 GPUs</td>
<td style="text-align: center;">GoodReads</td>
<td style="text-align: center;">81.57</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLMRec [45]</td>
<td style="text-align: center;">gpt-3.5-turbo, text-embedding-ada-002</td>
<td style="text-align: center;">LightGCN</td>
<td style="text-align: center;">Nvidia RTX 3090 GPU</td>
<td style="text-align: center;">MovieLens</td>
<td style="text-align: center;">36.43</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RLMRec [46]</td>
<td style="text-align: center;">gpt-3.5-turbo, text-embedding-ada-002</td>
<td style="text-align: center;">LightGCN</td>
<td style="text-align: center;">Nvidia RTX 3090 GPU</td>
<td style="text-align: center;">Amazon-book</td>
<td style="text-align: center;">14.83</td>
<td style="text-align: center;">Link</td>
</tr>
</tbody>
</table>
<p><em>In the ratio, node classification tasks are evaluated by accuracy, while recommendation tasks are evaluated by recall.
</em>*Recommendation.</p>
<h1>4.2 Discussion Analysis</h1>
<p>When evaluating models that combine LLMs with GNNs, the choice of test environment and test data is crucial to ensure the reliability and fairness of the results. In this section, the test environments of existing models and the selection of test data are summarized and described, and their comparison is shown in Table 2. As different test datasets are used in various studies covering multiple graph mining tasks, such as node classification, link prediction, and graph classification, in order to facilitate the comparison, we try to select a unified dataset with downstream task types as the classification. Due to the variety of graph-level tasks, it is inconvenient to perform comparative analysis, and only the data of node classification and recommendation tasks are selected for presentation here. In addition, to ensure the reproducibility of the results, we only selected the open source data. The table lists the hardware configurations (e.g., GPU model and number) used for each model, the specific test datasets, as well as the best-performing large language model and graph neural network model in the test. The organization of this information can help readers understand each model's computational efficiency and applicability and also provides a reliable reference for subsequent research for performance comparison and technology validation under different experimental conditions.</p>
<p>During the collation process, we observed that all models show improvements relative to the baseline, which powerfully demonstrates that combining LLMs and GNNs holds more incredible promise than using either model individually. Specifically, combining the robust language understanding and generation capabilities of LLMs with the graph-structured data processing strengths of GNNs results in a significant improvement in overall performance. This cross-model synergy not only improves the understanding of complex graph data but also enhances the model's performance in tasks</p>
<p>such as node classification, showing great potential for joint applications. This finding provides important insights for future research, emphasizing the importance of multi-model collaboration.
However, while we observe performance improvements across models for specific tasks, the overall improvement remains modest. For example, the recently proposed TAPE [47] model only improves by $1.6 \%$ over the baseline. This phenomenon may stem from the fact that the current method of combining LLMs and GNNs is too simple, which is more of a "stitching" rather than a true deep fusion. This simple combination fails to fully utilize the respective strengths of the two models, nor does it achieve true complementarity between them in terms of feature extraction and representation learning. As a result, despite some degree of performance improvement, the expected significant optimization is not achieved. This suggests that future research must explore more complex and efficient integration strategies to achieve deep synergy between LLMs and GNNs, thereby driving further improvement in model performance.</p>
<h1>5 Future Direction</h1>
<p>Based on the above analysis, it is clear that there are still many directions in this research area that have yet to be fully explored and deeply understood. Therefore, this section will further analyze these issues, focusing on the drawbacks and potential research opportunities in the current study, with a view to providing new ideas and insights for future academic exploration.</p>
<h3>5.1 Multimodal Graph Data Processing</h3>
<p>In graph data, nodes may be enriched with information in multiple modalities, such as text, images, and videos. These modalities may contain rich information, so understanding these multimodal data can help improve graph learning. A number of recent studies have explored the ability of LLMs to process and integrate multimodal data, and these studies have shown that LLMs exhibit significant capabilities in this area [108, 109], which makes it possible to apply LLMs to multimodal graph data. Future research will focus on exploring how to design a unified model to jointly encode data in different modalities such as graphs, text, and images. This will be applied to areas such as social network analysis, product recommendation, and molecular modeling to enhance the performance of models in complex, multimodal scenarios.</p>
<h3>5.2 Addressing the Hallucination Problem in Large Language Models</h3>
<p>While LLMs have shown a fantastic ability to generate text, they are prone to hallucinations and misinformation due to the fact that they tend to generate answers in a single pass and lack the ability to adjust dynamically. Hallucination means that the information generated by the model seems reasonable but is actually inaccurate, deviating from the user input, context, or even from the facts [110]. In specific sophisticated fields, such misinformation is unacceptable. Therefore, future research directions should focus on solving the hallucination problem and reducing the generation of misinformation, and this can be accomplished with the help of graph data. For example, it can be done by combining external knowledge graphs so that the big language model can reason step-by-step in generating answers and refer to reliable structured data sources to verify the accuracy of the information. Furthermore, using multi-hop reasoning and dynamic knowledge retrieval mechanisms enables the model to continuously adjust and correct its output according to the context, thus providing more accurate and trustworthy answers. By employing these strategies, the model will be more stable and reliable, especially in application scenarios that require high accuracy.</p>
<h3>5.3 Enhancing the Capability to Solve Complex Graph Tasks</h3>
<p>Currently, LLMs are primarily applied to basic graph tasks such as node classification and link prediction, but the remarkable capabilities that LLMs demonstrate in various areas suggest that their potential in graph data extends beyond these tasks. As a result, more and more research has explored their application to more complex graph tasks, such as graph generation [111], question answering over knowledge graph [112] and knowledge graph construction [113]. LLMs can be used to generate novel molecular structures, analyze complex relationship patterns in social networks, or assist in constructing more contextually connected knowledge graphs. The solution to these complex tasks will drive the further development of LLMs in a variety of fields, including biomedicine, social network analysis, and natural language processing.</p>
<p>Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining</p>
<h2>6 Conclusion</h2>
<p>In recent years, significant progress has been made in the application of LLMs in the field of graph mining. This study aims to provide an overview, summarize the research in this area, and provide potential directions for future research. We propose a new taxonomy based on different driving modes: GNN-driving-LLM, LLM-driving-GNN and GNN-LLM-co-driving. Each mode exhibits unique advantages and application potentials, especially when dealing with complex graph structures and textual information. The combination of LLMs and GNNs has brought new opportunities to graph mining. The semantic understanding capability of LLMs complements the structural information processing capability of GNNs, significantly improving the effectiveness of graph mining tasks. Despite the many opportunities presented by the combination of GNNs and LLMs, their high computational demands and model complexity remain challenges. Future research should explore optimizing the integration model of GNNs and LLMs to achieve more powerful graph learning and reasoning capabilities while ensuring computational efficiency, thus advancing the field of graph mining.</p>
<h2>References</h2>
<p>[1] Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. Fang: Leveraging social context for fake news detection using graph representation. In Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management, CIKM '20. ACM, October 2020.
[2] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013.
[3] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701-710, 2014.
[4] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks, 2016.
[5] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs, 2017.
[6] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
[7] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2017.
[8] Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, and Yoshua Bengio. Graph attention networks, 2018.
[9] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs, 2018.
[10] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Peng Cui, P. Yu, and Yanfang Ye. Heterogeneous graph attention network, 2021.
[11] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V. Chawla. Heterogeneous graph neural network. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD '19, page 793-803, New York, NY, USA, 2019. Association for Computing Machinery.
[12] Christopher D. Manning. Human Language Understanding \&amp; Reasoning. Daedalus, 151(2):127-138, 05 2022.
[13] Fabio Petroni, Tim Rocktschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. Language models as knowledge bases?, 2019.
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.
[15] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
[16] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021.</p>
<p>[17] Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Graph meets llms: Towards large graph models, 2023.
[18] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring the potential of large language models (llms) in learning on graphs, 2024.
[19] Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi. Towards graph foundation models: A survey and beyond, 2024.
[20] Ling Yang, Jiayi Zheng, Heyuan Wang, Zhongyi Liu, Zhilin Huang, Shenda Hong, Wentao Zhang, and Bin Cui. Individual and structural graph information bottlenecks for out-of-distribution generalization, 2023.
[21] Anton Korshunov, Ivan Beloborodov, Nazar Buzun, Valeriy Avanesov, and Sergey Kuznetsov. Social network analysis: methods and applications. Proceedings of the Institute for System Programming of RAS, 26(1):439-456, 2014.
[22] F. O. Isinkaye, Y. O. Folajimi, and B.A. Ojokoh. Recommendation systems: Principles, methods and evaluation. Egyptian Informatics Journal, 16(3):261-273, 2015.
[23] Xifeng Yan and Jiawei Han. gspan: Graph-based substructure pattern mining. In 2002 IEEE International Conference on Data Mining, 2002. Proceedings., pages 721-724. IEEE, 2002.
[24] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S Yu. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems, $\operatorname{PP}(99), 2021$.
[25] David Liben-Nowell and Jon Kleinberg. The link prediction problem for social networks. In Proceedings of the twelfth international conference on Information and knowledge management, pages 556-559, 2003.
[26] Lars Backstrom and Jure Leskovec. Supervised random walks: predicting and recommending links in social networks. In Proceedings of the fourth ACM international conference on Web search and data mining, pages $635-644,2011$.
[27] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural information processing systems, 31, 2018.
[28] Satu Elisa Schaeffer. Graph clustering. Computer science review, 1(1):27-64, 2007.
[29] Donatello Conte, Pasquale Foggia, Carlo Sansone, and Mario Vento. Thirty years of graph matching in pattern recognition. International journal of pattern recognition and artificial intelligence, 18(03):265-298, 2004.
[30] Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75-174, 2010.
[31] Michelle Girvan and Mark EJ Newman. Community structure in social and biological networks. Proceedings of the national academy of sciences, 99(12):7821-7826, 2002.
[32] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008, 2008.
[33] Akihiro Inokuchi, Takashi Washio, and Hiroshi Motoda. An apriori-based algorithm for mining frequent substructures from graph data. In Principles of Data Mining and Knowledge Discovery: 4th European Conference, PKDD 2000 Lyon, France, September 13-16, 2000 Proceedings 4, pages 13-23. Springer, 2000.
[34] Christian Borgelt and Michael R Berthold. Mining molecular fragments: Finding relevant substructures of molecules. In 2002 IEEE International Conference on Data Mining, 2002. Proceedings., pages 51-58. IEEE, 2002.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[38] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen,</p>
<p>Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simn Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.
[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[40] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding, 2020.
[41] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.
[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.
[43] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations, 2020.
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.
[45] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Llmrec: Large language models with graph augmentation for recommendation, 2024.
[46] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024, WWW '24. ACM, May 2024.</p>
<p>[47] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and Bryan Hooi. Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning, 2024.
[48] Qian Huang, Hongyu Ren, Peng Chen, Gregor Krmanc, Daniel Zeng, Percy Liang, and Jure Leskovec. Prodigy: Enabling in-context learning over graphs, 2023.
[49] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks, 2023.
[50] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang. One for all: Towards training one graph model for all classification tasks, 2024.
[51] Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N. Ioannidis, Xiang Song, Qing Ping, Sheng Wang, Carl Yang, Yi Xu, Belinda Zeng, and Trishul Chilimbi. Graph-aware language model pre-training on a large graph corpus can help multiple graph applications, 2023.
[52] Rui Xue, Xipeng Shen, Ruozhou Yu, and Xiaorui Liu. Efficient large language models fine-tuning on graphs, 2023.
[53] Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, and Qi Zhang. Adsgnn: Behavior-graph augmented relevance modeling in sponsored search, 2021.
[54] Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, and Huasha Zhao. Textgnn: Improving text encoder via graph neural network in sponsored search. In Proceedings of the Web Conference 2021, WWW '21. ACM, April 2021.
[55] Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, and Chao Huang. Graphedit: Large language models for graph structure learning, 2024.
[56] Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, and Xuecang Zhang. Leveraging large language models for node generation in few-shot learning on text-attributed graphs, 2024.
[57] Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, and Jiliang Tang. Label-free node classification on graphs with large language models (llms), 2024.
[58] Lianghao Xia, Ben Kao, and Chao Huang. Opengraph: Towards open graph foundation models, 2024.
[59] Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. Graphtext: Graph reasoning in text space, 2023.
[60] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for large language models, 2023.
[61] Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, and Shi Han. Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking, 2023.
[62] Yijian Qin, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Disentangled representation learning with large language models for text-attributed graphs, 2024.
[63] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language?, 2024.
[64] Chang Liu and Bo Wu. Evaluating large language models on graphs: Performance insights and comparative analysis, 2023.
[65] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models, 2024.
[66] He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery, 2023.
[67] Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, and Chuan Shi. Graphtranslator: Aligning graph model to large language model for open-ended tasks, 2024.
[68] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, and Chao Huang. Higpt: Heterogeneous graph language model, 2024.
[69] Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, and Qi Liu. Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning, 2023.
[70] Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, and Carl Yang. Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining, 2024.
[71] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Language is all a graph needs, 2024.</p>
<p>[72] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model, 2023.
[73] Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt, 2023.
[74] Haishuai Wang, Yang Gao, Xin Zheng, Peng Zhang, Hongyang Chen, Jiajun Bu, and Philip S. Yu. Graph neural architecture search with gpt-4, 2024.
[75] Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Chatrule: Mining logical rules with large language models for knowledge graph reasoning, 2024.
[76] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V. Chawla, and Panpan Xu. Graph neural prompting with large language models, 2023.
[77] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, and Xing Xie. Graphformers: Gnn-nested transformers for representation learning on textual graph, 2023.
[78] Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Xinyang Zhang, Qi Zhu, and Jiawei Han. Patton: Language model pretraining on text-rich networks, 2023.
[79] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning on large-scale text-attributed graphs via variational inference, 2023.
[80] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, and Jure Leskovec. Greaselm: Graph reasoning enhanced language models for question answering, 2022.
[81] Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2Mol: Cross-modal molecule retrieval with natural language queries. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 595-607, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
[82] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing, 2024.
[83] Philipp Seidl, Andreu Vall, Sepp Hochreiter, and Gnter Klambauer. Enhancing activity prediction models in drug discovery with the ability to understand human language, 2023.
[84] William Brannon, Wonjune Kang, Suyash Fulay, Hang Jiang, Brandon Roy, Deb Roy, and Jad Kabbara. Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings, 2024.
[85] Zhihao Wen and Yuan Fang. Prompt tuning on graph-augmented low-resource text classification, 2024.
[86] Yichuan Li, Kaize Ding, and Kyumin Lee. Grenade: Graph-centric language model for self-supervised representation learning on text-attributed graphs, 2023.
[87] Costas Mavromatis, Vassilis N. Ioannidis, Shen Wang, Da Zheng, Soji Adeshina, Jun Ma, Han Zhao, Christos Faloutsos, and George Karypis. Train your own gnn teacher: Graph-aware distillation on textual graphs, 2023.
[88] Tao Zou, Le Yu, Yifei Huang, Leilei Sun, and Bowen Du. Pretraining language models with text-attributed heterogeneous graphs, 2023.
[89] Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, and Qi Zhu. Can gnn be good adapter for llms?, 2024.
[90] Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. Efficient tuning and inference for large language models on textual graphs, 2024.
[91] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165-172, 2013.
[92] Mark S Granovetter. The strength of weak ties. American journal of sociology, 78(6):1360-1380, 1973.
[93] Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7370-7377, 2019.
[94] Derek J De Solla Price. Networks of scientific papers: The pattern of bibliographic references indicates the nature of the scientific research front. Science, 149(3683):510-515, 1965.
[95] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings, 2016.
[96] Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, and Yan Zhang. Is chatgpt a good recommender? a preliminary study, 2023.</p>
<p>[97] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022.
[98] Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, and Xiangang Li. Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences, 2023.
[99] Jin Huang, Xingjian Zhang, Qiaozhu Mei, and Jiaqi Ma. Can llms effectively leverage graph structural information through prompts, and why?, 2024.
[100] Yuntong Hu, Zheng Zhang, and Liang Zhao. Beyond text: A deep dive into large language models' ability on understanding graph data, 2023.
[101] Ulrik Brandes, Markus Eiglsperger, Jrgen Lerner, and Christian Pich. Graph markup language (graphml). 2013.
[102] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.
[103] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https : //github.com/kingoflolz/mesh-transformer-jax, May 2021.
[104] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
[105] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text, 2019.
[106] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations, 2021.
[107] Le Yu, Leilei Sun, Bowen Du, Chuanren Liu, Weifeng Lv, and Hui Xiong. Heterogeneous graph representation learning with relation awareness. IEEE Transactions on Knowledge and Data Engineering, page 1-1, 2022.
[108] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm, 2024.
[109] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
[110] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's song in the ai ocean: A survey on hallucination in large language models, 2023.
[111] Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, and Hong Mei. Exploring the potential of large language models in graph generation, 2024.
[112] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages $105-113,2019$.
[113] Ciyuan Peng, Feng Xia, Mehdi Naseriparsa, and Francesco Osborne. Knowledge graphs: Opportunities and challenges, 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
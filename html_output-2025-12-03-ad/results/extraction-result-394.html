<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-394 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-394</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-394</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-252383216</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.09874v2.pdf" target="_blank">Open-vocabulary Queryable Scene Representations for Real World Planning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e394.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e394.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLMap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-Language Map (NLMap)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-vocabulary, queryable spatial semantic scene representation built from exploration using class-agnostic region proposals and ensemble visual-language model features, storing per-ROI image embeddings plus estimated 3D position and size to enable language-conditioned queries for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLMap (VLM-based queryable scene representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Constructs a context point cloud C = {(φ_i, p_i, r_i)} by running class-agnostic ROI proposals over RGB images, extracting 512-d CLIP and 512-d ViLD image embeddings for each ROI, and estimating 3D centroid p_i and size r_i from depth; supports text queries via inner-product matching to CLIP text embeddings and ViLD features.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-world kitchen embodied planning (NLMap + SayCan)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Build a persistent, open-vocabulary scene map via exploration (RGBD), then given a natural-language instruction propose relevant objects (via an LLM), query map for object presence and 3D locations, generate executable options, and plan/execute multi-step navigation+manipulation tasks in a real kitchen environment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; navigation; object manipulation; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational+procedural (spatial: explicit 3D positions and sizes; object-relational: open-vocabulary object presence, identity and nearest-policy binding; procedural: enables LLM planner to sequence options conditioned on scene)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained visual-language models (CLIP, ViLD) applied to exploration RGBD data; scene construction from on-board sensors</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>explicit map construction from sensory stream (exploration), then text-query via inner-product similarity (zero-shot/ensemble); downstream few-shot prompting for planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit feature-point cloud / spatial map: per-ROI image feature vectors (CLIP/ViLD) combined with estimated 3D centroids and radii, clustered via multi-view fusion into object instances; natural-language queryable via text-image alignment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate (end-to-end execution), perception/query recall (object query success rate), planning/generative-plan correctness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>System-level: 55 tasks at 61.8% overall success (end-to-end) reported; perception/object-query success: 82% (scene 1) and 64% (scene 2) using ensemble + multi-view fusion; object-query ablations show CLIP or ViLD alone perform substantially worse; novel-object end-to-end tasks: 80% success; missing-object detection benchmark: 40% success (many failures due to false positives).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provided explicit spatial grounding (3D centroids) and open-vocabulary object presence that enabled an LLM planner to (a) generate correct multi-step sequences conditioned on scene objects, (b) expand navigation options to arbitrary discovered locations, and (c) detect some infeasible tasks when objects absent (terminate). Ensemble of CLIP+ViLD plus multi-view fusion reduced view-specific noise and improved robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Perception errors (false positives) often caused incorrect context grounding and planning failures; low camera resolution and exposure limited detection; dynamic objects/humans not handled (static scene assumption); some OOD objects led to false negatives for ViLD, while CLIP misfires on common items—necessitating ensemble; LLM planner still struggles with negation and typos.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to privileged SayCan (ground-truth perception) which had ~84% success on SayCan task set, NLMap+SayCan had 66.7% on that subset; vanilla SayCan (hard-coded object list) failed on novel-object tasks (0%); NLMap enabled 80% success on novel-object tasks that SayCan could not.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations: CLIP-only and ViLD-only retrieval give much lower object-query success; maximum-ensemble (max of CLIP and ViLD inner products) improves results; adding multi-view fusion to the ensemble gave +2% absolute in scene1 and +17% in scene2; removing NLMap/context conditioning collapses capability to handle novel/missing-object tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An explicit, open-vocabulary scene map built from VLM features plus 3D geometry effectively grounds an LLM planner that lacks direct sensory input: spatial knowledge is stored explicitly (positions, sizes), object-relational knowledge (open names, availability) is queried via text-image alignment, and procedural planning is enabled by conditioning the LLM on discovered objects and generated executable options.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Queryable Scene Representations for Real World Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e394.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e394.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context-aware SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context-Aware Integration of NLMap with SayCan (LLM-based planning conditioned on scene)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the SayCan framework in which an LLM planner (PaLM 540B) is conditioned on the list of discovered scene objects from NLMap; executable options are generated by templating discovered object names and binding them to existing policies via CLIP-based nearest-neighbor policy matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Context-aware SayCan (PaLM-based planner conditioned on NLMap)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses an LLM (PaLM 540B) for few-shot decomposition and scoring of options; templates map discovered object names to option strings (e.g., 'pick up [x]') and CLIP nearest-neighbor matches bind these option descriptions to existing manipulation/navigation policies; combined scores (LLM likelihood × affordance model) select options at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LLM-conditioned task planning for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an instruction and a list of available objects (from NLMap), the LLM is prompted (few-shot) to decompose into a sequence of options; each option is bound to a policy using CLIP-based nearest neighbor mapping; the planner must either produce executable sequences when objects present or immediately terminate ('done') when necessary objects are missing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following; multi-step planning; navigation+manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational+spatial (procedural sequences and decomposition inside LLM; object-relational binding of discovered names to affordances; spatial grounding via object locations supplied to planner prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM pretraining (PaLM) and in-context few-shot examples; NLMap perception for object availability/locations; CLIP for policy binding</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting for decomposition & plan generation; conditioning prompts include 'Scene: <list of discovered objects>' to ground planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural knowledge is implicit in LLM weights and elicited via few-shot prompts; ground-truth scene state is represented as a natural-language list ('Scene: apple, coke can') and template-based executable options; policies are bound symbolically to option strings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>generative planning correctness (plan accomplishes instruction and consistent with available objects), end-to-end task success rate when combined with NLMap and execution stack</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Generative LLM planning: 85% success on positive (all needed objects present) sets and 60% on negative (missing objects) sets; end-to-end SayCan tasks: NLMap+SayCan 66.7% vs privileged SayCan 84% (on SayCan-like tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When provided an accurate list of available objects, the LLM reliably decomposes instructions into procedurally correct sequences, and can be instructed to terminate for infeasible tasks; it handles multi-step decomposition and infers implicit objects when prompted with examples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Struggles on negative/negation reasoning (false positives in perception lead to wrong plans), sensitive to typos in object names during proposal/binding, and dependence on available manipulation policies (cannot execute arbitrary new manipulations without a suitable policy binding).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Vanilla SayCan (no NLMap grounding) cannot reason about objects outside its hard-coded list and fails on novel-object tasks (0%); privileged SayCan (with ground-truth perception) achieves higher success on SayCan tasks (~84%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing the scene-object conditioning from the planning prompt (i.e., reverting to SayCan prompting) eliminates ability to detect missing-object infeasibility; replacing CLIP-based policy binding reduces ability to bind novel object names to manipulable policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs (PaLM) encode strong procedural sequencing but require explicit, structured contextual grounding (a list of available objects + locations) to operate correctly in embodied tasks; templated option generation + CLIP-based binding allows reuse of a small policy set across many discovered objects, effectively linking object-relational knowledge to executable affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Queryable Scene Representations for Real World Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e394.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e394.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 540B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained language model (540B parameters) used here for few-shot object-proposal and for generating multi-step plans in the SayCan pipeline, demonstrating strong procedural and commonsense inference but requiring perceptual grounding for spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm: Scaling language modeling with pathways</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A very large transformer language model pre-trained on text corpora and used in this work in a few-shot prompting setup for (a) proposing relevant object names from free-form instructions and (b) generating action sequences/plans given instructions and scene-object context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LLM-based object proposal and planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Object-proposal: infer a list of objects (possibly implicit) from natural-language instructions; Planning: decompose an instruction into ordered actions/options compatible with available policies and discovered objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following; object proposal; procedural planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (inferring objects and action sequences; mapping language to affordance-like options)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora; in-context few-shot examples provided at runtime</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting (object-proposal prompts and planning prompts, with positive/negative examples and explicit 'Scene: ...' conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural and commonsense knowledge implicit in model weights and elicited via few-shot examples; outputs are natural-language sequences (plans) and lists of object names</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>object-proposal success rate by task family; generative planning correctness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Object-proposal success rates (PaLM 540B): instruction implication: 92%; crowd-sourced/unstructured: 96%; detailed/fine-grained descriptions: 72%; proper-granularity decomposition: 60% (Table II). Generative planning: 85% success on positive sets, 60% on negative sets.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Accurately infers implicit objects required by tasks (e.g., 'heat the taco' → taco, microwave), handles unstructured and multi-step instructions, and preserves fine-grained descriptors when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails on decompositions requiring fine granularity in many cases (60% success), sensitive to typos and out-of-distribution phrasing; struggles with negation and negative-condition reasoning (lower success on negative object sets).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Smaller PaLM variants show degraded performance: PaLM-62B and PaLM-8B show reductions in object-proposal success rates (see Table II in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Using smaller PaLM models (62B, 8B) sharply degrades performance on 'proper granularity' tasks and mildly reduces other categories, indicating scale matters for complex object-proposal and decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LLMs like PaLM-540B encode substantial procedural and object-relational inferential capacity (can propose objects and plan steps), but they do not internally represent scene geometry or object locations — they must be explicitly grounded with perceptual scene representations (NLMap) to succeed in embodied execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Queryable Scene Representations for Real World Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e394.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e394.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastively-trained visual-language model producing image and text embeddings that can be compared via inner product, used here to (a) encode ROIs for open-vocabulary querying and (b) bind textual option descriptions to available policies via nearest-neighbor in embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained image-text contrastive model that maps images and text to a shared embedding space; in NLMap, CLIP image embeddings (and CLIP text embeddings) are used to score alignment between textual queries and ROI image features, and CLIP nearest-neighbor is used to bind discovered object names to existing policy descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary RoI retrieval and policy binding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve top-k candidate ROI/contexts for a text query by computing inner products between CLIP text embeddings and CLIP image embeddings; use CLIP similarity to select the closest existing manipulation policy for a generated object-option string.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception (open-vocabulary retrieval); policy binding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (text-image alignment; semantic similarity for policy binding) and partial spatial (used with positional estimates but CLIP itself is visual-textual)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on large-scale image-text pairs</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot inner-product similarity between image features and CLIP text embeddings; nearest-neighbor search for policy binding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>dense image and text embeddings aligned in a joint space (used for retrieval and semantic matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>object retrieval success (open-vocabulary query), policy-binding correctness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CLIP alone has lower object-query success than the ensemble with ViLD; CLIP is better at detecting uncommon/out-of-distribution items and text/signs (paper reports CLIP better for uncommon objects but less robust for common objects). Exact numeric retrieval figures: CLIP-only baseline underperforms ensemble; ensemble + fusion yields 82%/64% per-scene (paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good at matching textual queries to ROIs for uncommon objects or text/sign recognition; effective for semantic nearest-neighbor policy binding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Less robust for common household objects (false negatives) compared to ViLD in this setup; single-model retrieval leads to noisy false positives that multi-view fusion helps reduce.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ViLD-only vs CLIP-only vs Ensemble (max of both) — ensemble outperforms single-model retrieval; adding multi-view fusion further improves robustness (+2% scene1, +17% scene2).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Replacing ensemble with CLIP-only or ViLD-only reduces query success; removing multi-view fusion from ensemble reduces performance, especially in scenes with noise/outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dense image-text embeddings from CLIP are effective for open-vocabulary retrieval and semantic policy binding but must be combined with other models (ViLD) and geometric/multi-view fusion to achieve reliable spatial/object-relational grounding for embodied planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Queryable Scene Representations for Real World Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e394.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e394.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViLD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViLD (Open-vocabulary detection with vision-language distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visual-language model adapted for open-vocabulary object detection that provides ROI image embeddings used in NLMap; it is better at detecting common objects in the scene but can miss out-of-distribution items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open-vocabulary object detection via vision and language knowledge distillation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViLD</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLM that outputs image embeddings optimized for open-vocabulary detection; used in NLMap as part of an ensemble (with CLIP) to produce per-ROI feature vectors that are queryable with text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary ROI encoding for scene mapping</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Encode proposed ROIs with ViLD image embeddings to provide robust detection of common objects across multiple views; used together with CLIP to form an ensemble retrieval metric for queries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception; object detection/retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (common-object recognition), contributes to spatial mapping when combined with geometry</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining / distillation from vision-language datasets</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot ROI embedding extraction and inner-product similarity with CLIP text embeddings (ensemble max rule)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>image embeddings representing visual patterns typically associated with common objects, compatible with text embeddings for retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>object retrieval success in open-vocabulary queries</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>ViLD alone reliably detects common objects (e.g., cans, apples) but suffers false negatives for out-of-distribution items like 'first aid station'; ViLD-only retrieval underperforms the ensemble in overall success.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Robust detection of frequent/common objects and contributes stable features for multi-view aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Misses unusual or sign-based objects that CLIP finds; single-model reliance causes decreased recall on some queries.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ViLD-only vs CLIP-only vs Ensemble — ensemble yields best trade-off; multi-view fusion on top of ensemble provides further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Using ViLD alone reduces detection of uncommon items; ensemble with CLIP recovers many of these misses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining a detection-oriented VLM (ViLD) with a more text-sign sensitive model (CLIP) yields complementary strengths for open-vocabulary scene representation; neither model alone suffices for robust embodied grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Queryable Scene Representations for Real World Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e394.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e394.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Object-Proposal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Object Proposal Module (few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM few-shot prompting module that, given a free-form human instruction, proposes a set of relevant object names (including implicit and fine-grained references) to be queried against the NLMap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM Object-Proposal (PaLM-based few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implements few-shot prompts (examples provided) to elicit from an LLM the set of object names and references required to fulfill an instruction; designed to handle inference of implied objects, unstructured language, fine-grained descriptors, and decomposition to proper granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Object proposal from natural language instructions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate an arbitrary human instruction (which may omit some nouns explicitly) into a structured list of candidate object names to be looked up in the NLMap and bound to executable actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language understanding; object-relational inference</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + procedural inference (infer which objects are relevant to achieving an instruction and what roles they play)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM pretraining + in-context few-shot examples</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting with example instruction → object lists</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>output: list of object name strings (possibly with descriptors) used to query visual map; internal reasoning implicit in LLM weights</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>object-proposal success rate by category (instruction implication, unstructured, detailed description, proper granularity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Object-proposal success (PaLM-540B): instruction implication 92% (25 cases), crowd-sourced/unstructured 96% (25 cases), fine-grained descriptions 72% (25 cases), decomposition/proper-granularity 60% (15 cases).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively infers objects implied by tasks and handles varied unstructured human utterances; preserves many fine-grained descriptors and supports multi-step decomposition in most cases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Often fails to expand or decompose categories to the precise granularity required (lowest performance on 'proper granularity'); sensitive to ambiguous or highly compositional descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to simple noun-extraction heuristics (prior work), this few-shot approach is more demanding and yields higher recall on implied/unstructured cases (quantitative baseline numbers not present in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Smaller LLM sizes reduce success rates substantially on granularity-sensitive tasks (see PaLM-62B, PaLM-8B results in paper Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can reliably propose relevant object sets from free-form instructions using few-shot prompts, providing the critical bridge from unstructured language to structured queries against perceptual maps; however, decomposition to precise granularity remains challenging and benefits from larger LLM scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Queryable Scene Representations for Real World Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e394.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e394.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-view Fusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-View Fusion Algorithm for NLMap</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that aggregates top-k context elements (ROI observations) for a queried object by clustering them via KL divergence between Gaussian proxies of 2D projections, then producing weighted estimates of 3D object locations and confidence scores to filter false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-view fusion (NLMap clustering & aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Takes top-k scored context elements per query (from ensemble metric D = max(CLIP, ViLD inner products), with each context element having (φ_i, p_i, r_i)), represents each as a 2D Gaussian N(p_i, α·r_i), clusters via KL divergence threshold λ, computes score-weighted averages for cluster centroids, boosts cluster confidence by multiplicity, and applies thresholds to decide object presence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregating multi-view observations into object instances and 3D locations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given multiple ROI observations across viewpoints for a text query, fuse detections to produce a small set of object hypotheses with improved location estimates and aggregated confidence to reduce single-view noise.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception; multi-view aggregation; spatial representation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (explicit 3D location inference) + object-relational (aggregated confidence and instance identification across views)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>geometric estimates from depth + per-ROI visual-language scores; algorithmic fusion (no new learning)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>deterministic clustering and weighted averaging (algorithmic), using KL divergence as a proxy for bounding-volume overlap</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>clusters of Gaussian proxies representing object instance locations and sizes; final outputs: fused 3D centroids, sizes and aggregate confidence scores</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>object presence detection accuracy (per-scene retrieval success), improvement over no-fusion baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Adding multi-view fusion to the ensemble metric produced a modest +2% absolute accuracy in scene1 and a substantial +17% in scene2 (paper reports these figures), demonstrating its effectiveness in reducing view-specific false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively removes outlier high-scoring but inconsistent single views, consolidates multi-view evidence to improve recall/precision of object detection and location estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still vulnerable to systematic false positives across views (e.g., repeated misdetections), and performance depends on initial ROI proposals and camera coverage; does not handle dynamic objects/humans (static assumption).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Ensemble (max(CLIP,ViLD)) without multi-view fusion is baseline; fusion substantially improves retrieval success in some scenes and provides more reliable location estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No fusion (k=1) reduces robustness; using KL divergence clustering and multiplicity-based score bonuses are key design choices that improved results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Algorithmic multi-view aggregation that leverages geometric estimates and consistency across views materially improves reliability of open-vocabulary queries for embodied planning, reducing single-view noise that would otherwise mislead an LLM planner that lacks raw sensory access.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Queryable Scene Representations for Real World Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can and not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Open-vocabulary object detection via vision and language knowledge distillation <em>(Rating: 2)</em></li>
                <li>Visual language maps for robot navigation <em>(Rating: 2)</em></li>
                <li>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-394",
    "paper_id": "paper-252383216",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "NLMap",
            "name_full": "Natural-Language Map (NLMap)",
            "brief_description": "An open-vocabulary, queryable spatial semantic scene representation built from exploration using class-agnostic region proposals and ensemble visual-language model features, storing per-ROI image embeddings plus estimated 3D position and size to enable language-conditioned queries for planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NLMap (VLM-based queryable scene representation)",
            "model_size": null,
            "model_description": "Constructs a context point cloud C = {(φ_i, p_i, r_i)} by running class-agnostic ROI proposals over RGB images, extracting 512-d CLIP and 512-d ViLD image embeddings for each ROI, and estimating 3D centroid p_i and size r_i from depth; supports text queries via inner-product matching to CLIP text embeddings and ViLD features.",
            "task_name": "Real-world kitchen embodied planning (NLMap + SayCan)",
            "task_description": "Build a persistent, open-vocabulary scene map via exploration (RGBD), then given a natural-language instruction propose relevant objects (via an LLM), query map for object presence and 3D locations, generate executable options, and plan/execute multi-step navigation+manipulation tasks in a real kitchen environment.",
            "task_type": "multi-step planning; navigation; object manipulation; instruction following",
            "knowledge_type": "spatial+object-relational+procedural (spatial: explicit 3D positions and sizes; object-relational: open-vocabulary object presence, identity and nearest-policy binding; procedural: enables LLM planner to sequence options conditioned on scene)",
            "knowledge_source": "pretrained visual-language models (CLIP, ViLD) applied to exploration RGBD data; scene construction from on-board sensors",
            "has_direct_sensory_input": true,
            "elicitation_method": "explicit map construction from sensory stream (exploration), then text-query via inner-product similarity (zero-shot/ensemble); downstream few-shot prompting for planning",
            "knowledge_representation": "explicit feature-point cloud / spatial map: per-ROI image feature vectors (CLIP/ViLD) combined with estimated 3D centroids and radii, clustered via multi-view fusion into object instances; natural-language queryable via text-image alignment",
            "performance_metric": "task success rate (end-to-end execution), perception/query recall (object query success rate), planning/generative-plan correctness",
            "performance_result": "System-level: 55 tasks at 61.8% overall success (end-to-end) reported; perception/object-query success: 82% (scene 1) and 64% (scene 2) using ensemble + multi-view fusion; object-query ablations show CLIP or ViLD alone perform substantially worse; novel-object end-to-end tasks: 80% success; missing-object detection benchmark: 40% success (many failures due to false positives).",
            "success_patterns": "Provided explicit spatial grounding (3D centroids) and open-vocabulary object presence that enabled an LLM planner to (a) generate correct multi-step sequences conditioned on scene objects, (b) expand navigation options to arbitrary discovered locations, and (c) detect some infeasible tasks when objects absent (terminate). Ensemble of CLIP+ViLD plus multi-view fusion reduced view-specific noise and improved robustness.",
            "failure_patterns": "Perception errors (false positives) often caused incorrect context grounding and planning failures; low camera resolution and exposure limited detection; dynamic objects/humans not handled (static scene assumption); some OOD objects led to false negatives for ViLD, while CLIP misfires on common items—necessitating ensemble; LLM planner still struggles with negation and typos.",
            "baseline_comparison": "Compared to privileged SayCan (ground-truth perception) which had ~84% success on SayCan task set, NLMap+SayCan had 66.7% on that subset; vanilla SayCan (hard-coded object list) failed on novel-object tasks (0%); NLMap enabled 80% success on novel-object tasks that SayCan could not.",
            "ablation_results": "Ablations: CLIP-only and ViLD-only retrieval give much lower object-query success; maximum-ensemble (max of CLIP and ViLD inner products) improves results; adding multi-view fusion to the ensemble gave +2% absolute in scene1 and +17% in scene2; removing NLMap/context conditioning collapses capability to handle novel/missing-object tasks.",
            "key_findings": "An explicit, open-vocabulary scene map built from VLM features plus 3D geometry effectively grounds an LLM planner that lacks direct sensory input: spatial knowledge is stored explicitly (positions, sizes), object-relational knowledge (open names, availability) is queried via text-image alignment, and procedural planning is enabled by conditioning the LLM on discovered objects and generated executable options.",
            "uuid": "e394.0",
            "source_info": {
                "paper_title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Context-aware SayCan",
            "name_full": "Context-Aware Integration of NLMap with SayCan (LLM-based planning conditioned on scene)",
            "brief_description": "An extension of the SayCan framework in which an LLM planner (PaLM 540B) is conditioned on the list of discovered scene objects from NLMap; executable options are generated by templating discovered object names and binding them to existing policies via CLIP-based nearest-neighbor policy matching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Context-aware SayCan (PaLM-based planner conditioned on NLMap)",
            "model_size": null,
            "model_description": "Uses an LLM (PaLM 540B) for few-shot decomposition and scoring of options; templates map discovered object names to option strings (e.g., 'pick up [x]') and CLIP nearest-neighbor matches bind these option descriptions to existing manipulation/navigation policies; combined scores (LLM likelihood × affordance model) select options at each step.",
            "task_name": "LLM-conditioned task planning for embodied agents",
            "task_description": "Given an instruction and a list of available objects (from NLMap), the LLM is prompted (few-shot) to decompose into a sequence of options; each option is bound to a policy using CLIP-based nearest neighbor mapping; the planner must either produce executable sequences when objects present or immediately terminate ('done') when necessary objects are missing.",
            "task_type": "instruction following; multi-step planning; navigation+manipulation",
            "knowledge_type": "procedural+object-relational+spatial (procedural sequences and decomposition inside LLM; object-relational binding of discovered names to affordances; spatial grounding via object locations supplied to planner prompts)",
            "knowledge_source": "LLM pretraining (PaLM) and in-context few-shot examples; NLMap perception for object availability/locations; CLIP for policy binding",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting for decomposition & plan generation; conditioning prompts include 'Scene: &lt;list of discovered objects&gt;' to ground planning",
            "knowledge_representation": "Procedural knowledge is implicit in LLM weights and elicited via few-shot prompts; ground-truth scene state is represented as a natural-language list ('Scene: apple, coke can') and template-based executable options; policies are bound symbolically to option strings.",
            "performance_metric": "generative planning correctness (plan accomplishes instruction and consistent with available objects), end-to-end task success rate when combined with NLMap and execution stack",
            "performance_result": "Generative LLM planning: 85% success on positive (all needed objects present) sets and 60% on negative (missing objects) sets; end-to-end SayCan tasks: NLMap+SayCan 66.7% vs privileged SayCan 84% (on SayCan-like tasks).",
            "success_patterns": "When provided an accurate list of available objects, the LLM reliably decomposes instructions into procedurally correct sequences, and can be instructed to terminate for infeasible tasks; it handles multi-step decomposition and infers implicit objects when prompted with examples.",
            "failure_patterns": "Struggles on negative/negation reasoning (false positives in perception lead to wrong plans), sensitive to typos in object names during proposal/binding, and dependence on available manipulation policies (cannot execute arbitrary new manipulations without a suitable policy binding).",
            "baseline_comparison": "Vanilla SayCan (no NLMap grounding) cannot reason about objects outside its hard-coded list and fails on novel-object tasks (0%); privileged SayCan (with ground-truth perception) achieves higher success on SayCan tasks (~84%).",
            "ablation_results": "Removing the scene-object conditioning from the planning prompt (i.e., reverting to SayCan prompting) eliminates ability to detect missing-object infeasibility; replacing CLIP-based policy binding reduces ability to bind novel object names to manipulable policies.",
            "key_findings": "LLMs (PaLM) encode strong procedural sequencing but require explicit, structured contextual grounding (a list of available objects + locations) to operate correctly in embodied tasks; templated option generation + CLIP-based binding allows reuse of a small policy set across many discovered objects, effectively linking object-relational knowledge to executable affordances.",
            "uuid": "e394.1",
            "source_info": {
                "paper_title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PaLM-540B",
            "name_full": "PaLM 540B",
            "brief_description": "A large pre-trained language model (540B parameters) used here for few-shot object-proposal and for generating multi-step plans in the SayCan pipeline, demonstrating strong procedural and commonsense inference but requiring perceptual grounding for spatial reasoning.",
            "citation_title": "Palm: Scaling language modeling with pathways",
            "mention_or_use": "use",
            "model_name": "PaLM 540B",
            "model_size": "540B",
            "model_description": "A very large transformer language model pre-trained on text corpora and used in this work in a few-shot prompting setup for (a) proposing relevant object names from free-form instructions and (b) generating action sequences/plans given instructions and scene-object context.",
            "task_name": "LLM-based object proposal and planning",
            "task_description": "Object-proposal: infer a list of objects (possibly implicit) from natural-language instructions; Planning: decompose an instruction into ordered actions/options compatible with available policies and discovered objects.",
            "task_type": "instruction following; object proposal; procedural planning",
            "knowledge_type": "procedural + object-relational (inferring objects and action sequences; mapping language to affordance-like options)",
            "knowledge_source": "pre-training on large text corpora; in-context few-shot examples provided at runtime",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting (object-proposal prompts and planning prompts, with positive/negative examples and explicit 'Scene: ...' conditioning)",
            "knowledge_representation": "Procedural and commonsense knowledge implicit in model weights and elicited via few-shot examples; outputs are natural-language sequences (plans) and lists of object names",
            "performance_metric": "object-proposal success rate by task family; generative planning correctness",
            "performance_result": "Object-proposal success rates (PaLM 540B): instruction implication: 92%; crowd-sourced/unstructured: 96%; detailed/fine-grained descriptions: 72%; proper-granularity decomposition: 60% (Table II). Generative planning: 85% success on positive sets, 60% on negative sets.",
            "success_patterns": "Accurately infers implicit objects required by tasks (e.g., 'heat the taco' → taco, microwave), handles unstructured and multi-step instructions, and preserves fine-grained descriptors when possible.",
            "failure_patterns": "Fails on decompositions requiring fine granularity in many cases (60% success), sensitive to typos and out-of-distribution phrasing; struggles with negation and negative-condition reasoning (lower success on negative object sets).",
            "baseline_comparison": "Smaller PaLM variants show degraded performance: PaLM-62B and PaLM-8B show reductions in object-proposal success rates (see Table II in paper).",
            "ablation_results": "Using smaller PaLM models (62B, 8B) sharply degrades performance on 'proper granularity' tasks and mildly reduces other categories, indicating scale matters for complex object-proposal and decomposition.",
            "key_findings": "Large LLMs like PaLM-540B encode substantial procedural and object-relational inferential capacity (can propose objects and plan steps), but they do not internally represent scene geometry or object locations — they must be explicitly grounded with perceptual scene representations (NLMap) to succeed in embodied execution.",
            "uuid": "e394.2",
            "source_info": {
                "paper_title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pre-training)",
            "brief_description": "A contrastively-trained visual-language model producing image and text embeddings that can be compared via inner product, used here to (a) encode ROIs for open-vocabulary querying and (b) bind textual option descriptions to available policies via nearest-neighbor in embedding space.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_size": null,
            "model_description": "Pretrained image-text contrastive model that maps images and text to a shared embedding space; in NLMap, CLIP image embeddings (and CLIP text embeddings) are used to score alignment between textual queries and ROI image features, and CLIP nearest-neighbor is used to bind discovered object names to existing policy descriptions.",
            "task_name": "Open-vocabulary RoI retrieval and policy binding",
            "task_description": "Retrieve top-k candidate ROI/contexts for a text query by computing inner products between CLIP text embeddings and CLIP image embeddings; use CLIP similarity to select the closest existing manipulation policy for a generated object-option string.",
            "task_type": "perception (open-vocabulary retrieval); policy binding",
            "knowledge_type": "object-relational (text-image alignment; semantic similarity for policy binding) and partial spatial (used with positional estimates but CLIP itself is visual-textual)",
            "knowledge_source": "pretraining on large-scale image-text pairs",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot inner-product similarity between image features and CLIP text embeddings; nearest-neighbor search for policy binding",
            "knowledge_representation": "dense image and text embeddings aligned in a joint space (used for retrieval and semantic matching)",
            "performance_metric": "object retrieval success (open-vocabulary query), policy-binding correctness",
            "performance_result": "CLIP alone has lower object-query success than the ensemble with ViLD; CLIP is better at detecting uncommon/out-of-distribution items and text/signs (paper reports CLIP better for uncommon objects but less robust for common objects). Exact numeric retrieval figures: CLIP-only baseline underperforms ensemble; ensemble + fusion yields 82%/64% per-scene (paper).",
            "success_patterns": "Good at matching textual queries to ROIs for uncommon objects or text/sign recognition; effective for semantic nearest-neighbor policy binding.",
            "failure_patterns": "Less robust for common household objects (false negatives) compared to ViLD in this setup; single-model retrieval leads to noisy false positives that multi-view fusion helps reduce.",
            "baseline_comparison": "ViLD-only vs CLIP-only vs Ensemble (max of both) — ensemble outperforms single-model retrieval; adding multi-view fusion further improves robustness (+2% scene1, +17% scene2).",
            "ablation_results": "Replacing ensemble with CLIP-only or ViLD-only reduces query success; removing multi-view fusion from ensemble reduces performance, especially in scenes with noise/outliers.",
            "key_findings": "Dense image-text embeddings from CLIP are effective for open-vocabulary retrieval and semantic policy binding but must be combined with other models (ViLD) and geometric/multi-view fusion to achieve reliable spatial/object-relational grounding for embodied planning.",
            "uuid": "e394.3",
            "source_info": {
                "paper_title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ViLD",
            "name_full": "ViLD (Open-vocabulary detection with vision-language distillation)",
            "brief_description": "A visual-language model adapted for open-vocabulary object detection that provides ROI image embeddings used in NLMap; it is better at detecting common objects in the scene but can miss out-of-distribution items.",
            "citation_title": "Open-vocabulary object detection via vision and language knowledge distillation",
            "mention_or_use": "use",
            "model_name": "ViLD",
            "model_size": null,
            "model_description": "A VLM that outputs image embeddings optimized for open-vocabulary detection; used in NLMap as part of an ensemble (with CLIP) to produce per-ROI feature vectors that are queryable with text embeddings.",
            "task_name": "Open-vocabulary ROI encoding for scene mapping",
            "task_description": "Encode proposed ROIs with ViLD image embeddings to provide robust detection of common objects across multiple views; used together with CLIP to form an ensemble retrieval metric for queries.",
            "task_type": "perception; object detection/retrieval",
            "knowledge_type": "object-relational (common-object recognition), contributes to spatial mapping when combined with geometry",
            "knowledge_source": "pretraining / distillation from vision-language datasets",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot ROI embedding extraction and inner-product similarity with CLIP text embeddings (ensemble max rule)",
            "knowledge_representation": "image embeddings representing visual patterns typically associated with common objects, compatible with text embeddings for retrieval",
            "performance_metric": "object retrieval success in open-vocabulary queries",
            "performance_result": "ViLD alone reliably detects common objects (e.g., cans, apples) but suffers false negatives for out-of-distribution items like 'first aid station'; ViLD-only retrieval underperforms the ensemble in overall success.",
            "success_patterns": "Robust detection of frequent/common objects and contributes stable features for multi-view aggregation.",
            "failure_patterns": "Misses unusual or sign-based objects that CLIP finds; single-model reliance causes decreased recall on some queries.",
            "baseline_comparison": "ViLD-only vs CLIP-only vs Ensemble — ensemble yields best trade-off; multi-view fusion on top of ensemble provides further gains.",
            "ablation_results": "Using ViLD alone reduces detection of uncommon items; ensemble with CLIP recovers many of these misses.",
            "key_findings": "Combining a detection-oriented VLM (ViLD) with a more text-sign sensitive model (CLIP) yields complementary strengths for open-vocabulary scene representation; neither model alone suffices for robust embodied grounding.",
            "uuid": "e394.4",
            "source_info": {
                "paper_title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM Object-Proposal",
            "name_full": "LLM-based Object Proposal Module (few-shot prompting)",
            "brief_description": "An LLM few-shot prompting module that, given a free-form human instruction, proposes a set of relevant object names (including implicit and fine-grained references) to be queried against the NLMap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM Object-Proposal (PaLM-based few-shot)",
            "model_size": null,
            "model_description": "Implements few-shot prompts (examples provided) to elicit from an LLM the set of object names and references required to fulfill an instruction; designed to handle inference of implied objects, unstructured language, fine-grained descriptors, and decomposition to proper granularity.",
            "task_name": "Object proposal from natural language instructions",
            "task_description": "Translate an arbitrary human instruction (which may omit some nouns explicitly) into a structured list of candidate object names to be looked up in the NLMap and bound to executable actions.",
            "task_type": "language understanding; object-relational inference",
            "knowledge_type": "object-relational + procedural inference (infer which objects are relevant to achieving an instruction and what roles they play)",
            "knowledge_source": "LLM pretraining + in-context few-shot examples",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting with example instruction → object lists",
            "knowledge_representation": "output: list of object name strings (possibly with descriptors) used to query visual map; internal reasoning implicit in LLM weights",
            "performance_metric": "object-proposal success rate by category (instruction implication, unstructured, detailed description, proper granularity)",
            "performance_result": "Object-proposal success (PaLM-540B): instruction implication 92% (25 cases), crowd-sourced/unstructured 96% (25 cases), fine-grained descriptions 72% (25 cases), decomposition/proper-granularity 60% (15 cases).",
            "success_patterns": "Effectively infers objects implied by tasks and handles varied unstructured human utterances; preserves many fine-grained descriptors and supports multi-step decomposition in most cases.",
            "failure_patterns": "Often fails to expand or decompose categories to the precise granularity required (lowest performance on 'proper granularity'); sensitive to ambiguous or highly compositional descriptors.",
            "baseline_comparison": "Compared to simple noun-extraction heuristics (prior work), this few-shot approach is more demanding and yields higher recall on implied/unstructured cases (quantitative baseline numbers not present in paper).",
            "ablation_results": "Smaller LLM sizes reduce success rates substantially on granularity-sensitive tasks (see PaLM-62B, PaLM-8B results in paper Table II).",
            "key_findings": "LLMs can reliably propose relevant object sets from free-form instructions using few-shot prompts, providing the critical bridge from unstructured language to structured queries against perceptual maps; however, decomposition to precise granularity remains challenging and benefits from larger LLM scale.",
            "uuid": "e394.5",
            "source_info": {
                "paper_title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Multi-view Fusion",
            "name_full": "Multi-View Fusion Algorithm for NLMap",
            "brief_description": "An algorithm that aggregates top-k context elements (ROI observations) for a queried object by clustering them via KL divergence between Gaussian proxies of 2D projections, then producing weighted estimates of 3D object locations and confidence scores to filter false positives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multi-view fusion (NLMap clustering & aggregation)",
            "model_size": null,
            "model_description": "Takes top-k scored context elements per query (from ensemble metric D = max(CLIP, ViLD inner products), with each context element having (φ_i, p_i, r_i)), represents each as a 2D Gaussian N(p_i, α·r_i), clusters via KL divergence threshold λ, computes score-weighted averages for cluster centroids, boosts cluster confidence by multiplicity, and applies thresholds to decide object presence.",
            "task_name": "Aggregating multi-view observations into object instances and 3D locations",
            "task_description": "Given multiple ROI observations across viewpoints for a text query, fuse detections to produce a small set of object hypotheses with improved location estimates and aggregated confidence to reduce single-view noise.",
            "task_type": "perception; multi-view aggregation; spatial representation",
            "knowledge_type": "spatial (explicit 3D location inference) + object-relational (aggregated confidence and instance identification across views)",
            "knowledge_source": "geometric estimates from depth + per-ROI visual-language scores; algorithmic fusion (no new learning)",
            "has_direct_sensory_input": false,
            "elicitation_method": "deterministic clustering and weighted averaging (algorithmic), using KL divergence as a proxy for bounding-volume overlap",
            "knowledge_representation": "clusters of Gaussian proxies representing object instance locations and sizes; final outputs: fused 3D centroids, sizes and aggregate confidence scores",
            "performance_metric": "object presence detection accuracy (per-scene retrieval success), improvement over no-fusion baseline",
            "performance_result": "Adding multi-view fusion to the ensemble metric produced a modest +2% absolute accuracy in scene1 and a substantial +17% in scene2 (paper reports these figures), demonstrating its effectiveness in reducing view-specific false positives.",
            "success_patterns": "Effectively removes outlier high-scoring but inconsistent single views, consolidates multi-view evidence to improve recall/precision of object detection and location estimation.",
            "failure_patterns": "Still vulnerable to systematic false positives across views (e.g., repeated misdetections), and performance depends on initial ROI proposals and camera coverage; does not handle dynamic objects/humans (static assumption).",
            "baseline_comparison": "Ensemble (max(CLIP,ViLD)) without multi-view fusion is baseline; fusion substantially improves retrieval success in some scenes and provides more reliable location estimates.",
            "ablation_results": "No fusion (k=1) reduces robustness; using KL divergence clustering and multiplicity-based score bonuses are key design choices that improved results reported.",
            "key_findings": "Algorithmic multi-view aggregation that leverages geometric estimates and consistency across views materially improves reliability of open-vocabulary queries for embodied planning, reducing single-view noise that would otherwise mislead an LLM planner that lacks raw sensory access.",
            "uuid": "e394.6",
            "source_info": {
                "paper_title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can and not as i say: Grounding language in robotic affordances",
            "rating": 2,
            "sanitized_title": "do_as_i_can_and_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Open-vocabulary object detection via vision and language knowledge distillation",
            "rating": 2,
            "sanitized_title": "openvocabulary_object_detection_via_vision_and_language_knowledge_distillation"
        },
        {
            "paper_title": "Visual language maps for robot navigation",
            "rating": 2,
            "sanitized_title": "visual_language_maps_for_robot_navigation"
        },
        {
            "paper_title": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "rating": 1,
            "sanitized_title": "lmnav_robotic_navigation_with_large_pretrained_models_of_language_vision_and_action"
        }
    ],
    "cost": 0.021199,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Open-vocabulary Queryable Scene Representations for Real World Planning</p>
<p>Boyuan Chen 
Fei Xia 
Brian Ichter 
Kanishka Rao 
Keerthana Gopalakrishnan 
Michael S Ryoo 
Austin Stone 
Daniel Kappler 
Open-vocabulary Queryable Scene Representations for Real World Planning</p>
<p>Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io</p>
<p>I. INTRODUCTION</p>
<p>For robots to perform varied, real-world tasks, they must be able to comprehend diverse human commands and then act on these commands in the context of their environment. Imagine a robot in a home environment tasked with "water the plants in the living room". It has to first identify relevant objects and locations within the scene (e.g., the watering can, the sink, and each potential plant) and then plan over these objects in sequential order (get the watering can, then go the sink, and then fill it up), conditioning on its affordances (e.g., can it carry a full watering can), and conditioning on the scene (e.g., how many plants there are, and where are they). Semantic representation and downstream mobile manipulation planners capable of accessing this representation emerge as critical challenges in such a pipeline.</p>
<p>Semantic understanding is crucial for a robot to achieve long-horizon tasks in unstructured environments. Though a robot can avoid building a semantic representation by finding objects each time they are required, e.g., with Object Goal Navigation [1], [2], this repeated exploration can be inefficient. A persistent scene representation on the other hand avoids this exploration, but past works are generally limited to locating object categories known during the construction of the representation and may not encode the open-vocabulary objects that arise from human queries, such as in "bring me the purple unicorn plush toy". Recent progress in contrastively trained visual language models offers a promising solution to open-ended scene presentation. Contrastive Language-Image Pre-training (CLIP) [3] models are trained on imagelanguage associations and can provide open-vocabulary image understanding and object detection [4]. They have demonstrated impressive zero-shot classification performance and thus might be used to build a semantic representation in a zero-shot manner.</p>
<p>Another challenge lies in connecting the semantic scene representation to a planning algorithm that is capable of acting We propose an open-vocabulary and queryable scene representation for real-world planning. A queryable scene representation is built from exploration. When the system receives a user query, it uses an LLM-based object proposal module to propose relevant objects to query the map. The returned object presence and location are used for LLM-based planning. We benchmark the method on robots from Everyday Robots. upon it. Recent progress in large language models (LLMs), has shown impressive few-shot performance in language comprehension, semantic understanding, and reasoning, as well as application to robotics problems like planning [5]- [7] and instruction following [8]. Using such models in embodied settings can provide significant challenges, most critically because LLMs are not grounded in the physical world. For example, [5] pioneers in using LLMs for planning, but it has no grounding in environmental context. In contrast, SayCan [6] showed how value functions of learned skills can provide such a grounding through selecting options scored highly by a language model and an affordance model. However, this is limited by the options provided and hardcoded knowledge of where objects exist.</p>
<p>In this work, we introduce Natural-Language Map (NLMap), a flexible and language-queryable spatial semantic representation based on visual-language models including ViLD and CLIP and integrate with SayCan. We show that NLMap grounds LLM-based planners in their environments, significantly improves long-horizon planning via natural language instructions in the open-world domain, and enables new tasks prior state-of-the-art algorithms failed to address. To summarize, we make the following contributions:  The key design of NLMap is to establish a queryable map. First, the agent explores the scene and provides a class-agnostic bounding box proposal based on objectness. We extract 512d CLIP features and 512d ViLD features of each bounding box and represent them as
a feature point cloud C ={(φ i ,p i ,r i )} i=1...N .
When queried with a piece of text, we visualize the heatmap of matches based on the alignment of text and visual features. Note that we can query with a single object name, or object families, such as "snack" or "fruit".</p>
<p>II. RELATED WORK</p>
<p>Semantic Scene Representations. Scene representation is a central theme in robot perception and planning. Semantic SLAM [9]- [11] is an augmentation over traditional SLAM, it assigns semantic features over geometric features provided by SLAM (points, lines, planes). Many representations are proposed, ranging from a faithful 3D recontruction [12] of the environment, to more object-centric ones [13], [14], such as object detection bounding boxes [15] and 3D bounding boxes [16]. Recently, topological maps [17], [18] and scene graphs [19], [20] emerge as an effective discrete representation of scenes. One issue with those representations is that they cannot be queried with natural language. Interfacing with those scene representations requires reducing the object set to a closed set, indicating that they are not as useful for LLM-based planners and that they are limited in an open-vocabulary setting. In contrast, our work allows the scene representation to be queried at test time with natural language. Concurrent work VLMaps [21] also explores this concept, by fusing pretrained visual-language model features into a geometric reconstruction of the scene. The representation is then used for visual-language navigation tasks via program synthesis. Object Goal Navigation. There is also a significant body of related work on object navigation, which focuses on flexible exploration to find objects in unknown scenes. A few of these algorithms construct a semantic map of the current region before planning in that region [1], [22]- [24]. Map-based methods are modular and interpretable and hence easier to deploy in the real world. Other algorithms [25]- [30] do not require a map and can decide where to go based directly on the current observations and memories, without maintaining a global representation of the environment. Recently, methods that leverage pre-trained image-text models can do zero-shot Object Goal Navigation [31], [32]. CoW [32] performs zero-shot object goal navigation by leveraging CLIP. LM-Nav [8] uses three pretrained model to perform visual language navigation. Our work differs from Object Goal Navigation since the eventual goal is not purely finding objects, but using object presence and location information for planning. Our work can use the representation from a single exploration for many downstream planning tasks without the need to run Object Goal Navigation every time. Planning with Scene Representations. In task and motion planning, scene representations are often composed of predicates compatible with symbolic planners [33], [34]. Recent progresses attempt to build a symbolic and geometric scene graph to facilitate task and motion planning [35]. However, they still require defining the objects in the scene. Recently LLM-based planners are more flexible [6], [7], [36] and do not require handcrafting predicates, however, they do not handle the complexity of open-vocabulary object proposal and require defining a set of objects involved in planning. They also fail to integrate perception in real robot experiments due to the difficulty of connecting unstructured natural language instruction to perception algorithms that need structured inputs.</p>
<p>III. PROBLEM STATEMENT</p>
<p>In this work, we aim to efficiently fulfill high-level, naturallanguage instructions, such as "Bring me a snack" or "I spilled my coffee, can you help?". This requires a robotic system to solve problems at the intersection of natural language comprehension, scene understanding, task planning, navigation, and manipulation. Recent work, SayCan [6], has shown how large language models can be applied to such problems through world-grounding affordance functions, allowing LLMs to understand what a robot can do from a state. However, SayCan did not provide scene-scale affordance grounding, and thus cannot reason over what a robot can do in a scene. To that end, we address two core problems (i) how to maintain open-vocabulary scene representations that are capable of locating arbitrary objects and (ii) how to merge such representations within long-horizon LLM planners to imbue them with scene understanding.</p>
<p>IV. NLMAP + SAYCAN</p>
<p>We provide a high-level description of our algorithm in Listing 1. The design of each component is described below:</p>
<p>A. Scene Representation</p>
<p>The scene representation is generated from an exploration phase of the unstructured scene, which our approach is agnostic to, but could be for example frontier exploration [37] or pre-determined waypoints. During this exploration, NLMap runs a class agnostic region proposal network as in ViLD [4] on all the observed RGB images. For each proposed region of interest (ROI) I i ∈I 1...N , our method uses an ensemble of VLM image encoders Φ 1...M [3], [4] to extract image embeddings φ i = [Φ j (I i )| j ∈ 1...M]. As shown in Fig. 2, such embedding can be queried with text at plan time since VLMs are capable of estimating the correlation between texts and images. In our setup we leverage CLIP [3] and ViLD [4] as visual encoders
φ i = [Φ clip img (I i ),Φ vild img (I i )],
where image-text-alignment is scored with inner product of image feature and CLIP text feature. We also extract the estimated location p i = (x i ,y i ,z i ) using depth at the center of the image as well as estimated size r i of the object in I i . Defining the tuple c i = (φ i , p i , r i ) as a context element, the collection C ={c i } i=1...N forms our scene representation.</p>
<p>B. Querying the Representation</p>
<p>To complete a task specified by human instruction, the robot will query the scene representation for relevant information. This is achieved by first parsing natural language instruction into a list of relevant object names, then using the names as keys to query object locations and availability. Finally, we generate executable options based on what's found in the scene, then plan and execute as instructed. 1) Object proposal: The core challenge of querying scene information is bridging unstructured natural language input and structured representations. In order to decide what objects to look up in the scene representation, we use few-shot prompting to let LLM actively propose required objects given an instruction. Different from previous work [8] that uses LLM to extract names from a sentence, our object proposal is much more demanding in four different ways as we will discuss in Sec. V-B.</p>
<p>In order to achieve a reliable object proposal that addresses four requirements, we introduce example prompts for each case and use the few-shot prompting technique of LLMs to propose them. The few-shot examples can be found on our project website.</p>
<p>2) Object Query: Given a list of object names {y i } i=1...O , we then query the scene representation for object locations and availability. This is achieved by finding top k nearest neighbor elements in C followed by a clustering algorithm to fuse multiview information. A threshold on a cluster's score determines if the queried object is found.We first define a metric D :C×Y →R where Y is the set of possible object names. We use the maximum ensemble of both CLIP and ViLD for the metric D defined below:
D :(φ i , p i , r i ), y i →max(D clip ,D vild ), where D clip = Φ clip img (I i ),Φ clip text (I i ) D vild = Φ vild img (I i ),Φ clip text (I i )
Here we use both CLIP embedding and ViLD embedding because the former detects out-of-distribution objects better while the latter is more robust to common objects as shown in Fig. 5. We can directly take the maximum over the two inner products because both of them are normalized vectors designed to be queried by the inner product CLIP text encoder. Given metric D, the top k nearest neighbor elements for object name y i can be found in the scene representation C. We note that based on the value of D(c i ,y i ), we can impose a threshold to filter out low-confidence detections. These top context elements are associated with ROIs, multiple of which may correspond to the same real-world 3D object instance. We then run a multi-view fusion algorithm to aggregate these context elements into 3d object locations and filter out objects that don't exist according to an aggregated score. Details of the algorithm can be found in Sec. VI-B.</p>
<p>C. Combining NLMap and SayCan</p>
<p>Our method constructs a scene representation queryable by natural language. Such representation can be connected with LLMbased planners to enable robots to operate in a truly uncontrolled environment. Previously, SayCan [6] presents a framework that allows robots to plan and execute in the real world following human instructions. We highlight the difference between our work and SayCan in Fig. 3. SayCan work as follows: with few-shot prompting, SayCan uses the scoring of a language model to break down a high-level instruction like "Bring me an apple" to "1. Find the apple, 2. Pick up the apple, 3. Bring it to you, 4. Put down the apple". Each option from a pre-defined list is scored by an LLM and an affordance prediction module. However, SayCan relies on a hardcoded list of object names, locations, and executable options so its capability is largely limited by the lack of contextual grounding.</p>
<p>NLMap makes up this missing component in SayCan. Our object proposal, combined with the object query, generates the relevant object names and locations conditioned on the instruction and the scene. There are two major remaining challenges.</p>
<p>1) Generate executable options: Vanilla SayCan [6] provides a list of skills associated with either 1) navigation policies to hard-coded locations 2) manipulation policies (pick and place) of objects, specified by object names. Given a detected object and its location, we can create a new skill "find the [object name]" bound to a navigation policy to that location. This means we can expand a small fixed set of navigation options to infinitely many options. On the other hand, although training manipulation policies for infinitely many objects is beyond the scope of our work, we can still augment the manipulation capability of SayCan by binding all possible references to a manipulable object with the available manipulation policies. This is achieved by finding CLIP nearest neighbor of object names. For example, given discovered objects, we can generate executable options like "pick up the red can" and "pick up a tin of coke". Our method will bind both of them to the closest manipulation policy "pick up coke can" with CLIP. This nearest neighbor query is similar to that used with BERT in [5]. Combine score for generated options Combine score for pre-defined options instruction like "Bring me an apple" to "1. Find the apple, 2. Pick up the apple, 3. Bring it to you, 4. Put down the apple". Each option from a pre-defined list is scored by an LLM and an affordance prediction module. It natural failed on "water the plant" task since plant is not contained in the pre-defined options.NLMap + SayCan Our method generates a list of relevant objects based on instruction with an LLM module, and then queries the NLMap to filter the object list and get object locations. A list of options is generated based on a template find/pick/place object, and then LLM-based planning module plan over these options.</p>
<p>2) Ground LLM planner with context: Unlike the setup in SayCan, which assumes all objects in the hard-coded list are present, our method is expected to tackle infeasible instructions, such as instructions involving objects that aren't present. SayCan weakly addresses this problem by grounding plans with local affordance, which is only conditioned on what's directly visible in the field of view rather than what's available in the entire scene. NLMap gives us a list of available objects so we can add the missing global contextual grounding to SayCan. This is achieved by modifying the original few shot prompts in SayCan to also condition the plan on discovered objects, expressed in templates like "Scene: apple, coke can." We include both positive examples when necessary objects are all present and negative examples when available objects cannot fulfill the instruction. In the former case, LLM is prompted to plan just like in vanilla SayCan; In the latter case, LLM is prompted to output the terminate signal "done" directly, indicating the task is infeasible.</p>
<p>With these components, we can ground SayCan with context awareness. After exploring the scene, when a human gives the robot an instruction, the robot will propose potentially involved objects in the scene and query the gathered scene representation for their locations and availability. NLMap then generates executable options, plans with LLM conditioned on what's found and finally executes the plan in the real world under the SayCan framework.</p>
<p>V. EXPERIMENTS</p>
<p>In this section, we evaluate NLMap and its individual components with real-world robotics tasks. We test a robot running NLMap in a real office kitchen, as shown in Fig. 4. We test the entire system in an end-to-end setting such that the robot attempts to accomplish tasks specified by humans with natural language. We list a subset of the manipulable objects in Fig. 4(a) receptacle locations in Fig. 4(b). The robot is a mobile manipulator from Everyday Robot, which has a mobile base and a 7-degree-offreedom arm, as shown in Fig. 4(c)  camera, which returns 640×512 RGBD images. Similar to Say-Can, we use a set of manipulation policies trained from imitation learning and PaLM 540B [38] as the LLM for all experiments, due to its good performance on new tasks with few-shot prompting. Throughout this section, all experiments share the same set of hyper-parameters and LLM prompts unless specified otherwise.</p>
<p>A full list of test instructions can be found on the project website.</p>
<p>A. Benchmarking NLMap + SayCan as a system</p>
<p>In this section, we demonstrate our natural language queryable representation can be combined with LLM planners to significantly augment the capability of real robot operation. We choose to combine NLMap with SayCan, a recent work that uses LLM planners to let robots plan and execute according to natural language instructions. One of the biggest limitations of SayCan, as stated in Sec. III, is that it has no global context awareness. By combing our method with SayCan using the method described in Sec. IV-C, we free SayCan from a fixed, hard-coded set of objects, locations, or executable options. With NLMap, SayCan can now perform a great number of previously unachievable tasks. In addition, we demonstrate that our method allows SayCan to plan with the global context to identify infeasible tasks. We quantitatively evaluate the real robot performance of NLMap + SayCan in Table I with three sets of benchmarks. We compare our method with a privileged version of SayCan, which uses ground truth perception results in the scene.</p>
<p>1) SayCan tasks: We hope to understand how much performance will be lost compared to SayCan due to the addition of perception and context-aware planning. Therefore, we benchmark 18 tasks adopted from 6 of the 7 task families from the original SayCan paper with 3 random tasks from each family (except for Embodiment family). Our method achieves a success rate of 66.7% among these tasks compared to the 84% of privileged SayCan. We also tried 2 tasks with deliberate typos 'ppsi" 'chpis". Our method failed in both instructions with typos, with one failure during object proposal and one failure due to policy binding. With these two typo experiments included, our method achieves an overall success rate of 60% compared to 65% in real robot experiments compared to privileged SayCan that has hard-coded object locations. This shows our NLMap maintains a reasonable overall success rate even if multiple components like object proposal, perception, and context-conditioned planning are added.</p>
<p>2) Novel objects: SayCan relies on a hard-coded list of object names, locations, and executable options. Since the hard-coded set of objects and executable options are finite, SayCan is incapable of performing tasks that involve objects or skills outside these small sets. However, since NLMap can propose and detect objects, and generate executable options itself, NLMap can be combined with SayCan to execute infinitely many tasks that involve such novel objects as described in Sec. IV-C. As shown in Table I, SayCan fails to plan nor execute any of these tasks while our method achieves a success rate of 80% in the end-to-end execution experiment. It even succeeds in some very out-of-distribution instructions such as "I want to watch TV, can you get a bottle of tea and put it there" or "Show me where is the first aid station". We note that manipulation policies used in this project are still limited to be with the objects that are visually similar to training objects in [6] and rely on the generalization to slightly out-of-distribution data. Therefore, the novel object names in this experiment are either used for navigation only, or for describing objects that are visually similar to training objects in [6]. Such constraint can be lifted in the future when a general text-conditioned manipulation policy is available but lifting it is beyond the scope of the project.</p>
<p>3) Missing Objects: Vanilla SayCan isn't grounded by what's available in the scene. If a necessary object is removed from the scene, there is no way for SayCan's LLM planner to tell the task is infeasible. With NLMap, we can use the method in Sec. IV-C to condition SayCan planning on what's actually detected. In this benchmark, we ask NLMap + SayCan to perform tasks that require objects not present in the scene. Instructions in the benchmark consist of size 15 subset of all instructions in the "novel object" benchmark since we cannot remove objects like "first aid station" from the wall. In a successful run, the robot is expected to not detect an object doesn't exist and output a termination signal immedi-   ately in its plan. Our method achieves a success rate of 40% in the missing object setting, where 56% of the total failure cases are due to false positive detections. Although vanilla SayCan will achieve a success rate of zero in comparison, this benchmark still indicates false positive detection is a challenge for context-aware planning.</p>
<p>B. Benchmarking Object Proposal</p>
<p>Object proposal is a foundational component in our framework to parse unstructured instructions into structured object names. We investigate the robustness and generalization capability of object proposal from four perspectives:</p>
<p>• Infer objects from implication of the instruction: e.g. "Heat up the taco" (taco, microwave) • Unstructured crowd-sourced instructions: e.g. "Redbull is my favorite drink, can I have a one please?" (redbull, human) • Objects with fine-grained description: e.g. "turn off the macbook with yellow stickers" (macbook with yellow stickers) • Decomposition to proper granularity: e.g. "check out what types of ingredients are available to cook a luxurious breakfast" (milk,eggs,bacon,bread,butter,cheese,ham,sausage...) A summary of result of each perspective can be found in Table II. 1) Infer objects from implication of the instruction: In previous work [8] that use LLM to extract object names from language, all object names are nouns that are directly present in the language input. However, in the real world, humans frequently give instructions that involve objects that have to be inferred from the implication of the task. We test object proposal on 25 such instructions and evaluate whether proposed objects would complete the task. Object proposal achieved a success rate of 92% in 25 test cases including "season the steak (salt, pepper)", "fillet the fish (fish, knife)".</p>
<p>2) Unstructured crowd-sourced instructions: Object proposal module is expected to take in instructions from a variety of highly unstructured formats. We evaluate the robustness of our object proposal on a set of 25 test instructions adopted from crowd-sourced instructions for SayCan. Object proposal achieved a success rate of 96% in this study, including multi-step tasks like "Move an multigrain chips to the table and an apple to the far counter". Object proposal succeeded in all 8 out of 9 multi-step tasks in this study.</p>
<p>3) Reference to objects with fine-grained description: Human instructions often involve reference to objects with fine-grained descriptions. Such descriptions are often important to visually identify a particular instance in the scene. Thus it's important for the object proposal to keep these fine-grained descriptions in its output. We evaluate object proposal on 25 test instructions that involve fine-grained descriptions by adjectives or clauses. The model attains a success rate of 72% in this experiment. The model even succeeded in some complicated descriptions like "mug in the shape of a donut". 4) Decomposition to proper granularity: Many instructions require a different level of object proposal granularity. Certain tasks can only be accomplished if the object proposal is more fine-grained. We evaluate object proposal on 15 tasks that require expanding a category mentioned in the instruction. Overall, the object proposal achieves a success rate of 60% in this set, indicating that proper granularity is still a hard challenge for LLM due to its multi-modality nature.</p>
<p>C. Benchmarking Object Queries to NLMap</p>
<p>In this section, we evaluate the open-vocabulary object query module on a list of 50 common objects in our testing kitchens. We run robot exploration and object query in two different kitchen scenes, each with some object deliberately missing. Our method uses both maximum ensemble metric D and multi-view fusion described in Sec. IV-B with k =4. We compare this choice with alternative embeddings and metrics like D clip or D vild . Maximum ensemble metric D without multi-view fusion is also evaluated as a baseline. We have k = 1 in the above three baselines since no multi-view fusion is happening. As shown in Table III, ViLD and CLIP embedding alone achieves a very low success rate in both environments. As illustrated in Fig. 5, we observe that ViLD embedding detects common objects like cans or apples more reliably while suffering from false negative detection of out-of-distribution objects such as "first aid station". On the other hand, CLIP embedding gives us better results on uncommon objects but is less robust for basic objects. Additionally CLIP embeddings better captures features of text and signs. Our method uses multi-view fusion in addition to the maximum ensemble. Multiview fusion leads to a slight 2% accuracy increase in scene 1 but a significant 17% increase in the second scene. This shows that multi-view fusion can help remove outlier observations that produce high likelihood scores but are actually noise by noticing a lack of detection of it from different views. Overall, the perception success rate for our method is 82% and 64% respectively in the two kitchens. Such accuracy is limited by the low resolution and exposure of our robot camera. However, since instructions don't always contain visually ambiguous objects like many in these test queries, perception is still reliable enough as we see in the real robot experiments Sec. V-A.</p>
<p>D. Benchmarking Context Grounded Planing</p>
<p>Failures from perception or object proposal are coupled with planning in real robot experiments. In this section, we ablate context-aware LLM planning as a standalone component, assuming correct object proposal and detection. We test LLM  planning in a generative way. A generated plan is considered correct if it will accomplish the instruction, is consistent with the available objects, and is executable. We benchmark generative planning with 80 test cases consisting of 40 instructions with 2 set of available objects for each. One set is a positive set that contains all needed objects for the task while the other set is a negative set with some necessary objects missing. To be considered successful, the planner should behave like Vanilla SayCan in the positive set while outputting the terminal signal immediately in the negative set. Our LLM planner, conditioned on available objects using the method described in Sec. IV-C, achieves a success rate of 85% and 60% on the 40 instructions with positive object set and negative set respectively. The performance gap is expected because negation is known to be a hard problem for LLM.  Fig. 6: Execution trajectory of proposed method on task "Compost the apple".</p>
<p>Note CLIP features allow the robot to understand the sign on the compost bin. The images are from the onboard camera of a robot from Everyday Robots.</p>
<p>VI. CONCLUSIONS We integrate NLMap, a flexible and queryable spatial semantic representation based on visual-language models including ViLD and CLIP with SayCan. We show that NLMap is a flexible scene representation that grounds LLM-based planners in their environments, significantly improving long-horizon planning via natural language instructions in open-worlded domain, enabling new tasks prior state-of-the-art algorithms failed to address. Future work. Currently, NLMap only handles a static scene representation without dynamic objects and human, which we will leave this for future work. All the modules used in NLMap + SayCan is pre-trained and deployed zero-shot. It is a great advantage but we hope to fine-tune them for better performance. Additionally, we will look into efficient exploration algorithms to speed up the creation of scene representation.</p>
<p>APPENDIX</p>
<p>A. Context-aware SayCan Algorithm</p>
<p>Our context-aware SayCan algorithm is similar to [6], it expands the last line LLM.plan(instruction, scene objects) in Listing 1. Compared to the original SayCan [6], our context-aware version needs a list of detected object names M, along with a list of template functions F as extra input. A template function maps an object name to an option name such as x −→ "pick up [x] . We note that the template function is used here because training manipulation policies beyond pick-and-place are beyond the scope of our project. If we have a language-conditioned policy in the future, we don't need to use template functions anymore. Trusting LLM for new options will suffice in that case. A full pseudo-code can be found in Algo 1.</p>
<p>Alg. 1 Context-Aware SayCan Execute π n (s n ) in the environment, updating state s n+1 21:</p>
<p>n=n+1</p>
<p>B. Multi-view fusion algorithm</p>
<p>In this section, we describe details of the multi-view fusion algorithm mentioned in Sec. IV-B. In the gathered scene representation C, multiple context elements may be associated with the same object. Each context element c i contains an estimation of object centroid p i and along with a object width r i . To simplify formulation, we use cylindrical bounding volumes to model 3d objects. We create such bounding boxes with center p i and radius r i in an upright position. Given each queried object name y i , we can quickly narrow down bounding box candidates by finding the top k nearest neighbors with metric D. We now have a problem similar to post-processing in object detection -for each real object instance, we may have overlapping bounding box predictions, which are supposed to be aggregated together. In computer vision, this is achieved by the NMS algorithm that group predictions based on the intersection over union(IOU) of the bounding box followed by keeping only the bounding box with the highest confidence in each group. We made three major changes to the NMS algorithm by noticing the special structure of our problem.</p>
<p>First, since our bounding volumes are not cubes, IOU is hard to compute. We instead use KL divergence of Gaussian distributions to model. For each cylindrical bounding box (p i , r i ) with a circular projection on the 2d plane, define Gaussian distribution G i = N (p i ,α · r i ). The 2d Gaussian will have its center at the estimated centroid and standard deviation proportional to the width of the object. KL divergence measures how different two distributions are so it acts like the IOU for gaussian distributions. When estimations have very different centers or sizes, they will be considered to correspond to two different object instances by our algorithm. Second, different from the setup in 2d object detection, different estimations of the same object in our problem are considered valid, independent data points that contribute to a better estimation of object location. Therefore, we don't discard non-maximum estimations in each clustered group, but rather use their score as importance weights to derive the final estimation through weighted average. Third, bounding boxes are directly filtered out based on a threshold on confidence score in 2d detection. In our setup, we give confidence scores a bonus based on how many elements there are by noticing available objects should be detected from multiple view points.</p>
<p>We then offer a formal algorithm box for multi-view fusion in Algo 2. Given object name y, we can use metric D to score each context element in C and find the top k ones. Denote the indices of top k context elements as K, sorted in descending order by score. For each context element c i =(φ i , p i , r i ) , define Gaussian distribution N i = N (p i ,α·r i ). In our experiments, we choose the monotonic increasing function f to be in the form f(x)=1+t− t x where t is some hyper-parameter.</p>
<p>Alg. 2 Multi-view Fusion in NLMap 1: Input: Sorted indices K, Scores S for context elements, Gaussian distributions for context elements N, KL threshold λ, score threshold β, monotonic increasing function f. if ∀ G ∈ Groups, ∀z ∈ G , i =z∧j =z then 6: if KL(N i , N j )&gt;λ then if G[0]·f(|G|)&gt;β then 13: P.append( i∈G piexp(Si) i∈G exp(Si) ) 14: return P The algorithm then outputs clustered locations for objects queried by name y.</p>
<p>C. Prompt used for object proposal and for planning</p>
<p>Listing 2: Object proposal prompt in NLMap + SayCan.</p>
<p>The task 'hold the snickers' may involve the following objects:snickers. The task 'wipe the table' may involve the following objects: table, napkin, sponge, towel. The task 'put a water bottle and an oatmeal next to the microwave' may involve the following objects:water bottle, oatmeal, microwave. The task 'place the mug in the cardboard box' may involve the following objects:mug, cardboard box. The task 'go to the fridge' may involve the following objects:fridge. The task 'put a grapefruit from the table into the bowl' may involve the following objects:grapefruit, table, bowl. The task 'can you open the glass jar' may involve the following objects:glass jar. The task 'heat up the taco and bring it to me' may involve the following objects:taco, human, microwave oven, fridge. The task 'hold the fancy plate with flower pattern' may involve the following objects:fancy plate with flower pattern. The task 'put the fruits in the fridge' may involve the following objects:fridge, apple, orange, banana, peach, grape, blueberry. The task 'get a sponge from the counter and put it in the sink' may involve the following objects:sponge, counter, sink. The task 'empty the water bottle' may involve the following objects:water bottle, sink. The task 'i am hungry, give me something to eat' may involve the following objects:human, candy, snickers, chips, apple, banana, orange. The task 'go to the trash can for bottles' may involve the following objects:trash can for bottles. The task 'put the apple in the basket and close the door' may involve the following objects:apple, basket, door. The task 'help me make a cup of coffee' may involve the following objects:cup, coffee, mug, coffee machine. The task 'check what time is it now' may involve the following objects:clock, watch. The task 'let go of the banana' may involve the following objects:banana, trash can. The task 'put the grapes in the bowl and then move the cheese to the table' may involve the following objects: grape, bowl, cheese. The task 'find a coffee machine' may involve the following objects:coffee machine. The task 'clean up the spilled coke' may involve the following objects:spilled coke, towel, mop, napkin, sponge. The task 'bring me some soft drinks' may involve the following objects:human, pepsi, coke, sprite, fanta, 7 up. The task 'boil some water' may involve the following objects:</p>
<p>water, kettle, sink, tap. The task 'wash the dishes' may involve the following objects: sink, tap, mug, plate, bowl, fork, spoon, knife. The task 'place a knife and a banana to the table' may involve the following objects:knife, banana, table.</p>
<p>Fig. 1 :
1NLMap + SayCan overview.</p>
<p>Fig. 2 :
2Natural Language Queryable Scene Representation.</p>
<p>Fig. 3 :
3Comparson of NLMap + SayCan with SayCan SayCan: With few-shot prompting, SayCan uses the scoring of a language model to break down a high-level</p>
<p>Fig. 4 :
4(a) a representative subset of objects that are used in manipulation (b) a representative subset of objects used as receptacles (e.g. for the task putting the cup next to the coffee machine) (c) a robot from Everyday Robots used in the experiment (d) The scene where we run the experiments, it is a kitchen in an office building.</p>
<p>1 )
1We propose an open-vocabulary, queryable semantic representation based on ViLD and CLIP. 2) We integrate NLMap into a language-based planner to enable grounding on the context. 3) We benchmark NLMap + SayCan in a real-world kitchen, showing it is capable of performing 55 tasks at 61.8% success rate. Notably, 35 of these tasks are impossible with previous state-of-the-art planners that do not have access to NLMap. arXiv:2209.09874v2 [cs.RO] 15 Oct 2022Context elements </p>
<p>Query: napkin box 
Query: tap 
Query: apple 
Query: fruit </p>
<p>Listing 1: High-level description of NLMap + SayCan algorithm. Note we only need to build scene representation once for each scene.Input: instruction 
if is_new_scene(): </p>
<h1>construct queryable scene representation</h1>
<p>rgbd_images = robot.scene_explore() 
bboxes = roi_proposal(rgbd_images) 
positions, sizes = extract_3d(rgbd_images, bboxes) 
phi = VLM.encode_image(rgbd_images, bboxes) 
nl_map = Context(phi, positions, sizes) 
save_nl_map(nl_map) 
else: 
nl_map = load_nl_map() </p>
<h1>extract relevant objects via LLM</h1>
<p>objects = LLM.object_proposal(instruction) </p>
<h1>extract text features</h1>
<p>queries = VLM.encode_text(objects) </p>
<h1>query the nl_map</h1>
<p>object_scores = queries.dot_product(nl_map.Phi) 
object_presence, locations 
= multiview_fusion(object_scores, nl_map) 
scene_objects = objects.filter_by(object_presence) </p>
<h1>planning with scene objects information</h1>
<p>LLM.plan(instruction, scene_objects) </p>
<p>. The main sensor is an RGBDcoke can 
7up can 
pepsi can 
lime soda 
redbull can 
multigrain 
chips </p>
<p>rice chips 
tea 
jalapeno 
chips </p>
<p>water bottle 
apple </p>
<p>Frontal view, 
Pre-manipulation pose </p>
<p>RGBD image, 640 x 512 </p>
<p>sink 
First-aid station </p>
<p>Woven basket 
Coffee machine </p>
<p>(a) 
(b) </p>
<p>(c) 
(d) </p>
<p>TABLE I :
IPlanning and execution success rate. NLMap +SayCan shows comparable performance as SayCan on instructions from[6] while enabling new tasks SayCan cannot do before due to its lack of contextual grounding. Planning success rate for NLMap + SayCan refers to that of generative planning. ( * SayCan uses privileged ground truth perception information, thus not able to handle objects out of the pre-defined list.)Task Family 
PaLM540B [38] PaLM62B PaLM8B </p>
<p>Instruction implication 
0.92 
0.84 
0.72 
Crowd-sourced 
0.96 
0.96 
0.72 
Detailed description 
0.72 
0.8 
0.6 
Proper granularity 
0.6 
0.2 
0.133 </p>
<p>TABLE II :
IIObject proposal achieves a very high success rate for all task 
families except the hardest set "proper granularity". The performance on 
task family "proper granularity" sharply declines when we use smaller 
models while other tasks families witnessed minor decline. </p>
<p>TABLE III :
IIIWe ablate different object query methods in two real-world scenes. Both ViLD and CLIP achieve low query success rate but the ensemble of their maximum score as well as our multi-view fusion algorithm provides a significant boost to the query success rate.CLIP embedding </p>
<p>ViLD embedding </p>
<p>Ensemble </p>
<p>landfill 
napkin box 
jar of 
white candy </p>
<p>jar of 
dried fruit 
tap 
box of tea </p>
<p>Queries: </p>
<p>Fig. 5: Comparison of different RoI retrieval method. We ablate using different </p>
<p>features to retrieval RoIs with natural language and found there are unique failure 
cases with either CLIP or ViLD features, while maximum ensemble of features 
provide the best results. </p>
<p>1 :
1Input: A high level instruction i, a list of detected scene object names M, a list of template functions F, state s 0 , and a set of skills Π and their affordance functions V Π along with their language descriptions d Π . 2: l A ← ["done"] 3: translate←{} 4: for o ∈M do 5: for f ∈F do π nn ←argmax π∈Π clip(d π ),clip(f(o)) translate[f(o)] = π nn Bind options to policies 9: n←1 10: while An−1 ="done" do for a∈A and a ∈ A do 13: π =translate[ a ] =p( a |i,M, an−1 ,..., a1 )6: </p>
<p>A .append(f(o)) 
Create executable options </p>
<p>7: </p>
<p>8: </p>
<p>11: </p>
<p>Q=∅ </p>
<p>12: </p>
<p>14: </p>
<p>q LLM </p>
<p>a </p>
<p>LLM score </p>
<p>15: </p>
<p>q affordance </p>
<p>a </p>
<p>=V π (s n ) 
Affordance </p>
<p>16: </p>
<p>q combined </p>
<p>a </p>
<p>=q affordance </p>
<p>a </p>
<p>q LLM </p>
<p>a </p>
<p>17: </p>
<p>Q=Q∪q combined </p>
<p>a </p>
<p>18: </p>
<p>a n =argmax a∈A Q </p>
<p>19: </p>
<p>π n =translate[ an ] </p>
<p>20: </p>
<p>2 :
2Groups ←[[K[0]]] 3: for i ∈K do4: </p>
<p>for j ∈K do </p>
<p>5: </p>
<p>ACKNOWLEDGEMENTSSpecial thanks to Arjun Majumdar, Andy Zeng and Karol Hausman for helpful discussions; we thank Peng Xu and Xiran Liu for helpful feedbacks on writing.Listing 3: Task planning prompt in NLMap + SayCan.Robot: Hi there, I'm a robot operating in an office kitchen. You can ask me to do various tasks and I'll tell you the sequence of actions I would do to accomplish your task. Human: Hold the snickers Available objects are: snickers. Explanation: Hold on means to pick it up. I will pick up the snickers. Robot: 1. pick up the snickers 2. done. Human: Put the trailmix here Available objects are: trailmix.Explanation: This is a single action of puting down the trailmix. Robot: 1. put down the trailmix 2. done.Human:Put a water bottle and an oatmeal next to the microwave Available objects are: water bottle, oatmeal, microwave.Explanation: I will find and move a water bottle to the microwave , then find and move an oatmeal to the microwave. Robot: 1. find the water 2. pick up the water 3. go to microwave 4. put down the water 5. find the oatmeal 6. pick up the oatmeal 7. go to the microwave 8. put down the oatmeal 9. done. Human: Put the mug in the cardboard box Available objects are: . Explanation: There is no mug nor cardboard box available so I will do nothing. Robot:1E. Robot experiment task listListing 8: Task List used in experiment. The scene setup is the same as in SayCan[6].put the coke can in the your gripper let go of the coke can come to the table deliver the red bull to the close counter throw away the water bottle put the apple back on the far counter bring me something to quench my thirst bring me a fruit bring me a bag of chips from close counter pick up the 7up and bring it to me pick up the water bottle and move it to the trash pick up the apple and move it to the far counter Please, move the pepsi to the close counter Would you throw away the bag of chips for me Redbull is my favorite drink, can I have one please Can you throw away the apple and bring me a coke How would you bring me an apple, a coke, and water bottle I just worked out, can you bring me a drink and a snack to recover? Please, move the ppsi to the close cuonter Would you throw away the bag of chpis for me Listing 9: Task List used in experiment, the scene set up is an office kitchen full of objects, plus testing objects: multigrain chip, basket, plant, sink, apple, first aid station, coke, sink, rice chip bag, coffee machine, water bottle, paper cup, lime sparkling water(green), yellow sign, snack jars of nuts, snack jar of dried fruits, snack jar of gums, snack jar of candy, mug, water fountain machine, tv, tea bottle, box of tea, energy cup, paper bowl, clip board, compost bin.F. Additional qualitative experiment resultsWe show additional qualitative experiment results inFig. 7,Fig. 8
Object goal navigation using goal-oriented semantic exploration. D S Chaplot, D Gandhi, A Gupta, R Salakhutdinov, Neural Information Processing Systems (NeurIPS). 2020D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdinov, "Object goal navigation using goal-oriented semantic exploration," in In Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>On evaluation of embodied navigation agents. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.06757arXiv preprintP. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva et al., "On evaluation of embodied navigation agents," arXiv preprint arXiv:1807.06757, 2018.</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable visual models from natural language supervision," in International Conference on Machine Learning, 2021.</p>
<p>Open-vocabulary object detection via vision and language knowledge distillation. X Gu, T.-Y Lin, W Kuo, Y Cui, arXiv:2104.13921arXiv preprintX. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, "Open-vocabulary object detection via vision and language knowledge distillation," arXiv preprint arXiv:2104.13921, 2021.</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, arXiv:2201.07207arXiv preprintW. Huang, P. Abbeel, D. Pathak, and I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," arXiv preprint arXiv:2201.07207, 2022.</p>
<p>Do as i can and not as i say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N Joshi, R Julian, D Kalashnikov, Y Kuang, K.-H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, M Yan, A Zeng, arXiv:2204.01691in arXiv preprintM. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng, "Do as i can and not as i say: Grounding language in robotic affordances," in arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, N Brown, T Jackson, L Luu, S Levine, K Hausman, B Ichter, arXiv:2207.05608in arXiv preprintW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter, "Inner monologue: Embodied reasoning through planning with language models," in arXiv preprint arXiv:2207.05608, 2022.</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. D Shah, B Osinski, B Ichter, S Levine, arXiv:2207.04429arXiv preprintD. Shah, B. Osinski, B. Ichter, and S. Levine, "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action," arXiv preprint arXiv:2207.04429, 2022.</p>
<p>Towards semantic slam using a monocular camera. J Civera, D Gálvez-López, L Riazuelo, J D Tardós, J M M Montiel, 2011 IEEE/RSJ international conference on intelligent robots and systems. J. Civera, D. Gálvez-López, L. Riazuelo, J. D. Tardós, and J. M. M. Montiel, "Towards semantic slam using a monocular camera," in 2011 IEEE/RSJ international conference on intelligent robots and systems, 2011.</p>
<p>Semantic slam based on object detection and improved octomap. L Zhang, L Wei, P Shen, W Wei, G Zhu, J Song, IEEE Access. L. Zhang, L. Wei, P. Shen, W. Wei, G. Zhu, and J. Song, "Semantic slam based on object detection and improved octomap," IEEE Access, 2018.</p>
<p>Probabilistic data association for semantic slam. S L Bowman, N Atanasov, K Daniilidis, G J Pappas, 2017 IEEE international conference on robotics and automation (ICRA). S. L. Bowman, N. Atanasov, K. Daniilidis, and G. J. Pappas, "Probabilistic data association for semantic slam," in 2017 IEEE international conference on robotics and automation (ICRA), 2017.</p>
<p>Gibson env: real-world perception for embodied agents. F Xia, A R Zamir, Z.-Y He, A Sax, J Malik, S Savarese, Computer Vision and Pattern Recognition (CVPR). F. Xia, A. R. Zamir, Z.-Y. He, A. Sax, J. Malik, and S. Savarese, "Gibson env: real-world perception for embodied agents," in Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on, 2018.</p>
<p>Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects. M Runz, M Buffier, L Agapito, 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). M. Runz, M. Buffier, and L. Agapito, "Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects," in 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2018.</p>
<p>Fusion++: Volumetric object-level slam. J Mccormac, R Clark, M Bloesch, A Davison, S Leutenegger, 2018 international conference on 3D vision (3DV). J. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger, "Fusion++: Volumetric object-level slam," in 2018 international conference on 3D vision (3DV), 2018.</p>
<p>Imvotenet: Boosting 3d object detection in point clouds with image votes. C R Qi, X Chen, O Litany, L J Guibas, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionC. R. Qi, X. Chen, O. Litany, and L. J. Guibas, "Imvotenet: Boosting 3d object detection in point clouds with image votes," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020.</p>
<p>Pointfusion: Deep sensor fusion for 3d bounding box estimation. D Xu, D Anguelov, A Jain, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionD. Xu, D. Anguelov, and A. Jain, "Pointfusion: Deep sensor fusion for 3d bounding box estimation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.</p>
<p>Topological planning with transformers for vision-and-language navigation. K Chen, J K Chen, J Chuang, M Vázquez, S Savarese, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionK. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese, "Topological planning with transformers for vision-and-language navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.</p>
<p>A behavioral approach to visual navigation with graph localization networks. K Chen, J P De Vicente, G Sepulveda, F Xia, A Soto, M Vázquez, S Savarese, arXiv:1903.00445arXiv preprintK. Chen, J. P. de Vicente, G. Sepulveda, F. Xia, A. Soto, M. Vázquez, and S. Savarese, "A behavioral approach to visual navigation with graph localization networks," arXiv preprint arXiv:1903.00445, 2019.</p>
<p>Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. S.-C Wu, J Wald, K Tateno, N Navab, F Tombari, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionS.-C. Wu, J. Wald, K. Tateno, N. Navab, and F. Tombari, "Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.</p>
<p>3d scene graph: A structure for unified semantics, 3d space, and camera. I Armeni, Z.-Y He, J Gwak, A R Zamir, M Fischer, J Malik, S Savarese, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionI. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, and S. Savarese, "3d scene graph: A structure for unified semantics, 3d space, and camera," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.</p>
<p>Visual language maps for robot navigation. C Huang, O Mees, A Zeng, W Burgard, arXiv:2210.05714arXiv preprintC. Huang, O. Mees, A. Zeng, and W. Burgard, "Visual language maps for robot navigation," arXiv preprint arXiv:2210.05714, 2022.</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, International Conference on Learning Representations (ICLR. 2020D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, "Learning to explore using active neural slam," in International Conference on Learning Representations (ICLR), 2020.</p>
<p>Neural topological slam for visual navigation. D S Chaplot, R Salakhutdinov, A Gupta, S Gupta, CVPR. D. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta, "Neural topological slam for visual navigation," in CVPR, 2020.</p>
<p>Scene memory transformer for embodied agents in long time horizon tasks. K Fang, F.-F Li, S Savarese, A Toshev, CVPR 2019. K. Fang, F.-F. Li, S. Savarese, and A. Toshev, "Scene memory transformer for embodied agents in long time horizon tasks," in CVPR 2019, 2019.</p>
<p>Real-time detection of moving objects in a dynamic scene from moving robotic vehicles. A Talukder, S Goldberg, L Matthies, A Ansar, Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453). 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453)A. Talukder, S. Goldberg, L. Matthies, and A. Ansar, "Real-time detection of moving objects in a dynamic scene from moving robotic vehicles," in Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453), 2003.</p>
<p>Visual-based obstacle detection: a purposive approach using the normal ow. J Santos-Victor, G Sandini, Proc. of the International Conference on Intelligent Autonomous Systems. of the International Conference on Intelligent Autonomous SystemsKarlsruhe, GermanyJ. Santos-Victor and G. Sandini, "Visual-based obstacle detection: a purposive approach using the normal ow," in Proc. of the International Conference on Intelligent Autonomous Systems, Karlsruhe, Germany, 1995.</p>
<p>Visual representations for semantic target driven navigation. A Mousavian, A Toshev, M Fišer, J Košecká, A Wahid, J Davidson, 2019 International Conference on Robotics and Automation (ICRA). A. Mousavian, A. Toshev, M. Fišer, J. Košecká, A. Wahid, and J. Davidson, "Visual representations for semantic target driven navigation," in 2019 International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, International Conference on Learning Representations. T. Chen, S. Gupta, and A. Gupta, "Learning exploration policies for navigation," in International Conference on Learning Representations, 2019.</p>
<p>Poni: Potential functions for objectgoal navigation with interaction-free learning. S K Ramakrishnan, D S Chaplot, Z Al-Halah, J Malik, K Grauman, Computer Vision and Pattern Recognition (CVPR), 2022 IEEE Conference on. S. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, and K. Grauman, "Poni: Potential functions for objectgoal navigation with interaction-free learning," in Computer Vision and Pattern Recognition (CVPR), 2022 IEEE Conference on, 2022.</p>
<p>Learning objectconditioned exploration using distributed soft actor critic. A Wahid, A Stone, K Chen, B Ichter, A Toshev, CoRRA. Wahid, A. Stone, K. Chen, B. Ichter, and A. Toshev, "Learning object- conditioned exploration using distributed soft actor critic," CoRR, 2020.</p>
<p>Zson: Zero-shot object-goal navigation using multimodal goal embeddings. A Majumdar, G Aggarwal, B Devnani, J Hoffman, D Batra, arXiv:2206.12403arXiv preprintA. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, "Zson: Zero-shot object-goal navigation using multimodal goal embeddings," arXiv preprint arXiv:2206.12403, 2022.</p>
<p>Clip on wheels: Zero-shot object navigation as object localization and exploration. S Y Gadre, M Wortsman, G Ilharco, L Schmidt, S Song, arXiv:2203.10421arXiv preprintS. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song, "Clip on wheels: Zero-shot object navigation as object localization and exploration," arXiv preprint arXiv:2203.10421, 2022.</p>
<p>Ffrob: An efficient heuristic for task and motion planning. C R Garrett, T Lozano-Pérez, L P Kaelbling, Algorithmic Foundations of Robotics XI. C. R. Garrett, T. Lozano-Pérez, and L. P. Kaelbling, "Ffrob: An efficient heuristic for task and motion planning," in Algorithmic Foundations of Robotics XI.</p>
<p>Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning. Proceedings of the International Conference on Automated Planning and Scheduling. the International Conference on Automated Planning and Scheduling--, "Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning," in Proceedings of the International Conference on Automated Planning and Scheduling, 2020.</p>
<p>Hierarchical planning for long-horizon manipulation with geometric and symbolic scene graphs. Y Zhu, J Tremblay, S Birchfield, Y Zhu, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021Y. Zhu, J. Tremblay, S. Birchfield, and Y. Zhu, "Hierarchical planning for long-horizon manipulation with geometric and symbolic scene graphs," in 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021.</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. A Zeng, A Wong, S Welker, K Choromanski, F Tombari, A Purohit, M Ryoo, V Sindhwani, J Lee, V Vanhoucke, arXiv:2204.00598arXiv preprintA. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke et al., "Socratic models: Composing zero-shot multimodal reasoning with language," arXiv preprint arXiv:2204.00598, 2022.</p>
<p>Frontier-based exploration using multiple robots. B Yamauchi, Proceedings of the second international conference on Autonomous agents. the second international conference on Autonomous agentsB. Yamauchi, "Frontier-based exploration using multiple robots," in Proceed- ings of the second international conference on Autonomous agents, 1998.</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
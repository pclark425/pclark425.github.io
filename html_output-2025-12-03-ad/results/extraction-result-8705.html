<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8705 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8705</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8705</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-b90655ce30812d3b1b84599b8dc328ee374bbb28</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b90655ce30812d3b1b84599b8dc328ee374bbb28" target="_blank">Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data</a></p>
                <p><strong>Paper Venue:</strong> Journal of Artificial Intelligence Research</p>
                <p><strong>Paper TL;DR:</strong> A principled approach to automatically construct a conceptual space from a set of known observations wherein the quality dimensions and domains are not known a priori, while preserving the semantic interpretation that is needed to infer linguistic descriptions for interaction with humans.</p>
                <p><strong>Paper Abstract:</strong> There is an increasing need to derive semantics from real-world observations to facilitate natural information sharing between machine and human. Conceptual spaces theory is a possible approach and has been proposed as mid-level representation between symbolic and sub-symbolic representations, whereby concepts are represented in a geometrical space that is characterised by a number of quality dimensions. Currently, much of the work has demonstrated how conceptual spaces are created in a knowledge-driven manner, relying on prior knowledge to form concepts and identify quality dimensions. This paper presents a method to create semantic representations using data-driven conceptual spaces which are then used to derive linguistic descriptions of numerical data. Our contribution is a principled approach to automatically construct a conceptual space from a set of known observations wherein the quality dimensions and domains are not known a priori. This novelty of the approach is the ability to select and group semantic features to discriminate between concepts in a data-driven manner while preserving the semantic interpretation that is needed to infer linguistic descriptions for interaction with humans. Two data sets representing leaf images and time series signals are used to evaluate the method. An empirical evaluation for each case study assesses how well linguistic descriptions generated from the conceptual spaces identify unknown observations. Furthermore, comparisons are made with descriptions derived on alternative approaches for generating semantic models.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8705.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8705.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Conceptual Spaces (Gärdenfors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geometrical mid-level representational framework in which concepts are regions in a multi-dimensional space defined by cognitively meaningful quality dimensions grouped into domains; similarity is measured metrically and concepts are convex regions across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Conceptual spaces (region/metric representation)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are represented as geometrical regions (convex hulls) across one or more domains; domains are integrated sets of quality dimensions (interpretable features), instances map to points (vectors) with one point per domain, similarity is computed using distances within domains and concepts are compared via inclusion/membership operations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid / feature-based / geometric (mid-level)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Concept formation, categorization, induction, semantic inference, similarity judgments, semantic grounding for linguistic description, content determination for NLG.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This paper operationalises conceptual spaces in a data-driven manner: (1) selects quality dimensions from numeric features via filter feature-selection (MIFS), (2) groups them into domains using biclique-based grouping on a label–feature bipartite graph, (3) represents concepts as convex hulls (convex regions) built from instances' points per domain, and (4) uses domain/dimension weights (from feature relevance) to capture context-dependent salience. The authors claim this pipeline supports generating natural-language descriptions of previously unseen numeric observations and preserves interpretability because dimensions are original features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Positioned explicitly as a mid-level bridge between symbolic and sub-symbolic representations: unlike symbolic systems, conceptual spaces support induction from limited observations; unlike sub-symbolic (connectionist) models, they provide interpretable, semantically meaningful dimensions and regions. Compared to fuzzy-set based linguistic description approaches, conceptual spaces are argued to better support content determination because they represent both similar concepts and representative multi-domain features rather than only linguistic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Assigning human-meaningful semantics to domains constructed purely data-driven is challenging (the paper notes difficulty interpreting data-driven domains perceptually); the method depends on an initial set of features and thus cannot learn from raw sensory input without precomputed features; biclique finding is NP-hard so the approach uses approximations; filter-based feature selection results vary by method and may impact resulting domains; convex-hull regions depend on available instances and may over-generalise if instances are sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Conceptual spaces functionally serve as an interpretable intermediate representation enabling both induction (learning/generalisation from instances via regions) and symbolic grounding (via a mapped symbol space), thus providing a principled route to generate linguistic descriptions from numeric data while retaining both similarity-based generalisation and explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8705.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8705.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instance-based representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instance-based / exemplar representation (instance vectors and convex regions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional representation where concepts are induced from stored instances (exemplars); new items are classified by similarity to these exemplars and concept regions are formed from instance collections (here via convex hulls).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Instance-based (exemplar) model</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Each observed example is vectorised into an instance consisting of one point per domain; a concept's region is derived from the set of its instances (points) per domain, operationalised here as the convex hull of the instances; membership/induction for a new instance is determined by measuring inclusion or proximity to these instance-derived regions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based / exemplar</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Categorization, induction (generalization from examples), concept learning, similarity-based inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper implements an instance-based method: vectorise labelled observations into domain-specific points, collect points per concept and domain, compute convex hulls to define sub-concept regions; induction for novel items is computed by testing point inclusion in those convex regions and using fuzzy membership for partial inclusion. The approach extends classic instance-based methods by splitting feature space into interpretable domains and using convex regions rather than nearest-neighbour-only decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared with classical instance-based learning that operates in a single flat feature space, this approach organises exemplars across multiple data-driven domains (quality-dimensions), improving interpretability and enabling richer multi-domain descriptions; it remains closer to exemplar models than to prototype-only representations because regions derive directly from instance distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Membership via convex hulls can be sensitive to outliers and sparse data; instance-based ranking and domain grouping rely on prior feature selection, so performance depends on quality of selected features; the paper notes that data-driven domains may lack direct perceptual semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Instance/exemplar storage plus domain-structured geometry enables both faithful generalisation (through convex regions) and semantic explanation (by returning contributing domains/dimensions), making instance-based formats compatible with linguistic characterisation when coupled with symbol mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8705.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8705.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic representation (symbol manipulation / labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic high-level representation using explicit symbols and rules to represent and reason about concepts; emphasised for explainability but limited in inductive learning from examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are represented by explicit symbolic primitives and rule-based structures; inference proceeds via symbol manipulation rather than similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Rule-based reasoning, explicit categorisation, high-level symbolic inference, and tasks requiring compositional structure and logical operations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper contrasts symbolic approaches with conceptual spaces: symbolic systems are noted as not fully satisfactory for inductive inference or concept learning from limited observations, motivating the mid-level conceptual spaces approach which can support both learning and semantic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Symbolic approaches are contrasted with sub-symbolic and conceptual-space approaches; conceptual spaces are presented as a mediating level that preserves interpretability while enabling induction, unlike purely symbolic rule-based models that struggle with induction from examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Symbolic approaches lack robust mechanisms for induction from a limited set of perceptual examples and often require hand-crafted rules and prior knowledge to initialise concepts and dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Symbolic representations are necessary for some forms of explicit reasoning, but alone they do not account for perceptually grounded learning; hybrid architectures that map between symbolic labels and conceptual spaces are advocated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8705.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8705.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sub-symbolic / Connectionist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sub-symbolic (connectionist / distributed) representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representations based on distributed patterns of activation (e.g., neural networks) that excel at categorisation but often lack interpretability and explicit semantic descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Sub-symbolic / distributed representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Knowledge is encoded in distributed activation patterns across units (weights/hidden activations), and categorisation is achieved via learned mappings without explicit symbolic primitives or interpretable dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / connectionist</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Perceptual categorization, pattern recognition, associative learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper notes that sub-symbolic approaches focus on categorisation but often neglect explainability: connectionist models typically cannot easily yield semantically meaningful descriptions of their learned internal representations, motivating a mid-level (conceptual spaces) approach that supports interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Sub-symbolic models are stronger on raw perceptual discrimination but weaker on semantic interpretability compared to conceptual spaces; conceptual spaces aim to combine the generalisation strengths of sub-symbolic models with interpretable dimensions akin to symbolic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Connectionist models' lack of transparency is singled out as a shortcoming for tasks requiring natural-language explanations or transparent content determination.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Bridging sub-symbolic representations and symbols via a geometrical mid-level (conceptual space) can combine flexibility of learning with explainability required for linguistic interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8705.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8705.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fuzzy set / Computing with Words</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fuzzy set theory and Computing with Words (Zadeh)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representational approach that maps numeric values onto graded linguistic categories using membership functions and fuzzy quantifiers to produce linguistic summaries and handle vagueness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Fuzzy set / computing-with-words representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Numeric feature intervals are partitioned via fuzzy membership functions into linguistic categories (e.g., 'low', 'high'), and fuzzy quantification models enable linguistic summaries (e.g., 'most temperatures are hot'); fuzzy membership functions are used here to map quality-dimension intervals to linguistic terms.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>graded / fuzzy / symbolic-linguistic</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Linguistic description of data (LDD), perception-based summarisation, quantification (e.g., 'most'), tactile/vague property description.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper adopts fuzzy membership functions (mu_q) in the definition of quality dimensions to enable mapping from numeric intervals to linguistic labels; it also notes that fuzzy-set-based LDD is a mature approach for generating linguistic summaries but argues conceptual spaces have advantages for content determination because they represent multi-domain relations and concept similarities, not only label assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Fuzzy approaches are strong at direct numeric-to-linguistic translation and summarisation (computing with words), but the paper argues they are less suited for content determination when multi-domain conceptual relations and similarity-driven concept-level descriptions are needed; conceptual spaces can produce richer descriptions (concept + representative features) while fuzzy systems primarily provide linguistic labels/rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Fuzzy descriptions often rely on learned fuzzy rules or manually crafted granulations and may not capture multi-domain conceptual relationships or concept-level similarity as naturally as conceptual spaces; the paper points out that fuzzy labeling alone does not provide the same content-determination capability as a conceptual-space-based approach.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Fuzzy membership is useful within a conceptual-space pipeline for lexicalisation and quantifying partial inclusion, but conceptual spaces are proposed to supplement fuzzy linguistic mechanisms to produce semantically richer, multi-domain natural language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8705.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8705.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol Space (two-layer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-layer Symbol Space (concept and quality layers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mapping layer introduced in this paper that links geometrical elements of the conceptual space (concepts and quality dimensions) to symbolic linguistic labels, represented as symbol-dimensions with values in [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Symbol space mapping (two-layer symbol dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>A knowledge-based two-layer symbol space contains concept-layer symbol dimensions (one per concept) and quality-layer symbol dimensions (one per quality dimension); each instance in the conceptual space is associated with a symbol vector concatenating concept-layer and quality-layer applicability values in [0,1], enabling lexicalisation and NLG.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / grounded mapping (hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Symbol grounding, lexicalisation, natural language generation (content determination, microplanning, realization), semantic annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper defines and uses this two-layer symbol space to perform content determination and lexicalisation: values in the concept layer indicate membership/similarity to known concepts, values in the quality layer indicate applicability of quality-dimension labels; combined, they drive linguistic description generation for novel observations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>This symbol-space mapping operationalises the link from geometric (conceptual space) representations to symbolic linguistic descriptions, addressing the symbol-grounding limitation of purely symbolic systems and the interpretability limitation of sub-symbolic systems by providing explicit grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Construction of the symbol space requires prior knowledge to name symbol dimensions and specify linguistic terms (the mapping is knowledge-based), so full automation of symbol labelling is not solved; values depend on the chosen membership/inclusion measures in the conceptual space.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>A dedicated symbol space is necessary to ground geometrical concept representations in linguistic terms; the two-layer design supports separate but joined signals for concept-level labelling and feature-level descriptors, enabling richer, explainable natural-language outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual Spaces: The Geometry of Thought <em>(Rating: 2)</em></li>
                <li>A New Direction in AI: Computing with Words <em>(Rating: 1)</em></li>
                <li>Linguistic description of data <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8705",
    "paper_id": "paper-b90655ce30812d3b1b84599b8dc328ee374bbb28",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "Conceptual Spaces",
            "name_full": "Theory of Conceptual Spaces (Gärdenfors)",
            "brief_description": "A geometrical mid-level representational framework in which concepts are regions in a multi-dimensional space defined by cognitively meaningful quality dimensions grouped into domains; similarity is measured metrically and concepts are convex regions across domains.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Conceptual spaces (region/metric representation)",
            "representational_format_description": "Concepts are represented as geometrical regions (convex hulls) across one or more domains; domains are integrated sets of quality dimensions (interpretable features), instances map to points (vectors) with one point per domain, similarity is computed using distances within domains and concepts are compared via inclusion/membership operations.",
            "format_type": "hybrid / feature-based / geometric (mid-level)",
            "cognitive_task_or_phenomenon": "Concept formation, categorization, induction, semantic inference, similarity judgments, semantic grounding for linguistic description, content determination for NLG.",
            "key_findings": "This paper operationalises conceptual spaces in a data-driven manner: (1) selects quality dimensions from numeric features via filter feature-selection (MIFS), (2) groups them into domains using biclique-based grouping on a label–feature bipartite graph, (3) represents concepts as convex hulls (convex regions) built from instances' points per domain, and (4) uses domain/dimension weights (from feature relevance) to capture context-dependent salience. The authors claim this pipeline supports generating natural-language descriptions of previously unseen numeric observations and preserves interpretability because dimensions are original features.",
            "comparison_with_other_formats": "Positioned explicitly as a mid-level bridge between symbolic and sub-symbolic representations: unlike symbolic systems, conceptual spaces support induction from limited observations; unlike sub-symbolic (connectionist) models, they provide interpretable, semantically meaningful dimensions and regions. Compared to fuzzy-set based linguistic description approaches, conceptual spaces are argued to better support content determination because they represent both similar concepts and representative multi-domain features rather than only linguistic labels.",
            "limitations_or_counter_evidence": "Assigning human-meaningful semantics to domains constructed purely data-driven is challenging (the paper notes difficulty interpreting data-driven domains perceptually); the method depends on an initial set of features and thus cannot learn from raw sensory input without precomputed features; biclique finding is NP-hard so the approach uses approximations; filter-based feature selection results vary by method and may impact resulting domains; convex-hull regions depend on available instances and may over-generalise if instances are sparse.",
            "theoretical_claims_or_implications": "Conceptual spaces functionally serve as an interpretable intermediate representation enabling both induction (learning/generalisation from instances via regions) and symbolic grounding (via a mapped symbol space), thus providing a principled route to generate linguistic descriptions from numeric data while retaining both similarity-based generalisation and explainability.",
            "uuid": "e8705.0",
            "source_info": {
                "paper_title": "Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "Instance-based representation",
            "name_full": "Instance-based / exemplar representation (instance vectors and convex regions)",
            "brief_description": "A functional representation where concepts are induced from stored instances (exemplars); new items are classified by similarity to these exemplars and concept regions are formed from instance collections (here via convex hulls).",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Instance-based (exemplar) model",
            "representational_format_description": "Each observed example is vectorised into an instance consisting of one point per domain; a concept's region is derived from the set of its instances (points) per domain, operationalised here as the convex hull of the instances; membership/induction for a new instance is determined by measuring inclusion or proximity to these instance-derived regions.",
            "format_type": "feature-based / exemplar",
            "cognitive_task_or_phenomenon": "Categorization, induction (generalization from examples), concept learning, similarity-based inclusion.",
            "key_findings": "The paper implements an instance-based method: vectorise labelled observations into domain-specific points, collect points per concept and domain, compute convex hulls to define sub-concept regions; induction for novel items is computed by testing point inclusion in those convex regions and using fuzzy membership for partial inclusion. The approach extends classic instance-based methods by splitting feature space into interpretable domains and using convex regions rather than nearest-neighbour-only decisions.",
            "comparison_with_other_formats": "Compared with classical instance-based learning that operates in a single flat feature space, this approach organises exemplars across multiple data-driven domains (quality-dimensions), improving interpretability and enabling richer multi-domain descriptions; it remains closer to exemplar models than to prototype-only representations because regions derive directly from instance distributions.",
            "limitations_or_counter_evidence": "Membership via convex hulls can be sensitive to outliers and sparse data; instance-based ranking and domain grouping rely on prior feature selection, so performance depends on quality of selected features; the paper notes that data-driven domains may lack direct perceptual semantics.",
            "theoretical_claims_or_implications": "Instance/exemplar storage plus domain-structured geometry enables both faithful generalisation (through convex regions) and semantic explanation (by returning contributing domains/dimensions), making instance-based formats compatible with linguistic characterisation when coupled with symbol mappings.",
            "uuid": "e8705.1",
            "source_info": {
                "paper_title": "Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "Symbolic representation",
            "name_full": "Symbolic representation (symbol manipulation / labels)",
            "brief_description": "A classic high-level representation using explicit symbols and rules to represent and reason about concepts; emphasised for explainability but limited in inductive learning from examples.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "Symbolic representation",
            "representational_format_description": "Concepts are represented by explicit symbolic primitives and rule-based structures; inference proceeds via symbol manipulation rather than similarity metrics.",
            "format_type": "symbolic",
            "cognitive_task_or_phenomenon": "Rule-based reasoning, explicit categorisation, high-level symbolic inference, and tasks requiring compositional structure and logical operations.",
            "key_findings": "The paper contrasts symbolic approaches with conceptual spaces: symbolic systems are noted as not fully satisfactory for inductive inference or concept learning from limited observations, motivating the mid-level conceptual spaces approach which can support both learning and semantic inference.",
            "comparison_with_other_formats": "Symbolic approaches are contrasted with sub-symbolic and conceptual-space approaches; conceptual spaces are presented as a mediating level that preserves interpretability while enabling induction, unlike purely symbolic rule-based models that struggle with induction from examples.",
            "limitations_or_counter_evidence": "Symbolic approaches lack robust mechanisms for induction from a limited set of perceptual examples and often require hand-crafted rules and prior knowledge to initialise concepts and dimensions.",
            "theoretical_claims_or_implications": "Symbolic representations are necessary for some forms of explicit reasoning, but alone they do not account for perceptually grounded learning; hybrid architectures that map between symbolic labels and conceptual spaces are advocated.",
            "uuid": "e8705.2",
            "source_info": {
                "paper_title": "Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "Sub-symbolic / Connectionist",
            "name_full": "Sub-symbolic (connectionist / distributed) representation",
            "brief_description": "Representations based on distributed patterns of activation (e.g., neural networks) that excel at categorisation but often lack interpretability and explicit semantic descriptions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "Sub-symbolic / distributed representation",
            "representational_format_description": "Knowledge is encoded in distributed activation patterns across units (weights/hidden activations), and categorisation is achieved via learned mappings without explicit symbolic primitives or interpretable dimensions.",
            "format_type": "distributed / connectionist",
            "cognitive_task_or_phenomenon": "Perceptual categorization, pattern recognition, associative learning.",
            "key_findings": "The paper notes that sub-symbolic approaches focus on categorisation but often neglect explainability: connectionist models typically cannot easily yield semantically meaningful descriptions of their learned internal representations, motivating a mid-level (conceptual spaces) approach that supports interpretability.",
            "comparison_with_other_formats": "Sub-symbolic models are stronger on raw perceptual discrimination but weaker on semantic interpretability compared to conceptual spaces; conceptual spaces aim to combine the generalisation strengths of sub-symbolic models with interpretable dimensions akin to symbolic representations.",
            "limitations_or_counter_evidence": "Connectionist models' lack of transparency is singled out as a shortcoming for tasks requiring natural-language explanations or transparent content determination.",
            "theoretical_claims_or_implications": "Bridging sub-symbolic representations and symbols via a geometrical mid-level (conceptual space) can combine flexibility of learning with explainability required for linguistic interaction.",
            "uuid": "e8705.3",
            "source_info": {
                "paper_title": "Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "Fuzzy set / Computing with Words",
            "name_full": "Fuzzy set theory and Computing with Words (Zadeh)",
            "brief_description": "A representational approach that maps numeric values onto graded linguistic categories using membership functions and fuzzy quantifiers to produce linguistic summaries and handle vagueness.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Fuzzy set / computing-with-words representation",
            "representational_format_description": "Numeric feature intervals are partitioned via fuzzy membership functions into linguistic categories (e.g., 'low', 'high'), and fuzzy quantification models enable linguistic summaries (e.g., 'most temperatures are hot'); fuzzy membership functions are used here to map quality-dimension intervals to linguistic terms.",
            "format_type": "graded / fuzzy / symbolic-linguistic",
            "cognitive_task_or_phenomenon": "Linguistic description of data (LDD), perception-based summarisation, quantification (e.g., 'most'), tactile/vague property description.",
            "key_findings": "The paper adopts fuzzy membership functions (mu_q) in the definition of quality dimensions to enable mapping from numeric intervals to linguistic labels; it also notes that fuzzy-set-based LDD is a mature approach for generating linguistic summaries but argues conceptual spaces have advantages for content determination because they represent multi-domain relations and concept similarities, not only label assignments.",
            "comparison_with_other_formats": "Fuzzy approaches are strong at direct numeric-to-linguistic translation and summarisation (computing with words), but the paper argues they are less suited for content determination when multi-domain conceptual relations and similarity-driven concept-level descriptions are needed; conceptual spaces can produce richer descriptions (concept + representative features) while fuzzy systems primarily provide linguistic labels/rules.",
            "limitations_or_counter_evidence": "Fuzzy descriptions often rely on learned fuzzy rules or manually crafted granulations and may not capture multi-domain conceptual relationships or concept-level similarity as naturally as conceptual spaces; the paper points out that fuzzy labeling alone does not provide the same content-determination capability as a conceptual-space-based approach.",
            "theoretical_claims_or_implications": "Fuzzy membership is useful within a conceptual-space pipeline for lexicalisation and quantifying partial inclusion, but conceptual spaces are proposed to supplement fuzzy linguistic mechanisms to produce semantically richer, multi-domain natural language descriptions.",
            "uuid": "e8705.4",
            "source_info": {
                "paper_title": "Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "Symbol Space (two-layer)",
            "name_full": "Two-layer Symbol Space (concept and quality layers)",
            "brief_description": "A mapping layer introduced in this paper that links geometrical elements of the conceptual space (concepts and quality dimensions) to symbolic linguistic labels, represented as symbol-dimensions with values in [0,1].",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "Symbol space mapping (two-layer symbol dimensions)",
            "representational_format_description": "A knowledge-based two-layer symbol space contains concept-layer symbol dimensions (one per concept) and quality-layer symbol dimensions (one per quality dimension); each instance in the conceptual space is associated with a symbol vector concatenating concept-layer and quality-layer applicability values in [0,1], enabling lexicalisation and NLG.",
            "format_type": "symbolic / grounded mapping (hybrid)",
            "cognitive_task_or_phenomenon": "Symbol grounding, lexicalisation, natural language generation (content determination, microplanning, realization), semantic annotation.",
            "key_findings": "The paper defines and uses this two-layer symbol space to perform content determination and lexicalisation: values in the concept layer indicate membership/similarity to known concepts, values in the quality layer indicate applicability of quality-dimension labels; combined, they drive linguistic description generation for novel observations.",
            "comparison_with_other_formats": "This symbol-space mapping operationalises the link from geometric (conceptual space) representations to symbolic linguistic descriptions, addressing the symbol-grounding limitation of purely symbolic systems and the interpretability limitation of sub-symbolic systems by providing explicit grounding.",
            "limitations_or_counter_evidence": "Construction of the symbol space requires prior knowledge to name symbol dimensions and specify linguistic terms (the mapping is knowledge-based), so full automation of symbol labelling is not solved; values depend on the chosen membership/inclusion measures in the conceptual space.",
            "theoretical_claims_or_implications": "A dedicated symbol space is necessary to ground geometrical concept representations in linguistic terms; the two-layer design supports separate but joined signals for concept-level labelling and feature-level descriptors, enabling richer, explainable natural-language outputs.",
            "uuid": "e8705.5",
            "source_info": {
                "paper_title": "Data-driven Conceptual Spaces: Creating Semantic Representations For Linguistic Descriptions Of Numerical Data",
                "publication_date_yy_mm": "2018-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual Spaces: The Geometry of Thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "A New Direction in AI: Computing with Words",
            "rating": 1,
            "sanitized_title": "a_new_direction_in_ai_computing_with_words"
        },
        {
            "paper_title": "Linguistic description of data",
            "rating": 2,
            "sanitized_title": "linguistic_description_of_data"
        }
    ],
    "cost": 0.01577725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Data-Driven Conceptual Spaces: Creating Semantic Representations for Linguistic Descriptions of Numerical Data</h1>
<p>Hadi Banaee<br>Erik Schaffernicht<br>Amy Loutfi<br>Centre for Applied Autonomous Sensor Systems<br>School of Science and Technology<br>Örebro University, SE-701 82 Örebro, Sweden</p>
<p>HADI.BANAEE@ORU.SE<br>ERIK.SCHAFFERNICHT@ORU.SE<br>AMY.LOUTFI@ORU.SE</p>
<h4>Abstract</h4>
<p>There is an increasing need to derive semantics from real-world observations to facilitate natural information sharing between machine and human. Conceptual spaces theory is a possible approach and has been proposed as mid-level representation between symbolic and sub-symbolic representations, whereby concepts are represented in a geometrical space that is characterised by a number of quality dimensions. Currently, much of the work has demonstrated how conceptual spaces are created in a knowledge-driven manner, relying on prior knowledge to form concepts and identify quality dimensions. This paper presents a method to create semantic representations using data-driven conceptual spaces which are then used to derive linguistic descriptions of numerical data. Our contribution is a principled approach to automatically construct a conceptual space from a set of known observations wherein the quality dimensions and domains are not known a priori. This novelty of the approach is the ability to select and group semantic features to discriminate between concepts in a data-driven manner while preserving the semantic interpretation that is needed to infer linguistic descriptions for interaction with humans. Two data sets representing leaf images and time series signals are used to evaluate the method. An empirical evaluation for each case study assesses how well linguistic descriptions generated from the conceptual spaces identify unknown observations. Furthermore, comparisons are made with descriptions derived on alternative approaches for generating semantic models.</p>
<h2>1. Introduction</h2>
<p>Artificial intelligence systems that interact with humans in natural language must be able to form concepts either directly from observations or indirectly from other concepts. New applications within several domains, such as cognitive robotics, will increasingly need to rely on automatic methods to form concepts directly from sensor data. Different approaches to creating the representational models necessary for concept formation have been described in the literature of artificial intelligence (AI). Symbolic approaches use explicit symbols as primitives to perform symbol manipulation in order to model high-level abstract concepts (Sun, 1999; Minsky, 1991). Sub-symbolic approaches focus often on the categorisation tasks per se, and process the activation patterns of concepts in the perceptual level using internally connected units (Sun, 1999; Gärdenfors, 1997). Neither approach is fully satisfactory (Gärdenfors, 1995). Generally, symbolic approaches do not perform the task of inductive</p>
<p>inference or concept learning, which is the process of performing a generalisation from a limited number of observations (Sun, 1999). On the other hand, sub-symbolic approaches often neglect the issue of explainability or interpretability, that is, the process of inferring meaningful descriptions from a set of (semantically enriched) representations. For instance, connectionist approaches are often not able to explain what the emerging learnt model represents (Aisbett \&amp; Gibbon, 2001).</p>
<p>Consequently, the theory of conceptual spaces was introduced by Gärdenfors (2000) as a mid-level representation between symbolic and sub-symbolic approaches (Aisbett \&amp; Gibbon, 2001) to address both concept learning and semantic inference (Aisbett \&amp; Gibbon, 2001; Holender, Nagi, Sudit, \&amp; Rickard, 2007). Conceptual spaces theory presents a framework consisting of a set of quality dimensions (i.e. cognitively meaningful attributes) in various domains within a geometrical structure in order to model, categorise, and represent the concepts in a multi-dimensional space (Gärdenfors, 2000). Concepts in a conceptual space are represented by their properties as regions within the domains. Criticism on the theory of conceptual spaces, an open question is how to automatically construct a conceptual space from numerical data (Keßler, 2006). In the literature, conceptual spaces have been principally derived in a knowledge-driven manner and operate on the assumption that there is prior knowledge from perceptual mechanisms or experts to manually initialise the elements of the conceptual space (i.e., domains, quality dimensions, and concepts' regions) (Agostaro, Augello, Pilato, Vassallo, \&amp; Gaglio, 2005; Rickard, Aisbett, \&amp; Gibbon, 2007).</p>
<p>This paper presents a data-driven approach to automatically construct conceptual spaces and perform concept formation based on input observations (i.e., exemplars) in a numerical data set. Instead of initialising domains and dimensions from a priori knowledge or forming concepts in a rule-based manner, the proposed data-driven construction process automatically determines the relevant domains and dimensions based on the ability to distinguish between exemplars of different concepts. We further propose a semantic inference process for the built conceptual space to provide explainability of new and unknown observations in natural language descriptions.</p>
<p>The remainder of the paper is structured as follows. Section 2 provides background on conceptual spaces and related work. In Section 3 we introduce our approach of automatically constructing conceptual spaces from observations. Afterwards, we present the process of inferring semantic descriptions for a set of unknown observation via the constructed conceptual space in Section 4. In Section 5 we present two case studies where our proposed approach is exploited. The output descriptions from the case studies are then assessed in Section 6 based on an empirical evaluation designed to show the advantages of our proposed framework. Finally, in Sections 7 and 8 we discuss the method and the obtained results and draw our conclusion.</p>
<h1>2. Background and Related Work</h1>
<p>In this section, we provide background on the theory of conceptual spaces. We then discuss related work focusing on two areas: First, we describe the role of the conceptual spaces theory in the field of AI, and the necessity of revising its definitions based on AI needs. Second, we present a brief review of the relevant semantic inference and linguistic descrip-</p>
<p>tion approaches, followed by a discussion of the advantage of using conceptual spaces as a semantic representation.</p>
<h1>2.1 On the Theory of Conceptual Spaces</h1>
<p>The theory of conceptual spaces was introduced as a knowledge representation framework which relies on the paradigm of cognitive semantics (Gärdenfors, 2000). This theory explores how various types of information can be represented, both from a psychological perspective and for developing an artificial system (Gärdenfors, 2004). A conceptual space is a geometrical structure which is defined by a set of quality dimensions. The quality dimensions present the features of objects in the space based on their measured quality values. One conceptual space can consist of multiple domains. A domain in the conceptual space is represented as a set of interdependent quality dimensions which are logically integrated. A typical example of a domain is colour that is defined by the quality dimensions hue, saturation, and brightness. Other examples of perceptual domains are shape, size, and weight (Gärdenfors, 2000).</p>
<p>Concepts in a conceptual space are represented as a set of regions through multiple domains. A concept is described based on its different properties in various domains. Properties are the convex regions in a single domain expressing a particular attribute of the domain. For example, green is a property corresponding to a region in the colour domain. An instance of a concept is defined as a set of points in the conceptual space, in which each point is located within a property region of that concept in one of the domain. As an example, in a conceptual space of fruits including the domains colour, taste, and size, the concept of apple can be represented as a set of regions within these domains (LeBlanc, 2010). In this conceptual space, one instance (object) of the concept of apple can take place in the properties of green, medium, and sweet, which are geometrical regions within the domains colour, size, and taste, respectively.</p>
<p>The metric definition of domains allows depicting the notion of similarity in a conceptual space. Measuring the similarity robustly eases the consideration of cognitive tasks such as concept formation, semantic inferences, induction, and concept learning (Holender et al., 2007). In concept representation, it is possible to assign weights to the domains or dimensions in order to distinguish between similar concepts in different contexts (Gärdenfors, 2004), which includes the notion of context into the conceptual spaces theory. Various formalisations of the conceptual spaces theory have been proposed in the literature (Aisbett \&amp; Gibbon, 2001; Rickard et al., 2007; Raubal, 2004; Holender et al., 2007), which attempted to mathematically formalise how to construct and perform induction in conceptual spaces.</p>
<h3>2.2 Related Work on Conceptual Spaces and AI</h3>
<p>As mentioned in the introduction, from the AI point of view, the aim of representing knowledge in a conceptual space is to develop an intuitive interpretation of the relationship between symbolic and sub-symbolic information (Gärdenfors, 2000; Aisbett \&amp; Gibbon, 2001). Gärdenfors has discussed thoroughly the role of conceptual spaces as a knowledge representation framework in AI systems (Gärdenfors, 2004), focusing on the tasks of induction and reasoning (Gärdenfors \&amp; Williams, 2001; Gärdenfors, 2005). Recently, Lieto et. al. (2017) have detailed the need for a conceptual representation as a knowledge representation</p>
<p>level in-between the symbolic and the sub-symbolic one. This offers cognitive architectures a common language enabling the interaction between different types of representations. Schockaert and Prade (2013) have focused on the problem of interpolative and extrapolative inference for different properties and concepts with the help of conceptual spaces. In addition to the theoretical AI problems, the feasibility of using conceptual spaces has been studied in various application domains of AI, such as geographical measurement (Schwering \&amp; Raubal, 2005; Adams \&amp; Raubal, 2009a), cognitive robotics (LeBlanc, 2010; Cubek, Ertel, \&amp; Palm, 2015; Chella, Frixione, \&amp; Gaglio, 2003), object recognition (Brezolin, Fiorini, de Borba Campos, \&amp; Bordini, 2015), and visual perception (Chella, Frixione, \&amp; Gaglio, 1997). A recent review (Zenker \&amp; Gärdenfors, 2015) discusses further applications in diverse research areas (semantic spaces, computing meanings, and philosophical perspectives).</p>
<p>Concept formation tightly connects the theory of conceptual spaces to the induction (and particularly learning) problem. The aim of many learning systems is a general description of a category of observations as concepts (Luger, 2005). If the input of a learning algorithm takes the form of instances, attributes, and concepts, then the process of learning is called concept description. Instance-based learning refers to a class of learning algorithms which predicate the labels for the unseen instances based on their similarity to the nearest training instances (Keogh, 2011). This model requires a similarity function to perform the task of concept descriptions. However, in instance-based learning, the similarity functions are usually applied within a single-domain full feature space (Aha, Kibler, \&amp; Albert, 1991).</p>
<p>A comparison of practicality and effectiveness in instance-based learning and conceptual spaces was presented by Lee and Portier (2007). Yet the authors did not include the model construction process in their discussion. In this paper, we propose an instance-based approach for concept formation that considers the role of the involved features to derive a multi-domain space and represent the concepts in such a space.</p>
<p>Involving data mining approaches in the process of deriving conceptual spaces has been seldom studied. Keßler (2006) outlined the idea of using conceptual spaces to describe data, with some discussions on the possibility of automatically generating such spaces from databases. Lee (2005) proposed a data mining method coupled with conceptual space, which addresses cognitive tasks such as concept formation using, e.g., clustering techniques. The main drawback of these approaches is that, in order to directly set up the domains, they rely on knowing about the semantics of the field beforehand. However, an essential challenge is to automatically provide interpretable features as integrated quality dimensions. This work attempts to identify a meaningful set of features out of predefined features in the data set, relying on the hypothesis that most discriminative features of concepts (classes) are the most representative quality dimensions in the conceptual space.</p>
<p>An open question in the field of conceptual spaces is how quality dimensions are identified (Gärdenfors, 2000). Once the process of constructing a conceptual space starts, as Quine (1969) noted, some innate quality dimensions are needed to make concept learning possible. However, there is no standard way to specify which set of initiated dimensions are cognitively sufficient to characterise the concepts to be learned. In many developed examples of conceptual spaces, determining the set of quality dimensions relies on the background knowledge, which comes from human perceptual or sensory dimensions. This question is more challenging when there is no prior knowledge to explain the semantics of dimensions, or there is a lack of knowledge about relevant quality dimensions (Gärdenfors, 2004). This</p>
<p>specific challenge motivates our investigation on how to derive the domains and quality dimensions in a data-driven manner.</p>
<p>Enabling data-driven domain creation for conceptual spaces extends their usefulness to problems they are not typically used for. However, keeping in mind the issues of induction and semantic inference in AI, the proposed approach particularly has the ability to deal with a class of learning problems that need a transparent interpretation for the overall model (not only interpretability of the decisions made).</p>
<h1>2.3 Related Work on Semantic Inferences and Linguistic Descriptions</h1>
<p>Revealing a semantic representation of unknown information connects linguistic description approaches to the present study, in a sense that one solution to determine content for datadriven information is to semantically model descriptive features of non-linguistic information and infer meaningful descriptions from the proposed model (Matthiessen, 1990).</p>
<p>Linguistic descriptions are the results of a natural language characterisation that covers the understandable information derived from the input data. This process is dependent on the semantic level of the informative features (attributes) that characterise and explain the data. This information can be obtained from various sources such as observations, sensor measurements, mathematical analysis or visual perceptions (Batyrshin, Sheremetov, \&amp; Herrera-Avelar, 2007). Importing descriptive features to computational systems includes the possibility of operating with linguistic information (Batyrshin et al., 2007).</p>
<p>The field of linguistic description of data (LDD) has emerged from the use of fuzzy set theory and soft computing to perform linguistic computations on data, which studies the methods for automatically describing numeric data sets by employing a set of linguistic terms (Ramos-Soto, Bugarín, \&amp; Barro, 2016). Fuzzy set theory is a well-studied approach to bridge between numeric and linguistic information, specifically in perception-based systems (Batyrshin et al., 2007; Kacprzyk \&amp; Zadrożny, 2010). The basic idea of linguistic descriptions comes from the works of Zadeh (2001) and Yager (1982) on computing with words, and later, computational theory of perception paradigms (Zadeh, 2000, 1996) to express the ability of computing systems in a linguistic manner (Diaz-Hermida, Pereira-Fariña, Vidal, \&amp; Ramos-Soto, 2016; Trivino \&amp; Sugeno, 2013). Within these paradigms, the promising tools are based on fuzzy quantification models to generate simple linguistic summaries of variables, such as "most of the temperatures are hot" (Delgado, Ruiz, Sánchez, \&amp; Vila, 2014). The fuzzy granulation provides propositions like low, increasing, significant, near future for the numerical features (Zadeh, 1997).</p>
<p>The studies on linguistic description also encompass the field of natural language generation (NLG). An NLG system generates human-readable natural language (Reiter, Dale, \&amp; Feng, 2000) from non-linguistic information. Content determination is the most relevant aspect of NLG for the linguistic characterisation of numeric data (Yu, Reiter, Hunter, \&amp; Mellish, 2007; Reiter, Sripada, \&amp; Robertson, 2003), which is the task of deriving interpretable information from the data to allow the system to reason symbolically instead of numerically (Reiter, 2007). Determining suitable content is mostly addressed using knowledge-driven approaches in order to comply with the domain or user requirements (Yu et al., 2007). Furthermore, Gkatzia (2015) reports that rule-based methods are dominant for current content selection approaches. However, when the content determination is performed in a data-</p>
<p>driven manner, as in this work, it is possible to reveal unknown information, which can be beyond the user requirements, though still meaningful to represent (Banaee \&amp; Loutfi, 2014).</p>
<p>Since conceptual spaces are developed to model the attributes of concepts for further reasoning, it can be employed as a robust framework to perform content determination using semantic inferences. The problem of modelling natural language using conceptual spaces is seldom investigated in the literature (Agostaro et al., 2005; Derrac \&amp; Schockaert, 2015). Aisbett, Rickard and Gibbon (2015) recently investigated the integration of conceptual spaces theory with the topic of computing with words by introducing a fuzzy representation of conceptual spaces' elements. Domains and dimensions in their work, however, are crisp elements with no role concerning the qualification of objects within the space. Also, Derrac and Schockaert (2015) attempted to derive the semantic relations within conceptual spaces built upon text documents. However, to the best of our knowledge, there is no study on the conceptual spaces to derive natural language descriptions for the numeric inputs through a conceptual representation. The advantage of using conceptual spaces over the fuzzy set theory (as the primary approach for linguistic description) is in the content determination task, where the inference of linguistic descriptions for unknown observations can be easily modelled. This description will include both similar concepts and representative features from a multi-domain space, while in the learned fuzzy rules, the linguistic description of observations includes the fuzzy linguistic labels (Aisbett et al., 2015).</p>
<h1>3. Data-Driven Construction of Conceptual Spaces</h1>
<p>This section presents how to automatically construct a conceptual space from a numeric data set. Our approach to constructing conceptual spaces is called data-driven because it is automatically constructed by processing the data matrix of the observations based on the variable values and class labels. This way of construction is in contrast with the knowledgedriven conceptual spaces that have to be manually constructed using psychologically or scientifically pre-defined knowledge about the relations between quality dimensions, domains and the concepts' regions (Gärdenfors, 2004; Agostaro et al., 2005).</p>
<p>The process of constructing a conceptual space is about determining its essential elements. According to Aisbett and Gibbon (2001), and Raubal (2004), the definition of a conceptual space is as follows:</p>
<p>Definition 1. A conceptual space $S$ is defined as a 4 -tuple $\langle\mathcal{Q}, \Delta, \mathcal{C}, \Gamma\rangle$, where $\mathcal{Q}$ is a set of quality dimensions, $\Delta$ is a set of domains, $\mathcal{C}$ is a set of concepts in the space $S$, and $\Gamma$ is a set of instances representing the concepts.</p>
<p>The representations of the elements are rigorously explained in further definitions (from 2 to 5). In order to automate the process of constructing conceptual spaces, the definitions of the conceptual spaces' elements are slightly modified compared to previous formulations (Adams \&amp; Raubal, 2009b; Rickard et al., 2007). These modifications consider the fact that the constructed conceptual space will be utilised as a semantic representation model for further inferences.</p>
<p>To start the constructing process, we assume that a given data set $\mathcal{M}$ contains a set of possible class labels, a set of predefined features, and the input observations with known</p>
<p>class labels, which are characterised by feature values. Formally, given a set of class labels $\mathcal{Y}=\left{y_{1}, \ldots, y_{m}\right}$ and a set of features $\mathcal{F}=\left{X_{1}, \ldots, X_{n}\right}$, let $\mathcal{D}$ be the set of known observations, denoted by $\mathcal{D}=\left{o_{i}:\left(\mathbf{x}<em i="i">{o</em>}}, y_{o_{i}}\right)\right}$, where $o_{i}$ consists of a n-dimensional feature vector $\mathbf{x<em i="i">{o</em>}}=\left[x_{1}, \ldots, x_{n}\right]$, and an output label $y_{o_{i}} \in \mathcal{Y}$. The component $x_{j}(j=1, \ldots n)$ in the vector $\mathbf{x<em i="i">{o</em>$.}}$ is the measured value of the corresponding feature $X_{j} \in \mathcal{F</p>
<p>Here, each feature $X$ is defined as a couple of values $X:\left\langle H_{X}, I_{X}\right\rangle$, where $H_{X}$ indicates the linguistic name of the feature, and $I_{X}$ is either a numeric interval or a categorical set that presents the possible range of values for $X$.</p>
<p>Example 1. Consider the leaf data set (Silva, Marcal, \&amp; da Silva, 2013) which is a set of photographed leaf samples (observation set $\mathcal{D}^{l}$ ) from various plant species (classes) such as: $\mathcal{Y}=\left{y_{\mathrm{qr}}:\right.$ 'Quercus Robur', $y_{\mathrm{ap}}:$ 'Acer Palmatum', $y_{\mathrm{no}}:$ 'Nerium Oleander', $y_{\mathrm{tt}}$ : 'Tilia Tomentosa', ... $}$. This data set includes a set of measurable features to characterise the features of each leaf sample, such as: $\mathcal{F}=\left{X_{\mathrm{el}}:\right.$ elongation, $X_{\mathrm{lo}}:$ lobedness, $X_{\mathrm{co}}:$ convexity, $X_{\mathrm{ro}}:$ roundness, $X_{\mathrm{so}}:$ solidity, $X_{\mathrm{in}}:$ indentation, $\left.\ldots\right}$. So, an observed leaf such as $o_{i} \in \mathcal{D}^{l}$ that is labelled by $y_{\mathrm{tt}}$ takes the feature values as: $o_{i}:\left(\mathbf{x}<em i="i">{o</em>}}, y_{\mathrm{tt}}\right)$, where $\mathbf{x<em i="i">{o</em>\right]$.}}=\left[x_{\mathrm{el}}, x_{1 \mathrm{o}}, x_{\mathrm{co}}, x_{\mathrm{ro}}, x_{\mathrm{so}}, x_{\mathrm{in}</p>
<p>The goal of this section is to find a mapping from the elements of a data set $\mathcal{M}$ to various elements needed to define a conceptual space $S$. In short, this mapping is achieved by performing the following steps:</p>
<ul>
<li>Initialise the primitive known concepts using the class labels. Consequently, the conceptual space $S$, which models the data set $\mathcal{M}$, will consist of a set of concepts $\mathcal{C}=\left{C_{1}, \ldots, C_{m}\right}$, where $|\mathcal{C}|=|\mathcal{Y}|$. Thus, the notation $C_{y}$ indicates the concept which corresponds to the class label $y \in \mathcal{Y} .^{1}$</li>
<li>Specify the quality dimensions $\mathcal{Q}$ and domains $\Delta$. The quality dimensions are specified via selecting a subset of the features such that $\mathcal{Q} \subset \mathcal{F}$, and the domains are determined based on ranking and grouping the set of selected features as the quality dimensions (Section 3.1).</li>
<li>Form the representation of each concept $C_{y}$ within the derived domains $\Delta$, based on the known corresponding instances (Section 3.2).</li>
</ul>
<p>Figure 1 illustrates the steps of constructing a conceptual space from a set of numeric data, which are explained in the following sections.</p>
<h1>3.1 Domain and Quality Dimension Specification: A Feature Selection Approach</h1>
<p>A data-driven approach to build a conceptual space makes no prior assumption about the domains. Rather, the known labelled observations and features are the inputs from which the quality dimensions and domains will be extracted. This approach aims to propose a set of observation-based associations between classes of data as concepts and the grouped subsets of features as domains. As a domain is an integrated subset of quality dimensions,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the main steps for constructing a conceptual space from a set of numeric data. The domain and dimension specification is explained in Section 3.1, and the concept representation is described in Section 3.2.
and the quality dimensions are the subset of the initialised features, the first step is to determine a subset of informative features. This determination is performed by applying feature selection methods. Before explaining these methods, we recall the formal definitions of a quality dimension and a domain. As mentioned before, these definitions are reshaped in a novel manner to be utilised in the task of semantic inference.</p>
<p>Definition 2. A quality dimension $q_{X} \in \mathcal{Q}$ is a triple $\left\langle H_{q}, I_{q}, \mu_{q}\right\rangle$, which corresponds to a selected feature $X \in \mathcal{F} . H_{q}$ is the linguistic name of the quality dimension $q_{X}$, which is equal to $H_{X}$ and $I_{q}$ indicates the range of possible values for the quality dimension $q_{X}$, which is equal to $I_{X} . \mu_{q}$ is defined as a family of fuzzy membership functions ${ }^{2}$ to map the subintervals of $I_{q}$ onto a set of linguistic terms.</p>
<p>Definition 3. A domain $\delta$ is a triple $\left\langle\mathcal{Q}(\delta), \mathcal{C}(\delta), \omega_{\delta}\right\rangle$, where $\mathcal{Q}(\delta) \subset \mathcal{Q}$ is the set of integral quality dimensions involved in $\delta, \mathcal{C}(\delta) \subset \mathcal{C}$ is the set of concepts that are represented in $\delta$, and $\omega(\delta)$ is a weight function ${ }^{3}$ presenting the assigned salient weight between a concept and a quality dimension in $\delta$.</p>
<p>Example 2. Consider the leaf data set from Example 1, suppose that a quality dimension is elongation, which is defined as $q_{\mathrm{el}}=\left\langle\right.$ 'elongation', $[0,1], \mu_{\mathrm{el}}$ ), and another one is lobedness, defined as $q_{10}=\left\langle\right.$ 'lobedness', $\left.(0, \inf ), \mu_{10}\right\rangle$. One can conceptualise the leaves in various domains such as Shape, Texture, Colour, etc. Then, both elongation and lobedness quality dimensions can belong to the shape domain. Moreover, $\mu_{\mathrm{el}}$ can return the linguistic labels for elongation as: 'circular', 'elliptical', 'elongated'.</p>
<p>Since we construct the domains in a data-driven way without involving prior knowledge, it can be difficult to assign a semantic interpretation to the constructed domains, which reflects human perception. However, the provided space is counted as a conceptual space because of its ability to represent the concept formation and the semantic similarities between concepts and instances across the domains. Thus, within the proposed model, the set of quality dimensions of a domain is an integral subset of all quality dimensions which are analytically relevant or dependent to each other.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Identifying the most characteristic features of the data from the initialised set of features is an essential task, which is generally performed by feature extraction approaches (Bengio, Courville, \&amp; Vincent, 2013). There are two principal ways to extract informative features: feature transformation and feature selection (Guyon, Gunn, Nikravesh, \&amp; Zadeh, 2008). The first approach finds a projection from the original feature space into a lower dimensional feature space. Transforming the original features into this lower dimensional space usually alters any associated descriptive attributes connected to the features. Therefore, the semantic meaning of the resulting features is often difficult, if not impossible, to assess (Guyon \&amp; Elisseeff, 2003). The second approach selects a subset of original features by keeping relevant features and discarding the irrelevant ones. The retained features are not altered, and the original semantic meaning of those features stays intact. Since our goal is to exploit external knowledge of the original features, we apply feature subset selection techniques.</p>
<p>Both relevance and redundancy are essential criteria to consider in feature selection. A subset of features is optimal if the relevance between selected features and the target classes is maximal, and the redundancy among the selected features is minimal. These two criteria guarantee that the selected features are adequate to distinguish the classes of data with the smallest number of features (Duch, 2006).</p>
<p>The proposed approach employs feature selection methods to specify the quality dimensions and domains in two phases: feature subset ranking and feature subset grouping. The feature subset ranking phase determines which features are most representative for every single target class, independent from other classes. Since a concept can rather be represented by one or several groups of features as domains, the feature subset grouping phase categorises the ranked features in a way to recognise what features are most related to each other based on their relevancy to the concepts. The following subsections show the two phases of the proposed solution, (1) feature subset ranking and (2) feature subset grouping, in order to specify the suitable quality dimensions within a set of domains.</p>
<h1>3.1.1 Feature Subset Ranking</h1>
<p>Feature subset selection algorithms are categorised as either filter methods or wrapper methods (Witten \&amp; Frank, 2005). Filter methods determine the subset of features based on the statistical characteristics of the input data set without referring to the used classifier. Wrapper methods are dependent on the learning algorithm (i.e., target classifier) that evaluates the selected subset of features based on the performance of the used learning algorithm. In the present work, the aim is to identify the meaningful set of understandable attributes out of predefined features, but not to classify the input data. So, filter methods are chosen to be used for feature selection, since this category of methods is independent of the final classifier approach and it derives an informative subset of features with respect to the input data set labels.</p>
<p>Filter methods rank the features using a scoring function, usually by employing a statistical or information theoretical measure to quantify relevance and redundancy. The top scored features are kept as selected features (or low scored ones are removed from the resulting subset). In this work, mutual information, one of the commonly employed scoring functions, is used (Brown, 2009). One such technique is MIFS (mutual information-based</p>
<p>feature selection) (Battiti, 1994). For an input set of features $\mathcal{F}$ and 2-class labelled data $\mathcal{D}$, MIFS adds the feature $X_{i} \in \mathcal{F}$ to the already chosen subset of features $\mathcal{F}^{\prime}$, in order to maximise</p>
<p>$$
I\left(\mathcal{D}, X_{i}\right)-\beta \sum_{X_{j} \in \mathcal{F}^{\prime}} I\left(X_{i}, X_{j}\right)
$$</p>
<p>where $I(Y, X)$ is the mutual information ${ }^{4}$ between the variables $Y$ and $X$ (Torkkola, 2008). The first term in equation 1 attempts to maximise the relevance of feature $X_{i}$ to target labelled data, and the second term tries to minimise the redundancy between $X_{i}$ and the already selected features in $\mathcal{F}^{\prime}$ (using a balancing parameter $\beta$ ). In our work, the term $I(Y, X)$ is estimated via histograms, but other estimation methods are applicable as well (Schaffernicht, Kaltenhaeuser, Verma, \&amp; Gross, 2010). The MIFS technique is generally a heuristic approximation since there is no independent assessment of the joint mutual information to determine when a feature is relevant to the class labels (Torkkola, 2008). The proposed method is not dependent on the use of the MIFS algorithm. It can be substituted by other approximations of the joint mutual information (Fleuret, 2004; Peng, Long, \&amp; Ding, 2005; Brown, Pocock, Zhao, \&amp; Luján, 2012). It is notable that different filter methods do not necessarily produce the same ranking of the features. However, the focus here is to reach to a good enough set of features representing the data classes with high relevance and low redundancy (Webb, 2003).</p>
<p>The proposed method for feature subset ranking starts with defining a new set of input data for each target label $y$, wherein the data set of known observations $\mathcal{D}$ is split into two classes: class $y$ including all the observations labelled by class $y$, denoted by $\mathcal{D}<em _bar_y="\bar{y">{y}$, and class $\bar{y}=\left{\mathcal{Y} \backslash y\right}$ including the rest of observations labelled by other classes than class $y$, denoted by $\mathcal{D}</em>}}$. Then the MIFS procedure is applied to the feature set $\mathcal{F}$ considering the generated 2-class data set $\mathcal{D<em y="y">{y \bar{y}}=\left{\mathcal{D}</em>\right}$. By separating one class (concept) of data from the other classes, the output of the feature ranking algorithm will return the features that individually characterise the observations of this concept and separate it from the rest. The output of the filter method for each label is a sorted list of features with a score for each feature. Formally, the output for a class $y$ is a ranked list of features with the highest scores according to $y$, as} \cup \mathcal{D}_{\bar{y}</p>
<p>$$
\mathcal{R}(y)=\left{\left(X, w_{y, X}\right) \mid X \in \mathcal{F}, w_{y, X} \in[0,1]\right}
$$</p>
<p>where $w_{y, X}$ is the normalised weight (or the score) that is assigned to the relation of feature $X$ and label $y$. The features in $\mathcal{R}(y)$ are the $k$ most relevant features of the class label $y$. From a conceptual point of view, these $k$ features of $\mathcal{R}(y)$ are the suitable candidates to be the quality dimensions that distinguish the concept $C_{y}$ from the other concepts. The score $w_{y, X}$ determines the importance of the selected feature $X$ to represent the class label $y$. From the conceptual space point of view, the scores indicating the weights show the significance of the chosen quality dimensions for $C_{y}$.</p>
<p>Algorithm 1 shows the steps for finding the ranked scored list of features for each label $y$. The output of the algorithm is then a set of filter method results for all the class labels, denoted by $\mathcal{R}=\left{\mathcal{R}\left(y_{1}\right), \ldots, \mathcal{R}\left(y_{m}\right)\right}$. In this algorithm, $\mathcal{F}^{\prime}$ is the set of all features that</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Algorithm 1: Feature Subset Ranking</h1>
<div class="codehilite"><pre><span></span><code>Function FeatureRanking<span class="p">(</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>Y<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>F<span class="p">})</span><span class="err">\</span><span class="p">)</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>R<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>F<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>emptyset<span class="err">\</span><span class="p">)</span>
foreach <span class="err">\</span><span class="p">(</span>y <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>Y<span class="p">}</span><span class="err">\</span><span class="p">)</span> do
    <span class="o">//</span> Define <span class="mi">2</span><span class="o">-</span>class data set
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span>y<span class="p">}</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span>o_<span class="p">{</span>i<span class="p">}:</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">},</span> y_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span> <span class="err">\</span>mid y_<span class="p">{</span>i<span class="p">}</span><span class="o">=</span>y<span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>bar<span class="p">{</span>y<span class="p">}}</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span>o_<span class="p">{</span>i<span class="p">}:</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">},</span> y_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span> <span class="err">\</span>mid y_<span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>neq y<span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span>y <span class="err">\</span>bar<span class="p">{</span>y<span class="p">}}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span>y<span class="p">}</span> <span class="err">\</span>cup <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>bar<span class="p">{</span>y<span class="p">}}</span><span class="err">\</span><span class="p">)</span>
    <span class="o">//</span> Find the list of top scored features <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>X<span class="p">,</span> w_<span class="p">{</span>y<span class="p">,</span> X<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>R<span class="p">}(</span>y<span class="p">)</span> <span class="err">\</span>leftarrow <span class="err">\</span>operatorname<span class="p">{</span>MIFS<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>D_<span class="p">{</span>y <span class="err">\</span>bar<span class="p">{</span>y<span class="p">}},</span> <span class="err">\</span>mathcal<span class="p">{</span>F<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>F<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>F<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span> <span class="err">\</span>cup<span class="err">\</span>left<span class="err">\</span><span class="p">{</span>X <span class="err">\</span>mid <span class="err">\</span>exists<span class="err">\</span>left<span class="p">(</span>X<span class="p">,</span> w_<span class="p">{</span>y<span class="p">,</span> X<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>R<span class="p">}(</span>y<span class="p">)</span> <span class="err">\</span>wedge X <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>F<span class="p">},</span> w <span class="err">\</span><span class="k">in</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
return <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>R<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>F<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>appeared (at least once) in the ranked features:</p>
<p>$$
\mathcal{F}^{\prime}=\bigcup_{y \in Y}\left{X \mid \exists\left(X, w_{y, X}\right) \in \mathcal{R}(y) \wedge X \in \mathcal{F}, w_{y, X} \in[0,1]\right}
$$</p>
<p>where $\mathcal{F}^{\prime} \subset \mathcal{F}$. The set of features $\mathcal{F}^{\prime}$ is the set of features to become quality dimensions. However, in feature grouping, some of these features may be filtered out from the target set of quality dimensions. Note that the time complexity of the feature subset ranking (MIFS) is quadratic (Bollacker \&amp; Ghosh, 1996) depending on the number of initial features.</p>
<p>Example 3. Continuing of the leaf data set in Example 1, suppose that after applying the MIFS method, elongation, lobedness, and roundness are selected as the top features for $y_{\mathrm{tt}}$, $\mathcal{R}\left(y_{\mathrm{tt}}\right)=\left{\left(X_{\mathrm{el}}, w_{y_{\mathrm{tt}}, X_{\mathrm{el}}}\right),\left(X_{\mathrm{lo}}, w_{y_{\mathrm{tt}}, X_{\mathrm{lo}}}\right),\left(X_{\mathrm{ro}}, w_{y_{\mathrm{tt}}, X_{\mathrm{ro}}}\right)\right}$. Also, elongation, roundness, and indentation are selected for $y_{\mathrm{no}}, \mathcal{R}\left(y_{\mathrm{no}}\right)=\left{\left(X_{\mathrm{el}}, w_{y_{\mathrm{no}}, X_{\mathrm{el}}}\right),\left(X_{\mathrm{ro}}, w_{y_{\mathrm{no}}, X_{\mathrm{ro}}}\right),\left(X_{\mathrm{in}}, w_{y_{\mathrm{no}}, X_{\mathrm{in}}}\right)\right}$. Then, $\mathcal{F}^{\prime}=\left{X_{\mathrm{el}}, X_{\mathrm{lo}}, X_{\mathrm{ro}}, X_{\mathrm{in}}\right}$.</p>
<h3>3.1.2 Feature Subset Grouping</h3>
<p>Based on the definitions in conceptual space theory, a quality dimension usually appears in a single domain along other relevant dimensions to represent a specific aspect of conceptualised observations (Gärdenfors, 2000; Zenker \&amp; Gärdenfors, 2015). It might be possible to have the same dimension in various domains, but this requires a priori knowledge (Banaee \&amp; Loutfi, 2014). Moreover, repeating dimensions in a multi-domain space increases the redundancy, and consequently decreases the distinction of the domains for a meaningful concept representation. Therefore, the selected features are divided into distinct partitions of features as target domains, in order to avoid either creating a single domain of full features, or repeating features in all of the constructed domains.</p>
<p>Here we propose a heuristic method to detect distinct subsets of features, where the features in each subset are highly representative of the most relevant classes. The output set $\mathcal{R}$ in Algorithm 1 is a set of ranked features for each label. It is obvious that some features might be repeated in the ranked set of different class labels in $\mathcal{R}$. From the information in set $\mathcal{R}$, the goal is to extract those subsets of features that are associated to each other based on their co-appearance in the ranked features of each class. We first introduce a</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A weighted bipartite graph with two sets of vertices from the labels $\mathcal{Y}$ and the selected features $\mathcal{F}^{\prime}$. Also, an example of a biclique shown in the highlighted edges.
graph representation of the label-feature relation, and then derive the correlated features using a greedy search on this graph. More specifically, we build up a bipartite graph and search for the bicliques that identify the most associated feature subsets (i.e., domains).</p>
<p>Let $G=\left(V_{\mathcal{Y}} \cup V_{\mathcal{F}^{\prime}}, E, w\right)$ be a bipartite graph with two sets of vertices $V_{Y}$ and $V_{\mathcal{F}^{\prime}}$, a set of edges $E$, and $w: V_{\mathcal{Y}} \times V_{\mathcal{F}^{\prime}} \rightarrow \mathbb{R}$ as a weight function for the edges. The vertex set $V_{\mathcal{Y}}$ denotes the class labels in $\mathcal{Y}$. The vertex set $V_{\mathcal{F}^{\prime}}$ denotes the top-ranked features in $\mathcal{F}^{\prime}$. A vertex $v_{y} \in V_{\mathcal{Y}}$ is connected to a vertex $v_{X} \in V_{\mathcal{F}^{\prime}}$ if $X \in \mathcal{F}^{\prime}$ has been selected for $y \in \mathcal{Y}$ in Algorithm 1. In other words, for each pair $\left(X, w_{y, X}\right) \in \mathcal{R}(y)$ a new edge $v_{y} v_{X}$ is added to the edge set $E$ of bipartite graph $G$ between vertices $v_{y}$ and $v_{X}$, where the weight of the edge $v_{y} v_{X}$ is denoted by $w\left(v_{y} v_{X}\right)=w_{y, X}$. Figure 2 is an illustration of such weighted bipartite graph $G$.</p>
<p>The idea of grouping features is to find the maximal connected subgraphs in $G$. More precisely, a subset of features which are all connected to the same set of classes is a suitable subset of features for feature grouping. A biclique $\hat{G} \subset G$ is a special bipartite graph where every vertex in one part of vertices is connected to all the vertices in the other part of the vertices. The highlighted edges in Figure 2 depicts an example of a biclique in the given bipartite graph. Let $\hat{G}$ be a biclique denoted by $\hat{G}=\left(\hat{V}<em _mathcal_F="\mathcal{F">{\mathcal{Y}} \cup \hat{V}</em>}^{\prime}}, \hat{E}, w\right)$, where $\hat{V<em _mathcal_Y="\mathcal{Y">{\mathcal{Y}} \subset V</em>}}$, $\hat{V<em _mathcal_F="\mathcal{F">{\mathcal{F}^{\prime}} \subset V</em>}^{\prime}}$. In this biclique, assume $\left|\hat{V<em _mathcal_F="\mathcal{F">{\mathcal{Y}}\right|=\hat{m},\left|\hat{V}</em>}^{\prime}}\right|=\hat{n}$, thus $|\hat{E}|=\hat{m} \times \hat{n}$. The proposed approach is looking for a biclique with the highest score as $\hat{G<em _hat_G="\hat{G">{\text {max }}$ among all the bicliques in $G$. The score of a biclique $\hat{G}$ is calculated using a scoring function $\operatorname{Score}</em>$, based on the weights of its edges, as follows:}</p>
<p>$$
\operatorname{Score}<em v__y="v_{y">{\hat{G}}=\sum</em>} \in \hat{V<em v__X="v_{X">{\mathcal{Y}}}\left(\prod</em>} \in \hat{V<em y="y">{\mathcal{F}^{\prime}}} w\left(v</em>
$$} v_{X}\right)\right) / \hat{n</p>
<p>This scoring function calculates the average of the weights associated in the biclique. Generally, this scoring function will return higher values for the bicliques with a higher number of class labels and lower number of features.</p>
<p>In the selected biclique $\left(\hat{G}<em _mathcal_F="\mathcal{F">{\max }\right)$, the involved features $\hat{V}</em>$ is eliminated from the graph $G$, since these features are already assigned to a domain. After that, the process of finding the best biclique repeats on the updated graph $G$ to find the next maximal biclique. Algorithm 2 shows the steps of determining the domains with feature subset grouping. In general, the problem of finding maximum edge biclique in a bipartite graph is an NP-complete problem (Shaham, Yu, \&amp; Li, 2016). However, the}^{\prime}}$ then will be the subset of features as the quality dimensions of a domain $\delta$. To identify the next domain, the set of features $\hat{V}_{\mathcal{F}^{\prime}</p>
<h1>Algorithm 2: Feature Subset Grouping</h1>
<div class="codehilite"><pre><span></span><code>Function FeatureGrouping \(\left(\mathcal{R}, \mathcal{F}^{\prime}, \mathcal{Y}\right)\)
    \(\Delta, Q \leftarrow \emptyset\)
    // Build bipartite graph
    \(V_{\mathcal{Y}} \leftarrow Y\)
    \(V_{\mathcal{F}^{\prime}} \leftarrow \mathcal{F}^{\prime}\)
    foreach \(\left(X, w_{y, X}\right) \in \mathcal{R}(y): y \in \mathcal{Y}\) do
        \(E \leftarrow E \cup v_{y} v_{X}\)
        \(w\left(v_{y} v_{X}\right) \leftarrow w_{y, X}\)
    \(G=\left(V_{\mathcal{Y}} \cup V_{\mathcal{F}^{\prime}}, E, w\right)\)
    // Find max cliques as domains
    do
        \(\hat{G}_{\max }\left(\hat{V}_{\mathcal{Y}} \cup \hat{V}_{\mathcal{F}^{\prime}}, \hat{E}, w\right) \leftarrow \operatorname{MaxBiclique}(G)\)
        if \(\hat{G}_{\max }=\emptyset\) then
            break
            \(\delta:\left\langle\mathcal{Q}(\delta), \mathcal{C}(\delta), \omega_{\delta}\right\rangle \leftarrow\left\langle\hat{V}_{\mathcal{F}^{\prime}}, \hat{V}_{\mathcal{Y}}, w\right\rangle\)
            \(\Delta \leftarrow \Delta \cup \delta\)
            \(\mathcal{Q} \leftarrow \mathcal{Q} \cup \mathcal{Q}(\delta)\)
            // Update bipartite graph
            \(G \leftarrow\left(V_{\mathcal{Y}} \cup\left(V_{\mathcal{F}^{\prime}} \backslash \hat{V}_{\mathcal{F}^{\prime}}\right),(E \backslash \hat{E}), w\right)\)
    until \(\left(\mathcal{Q}=\mathcal{F}^{\prime}\right)\)
    return \(\Delta, \mathcal{Q}\)
</code></pre></div>

<p>developed approximation method for feature grouping approach has polynomial time complexity (depending on the number of classes, not features) due to the fact that the degree of the class vertices in the constructed bipartite graph is a constant number (k). Thus, by scaling the features size or the class size, the approach is still computationally feasible.</p>
<p>As stated in Definition 3, a domain $\delta$ is a triple $\left\langle Q(\delta), C(\delta), \omega_{\delta}\right\rangle$. The weight function $\omega_{\delta}=\mathcal{C}(\delta) \times \mathcal{Q}(\delta) \rightarrow \mathbb{R}$ is a function presenting the assigned salient weight between a concept $C_{y} \in \mathcal{Y}(\delta)$ and a quality dimension $q_{X} \in \mathcal{Q}(\delta)$. For a chosen biclique $\hat{G}<em _mathcal_Y="\mathcal{Y">{\max }=\left(\hat{V}</em>}} \cup\right.$ $\left.\hat{V<em _delta="\delta">{\mathcal{F}^{\prime}}, \hat{E}, w\right)$, we can construct a domain $\delta$ as a triple $\left\langle\mathcal{Q}(\delta), \mathcal{C}(\delta), \omega</em>}\right\rangle=\left\langle\hat{V<em _mathcal_Y="\mathcal{Y">{\mathcal{F}^{\prime}}, \hat{V}</em>}}, w\right\rangle$. More specifically, for a constructed domain $\delta=\left\langle\mathcal{Q}(\delta), \mathcal{C}(\delta), \omega_{\delta}\right\rangle$, the set of quality dimensions is $Q(\delta)=\left{q_{X} \mid v_{X} \in \hat{V<em y="y">{\mathcal{F}^{\prime}}\right}$. Also, the set of concepts related to $\delta$ is defined as $C(\delta)=$ $\left{C</em>} \mid v_{y} \in \hat{V<em _delta="\delta">{\mathcal{Y}}\right}$. Then $\omega</em>\right)$.}\left(C_{y}, q_{X}\right)=w\left(v_{y} v_{X</p>
<p>It is worth noting that for the next iteration of finding bicliques, the vertices with the labels $\hat{V}_{\mathcal{Y}}$ of a chosen biclique are not eliminated while updating the bipartite graph, because a class label can be involved in other bicliques in further iterations. From a conceptual space point of view, it is also meaningful, since a concept can be represented in several domains.</p>
<p>Example 4. The corresponding bipartite graph to the ranked features from Example 3 is illustrated in Figure 3. In this bipartite graph, one biclique is highlighted, which can potentially be the best biclique. If so, then the features elongation and roundness will become the quality dimensions of a new domain $\delta$ as: $\mathcal{Q}(\delta)=\left{q_{\mathrm{el}}, q_{\mathrm{ro}}\right}$ and $\mathcal{C}(\delta)=$ $\left{C_{\mathrm{tt}}, C_{\mathrm{no}}\right}$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A bigraph graph and one selected biclique (blue edges) for the leaf example (explained in Example 4).</p>
<h1>3.2 Concept Representation: An Instance-Based Approach</h1>
<p>The essential concern to represent a concept in a conceptual space is to decide which are the most relevant quality dimensions and consequently most relevant domains. A concept may be represented in one domain or in several domains. The important point is that a concept is not necessarily associated with a certain subset of domains, but usually, one domain or a few numbers of domains are prominent to represent a concept (Gärdenfors, 2000). In Section 3.1, the selected features are grouped into a set of domains out of the extracted bicliques. Using the fact that each class label will be involved in at least one selected biclique, then the corresponding concept is assigned to (or associated with) a certain number of domains (at least one). With the output of Algorithm 2 we already know which concepts are associated with which domains.</p>
<p>It is possible that one concept appears in two bicliques, which means the concept is relevant to both specified domains. This fact is consistent with the conceptual spaces theory since a concept is not always represented within a single domain. The typical example is the concept of 'apple' which is represented with more than a single domain, such as colour, taste, size, etc. A concept with merely one related domain is called property (Gärdenfors, 2000). In fact, a property is a special form of a concept defined in a single domain (Gärdenfors \&amp; Warglien, 2012). For example, the colour 'green' is a property which is represented only in the colour domain. Thus, a concept can be specified as a single property within a single domain (e.g., green), or as a collection of properties within several domains (e.g., apple). Depending on the domain specification process, a class label in the input data set might be represented either as a property in one domain or as a concept in several domains. The problem of deriving the domains in a data-driven manner is that for a concept represented in several domains, there is no trivial interpretation for the meaning of its properties within the domains. This issue comes from the fact that the interpretation of the data-driven domains themselves is also tricky.</p>
<p>For a set of concepts $\mathcal{C}=\left{C_{y_{1}}, \ldots, C_{y_{n}}\right}$, the problem is how to formulate the geometrical representation of concepts in the conceptual space with the extracted set of domains $\Delta$. In general, a natural concept is a collection of regions across one or more domains along with a set of salient weights to the domains (Gärdenfors, 2000). For a concept $C_{y}$, let $\Delta(y) \in \Delta$ be a subset of domains that contain concept $C_{y}$ in their concept sets, as $\Delta(y)=\left{\delta_{i} \mid \delta_{i} \in\right.$ $\left.\Delta \wedge C_{y} \in C\left(\delta_{i}\right)\right}$, assuming that $|\Delta(y)|=k$. The concept $C_{y}$ is presented by a collection of sub-concepts, denoted: $C_{y}=\left{c_{y}^{1}, \ldots, c_{y}^{k}\right}$, where each $c_{y}^{i}$ is the representation of $C_{y}$ within the domain $\delta_{i} \in \Delta(y)$.</p>
<h1>Algorithm 3: Concept Representation</h1>
<p>Input: $\mathcal{D}<em y="y">{y} \subset \mathcal{D}$ : set of observations that are labelled by $y \in \mathcal{Y}$ and $\Delta(y) \subseteq \Delta$ : domains that contain $C</em>$ in their concept sets.
Output: A Concept $C_{y}=\left{c_{y}^{1}, \ldots, c_{y}^{k}\right}$, representing label $y$ in the conceptual space.
foreach $o \in \mathcal{D}<em o="o">{y}$ do
$\gamma</em>)$
// determine $p_{\gamma}^{1}, \ldots, p_{\gamma}^{k}$ in $\delta_{1}, \ldots, \delta_{k}$
$\Gamma(y) \leftarrow \Gamma(y) \cup \gamma_{o}$
// $\delta_{i}=\left\langle Q\left(\delta_{i}\right), C\left(\delta_{i}\right), \omega_{\delta_{i}}\right\rangle, 1 \leq i \leq k$
foreach $\delta_{i} \in \Delta(y)$
do
$c_{y}^{i} \leftarrow \emptyset$
foreach $\gamma \in \Gamma(y)$ do
$P_{y}^{i} \leftarrow P_{y}^{i} \cup\left{p_{\gamma}^{i}\right}$
$\eta \leftarrow$ ConvexHull $\left(P_{y}^{i}\right)$
$\phi \leftarrow\left{\omega_{\delta_{i}}\left(C_{y}, q^{i}\right) \mid C_{y} \in \mathcal{C}\left(\delta_{i}\right), q^{i} \in \mathcal{Q}\left(\delta_{i}\right)\right}$
$c_{y}^{i} \leftarrow\langle\eta, \phi\rangle$
$C_{y} \leftarrow C_{y} \cup c_{y}^{i}$} \leftarrow \operatorname{Vectorise}(o, \Delta(y), \mathcal{Q</p>
<p>Definition 4. A sub-concept $c_{y}^{i}$, representing the concept $C_{y}$ in the domain $\delta_{i}$, is defined as a tuple $\langle\eta, \phi\rangle$, where $\eta$ is the region representing the geometrical area of $C_{y}$ in the domain $\delta_{i}$, and $\phi$ is a set of weights indicating the assigned degrees of salience between $C_{y}$ and each quality dimension $q \in \mathcal{Q}\left(\delta_{i}\right)$.</p>
<p>In order to represent a concept, we define the representation of its sub-concepts. The following two subsections describe the way to formally represent a concept, by defining its regions and its set of weights, respectively. Algorithm 3 shows the steps of the concept representation, with the required parameters to represent a concept $C_{y}$.</p>
<h3>3.2.1 Convex Regions of Concepts</h3>
<p>The identification of the geometrical regions of concepts is based on the location of the known observations. The concept $C_{y} \in \mathcal{C}$ is represented using the subset of observations $\mathcal{D}<em 1="1">{y}=\left{o</em>}, o_{2}, \ldots, o_{n_{y}}\right}$ which are labelled with $y \in \mathcal{Y}$. We define $\Gamma(y)$ as the set of instances related to the observations in $\mathcal{D<em 1="1">{y}$, denoted by $\Gamma(y)=\left{\gamma</em> \Gamma(y)$.
Definition 5. An instance $\gamma$ related to the concept $C_{y}$ is a finite set of n-dimensional points $\gamma=\left{p_{\gamma}^{1}, \ldots, p_{\gamma}^{k}\right}$ with a one-to-one mapping from the instance points to the domains $\Delta(y)$, where $|\Delta(y)|=\left|C_{y}\right|=k$.}, \gamma_{2}, \ldots, \gamma_{n_{y}}\right}$. These instances then specify the geometrical representation of the concept $C_{y}$ as a set of regions within the domains. The set of all instances $\Gamma$ in a conceptual space $S$ is defined as: $\Gamma=\bigcup_{y \in \mathcal{Y}</p>
<p>An instance $\gamma_{o} \in \Gamma(y)$ is the representation of the observation $o \in \mathcal{D}<em o="o">{y}$. The points of $\gamma</em>}$ are the values of the associated quality dimensions, which are stored in the feature vector $\mathbf{x<em _gamma="\gamma">{o}$. Formally, each point $p</em>}^{i} \in \gamma_{o}$ in a domain $\delta_{i} \in \Delta(y)$ is a numeric vector of the values of the quality dimensions in $\delta_{i}$, denoted: $p_{\gamma}^{i}=<q_{1}\left(\gamma_{o}\right), \ldots, q_{\left|Q\left(\delta_{i}\right)\right|}\left(\gamma_{o}\right)>$, which is a sub-vector of the feature vector $\mathbf{x<em i="i">{o}$ that includes the features associated with the quality dimensions in $\mathcal{Q}\left(\delta</em>$ using the feature vector of the observation $o$ is called vectorisation.}\right)$. This process of determining the points of an instance $\gamma_{o</p>
<p>Since all the instances with the label $y$ have a point $p^{i}$ in domain $\delta_{i} \in \Delta(y)$, to identify the convex region $\eta$ of a sub-concept $c_{y}^{i}$, we need to know the location of all these points in the domain. Let $P_{y}^{i}$ be the collection of all the points which their corresponding instances are labelled by $y$, and these points are located in domain $\delta_{i}$. So, $P_{y}^{i}=\left{p_{\gamma_{1}}^{i}, p_{\gamma_{2}}^{i}, \ldots, p_{\gamma_{n_{y}}}^{i}\right}$, where $p_{\gamma_{j}}^{i} \in \gamma_{j}, \gamma_{j} \in \Gamma(y)$, and $j=1 \ldots n_{y}$.</p>
<p>Example 5. Figure 4a consists of two domains $\delta_{a}$ and $\delta_{b}$ with two and three quality dimensions, respectively. Assume a concept $C_{y}$ has two sub-concepts $c_{y}^{a}$ and $c_{y}^{b}$ within these domains. So, each instance $\gamma_{j} \in \Gamma(y)$ includes two points $p_{j}^{a}$ and $p_{j}^{b}$ located in the domains. Figure 4a depicts the set of points $P_{y}^{a}$ and $P_{y}^{b}$, which will be used to represent sub-concepts $c_{y}^{a}$ and $c_{y}^{b}$, respectively.</p>
<p>The convexity, connectedness, and betweenness are the geometrical criteria required to define a region for a concept in the theory of conceptual spaces (Gärdenfors, 2000). The convexity of concepts is crucial to facilitate the learnability of concepts through the instances (Gärdenfors, 2014). A convex region is a geometric structure within a multidimensional domain which satisfies convexity and connectedness criteria. There are various approaches to identify the convex region covering a set of giving points, such as convex hull and Voronoi tessellations algorithms, or defining an ellipsoid around the points (Gärdenfors, 2000; Adams \&amp; Raubal, 2009b). For our purpose, the convex hull (CH) is a more convenient choice among others because it also satisfies the betweenness criterion. Since the concepts' regions are formed by its instances, both ellipsoid and Voronoi regions assign some points of the space to a concept's region, which are not necessarily between the concept's known instances. For a sub-concept $c_{y}^{i}$, the convex region $\eta$ is defined as the convex hull (i.e., convex polytope) of the points in $P_{y}^{i}$, as: $\eta\left(c_{y}^{i}\right)=C H\left(P_{y}^{i}\right)$.</p>
<p>Example 6. Figure 4b shows the convex regions of the sub-concepts $c_{y}^{a}$ and $c_{y}^{b}$. The convex hull $\eta\left(c_{y}^{a}\right)$ is a 2 D polygon in $\delta_{a}$, and $\eta\left(c_{y}^{b}\right)$ is a 3 D polytope in $\delta_{b}$. These convex hulls are calculated based on the points $P_{y}^{a}$ and $P_{y}^{b}$ from the instances in $\Gamma(y)$ in Figure 4a.</p>
<h1>3.2.2 Context-Dependent Weights of Concepts</h1>
<p>Depending on the context, the salience given to various aspects of a concept may vary (Gärdenfors, 2000). In the example of the apple concept, in one context the taste domain might be more prominent, but in another context, shape domain can be salient. In contrast, in such examples of concepts that there is no common knowledge about the salience of the domains in various concepts, the data itself determines the salience of domains and quality dimensions and defines the context-based weights for the concepts. In other words, the observations from different contexts define which domain and quality dimensions are more important to represent the given concepts.</p>
<p>Example 7. For the example of leaf data set, suppose that shape and colour are the domains and suppose that we want to differ between the contexts of Swedish leaves and Japanese leaves. Knowing the common-sense knowledge about these contexts might be useless to determine the weights of the domains and dimensions. However, based on the observed data in each of these contexts, one can realise that e.g., the quality dimensions in</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Instances in $\Gamma(y)$ and their corresponding set of points, $P_{y}^{a}$ and $P_{y}^{b}$.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Convex regions of the sub-concepts $c_{y}^{a}$ and $c_{y}^{b}$.</p>
<p>Figure 4: A concept representation example in a conceptual space with domains $\delta_{a}$ and $\delta_{b}$.
the shape domain are more salient rather than the colour domain to represent the Swedish leaves, but this inference may not be necessarily valid for Japanese leaves.</p>
<p>These relative degrees of salience assigned to the dimensions of the domains implicitly represent the notion of context. Here, the context-dependent weights are already embedded in the representation by calculating the relevance of quality dimensions to the concepts (i.e., the weights of the bipartite graph) while specifying the domains. The salient weights $\phi$ for a sub-concept $c_{y}^{i}$ come from the assigned weights $\omega_{\delta_{i}}$ in $\delta_{i}$ between $C_{y} \in \mathcal{C}\left(\delta_{i}\right)$ and any quality dimension in $\mathcal{Q}\left(\delta_{i}\right)$. Formally:</p>
<p>$$
\phi\left(c_{y}^{i}\right)=\left{\omega_{\delta_{i}}\left(C_{y}, q^{i}\right) \mid C_{y} \in \mathcal{C}\left(\delta_{i}\right), q^{i} \in \mathcal{Q}\left(\delta_{i}\right)\right}
$$</p>
<p>So, each sub-concept has its own set of weights in relation to the domain's quality dimensions. This point individualises the definition of context-dependent weights from the</p>
<p>definition of context weights in other developed conceptual spaces. In other conceptual spaces, a set of overall weights is assigned to the domains without considering the role. However, here, for two independent sub-concepts, the assigned weights in the same domain might vary.</p>
<h1>4. Semantic Inference in Conceptual Spaces</h1>
<p>This section aims to design an inference approach in order to provide a semantic characterisation for a new set of unknown observations. The focus of this section is on solving two main questions: (1) how a new (unknown) observation is represented in a conceptual space, and (2) how this representation enables the inference of semantic descriptions for the observation. The first question refers to the problem of induction in conceptual spaces theory (Gärdenfors, 2005). To develop such inductions in a conceptual space, it is important to realise which concepts represent a new observed instance. Due to the geometrical representation of the conceptual space, the similarity between the instances in the space enables us to define the notion of inclusion as an operator to measure the similarity of new observations to the specified concepts within the metric domains. The second question refers to the problem of symbol grounding in the conceptual space theory (Aisbett \&amp; Gibbon, 2001). The inference of a semantic representation for any input observation in natural language is enabled by defining a symbol space.</p>
<p>In general, the proposed inference in a conceptual space consists of the following steps:</p>
<ul>
<li>Defining the symbol space, based on the prior knowledge for linguistic characterisation of the concepts and quality dimensions,</li>
<li>Inferring linguistic descriptions, for each new unknown observation, based on the inclusion of its corresponding instance in the concepts:</li>
<li>Inference in conceptual space: specifying the geometrical location of a new instance within the conceptual space, examining the inclusion of the instance, and determining the linguistic labels in the symbol space from the associated concepts and dimensions,</li>
<li>Inference in symbol space: annotating and characterising the instance based on the provided set of symbolic terms, and generating linguistic descriptions.</li>
</ul>
<p>Example 8. Consider the concepts and quality dimensions of the leaf conceptual space in Example 4. A new observed leaf can be either linguistically represented by a known concept (e.g., $C_{\text {no }}$ ) where "The new observation is a Nerium leaf.", or by a set of related quality dimensions (e.g., $q_{\text {el }}$ and $q_{\text {ro }}$ ), such as "The new observation is an elongated and lance-shaped leaf."</p>
<p>Figure 5 illustrates the step of inferring linguistic descriptions for an unknown observation through the constructed conceptual spaces and its corresponding symbol space. The details of each component are explained in Section 4.2, after formally defining the symbol space in Section 4.1.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Illustration of the steps of inferring linguistic descriptions for an unknown observation via the constructed conceptual spaces and its corresponding symbol space. The details of the semantic inference step are explained in Section 4.2.</p>
<h1>4.1 Symbol Space Definition</h1>
<p>According to a general formulation proposed by Aisbett and Gibbon (2001), a conceptual space can be augmented with a symbol space. This extension provides an internal mapping between geometrical elements in conceptual space (such as concepts, dimensions, domains, etc.) and the symbolic labels (typically words) in symbol space.</p>
<p>Definition 6. A symbol space $\mathcal{S}$ of size $n$ is a space containing $n$ symbol dimensions $\mathcal{L}^{\mathcal{S}}$, wherein each concept and quality dimension in the conceptual space is linked to a symbol dimension. Symbol dimensions are isomorphic to the real number interval $[0,1]$.</p>
<p>Based on the definition of Aisbett and Gibbon, the symbol dimensions need to be named by the primitive input labels of the associated concepts (classes) and quality dimensions (features). The construction of the symbol space is a knowledge-based process, wherein the prior knowledge is encoded (Aisbett \&amp; Gibbon, 2001). The prior knowledge specifies the symbolic expressions in natural language form, related to the elements of conceptual space $S$.</p>
<p>We propose a two-layer symbol space containing the symbol dimensions of the concepts, $\mathcal{L}^{\mathcal{C}}=\left[d_{C_{1}}, d_{C_{2}}, \ldots\right]$, as concept layer, and the symbol dimensions of quality dimensions, $\mathcal{L}^{\mathcal{Q}}=\left[d_{q_{1}}, d_{q_{2}}, \ldots\right]$, as quality layer. So, for every concept $C_{y} \in \mathcal{C}$, there is a symbol dimension in the concept layer, and for each quality dimension $q \in \mathcal{Q}$, there is a symbol dimension in the quality layer. Figure 6 shows the associations between the elements in a conceptual space and the two-layer symbol dimensions in a symbol space.</p>
<p>Any instance in a conceptual space is associated with a point in the symbol space, namely a symbol vector. For a given instance $\gamma$, the associated symbol vector $\mathcal{V}<em _gamma="\gamma">{\gamma}$ in $\mathcal{S}$ specifies the applicability of the symbol dimensions for $\gamma$ in the range 0 to 1 for each dimension (Aisbett \&amp; Gibbon, 2001). The symbol vector $\mathcal{V}</em>}$ is a concatenation of two vectors $\mathcal{V<em _gamma="\gamma">{\gamma}:&lt;\mathcal{V}</em>}^{\mathcal{C}}, \mathcal{V<em _gamma="\gamma">{\gamma}^{\mathcal{Q}}&gt;$, one vector in the concept layer and one vector in the quality layer, respectively. Thus, $\left|\mathcal{V}</em>\right|$.}^{\mathcal{C}}\right|=\left|\mathcal{L}^{\mathcal{C}}\right|$, and $\left|\mathcal{V}_{\gamma}^{\mathcal{Q}}\right|=\left|\mathcal{L}^{\mathcal{Q}</p>
<p>Example 9. Consider the conceptual space of leaves $S^{l}$ in Example 4. For a new leaf sample as an instance $\gamma$, the symbol vector $\mathcal{V}<em _gamma="\gamma">{\gamma}$ is defined as a 4 -dimensional vector with the concatenation of $\mathcal{V}</em>}^{\mathcal{C}}=\left\langle v_{d_{88}}, v_{d_{88}}&gt;\right.$, and $, \mathcal{V<em d__81="d_{81">{\gamma}^{\mathcal{Q}}=\left\langle v</em>&gt;.\right.$}}, v_{d_{88}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Conceptual Space</th>
<th style="text-align: center;">Symbol Space</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{C}=\left{C_{1}, C_{2} \ldots\right}$</td>
<td style="text-align: center;">Concept Layer</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{Q}=\left{q_{1}, q_{2} \ldots\right}$</td>
<td style="text-align: center;">$\mathcal{L}^{\mathcal{C}}=\left[d_{C_{1}}, d_{C_{2}}, \ldots\right]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Quality Layer</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathcal{L}^{\mathcal{Q}}=\left[d_{q_{1}}, d_{q_{2}}, \ldots\right]$</td>
</tr>
</tbody>
</table>
<p>Figure 6: Schematic of a conceptual space and the coupled symbol space.</p>
<p>The symbol vector is defined as a two-element vector, wherein each $v_{i} \in \mathcal{V}<em i="i">{\gamma}$ consists of a pair of values $v</em>}=$ (label, value). The label shows the related symbolic term, and the value shows how representative is the instance to the dimension $d_{i}$ (either how similar to its concepts or how related to the quality dimensions). The notion $\mathcal{V<em y="y">{\gamma}^{\mathcal{C}}\left(C</em>}\right)=v_{d_{\partial_{y}}}$ indicates the value of the symbol vector in the concept layer for the dimension related to the concept $C_{y}$, and similarly, $\mathcal{V<em j="j">{\gamma}^{\mathcal{Q}}\left(q</em>$. The further sections explain how the elements of a symbol vector for a new instance are assigned values based on the inclusion of the instance within the domains.}\right)=v_{d_{q_{j}}}$ indicates the value of the symbol vector in the quality layer for the dimension related to the quality dimension $q_{j</p>
<h1>4.2 Inferring Linguistic Descriptions for Unknown Observations</h1>
<p>For any given unknown observation, the goal is to infer a semantic description in natural language form. The core of the inference process is to cope with the notion of similarity in conceptual spaces. In order to place the new instances in the space and choose the best concepts that include (or are similar enough to) an observation (Rickard et al., 2007). The metric structure of a geometrical conceptual space enables the model to measure the semantic similarity of concepts and instances in the space (Adams \&amp; Raubal, 2009b). The proposed construction of the conceptual space in Section 3 facilitates these measurements since the representations of concepts and instances span across domains using the geometric elements i.e., convex regions and points. From the point of view of NLG, inferring linguistic descriptions for unknown observations covers the main tasks of an NLG pipeline for generating natural language text out of non-linguistic data: Content determination, Microplanning (including lexicalisation), and Realisation (Reiter et al., 2000). This phase employs various developed methods for linguistic descriptions (i.e., fuzzy set theory, see Ramos-Soto et al., 2016) in order to ease the process of quantifying the location of unknown samples within a conceptual space, and infer the proper linguistic terms.</p>
<p>The process of inferring linguistic descriptions for an instance $\gamma^{\prime}$ is presented in two following phases.</p>
<ul>
<li>Phase A: Inference in Conceptual Space, that first determines the inclusion of the new instance $\gamma^{\prime}$ in the concepts within the domains $\Delta\left(\gamma^{\prime}\right)$ using semantic similarity, and then sets the values of symbol vector $\mathcal{V}_{\gamma^{\prime}}$, (performing content determination).</li>
<li>Phase B: Inference in Symbol Space, that verbalises the symbol vector $\mathcal{V}_{\gamma^{\prime}}$ into a set of lexical items which are human-readable descriptions, (performing lexicalisation and realisation).</li>
</ul>
<p>For a given set of new observations $\mathcal{D}^{\prime}=\left{o_{i}^{\prime}:\left(\mathbf{x}<em i="i">{o</em>$. The details of the phases A and B are explained in the following sections.}^{\prime}}\right)\right}$, let $\gamma^{\prime}$ be the corresponding instance to the unknown observation $o^{\prime} \in \mathcal{D}^{\prime}$ which is not assigned to any of the known class labels of $\mathcal{Y}$. Also, let $\Delta\left(\gamma^{\prime}\right) \subseteq \Delta$ be a set of domains that $\gamma^{\prime}$ has corresponding points in each of them ${ }^{5}$, where $\left|\Delta\left(\gamma^{\prime}\right)\right|=k^{\prime</p>
<h1>4.2.1 Phase A: Inference in Conceptual Space</h1>
<p>In this phase, we first compare the new instance with each of the concepts using the similarity measure to check whether it is included within concept regions or not. Then, based on the result of this inclusion, we initialise a symbol vector and set its values in both concept layer and quality layer. From the NLG perspective, this performs the task of content determination (Reiter, 2007), that decides which set of information is required to characterise a new observation in the final description. The process of checking the inclusion is a simple fuzzy extension of an instance-based method that measures the membership of the new instance to the nearest labelled region of instances. Although it can be done by any classification approach, we formulate the process with respect to the definitions of the introduced conceptual space.</p>
<p>For an instance $\gamma^{\prime}$, the symbol vector $\mathcal{V}<em _gamma_prime="\gamma^{\prime">{\gamma^{\prime}}$ is calculated based on the inclusion of the instance points $p</em>$, and their relations to the sub-concepts' regions within the domains. One instance can be located in the space differently as follows:}}$ in different regions within the domains. As defined before, $\gamma^{\prime}$ is represented with a set of points $\gamma^{\prime}=\left{p_{\gamma^{\prime}}^{1}, \ldots, p_{\gamma^{\prime}}^{k^{\prime}}\right}$. In general, with placing a new instance in a conceptual space, four cases can occur. Without losing generality, assume $\gamma^{\prime}$ consists of two points in two domains $\delta_{a}$ and $\delta_{b}$, denoted by $\gamma^{\prime}=\left{p_{\gamma}^{a}, p_{\gamma}^{b}\right}$. Also assume that there are two concepts $C_{y_{1}}$ and $C_{y_{2}}$ that have been represented in one or both of these two domains. Figure 7 shows the four different cases concerning various positions of the points $p^{a}$ and $p^{b</p>
<ol>
<li>Totally included in a concept within all the domains (case one, Figure 7a),</li>
<li>Partially included in just a concept (case two, Figure 7b),</li>
<li>Partially included in two distinct concepts (case three, Figure 7c),</li>
<li>Not included in any concept (case four, Figure 7d).
<sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>It is notable that a new observation is not necessarily defined in all domains since there might be no calculated values for some of the features/quality dimensions. So, the corresponding instance may not have points in all the provided domains.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
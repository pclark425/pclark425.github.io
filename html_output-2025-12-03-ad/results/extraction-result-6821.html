<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6821 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6821</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6821</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-269449073</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.18130v1.pdf" target="_blank">Logic Agent: Enhancing Validity with Logic Rule Invocation</a></p>
                <p><strong>Paper Abstract:</strong> Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process. This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6821.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6821.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Agent (LA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-based, neuro‑symbolic constrained-generation framework that converts natural language into propositional logic and exposes callable inference-rule functions for LLMs to invoke, enforcing valid logical deduction at decoding time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic Agent (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that treats an LLM as a decision-making agent which selects from a toolkit of predefined propositional-logic functions (e.g., Contrapositive, Transitive, De_Morgans and categorical-proposition functions) and applies constrained generation to guarantee valid inference chains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neuro-symbolic: transformer LLM + callable propositional-logic functions + constrained decoding</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>No additional model finetuning reported; evaluated with few-shot in‑context examples and tested on benchmarks (ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Rule-guided generation: convert NL to structured logic, then prompt the LLM to choose and invoke formal deduction functions to build and verify inference chains (constrained decoding + callable tools).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mixed suite: multi-choice reading-comprehension logical reasoning (ReClor, AR-LSAT, LogiQA22), NLI (ConTRoL, NaN-NLI) and synthetic formal logic True/False tasks (RuleTaker, ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MCRC (multi-choice reading comp), NLI (entailment/contradiction/neutral), TF (yes/no formal logic proofs / propositional entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Varies by underlying LLM; examples: GPT-3.5-TURBO on RuleTaker LA 71.30% (Direct 55.33%), GPT-4 on ProofWriter LA 68.42% (Direct 61.58%), Davinci-002 on LogiQA22 LA 30.68% (Direct 11.02%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LA consistently improves accuracy over Direct-answering and CoT baselines across models and datasets (examples: +15.97 percentage points for GPT-3.5 on RuleTaker; +6.84 pp for GPT-4 on ProofWriter; +19.66 pp for Davinci-002 on LogiQA22).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Packaging logic rules as callable functions and enforcing constrained decoding yields consistent, sometimes large, accuracy gains in strict logical-reasoning tasks across both base and advanced LLMs, improving interpretability and validity of chains of reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependent on the underlying LLM capability; currently focused on propositional logic and select deduction rules (limited coverage of predicate/modal/advanced logics); potential additional compute cost (notably when using large models as parsers); generalizability to out-of-distribution real-world domains not yet established.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Agent: Enhancing Validity with Logic Rule Invocation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6821.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6821.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (LA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Logic Agent framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (chat transformer) used within the LA framework to parse natural language into propositional logic and invoke deduction functions, showing improved performance on complex logic benchmarks when constrained by LA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat-based large transformer model (exact parameter count not reported in paper); used both as the decision-making agent in LA and optionally as a neural logic parser.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; used with LA's neuro-symbolic constrained decoding (LLM + callable logic-function toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (pretrained by OpenAI); evaluated via few-shot in-context examples on listed benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>LA rule-guided generation (LLM selects and calls formal deduction functions); compared against Direct and CoT prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter (representative); also evaluated on ReClor, RuleTaker, NaN-NLI, ConTRoL</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>ProofWriter: synthetic formal logic dataset for generating implications/proofs and answering yes/no questions; ReClor/others test multi-step verbal logical reasoning and NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>True-or-False logical deduction (ProofWriter) and multi-choice logical reading comprehension (ReClor)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProofWriter: GPT-4 LA 68.42% vs Direct 61.58% and CoT 60.64%; ReClor: GPT-4 LA 89.47% vs Direct 88.54% and CoT 89.06%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>+6.84 percentage points on ProofWriter relative to Direct; modest gains on some MCRC datasets (sub-1 to few percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 benefits from LA especially on synthetic formal reasoning datasets that require precise rule application; LA increases correctness of inference chains and improves accuracy beyond both Direct and CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Using GPT-4 as a parser adds computation cost and sometimes variable parsing quality; GPT-4 still exhibits occasional inconsistency in constructing new logical expressions and remains constrained by LLM limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Agent: Enhancing Validity with Logic Rule Invocation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6821.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6821.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (LA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-TURBO with Logic Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5-TURBO evaluated under LA; shows large improvements on synthetic TF benchmarks when invoking LA's rule functions compared to Direct and CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-TURBO</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model (GPT-3.5 family) used with few-shot prompting; exact parameter count not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; used with LA constrained-decoding and callable logic functions</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained by OpenAI; evaluated with three in-context examples per task</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>LA rule-guided generation (decision-making LLM invokes logic-rule functions); compared to Direct and CoT baselines</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RuleTaker (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Synthetic dataset for testing formal rule application with yes/no (True/False) questions based on stated facts and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>True-or-False propositional logic deduction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>RuleTaker: GPT-3.5 Direct 55.33%, CoT 55.88%, LA 71.30%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>+15.97 percentage points over Direct; +15.42 pp over CoT on RuleTaker.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large relative gains on formal TF datasets indicate LA can substantially raise logical deduction accuracy for mid-tier LLMs that otherwise struggle with strict rule application.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Gains rely on the quality of parsing into logic and on LLM's ability to choose rules; LA's rule set currently limited to propositional logic constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Agent: Enhancing Validity with Logic Rule Invocation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6821.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6821.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAVINCI-002 (LA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAVINCI-002 with Logic Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Davinci-002 (GPT family base model) evaluated with LA shows major relative improvements on difficult MCRC datasets (e.g., LogiQA22) compared to direct/coT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DAVINCI-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT base model (API name Davinci-002) used with few-shot prompting and LA; paper does not report exact parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; used alongside LA's callable logic functions and constrained decoding</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained by OpenAI; evaluated with in-context examples on logical benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>LA rule-guided generation invoked by prompts; compared to Direct and CoT</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LogiQA22 (representative MCRC)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Chinese civil-service exam multi-choice reading-comprehension dataset focusing on logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-choice reading comprehension (logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LogiQA22: Davinci-002 Direct 11.02%, CoT 13.27%, LA 30.68%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>+19.66 percentage points over Direct; +17.41 pp over CoT on LogiQA22.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LA can substantially boost performance of base models on challenging MCRC logical reasoning datasets, turning poor baseline performance into competitive results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Absolute accuracy remains far below human ceilings on some datasets; LA improvements depend on accurate parsing of NL to logic and are limited to the rule set provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Agent: Enhancing Validity with Logic Rule Invocation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6821.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6821.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B (LA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-13B with Logic Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's open 13B LLaMA-2 model evaluated with LA shows modest improvements on TF benchmarks and MCRC when invoking the constrained logic toolkit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM from Meta with 13B parameters (reported in paper as LLaMA-2-13B); used with in-context examples plus LA invocation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; integrated with LA's callable propositional-logic functions and constrained decoding</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained by Meta; evaluated few-shot on listed benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>LA rule-guided generation invoked by the model acting as an agent to call inference functions</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RuleTaker (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Synthetic propositional logic True/False tasks requiring rule application.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>True-or-False propositional deduction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>RuleTaker: LLaMA-2 Direct 25.50%, CoT 25.44%, LA 28.79%. ProofWriter: Direct 23.39%, LA 25.11%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>+3.29 percentage points over Direct on RuleTaker; +1.72 pp on ProofWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LA improves reasoning accuracy for an open 13B model, though absolute gains are smaller compared to larger/advanced models—indicating LA helps but does not fully close the gap to top models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Improvements modest on smaller models; absolute performance still low on many tasks, reflecting dependency on base model reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Agent: Enhancing Validity with Logic Rule Invocation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6821.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6821.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B (LA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MIXTRAL-8X7B-V0.1 with Logic Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MistralAI Mixture-of-Experts model (Mixtral-8x7B) evaluated with LA showing meaningful accuracy gains on both MCRC and TF tasks when using rule invocation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MIXTRAL-8X7B-V0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-Experts (MoE) transformer by MistralAI (denoted 8x7B in paper), evaluated with LA rule-guided generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B (Mixture-of-Experts)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>MoE transformer; integrated with LA's callable logic functions</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained by MistralAI; evaluated few-shot on logical reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>LA constrained generation where the model selects and calls logic-rule functions</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Synthetic formal-logic True/False dataset requiring rule chaining and proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>True-or-False formal deduction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProofWriter: Mixtral Direct 44.80%, CoT 45.85%, LA 55.68%. Also MCRC/other datasets show gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>+10.88 percentage points over Direct on ProofWriter; +9.83 pp over CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mixtral shows large relative improvements with LA on formal reasoning tasks, demonstrating LA's effectiveness across model architectures including MoE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Although gains are substantial, absolute accuracy remains below advanced models (GPT-4) on many tasks; LA's rule set remains propositional.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Agent: Enhancing Validity with Logic Rule Invocation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6821.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6821.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that elicits intermediate step-by-step reasoning traces from LLMs (few-shot or zero-shot CoT) used as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought (prompting technique)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompting strategy applied to existing transformer LLMs to induce stepwise reasoning outputs without changing model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>prompting-based method (no architectural change)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Few-shot in-context exemplars used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-Thought prompting (eliciting intermediate reasoning steps); compared against Direct and LA methods.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same mixed suite of MCRC, NLI, and TF datasets used as baselines in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MCRC, NLI, TF (same tasks as LA evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT marginally outperforms Direct on average but inconsistently: examples from paper: GPT-3.5 ReClor Direct 56.28% vs CoT 56.90% (+0.62pp); GPT-4 ReClor Direct 88.54% vs CoT 89.06% (+0.52pp).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Slight average improvement over Direct prompting but sometimes detrimental; LA typically outperforms CoT across datasets and models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT provides modest improvements relative to Direct in some cases but lacks strict verification of logical validity; LA's explicit rule invocation yields stronger and more consistent gains for strict logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CoT does not guarantee logically valid intermediate steps; susceptible to hallucinated or invalid reasoning chains and less reliable for strict, rule-bound logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Agent: Enhancing Validity with Logic Rule Invocation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6821.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6821.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 parser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as neural logic parser</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using GPT-4 to translate natural language statements into the LA framework's propositional logic syntax (neural parsing) yields a small performance edge at additional computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (as parser)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 repurposed via prompting to output structured propositional logic that is compatible with LA's deduction functions; compared against the default deterministic parser.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer used as a neural-to-formal-logic translator</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Prompt-engineered examples for parsing; evaluation used same downstream datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Neural parsing of NL to propositional logic followed by application of LA functions</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (results reported in Table 3: ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same suite; Table 3 compares default parser vs GPT-4 parser for GPT-3.5-TURBO agent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Parsing to propositional logic as part of MCRC/NLI/TF evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using GPT-4 as parser produced slight improvements for GPT-3.5-TURBO: e.g., ReClor 59.73% (default) -> 60.65% (GPT-4 parser); LogiQA22 42.98% -> 44.07%; ConTRoL 62.01% -> 65.24%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Small positive deltas (typically 0.9–3.2 percentage points) when replacing default parser with GPT-4-based parsing for GPT-3.5-TURBO.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 can serve effectively as a neural parser, slightly improving downstream LA performance, but adds compute cost and exhibits some variability in parsing quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher computation and potential variability in parsing outputs; cost/benefit tradeoff depending on resources and need for absolute parsing consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Agent: Enhancing Validity with Logic Rule Invocation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>Logicllm: Exploring self-supervised logic-enhanced training for large language models <em>(Rating: 1)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 1)</em></li>
                <li>RuleTaker <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6821",
    "paper_id": "paper-269449073",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "LA",
            "name_full": "Logic Agent (LA)",
            "brief_description": "An agent-based, neuro‑symbolic constrained-generation framework that converts natural language into propositional logic and exposes callable inference-rule functions for LLMs to invoke, enforcing valid logical deduction at decoding time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Logic Agent (framework)",
            "model_description": "Framework that treats an LLM as a decision-making agent which selects from a toolkit of predefined propositional-logic functions (e.g., Contrapositive, Transitive, De_Morgans and categorical-proposition functions) and applies constrained generation to guarantee valid inference chains.",
            "model_size": null,
            "architecture_type": "neuro-symbolic: transformer LLM + callable propositional-logic functions + constrained decoding",
            "training_data": "No additional model finetuning reported; evaluated with few-shot in‑context examples and tested on benchmarks (ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, ProofWriter).",
            "reasoning_method": "Rule-guided generation: convert NL to structured logic, then prompt the LLM to choose and invoke formal deduction functions to build and verify inference chains (constrained decoding + callable tools).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Multiple (ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, ProofWriter)",
            "benchmark_description": "Mixed suite: multi-choice reading-comprehension logical reasoning (ReClor, AR-LSAT, LogiQA22), NLI (ConTRoL, NaN-NLI) and synthetic formal logic True/False tasks (RuleTaker, ProofWriter).",
            "task_type": "MCRC (multi-choice reading comp), NLI (entailment/contradiction/neutral), TF (yes/no formal logic proofs / propositional entailment)",
            "performance_metric": "Exact-match accuracy (%)",
            "performance_value": "Varies by underlying LLM; examples: GPT-3.5-TURBO on RuleTaker LA 71.30% (Direct 55.33%), GPT-4 on ProofWriter LA 68.42% (Direct 61.58%), Davinci-002 on LogiQA22 LA 30.68% (Direct 11.02%).",
            "comparison_with_baseline": "LA consistently improves accuracy over Direct-answering and CoT baselines across models and datasets (examples: +15.97 percentage points for GPT-3.5 on RuleTaker; +6.84 pp for GPT-4 on ProofWriter; +19.66 pp for Davinci-002 on LogiQA22).",
            "key_findings": "Packaging logic rules as callable functions and enforcing constrained decoding yields consistent, sometimes large, accuracy gains in strict logical-reasoning tasks across both base and advanced LLMs, improving interpretability and validity of chains of reasoning.",
            "limitations": "Dependent on the underlying LLM capability; currently focused on propositional logic and select deduction rules (limited coverage of predicate/modal/advanced logics); potential additional compute cost (notably when using large models as parsers); generalizability to out-of-distribution real-world domains not yet established.",
            "uuid": "e6821.0",
            "source_info": {
                "paper_title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4 (LA)",
            "name_full": "GPT-4 with Logic Agent framework",
            "brief_description": "GPT-4 (chat transformer) used within the LA framework to parse natural language into propositional logic and invoke deduction functions, showing improved performance on complex logic benchmarks when constrained by LA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI chat-based large transformer model (exact parameter count not reported in paper); used both as the decision-making agent in LA and optionally as a neural logic parser.",
            "model_size": null,
            "architecture_type": "transformer; used with LA's neuro-symbolic constrained decoding (LLM + callable logic-function toolkit)",
            "training_data": "Not specified in this paper (pretrained by OpenAI); evaluated via few-shot in-context examples on listed benchmarks.",
            "reasoning_method": "LA rule-guided generation (LLM selects and calls formal deduction functions); compared against Direct and CoT prompting baselines.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter (representative); also evaluated on ReClor, RuleTaker, NaN-NLI, ConTRoL",
            "benchmark_description": "ProofWriter: synthetic formal logic dataset for generating implications/proofs and answering yes/no questions; ReClor/others test multi-step verbal logical reasoning and NLI.",
            "task_type": "True-or-False logical deduction (ProofWriter) and multi-choice logical reading comprehension (ReClor)",
            "performance_metric": "Exact-match accuracy (%)",
            "performance_value": "ProofWriter: GPT-4 LA 68.42% vs Direct 61.58% and CoT 60.64%; ReClor: GPT-4 LA 89.47% vs Direct 88.54% and CoT 89.06%.",
            "comparison_with_baseline": "+6.84 percentage points on ProofWriter relative to Direct; modest gains on some MCRC datasets (sub-1 to few percentage points).",
            "key_findings": "GPT-4 benefits from LA especially on synthetic formal reasoning datasets that require precise rule application; LA increases correctness of inference chains and improves accuracy beyond both Direct and CoT baselines.",
            "limitations": "Using GPT-4 as a parser adds computation cost and sometimes variable parsing quality; GPT-4 still exhibits occasional inconsistency in constructing new logical expressions and remains constrained by LLM limitations.",
            "uuid": "e6821.1",
            "source_info": {
                "paper_title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-3.5 (LA)",
            "name_full": "GPT-3.5-TURBO with Logic Agent",
            "brief_description": "GPT-3.5-TURBO evaluated under LA; shows large improvements on synthetic TF benchmarks when invoking LA's rule functions compared to Direct and CoT prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-TURBO",
            "model_description": "OpenAI chat model (GPT-3.5 family) used with few-shot prompting; exact parameter count not provided in paper.",
            "model_size": null,
            "architecture_type": "transformer; used with LA constrained-decoding and callable logic functions",
            "training_data": "Pretrained by OpenAI; evaluated with three in-context examples per task",
            "reasoning_method": "LA rule-guided generation (decision-making LLM invokes logic-rule functions); compared to Direct and CoT baselines",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "RuleTaker (representative)",
            "benchmark_description": "Synthetic dataset for testing formal rule application with yes/no (True/False) questions based on stated facts and rules.",
            "task_type": "True-or-False propositional logic deduction",
            "performance_metric": "Exact-match accuracy (%)",
            "performance_value": "RuleTaker: GPT-3.5 Direct 55.33%, CoT 55.88%, LA 71.30%.",
            "comparison_with_baseline": "+15.97 percentage points over Direct; +15.42 pp over CoT on RuleTaker.",
            "key_findings": "Large relative gains on formal TF datasets indicate LA can substantially raise logical deduction accuracy for mid-tier LLMs that otherwise struggle with strict rule application.",
            "limitations": "Gains rely on the quality of parsing into logic and on LLM's ability to choose rules; LA's rule set currently limited to propositional logic constructs.",
            "uuid": "e6821.2",
            "source_info": {
                "paper_title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DAVINCI-002 (LA)",
            "name_full": "DAVINCI-002 with Logic Agent",
            "brief_description": "Davinci-002 (GPT family base model) evaluated with LA shows major relative improvements on difficult MCRC datasets (e.g., LogiQA22) compared to direct/coT prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DAVINCI-002",
            "model_description": "OpenAI GPT base model (API name Davinci-002) used with few-shot prompting and LA; paper does not report exact parameter count.",
            "model_size": null,
            "architecture_type": "transformer; used alongside LA's callable logic functions and constrained decoding",
            "training_data": "Pretrained by OpenAI; evaluated with in-context examples on logical benchmarks",
            "reasoning_method": "LA rule-guided generation invoked by prompts; compared to Direct and CoT",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LogiQA22 (representative MCRC)",
            "benchmark_description": "Chinese civil-service exam multi-choice reading-comprehension dataset focusing on logical reasoning.",
            "task_type": "Multi-choice reading comprehension (logical reasoning)",
            "performance_metric": "Exact-match accuracy (%)",
            "performance_value": "LogiQA22: Davinci-002 Direct 11.02%, CoT 13.27%, LA 30.68%.",
            "comparison_with_baseline": "+19.66 percentage points over Direct; +17.41 pp over CoT on LogiQA22.",
            "key_findings": "LA can substantially boost performance of base models on challenging MCRC logical reasoning datasets, turning poor baseline performance into competitive results.",
            "limitations": "Absolute accuracy remains far below human ceilings on some datasets; LA improvements depend on accurate parsing of NL to logic and are limited to the rule set provided.",
            "uuid": "e6821.3",
            "source_info": {
                "paper_title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaMA-2-13B (LA)",
            "name_full": "LLaMA-2-13B with Logic Agent",
            "brief_description": "Meta's open 13B LLaMA-2 model evaluated with LA shows modest improvements on TF benchmarks and MCRC when invoking the constrained logic toolkit.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B",
            "model_description": "Open-source LLM from Meta with 13B parameters (reported in paper as LLaMA-2-13B); used with in-context examples plus LA invocation.",
            "model_size": "13B",
            "architecture_type": "transformer; integrated with LA's callable propositional-logic functions and constrained decoding",
            "training_data": "Pretrained by Meta; evaluated few-shot on listed benchmarks",
            "reasoning_method": "LA rule-guided generation invoked by the model acting as an agent to call inference functions",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "RuleTaker (representative)",
            "benchmark_description": "Synthetic propositional logic True/False tasks requiring rule application.",
            "task_type": "True-or-False propositional deduction",
            "performance_metric": "Exact-match accuracy (%)",
            "performance_value": "RuleTaker: LLaMA-2 Direct 25.50%, CoT 25.44%, LA 28.79%. ProofWriter: Direct 23.39%, LA 25.11%.",
            "comparison_with_baseline": "+3.29 percentage points over Direct on RuleTaker; +1.72 pp on ProofWriter.",
            "key_findings": "LA improves reasoning accuracy for an open 13B model, though absolute gains are smaller compared to larger/advanced models—indicating LA helps but does not fully close the gap to top models.",
            "limitations": "Improvements modest on smaller models; absolute performance still low on many tasks, reflecting dependency on base model reasoning capability.",
            "uuid": "e6821.4",
            "source_info": {
                "paper_title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Mixtral-8x7B (LA)",
            "name_full": "MIXTRAL-8X7B-V0.1 with Logic Agent",
            "brief_description": "MistralAI Mixture-of-Experts model (Mixtral-8x7B) evaluated with LA showing meaningful accuracy gains on both MCRC and TF tasks when using rule invocation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MIXTRAL-8X7B-V0.1",
            "model_description": "Mixture-of-Experts (MoE) transformer by MistralAI (denoted 8x7B in paper), evaluated with LA rule-guided generation.",
            "model_size": "8x7B (Mixture-of-Experts)",
            "architecture_type": "MoE transformer; integrated with LA's callable logic functions",
            "training_data": "Pretrained by MistralAI; evaluated few-shot on logical reasoning benchmarks",
            "reasoning_method": "LA constrained generation where the model selects and calls logic-rule functions",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter (representative)",
            "benchmark_description": "Synthetic formal-logic True/False dataset requiring rule chaining and proof generation.",
            "task_type": "True-or-False formal deduction",
            "performance_metric": "Exact-match accuracy (%)",
            "performance_value": "ProofWriter: Mixtral Direct 44.80%, CoT 45.85%, LA 55.68%. Also MCRC/other datasets show gains.",
            "comparison_with_baseline": "+10.88 percentage points over Direct on ProofWriter; +9.83 pp over CoT.",
            "key_findings": "Mixtral shows large relative improvements with LA on formal reasoning tasks, demonstrating LA's effectiveness across model architectures including MoE.",
            "limitations": "Although gains are substantial, absolute accuracy remains below advanced models (GPT-4) on many tasks; LA's rule set remains propositional.",
            "uuid": "e6821.5",
            "source_info": {
                "paper_title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "Prompting technique that elicits intermediate step-by-step reasoning traces from LLMs (few-shot or zero-shot CoT) used as a baseline in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought (prompting technique)",
            "model_description": "A prompting strategy applied to existing transformer LLMs to induce stepwise reasoning outputs without changing model weights.",
            "model_size": null,
            "architecture_type": "prompting-based method (no architectural change)",
            "training_data": "Few-shot in-context exemplars used in experiments",
            "reasoning_method": "Chain-of-Thought prompting (eliciting intermediate reasoning steps); compared against Direct and LA methods.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Multiple (ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, ProofWriter)",
            "benchmark_description": "Same mixed suite of MCRC, NLI, and TF datasets used as baselines in the paper.",
            "task_type": "MCRC, NLI, TF (same tasks as LA evaluation)",
            "performance_metric": "Exact-match accuracy (%)",
            "performance_value": "CoT marginally outperforms Direct on average but inconsistently: examples from paper: GPT-3.5 ReClor Direct 56.28% vs CoT 56.90% (+0.62pp); GPT-4 ReClor Direct 88.54% vs CoT 89.06% (+0.52pp).",
            "comparison_with_baseline": "Slight average improvement over Direct prompting but sometimes detrimental; LA typically outperforms CoT across datasets and models in this paper.",
            "key_findings": "CoT provides modest improvements relative to Direct in some cases but lacks strict verification of logical validity; LA's explicit rule invocation yields stronger and more consistent gains for strict logical reasoning.",
            "limitations": "CoT does not guarantee logically valid intermediate steps; susceptible to hallucinated or invalid reasoning chains and less reliable for strict, rule-bound logical tasks.",
            "uuid": "e6821.6",
            "source_info": {
                "paper_title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4 parser",
            "name_full": "GPT-4 used as neural logic parser",
            "brief_description": "Using GPT-4 to translate natural language statements into the LA framework's propositional logic syntax (neural parsing) yields a small performance edge at additional computational cost.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (as parser)",
            "model_description": "GPT-4 repurposed via prompting to output structured propositional logic that is compatible with LA's deduction functions; compared against the default deterministic parser.",
            "model_size": null,
            "architecture_type": "transformer used as a neural-to-formal-logic translator",
            "training_data": "Prompt-engineered examples for parsing; evaluation used same downstream datasets",
            "reasoning_method": "Neural parsing of NL to propositional logic followed by application of LA functions",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Multiple (results reported in Table 3: ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, ProofWriter)",
            "benchmark_description": "Same suite; Table 3 compares default parser vs GPT-4 parser for GPT-3.5-TURBO agent.",
            "task_type": "Parsing to propositional logic as part of MCRC/NLI/TF evaluation pipeline",
            "performance_metric": "Exact-match accuracy (%)",
            "performance_value": "Using GPT-4 as parser produced slight improvements for GPT-3.5-TURBO: e.g., ReClor 59.73% (default) -&gt; 60.65% (GPT-4 parser); LogiQA22 42.98% -&gt; 44.07%; ConTRoL 62.01% -&gt; 65.24%.",
            "comparison_with_baseline": "Small positive deltas (typically 0.9–3.2 percentage points) when replacing default parser with GPT-4-based parsing for GPT-3.5-TURBO.",
            "key_findings": "GPT-4 can serve effectively as a neural parser, slightly improving downstream LA performance, but adds compute cost and exhibits some variability in parsing quality.",
            "limitations": "Higher computation and potential variability in parsing outputs; cost/benefit tradeoff depending on resources and need for absolute parsing consistency.",
            "uuid": "e6821.7",
            "source_info": {
                "paper_title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "Logicllm: Exploring self-supervised logic-enhanced training for large language models",
            "rating": 1,
            "sanitized_title": "logicllm_exploring_selfsupervised_logicenhanced_training_for_large_language_models"
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 1,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        },
        {
            "paper_title": "RuleTaker",
            "rating": 2
        }
    ],
    "cost": 0.0189755,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logic Agent: Enhancing Validity with Logic Rule Invocation
28 Apr 2024</p>
<p>Hanmeng Liu liuhanmeng@zju.edu.cn 
Zhiyang Teng zhiyang.teng@bytedance.com 
Chaoli Zhang chaolizcl@zjnu.edu.cn 
Yue Zhang zhangyue@westlake.edu.cn </p>
<p>Westlake University</p>
<p>Zhejiang Normal University</p>
<p>Westlake University</p>
<p>Logic Agent: Enhancing Validity with Logic Rule Invocation
28 Apr 2024C984E75210F399C73FB8C7D0C9DE0C47arXiv:2404.18130v1[cs.AI]
Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks.Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness.Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation.Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms.The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process.This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence.Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.Context:If the Moon's surface was once a magma ocean, then the distribution of many elements on it should be continuous.The magma ocean holds the key to unlocking the mystery of the Moon's origin.If the existence of a magma ocean is confirmed, then the 'Giant Impact Hypothesis' becomes the most plausible explanation for the Moon's origin.Question:From this, we can infer: Options: A. If the Moon's surface was never a magma ocean, then the distribution of elements on its surface is not continuous.B. If the 'Giant Impact Hypothesis' is not the most plausible explanation for the Moon's origin, then it indicates that the distribution of elements on the Moon's surface is not continuous.C. If the distribution of elements on the Moon's surface is not continuous, then the Moon's surface was never a magma ocean.D. If the distribution of elements on the Moon's surface is continuous, then the 'Giant Impact Hypothesis' will become the most plausible.</p>
<p>Introduction</p>
<p>The quest for augmenting the reasoning capabilities of language models has been a focal point of recent advancements in the evolving landscape of artificial intelligence.Chain-of-Thought (CoT) prompting (Wei et al., 2022;Kojima et al., 2022;Wang et al., 2022b;Chu et al., 2023) marked a significant stride in this journey, revealing the potential of large language models (LLMs) to mimic human-like reasoning processes.These advancements have led to remarkable achievements, with LLMs demonstrating proficiency in a variety of competitive examinations, including those focused on mathematics (Li et al., 2022;He-Yueya et al., Atoms: P: Moon's surface was a magma ocean Q: Continuous distribution of elements R: the 'Giant Impact Hypothesis' is the most plausible explanation Implies: 2023; Imani et al., 2023) and reading comprehension (Wang et al., 2023;Xiao et al., 2023).
P → Q P → R
However, despite its implications in various reasoning tasks, CoT has faced limitations, particularly in validating reasoning and ensuring the informativeness of its outputs.(Lanham et al., 2023) Their performance in logical reasoning tasks, a critical component of examinations like the Law School Admission Test (LSAT) and Chinese civil service selection exams, remains notably inferior to that of well-trained humans (liu et al., 2023).</p>
<p>Figure 1 shows an example of such questions.Crafted by experts to challenge human logical reasoning abilities, they require a valid and rule-bound chain of logic that is often non-trivial to discern.Testees must engage in abstract thinking, translating contexts into logical symbols and applying strict inference rules to form logical chains.This gap highlights a critical challenge: the ability of LLMs to consistently follow rules and verify the validity of logic chains.As illustrated in Figure 1, a GPT-4 model struggles with the deduction of the contrapositive law, despite having conceptual knowledge of it.One reason can be that there is no strict guarantee for a statistical system such as LLM to ensure correct complex reasoning chains across contexts.</p>
<p>Inspired by the integration of the neural network with formal symbolic solvers (Azerbayev et al., 2023;Jiang et al., 2023;Thakur et al., 2023), tooluse (Gao et al., 2023;Schick et al., 2023;Paranjape et al., 2023), and constrained decoding (Du et al., 2020;Geng et al., 2024), we address this issue by introducing Logic Agent (LA), an agent-based constrained generation framework, leveraging propositional logic and inference rules as fundamental guides to constructing logically sound inference chains.LA is designed to steer Large Language Models (LLMs) toward a trajectory of enhanced logical coherence and interpretability by introducing symbolic reasoning.In particular, we let an LLM serve as a decision-making agent and make a callable symbolic reasoning agent by assembling a set of essential formal logical rules.The LLM agent is taught to make use of the symbolic reasoning agent in its instructions so that formal reasoning steps can be guaranteed strictly correct.</p>
<p>With LA, LLMs are guided towards a path of logical coherence and interpretability.We first define the essentials of compositional logic, i.e. the logic components and syntax.This step serves as the initial step, converting complex natural language statements into structured compositional logic representations.Second, we define the functions for applying deduction rules, given a logic expression, we are able to form an inference chain with implicit logic.These functions are tools for LLMs to use.Lastly, we prompt LLMs to decide which rule to apply in different states.When LLMs call a rule, the output of the corresponding function is guaranteed to give valid logic chains for LLMs to make judgments on the truthfulness of the hypotheses.</p>
<p>In our study, we rigorously evaluated the Logic Agent (LA) framework using a mix of commercial and open-source Large Language Models, including OpenAI's GPT-4 and various Hugging Face models.Our findings, across this diverse range of models, consistently highlight LA's effectiveness in enhancing logical reasoning in complex tasks.Alongside our experimental insights, we're releas-ing our code to contribute to ongoing research.To the best of our knowledge, this is the first initiative to integrate propositional logic into LLMs at such a scale.</p>
<p>Related Work</p>
<p>Traditional pre-trained models have primarily tackled logical reasoning through statistical training, a connectionist approach that often misinterprets the complexity of language.Similarly, formal symbolic systems, while precise, struggle with the adaptability needed for diverse linguistic phenomena.This backdrop sets the stage for the introduction of new approaches to complex reasoning in Large Language Models (LLMs).</p>
<p>Reasoning Paradigms in Large Language Model Prompting: The development of fewshot (Wei et al., 2022) and zero-shot (Kojima et al., 2022) Chain-of-Thought prompting has been instrumental in enabling LLMs to tackle complex reasoning tasks.Subsequent developments have introduced varied data structures, such as Tree-of-Thought (Yao et al., 2023), Graph-of-Thought (Besta et al., 2024), and Program-of-Thought (Chen et al., 2022), enhancing LLMs' capabilities to reflect on and evaluate their reasoning processes.Moving beyond basic prompting strategies, the ReAct model (Yao et al., 2022) intertwines reasoning with actionable tasks like search, while the Selection-Inference framework (Creswell et al., 2023) employs a two-step process of context formation and logical chaining.Although these approaches parallel ours in process structure, they do not incorporate explicit logical rules, and the chaining mechanism is entirely modeldependent.The use of external tools within prompting paradigms, particularly for tasks necessitating additional knowledge, represents another significant advancement.In mathematical reasoning, tools such as calculators have proven invaluable.Analogously, in our methodology, predefined functions for applying inference rules are akin to external tools, a concept previously unexplored in this context.Another paradigm shift in LLM prompting is the division of complex tasks into subproblems or the collaborative engagement of diverse models.Cumulative reasoning (Zhang et al., 2023) adopts a streamlined, iterative approach utilizing distinct LLMs as AI agents; ScratchPad (Nye et al., 2021)  Implies( Atom(Moon's surface was once a magma ocean), Atom(the distribution of many elements on it should be continuous) ) Implies( Atom(existence of a magma ocean is confirmed), Atom(the 'Giant Impact Hypothesis' becomes the most plausible explanation for the Moon's origin) )</p>
<p>…</p>
<p>Option A Implies(Not(Atom(Moon's surface was once a magma ocean)), Not(Atom(the distribution of many elements on it should be continuous)) ) Given the original statement Implies( Atom(Moon's surface was once a magma ocean), Atom(the distribution of many elements on it should be continuous) ), applying Contrapositive(Atom(the distribution of many elements on it should be continuous), Atom(Moon's surface was once a magma ocean), ):Implies( Not(Atom(Moon's surface was once a magma ocean)), Not(Atom(the distribution of many elements on it should be continuous))), Option A can not be inferred from the contrapositive law.…</p>
<p>Parsed Logic</p>
<p>Logic rules Guided Generation Kalai, 2024) envisions LLMs as orchestrators in a collaborative environment, responsible for decomposing complex tasks, delegating sub-tasks to specialized models, facilitating inter-model communication, and applying critical analysis throughout.Our approach similarly harnesses the LLMs' decision-making capability in selecting appropriate inference rules, aligning with this broader trend of utilizing LLMs for complex, collaborative reasoning processes.Unlike previous attempts, we leverage the computational power and contextual understanding of LLMs to act as agents that dynamically invoke logic rules.This integration enables the LLMs to not only process language with their inherent sophistication but also apply logical reasoning in a structured and accurate manner, akin to utilizing a calculator for mathematical enhancements.Apart from that, recent studies have explored instruct-tuning Large Language Models (LLMs) with specific datasets to enhance their abstract reasoning capabilities.LogiCoT (Liu et al., 2023) fine-tunes an LLAMA-7B model using logical chaining data, demonstrating significant improvements across various logical reasoning tasks; LogicLLM (Jiao et al., 2023) employs a self-supervised post-training approach tailored for logical reasoning enhancements; Symbol-LLM (Xu et al., 2023) leverages symbolic data within a two-stage tuning framework to imbue a LLAMA-2-CHAT model with symbolic knowledge.While these approaches underscore the potential of finetuning strategies in augmenting LLMs, our work distinguishes itself as the first to specifically address and enhance logical reasoning capabilities at the decoding stage, employing a multi-agent strategy to elevate the process.Formal Reasoning: Formal reasoning systems have primarily been developed to address mathe-matical challenges.Peano (Poesia and Goodman, 2023), designed to solve educational mathematical problems, employs dependent types to encode mathematical definitions and proofs, echoing the structured approach in our work.Yet, our focus diverges towards logical reasoning scenarios, an area where systems like Peano have traditionally been less potent.Addressing formal logical reasoning, LINC (Olausson et al., 2023) leverages LLMs as FOL language translators to attain formal representations of contextual information, complemented by traditional theorem provers for validation.LINC's approach, which employs a voting strategy to resolve inconsistencies in FOL language generation, contrasts with our method which adopts a more flexible propositional logic to distill the abstract essence of context while meticulously controlling the validity of generative reasoning.Furthermore, the exploration of language models as theorem provers has introduced systems like Lang-Pro (Abzianidze, 2017), a natural language theorem prover that harnesses higher-order logic to assess linguistic expressions' consistency.LangPro's reliance on CCG parsing and a dedicated knowledge base for generating Lambda Logical Forms (LLFs) presents a contrast to our work, which utilizes propositional logic, thereby circumventing the need for a theorem-proving knowledge base.In parallel, semantic-constrained decoding techniques, as exemplified by NEUROLOGIC DE-CODING (Lu et al., 2020), enable language models to generate contextually coherent text while adhering to complex lexical constraints.Our approach resonates with this paradigm, albeit with a distinct focus on employing constrained generation paired with guided deduction rules, thereby carving a unique niche in the landscape of formal reasoning and logical inference.</p>
<p>Distinctively, we encapsulate the logical reasoning process into callable function forms, packaging logic rules as tools for LLM agents.This strategic shift in leveraging LLMs as autonomous decisionmakers, equipped with a toolkit of generalized logic reasoning functions, marks a significant departure from existing models.</p>
<p>Figure 2 presents the Logic Agent (LA) framework's architecture.Initially, natural language inputs undergo logic parsing on the left, resulting in structured logic forms (see Section 3.1).The center highlights the application of deduction rules for logical inference (see Section 3.2).Finally, on the right, the constrained generation process employs these inferences to produce contextually relevant and logically coherent outputs, illustrating the LA's systematic approach to enhancing reasoning in large language models (see Section 3.3).At the heart of LA lies the meticulous definition and utilization of compositional logic essentials, encompassing both the critical logic components and their associated syntax.This pivotal initial step involves the intricate transformation of complex natural language statements into structured representations of compositional logic.</p>
<p>Logical Construct Classes</p>
<p>Within LA, various classes of logical constructs are parsed and utilized.These include:</p>
<p>Variable: Represents a variable symbol in logical expressions.Atom: Denotes an atomic formula, the fundamental unit of logical statements.Not: Embodies the negation operation in logic.And: Indicates logical conjunction, combining multiple propositions.Or: Symbolizes logical disjunction, offering alternative propositions.Implies: Represents the implication relationship between propositions.Equiv: Denotes logical equivalence between statements.Exists and Forall: Represent existential and universal quantification, respectively, allowing for the expression of propositions about 'some' or 'all' entities within a domain.Rule-based functions within LA parse these logical constructs and quantified sentences, ensuring accurate representation and manipulation of logical expressions.</p>
<p>Inference Rules</p>
<p>On this foundational layer, LA incorporates a suite of defined functions for applying various deduction rules.These functions serve as advanced tools for LLMs, facilitating the formation of inference chains that integrate both explicit and implicit logic elements.This enables LLMs to navigate the complexities of logical deduction, maintaining structured and coherent reasoning throughout.</p>
<p>The key inference rules and their corresponding functions in LA include:</p>
<p>Contrapositive: A function applying the contrapositive law, turning implications into their logically equivalent forms.Transitive: A function for the transitive law, linking propositions through a common term.De_Morgans: Implements De Morgan's laws, transforming conjunctions and disjunctions while preserving logical equivalence.</p>
<p>We also integrate the foundational principles of categorical propositions, which is essential to syllogistic logic.There are four key proposition types: SAP (A) -Universal Affirmative, SIP (I) -Particular Affirmative, SEP (E) -Universal Negative, and SOP (O) -Particular Negative.Below are the corresponding functions:</p>
<p>Contradictory: A function handling contradictory relationships, identifying mutually exclusive propositions.Contrary: Manages contrary relationships, where two propositions cannot be true simultaneously but can be false together.</p>
<p>Subcontrary: Deals with subcontrary relationships, where two propositions cannot be false simultaneously but can be true together.Subalternation_forward and Subalternation_backward: Functions facilitating subalternation, capturing the inferential relationships between universal and particular propositions.Through these specialized functions, LA empowers LLMs to apply logical reasoning accurately and effectively, enhancing their capability to tackle complex reasoning tasks with a higher degree of precision and reliability.</p>
<p>Rule-Guided Generation</p>
<p>We prompt LLMs to discern and decide upon the most appropriate rule to apply in varying states of reasoning.This dynamic interaction empowers LLMs to judiciously invoke the corresponding functions, each meticulously crafted to guarantee the generation of valid logic chains.Consequently, LLMs are equipped with a powerful mechanism to scrutinize the veracity of hypotheses, making informed judgments based on the logically consistent chains produced.("E" refers to "entailment"; "C" refers to "contradiction"; "N" refers to "neutral".)</p>
<p>the guided generation process and leverage the capabilities of existing LLMs developed by OpenAI and HuggingFace.These models offer a robust starting point, owing to their advanced language understanding and processing abilities.However, our approach goes beyond the conventional use of LLMs by optimizing each component for its specific role in the logical reasoning process.This targeted optimization is key to transcending the current limitations of LLMs in handling the nuanced and rule-bound nature of logical reasoning tests.By integrating a structured, rule-guided reasoning methodology into the operational framework of LLMs, LA aims to improve not only the logical precision of these models but also their interpretability and coherence.The incorporation of propositional logic, deduction rules, and a strategic prompting mechanism positions LA as an innovative approach.It seeks to bridge the current divide between the computational efficiency of LLMs and the detailed, logical discernment typical of human reasoning.</p>
<p>Tasks</p>
<p>We consider various logical reasoning tasks, including Multi-Choice Reading Comprehension (MCRC), Natural Language Inference (NLI), and True-or-False questions (TF).</p>
<p>The datasets we use are listed in Table 1.Re-Clor (Yu et al., 2020), AR-LSAT (Wang et al., 2022a), andLogiQA22 (liu et al., 2023) are three renowned multi-choice reading comprehension datasets for logical reasoning.ReClor and AR-LSAT are collected from verbal reasoning questions in competitive tests like the LSAT (Law School Admission Test) exam.LogiQA22 is collected from the Chinese Civil Service Examination in the year 2022.ConTRoL (Liu et al., 2021) and NaN-NLI (Truong et al., 2022) are two logical reasoning datasets for the natural language inference task.The task is to decide whether a hypothesis can be logically entailed by the premises.ConTRoL features entailment relationships for long texts, and NaN-NLI is for negations.Both datasets are threeway classification tasks.RuleTaker (Clark et al., 2020) and ProofWriter (Tafjord et al., 2020) are two synthetic datasets widely used in formal logic reasoning.They take the form of yes-or-no questions, which are designed to test the ability of models to understand and apply rules and facts stated in natural language.</p>
<p>The tasks are evaluated with few-shot prompting, we use three in-context examples, covering different inference rule scenarios.For the implementation, we use a series of models from the OpenAI suite, including DAVINCI-002, GPT-3.5-TURBO, and GPT-4.DAVINCI-002 is the GPT base model currently supported by OpenAI API.GPT-3.5-TURBO and GPT-4 are two chat models available in the OpenAI API.Furthermore, we extend our evaluation to Huggingface models like LLAMA-2-13B (Touvron et al., 2023) and MIXTRAL-8X7B-V0.1 (Jiang et al., 2024), thereby encompassing a broad spectrum of AI models.LLAMA-2-13B is a 13B open LLM developed by Meta.MIXTRAL-8X7B-V0.1 is a Mixture-of-Expert (MoE) model developed by MistralAI.This diverse selection includes both base and instruction-tuned models, covering a range of open-source and closed-source options, to provide a comprehensive overview of the capabilities and performance variations across different AI architectures in logical reasoning tasks.We use the guidance library1 for implementing our rule-constrained generation framework.</p>
<p>Experiments</p>
<p>We employ a diverse range of datasets and models to ensure a robust and thorough assessment of our framework.We detail our experimental setup, the metrics used for evaluation, and our main findings.</p>
<p>Experimental Setup</p>
<p>Baselines: Our experimental baselines comprise two distinct approaches: direct answering and Chain-of-Thought (CoT) reasoning.To facilitate a fair comparison between base models and instruction-tuned models, we provide three incontext examples for both the direct answering and the CoT scenarios.This approach aids LLMs in generating answers that can be directly compared with the gold labels.</p>
<p>Data preprocessing</p>
<p>• For the Multiple-Choice Reading Comprehension (MCRC) task, we combine the context, question, and options to form a single input.</p>
<p>• In Natural Language Inference tasks, premises and hypotheses are concatenated, with a distinct identifier prefacing each segment.</p>
<p>• For True-or-False questions, we concatenate the context with the question to generate a cohesive input prompt.</p>
<p>Metrics To assess the performance of LLMs in our experiments, we employ the exact-match metric.This involves prompting LLMs to generate answers either as the first token (direct answer) or at the end of the generation process (CoT and LA).The extracted answers are then compared with the gold labels to calculate the accuracy score.</p>
<p>Results</p>
<p>The primary outcomes of our experiments are summarized in Table 2, where we juxtapose the performances of different models under various logical reasoning tasks.These tasks span multiple-choice reading comprehension (MCRC), natural language inference (NLI), and true-or-false (TF) questions, utilizing datasets such as ReClor, AR-LSAT, and LogiQA22 for MCRC, ConTRoL and NaN-NLI for NLI, and RuleTaker and ProofWriter for TF tasks.The human performance benchmarks, as referenced in the table, are sourced from prior research (Yu et al., 2020;Wang et al., 2022a;liu et al., 2023).</p>
<p>Direct Answer vs. Chain-of-Thought (CoT): Our analysis reveals that, in the context of the logical reasoning tasks tested, the few-shot CoT approach marginally outperforms the direct answer methodology.However, this superiority is not uniform across all cases.In certain instances, the CoT method appears to detrimentally impact the overall results, suggesting limitations in the effectiveness of CoT prompting in some logical reasoning scenarios.This observation highlights the inherent challenge in using CoT prompting to navigate the complexities of logical reasoning, especially in tasks where intricate inference is required.</p>
<p>Performance Across Models: Our further analysis delves into the performance distinctions across various models, highlighting the contrasts between advanced models such as GPT-4 and base models like DAVINCI-002 and LLAMA-2-13B.DAVINCI-002, as a base model, shows distinct performance characteristics under the LA framework.For instance, in the MCRC task on the ReClor dataset, DAVINCI-002 under LA achieves a 27.45% accuracy, a notable improvement from its Direct answer performance at 20.41%.This trend is consistent across other datasets, such as in LogiQA22, where DAVINCI-002's accuracy increases from 11.02% (Direct) to 30.68% (LA).These results suggest that the structured reason-ing provided by LA can significantly enhance the logical reasoning abilities of even base models, enabling them to outperform their standard configurations.</p>
<p>Similarly, LLAMA-2-13B, another base model, exhibits a marked performance enhancement with the application of LA.In the TF task using the RuleTaker dataset, LLAMA-2-13B registers an accuracy of 28.79% under LA, compared to 25.50% in the Direct answering format.In the more challenging ProofWriter dataset, the model improves from 23.39% (Direct) to 25.11% (LA).These improvements, while not as pronounced as those seen with advanced models like GPT-4, nonetheless indicate that LA can elevate the performance of base models in logical reasoning tasks.</p>
<p>Comparatively, advanced models like GPT-4 demonstrate a more significant leap in performance with the LA approach.This is particularly evident in datasets that require complex logical deductions, such as ProofWriter, where GPT-4 with LA achieves a 68.42% accuracy, substantially higher than both its Direct (61.58%) and CoT (60.64%) counterparts.</p>
<p>This comparative analysis across different models underscores the versatility of the LA framework.While advanced models like GPT-4 naturally exhibit higher baseline performances, the introduction of LA leads to substantial improvements in logical reasoning tasks across all model types, including base models like DAVINCI-002 and LLAMA-2-13B.This suggests that LA's structured, rule-guided reasoning approach is universally beneficial, enhancing the logical reasoning capabilities of a wide range of LLMs.</p>
<p>LA's Efficacy: The implementation of LA consistently enhances accuracy across various datasets, underscoring its effectiveness in logical reasoning.In the TF tasks using the RuleTaker dataset, LA with GPT-3.5 achieves an impressive 71.30% accuracy, a substantial leap from the 55.33% in the Direct approach and 55.88% in the CoT approach.Similarly, in the ProofWriter dataset, GPT-3.5 with LA reaches 73.85% accuracy, outperforming both its Direct (54.68%) and CoT (53.02%) formats.These figures highlight LA's capability to significantly refine the reasoning process in LLMs, enabling them to handle complex logic with greater precision and reliability.The improvement is even more pronounced with advanced models like GPT-4, where the accuracy in the RuleTaker dataset jumps to 65.84% under LA, compared to 59.85% (Direct) and 61.43% (CoT).This consistent pattern across various models and datasets firmly establishes LA as a transformative approach in logical reasoning, bridging the gap between computational AI and nuanced human-like reasoning.We present a detailed case study in Appendix A. This case study meticulously demonstrates how LA navigates complex logical reasoning tasks, showcasing its capabilities and the enhancements it brings to the decoding stage of Large Language Models (LLMs).</p>
<p>Task-Specific Insights: Delving into taskspecific performances, we observe that LA aligns exceptionally well with the demands of MCRC and NLI tasks, as evidenced by GPT-4's superior performance in the ReClor and NaN-NLI datasets.The tailored application of LA's rule-based reasoning to each task's unique requirements elucidates its broad applicability and effectiveness.The differential performance uplifts across datasets highlight the adaptability of LA.For instance, the significant accuracy increase in the ProofWriter dataset for GPT-4 underscores LA's capacity to handle datasets requiring complex logical deductions.This adaptability is crucial for tailoring reasoning enhancements to specific task demands.</p>
<p>Discussion</p>
<p>GPT-4 as Logic Parser</p>
<p>GPT-4, despite its occasional inconsistencies in generating new logical expressions, exhibits a noteworthy capability in parsing natural language into formal logic.This ability is particularly relevant to our "Chain-of-Logic" (LA) framework, where accurate translation of natural language into propositional logic is crucial.</p>
<p>To harness GPT-4's parsing capabilities, we crafted specific prompts aimed at guiding the model to translate natural language statements into propositional logic forms.These forms are then seamlessly integrated into the deduction functions of LA.A critical requirement for this integration is the compatibility of GPT-4's output with our framework's syntax.Therefore, the prompts are designed not only to elicit the correct logical structures but also to ensure that these structures adhere to the syntax conventions of our default parser.</p>
<p>To evaluate the effectiveness of GPT-4 in this role, we conducted experiments comparing its parsing capabilities with our default logic parser.The comparative results, as detailed in strate a slight edge in performance when utilizing GPT-4 as a parser.This finding underscores the efficiency and accuracy of GPT-4 in interpreting and translating complex logical statements from natural language into formal logic constructs.However, it's important to consider the trade-offs involved.Utilizing GPT-4 as a parser introduces additional computational costs, and there may be instances of variability in the parsing quality.These factors necessitate a careful assessment of the costbenefit ratio, especially in scenarios where computational resources are a limiting factor or where absolute consistency in logic parsing is critical.</p>
<p>Our findings suggest that while GPT-4 can effectively augment our framework as a neural parser, its integration should be strategically employed, taking into account the specific requirements and constraints of the given logical reasoning task.The potential of GPT-4 to enhance the versatility and adaptability of logical reasoning frameworks is clear, yet its application needs to be tempered with an understanding of its limitations and costs.</p>
<p>Ablation study</p>
<p>An essential aspect of our research was to ascertain the specific contribution of the parsed logic within the LA method.To achieve this, we conducted an ablation study where we tested the impact of augmenting text with parsed logic on the direct answer approach, while deliberately omitting the constrained generation component integral to LA.</p>
<p>This approach allowed us to isolate and understand the effectiveness of the logic parsing process in isolation.By comparing the performance of models using only parsed logic-augmented text for direct answering with their performance under the full LA framework, we could assess the incremental value added by the constrained generation aspect of LA.</p>
<p>We choose one dataset from each task and use GPT-3.5-TURBO as the tested model.The results are shown in Figure 3. Across the three datasets, we observed a noticeable decrease in accuracy when the models were deprived of the constrained generation process and relied solely on the parsed logicaugmented text.This decline in performance underscores the significance of the constrained generation component in the LA framework.It highlights that while the logic parsing capability is a valuable contributor to the model's overall performance, the full potential of LA is realized only when it is coupled with the sophisticated generation constraints that guide the model towards more logically coherent and accurate conclusions.</p>
<p>Conclusion</p>
<p>In this study, we present Logic Agent (LA), an innovative framework guided by logic rules to enhance the logical reasoning capabilities of Large Language Models (LLMs).Our comprehensive experiments across various models and datasets demonstrate that LA, with its integration of propositional logic and deduction rules, consistently surpasses traditional reasoning approaches.Notably, it shows superior performance in tasks requiring intricate logical deductions, highlighting its potential to bridge the gap between AI computational power and human-like logical reasoning.The exploration of GPT-4 as a neural logic parser further reveals the feasibility and challenges of incorporating advanced LLMs within logical reasoning systems.Looking ahead, the refinement of LA for broader applications and its scalability remain pivotal areas for future research.In sum, the LA framework not only elevates the performance of LLMs in complex reasoning tasks but also paves the way for more sophisticated and interpretable AI reasoning capabilities.</p>
<p>Limitations</p>
<p>While the Logic Agent (LA) framework presents significant advancements in logical reasoning with Large Language Models (LLMs), it is important to acknowledge its limitations for a balanced understanding of our research.</p>
<p>Dependency on Model Capability: The effectiveness of LA is partly contingent on the underlying capabilities of the LLMs used.This dependency indicates that the full potential of LA might be limited by the current state of LLM technology.</p>
<p>Scope of Logical Reasoning: Currently, LA primarily focuses on propositional logic and certain deduction rules.Its applicability to other forms of logic, such as predicate logic or modal logic, has not been extensively explored, which might restrict its utility in more diverse logical reasoning scenarios.</p>
<p>Generalizability: While the framework has shown promise across various datasets, the generalizability of LA to real-world scenarios or domains beyond those tested remains an area for future investigation.</p>
<p>A Case Study</p>
<p>Question:</p>
<p>If a person is wise, then they must be humble; and a person can only be humble if they recognize their own shortcomings.However, if a person cannot accept others' opinions, then they will not recognize their own shortcomings.From this, we can deduce:</p>
<p>A. If a person recognizes their own shortcomings, they are wise.</p>
<p>B. If a person cannot accept others' opinions, they are not wise.</p>
<p>C. If a person can accept others' opinions, they will recognize their own shortcomings.</p>
<p>D. If a person does not recognize their own shortcomings, they definitely cannot accept others' opinions.</p>
<p>Reasoning:</p>
<p>To solve this, we'll first translate the given statement and the options into first-order logic notations, then use logical deduction rules, including contraposition, to determine which option is a valid inference.</p>
<p>Step 1: Identify Key Propositions and Relations Given Premises Translated into Functions: Premise 1: Implies(Atom(Wise), Atom(Humble)) Premise 2: Implies(Atom(Humble), Atom(RecognizeShortcomings))</p>
<p>Premise 3: Implies(Not(Atom(AcceptOpinions)), Not(Atom(RecognizeShortcomings)))</p>
<p>Translating Options into Functions: Option A: "If a person recognizes their own shortcomings, they are wise."</p>
<p>Implies(Atom(RecognizeShortcomings), Atom(Wise))</p>
<p>Option B: "If a person cannot accept others' opinions, they are not wise."Implies(Not(Atom(AcceptOpinions)), Not(Atom(Wise)))</p>
<p>Option C: "If a person can accept others' opinions, they will recognize their own shortcomings."</p>
<p>Implies(Atom(AcceptOpinions), Atom(RecognizeShortcomings))</p>
<p>Option D: "If a person does not recognize their own shortcomings, they definitely cannot accept others' opinions."</p>
<p>Implies(Not(Atom(RecognizeShortcomings)), Not(Atom(AcceptOpinions))) Logical Deduction Using Functions:</p>
<p>Step 2: Use Deduction Laws Applying Transitive(Implies(Atom(Wise), Atom(Humble)), Implies(Atom(Humble), Atom(RecognizeShortcomings))): Implies(Atom(Wise), Atom(RecognizeShortcomings))</p>
<p>Applying Contrapositive(Implies(Not(Atom(AcceptOpinions)), Not(Atom(RecognizeShortcomings)))): Implies(Atom(RecognizeShortcomings), Atom(AcceptOpinions))</p>
<p>Evaluating Options: Option A: "Recognizing shortcomings makes one wise."This would require reversing our transitive output, not directly supported by the logic functions used.</p>
<p>Option B: "Not accepting opinions implies not being wise."This follows from the combined use of premises through transitive and contrapositive applications, suggesting Implies(Not(Atom(AcceptOpinions)), Not(Atom(Wise))).</p>
<p>Option C: "Accepting opinions leads to recognizing shortcomings."Against the contrapositive application of premise 3.</p>
<p>Option D: "Not recognizing shortcomings means not accepting opinions."Against the contrapositive application of premise 3.</p>
<p>Based on the function calls and logical deductions made within the LA framework, Option B Implies(Not(Atom(AcceptOpinions)), Not(Atom(Wise))) is logically valid and directly supported by the deductions made.</p>
<p>The correct answer is Option B</p>
<p>Figure 1 :
1
Figure 1: An example of logical reasoning problems in competitive exams.GPT-4 can handle abstract logical reasoning, however, it fails to conduct a valid inference chain.</p>
<p>Figure 2 :
2
Figure 2: The architecture of the LA framework.Highlighted texts are the output of pre-defined functions.</p>
<p>Figure 3 :
3
Figure 3: GPT-3.5-TURBOresults on ablation test.</p>
<p>Table 1 :
1
We use in-context examples to demonstrate how these functions are called in The statistics of the datasets.
DatasetSizeTargetReClor dev5004-way multi-choiceAR-LSAT test2305-way multi-choiceLogiQA221,354 4-way multi-choiceConTRoL test805E, C, NNaN-NLI test259E, C, NRuleTaker dev10,068Yes, NoProofWriter dev 10,158Yes, No</p>
<p>Table 2 :
2
Main results.All results are in %.
TaskMCRCNLITFDatasetReclor AR-LSAT LogiQA22 ConTRoL NaN-NLI RuleTaker ProofWriterHuman avg.63.0056.0083.0087.0094.0084.0082.00Human Ceiling100.0091.0099.0094.00100.0095.0093.00GPT-3.5-Direct56.2851.3141.1457.9456.8655.3354.68GPT-3.5-CoT56.9051.4542.9258.2955.5455.8853.02GPT-3.5-LA59.7355.2942.9862.0161.3471.3073.85GPT-4-Direct88.5474.2160.1156.3477.0759.8561.58GPT-4-CoT89.0673.4958.4356.9777.8361.4360.64GPT-4-LA89.4777.2860.6758.9380.6665.8468.42Davinci-002-Direct20.4113.5411.028.4310.7825.9822.54Davinci-002-CoT19.4318.8513.2713.6115.3426.8427.33Davinci-002-LA27.4522.6030.6815.5824.7332.1033.54LLaMA-2-Direct17.3112.7018.5520.1222.0825.5023.39LLaMA-2-CoT15.6213.7616.0321.7525.4422.3923.16LLaMA-2-LA23.7621.6330.2125.4822.7628.7925.11Mixtral-8x7b-Direct 48.9241.4038.9750.8450.1346.8444.80Mixtral-8x7b-CoT49.2144.3340.9650.3253.0448.5245.85Mixtral-8x7b-LA50.5845.9544.9252.2555.9652.5355.68</p>
<p>Table 3 ,
3
demon-
DatasetDefault parser GPT-4 parserReClor dev59.7360.65AR-LSAT test55.2955.87LogiQA2242.9844.07ConTRoL62.0165.24NaN-NLI61.3463.48RuleTaker dev71.3071.45ProofWriter dev73.8572.13Table 3: GPT-3.5-TURBO model results with GPT-4 asthe parser.
https://github.com/guidance-ai/guidance</p>
<p>Lasha Abzianidze, arXiv:1708.09417Langpro: Natural language theorem prover. 2017arXiv preprint</p>
<p>Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, Jeremy Avigad, arXiv:2302.12433Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. 2023arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michał Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the 38th AAAI Conference on Artificial Intelligence. the 38th AAAI Conference on Artificial Intelligence2024</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, arXiv:2309.15402A survey of chain of thought reasoning: Advances, frontiers and future. 2023arXiv preprint</p>
<p>Peter Clark, Oyvind Tafjord, Kyle Richardson, arXiv:2002.05867Transformers as soft reasoners over language. 2020arXiv preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Anushree Venkatesh, and Dilek Hakkani-Tur. Yuheng Du, Shereen Oraby, Vittorio Perera, Minmin Shen, Anjali Narayan-Chen, Tagyoung Chung, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, IrelandAssociation for Computational Linguistics2020Schema-guided natural language generation</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Grammar-constrained decoding for structured nlp tasks without finetuning. Saibo Geng, Martin Josifoski, Maxime Peyrard, Robert West, 2024</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, arXiv:2304.091022023arXiv preprint</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, H Shrivastava, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2024Mixtral of experts</p>
<p>. Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, and Guillaume Lample. 2023. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs</p>
<p>Logicllm: Exploring self-supervised logic-enhanced training for large language models. Fangkai Jiao, Zhiyang Teng, Shafiq Joty, Bosheng Ding, Aixin Sun, Zhengyuan Liu, Nancy F Chen, 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Jackson Kernion, et al. 2023. Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, arXiv:2307.13702arXiv preprint</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336On the advance of making language models better reasoners. 2022arXiv preprint</p>
<p>Hanmeng Liu, Leyang Cui, Jian Liu, Yue Zhang, 10.1609/aaai.v35i15.17580Natural language inference in context -investigating contextual reasoning over long texts. Proceedings of the AAAI Conference on Artificial Intelligence. 202135</p>
<p>Logicot: Logical chain-of-thought instruction tuning. Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Glore: Evaluating logical reasoning of large language models. Zhiyang Hanmeng Liu, Ruoxi Teng, Jian Ning, Qiji Liu, Yue Zhou, Zhang, 2023</p>
<p>Neurologic decoding:(un) supervised neural text generation with predicate logic constraints. Ximing Lu, Peter West, Rowan Zellers, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, arXiv:2010.128842020arXiv preprint</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , arXiv:2303.09014Automatic multistep reasoning and tool-use for large language models. Art2023arXiv preprint</p>
<p>Peano: learning formal mathematical reasoning. Gabriel Poesia, Noah D Goodman, Philosophical Transactions of the Royal Society A. 381202200442023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.04761arXiv:2401.12954Meta-prompting: Enhancing language models with task-agnostic scaffolding. Mirac Suzgun, Adam Tauman, Kalai , 2023. 2024arXiv preprint</p>
<p>Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, arXiv:2012.13048Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2020arXiv preprint</p>
<p>A language-agent approach to formal theorem-proving. Amitayush Thakur, Yeming Wen, Swarat Chaudhuri, 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Not another negation benchmark: The nan-nli test suite for sub-clausal negation. Hung Thinh, Yulia Truong, Timothy Otmakhova, Trevor Baldwin, Jey Cohn, Karin Han Lau, Verspoor, 2022</p>
<p>Zero-shot learners for natural language understanding via a unified multiple-choice perspective. Junjie Wang, Ping Yang, Ruyi Gan, Yuxiang Zhang, Jiaxing Zhang, Tetsuya Sakai, IEEE Access. 2023</p>
<p>From lsat: The progress and challenges of complex reasoning. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, Nan Duan, Speech, and Language Processing. 2022a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Evaluating reading comprehension exercises generated by llms: A showcase of chatgpt in education applications. Changrong Xiao, Sean Xin Xu, Kunpeng Zhang, Yufang Wang, Lei Xia, Workshop on Innovative Use of NLP for Building Educational Applications. 2023</p>
<p>Symbol-llm: Towards foundational symbolcentric interface for large language models. Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu. 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, ArXiv, abs/2305.106012023</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, ArXiv, abs/2210.036292022</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations (ICLR). 2020</p>
<p>Cumulative reasoning with large language models. Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao, ArXiv, abs/2308.043712023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3328 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3328</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3328</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-3e565c544a8639cc9c7568833e484d7610f5e5d4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4" target="_blank">Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A novel approach is proposed, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which verifies its effectiveness in selecting in- context examples.</p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3328.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3328.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (OpenAI TEXT-DAVINCI-002 engine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained autoregressive transformer language model used in this paper in zero-shot and few-shot modes; the authors use both direct prompting and chain-of-thought (CoT) prompting to elicit multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (TEXT-DAVINCI-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model provided by OpenAI (accessed via the API TEXT-DAVINCI-002). Used as zero-shot, few-shot, and chain-of-thought prompted variants to generate solutions and final answers for TABMWP.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot prompting (in-context learning)', 'chain-of-thought prompting (CoT)', 'few-shot-CoT (CoT with in-context examples)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zero-shot: supply problem text+table to GPT-3 with no exemplars. Few-shot: prepend K in‑context examples (2 in main experiments) and then the test problem. CoT: append prompt such as "Let's think step by step" (or provide exemplar solutions) so the model generates intermediate multi-step reasoning before the answer. Few-shot-CoT combines exemplars that include solution steps with the test example.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar prompting styles with and without explicit step-by-step (CoT) reasoning; diversity comes from example selection (random vs retrieval vs learned selection) rather than model architecture changes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TABMWP (Tabular Math Word Problems)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A new dataset introduced in this paper consisting of 38,431 grade-level math word problems that require multi-step mathematical reasoning over semi-structured tabular contexts (presented as text/image/table). Tasks include free-text numeric answers and multiple-choice questions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On TABMWP test split (reported averages in paper Table 3): Zero-shot GPT-3: 56.96% avg; Zero-shot-CoT GPT-3: 57.61% avg; Few-shot (2-shot, random example selection): 57.13% avg; Few-shot-CoT (2-shot, random): 62.92% avg. Reported variability: few-shot-CoT (random) across 3 trials = 62.92% ± 2.30 (Table 8). Dev-shot sweep (1k dev examples, Table 9): few-shot-CoT random: 2-shot 65.2% ±4.01, 3-shot 65.7% ±1.16, 4-shot 67.7% ±0.78, 5-shot 67.5% ±0.98.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Chain-of-thought (CoT) prompting improves performance over non-CoT variants, especially in few-shot: few-shot-CoT (62.92%) > few-shot (57.13%) by ~5.79 percentage points. Zero-shot-CoT gives only a small improvement over zero-shot. Increasing number of in-context exemplars with random selection raises accuracy and reduces variance up to ~4 shots, but improvement plateaus. Example selection method substantially affects few-shot performance (see PromptPG entry).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing step-by-step reasoning exemplars (CoT) improves GPT-3's numeric/tabular reasoning, and few-shot CoT is substantially stronger than plain few-shot; however, performance is sensitive to which in-context examples are used (high variance across selections).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Zero-shot and zero-shot-CoT are only modestly different (56.96% vs 57.61%), indicating CoT alone without exemplars gives small gains. Few-shot performance can vary widely with different in-context example selections; naive random selection yields unstable results and can be near-chance on some selections (cited instability literature and ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3328.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3328.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptPG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PromptPG: Dynamic Prompt Learning via Policy Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that learns a policy (BERT-based + linear layer) via policy gradient (REINFORCE) to select in-context examples for GPT-3, constructing prompts dynamically to maximize answer reward on TABMWP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PromptPG (policy for example selection) + GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A BERT [CLS]-based contextual encoder with a small linear layer produces embeddings for candidate examples and a test problem; a softmax dot-product policy samples K examples from a candidate pool. Policy parameters (linear layer) are trained with REINFORCE using binary reward (correct = +1, incorrect = -1) after querying GPT-3 (TEXT-DAVINCI-002) with the constructed prompt; BERT parameters are fixed. In experiments K=2; candidate pool sizes and training set sizes are ablated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>policy linear layer hidden dim = 768; BERT base used (frozen); GPT-3 engine = TEXT-DAVINCI-002 (size not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['learned dynamic in-context example selection (policy-gradient)', 'chain-of-thought prompting (when used in few-shot-CoT configuration)', 'interaction with black-box large LM (GPT-3) as environment']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Policy-gradient selection: for each training problem, sample K in-context examples from a candidate pool according to π_θ(e|p) (dot product between BERT-encoded embeddings followed by softmax). Construct prompt with those selected exemplars plus the test example and query GPT-3 to get an answer; compute reward via exact-match numeric evaluation; update policy via REINFORCE (log-prob * reward). CoT: prompts and exemplars include multi-step solutions (few-shot-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Enables diversified prompt composition by learning to pick examples that produce better reasoning behaviors in GPT-3; the method does not change the LM's internal reasoning algorithm but changes the exemplar distribution presented to it, increasing effective diversity of reasoning demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TABMWP (Tabular Math Word Problems)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-3 entry. PromptPG is trained/evaluated to select in-context examples that maximize GPT-3 accuracy on TABMWP problems requiring multi-hop math over tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On TABMWP test split: PromptPG (few-shot-CoT with learned dynamic selection) achieves 68.23% avg accuracy (Table 3). This is reported as an improvement of 5.31 percentage points over the best baseline few-shot-CoT GPT-3 (62.92%). Trial-level (three seeds) results in Table 8: 68.85%, 65.63%, 70.22% => average 68.23% ±1.92. On a 1k-dev ablation (Table 9), PromptPG (2-shot) achieved 70.9% ±1.27 while few-shot-CoT random 4-shot achieved 67.7% ±0.78, demonstrating data efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>PromptPG vs random selection: PromptPG outperforms random few-shot-CoT (68.23% vs 62.92% test avg) and reduces variance. PromptPG vs nearest-neighbor retrieval: PromptPG selects exemplars that share reasoning structure rather than superficial semantic similarity; nearest-neighbor sometimes selects superficially similar but unhelpful examples and leads to errors (illustrated in case studies, Figures 6). Manual fixed selection yields zero variance but only ~1.7% improvement over random; PromptPG yields larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning to select in-context examples via policy-gradient substantially increases accuracy and reduces selection instability for CoT few-shot GPT-3 on semi-structured tabular math problems. The learned policy tends to pick examples that exercise similar multi-step mathematical reasoning rather than only surface similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Ablations show there are sweet spots: using too many training examples or too large a candidate pool can degrade performance (policy harder to learn). PromptPG fails on some problems with very complex tables or that require high-level math abilities (case study failure examples). Also PromptPG requires interaction with the black-box LM (API calls) which is compute/monetary costlier than simple retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3328.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3328.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UnifiedQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UnifiedQA (T5-based QA model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-based, text-to-text question-answering model evaluated in pre-trained and fine-tuned settings on TABMWP as a baseline for text QA on semi-structured inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UnifiedQA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-based QA system pre-trained on a mixture of QA datasets; in this paper evaluated in three parameter sizes (SMALL, BASE, LARGE) and both as pre-trained (no TABMWP fine-tuning) and fine-tuned on TABMWP.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>SMALL: 60M, BASE: 220M, LARGE: 770M (as reported in paper appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['fine-tuning on TABMWP (supervised)', 'text-to-text QA generation (no explicit CoT used in main UnifiedQA baselines)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>UnifiedQA is fine-tuned on TABMWP training split to map textualized semi-structured table+question inputs to answers; the paper does not use explicit chain-of-thought prompting for UnifiedQA in reported baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single supervised fine-tuning approach per model; diversity examined via different model sizes but not via different prompting styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TABMWP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-3 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Pre-trained UnifiedQA (no fine-tune) performed poorly (LARGE pre-trained avg 15.96%). Fine-tuned UnifiedQA on TABMWP: SMALL 29.79% avg, BASE 43.52% avg, LARGE 57.35% avg (Table 3). Performance increases substantially with model size when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Fine-tuning substantially improves UnifiedQA versus pre-trained; however fine-tuned UnifiedQA_LARGE (57.35%) is below PromptPG+GPT-3 (68.23%). Pre-trained TAPEX/UnifiedQA show TAPEX advantages on table data (see TAPEX entry).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Supervised fine-tuning of UnifiedQA on TABMWP scales positively with model size and yields competitive results, but few-shot GPT-3 with CoT and learned prompt selection (PromptPG) outperform fine-tuned UnifiedQA in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Pre-trained UnifiedQA without fine-tuning is not effective on TABMWP; smaller UnifiedQA sizes perform substantially worse than larger ones, indicating model capacity matters for supervised fine-tuning on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3328.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3328.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPEX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAPEX (Table pre-training via neural SQL executor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BART-based language model pre-trained on structured tabular data and evaluated here as a strong baseline for table question answering on TABMWP in pre-trained and fine-tuned settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TAPEX</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BART-based model pre-trained on table-text corpora to mimic SQL-style execution and table reasoning; evaluated in BASE and LARGE sizes and fine-tuned on TABMWP.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BASE: 140M, LARGE: 400M (as reported in appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['table-aware pretraining plus supervised fine-tuning', 'text-to-text generation of answers (no explicit CoT in TAPEX baselines)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>TAPEX uses pretraining objectives that incorporate tabular structures (neural SQL executor style) and is fine-tuned on TABMWP for end-to-end QA, relying on learned table-text representations rather than external CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single supervised fine-tuning approach; method diversity arises from model size and pretraining on table data rather than prompting styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TABMWP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-3 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Pre-trained TAPEX_BASE avg 15.73%, TAPEX_LARGE pre-trained 18.59% (Table 3). Fine-tuned TAPEX_BASE: 48.27% avg; TAPEX_LARGE: 58.52% avg. Fine-tuned TAPEX_LARGE slightly outperforms UnifiedQA_LARGE fine-tuned (58.52% vs 57.35%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>TAPEX benefits from table-focused pretraining: fine-tuned TAPEX generally outperforms pre-trained textual-only baselines and matches/unseats UnifiedQA when fine-tuned and large; nevertheless PromptPG+GPT-3 still outperforms fine-tuned TAPEX_LARGE.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Table-specific pretraining yields gains when models are fine-tuned on TABMWP; TAPEX_LARGE fine-tuned performs competitively but is below PromptPG+GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Pre-trained TAPEX without fine-tuning performs poorly on TABMWP; TAPEX still requires fine-tuning on TABMWP to reach reasonable accuracy, indicating fine-tuning remains important for domain-specific table math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3328.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3328.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context selection strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context example selection strategies (random, retrieval/nearest-neighbor, same-type heuristics, manual, learned RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different ways of selecting which examples to include as in-context demonstrations for few-shot prompting; the paper compares random selection, semantic retrieval/nearest-neighbor, heuristics (same question/answer type, complexity), manual fixed selection, and learned selection via PromptPG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Selection strategies applied to choose K in-context examples from candidate pools to form few-shot prompts for GPT-3. Strategies include: random sampling, nearest-neighbor semantic retrieval, heuristics (match question/answer types or example complexity), manual fixed selection (best-performing pair chosen by validation), and learned selection (PromptPG policy-gradient).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['random sampling', 'semantic retrieval / nearest-neighbor', 'heuristic selection (same question type, same answer type, most complex)', 'manual fixed selection', 'policy-gradient learned selection (PromptPG)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Random: pick K examples uniformly. Nearest-neighbor: retrieve semantically similar examples to the test case. Heuristics: pick examples sharing question/answer type or highest complexity. Manual: pick examples chosen for highest one-shot validation performance. Learned selection: train a policy to pick examples that empirically maximize downstream reward (PromptPG).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>These strategies produce prompts of varying diversity; random gives high variance, nearest-neighbor biases toward surface similarity, heuristics aim to improve task relevance, manual fixes variance but may not generalize, and learned selection optimizes exemplar choice to produce better reasoning behavior in the target LM.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TABMWP (affects few-shot GPT-3 performance)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Selection strategies are evaluated by how well resulting few-shot prompts elicit correct multi-step reasoning from GPT-3 on TABMWP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Representative comparisons (Table 8 & ablations): Few-shot-CoT random (2-shot) avg 62.92% ±2.30; manual fixed selection: small gain over random (~+1.7% reported in text) but zero variance; nearest-neighbor retrieval improved stability but sometimes led to incorrect outputs due to superficial similarity (case studies); PromptPG (learned) improved to 68.23% ±1.92. Heuristic same question/answer type improved accuracy and reduced variance relative to pure random.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Learned selection (PromptPG) > nearest-neighbor retrieval > heuristics (same-type) > random selection > manual fixed in terms of combined accuracy and stability on TABMWP; manual has zero variance but small gain. Nearest-neighbor can fail when surface similarity does not match reasoning structure (case studies show erroneous outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Which exemplars are presented has a large impact: selecting exemplars that align with the required reasoning steps (not just surface semantics) yields better and more stable performance; a learned policy (PromptPG) is effective at finding such exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Nearest-neighbor retrieval sometimes produced examples superficially similar but misleading for reasoning (shown in case studies leading to wrong answers). Increasing candidate pool size too much or training examples beyond certain thresholds degrades PromptPG performance (policy learning difficulty). Manual selection yields zero variance but only marginal accuracy improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3328.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3328.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting / generating intermediate reasoning steps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks the language model to produce intermediate step-by-step reasoning (a 'chain of thought') before the final answer; used both in zero-shot (instructional phrase) and few-shot (exemplar solutions) forms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT prompting either inserts a cue like "Let's think step by step" (zero-shot-CoT) or provides exemplar problems that include explicit solution steps (few-shot-CoT), thereby encouraging the model to produce multi-step reasoning traces which can improve final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['explicit step-by-step solution generation (CoT)', 'few-shot exemplars that include full solutions']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>In zero-shot-CoT, a short instruction triggers chain-of-thought generation; in few-shot-CoT, the exemplars themselves include full multi-step solutions preceding the final answer, and the model is expected to follow similar solution patterns for the test item.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>CoT is a single reasoning style (stepwise decomposition) applied across experiments; diversity arises in whether CoT is provided via instruction-only or via exemplars and in the selection of those exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TABMWP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-3 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Few-shot-CoT (2-shot random) on TABMWP test: 62.92% avg (best baseline). Zero-shot-CoT: 57.61% avg (only small improvement over zero-shot 56.96%). Combining CoT with PromptPG selection yields 68.23% avg.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT clearly helps in few-shot setups, with few-shot-CoT improving several points over few-shot without CoT. CoT without exemplar selection (random) is still unstable; selection methods amplify CoT gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing CoT exemplars is an effective way to elicit multi-step mathematical reasoning from GPT-3 on tabular problems, but the choice of exemplars remains a critical factor.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Zero-shot-CoT yields only marginal gains over zero-shot; CoT benefit depends strongly on exemplars and selection strategy (poor exemplars can mislead the model).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3328.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3328.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TABMWP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TABMWP (Tabular Math Word Problems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset introduced in this paper: 38,431 open-domain grade-level math word problems that require numerical reasoning grounded in semi-structured tabular contexts, each annotated with multi-step natural-language solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset includes three table formats (image, semi-structured text, structured), two question types (free-text numeric answers and multiple-choice), and three answer text types (extractive text, boolean, other); split 23,059 train / 7,686 dev / 7,686 test. Average table: 5.9 rows, 2.2 columns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['multi-hop mathematical reasoning over heterogeneous (tabular + textual) inputs', 'grounded cell lookup + arithmetic operations + multi-step solution chains']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Problems require aligning text clues to table cells, retrieving numeric values, performing arithmetic (multiply/add/subtract/divide), and multi-step composition of operations; many problems require multi-hop reasoning across table rows/cells plus textual question understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Dataset itself embodies diverse reasoning styles needed (lookup, alignment, arithmetic composition); experimental methods in the paper attempt to elicit these via CoT, in-context exemplars, supervised fine-tuning, and learned exemplar selection.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TABMWP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See name_full.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Best reported system (PromptPG + GPT-3 few-shot-CoT): 68.23% avg on the test split. Fine-tuned TAPEX_LARGE: 58.52% avg. Fine-tuned UnifiedQA_LARGE: 57.35% avg. Human performance (AMT study): 90.22% avg. Heuristic baselines: ~15% average.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Language-model prompting with learned exemplar selection (PromptPG) outperforms supervised fine-tuning of table-aware models in these experiments; however, humans remain substantially better, and model performance varies by question/answer type and grade level.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TABMWP is a challenging benchmark that reveals sensitivity of LMs to exemplar selection and benefits from CoT and learned prompt selection; the dataset highlights limitations of both vanilla few-shot prompting and current table-QA models on semi-structured math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Many models perform substantially worse on higher-grade (7-8) problems; some prompting strategies (naive nearest-neighbor) can cause wrong reasoning traces; humans outperform all automatic methods by a large margin (~21.99% over PromptPG).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>What makes good in-context examples for gpt-3? <em>(Rating: 2)</em></li>
                <li>TAPEX: Table pre-training via learning a neural sql executor <em>(Rating: 2)</em></li>
                <li>UnifiedQA: Crossing format boundaries with a single QA system <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3328",
    "paper_id": "paper-3e565c544a8639cc9c7568833e484d7610f5e5d4",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-3 (text-davinci-002)",
            "name_full": "GPT-3 (OpenAI TEXT-DAVINCI-002 engine)",
            "brief_description": "A large pre-trained autoregressive transformer language model used in this paper in zero-shot and few-shot modes; the authors use both direct prompting and chain-of-thought (CoT) prompting to elicit multi-step reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (TEXT-DAVINCI-002)",
            "model_description": "Autoregressive transformer language model provided by OpenAI (accessed via the API TEXT-DAVINCI-002). Used as zero-shot, few-shot, and chain-of-thought prompted variants to generate solutions and final answers for TABMWP.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot prompting (in-context learning)",
                "chain-of-thought prompting (CoT)",
                "few-shot-CoT (CoT with in-context examples)"
            ],
            "reasoning_methods_description": "Zero-shot: supply problem text+table to GPT-3 with no exemplars. Few-shot: prepend K in‑context examples (2 in main experiments) and then the test problem. CoT: append prompt such as \"Let's think step by step\" (or provide exemplar solutions) so the model generates intermediate multi-step reasoning before the answer. Few-shot-CoT combines exemplars that include solution steps with the test example.",
            "diversity_of_methods": "Similar prompting styles with and without explicit step-by-step (CoT) reasoning; diversity comes from example selection (random vs retrieval vs learned selection) rather than model architecture changes.",
            "reasoning_task_name": "TABMWP (Tabular Math Word Problems)",
            "reasoning_task_description": "A new dataset introduced in this paper consisting of 38,431 grade-level math word problems that require multi-step mathematical reasoning over semi-structured tabular contexts (presented as text/image/table). Tasks include free-text numeric answers and multiple-choice questions.",
            "performance_by_method": "On TABMWP test split (reported averages in paper Table 3): Zero-shot GPT-3: 56.96% avg; Zero-shot-CoT GPT-3: 57.61% avg; Few-shot (2-shot, random example selection): 57.13% avg; Few-shot-CoT (2-shot, random): 62.92% avg. Reported variability: few-shot-CoT (random) across 3 trials = 62.92% ± 2.30 (Table 8). Dev-shot sweep (1k dev examples, Table 9): few-shot-CoT random: 2-shot 65.2% ±4.01, 3-shot 65.7% ±1.16, 4-shot 67.7% ±0.78, 5-shot 67.5% ±0.98.",
            "comparison_of_methods": "Chain-of-thought (CoT) prompting improves performance over non-CoT variants, especially in few-shot: few-shot-CoT (62.92%) &gt; few-shot (57.13%) by ~5.79 percentage points. Zero-shot-CoT gives only a small improvement over zero-shot. Increasing number of in-context exemplars with random selection raises accuracy and reduces variance up to ~4 shots, but improvement plateaus. Example selection method substantially affects few-shot performance (see PromptPG entry).",
            "key_findings": "Providing step-by-step reasoning exemplars (CoT) improves GPT-3's numeric/tabular reasoning, and few-shot CoT is substantially stronger than plain few-shot; however, performance is sensitive to which in-context examples are used (high variance across selections).",
            "counter_examples_or_negative_results": "Zero-shot and zero-shot-CoT are only modestly different (56.96% vs 57.61%), indicating CoT alone without exemplars gives small gains. Few-shot performance can vary widely with different in-context example selections; naive random selection yields unstable results and can be near-chance on some selections (cited instability literature and ablations).",
            "uuid": "e3328.0",
            "source_info": {
                "paper_title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "PromptPG",
            "name_full": "PromptPG: Dynamic Prompt Learning via Policy Gradient",
            "brief_description": "A method introduced in this paper that learns a policy (BERT-based + linear layer) via policy gradient (REINFORCE) to select in-context examples for GPT-3, constructing prompts dynamically to maximize answer reward on TABMWP.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PromptPG (policy for example selection) + GPT-3",
            "model_description": "A BERT [CLS]-based contextual encoder with a small linear layer produces embeddings for candidate examples and a test problem; a softmax dot-product policy samples K examples from a candidate pool. Policy parameters (linear layer) are trained with REINFORCE using binary reward (correct = +1, incorrect = -1) after querying GPT-3 (TEXT-DAVINCI-002) with the constructed prompt; BERT parameters are fixed. In experiments K=2; candidate pool sizes and training set sizes are ablated.",
            "model_size": "policy linear layer hidden dim = 768; BERT base used (frozen); GPT-3 engine = TEXT-DAVINCI-002 (size not specified in paper)",
            "reasoning_methods": [
                "learned dynamic in-context example selection (policy-gradient)",
                "chain-of-thought prompting (when used in few-shot-CoT configuration)",
                "interaction with black-box large LM (GPT-3) as environment"
            ],
            "reasoning_methods_description": "Policy-gradient selection: for each training problem, sample K in-context examples from a candidate pool according to π_θ(e|p) (dot product between BERT-encoded embeddings followed by softmax). Construct prompt with those selected exemplars plus the test example and query GPT-3 to get an answer; compute reward via exact-match numeric evaluation; update policy via REINFORCE (log-prob * reward). CoT: prompts and exemplars include multi-step solutions (few-shot-CoT).",
            "diversity_of_methods": "Enables diversified prompt composition by learning to pick examples that produce better reasoning behaviors in GPT-3; the method does not change the LM's internal reasoning algorithm but changes the exemplar distribution presented to it, increasing effective diversity of reasoning demonstrations.",
            "reasoning_task_name": "TABMWP (Tabular Math Word Problems)",
            "reasoning_task_description": "See GPT-3 entry. PromptPG is trained/evaluated to select in-context examples that maximize GPT-3 accuracy on TABMWP problems requiring multi-hop math over tables.",
            "performance_by_method": "On TABMWP test split: PromptPG (few-shot-CoT with learned dynamic selection) achieves 68.23% avg accuracy (Table 3). This is reported as an improvement of 5.31 percentage points over the best baseline few-shot-CoT GPT-3 (62.92%). Trial-level (three seeds) results in Table 8: 68.85%, 65.63%, 70.22% =&gt; average 68.23% ±1.92. On a 1k-dev ablation (Table 9), PromptPG (2-shot) achieved 70.9% ±1.27 while few-shot-CoT random 4-shot achieved 67.7% ±0.78, demonstrating data efficiency.",
            "comparison_of_methods": "PromptPG vs random selection: PromptPG outperforms random few-shot-CoT (68.23% vs 62.92% test avg) and reduces variance. PromptPG vs nearest-neighbor retrieval: PromptPG selects exemplars that share reasoning structure rather than superficial semantic similarity; nearest-neighbor sometimes selects superficially similar but unhelpful examples and leads to errors (illustrated in case studies, Figures 6). Manual fixed selection yields zero variance but only ~1.7% improvement over random; PromptPG yields larger gains.",
            "key_findings": "Learning to select in-context examples via policy-gradient substantially increases accuracy and reduces selection instability for CoT few-shot GPT-3 on semi-structured tabular math problems. The learned policy tends to pick examples that exercise similar multi-step mathematical reasoning rather than only surface similarity.",
            "counter_examples_or_negative_results": "Ablations show there are sweet spots: using too many training examples or too large a candidate pool can degrade performance (policy harder to learn). PromptPG fails on some problems with very complex tables or that require high-level math abilities (case study failure examples). Also PromptPG requires interaction with the black-box LM (API calls) which is compute/monetary costlier than simple retrieval.",
            "uuid": "e3328.1",
            "source_info": {
                "paper_title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "UnifiedQA",
            "name_full": "UnifiedQA (T5-based QA model)",
            "brief_description": "A T5-based, text-to-text question-answering model evaluated in pre-trained and fine-tuned settings on TABMWP as a baseline for text QA on semi-structured inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "UnifiedQA",
            "model_description": "T5-based QA system pre-trained on a mixture of QA datasets; in this paper evaluated in three parameter sizes (SMALL, BASE, LARGE) and both as pre-trained (no TABMWP fine-tuning) and fine-tuned on TABMWP.",
            "model_size": "SMALL: 60M, BASE: 220M, LARGE: 770M (as reported in paper appendix)",
            "reasoning_methods": [
                "fine-tuning on TABMWP (supervised)",
                "text-to-text QA generation (no explicit CoT used in main UnifiedQA baselines)"
            ],
            "reasoning_methods_description": "UnifiedQA is fine-tuned on TABMWP training split to map textualized semi-structured table+question inputs to answers; the paper does not use explicit chain-of-thought prompting for UnifiedQA in reported baselines.",
            "diversity_of_methods": "Single supervised fine-tuning approach per model; diversity examined via different model sizes but not via different prompting styles.",
            "reasoning_task_name": "TABMWP",
            "reasoning_task_description": "See GPT-3 entry.",
            "performance_by_method": "Pre-trained UnifiedQA (no fine-tune) performed poorly (LARGE pre-trained avg 15.96%). Fine-tuned UnifiedQA on TABMWP: SMALL 29.79% avg, BASE 43.52% avg, LARGE 57.35% avg (Table 3). Performance increases substantially with model size when fine-tuned.",
            "comparison_of_methods": "Fine-tuning substantially improves UnifiedQA versus pre-trained; however fine-tuned UnifiedQA_LARGE (57.35%) is below PromptPG+GPT-3 (68.23%). Pre-trained TAPEX/UnifiedQA show TAPEX advantages on table data (see TAPEX entry).",
            "key_findings": "Supervised fine-tuning of UnifiedQA on TABMWP scales positively with model size and yields competitive results, but few-shot GPT-3 with CoT and learned prompt selection (PromptPG) outperform fine-tuned UnifiedQA in this benchmark.",
            "counter_examples_or_negative_results": "Pre-trained UnifiedQA without fine-tuning is not effective on TABMWP; smaller UnifiedQA sizes perform substantially worse than larger ones, indicating model capacity matters for supervised fine-tuning on this dataset.",
            "uuid": "e3328.2",
            "source_info": {
                "paper_title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "TAPEX",
            "name_full": "TAPEX (Table pre-training via neural SQL executor)",
            "brief_description": "A BART-based language model pre-trained on structured tabular data and evaluated here as a strong baseline for table question answering on TABMWP in pre-trained and fine-tuned settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TAPEX",
            "model_description": "BART-based model pre-trained on table-text corpora to mimic SQL-style execution and table reasoning; evaluated in BASE and LARGE sizes and fine-tuned on TABMWP.",
            "model_size": "BASE: 140M, LARGE: 400M (as reported in appendix)",
            "reasoning_methods": [
                "table-aware pretraining plus supervised fine-tuning",
                "text-to-text generation of answers (no explicit CoT in TAPEX baselines)"
            ],
            "reasoning_methods_description": "TAPEX uses pretraining objectives that incorporate tabular structures (neural SQL executor style) and is fine-tuned on TABMWP for end-to-end QA, relying on learned table-text representations rather than external CoT prompting.",
            "diversity_of_methods": "Single supervised fine-tuning approach; method diversity arises from model size and pretraining on table data rather than prompting styles.",
            "reasoning_task_name": "TABMWP",
            "reasoning_task_description": "See GPT-3 entry.",
            "performance_by_method": "Pre-trained TAPEX_BASE avg 15.73%, TAPEX_LARGE pre-trained 18.59% (Table 3). Fine-tuned TAPEX_BASE: 48.27% avg; TAPEX_LARGE: 58.52% avg. Fine-tuned TAPEX_LARGE slightly outperforms UnifiedQA_LARGE fine-tuned (58.52% vs 57.35%).",
            "comparison_of_methods": "TAPEX benefits from table-focused pretraining: fine-tuned TAPEX generally outperforms pre-trained textual-only baselines and matches/unseats UnifiedQA when fine-tuned and large; nevertheless PromptPG+GPT-3 still outperforms fine-tuned TAPEX_LARGE.",
            "key_findings": "Table-specific pretraining yields gains when models are fine-tuned on TABMWP; TAPEX_LARGE fine-tuned performs competitively but is below PromptPG+GPT-3.",
            "counter_examples_or_negative_results": "Pre-trained TAPEX without fine-tuning performs poorly on TABMWP; TAPEX still requires fine-tuning on TABMWP to reach reasonable accuracy, indicating fine-tuning remains important for domain-specific table math problems.",
            "uuid": "e3328.3",
            "source_info": {
                "paper_title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "In-context selection strategies",
            "name_full": "In-context example selection strategies (random, retrieval/nearest-neighbor, same-type heuristics, manual, learned RL)",
            "brief_description": "Different ways of selecting which examples to include as in-context demonstrations for few-shot prompting; the paper compares random selection, semantic retrieval/nearest-neighbor, heuristics (same question/answer type, complexity), manual fixed selection, and learned selection via PromptPG.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Selection strategies applied to choose K in-context examples from candidate pools to form few-shot prompts for GPT-3. Strategies include: random sampling, nearest-neighbor semantic retrieval, heuristics (match question/answer types or example complexity), manual fixed selection (best-performing pair chosen by validation), and learned selection (PromptPG policy-gradient).",
            "model_size": null,
            "reasoning_methods": [
                "random sampling",
                "semantic retrieval / nearest-neighbor",
                "heuristic selection (same question type, same answer type, most complex)",
                "manual fixed selection",
                "policy-gradient learned selection (PromptPG)"
            ],
            "reasoning_methods_description": "Random: pick K examples uniformly. Nearest-neighbor: retrieve semantically similar examples to the test case. Heuristics: pick examples sharing question/answer type or highest complexity. Manual: pick examples chosen for highest one-shot validation performance. Learned selection: train a policy to pick examples that empirically maximize downstream reward (PromptPG).",
            "diversity_of_methods": "These strategies produce prompts of varying diversity; random gives high variance, nearest-neighbor biases toward surface similarity, heuristics aim to improve task relevance, manual fixes variance but may not generalize, and learned selection optimizes exemplar choice to produce better reasoning behavior in the target LM.",
            "reasoning_task_name": "TABMWP (affects few-shot GPT-3 performance)",
            "reasoning_task_description": "Selection strategies are evaluated by how well resulting few-shot prompts elicit correct multi-step reasoning from GPT-3 on TABMWP.",
            "performance_by_method": "Representative comparisons (Table 8 & ablations): Few-shot-CoT random (2-shot) avg 62.92% ±2.30; manual fixed selection: small gain over random (~+1.7% reported in text) but zero variance; nearest-neighbor retrieval improved stability but sometimes led to incorrect outputs due to superficial similarity (case studies); PromptPG (learned) improved to 68.23% ±1.92. Heuristic same question/answer type improved accuracy and reduced variance relative to pure random.",
            "comparison_of_methods": "Learned selection (PromptPG) &gt; nearest-neighbor retrieval &gt; heuristics (same-type) &gt; random selection &gt; manual fixed in terms of combined accuracy and stability on TABMWP; manual has zero variance but small gain. Nearest-neighbor can fail when surface similarity does not match reasoning structure (case studies show erroneous outputs).",
            "key_findings": "Which exemplars are presented has a large impact: selecting exemplars that align with the required reasoning steps (not just surface semantics) yields better and more stable performance; a learned policy (PromptPG) is effective at finding such exemplars.",
            "counter_examples_or_negative_results": "Nearest-neighbor retrieval sometimes produced examples superficially similar but misleading for reasoning (shown in case studies leading to wrong answers). Increasing candidate pool size too much or training examples beyond certain thresholds degrades PromptPG performance (policy learning difficulty). Manual selection yields zero variance but only marginal accuracy improvement.",
            "uuid": "e3328.4",
            "source_info": {
                "paper_title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought (CoT) prompting / generating intermediate reasoning steps",
            "brief_description": "A prompting technique that asks the language model to produce intermediate step-by-step reasoning (a 'chain of thought') before the final answer; used both in zero-shot (instructional phrase) and few-shot (exemplar solutions) forms.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "CoT prompting either inserts a cue like \"Let's think step by step\" (zero-shot-CoT) or provides exemplar problems that include explicit solution steps (few-shot-CoT), thereby encouraging the model to produce multi-step reasoning traces which can improve final-answer accuracy.",
            "model_size": null,
            "reasoning_methods": [
                "explicit step-by-step solution generation (CoT)",
                "few-shot exemplars that include full solutions"
            ],
            "reasoning_methods_description": "In zero-shot-CoT, a short instruction triggers chain-of-thought generation; in few-shot-CoT, the exemplars themselves include full multi-step solutions preceding the final answer, and the model is expected to follow similar solution patterns for the test item.",
            "diversity_of_methods": "CoT is a single reasoning style (stepwise decomposition) applied across experiments; diversity arises in whether CoT is provided via instruction-only or via exemplars and in the selection of those exemplars.",
            "reasoning_task_name": "TABMWP",
            "reasoning_task_description": "See GPT-3 entry.",
            "performance_by_method": "Few-shot-CoT (2-shot random) on TABMWP test: 62.92% avg (best baseline). Zero-shot-CoT: 57.61% avg (only small improvement over zero-shot 56.96%). Combining CoT with PromptPG selection yields 68.23% avg.",
            "comparison_of_methods": "CoT clearly helps in few-shot setups, with few-shot-CoT improving several points over few-shot without CoT. CoT without exemplar selection (random) is still unstable; selection methods amplify CoT gains.",
            "key_findings": "Providing CoT exemplars is an effective way to elicit multi-step mathematical reasoning from GPT-3 on tabular problems, but the choice of exemplars remains a critical factor.",
            "counter_examples_or_negative_results": "Zero-shot-CoT yields only marginal gains over zero-shot; CoT benefit depends strongly on exemplars and selection strategy (poor exemplars can mislead the model).",
            "uuid": "e3328.5",
            "source_info": {
                "paper_title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "TABMWP dataset",
            "name_full": "TABMWP (Tabular Math Word Problems)",
            "brief_description": "A dataset introduced in this paper: 38,431 open-domain grade-level math word problems that require numerical reasoning grounded in semi-structured tabular contexts, each annotated with multi-step natural-language solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Dataset includes three table formats (image, semi-structured text, structured), two question types (free-text numeric answers and multiple-choice), and three answer text types (extractive text, boolean, other); split 23,059 train / 7,686 dev / 7,686 test. Average table: 5.9 rows, 2.2 columns.",
            "model_size": null,
            "reasoning_methods": [
                "multi-hop mathematical reasoning over heterogeneous (tabular + textual) inputs",
                "grounded cell lookup + arithmetic operations + multi-step solution chains"
            ],
            "reasoning_methods_description": "Problems require aligning text clues to table cells, retrieving numeric values, performing arithmetic (multiply/add/subtract/divide), and multi-step composition of operations; many problems require multi-hop reasoning across table rows/cells plus textual question understanding.",
            "diversity_of_methods": "Dataset itself embodies diverse reasoning styles needed (lookup, alignment, arithmetic composition); experimental methods in the paper attempt to elicit these via CoT, in-context exemplars, supervised fine-tuning, and learned exemplar selection.",
            "reasoning_task_name": "TABMWP",
            "reasoning_task_description": "See name_full.",
            "performance_by_method": "Best reported system (PromptPG + GPT-3 few-shot-CoT): 68.23% avg on the test split. Fine-tuned TAPEX_LARGE: 58.52% avg. Fine-tuned UnifiedQA_LARGE: 57.35% avg. Human performance (AMT study): 90.22% avg. Heuristic baselines: ~15% average.",
            "comparison_of_methods": "Language-model prompting with learned exemplar selection (PromptPG) outperforms supervised fine-tuning of table-aware models in these experiments; however, humans remain substantially better, and model performance varies by question/answer type and grade level.",
            "key_findings": "TABMWP is a challenging benchmark that reveals sensitivity of LMs to exemplar selection and benefits from CoT and learned prompt selection; the dataset highlights limitations of both vanilla few-shot prompting and current table-QA models on semi-structured math problems.",
            "counter_examples_or_negative_results": "Many models perform substantially worse on higher-grade (7-8) problems; some prompting strategies (naive nearest-neighbor) can cause wrong reasoning traces; humans outperform all automatic methods by a large margin (~21.99% over PromptPG).",
            "uuid": "e3328.6",
            "source_info": {
                "paper_title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "What makes good in-context examples for gpt-3?",
            "rating": 2
        },
        {
            "paper_title": "TAPEX: Table pre-training via learning a neural sql executor",
            "rating": 2
        },
        {
            "paper_title": "UnifiedQA: Crossing format boundaries with a single QA system",
            "rating": 2
        }
    ],
    "cost": 0.020805499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Dynamic Prompt Learning via Policy Gradient FOR SEMI-STRUCTURED MATHEMATICAL REASONING</h1>
<p>Pan Lu ${ }^{1,3}$, Liang Qiu ${ }^{1}$, Kai-Wei Chang ${ }^{1}$, Ying Nian Wu ${ }^{1}$, Song-Chun Zhu ${ }^{1}$, Tanmay Rajpurohit ${ }^{2}$, Peter Clark ${ }^{3}$, Ashwin Kalyan ${ }^{3}$<br>${ }^{1}$ University of California, Los Angeles, ${ }^{2}$ Georgia Institute of Technology, ${ }^{3}$ Allen Institute for AI</p>
<h4>Abstract</h4>
<p>Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TABMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TABMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multichoice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TABMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TABMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by $5.31 \%$ on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Developing machines equipped with mathematical reasoning capabilities is one of the long-standing goals of artificial intelligence. Solving math word problems (MWPs) is a well-defined task to diagnose the ability of intelligent systems to perform numerical reasoning and problem-solving as humans. A surge of datasets has been proposed to facilitate the research in this domain (Upadhyay \&amp; Chang, 2017; Amini et al., 2019; Miao et al., 2020; Cobbe et al., 2021). However, most existing MWP datasets focus on textual math word problems only. Tables, widely distributed in different documents such as invoices, health records, and financial reports, contain rich structured information different from unstructured text. Solving math word problems in such a tabular context is much more challenging than existing MWP benchmarks since the system needs to make cell selections and align heterogeneous information before performing further numerical reasoning.
To fill this gap, we propose Tabular Math Word Problems (TABMWP), a new large-scale dataset that contains 38,431 math word problems with tabular context, taken from grade-level math curricula. There are two question types: free-text questions in which the answer is an integer or decimal number, and multi-choice questions where the answer is a text span chosen from option candidates. Different from existing MWP datasets, each problem in TABMWP is accompanied by a tabular context, which is represented in three formats: an image, a semi-structured text, and a structured</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">square beads</th>
<th style="text-align: left;">$\$ 2.97$ per kilogram</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">oval beads</td>
<td style="text-align: left;">$\$ 3.41$ per kilogram</td>
</tr>
<tr>
<td style="text-align: left;">flower-shaped beads</td>
<td style="text-align: left;">$\$ 2.18$ per kilogram</td>
</tr>
<tr>
<td style="text-align: left;">star-shaped beads</td>
<td style="text-align: left;">$\$ 1.95$ per kilogram</td>
</tr>
<tr>
<td style="text-align: left;">heart-shaped beads</td>
<td style="text-align: left;">$\$ 1.52$ per kilogram</td>
</tr>
<tr>
<td style="text-align: left;">spherical beads</td>
<td style="text-align: left;">$\$ 3.42$ per kilogram</td>
</tr>
<tr>
<td style="text-align: left;">rectangular beads</td>
<td style="text-align: left;">$\$ 1.97$ per kilogram</td>
</tr>
</tbody>
</table>
<p>| Question: If Tracy buys 5 kilograms of spherical beads, 4 kilograms of <br> star-shaped beads, and 3 kilograms of flower-shaped beads, how much <br> will she spend? (unit: \$) <br> Answer: 31.44 <br> Solution: <br> Find the cost of the spherical beads. Multiply: $\$ 3.42 \times 5=\$ 17.10$. <br> Find the cost of the star-shaped beads. Multiply: $\$ 1.95 \times 4=\$ 7.80$. <br> Find the cost of the flower-shaped beads. Multiply: $\$ 2.18+3=\$ 6.54$. <br> Now find the total cost by adding: $\$ 17.10+\$ 7.80+\$ 6.54=\$ 31.44$. <br> She will spend $\$ 31.44$. |  |
| :--: | :--: | :--: |
| Sandwich sales |  |  |
| Shop | Tuna | Egg salad |
| City Cafe | 6 | 5 |
| Sandwich City | 3 | 12 |
| Express Sandwiches | 7 | 17 |
| Sam's Sandwich Shop | 1 | 6 |
| Kelly's Subs | 3 | 4 |</p>
<p>Question: As part of a project for health class, Cara surveyed local delis about the kinds of sandwiches sold. Which shop sold fewer sandwiches, Sandwich City or Express Sandwiches?
Options: (A) Sandwich City (B) Express Sandwiches
Answer: (A) Sandwich City
Solution:
Add the numbers in the Sandwich City row. Then, add the numbers in the Express Sandwiches row.
Sandwich City: $3+12=15$. Express Sandwiches: $7+17=24$.
15 is less than 24 . Sandwich City sold fewer sandwiches.</p>
<p>Figure 1: Two examples from the TABMWP dataset. The example above is a free-text problem with a numerical answer; the example below is a multi-choice problem with a textual answer.
table. Each problem is also annotated with a detailed solution that reveals the multi-step reasoning steps to ensure full explainability. To solve problems in TABMWP, a system requires multi-hop mathematical reasoning over heterogeneous information by looking up table cells given textual clues and conducting multi-step operations to predict the final answer. Take the problem above in Figure 1 as an example. To answer the question "how much will she spend (if Tracy buys three kinds of beads)?", we first need to look up the corresponding three rows in the given table, calculate the individual cost for each kind of bead, and finally sum three costs up to get the answer of 31.44.</p>
<p>Inspired the success of the large pre-trained language model GPT-3 (Brown et al., 2020) in solving math word problems (Wei et al., 2022; Wang et al., 2022), we first build a strong baseline using few-shot GPT-3 on TABMWP. A few in-context examples are randomly selected from the training set, along with the test example, and are constructed as a prompt for GPT-3 to predict the answer. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Zhao et al., 2021; Liu et al., 2022a; Lu et al., 2022c). It could be worse on TABMWP since its problems are distributed across multiple question types and diverse table layouts. Liu et al. (2022a) try to address this issue by retrieving semantically similar examples. However, this method might not work well sometimes on TABMWP because it is not capable of measuring the similarity of structured information, such as the number of cells in tables.</p>
<p>To alleviate this challenge, we further propose a novel approach that can learn to select in-context examples from a small amount of training data via policy gradient for prompt learning, termed PromptPG. As illustrated in Figure 2, an agent learns to find optimal in-context examples from a candidate pool, with the goal of maximizing the prediction rewards on given training examples when interacting with the GPT-3 environment. A policy network defines the strategy of how to select the in-context examples given the current training example. The policy network is built on top of the language model BERT (Devlin et al., 2018) with fixed parameters, followed by a one-layer linear neural network with learnable parameters. The learnable parameters are updated following the policy gradient strategy (Sutton et al., 1998). Unlike random selection (Wei et al., 2022; Wang et al., 2022), brute-force search, or retrieval-based selection (Liu et al., 2022a), PromptPG learns to construct the prompt dynamically given the candidate pool when interacting with the GPT-3 API.</p>
<p>We implement two state-of-the-art methods as baselines, i.e., UnifiedQA (Khashabi et al., 2020) on general question answering and TAPEX (Liu et al., 2022b) on tabular question answering. Both are implemented in pre-trained and fine-tuned settings. Experimental results show that our model PromptPG can achieve an overall accuracy of $68.23 \%$ on TABMWP, which greatly surpasses previous methods by a large margin of up to $5.31 \%$. Further analysis demonstrates that PromptPG can select better in-context examples compared with a wide range of existing selection strategies and reduce the prediction variance significantly compared to random selection.</p>
<p>The main contributions of our work are as follows: (a) We present a new large-scale dataset, TABMWP, the first dataset for math word problems with tabular context; (b) We propose a novel</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Our proposed PromptPG is able to learn to select performing in-context examples via policy gradient when interacting with the GPT-3 API without any manually designed heuristics.
approach, PromptPG, which learns the prompt dynamically via policy gradient to select in-context examples for few-shot GPT-3. To the best of our knowledge, it is the first work that applies reinforcement learning to select in-context examples for the few-shot GPT-3 model; (c) Experimental results show that PromptPG achieves an improvement of up to 5.31\% on TABMWP over existing methods, with reduced selection instability compared to random selection.</p>
<h1>2 The TabMWP Dataset</h1>
<h3>2.1 TASK FORMULATION</h3>
<p>A tabular math word problem $p$ is represented as a pair $(t, q)$, where $t$ is a table context and $q$ is a question. The table $t$ could be represented in a visual format as an image, semi-structured text, or a structured database. In this work, we focus on the semi-structured format as the table context for simplicity. The table $t$ features complicated layouts and formats: it contains multiple rows and columns, and each cell can be a string of text, a string of a number, or a mix of them. Depending on the question and answer types, the question $q$ may be accompanied by multiple-choice options $c=\left{c_{1}, c_{2}, \ldots, c_{n}\right}$ or a unit $u$. Given a semi-structured tabular context $t$ and an unstructured question text $q$, the task is to generate the answer $a$, which is either numerical only text for a freetext question, or a text span from given options for a multiple-choice question.</p>
<h3>2.2 DATASET CONSTRUCTION</h3>
<p>Data collection. We construct TABMWP based on openly available content and more details are provided in Appendix A.1. Only math word problems that are accompanied by a tabular context and a detailed solution are collected. We develop a script to extract the tabular context, the question, options that apply, the correct answer, and the solution for each problem. These elements can be precisely identified using HTML tags. For each table, we take a screenshot and store its raw text.</p>
<p>Data preprocessing. To make TABMWP compatible with various baselines, we represent the tabular context as three formats: an image, semi-structured text, and a structured spreadsheet. The semi-structured format is created by converting the raw table text into a flattened token sequence, with each row separated by a newline character ' $\backslash \mathrm{n}$ ' and each column separated by ' $\mid$ '. The semistructured text is further transformed to the structured format, which can be easily retrieved and executed by SQL-based methods (Liu et al., 2022b) using packages like pandas. For clarity, the table title is separated from the raw table. Examples of three formats are shown in Appendix A.1.</p>
<p>For better quantitative evaluation, we formalize the TABMWP problems as two question types: (a) free-text questions, where the answer is numerical text only and the unit text is separately extracted; and (b) multi-choice questions, the answer of which is the text span from choice options. The order of choice options is shuffled to alleviate distribution bias. Redundant information in solutions is removed, and some solutions are manually rewritten to be more human-readable. Finally, problems with the same table, question, and answer text are regarded as redundant and thus removed. We further conduct quality control to ensure data quality, which is discussed in Appendix A.1.</p>
<h1>2.3 DATASET STATISTICS</h1>
<p>Key statistics. The TABMWP dataset contains 38,431 tabular math word problems, which are partitioned with 6:2:2 into the training, development, and test splits, corresponding to $23,059,7,686$, and 7,686 problems. Their main statistics are shown in Table 1. $74.7 \%$ of the questions in TABMWP belong to free-text questions, while $25.3 \%$ are multi-choice questions. There are 28,876 different questions, 6,153 different answers, and 35,442 different solutions, indicating that TABMWP has a rich diversity in the problem distribution. The questions have an average of 22.1 words in length and solutions of 49.5 , showing that they have lexical richness.
One distinct characteristic of TABMWP is that each problem is accompanied by a tabular context, without which the problem would be unsolvable. There are 37,644 different tables in total, and $60.5 \%$ of the tables have a title. The table has an average of 5.9 rows and 2.2 columns, which results in an average of 12.9 cells and a maximum of 54 cells. These statistics suggest that tables in TABMWP distribute diversely across semantics and layouts.</p>
<p>Comparison to existing datasets. As shown in Table 2, TABMWP differs from related datasets in various aspects: (1) TABMWP is the first dataset to study math word problems over tabular context on open domains and is the largest in terms of data size; (2) Problems in TABMWP are annotated with the tabular context, unlike previous MWP datasets in the first segment; (3) Different from Table QA datasets like FinQA, TAT-QA, and MultiHiertt, a lack of either mathematical reasoning or the tabular context renders the problems in TABMWP unanswerable; (4) There are two question types in TABMWP, and the answer could be a text span, an integer number, or a decimal number; (5) Each problem is annotated with natural language solutions to reveal multi-hop reasoning steps.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">#Table</th>
<th style="text-align: center;">Need <br> Math?</th>
<th style="text-align: center;">Need <br> Table?</th>
<th style="text-align: center;">Table Type <br> Domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Question Type <br> Free-text</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answer Type</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Solution <br> Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">Format</td>
<td style="text-align: center;">Free-text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Integer</td>
<td style="text-align: center;">Decimal</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dolphin18K (2016)</td>
<td style="text-align: center;">831</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">formula</td>
</tr>
<tr>
<td style="text-align: center;">DRAW-1K (2017)</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">formula</td>
</tr>
<tr>
<td style="text-align: center;">Math23K (2017)</td>
<td style="text-align: center;">23,162</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">formula</td>
</tr>
<tr>
<td style="text-align: center;">MathQA (2019)</td>
<td style="text-align: center;">37,297</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">formula</td>
</tr>
<tr>
<td style="text-align: center;">ASDiv (2020)</td>
<td style="text-align: center;">2,305</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">formula</td>
</tr>
<tr>
<td style="text-align: center;">SVAMP (2021)</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">formula</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K (2021)</td>
<td style="text-align: center;">8,792</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">text</td>
</tr>
<tr>
<td style="text-align: center;">IconQA (2021b)</td>
<td style="text-align: center;">107,439</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">FinQA (2021)</td>
<td style="text-align: center;">8,281</td>
<td style="text-align: center;">2,766</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">76.6\%</td>
<td style="text-align: center;">finance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">program</td>
</tr>
<tr>
<td style="text-align: center;">TAT-QA (2021)</td>
<td style="text-align: center;">16,552</td>
<td style="text-align: center;">2,747</td>
<td style="text-align: center;">50.0\%</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">finance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">MultiHiertt (2022)</td>
<td style="text-align: center;">10,440</td>
<td style="text-align: center;">9,843</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">89.8\%</td>
<td style="text-align: center;">finance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">TABMWP (ours)</td>
<td style="text-align: center;">38,431</td>
<td style="text-align: center;">37,644</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">open</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">text</td>
</tr>
</tbody>
</table>
<p>Table 2: A comparison of MWP and Table QA datasets that require numerical reasoning. text*: each table in TABMWP is accompanied by an image format.</p>
<h2>3 MEthods</h2>
<h3>3.1 Few-shot GPT-3 for TABMWP</h3>
<p>Provided with a few in-context examples of math word problems as the context, GPT-3 can generate the answer for a test problem, and shows impressive performance across different MWP datasets (Wei et al., 2022; Wang et al., 2022). Inspired by its success, we first build a strong baseline using few-shot GPT-3 on our TABMWP dataset. Specifically, a few training examples, along with the test example $p_{i}$, are provided to GPT-3 for the answer prediction. Each training example consists of a table context $t$, a question $q$, options $c$ that apply, and an answer $a$. To make the few-shot</p>
<p>GPT-3 model workable on TABMWP, we utilize the semi-structured format as the tabular context. Following Wei et al. (2022), a solution $s$ can be augmented in front of the answer $a$ to reveal the multi-step reasoning process, which is able to boost the prediction performance.</p>
<h1>3.2 Dynamic Prompting via Policy Gradient</h1>
<p>The in-context examples can be randomly (Wei et al., 2022; Wang et al., 2022) or retrieval-based selected (Liu et al., 2022a) from the training set. Recent research, however, has shown that few-shot GPT-3 can be highly unstable across different selections of in-context examples and permutations of those examples (Zhao et al., 2021; Liu et al., 2022a; Lu et al., 2022c). This instability may be more severe on TABMWP, where examples are more distinct because they include both unstructured questions of various types and semi-structured tables in various layouts. To alleviate this issue, we aim to propose a novel approach that can learn to select performing in-context examples using a policy gradient strategy, without brute-force searching or manually designed heuristics.</p>
<p>Formally, given a TABMWP problem $p_{i}$, we want the agent to find $K$ in-context examples $e_{i}=$ $\left{e_{i}^{1}, e_{i}^{2}, \ldots, e_{i}^{K}\right}$ from a candidate pool $E_{\text {cand }}$, and generate the answer $\hat{a}<em i="i">{i}$, maximizing a reward $r</em>}=R\left(\hat{a<em i="i">{i} \mid p</em>\right)$. The in-context examples are selected according to a policy</p>
<p>$$
e_{i}^{k} \sim \pi_{\theta}\left(e_{i} \mid p_{i}\right), e_{i}^{k} \in E_{\text {cand }}, e_{i}^{k} \text { are independent for } k={1,2, \ldots, K}
$$</p>
<p>where $\theta$ are the policy's parameters. The answer is generated through: $\hat{a}<em i="i">{i}=\operatorname{GPT}-3\left(e</em>}, p_{i}\right)$ using the selected examples and the given problem as the input prompt. The reward is then computed by evaluating the generated answer $\hat{a<em i="i">{i}$ with respect to the ground truth answer $a</em>$ :</p>
<p>$$
r_{i}=R\left(\hat{a}<em i="i">{i} \mid p</em>}\right)=\operatorname{EVAL}\left(\hat{a<em i="i">{i}, a</em> \in{-1,1}
$$}\right), r_{i</p>
<p>The function $\operatorname{EVAL}()$ returns a reward of 1 if the generated answer aligned with the label and -1 otherwise. Our goal is to maximize the expected reward of the generated answer under the policy $\mathbb{E}<em i="i">{e</em>\right)\right)]$. We optimize the reward with respect to the parameters of the policy network using the Policy Gradient method (Sutton et al., 1998). The expected reward cannot be computed in closed form, so we compute an unbiased estimation with Monte Carlo Sampling,} \sim \pi_{\theta}\left(e_{i} \mid p_{i}\right)}[R\left(\operatorname{GPT}-3\left(e_{i}, p_{i</p>
<p>$$
\mathbb{E}<em i="i">{e</em>\right)
$$} \sim \pi_{\theta}\left(e_{i} \mid p_{i}\right)}\left[R\left(\operatorname{GPT}-3\left(e_{i}, p_{i}\right)\right)\right] \approx \frac{1}{N} \sum_{i=1}^{N} R\left(\operatorname{GPT}-3\left(e_{i}, p_{i}\right)\right), e_{i} \sim \pi_{\theta}\left(e_{i} \mid p_{i</p>
<p>where $N$ is the size of each batch yielded from our training problem set $P_{\text {train }}$. In this work, we experiment using the REINFORCE policy gradient algorithm (Williams, 1992):</p>
<p>$$
\begin{aligned}
\nabla \mathbb{E}<em i="i">{e</em>} \sim \pi_{\theta}\left(e_{i} \mid p_{i}\right)}\left[R\left(\operatorname{GPT}-3\left(e_{i}, p_{i}\right)\right)\right] &amp; =\mathbb{E<em i="i">{e</em>\right)\right) \
&amp; \approx \frac{1}{N} \sum_{i=1}^{N} \nabla_{\theta} \log \left(\pi_{\theta}\left(e_{i} \mid p_{i}\right)\right) R\left(\operatorname{GPT}-3\left(e_{i}, p_{i}\right)\right), e_{i} \sim \pi_{\theta}\left(e_{i} \mid p_{i}\right)
\end{aligned}
$$} \sim \pi_{\theta}\left(e_{i} \mid p_{i}\right)} \nabla_{\theta} \log \left(\pi_{\theta}\left(e_{i} \mid p_{i}\right)\right) R\left(\operatorname{GPT}-3\left(e_{i}, p_{i</p>
<p>Intuitively, if the predicted answer is correct, we update the policy so that the probability of selecting the same prompts gets higher. Otherwise, we update the policy to reduce the probability of selecting such less matched examples. The learning process is summarized in Algorithm 1 in the appendix.</p>
<p>To get the contextualized representation of the given problem and candidate examples, we use the BERT (Devlin et al., 2018) [CLS] token representation as the problem encoding. We add a small linear layer on top of the BERT final pooling layer. That allows our model to learn both the semantic similarity that the pre-trained BERT model provides and the hidden logical similarity shared among the math problems. During training, the parameters of BERT are fixed and only the appended linear layer is updated, i.e., $\theta$ is composed of the learnable parameters $\mathbf{W}$ and $\mathbf{b}$ :</p>
<p>$$
\begin{aligned}
\mathbf{h}\left(e_{i}\right) &amp; =\mathbf{W}\left(\operatorname{BERT}\left(e_{i}\right)\right)+\mathbf{b} \
\mathbf{h}\left(p_{i}\right) &amp; =\mathbf{W}\left(\operatorname{BERT}\left(p_{i}\right)\right)+\mathbf{b} \
\pi_{\theta}\left(e_{i} \mid p_{i}\right) &amp; =\frac{\exp \left[\mathbf{h}\left(e_{i}\right) \cdot \mathbf{h}\left(p_{i}\right)\right]}{\sum_{e_{i}^{\prime} \in E_{\text {cand }}} \exp \left[\mathbf{h}\left(e_{i}^{\prime}\right) \cdot \mathbf{h}\left(p_{i}\right)\right]}
\end{aligned}
$$</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 EXPERIMENTAL SETTINGS</h3>
<p>Baselines. We first develop two large language models, UnifiedQA (Khashabi et al., 2020) and TAPEX (Liu et al., 2022b), in both pre-trained and fine-tuned settings, as strong baselines on TABMWP. Different model sizes are included to examine the performance across different model capacities. We further implement the zero-shot GPT-3 model, the few-shot GPT-3 model, and their chain-of-thought (CoT) reasoning variants (Wei et al., 2022). We also study the heuristic guess baseline and human performance to analyze the lower and upper bounds on TABMWP, respectively.</p>
<p>Evaluation metric. The answer part is extracted from the GPT-3 generation using manually designed regular expressions. To evaluate the baselines and our method, we utilize the accuracy metric to determine if the generated answer is correct given the ground truth answer. For free-text problems where the answer is set as a number, we normalize the prediction and the label to decimal numbers with two-digit precision and check if their values are equivalent. For multi-choice problems, we choose the most similar one from options to the generated answer following Khashabi et al. (2020).</p>
<p>Implementation details. Fine-tuned UnifiedQA and TAPEX baselines are trained on the train split and evaluated on the test split. Few-shot GPT-3 and few-shot-CoT GPT-3 randomly select two incontext examples from the training data to build the prompt. Our PromptPG is built on top of few-shot GPT-3 with a different selection strategy: (a) in the training stage, the agent learns to select two examples from 20 candidates and is evaluated on 160 training examples to calculate the reward; (b) in the test stage, the agent with an optimal policy chooses two examples from 20 candidates for each test example. The candidates are randomly selected from the training set. Experiments for two few-shot GPT-3 baselines and our PromptPG are repeated three times, and the average accuracy is reported in Table 3. More implementation details can be found in Appendix A.4.</p>
<h3>4.2 EXPERIMENTAL RESULTS</h3>
<p>Table 3 demonstrates the results of different baselines and our method on the TABMWP dataset. Benefiting from pre-training on the tabular corpus, the TAPEX baseline performs better on average than UnifiedQA with a similar model size, which is only pre-trained on unstructured textual data. Increasing the model size can improve the prediction accuracy for both UnifiedQA and TAPEX. Fine-tuned on TABMWP, the baseline models can significantly improve the prediction performance on the average and all aggregated accuracy metrics.</p>
<p>Without any examples provided to GPT-3, zero-shot GPT-3 achieves a comparable accuracy to the best fine-tuned baselines, UnifiedQA $<em _LARGE="{LARGE" _text="\text">{\text {LARGE }}$ and TAPEX $</em>$, showing its surprisingly good generalization ability on TABMWP. Provided with two randomly sampled in-context examples as the prompt, few-shot GPT-3 gets an improvement of $0.17 \%$. Generating the multi-step solution before the answer, the few-shot-CoT GPT-3 model reports the best performance among all of these baseline models, with an accuracy of $62.92 \%$. Unlike few-shot-CoT GPT-3 randomly selecting the in-context examples, our proposed PromptPG learns to select performing examples with the help of policy gradient. PromptPG establishes a state-of-the-art performance on the TABMWP dataset: it surpasses the best baseline few-shot-CoT GPT-3 by $5.31 \%$ on average. PromptPG shows its consistent advantages on two question types, two grade groups, and most of the answer types.}</p>
<p>Heuristic guess and human performance. The accuracy of multi-choice questions by heuristic guess is $39.81 \%$, which aligns with the fact that there are 2.88 options on average. The accuracy for free-text questions is considerably low since the inputs of TABMWP problems do not have direct clues for the answers. Humans outperform all benchmarks consistently across question types, answer types, and grade groups, with a $21.99 \%$ average accuracy advantage over our best performing PromptPG. This gap is to be filled by future research on semi-structured mathematical reasoning.</p>
<p>Problem types and difficulty. Among all the baselines, we find it is easier for models to answer multi-choice questions than free-text questions. Questions with the boolean (BOOL) and other (OTH) answer types tend to have lower accuracy scores than the extractive (EXTR) answer type, because the former ones need the abilities of fact verification and language understanding on diverse options, respectively. It is also not surprising for us to find that all the models perform worse on problems in grades 7-8 than in a lower-level group of 1-6.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Training <br> Data</th>
<th style="text-align: center;">Selection <br> Strategy</th>
<th style="text-align: center;">Question Types</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answer Types</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Grades</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FREE</td>
<td style="text-align: center;">MC</td>
<td style="text-align: center;">INT</td>
<td style="text-align: center;">DEC</td>
<td style="text-align: center;">EXTR</td>
<td style="text-align: center;">BOOL</td>
<td style="text-align: center;">OTH</td>
<td style="text-align: center;">$1-6$</td>
<td style="text-align: center;">7-8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Heuristic Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Heuristic guess</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.71</td>
<td style="text-align: center;">39.81</td>
<td style="text-align: center;">8.37</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">30.80</td>
<td style="text-align: center;">51.22</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">17.55</td>
<td style="text-align: center;">12.27</td>
<td style="text-align: center;">15.29</td>
</tr>
<tr>
<td style="text-align: center;">Human performance</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">84.61</td>
<td style="text-align: center;">93.32</td>
<td style="text-align: center;">84.95</td>
<td style="text-align: center;">83.29</td>
<td style="text-align: center;">97.18</td>
<td style="text-align: center;">88.69</td>
<td style="text-align: center;">96.20</td>
<td style="text-align: center;">94.27</td>
<td style="text-align: center;">81.28</td>
<td style="text-align: center;">90.22</td>
</tr>
<tr>
<td style="text-align: center;">pre-trained Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA $_{\text {SmaA }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">43.62</td>
<td style="text-align: center;">1.37</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">38.70</td>
<td style="text-align: center;">49.78</td>
<td style="text-align: center;">37.14</td>
<td style="text-align: center;">15.57</td>
<td style="text-align: center;">7.65</td>
<td style="text-align: center;">12.18</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA $_{\text {BASE }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">43.02</td>
<td style="text-align: center;">5.28</td>
<td style="text-align: center;">1.97</td>
<td style="text-align: center;">37.08</td>
<td style="text-align: center;">50.11</td>
<td style="text-align: center;">38.10</td>
<td style="text-align: center;">17.14</td>
<td style="text-align: center;">11.11</td>
<td style="text-align: center;">14.56</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA $_{\text {LARGE }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.48</td>
<td style="text-align: center;">48.80</td>
<td style="text-align: center;">5.19</td>
<td style="text-align: center;">1.72</td>
<td style="text-align: center;">48.33</td>
<td style="text-align: center;">50.33</td>
<td style="text-align: center;">40.00</td>
<td style="text-align: center;">19.78</td>
<td style="text-align: center;">10.87</td>
<td style="text-align: center;">15.96</td>
</tr>
<tr>
<td style="text-align: center;">TAPEX $_{\text {BASE }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.32</td>
<td style="text-align: center;">39.76</td>
<td style="text-align: center;">8.68</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">35.06</td>
<td style="text-align: center;">47.11</td>
<td style="text-align: center;">20.95</td>
<td style="text-align: center;">18.67</td>
<td style="text-align: center;">11.81</td>
<td style="text-align: center;">15.73</td>
</tr>
<tr>
<td style="text-align: center;">TAPEX $_{\text {LARGE }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.80</td>
<td style="text-align: center;">46.59</td>
<td style="text-align: center;">10.62</td>
<td style="text-align: center;">1.72</td>
<td style="text-align: center;">46.91</td>
<td style="text-align: center;">48.11</td>
<td style="text-align: center;">30.48</td>
<td style="text-align: center;">22.65</td>
<td style="text-align: center;">13.18</td>
<td style="text-align: center;">18.59</td>
</tr>
<tr>
<td style="text-align: center;">fine-tuned Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA $_{\text {SmaA }}$</td>
<td style="text-align: center;">23,059</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.27</td>
<td style="text-align: center;">51.31</td>
<td style="text-align: center;">27.27</td>
<td style="text-align: center;">2.83</td>
<td style="text-align: center;">52.28</td>
<td style="text-align: center;">48.11</td>
<td style="text-align: center;">69.52</td>
<td style="text-align: center;">35.85</td>
<td style="text-align: center;">21.71</td>
<td style="text-align: center;">29.79</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA $_{\text {BASE }}$</td>
<td style="text-align: center;">23,059</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.02</td>
<td style="text-align: center;">70.68</td>
<td style="text-align: center;">40.74</td>
<td style="text-align: center;">7.90</td>
<td style="text-align: center;">84.09</td>
<td style="text-align: center;">55.67</td>
<td style="text-align: center;">73.33</td>
<td style="text-align: center;">53.31</td>
<td style="text-align: center;">30.46</td>
<td style="text-align: center;">43.52</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA $_{\text {LARGE }}$</td>
<td style="text-align: center;">23,059</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">48.67</td>
<td style="text-align: center;">82.18</td>
<td style="text-align: center;">55.97</td>
<td style="text-align: center;">20.26</td>
<td style="text-align: center;">94.63</td>
<td style="text-align: center;">68.89</td>
<td style="text-align: center;">79.05</td>
<td style="text-align: center;">65.92</td>
<td style="text-align: center;">45.92</td>
<td style="text-align: center;">57.35</td>
</tr>
<tr>
<td style="text-align: center;">TAPEX $_{\text {BASE }}$</td>
<td style="text-align: center;">23,059</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">39.59</td>
<td style="text-align: center;">73.09</td>
<td style="text-align: center;">46.85</td>
<td style="text-align: center;">11.33</td>
<td style="text-align: center;">84.19</td>
<td style="text-align: center;">61.33</td>
<td style="text-align: center;">69.52</td>
<td style="text-align: center;">56.70</td>
<td style="text-align: center;">37.02</td>
<td style="text-align: center;">48.27</td>
</tr>
<tr>
<td style="text-align: center;">TAPEX $_{\text {LARGE }}$</td>
<td style="text-align: center;">23,059</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">51.00</td>
<td style="text-align: center;">80.02</td>
<td style="text-align: center;">59.92</td>
<td style="text-align: center;">16.31</td>
<td style="text-align: center;">95.34</td>
<td style="text-align: center;">64.00</td>
<td style="text-align: center;">73.33</td>
<td style="text-align: center;">67.11</td>
<td style="text-align: center;">47.07</td>
<td style="text-align: center;">58.52</td>
</tr>
<tr>
<td style="text-align: center;">Prompting Baselines w/ GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">53.57</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">55.55</td>
<td style="text-align: center;">45.84</td>
<td style="text-align: center;">78.22</td>
<td style="text-align: center;">55.44</td>
<td style="text-align: center;">54.29</td>
<td style="text-align: center;">63.37</td>
<td style="text-align: center;">48.41</td>
<td style="text-align: center;">56.96</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot-CoT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">54.36</td>
<td style="text-align: center;">66.92</td>
<td style="text-align: center;">55.82</td>
<td style="text-align: center;">48.67</td>
<td style="text-align: center;">78.82</td>
<td style="text-align: center;">55.67</td>
<td style="text-align: center;">51.43</td>
<td style="text-align: center;">63.62</td>
<td style="text-align: center;">49.59</td>
<td style="text-align: center;">57.61</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot (2-shot)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">54.69</td>
<td style="text-align: center;">64.11</td>
<td style="text-align: center;">58.36</td>
<td style="text-align: center;">40.40</td>
<td style="text-align: center;">75.95</td>
<td style="text-align: center;">52.41</td>
<td style="text-align: center;">53.02</td>
<td style="text-align: center;">63.10</td>
<td style="text-align: center;">49.16</td>
<td style="text-align: center;">57.13</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot-CoT (2-shot)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">60.76</td>
<td style="text-align: center;">69.09</td>
<td style="text-align: center;">60.04</td>
<td style="text-align: center;">63.58</td>
<td style="text-align: center;">76.49</td>
<td style="text-align: center;">61.19</td>
<td style="text-align: center;">67.30</td>
<td style="text-align: center;">68.62</td>
<td style="text-align: center;">55.31</td>
<td style="text-align: center;">62.92</td>
</tr>
<tr>
<td style="text-align: center;">PromptPG w/ GPT-3 (Ours)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Few-shot-CoT (2-shot)</td>
<td style="text-align: center;">$160+20$</td>
<td style="text-align: center;">Dynamic</td>
<td style="text-align: center;">66.17</td>
<td style="text-align: center;">74.11</td>
<td style="text-align: center;">64.12</td>
<td style="text-align: center;">74.16</td>
<td style="text-align: center;">76.19</td>
<td style="text-align: center;">72.81</td>
<td style="text-align: center;">65.71</td>
<td style="text-align: center;">71.20</td>
<td style="text-align: center;">64.27</td>
<td style="text-align: center;">68.23</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation results of various baselines and our method on TABMWP. Training Data: number of used training data; Selection Strategy: strategy of selecting in-context examples for few-shot GPT-3; FREE: free-text questions; MC: multi-choice questions; INT: integer answers; DEC: decimal answers; EXTR: extractive text answers; BOOL: Boolean text answers; OTH: other text answers.</p>
<h1>4.3 Ablation Study</h1>
<p>Here, we will study how different factors have an effect on the performances of baselines and our method on TABMWP. Experiments are conducted on 1,000 development examples.</p>
<p>Blind study of the dataset. We evaluate the information gain of each component of the TABMWP problems by removing it from model inputs. To eliminate the impact and variance caused by example selection, the study is conducted using the zero-shot GPT-3 model. As shown in Table 4, there is a dramatic decline when either the tabular context $(\mathrm{T})$ or the question text $(\mathrm{Q})$ is missing from the inputs. For example, $\mathrm{T} \rightarrow \mathrm{A}$ and $\mathrm{Q} \rightarrow \mathrm{A}$ only attain an average accuracy of $6.10 \%$ and $7.00 \%$, respectively, and their accuracies are near to zero on the multi-choice questions. Taking both tabular and textual data as inputs $(\mathrm{TQ} \rightarrow \mathrm{A})$, the model significantly beats the heuristic guess. With the complete input information $(\mathrm{TQ}(\mathrm{C}) \rightarrow \mathrm{A})$, the full model achieves the best performance. The blind study shows that our TABMWP is robust and reliable in distribution, and all input components are indispensable parts that provide necessary information for answering the questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Format</th>
<th style="text-align: center;">FREE</th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;">INT</th>
<th style="text-align: center;">DEC</th>
<th style="text-align: center;">EXTR</th>
<th style="text-align: center;">BOOL</th>
<th style="text-align: center;">OTH</th>
<th style="text-align: center;">1-6</th>
<th style="text-align: center;">7-8</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Heuristic guess</td>
<td style="text-align: center;">$\mathrm{TQ}(\mathrm{C}) \rightarrow \mathrm{A}$</td>
<td style="text-align: center;">7.31</td>
<td style="text-align: center;">40.36</td>
<td style="text-align: center;">9.20</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">34.44</td>
<td style="text-align: center;">47.32</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">17.99</td>
<td style="text-align: center;">13.96</td>
<td style="text-align: center;">16.40</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot GPT-3</td>
<td style="text-align: center;">$\mathrm{T} \rightarrow \mathrm{A}$</td>
<td style="text-align: center;">8.28</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">10.24</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">9.41</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">6.10</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot GPT-3</td>
<td style="text-align: center;">$\mathrm{Q} \rightarrow \mathrm{A}$</td>
<td style="text-align: center;">9.24</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">10.94</td>
<td style="text-align: center;">2.68</td>
<td style="text-align: center;">1.32</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">10.23</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">7.00</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot GPT-3</td>
<td style="text-align: center;">$\mathrm{T}(\mathrm{C}) \rightarrow \mathrm{A}$</td>
<td style="text-align: center;">8.28</td>
<td style="text-align: center;">41.82</td>
<td style="text-align: center;">10.24</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">36.42</td>
<td style="text-align: center;">50.89</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">23.60</td>
<td style="text-align: center;">8.12</td>
<td style="text-align: center;">17.50</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot GPT-3</td>
<td style="text-align: center;">$\mathrm{Q}(\mathrm{C}) \rightarrow \mathrm{A}$</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">33.09</td>
<td style="text-align: center;">10.94</td>
<td style="text-align: center;">2.01</td>
<td style="text-align: center;">25.17</td>
<td style="text-align: center;">44.64</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">21.29</td>
<td style="text-align: center;">7.11</td>
<td style="text-align: center;">15.70</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot GPT-3</td>
<td style="text-align: center;">$\mathrm{TQ} \rightarrow \mathrm{A}$</td>
<td style="text-align: center;">55.31</td>
<td style="text-align: center;">68.36</td>
<td style="text-align: center;">56.60</td>
<td style="text-align: center;">50.34</td>
<td style="text-align: center;">79.47</td>
<td style="text-align: center;">54.46</td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;">66.34</td>
<td style="text-align: center;">47.46</td>
<td style="text-align: center;">58.90</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot GPT-3 (full model)</td>
<td style="text-align: center;">$\mathrm{TQ}(\mathrm{C}) \rightarrow \mathrm{A}$</td>
<td style="text-align: center;">54.76</td>
<td style="text-align: center;">72.00</td>
<td style="text-align: center;">56.42</td>
<td style="text-align: center;">48.32</td>
<td style="text-align: center;">76.82</td>
<td style="text-align: center;">66.07</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">67.00</td>
<td style="text-align: center;">47.97</td>
<td style="text-align: center;">59.50</td>
</tr>
</tbody>
</table>
<p>Table 4: Blind studies on TABMWP. T: tabular context; Q: question; C: choice options; A: answer. $\mathrm{Q}(\mathrm{C})$ means choice options come after the question in the input, while Q refers to the question only.</p>
<p>Number of training examples. We study the effect of different numbers of training examples on our dynamic prompt learning in Figure 3 (a). With more training examples, the prediction accuracy first gradually increases to a peak of around 160 training examples. After that, the accuracy goes down with a growing variance. We reckon it is because the policy gradient algorithm can benefit from the scaling-up training data but fails to exploit more examples efficiently.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Accuracy w.r.t. different numbers of training examples, given 20 candidate examples.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Accuracy w.r.t. different numbers of candidates, given 80 and 160 training examples.</p>
<p>Figure 3: Accuracy w.r.t. different numbers of training and candidate examples. Experiments are conducted on 1,000 development instances, and each setting is repeated with four random seeds.</p>
<p>Number of candidate examples. In Figure 3 (b), we investigate how different numbers of candidate examples can affect policy learning performance. With the increasing candidate number, it is observed that the prediction accuracy will first go up and then go down after a threshold, given 80 or 160 training examples. It is probably because when the candidate pool is too small, the policy gradient algorithm has a limited action space to explore enough problem types. In contrast, too many candidates could make the algorithm hard to learn an optimal policy in a large search space.</p>
<p>Different selection strategies. In Table 5, we compare the proposed PromptPG with random selection and other heuristic-based example selection strategies for the few-shot-CoT GPT-3 model. Compared to random selection, selecting the same question or answer type of examples helps the model to take the task-relevant examples as the prompt, thus improving the accuracy and reducing the variance. Choosing the most complex examples does not boost the prediction performance consistently. Manual selection selects the two examples from 20 with the highest evaluation accuracy on one-shot-CoT GPT-3 as the fixed set of in-context examples. Although it achieves the lowest prediction variance of 0 , it only improves by $1.7 \%$ over random selection. The most semantically similar examples, as a kind of nearest neighbor search of the test example, help construct the performing and stable prompt for GPT-3. PromptPG shows its effectiveness in selecting optimal in-context examples over other strategies and largely reduces the instability.</p>
<h1>4.4 CASE Study</h1>
<p>We conduct the case study in Appendix A.7. We visualize the two in-context examples selected by strategies of our PromptPG, nearest neighbor search, and random selection, in Figure 5, 6, and 7, respectively. The nearest neighbor search strategy selects the "superficially" similar examples to the test example. Instead, PromptPG tends to select examples that have multiple reasoning steps in the solution and similar abilities in mathematical reasoning, which results in higher prediction accuracy. Successful examples in Figure 8 - 12 show that PromptPG is able to generate reasonable reasoning steps to predict correct answers for a wide range of TABMWP problems. Failure examples in Figure 13 - 18 suggest that PromptPG has limitations when solving problems provided with complex tabular contexts or requiring a high-level ability of mathematical reasoning.</p>
<h2>5 Related Work</h2>
<h3>5.1 Math Word Problems</h3>
<p>The task of solving Math Word Problems (MWPs) is to predict the answer given a natural language description of a math problem. There have been great efforts in developing datasets for MWPs,</p>
<p>including Math23K (Wang et al., 2017), MathQA (Amini et al., 2019), ASDiv (Miao et al., 2020), SVAMP (Patel et al., 2021), and Lila (Mishra et al., 2022). However, these datasets only involve the textual modality, and most are limited to a small data scale. Some recent datasets like DVQA (Kafle et al., 2018), IconQA (Lu et al., 2021b), Geometry3K (Lu et al., 2021a), and UniGeo (Chen et al., 2022) introduce math problems with diagrams as the visual context, where the system needs to perform mathematical reasoning over multi-modal information. To the best of our knowledge, our dataset TABMWP is the first dataset that requires mathematical reasoning over heterogeneous information from both the textual question and the tabular context. To solve MWPs, one popular line of previous methods is to generate the intermediate expressions and execute them to get the final answers (Huang et al., 2017; Roy \&amp; Roth, 2017; Amini et al., 2019). Inspired by the recent progress achieved by GPT-3 in solving MWPs (Wei et al., 2022; Wang et al., 2022; Kojima et al., 2022), we evaluate TABMWP using GPT-3 models in zero-shot and few-shot learning manners.</p>
<h1>5.2 Table QA Datasets</h1>
<p>Table Question Answering (Table QA) refers to the task of answering questions about tabular data. Numerous datasets have been developed for Table QA. For example, TabMCQ (Jauhar et al., 2016) is an early dataset collected from grade exams. Datasets like WTQ (Pasupat \&amp; Liang, 2015), WikiSQL (Zhong et al., 2017), and SQA (Iyyer et al., 2017) contain semi-structured tables from Wikipedia, while Spider (Yu et al., 2018) collects structured tables sourced from databases. Recent work aims at introducing datasets that require multi-hop reasoning between the textual and tabular data: HybridQA (Chen et al., 2020b), OTTQA (Chen et al., 2020a), MultiModalQA (Talmor et al., 2020), AIT-QA (Katsis et al., 2021), and FeTaQA (Nan et al., 2022). Datasets most related to our TABMWP dataset are FinQA (Chen et al., 2021), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) because they need numerical reasoning on financial reports with tabular data. Note that $77.6 \%$ of questions in TAT-QA can be solvable without mathematical reasoning and $50.0 \%$ of questions in FinQA are not table-must to be answered. In contrast, our proposed TABMWP collects questions where both mathematical reasoning and tabular context are necessary.</p>
<h3>5.3 Prompt Learning for Language Models</h3>
<p>Large pre-trained language models, such as GPT-3 (Brown et al., 2020), have shown their remarkable ability of few-shot learning on a wide range of downstream tasks (Houlsby et al., 2019; Brown et al., 2020; Ma et al., 2022; Lu et al., 2022a). Given a few in-context examples as demonstrations, GPT-3 can generalize to unseen test examples without parameter updating. For example, Wei et al. (2022) randomly select different in-context examples from the training set and formulate their corresponding prompt with a test sample. However, recent studies show that few-shot GPT-3 highly depends on the selection of in-context examples and could be unstable, varying from the near chance to near state-of-the-art performance (Zhao et al., 2021; Liu et al., 2022a; Lu et al., 2022b). To mitigate the volatility of selecting in-context examples, Lu et al. (2022c) propose retrieving relevant examples that are semantically similar to the test sample. Other possible strategies could be using brute-force permutation search or relying on manually designed heuristics like choosing the most complex examples. Inspired by reinforcement learning's ability to search for an optimal action policy, we propose applying the policy gradient strategy (Sutton et al., 1998) to learn to select in-context examples more efficiently and stably without designing human-designed heuristics.</p>
<h2>6 CONCLUSION</h2>
<p>In this paper, we propose TABMWP, the first large-scale dataset for math word problems in tabular contexts. TABMWP contains 38,431 open-domain problems with two question types and three answer types, and each problem is annotated with a multi-step solution. We evaluate TABMWP using state-of-the-art QA and TableQA methods in both pre-trained and fine-tuned settings, as well as the large pre-trained language model GPT-3. We further propose a novel approach, PromptPG, for few-shot GPT-3, which utilizes policy gradient to learn to select in-context examples from the training data and construct the performing prompt for the test example. Experimental results show that PromptPG outperforms existing strong baselines by a large margin of $5.31 \%$ and reduces the accuracy volatility compared to random selection. To the best of our knowledge, it is the first work that applies reinforcement learning to select in-context examples for the few-shot GPT-3 model.</p>
<h1>7 ACKNOWLEDGMENT</h1>
<p>We would like to thank Zhou Yu and Jiuxiang Gu for insightful discussions on dataset collection. We thank Muhao Chen and Yao Fu for constructive suggestions in developing baselines and experiments. The work does not relate to Liang Qiu's position at Amazon Alexa.</p>
<h2>REFERENCES</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pp. 2357-2367, 2019.</p>
<p>Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617, 2018.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:1877-1901, 2020.</p>
<p>Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. In The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Yang Wang, and William W Cohen. Open question answering over tables and text. In International Conference on Learning Representations (ICLR), 2020a.</p>
<p>Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1026-1036, 2020b.</p>
<p>Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. Finqa: A dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3697-3711, 2021.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning (ICML), pp. 2790-2799. PMLR, 2019.</p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma. How well do computers solve math word problems? large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 887-896, 2016.</p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, and Jian Yin. Learning fine-grained expressions to solve math word problems. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pp. 805-814, 2017.</p>
<p>Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1821-1831, 2017.</p>
<p>Sujay Kumar Jauhar, Peter Turney, and Eduard Hovy. Tabmcq: A dataset of general knowledge tables and multiple-choice questions. arXiv preprint arXiv:1602.03960, 2016.</p>
<p>Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5648-5656, 2018.</p>
<p>Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, and David Carter. Batch policy gradient methods for improving neural conversation models. arXiv preprint arXiv:1702.03334, 2017.</p>
<p>Yannis Katsis, Saneem Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Mustafa Canim, Michael Glass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen, Karthik Sankaranarayanan, et al. Aitqa: Question answering dataset over complex tables in the airline industry. arXiv preprint arXiv:2106.12944, 2021.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa system. In Findings of the Association for Computational Linguistics (EMNLP), pp. 1896-1907, 2020.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, Online, July 2020. Association for Computational Linguistics (ACL).</p>
<p>Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100-114, 2022a.</p>
<p>Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. Tapex: Table pre-training via learning a neural sql executor. In International Conference on Learning Representations (ICLR), 2022b.</p>
<p>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021a.</p>
<p>Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021b.</p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS 2022), 2022a.</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. arXiv preprint arXiv:2212.10535, 2022b.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 8086-8098, 2022c.</p>
<p>Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 975-984, 2020.</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. In The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928-1937. PMLR, 2016.</p>
<p>Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Lin, Neha Verma, Rui Zhang, Wojciech Kryściński, Nick Schoelkopf, Riley Kong, Xiangru Tang, et al. Fetaqa: Free-form table question answering. Transactions of the Association for Computational Linguistics (TACL), 10:35-49, 2022.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJNLP), pp. 14701480, 2015.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pp. 2080-2094, 2021.</p>
<p>Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2219-2225. IEEE, 2006.</p>
<p>Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng Gao, and Song-Chun Zhu. Valuenet: A new dataset for human value driven dialogue system. IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), 44(5):2468-2484, 2022.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 21:1-67, 2020.</p>
<p>Subhro Roy and Dan Roth. Unit dependency graph and its application to arithmetic word problem solving. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2017.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pp. 387-395. PMLR, 2014.</p>
<p>Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning. MIT press Cambridge, 1998.</p>
<p>Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: complex question answering over text, tables and images. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Shyam Upadhyay and Ming-Wei Chang. Annotating derivations: A new evaluation strategy and dataset for algebra word problems. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (ACL), pp. 494-504, 2017.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 845-854, 2017.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229-256, 1992.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3911-3921, 2018.</p>
<p>Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 6588-6600, 2022.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (ICML), pp. 12697-12706. PMLR, 2021.</p>
<p>Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.</p>
<p>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-JCNLP), pp. 3277-3287, 2021.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 DATASET COLLECTION</h2>
<p>The raw problems are collected from an online learning website, $\mathrm{IXL}^{2}$, which hosts a large number of high-quality math problems curated by educational experts.</p>
<p>Quality control. The goal of constructing TABMWP is to collect math word problems that necessitate multi-hop mathematical reasoning between the question and the tabular context. Therefore, we ask human experts to filter problems that can be solved either without the context of the table or by looking up table cells without numerical reasoning. To further ensure data quality, we ask human experts to perform a final review to re-check the dataset and manually revise incorrect annotations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question types</th>
<th style="text-align: left;">Answer types (\%)</th>
<th style="text-align: left;">Descriptions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Free-text</td>
<td style="text-align: left;">Integer (59.50\%)</td>
<td style="text-align: left;">The answer is an integer number, e.g., "40", "1,207", "-3".</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Decimal (15.23\%)</td>
<td style="text-align: left;">The answer is a decimal or a fraction number, e.g., "192.80", "68/217".</td>
</tr>
<tr>
<td style="text-align: left;">Multi-choice</td>
<td style="text-align: left;">Extractive (13.01\%)</td>
<td style="text-align: left;">The answer could be extracted from the table context.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Boolean (10.97\%)</td>
<td style="text-align: left;">The answer is Boolean, e.g., "yes"/"no", "true"/"false", "linear"/"nonlear".</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Other (1.29\%)</td>
<td style="text-align: left;">The answer belongs to other text types, e.g., a statement.</td>
</tr>
</tbody>
</table>
<p>Table 6: Format diversity of questions and answers in TABMWP.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Image format</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Semi-structured format</th>
<th style="text-align: center;">Structured format</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Field day schedule</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Table title: Field day schedule</td>
<td style="text-align: center;">Table title: Field day schedule</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Event</td>
<td style="text-align: center;">Begin</td>
<td style="text-align: center;">End</td>
<td style="text-align: center;">Table text:</td>
<td style="text-align: center;">Event</td>
<td style="text-align: center;">Begin</td>
<td style="text-align: center;">End</td>
</tr>
<tr>
<td style="text-align: center;">water balloon toss</td>
<td style="text-align: center;">11:30 A.M.</td>
<td style="text-align: center;">11:50 A.M.</td>
<td style="text-align: center;">Event | Begin | End</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">water balloon toss</td>
<td style="text-align: center;">11:30 A.M.</td>
</tr>
<tr>
<td style="text-align: center;">obstacle course</td>
<td style="text-align: center;">12:05 P.M.</td>
<td style="text-align: center;">12:25 P.M.</td>
<td style="text-align: center;">water balloon toss | 11:30 A.M. | 11:50 A.M.</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">obstacle course</td>
<td style="text-align: center;">12:05 P.M.</td>
</tr>
<tr>
<td style="text-align: center;">parachute ball toss</td>
<td style="text-align: center;">12:30 P.M.</td>
<td style="text-align: center;">1:30 P.M.</td>
<td style="text-align: center;">obstacle course | 12:05 P.M. | 12:25 P.M.</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">parachute ball toss</td>
<td style="text-align: center;">12:30 P.M.</td>
</tr>
<tr>
<td style="text-align: center;">jump rope race</td>
<td style="text-align: center;">1:40 P.M.</td>
<td style="text-align: center;">2:05 P.M.</td>
<td style="text-align: center;">parachute ball toss | 12:30 P.M. | 1:30 P.M.</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">jump rope race</td>
<td style="text-align: center;">1:40 P.M.</td>
</tr>
<tr>
<td style="text-align: center;">balloon stomp</td>
<td style="text-align: center;">2:15 P.M.</td>
<td style="text-align: center;">2:35 P.M.</td>
<td style="text-align: center;">jump rope race | 1:40 P.M. | 2:05 P.M.</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">balloon stomp</td>
<td style="text-align: center;">2:15 P.M.</td>
</tr>
<tr>
<td style="text-align: center;">relay race</td>
<td style="text-align: center;">2:50 P.M.</td>
<td style="text-align: center;">3:40 P.M.</td>
<td style="text-align: center;">balloon stomp | 2:15 P.M. | 2:35 P.M.</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">relay race</td>
<td style="text-align: center;">2:50 P.M.</td>
</tr>
<tr>
<td style="text-align: center;">hula hoop contest</td>
<td style="text-align: center;">3:55 P.M.</td>
<td style="text-align: center;">4:30 P.M.</td>
<td style="text-align: center;">relay race | 2:50 P.M. | 3:40 P.M.</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">hula hoop contest</td>
<td style="text-align: center;">3:55 P.M.</td>
</tr>
<tr>
<td style="text-align: center;">potato sack race</td>
<td style="text-align: center;">4:40 P.M.</td>
<td style="text-align: center;">5:15 P.M.</td>
<td style="text-align: center;">hula hoop contest | 3:55 P.M. | 4:30 P.M.</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">potato sack race</td>
<td style="text-align: center;">4:40 P.M.</td>
</tr>
</tbody>
</table>
<p>Table 7: Three different formats for the tables in the TABMWP dataset.</p>
<h2>A. 2 HUMAN STUDY</h2>
<p>To examine how humans perform on our TABMWP dataset, we released the human evaluation task on Amazon Mechanical Turk (AMT) to the test split. We designed two sub-tasks for the human study: answering the free-text questions and answering the multi-choice questions. The user interfaces for the two sub-tasks are shown in Figure 4. Each human intelligence task (HIT) contains 5 exam questions and 15 test questions. A worker should have a HIT Approval Rate of $98 \%$ or higher and be approved with 5,000 or more HITs. The worker is provided with detailed instructions at the beginning and needs to pass at least 3 free-text exam questions or 4 multi-choice exam questions to be qualified for the human study. Each HIT is assigned to two different workers. We assign a reward of $\$ 0.80$ and $\$ 0.60$ for one HIT of free-text and multi-choice sub-tasks, respectively.</p>
<h2>A. 3 THE PROMPTPG ALGORITHM</h2>
<p>The pipeline of PromptPG to learn to select in-context examples is summarized in Algorithm 1.</p>
<h2>A. 4 IMPLEMENTATION DETAILS</h2>
<p>Heuristics guess. To investigate the lower bound of the accuracy on TABMWP, we design simple heuristics to guess answers for each question type. For multi-choice questions, we randomly</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: User interfaces of human study for free-text and multi-choice questions.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Dynamic Prompt Learning via Policy Gradient (PromptPG)
Input: Initial policy \(\pi_{\theta_{0}\), training example set \(P_{\text {train }}\), candidate example set \(E_{\text {cand }}, \#\) of training epochs \(N\)
Output: Learned policy \(\pi_{\theta}\)
    function \(\operatorname{REINFORCE}\left(\pi_{\theta_{0}}, P_{\text {train }}, E_{\text {cand }}, N\right)\)
        Initialize policy network \(\pi\) with parameter \(\theta_{0}\)
        for epoch \(=1,2, \ldots, N\) do
            for \(P_{\text {batch }} \in P_{\text {train }}\) do
                \(\mathcal{L}_{\text {batch }} \leftarrow 0\)
                for \(p_{i} \in P_{\text {batch }}\) do
                    Sample \(e_{i}^{k} \sim \pi_{\theta}\left(e_{i} \mid p_{i}\right), e_{i}^{k} \in E_{\text {cand }}, k=\{1, \ldots, K\}\)
                    \(\hat{a}_{i} \leftarrow \operatorname{GPT}-3\left(e_{i}^{1}, \ldots, e_{i}^{k}, p_{i}\right)\)
                    \(r_{i} \leftarrow \operatorname{EVAL}\left(\hat{a}_{i}, a_{i}\right), r_{i} \in\{-1,1\}\)
                    \(\mathcal{L}_{\text {batch }} \leftarrow \mathcal{L}_{\text {batch }}-r_{i} \cdot \ln \pi_{\theta}\left(e_{i} \mid p_{i}\right)\)
                    end for
                    Optimize \(\mathcal{L}_{\text {batch }} \operatorname{wrt} . \theta\)
                    end for
        end for
        return \(\pi_{\theta}\)
    end function
</code></pre></div>

<p>select one from the given options with even probabilities. For free-text questions on TABMWP, the answers could only be integral or decimal numbers. Intuitively, we take advantage of regular expressions to extract all the numbers from the tabular context and the question text as candidates, and then randomly choose one number as the prediction.
UnifiedQA baselines. UnifiedQA (Khashabi et al., 2020) is a T5-based (Raffel et al., 2020) QA system that was pre-trained on 8 seed QA datasets of multiple formats but with a unified text-to-text paradigm. We load the pre-trained checkpoint as the pre-trained baseline and train it on TABMWP as the fine-tuned baseline. Three different parameter sizes are compared: SMALL (60M), BASE (220M), and LARGE (770M).
TAPEX baselines. TAPEX (Liu et al., 2022b) is a BART-based (Lewis et al., 2020) language model pre-trained on structured tabular data to mimic the behavior of a SQL executor that can answer table-based questions. TAPEX shows state-of-the-art performance on four table-related datasets. We establish the pre-trained and fine-tuned baselines on top of TAPEX with two model sizes: BASE (140M) and LARGE (400M).</p>
<p>Zero-shot GPT-3 and zero-shot-CoT GPT-3. We establish the zero-shot baseline based on GPT-3 (Brown et al., 2020). The zero-shot setup follows the format of TQ(C) $\rightarrow$ A where the input is the concatenation of tokens of the tabular context (T), the question text (Q), and choice options (C) that apply while the output is to predict the answer (A). Following Kojima et al. (2022), we further build zero-shot-CoT GPT-3, which refers to the GPT-3 model with a chain-of-thought (CoT) prompt. Specifically, we add the prompt "Let's think step by step" at the end of the input to ask the model to generate the multi-step solution (S) to mimic the reasoning process as humans. Then the model takes the raw input and the newly generated solution to predict the final answer.</p>
<p>Few-shot GPT-3 and few-shot-CoT GPT-3. In the few-shot setting, we follow the standard prompting (Wei et al., 2022) where in-context examples are randomly selected from the training data as demonstrations for the text example. Similarly, the few-shot-CoT GPT-3 baseline takes the prompt template of TQ(C) $\rightarrow$ SA to generate the solution before the final answer.</p>
<p>Experimental details. Our experiments for UnifiedQA baselines, TAPEX baselines, and our proposed PromptPG are conducted using PyTorch on two Nvidia RTX 3090 GPUs. For fine-tuning the UnifiedQA and TAPEX baselines, we use the Adam optimizer (Kingma \&amp; Ba, 2014) with an initial learning rate of $5 \mathrm{e}-5$. The training process takes 10 epochs with a batch size of 16. The maximum number of input tokens is set as 200 and the maximum output length is 100 .</p>
<p>In our proposed PromptPG, the embedding size of the added linear neural network is 768. To learn the policy network, we use the Adam optimizer with an initial learning rate of $1 \mathrm{e}-3$. The maximum number of training epochs is 30 , with a batch size of 20 . The training process is stopped early if there is any NaN value in the loss for a batch of training data.</p>
<p>For the GPT-3 engine, we use TEXT-DAVINCI-002, the most capable engine recommended by the official documentation. The temperature is set as 0 and the top probability is set as 1.0 to get the most deterministic prediction. The maximum number of tokens allowed for generating text is 512. Both the frequency penalty and the presence penalty are set as the default value, i.e., 0 .</p>
<h1>A. 5 MORE EXPERIMENTAL RESULTS</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Selection strategy</th>
<th style="text-align: center;"># training <br> examples</th>
<th style="text-align: center;"># candidate <br> examples</th>
<th style="text-align: center;"># few-shot <br> examples</th>
<th style="text-align: center;">Trial 1</th>
<th style="text-align: center;">Trial 2</th>
<th style="text-align: center;">Trial 3</th>
<th style="text-align: center;">Average (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Few-shot GPT-3</td>
<td style="text-align: left;">Random selection</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">58.12</td>
<td style="text-align: center;">57.00</td>
<td style="text-align: center;">56.27</td>
<td style="text-align: center;">$57.13 \pm 0.93$</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot-CoT GPT-3</td>
<td style="text-align: left;">Random selection</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">59.85</td>
<td style="text-align: center;">63.52</td>
<td style="text-align: center;">65.39</td>
<td style="text-align: center;">$62.92 \pm 2.30$</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot-CoT GPT-3</td>
<td style="text-align: left;">PromptPG (ours)</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\mathbf{6 8 . 8 5}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 2 2}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 2 3} \pm \mathbf{1 . 9 2}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Experimental settings and raw accuracy results of random selection and our PromptPG for the few-shot GPT-3 model on the TABMWP test split. For each setting, we repeat the experiment with the same set of three different random seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Selection strategy</th>
<th style="text-align: center;">Shot number</th>
<th style="text-align: center;">Acc. (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Few-shot-CoT GPT-3</td>
<td style="text-align: left;">Random selection</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$65.2 \pm 4.01$</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot-CoT GPT-3</td>
<td style="text-align: left;">Random selection</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$65.7 \pm 1.16$</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot-CoT GPT-3</td>
<td style="text-align: left;">Random selection</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$67.7 \pm 0.78$</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot-CoT GPT-3</td>
<td style="text-align: left;">Random selection</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$67.5 \pm 0.98$</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot-CoT GPT-3</td>
<td style="text-align: left;">PromptPG (ours)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\mathbf{7 0 . 9} \pm \mathbf{1 . 2 7}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Results of different numbers of few-shot examples on 1,000 development examples.
Number of few-shot examples. We study the few-shot-CoT GPT-3 model with random selection in terms of the different numbers of in-context shots. For each number of in-context shots, the experiment was conducted on 1,000 development examples and repeated three times. The results are shown in Table 9. When increasing the number of in-context shots from the current 2 to 4, the few-shot-CoT GPT-3 model reduces the prediction variance from the random selection of incontext shots and achieves an accuracy improvement of 2.5\%. When the number of in-context shots is increased to 5 , the model with random selection does not gain further benefits. Our PromptPG displays impressive advantages over random selection in terms of data efficiency and prediction accuracy. With only two in-context shots, PromptPG achieves the highest accuracy of 70.9\% and a comparable low deviation compared to random selection with more shots.</p>
<h1>A. 6 Related work of Policy Gradient</h1>
<p>Policy gradient is an approach to solving reinforcement learning problems that target modeling and optimizing the policy directly. Many policy gradient algorithms have been proposed in the past decade (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2016; Schulman et al., 2017; BarthMaron et al., 2018). They have been proven effective in areas like robotics (Peters \&amp; Schaal, 2006) and chatbots (Kandasamy et al., 2017). In recent work that focuses on aligning language models with human values (Ouyang et al., 2022; Qiu et al., 2022; Glaese et al., 2022), policy gradient has been used to optimize language models with rewards learned from human feedback and preference. To the best of our knowledge, our PromptPG is the first work that proposes to select prompts dynamically for large pre-trained language models in the mathematical reasoning field.</p>
<h2>A. 7 CASE STUDY EXAMPLES</h2>
<p>$\triangleright$ In-context example 1 (ID: 28463)</p>
<h1>Table:</h1>
<p>Option | Change in phone price
Add an upgrade | \$60
Buy a used phone | -\$75
Option
Add an upgrade
Buy a used phone</p>
<h2>Change in phone price</h2>
<p>\$60
-\$75</p>
<p>Question: Luna is looking at the price of new cell phones online. Her favorite company, OrangeTech, has a special this weekend. Luna can add an upgrade to a phone for an additional cost, or she can buy a used phone to get a discount. The change in price for each option is shown in the table. Which option results in a greater change in price?
Options: (A) adding an upgrade (B) buying a used phone Answer:
(Step 1) To find the option that results in a greater change in price, use absolute value. Absolute value tells you how much the price changes.
(Step 2) Add an upgrade: $|\$ 60|=\$ 60$
(Step 3) Buy a used phone: $|\cdot \$ 75|=\$ 75$
(Step 4) Buying a used phone results in a greater change in price. It reduces the price by $\$ 75$. The answer is buying a used phone.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\triangleright$ In-context example 2 (ID: 13974)</th>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Table:</td>
<td style="text-align: left;">heart-shaped beads</td>
<td style="text-align: center;">$\$ 3 /$ kilogram</td>
</tr>
<tr>
<td style="text-align: left;">heart-shaped beads</td>
<td style="text-align: left;">$\$ 3 /$ kilogram</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">rectangular beads | \$2/kilogram</td>
<td style="text-align: left;">rectangular beads</td>
<td style="text-align: center;">$\$ 2 /$ kilogram</td>
</tr>
<tr>
<td style="text-align: left;">spherical beads | \$2/kilogram</td>
<td style="text-align: left;">spherical beads</td>
<td style="text-align: center;">$\$ 2 /$ kilogram</td>
</tr>
<tr>
<td style="text-align: left;">oval beads | \$2/kilogram</td>
<td style="text-align: left;">oval beads</td>
<td style="text-align: center;">$\$ 2 /$ kilogram</td>
</tr>
</tbody>
</table>
<p>Question: Rebecca bought 2.5 kilograms of oval beads. How much did she spend? (Unit: \$)</p>
<h2>Answer:</h2>
<p>(Step 1) Find the cost of the oval beads. Multiply the price per kilogram by the number of kilograms.
(Step 2) $\$ 2 \times 2.5=\$ 5$
(Step 3) She spent $\$ 5$. The answer is 5 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\triangleright$ Test example (ID: 17417)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Table:</td>
<td style="text-align: center;">Birthday party</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">[TITLE]: Birthday party</td>
<td style="text-align: center;">Activity</td>
<td style="text-align: center;">Parents</td>
<td style="text-align: center;">Children</td>
</tr>
<tr>
<td style="text-align: left;">Activity</td>
<td style="text-align: center;">Parents | Children</td>
<td style="text-align: center;">Singing</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: left;">Singing | 14</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">Eating cake</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Eating cake | 5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Jumping rope</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Jumping rope | 16</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">Swimming</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Swimming | 16</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">Playing tag</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Playing tag | 4</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Question: At Josie's birthday party, children and parents celebrated by participating in some activities. How many more children than parents are playing tag? (Unit: children)</p>
<h2>Answer:</h2>
<p>(Step 1) To find the difference between the number of children and parents playing tag, subtract the number of parents from the number of children.
(Step 2) $9-4=5$
(Step 3) There are 5 more children than parents playing tag. The answer is 5.
Output: 5
Figure 5: Two in-context examples selected by PromptPG, the prompt, and the correct prediction. The selected examples require similar abilities of mathematical reasoning to the test example.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\triangleright$ In-context example 1 (ID: 18429)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Table:</td>
<td style="text-align: center;">Children's weights (lbs)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TITLE: Children's weights (lbs)</td>
<td style="text-align: center;">Stem</td>
<td style="text-align: center;">Leaf</td>
</tr>
<tr>
<td style="text-align: center;">Stem | Leaf</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">$1 \mid 7$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">$2 \mid 4$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$3 \mid$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$4 \mid$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$2,2,8$</td>
</tr>
<tr>
<td style="text-align: center;">$5 \mid 2,2,8$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">$6 \mid 6$</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1,3</td>
</tr>
<tr>
<td style="text-align: center;">$7 \mid 1,3$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">7,8</td>
</tr>
<tr>
<td style="text-align: center;">$8 \mid 7,8$</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$9 \mid 0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Question: Dr. Thornton, a pediatrician, weighed all the children who recently visited her office. How many children weighed exactly 52 pounds? (Unit: children)</p>
<h1>Answer:</h1>
<p>(Step 1) For the number 52, the stem is 5, and the leaf is 2. Find the row where the stem is 5. In that row, count all the leaves equal to 2 .
(Step 2) You counted 2 leaves, which are blue in the stem-and-leaf plot above.
(Step 3) 2 children weighed exactly 52 pounds. The answer is 2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\triangleright$ In-context example 2 (ID: 18129)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Table: <br> whole wheat pancake mix $\mid \$ 1 / l b$ buttermilk pancake mix $\mid \$ 2 / l b$ chocolate chip pancake mix $\mid \$ 2 / l b$ blueberry pancake mix $\mid \$ 1 / l b$ plain pancake mix $\mid \$ 2 / l b$ raspberry pancake mix $\mid \$ 2 / l b$</td>
<td style="text-align: center;">whole wheat pancake mix</td>
<td style="text-align: center;">\$1 per lb</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">buttermilk pancake mix</td>
<td style="text-align: center;">\$2 per lb</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">chocolate chip pancake mix</td>
<td style="text-align: center;">\$2 per lb</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">blueberry pancake mix</td>
<td style="text-align: center;">\$1 per lb</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">plain pancake mix</td>
<td style="text-align: center;">\$2 per lb</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">raspberry pancake mix</td>
<td style="text-align: center;">\$2 per lb</td>
</tr>
</tbody>
</table>
<p>Question: If Vince buys 3.4 pounds of buttermilk pancake mix, how much will he spend? (Unit: \$)</p>
<h2>Answer:</h2>
<p>(Step 1) Find the cost of the buttermilk pancake mix. Multiply the price per pound by the number of pounds. (Step 2) $\$ 2 \times 3.4=\$ 6.80$
(Step 3) He will spend $\$ 6.80$. The answer is 6.80 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\triangleright$ Test example (ID: 17417)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Table:</td>
<td style="text-align: center;">Birthday party</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TITLE: Birthday party</td>
<td style="text-align: center;">Activity</td>
<td style="text-align: center;">Parents</td>
<td style="text-align: center;">Children</td>
</tr>
<tr>
<td style="text-align: center;">Activity | Parents</td>
<td style="text-align: center;">Children</td>
<td style="text-align: center;">Singing</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">Singing | $14 \mid 20$</td>
<td style="text-align: center;">Eating cake | 5 | 10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Eating cake | $16 \mid 20$</td>
<td style="text-align: center;">Jumping rope | $16 \mid 20$</td>
<td style="text-align: center;">Jumping rope</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Swimming | $16 \mid 19$</td>
<td style="text-align: center;">Swimming</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">19</td>
</tr>
<tr>
<td style="text-align: center;">Playing tag | $4 \mid 9$</td>
<td style="text-align: center;">Playing tag</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">9</td>
</tr>
</tbody>
</table>
<p>Question: At Josie's birthday party, children and parents celebrated by participating in some activities. How many more children than parents are playing tag? (Unit: children)</p>
<h2>Answer:</h2>
<p>(Step 1) For the activity "Playing tag", the stem is 4, and the leaf is 9. Find the row where the stem is 4. In that row, count all the leaves equal to 9 .
(Step 2) You counted 9 leaves, which are blue in the stem-and-leaf plot above. 9 children are playing tag. The answer is 9 .
Output: 9
Ground truth: 5
Figure 6: Two in-context examples selected by nearest neighbor search, the prompt, and the final wrong prediction. The selected examples are only semantically similar to the test example.</p>
<p>$\triangleright$ In-context example 1 (ID: 13033)
Table:
[TITLE]: Watermelons harvested
Day | Number of watermelons
Wednesday | 59
Thursday | 51
Friday | 53
Saturday | 52
Question: A farmer recalled how many watermelons were harvested in the past 4 days. On which day were the most watermelons harvested?
Options: (A) Wednesday (B) Thursday (C) Friday (D) Saturday
Answer:
(Step 1) Find the greatest number in the table. Remember to compare the numbers starting with the highest place value. The greatest number is 59 .
(Step 2) Now find the corresponding day. Wednesday corresponds to 59. The answer is Wednesday.
$\triangleright$ In-context example 2 (ID: 32386)
Table:
[TITLE]: Basketball hoops
Park | Number of basketball hoops
Heron Park | 2
Kelly Park | 7
Westfield Park | 4
Pinehurst Park | 4
Linden Park | 3
Mooreland Park | 7
Crestview Park | 2
Riverfront Park | 4
Question: The parks department compared how many basketball hoops there are at each park. What is the mode of the numbers?
Answer:
(Step 1) Read the numbers from the table.
(Step 2) First, arrange the numbers from least to greatest: $2,2,3,4,4,4,7,7$.
(Step 3) Now count how many times each number appears.
(Step 4) 2 appears 2 times. 3 appears 1 time. 4 appears 3 times. 7 appears 2 times.
(Step 5) The number that appears most often is 4 .
(Step 6) The mode is 4 . The answer is 4 .
$\triangleright$ Text example (ID: 17417)
Table:
[TITLE]: Birthday party
Activity | Parents | Children
Singing | 14 | 20
Eating cake | 5 | 10
Jumping rope | 16 | 20
Swimming | 16 | 19
Playing tag | 4 | 9
| Birthday party |
| :-- | :-- |
| Activity | Parents | Children |
| Singing | 14 | 20 |
| Eating cake | 5 | 10 |
| Jumping rope | 16 | 20 |
| Swimming | 16 | 19 |
| Playing tag | 4 | 9 |</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Birthday party</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Activity</td>
<td style="text-align: right;">Parents</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Singing</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Eating cake</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Jumping rope</td>
<td style="text-align: right;">16</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Swimming</td>
<td style="text-align: right;">16</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Playing tag</td>
<td style="text-align: right;">4</td>
</tr>
</tbody>
</table>
<p>Question: At Josie's birthday party, children and parents celebrated by participating in some activities. How many more children than parents are playing tag? (Unit: children)
Answer:
(Step 1) Find the difference between the number of children and the number of parents for each activity.
(Step 2) For singing, the difference is 6 .
(Step 3) For eating cake, the difference is 5 .
(Step 4) For jumping rope, the difference is 4 .
(Step 5) For swimming, the difference is 3 .
(Step 6) For playing tag, the difference is 5 .
(Step 7) The greatest difference is 6 . The answer is 6 .
Output: 6
Ground truth: 5
Figure 7: Two randomly selected in-context examples, the prompt, and the final prediction. The selected examples have limited relevance to the test example, resulting in the wrong output.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Table:</th>
<th style="text-align: center;">Math teachers</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">[TITLE] Math teachers</td>
<td style="text-align: center;">High school</td>
<td style="text-align: center;">Number of math teachers</td>
</tr>
<tr>
<td style="text-align: left;">High school | Number of math teachers</td>
<td style="text-align: center;">Central High</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: left;">Central High | 9</td>
<td style="text-align: center;">Hillview High</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">Hillview High | 10</td>
<td style="text-align: center;">Westside High</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Westside High | 4</td>
<td style="text-align: center;">Moore High</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Moore High | 2</td>
<td style="text-align: center;">River High</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">River High | 6</td>
<td style="text-align: center;">Northside High</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Northside High | 4</td>
<td style="text-align: center;">Lincoln High</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Lincoln High | 8</td>
<td style="text-align: center;">Thompson High</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Thompson High | 5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Question: The school district compared how many math teachers each high school has. What is the mean of the numbers?
Answer:
(Step 1) The mean is the average of the numbers.
(Step 2) To find the mean, add all the numbers and divide by the number of items.
(Step 3) $9+10+4+2+6+4+8+5=48$
(Step 4) There are 8 items, so the mean is $48 \div 8=6$.
(Step 5) The mean of the numbers is 6 .
Output: 6
Figure 8: The correct prediction from our PromptPG for a free-text question example. This example requires taking the mean of eight numbers from the table via addition and division.</p>
<p>Table:
topaz $\mid \$ 18.55$ per lb
amethyst $\mid \$ 19.88$ per lb
tiger's eye $\mid \$ 10.29$ per lb
fool's gold $\mid \$ 16.00$ per lb
quartz $\mid \$ 14.63$ per lb
calcite $\mid \$ 15.39$ per lb
granite $\mid \$ 19.23$ per lb
| topaz | $\$ 18.55$ per lb |
| :-- | :-- |
| amethyst | $\$ 19.88$ per lb |
| tiger's eye | $\$ 10.29$ per lb |
| fool's gold | $\$ 16.00$ per lb |
| quartz | $\$ 14.63$ per lb |
| calcite | $\$ 15.39$ per lb |
| granite | $\$ 19.23$ per lb |</p>
<p>Question: Austin went to the store and bought 3 pounds of fool's gold, 1 pound of topaz, and 2 pounds of tiger's eye. How much did he spend? (unit: \$)</p>
<h1>Answer:</h1>
<p>(Step 1) To find the total amount Austin spent, multiply the price per pound of each type of stone by the number of pounds of that stone that he bought, and then add all of the products together.
(Step 2) 3 pounds of fool's gold: $\$ 16.00$ per pound $\times 3$ pounds $=\$ 48.00$
(Step 3) 1 pound of topaz: $\$ 18.55$ per pound $\times 1$ pound $=\$ 18.55$
(Step 4) 2 pounds of tiger's eye: $\$ 10.29$ per pound $\times 2$ pounds $=\$ 20.58$
(Step 5) $\$ 48.00+\$ 18.55+\$ 20.58=\$ 87.13$
(Step 6) Austin spent $\$ 87.13$.
Output: 87.13
Figure 9: The correct prediction from our PromptPG for a free-text question example. This example requires looking up three items and their corresponding prices in the table, calculating their costs, and finally summing them up to get the final answer.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.ixl.com/math&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
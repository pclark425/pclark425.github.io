<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8762 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8762</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8762</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271097348</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.08223v2.pdf" target="_blank">Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting</a></p>
                <p><strong>Paper Abstract:</strong> Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs. In this work, we introduce Speculative RAG - a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long context. Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. Extensive experiments demonstrate that Speculative RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PopQA, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy by up to 12.97% while reducing latency by 50.83% compared to conventional RAG systems on PubHealth.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8762.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8762.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPECULATIVE RAG (verifier self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPECULATIVE Retrieval-Augmented Generation — verifier self-reflection scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's RAG framework generates multiple answer drafts with a small instruction-tuned 'drafter' LM and then uses an off-the-shelf larger 'verifier' LM to score drafts; the verifier uses both a self-consistency probability and a prompted yes/no self-reflection probability to pick the final answer in a single verification pass.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>M Drafter-7B (instruction-tuned) + M Verifier-7B or M Verifier-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Drafter: Mistral 7B (v0.1) instruction-tuned as a specialist RAG drafter to generate answer drafts and concise rationales. Verifier: off-the-shelf generalist LM (Mistral 7B or Mixtral 8x7B) used frozen (no fine-tuning) to score/verify drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Verifier self-reflection (prompted Yes/No) + self-consistency scoring</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each draft α and rationale β generated by the drafter, the verifier computes (1) a self-consistency score ρ_Self-contain = P(α,β | Q) (product of token probs for α and β given Q) and (2) a self-reflection score ρ_Self-reflect = P("Yes" | Q, α, β, R) where R is a short self-reflection prompt (e.g., "Do you think the explanation supports the answers? (Yes or No)"). These are multiplied with the drafter's generation confidence ρ_Draft to form ρ_j = ρ_Draft • ρ_Self-contain • ρ_Self-reflect, and the highest-scoring draft is selected. The verifier scoring is computed in one forward pass (no iterative generate-reflect cycles).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain and closed-set retrieval-augmented QA benchmarks (TriviaQA, MuSiQue, PopQA, PubHealth, ARC-Challenge; HotpotQA in multi-hop eval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-intensive question answering and claim verification: free-form and closed-set QA where models must use retrieved documents to generate or select correct answers (evaluated mainly by accuracy / exact match-containment for free-form).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>M Verifier-8x7B + M Drafter-7B reported accuracies: TriviaQA 74.24% ; MuSiQue 31.57% ; PopQA 57.54% ; PubHealth 76.60% ; ARC-Challenge 80.55% (accuracy % from Table 1). The paper also reports up to +12.97% absolute improvement on PubHealth compared to a strong standard RAG baseline (Mixtral-Instruct 8x7B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Ablation: removing the verifier self-reflection score (ρ_Self-reflect) caused ~2.0% absolute drop on TriviaQA and ~0.8% absolute drop on PubHealth relative to the full SPECULATIVE RAG configuration; the paper does not provide per-dataset numbers for all benchmarks without reflection (approx: TriviaQA ≈ 72.2% when removing ρ_Self-reflect, PubHealth ≈ 75.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered self-reflection question (Yes/No) fed into the frozen generalist LM; aggregation of autoregressive token probabilities (language-model probabilities) to compute both self-containment and self-reflection scores; combined multiplicatively with drafter generation probabilities. Implementation requires rationale generation by the drafter but no special fine-tuning of the verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Full SPECULATIVE RAG (with verifier self-reflection and self-consistency) outperforms baselines across datasets (see Table 1). Ablations show removing ρ_Self-reflect reduces accuracy (~2.0% on TriviaQA, ~0.8% on PubHealth). Removing verification entirely (random selection) yields much larger drops (−5.69% TriviaQA, −5.37% PubHealth), demonstrating verification+reflection materially improves final answer selection. SPECULATIVE RAG also reduces latency while improving accuracy vs standard RAG baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations: (1) Rationale generation increases inference cost during drafting (though mitigated via parallelism). (2) Multi-perspective sampling and the overall framework are less helpful for HotpotQA 'bridge-type' questions that require identifying an intermediate 'bridge entity' rather than contrasting multiple perspectives. (3) The verifier is single-pass (no iterative reflection cycles) so potential iterative improvement behaviors are not explored; (4) reliance on instruction-tuned drafter (requires curated data and training). The paper also notes self-reflection prompt variants are robust (performance stable across prompt variants).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Self-Reflective RAG and Corrective RAG baselines: SPECULATIVE RAG (M Verifier-8x7B + M Drafter-7B) outperforms Mixtral-Instruct 8x7B by small margins on several datasets and substantially on PubHealth (+12.97%). SPECULATIVE RAG achieves better accuracy while reducing latency vs standard RAG; ablations show the verifier's self-reflection/self-consistency scoring contributes meaningfully to gains compared to using only drafter probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations reported: removing ρ_Draft yields minimal decline (≈0.19% TriviaQA, ≈1.12% PubHealth); removing ρ_Self-contain or ρ_Self-reflect each causes similar drops (~2.0% TriviaQA, ~0.8% PubHealth). Random selection (no verification) decreases performance substantially (−5.69% TriviaQA, −5.37% PubHealth). Different self-reflection prompt phrasings produce similar accuracy (Table 7). Finetuning the drafter without rationale causes significant performance drop (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8762.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8762.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Reflective RAG (Asai et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior RAG approach that instruction-tunes a general-purpose LM to emit special self-reflection tags that guide retrieval and critique of evidence before producing answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Reflective RAG</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (instruction-tuned Self-RAG reproduction in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned variant of a 7B LM (Mistral-Instruct 7B style) trained to generate self-reflection tags and to dynamically manage retrieval/retrieval annotation in instruction-tuning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-reflection tags / reflection-token generation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model is instruction-tuned to produce special reflection tokens that guide retrieval and critique the relevance of retrieved documents prior to generating final answers. This requires additional instruction-tuning of the LM itself.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same RAG QA benchmarks used in this paper (TriviaQA, MuSiQue, PopQA, PubHealth, ARC-Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-intensive QA where models retrieve supporting documents and must judge/critique evidence relevance before answering.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reproduced numbers in this paper for Self-RAG (Mistral-7B): TriviaQA 64.84% ; MuSiQue 21.72% ; PopQA 52.68% ; PubHealth 72.44% ; ARC-Challenge 74.91% (accuracy % as reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not directly reported in this paper for the exact Self-RAG ablation, but the authors note instruction-tuning Self-RAG requires additional tuning resources and may impact generality; standard RAG baselines (different LMs) show varying results (e.g., standard Mistral-Instruct 7B had TriviaQA 67.11% in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Instruction-tuning the LM to output explicit self-reflection tokens/tags that alter retrieval/annotation behaviour and evidence selection; this is baked into the generator rather than using a separate verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Prior work claimed benefits from explicit self-reflection training; in this paper Self-RAG is used as a competitive baseline, but its numeric performance relative to SPECULATIVE RAG is mixed (SPECULATIVE RAG outperforms Self-RAG on the reported benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported (in this paper's related work): requires additional instruction-tuning of generic LMs which is resource-intensive and can cause catastrophic forgetting or overfitting; Self-RAG also results in higher latency in practice due to extra generated self-reflection tokens and evidence-selection overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared as a baseline: SPECULATIVE RAG (which separates drafting and verification) outperforms Self-RAG while avoiding extra instruction-tuning of the verifier. Self-RAG implements self-reflection inside the generator; SPECULATIVE RAG externalizes verification.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8762.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8762.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Corrective RAG (CRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Corrective Retrieval-Augmented Generation (Yan et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RAG variant that adds a lightweight external retrieval evaluator to assess and refine retrieved documents before answer generation (focuses on retrieval quality rather than generator self-critique).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Corrective RAG</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CRAG reproduced with Mistral-7B (as reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corrective RAG uses a lightweight retrieval evaluator module combined with a generator LM (here reproduced with Mistral-7B); the evaluator is external to the generator and is not intended to perform high-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>External retrieval evaluator (not self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A separate, lightweight module evaluates the quality/relevance of retrieved documents and refines the retrieval input passed to the generator; it does not perform answer-level self-critique or iterative answer improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RAG QA benchmarks (TriviaQA, PopQA, PubHealth, ARC-Challenge) as used in paper comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Refinement of retrieved context to improve generator outputs for knowledge-intensive QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reproduced CRAG numbers in this paper for Mistral-7B: TriviaQA 59.03% ; PopQA 49.46% ; PubHealth 59.04% ; ARC-Challenge 74.87% (MuSiQue not reported in table excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not enumerated as an ablation here; standard RAG (Mistral 7B) baseline numbers in Table 1 are TriviaQA 54.15% ; PopQA 31.38% ; PubHealth 34.85% ; ARC-Challenge 42.75% — CRAG improves some metrics vs those particular baselines in the reproduced table.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External evaluator module that scores/refines retrieved documents prior to generation (retrieval-level correction rather than answer-level reflection).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Reported improvements over certain standard RAG baselines in the reproduced table (varies by dataset), but the paper notes CRAG lacks high-level reasoning capability compared to approaches that verify answer candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes CRAG's evaluator lacks capacity for high-level reasoning and therefore may not sufficiently improve final answers where reasoning over evidence is required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>CRAG focuses on retrieval refinement (pre-generation), whereas SPECULATIVE RAG focuses on draft generation by specialists and answer-level verification by a generalist verifier; SPECULATIVE RAG outperforms CRAG on the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8762.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8762.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General iterative / self-critique mentions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative LLM refinement / self-critique (generic mentions in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references a body of work that uses iterative LLM refinement or self-critique (generate-then-reflect, self-critique, etc.) often implemented via instruction-tuning to improve retrieval or generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LMs (literature-level mention; e.g., instruction-tuned LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generic references to LMs that have been instruction-tuned to perform iterative refinement or self-critique; sizes and exact training vary by cited prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative LLM refinement / self-critique (literature)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Described generically in the intro/related work as approaches that perform multiple retrieval-generation-reflection iterations or are instruction-tuned to internally self-critique and guide retrieval (e.g., Self-RAG). The paper contrasts these with SPECULATIVE RAG's single-pass verifier over multiple drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General (various RAG and retrieval-augmented tasks) as cited in related work</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-intensive QA tasks where iterative refinement has been applied in prior literature to improve evidence incorporation and reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not quantified specifically in this paper (general literature statement). Paper notes existing iterative/self-critique approaches may improve retrieval outcomes but typically require extra instruction-tuning or increased latency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported (literature-level discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Typically instruction-tuning to produce critique/reflection outputs or multi-step generate-and-retrieve loops (varies by cited method).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites that recent RAG advancements focus on iterative LLM refinement or self-critique, but emphasizes such methods often need extra instruction-tuning and can increase latency; SPECULATIVE RAG is presented as an alternative that avoids extra verifier tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>General critique in paper: iterative/self-critique methods often increase latency and require extra instruction-tuning, which can cause forgetting/overfitting and be impractical in many deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Reflective RAG <em>(Rating: 2)</em></li>
                <li>Corrective retrieval augmented generation <em>(Rating: 2)</em></li>
                <li>Fast inference from transformers via speculative decoding <em>(Rating: 2)</em></li>
                <li>Draft & verify: Lossless large language model acceleration via self-speculative decoding <em>(Rating: 1)</em></li>
                <li>Generate-then-ground in retrieval-augmented generation for multi-hop question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8762",
    "paper_id": "paper-271097348",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "SPECULATIVE RAG (verifier self-reflection)",
            "name_full": "SPECULATIVE Retrieval-Augmented Generation — verifier self-reflection scoring",
            "brief_description": "This paper's RAG framework generates multiple answer drafts with a small instruction-tuned 'drafter' LM and then uses an off-the-shelf larger 'verifier' LM to score drafts; the verifier uses both a self-consistency probability and a prompted yes/no self-reflection probability to pick the final answer in a single verification pass.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "M Drafter-7B (instruction-tuned) + M Verifier-7B or M Verifier-8x7B",
            "model_description": "Drafter: Mistral 7B (v0.1) instruction-tuned as a specialist RAG drafter to generate answer drafts and concise rationales. Verifier: off-the-shelf generalist LM (Mistral 7B or Mixtral 8x7B) used frozen (no fine-tuning) to score/verify drafts.",
            "reflection_method_name": "Verifier self-reflection (prompted Yes/No) + self-consistency scoring",
            "reflection_method_description": "For each draft α and rationale β generated by the drafter, the verifier computes (1) a self-consistency score ρ_Self-contain = P(α,β | Q) (product of token probs for α and β given Q) and (2) a self-reflection score ρ_Self-reflect = P(\"Yes\" | Q, α, β, R) where R is a short self-reflection prompt (e.g., \"Do you think the explanation supports the answers? (Yes or No)\"). These are multiplied with the drafter's generation confidence ρ_Draft to form ρ_j = ρ_Draft • ρ_Self-contain • ρ_Self-reflect, and the highest-scoring draft is selected. The verifier scoring is computed in one forward pass (no iterative generate-reflect cycles).",
            "task_name": "Open-domain and closed-set retrieval-augmented QA benchmarks (TriviaQA, MuSiQue, PopQA, PubHealth, ARC-Challenge; HotpotQA in multi-hop eval)",
            "task_description": "Knowledge-intensive question answering and claim verification: free-form and closed-set QA where models must use retrieved documents to generate or select correct answers (evaluated mainly by accuracy / exact match-containment for free-form).",
            "performance_with_reflection": "M Verifier-8x7B + M Drafter-7B reported accuracies: TriviaQA 74.24% ; MuSiQue 31.57% ; PopQA 57.54% ; PubHealth 76.60% ; ARC-Challenge 80.55% (accuracy % from Table 1). The paper also reports up to +12.97% absolute improvement on PubHealth compared to a strong standard RAG baseline (Mixtral-Instruct 8x7B).",
            "performance_without_reflection": "Ablation: removing the verifier self-reflection score (ρ_Self-reflect) caused ~2.0% absolute drop on TriviaQA and ~0.8% absolute drop on PubHealth relative to the full SPECULATIVE RAG configuration; the paper does not provide per-dataset numbers for all benchmarks without reflection (approx: TriviaQA ≈ 72.2% when removing ρ_Self-reflect, PubHealth ≈ 75.8%).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered self-reflection question (Yes/No) fed into the frozen generalist LM; aggregation of autoregressive token probabilities (language-model probabilities) to compute both self-containment and self-reflection scores; combined multiplicatively with drafter generation probabilities. Implementation requires rationale generation by the drafter but no special fine-tuning of the verifier.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: Full SPECULATIVE RAG (with verifier self-reflection and self-consistency) outperforms baselines across datasets (see Table 1). Ablations show removing ρ_Self-reflect reduces accuracy (~2.0% on TriviaQA, ~0.8% on PubHealth). Removing verification entirely (random selection) yields much larger drops (−5.69% TriviaQA, −5.37% PubHealth), demonstrating verification+reflection materially improves final answer selection. SPECULATIVE RAG also reduces latency while improving accuracy vs standard RAG baselines.",
            "limitations_or_failure_cases": "Reported limitations: (1) Rationale generation increases inference cost during drafting (though mitigated via parallelism). (2) Multi-perspective sampling and the overall framework are less helpful for HotpotQA 'bridge-type' questions that require identifying an intermediate 'bridge entity' rather than contrasting multiple perspectives. (3) The verifier is single-pass (no iterative reflection cycles) so potential iterative improvement behaviors are not explored; (4) reliance on instruction-tuned drafter (requires curated data and training). The paper also notes self-reflection prompt variants are robust (performance stable across prompt variants).",
            "comparison_to_other_methods": "Compared to Self-Reflective RAG and Corrective RAG baselines: SPECULATIVE RAG (M Verifier-8x7B + M Drafter-7B) outperforms Mixtral-Instruct 8x7B by small margins on several datasets and substantially on PubHealth (+12.97%). SPECULATIVE RAG achieves better accuracy while reducing latency vs standard RAG; ablations show the verifier's self-reflection/self-consistency scoring contributes meaningfully to gains compared to using only drafter probabilities.",
            "ablation_study_results": "Ablations reported: removing ρ_Draft yields minimal decline (≈0.19% TriviaQA, ≈1.12% PubHealth); removing ρ_Self-contain or ρ_Self-reflect each causes similar drops (~2.0% TriviaQA, ~0.8% PubHealth). Random selection (no verification) decreases performance substantially (−5.69% TriviaQA, −5.37% PubHealth). Different self-reflection prompt phrasings produce similar accuracy (Table 7). Finetuning the drafter without rationale causes significant performance drop (Table 8).",
            "uuid": "e8762.0",
            "source_info": {
                "paper_title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-RAG",
            "name_full": "Self-Reflective RAG (Asai et al., 2023)",
            "brief_description": "A prior RAG approach that instruction-tunes a general-purpose LM to emit special self-reflection tags that guide retrieval and critique of evidence before producing answers.",
            "citation_title": "Self-Reflective RAG",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (instruction-tuned Self-RAG reproduction in this paper)",
            "model_description": "An instruction-tuned variant of a 7B LM (Mistral-Instruct 7B style) trained to generate self-reflection tags and to dynamically manage retrieval/retrieval annotation in instruction-tuning datasets.",
            "reflection_method_name": "Self-reflection tags / reflection-token generation",
            "reflection_method_description": "The model is instruction-tuned to produce special reflection tokens that guide retrieval and critique the relevance of retrieved documents prior to generating final answers. This requires additional instruction-tuning of the LM itself.",
            "task_name": "Same RAG QA benchmarks used in this paper (TriviaQA, MuSiQue, PopQA, PubHealth, ARC-Challenge)",
            "task_description": "Knowledge-intensive QA where models retrieve supporting documents and must judge/critique evidence relevance before answering.",
            "performance_with_reflection": "Reproduced numbers in this paper for Self-RAG (Mistral-7B): TriviaQA 64.84% ; MuSiQue 21.72% ; PopQA 52.68% ; PubHealth 72.44% ; ARC-Challenge 74.91% (accuracy % as reported in Table 1).",
            "performance_without_reflection": "Not directly reported in this paper for the exact Self-RAG ablation, but the authors note instruction-tuning Self-RAG requires additional tuning resources and may impact generality; standard RAG baselines (different LMs) show varying results (e.g., standard Mistral-Instruct 7B had TriviaQA 67.11% in Table 1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Instruction-tuning the LM to output explicit self-reflection tokens/tags that alter retrieval/annotation behaviour and evidence selection; this is baked into the generator rather than using a separate verifier.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Prior work claimed benefits from explicit self-reflection training; in this paper Self-RAG is used as a competitive baseline, but its numeric performance relative to SPECULATIVE RAG is mixed (SPECULATIVE RAG outperforms Self-RAG on the reported benchmarks).",
            "limitations_or_failure_cases": "Reported (in this paper's related work): requires additional instruction-tuning of generic LMs which is resource-intensive and can cause catastrophic forgetting or overfitting; Self-RAG also results in higher latency in practice due to extra generated self-reflection tokens and evidence-selection overhead.",
            "comparison_to_other_methods": "Compared as a baseline: SPECULATIVE RAG (which separates drafting and verification) outperforms Self-RAG while avoiding extra instruction-tuning of the verifier. Self-RAG implements self-reflection inside the generator; SPECULATIVE RAG externalizes verification.",
            "ablation_study_results": null,
            "uuid": "e8762.1",
            "source_info": {
                "paper_title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Corrective RAG (CRAG)",
            "name_full": "Corrective Retrieval-Augmented Generation (Yan et al., 2024)",
            "brief_description": "A RAG variant that adds a lightweight external retrieval evaluator to assess and refine retrieved documents before answer generation (focuses on retrieval quality rather than generator self-critique).",
            "citation_title": "Corrective RAG",
            "mention_or_use": "use",
            "model_name": "CRAG reproduced with Mistral-7B (as reported in this paper)",
            "model_description": "Corrective RAG uses a lightweight retrieval evaluator module combined with a generator LM (here reproduced with Mistral-7B); the evaluator is external to the generator and is not intended to perform high-level reasoning.",
            "reflection_method_name": "External retrieval evaluator (not self-reflection)",
            "reflection_method_description": "A separate, lightweight module evaluates the quality/relevance of retrieved documents and refines the retrieval input passed to the generator; it does not perform answer-level self-critique or iterative answer improvement.",
            "task_name": "RAG QA benchmarks (TriviaQA, PopQA, PubHealth, ARC-Challenge) as used in paper comparisons",
            "task_description": "Refinement of retrieved context to improve generator outputs for knowledge-intensive QA tasks.",
            "performance_with_reflection": "Reproduced CRAG numbers in this paper for Mistral-7B: TriviaQA 59.03% ; PopQA 49.46% ; PubHealth 59.04% ; ARC-Challenge 74.87% (MuSiQue not reported in table excerpt).",
            "performance_without_reflection": "Not enumerated as an ablation here; standard RAG (Mistral 7B) baseline numbers in Table 1 are TriviaQA 54.15% ; PopQA 31.38% ; PubHealth 34.85% ; ARC-Challenge 42.75% — CRAG improves some metrics vs those particular baselines in the reproduced table.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External evaluator module that scores/refines retrieved documents prior to generation (retrieval-level correction rather than answer-level reflection).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Reported improvements over certain standard RAG baselines in the reproduced table (varies by dataset), but the paper notes CRAG lacks high-level reasoning capability compared to approaches that verify answer candidates.",
            "limitations_or_failure_cases": "Paper notes CRAG's evaluator lacks capacity for high-level reasoning and therefore may not sufficiently improve final answers where reasoning over evidence is required.",
            "comparison_to_other_methods": "CRAG focuses on retrieval refinement (pre-generation), whereas SPECULATIVE RAG focuses on draft generation by specialists and answer-level verification by a generalist verifier; SPECULATIVE RAG outperforms CRAG on the reported benchmarks.",
            "ablation_study_results": null,
            "uuid": "e8762.2",
            "source_info": {
                "paper_title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "General iterative / self-critique mentions",
            "name_full": "Iterative LLM refinement / self-critique (generic mentions in paper)",
            "brief_description": "The paper references a body of work that uses iterative LLM refinement or self-critique (generate-then-reflect, self-critique, etc.) often implemented via instruction-tuning to improve retrieval or generation quality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various LMs (literature-level mention; e.g., instruction-tuned LMs)",
            "model_description": "Generic references to LMs that have been instruction-tuned to perform iterative refinement or self-critique; sizes and exact training vary by cited prior work.",
            "reflection_method_name": "Iterative LLM refinement / self-critique (literature)",
            "reflection_method_description": "Described generically in the intro/related work as approaches that perform multiple retrieval-generation-reflection iterations or are instruction-tuned to internally self-critique and guide retrieval (e.g., Self-RAG). The paper contrasts these with SPECULATIVE RAG's single-pass verifier over multiple drafts.",
            "task_name": "General (various RAG and retrieval-augmented tasks) as cited in related work",
            "task_description": "Knowledge-intensive QA tasks where iterative refinement has been applied in prior literature to improve evidence incorporation and reduce hallucination.",
            "performance_with_reflection": "Not quantified specifically in this paper (general literature statement). Paper notes existing iterative/self-critique approaches may improve retrieval outcomes but typically require extra instruction-tuning or increased latency.",
            "performance_without_reflection": "Not reported (literature-level discussion).",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Typically instruction-tuning to produce critique/reflection outputs or multi-step generate-and-retrieve loops (varies by cited method).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites that recent RAG advancements focus on iterative LLM refinement or self-critique, but emphasizes such methods often need extra instruction-tuning and can increase latency; SPECULATIVE RAG is presented as an alternative that avoids extra verifier tuning.",
            "limitations_or_failure_cases": "General critique in paper: iterative/self-critique methods often increase latency and require extra instruction-tuning, which can cause forgetting/overfitting and be impractical in many deployments.",
            "uuid": "e8762.3",
            "source_info": {
                "paper_title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Reflective RAG",
            "rating": 2,
            "sanitized_title": "selfreflective_rag"
        },
        {
            "paper_title": "Corrective retrieval augmented generation",
            "rating": 2,
            "sanitized_title": "corrective_retrieval_augmented_generation"
        },
        {
            "paper_title": "Fast inference from transformers via speculative decoding",
            "rating": 2,
            "sanitized_title": "fast_inference_from_transformers_via_speculative_decoding"
        },
        {
            "paper_title": "Draft & verify: Lossless large language model acceleration via self-speculative decoding",
            "rating": 1,
            "sanitized_title": "draft_verify_lossless_large_language_model_acceleration_via_selfspeculative_decoding"
        },
        {
            "paper_title": "Generate-then-ground in retrieval-augmented generation for multi-hop question answering",
            "rating": 1,
            "sanitized_title": "generatethenground_in_retrievalaugmented_generation_for_multihop_question_answering"
        }
    ],
    "cost": 0.01580345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SPECULATIVE RAG: ENHANCING RETRIEVAL AUG-MENTED GENERATION THROUGH DRAFTING
27 Feb 2025</p>
<p>Zilong Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#122;&#108;&#119;&#97;&#110;&#103;&#64;&#117;&#99;&#115;&#100;&#46;&#101;&#100;&#117;">&#122;&#108;&#119;&#97;&#110;&#103;&#64;&#117;&#99;&#115;&#100;&#46;&#101;&#100;&#117;</a> 
University of California
San Diego</p>
<p>Zifeng Wang 
Google Cloud AI Research</p>
<p>Long T Le 
Google Cloud AI Research</p>
<p>Huaixiu Steven Zheng 
Google DeepMind</p>
<p>Swaroop Mishra 
Google DeepMind</p>
<p>Vincent Perot 
Google DeepMind</p>
<p>Yuwei Zhang 
University of California
San Diego</p>
<p>Anush Mattapalli 
Google Cloud AI</p>
<p>Ankur Taly 
Google Cloud AI</p>
<p>Jingbo Shang 
University of California
San Diego</p>
<p>Chen-Yu Lee <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#104;&#101;&#110;&#121;&#117;&#108;&#101;&#101;&#64;&#103;&#111;&#111;&#103;&#108;&#101;&#46;&#99;&#111;&#109;">&#99;&#104;&#101;&#110;&#121;&#117;&#108;&#101;&#101;&#64;&#103;&#111;&#111;&#103;&#108;&#101;&#46;&#99;&#111;&#109;</a> 
Google Cloud AI Research</p>
<p>Tomas Pfister 
Google Cloud AI Research</p>
<p>Zilong Wang 
SPECULATIVE RAG: ENHANCING RETRIEVAL AUG-MENTED GENERATION THROUGH DRAFTING
27 Feb 202584BD06CCA85F2B8458F0B53482FE4105arXiv:2407.08223v2[cs.CL]
Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses.Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs.In this work, we introduce SPECULATIVE RAG -a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM.Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft.This approach enhances comprehension of each subset and mitigates potential position bias over long context.Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts.Extensive experiments demonstrate that SPECULATIVE RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PopQA, PubHealth, and ARC-Challenge benchmarks.It notably enhances accuracy by up to 12.97% while reducing latency by 50.83% compared to conventional RAG systems on PubHealth.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) have demonstrated remarkable success in question answering tasks (Brown et al., 2020;Achiam et al., 2023;Team et al., 2023).Trained on massive datasets, LLMs leverage their extensive parametric memory to generate seemingly plausible responses to user queries (Kojima et al., 2022;Kamalloo et al., 2023).However, when faced with knowledgeintensive questions demanding up-to-date information or obscure facts (Petroni et al., 2021), LLMs struggle with factual inaccuracies and produce hallucinated contents (Huang et al., 2023).</p>
<p>Retrieval Augmented Generation (RAG) has emerged as a promising solution to mitigate these issues.By incorporating information retrieved from an external database into the context (Gao et al., 2023b), RAG effectively reduces factual errors in knowledge-intensive tasks.This approach not only enables easy and efficient access to vast databases but also facilitates timely and accurate knowledge integration.Due to the inherent limitations in the precision of current dense retrievers and the vastness of knowledge required to answer complex questions (Chen et al., 2022), RAG systems typically retrieve multiple documents to ensure the inclusion of all necessary information in the context (Petroni et al., 2021).This practice inevitably increases the length of the input to the LLMs, presenting significant challenges, particularly since encoding lengthy retrieved documents incurs additional latency and require more complex reasoning.Recent studies have explored ways to extend the context length limit of LLMs (Ding et al., 2023;Reid et al., 2024;Ma et al., 2024), yet achieving well-grounded reasoning over extended contexts remains an open question (Liu et al., 2024;Li et al., 2024).Consequently, striking a balance between efficiency and effectiveness in RAG has become a central research question in the literature.Existing work on RAG systems primarily concentrates on improving the quality of contextual information in retrieval outcomes, but often neglecting the latency issues associated with these systems (Ma et al., 2023;Baek et al., 2023;Yan et al., 2024;Xie et al., 2023;Asai et al., 2023;Feng et al., 2023).These methods typically rely on multiple refinement iterations or customized instruction-tuning for self-critique abilities.Integrating such enhancements into generic LMs requires additional training or increased latency, posing practical challenges in real-world applications.</p>
<p>To this end, we introduce SPECULATIVE RAG, a RAG framework designed to offload computational burden to a smaller, specialist LM that serves as an efficient and robust RAG module for existing generalist LMs.Inspired by Speculative Decoding (Leviathan et al., 2023;Chen et al., 2023a;Xia et al., 2024a), which accelerates auto-regressive LM inference by concurrently generating multiple draft tokens with a smaller model and verifying them in parallel with the base model, our approach adapts this concept to RAG.</p>
<p>In SPECULATIVE RAG, we partition retrieved documents into subsets for drafting answer candidates.We cluster the retrieved documents by content similarity and sample one document from each cluster to form a subset, minimizing redundancy and maximizing diversity.These document subsets are then fed to multiple instances of the RAG module, which generate draft answers with corresponding rationales in parallel.This smaller, specialized RAG module, excels at reasoning over retrieved documents and can rapidly produce accurate responses.Subsequently, the generalist LM bypasses the detailed review of potentially repetitive documents, focusing instead on validating the drafts against the rationales to determine the most accurate answer.We utilize the strong language modeling capabilities of generalist LMs, calculating the conditional generation probability of the answer drafts and rationales as a confidence score.Our key contributions are:</p>
<p>• We introduce a novel RAG framework that employs a smaller specialist RAG drafter to generate high-quality draft answers.Each draft is derived from a distinct subset of retrieved documents, offering diverse perspectives while reducing input token counts per draft.</p>
<p>• The generalist LM, operating with the RAG drafter, requires no additional tuning.It simply verifies and integrates the most promising draft into the final answer.This approach enhances comprehension of each subset and mitigates potential lost-in-the-middle (Liu et al., 2024) phenomenon.• Our method significantly accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single, unbiased verification pass over the drafts in parallel.Extensive experiments on 5 free-form question-answering and closed-set generation benchmarks demonstrate the superior effectiveness and efficiency of the method.</p>
<p>RELATED WORKS</p>
<p>Retrieval Augmented Generation Retrieval Augmented Generation (RAG) enhances LLMs by retrieving relevant documents from external databases and incorporating them into the generation process (Gao et al., 2023b;Lewis et al., 2020;Khandelwal et al., 2020;Izacard &amp; Grave, 2021;Luo et al., 2023a;Xia et al., 2024b;Wang et al., 2024).Recent work has primarily focused on enabling LLMs to understand when and what to retrieve (Ma et al., 2023;Chen et al., 2023b;Jiang et al., 2023b;Schick et al., 2024), or designing approaches to better utilize contexts (Yu et al., 2023;Yoran et al., 2023;Wang et al., 2023b;Sarthi et al., 2024;Baek et al., 2023;Xu et al., 2023;Kim et al., 2024).Among them, SAIL (Luo et al., 2023a) fine-tunes a pre-trained LLM on web search data to filter irrelevant contents.Self-Reflective RAG (Asai et al., 2023) introduces reflection tokens to guide retrieval and annotation in instruction-tuning datasets.However, both approaches require additional instruction-tuning of generic LLMs, which is resource-intensive and may lead to forgetting or over-fitting (Luo et al., 2023b).Furthermore, long context with retrieved documents can suffer from computational inefficiency and position bias (Liu et al., 2024).Corrective RAG (Yan et al., 2024) on the other hand proposes a lightweight retrieval evaluator, but it lacks the capability for high-level reasoning.In contrast, our proposed SPECULATIVE RAG addresses these limitations by leveraging a smaller RAG drafter model to efficiently understand diverse perspectives in retrieval results and generate drafts for the generalist LMs to verify and integrate.</p>
<p>Speculative Decoding Speculative decoding (Stern et al., 2018;Xia et al., 2023;Chen et al., 2023a;Leviathan et al., 2023;Xia et al., 2024a) aims to reduce auto-regressive decoding latency through a draft-then-verify paradigm.This involves drafting multiple future tokens with a small model and verifying them in parallel with the target model (Xia et al., 2024a).The draft model is typically either an independent model from the same series (Leviathan et al., 2023;Chen et al., 2023a) or the target model itself (Zhang et al., 2023a;Cai et al., 2024).Our approach extends this concept from token-level drafting to answer-level drafting.In contrast to traditional verification criteria (Stern et al., 2018;Xia et al., 2023;Leviathan et al., 2023;Chen et al., 2023a;Miao et al., 2024), which accept or reject tokens based on their generation probabilities, we leverage language modeling objectives to directly assess the confidence of entire answer drafts.</p>
<p>SPECULATIVE RETRIEVAL AUGMENTED GENERATION THROUGH DRAFTING</p>
<p>Problem Formulation In knowledge intensive tasks, each entry can be represented as (Q, D, A), where Q is a question or statement that requires additional knowledge; D = {d 1 , ..., d n } is a set of n documents retrieved from the database; A is the expected answer.Particularly, in question answering tasks, Q and A are the question and the expected answer in natural language form; in the statement verification tasks, Q is a statement and A ∈ {True, False} is a Boolean value indicating the statement's correctness; in the multiple choice tasks, Q is a question with a few options and A ∈ {A, B, C, ...} is the index of the correct answer.The objective of a RAG system is to generate a fluent response containing the expected answer or select the expected answer from the provided options based on the context provided by the retrieved supporting documents.</p>
<p>OVERVIEW</p>
<p>We introduce Speculative Retrieval Augmented Generation (SPECULATIVE RAG), as illustrated in Figure 1.We aim at enhancing the reasoning ability of LLMs over retrieved documents without compromising processing speed.Instead of relying on brute-force parameter scaling or instruction-tuning an entire LM to handle knowledge-intensive tasks, we propose a divide-and-conquer approach.We utilize a smaller specialist LM, the RAG drafter, to rapidly generate multiple answer drafts based on retrieved results.Then, a larger generalist LM, the RAG verifier, assesses these drafts, selects the best one based on its rationale, and integrates it into the generation results.Specifically, as shown in Algorithm 1, we first cluster the retrieved documents with regard to their relation to the posed question, where each cluster represents one perspective in the retrieval results (Line 2).Then we sample one document from each cluster into a subset so the documents in this subset covers the multiple perspectives in the retrieval results.We aim at minimizing redundancy and increase the diversity of the documents (Line 5 to 8).We denote one subset as δ ⊂ D that contains retrieved documents with diverse contents and multiple perspectives in the retrieval results.Then, we distribute each subset δ to a RAG drafter endpoint M Drafter with the posed question Q to generate the answer draft α and the rationale β in parallel (Line 12).The RAG drafter is instruction-tuned to be a specialist in understanding the retrieved documents and produce rationales that are faithful to the input documents.It is smaller than generalist LMs, and its parallel processing further ensures high efficiency.For each draft-rationale pair (α, β) from M Drafter , we compute a confidence score with the generalist LM M Verifier based on the question Q and corresponding rationale β (Line 13).It is worth mentioning that M Verifier does not need to be instruction-tuned since we leverage its language modeling ability already learned during pre-training.Meanwhile, M Verifier can verify the drafts based on the informative rationale provided by M Drafter instead of processing tedious or possibly redundant retrieved documents.Finally, we select the answer draft with the highest confidence score as the final answer and integrate it into the generation results of the generalist LM (Line 15).</p>
<p>SPECIALIST RAG DRAFTER</p>
<p>Instead of tuning a large generalist LM for the RAG scenario, we leverage a smaller specialist LM, M Drafter , to understand retrieved documents.M Drafter is specialized in answering the given question based on the supporting documents and not expected to cope with general problems.It serves as a RAG module for the generalist LMs when solving knowledge-intensive tasks.We train M Drafter to generate both the answer draft and the rationale to better understand the contextual documents.</p>
<p>Instruction Tuning Given a triplet (Q, A, D), where Q is a general query, A is the response, and D is a retrieved supporting document, we augment it with the rationale of the response A based on the document D. We denote the rationale as E which extracts essential information from the document and explains why the response is reasonable to the query concisely (Hsieh et al., 2023) so it is of shorter length and delivers information coherent with the original document.We leverage relatively strong LMs to automatically synthesize the rationale E for each triplet.Specifically, we directly query the strong LM to understand the knowledge from the document and provide the intermediate rationale between the instruction and response.Refer to Appendix G for detailed prompts.After generating the rationale, we finetune a pre-trained LM using the standard language modeling objective, maximizing the likelihood:
E (Q,A,D,E) log P MDrafter (A, E | Q, D), where (Q, A, D, E) is an augmented entry in the dataset; P MDrafter (A, E | Q, D)
is the probability of generating the response and rationale based on the query and document.We use this instruction-tuned model as the specialist RAG drafter which learns to generate a well-grounded response and rationale given the query and relevant documents.</p>
<p>Multi-Perspective Sampling For each knowledge-intensive question, we retrieve a set of documents from the database using the posed question as the retrieval query.These documents may contain diverse content due to the ambiguity inherent in the query.To minimize redundancy and enhance diversity of the document subsets used for generating answer drafts, we employ a multiperspective sampling strategy.We first cluster the documents into a few topics using an instructionaware embedding model (Peng et al., 2024) and the K-Means clustering (Jin &amp; Han, 2011).
emb(d 1 ), ..., emb(d n ) = E(d 1 , ..., d n |Q) {c 1 , ..., c k } = K-Means(emb(d 1 ), ..., emb(d n )) δ = random.sample(c) for c ∈ {c i } k 1
where E is an instruction-aware embedding model which embeds a string with regard to a provided instruction (the posed question Q); emb(d i ) is the embedding for the retrieved document d i ; c j is a cluster of retrieved documents with similar topics and contents; k is a hyper-parameter that controls the number of clusters.We sample one document from each cluster into a document subset δ so each subset contains k documents of diverse contents.In total, we construct m subsets for parallel inference with the RAG drafter.</p>
<p>RAG Drafting</p>
<p>We run M Drafter over the m document subsets and produce corresponding answer drafts.Refer to Appendix H for detailed prompt.We incorporate each document subset into the prompt and query M Drafter for responses.We obtain m drafts as the answer candidates and each draft is grounded based on the multiple perspectives in the retrieval results.Specifically, given a document subset δ j = {d j1 , .., d j k }, we query M Drafter in parallel with the following prompt for the answer draft and rationale: Q, d j1 , ..., d j k → α j , β j , where the prompt contains the posed question Q along with the document subset; the generation result contains the answer draft α and the rationale β.We denote the conditional generation probability as ρ Draft,j = P (β j |Q, d j1 , ..., d j k ) + P (α j |Q, d j1 , ..., d j k , β j ), which measures the reliability of generating rationales and the confidence in producing answer drafts.</p>
<p>GENERALIST RAG VERIFIER</p>
<p>After generating drafts and the rationale from the RAG drafter M Drafter , we evaluate them by a generalist LM M Verifier to filter out the less reliable drafts and select the best answer.The generalist LM can be any off-the-shelf pre-trained LM.We only consider the draft-rationale pair (α, β) and skip the tedious and redundant retrieval results.We resort to the language modeling ability of the generalist LM to rank and select the draft-rationale pairs.</p>
<p>Evaluation Scores First, we calculate the self-consistency score by determining the conditional probability of generating a draft-rationale pair given the question, ρ Self-contain = P (α, β|Q).This score helps assess whether the draft and rationale are self-consistent in the context of the question.Given the characteristics of language modeling, a self-consistent draft-rationale pair is expected to yield a higher probability.Furthermore, we incorporate a self-reflection statement R that prompts M Verifier to assess the reliability of an answer draft (e.g."Do you think the rationale supports the answer, yes or no?").We define the self-reflection score as ρ Self-reflect = P ("Yes"|Q, α, β, R) where we compute the conditional probability of the positive answer ("Yes") to the self-reflection statement.</p>
<p>Computation Method We can efficiently compute the self-consistency and self-reflection scores within one forward pass of M Verifier .Given a question Q and a draft-rationale pair (α, β), we construct a prompt [Q, α, β, R, "Yes"], where R is the self-reflection statement.We encode the prompt with M Verifier , and acquire the probability of each token conditioned on the previous tokens P (t i |t &lt;i ).We leverage this auto-regressive feature and aggregate the probability of the relevant tokens to compute the self-consistent score ρ Self-contain and self-reflection score ρ Self-reflect .
Q, ρ SC α, β, R, ρ SR "Yes" −−−−−−−−−−−−→ ⇒ ρSC = t i ∈α P (ti|t&lt;i) • t i ∈β P (ti|t&lt;i) ρSR = t i ∈"Yes" P (ti|t&lt;i)
Finally, we produce the final score, ρ j = ρ Draft,j • ρ SC,j • ρ SR,j , and then select the most reliable answer as the final answer to the question Â = arg max αj ρ j .</p>
<p>EXPERIMENTS</p>
<p>We evaluate our proposed SPECULATIVE RAG on five public retrieval augmented generation benchmarks: TriviaQA (unfiltered) (Joshi et al., 2017), MuSiQue (Trivedi et al., 2022), PopQA (Mallen et al., 2023), PubHealth (Zhang et al., 2023b), and ARC-Challenge (Clark et al., 2018).We provide representative examples for case study in Appendix J. TriviaQA, MuSiQue, PopQA are challenging open-domain question answering datasets where RAG systems are required to answer questions on factual knowledge.TriviaQA and PopQA typically require one accurate piece of evidence from the documents, whereas MuSiQue demands multiple documents to construct a multi-hop reasoning chain.More detailed experiments on multi-hop reasoning can be found in Appendix F. Following previous works (Guu et al., 2020;Asai et al., 2023;Yan et al., 2024), we evaluate performance of the free-form generation based on whether gold answers are contained within the generated response or not.PubHealth and ARC-Challenge are closed-set generation datasets.PubHealth is a dataset of medical claims spanning a variety of biomedical subjects and it requires the RAG system to verify a given claim based on the retrieved documents.ARC-Challenge introduces a multi-choice question answering dataset, composed of science exam questions from grade 3 to grade 9.For closed-set generation tasks, we use accuracy to evaluate whether the generated answers match the ground truth.</p>
<p>BASELINES</p>
<p>Standard RAG For standard RAG, we incorporate all the retrieved documents into the prompt as contextual information.Refer to Appendix I for detailed prompts.We run standard RAG experiments on off-the-shelf LLMs including Mistral 7B , Mistral-Instruct 7B (Jiang et al., 2023a), Mixtral 8x7B , Mixtral-Instruct 8x7B (Jiang et al., 2024), and Alpaca 7B (Dubois et al., 2024).We also include the performance of Toolformer (Schick et al., 2024) and SAIL (Luo et al., 2023a) which are originally reported from Asai et al. (2023).Toolformer 7B is an LM instruction-tuned to use tools including a search engine, and SAIL 7B is an LM instruction-tuned on the Alpaca instruction tuning set augmented with search results from different sources such as DuckDuckGo and Wikipedia.</p>
<p>Self-Reflective RAG and Corrective RAG Self-Reflective RAG (Self-RAG) (Asai et al., 2023) and Corrective RAG (CRAG) (Yan et al., 2024) are more advanced RAG systems that enhances the quality of contextual information in the retrieval results.CRAG introduces an external evaluator to assess the quality of retrieved documents, and to refine them before the response generation.Self-RAG instruction-tunes an LM to generate special self-refection tags.These tags guides the LM to dynamically retrieve documents when necessary, critique the retrieved documents relevance before generating responses.Self-CRAG is to apply the Self-RAG approach on the refined documents of CRAG.We adopt the same backbone LLMs across all methods as our proposed SPECULATIVE RAG for fair comparisons.</p>
<p>EXPERIMENT SETTINGS</p>
<p>In our experiments, we utilize Mistral 7B (v0.1) as our base LM for the RAG drafter.For RAG verifier, we employ either Mistral 7B (v0.1) or Mixtral 8x7B (v0.1) without any fine-tuning, denoted as M Verifier-7B or M Verifier-8x7B .We pre-compute embeddings of retrieved documents using a lightweight instruction-aware embedding model InBedder Roberta (Peng et al., 2024) as part of the retrieval process.Inference is conducted using the vLLM framework (Kwon et al., 2023) with greedy decoding (temperature = 0).We adopt the same experiment settings from Asai et al. (2023) and include a more challenging benchmark, MuSiQue (Trivedi et al., 2022).Our focus is on RAG reasoning rather than evidence citation, so we omit the other two long-form generation benchmarks, Biography (Min et al., 2023) and ALCE-ASQA (Gao et al., 2023a).On TriviaQA, PopQA, Pub-Health, and ARC-Challenge, we retrieve top 10 documents and generate 5 drafts per query (m = 5), with each draft based on a subset of 2 documents (k = 2).For MuSiQue, we retrieve top 15 documents and generate 10 drafts for each query (m = 10), each using a subset of 6 documents due to more complex reasoning.Further details regarding instruction-tuning can be found in Appendix A.</p>
<p>MAIN RESULTS</p>
<p>We compare SPECULATIVE RAG with standard RAG approaches, as well as the more advanced Self-Reflective RAG and Corrective RAG on five datasets: TriviaQA, MuSiQue, PopQA, Pub-Health, and ARC-Challenge.We report the performance of M Drafter-7B when used alone or paired with the RAG verifier (e.g.M Verifier-7B , M Verifier-8x7B ).Following prior work (Asai et al., 2023;Yan et al., 2024), we report accuracy as the performance metric.We attribute these improvements to the superior reasoning capabilities of the RAG drafter over the retrieved documents in SPECULA-TIVE RAG.By minimizing the redundancy in the sampled documents, the RAG drafter generates higher quality answer drafts based on diverse perspectives from the retrieval results.</p>
<p>Superior Performance over Baselines</p>
<p>Reliable Scoring by RAG Verifier The reliable draft verification by the generalist LM also contributes to the enhanced performance.The performance improves remarkably comparing M Drafter-7B and M Verifier-7B + M Drafter-7B .The instruction-tuned RAG drafter is specialized in generating answer drafts based on the retrieved documents while the language modeling capabilities of generic LMs are leveraged to validate each draft in light of its rationale.This method is both effective and easy to implement, showcasing the effectiveness of this verification approach.</p>
<p>EFFECTS OF GENERATED RATIONALE FOR VERIFICATION</p>
<p>In SPECULATIVE RAG, we utilize the generated rationale β from the RAG drafter as an indicator of the trustworthiness of answer drafts α.</p>
<p>Shortened length compared to the retrieved documents.The rationales highlight relevant points, omit redundant information, and bridge logical gaps between drafts and their supporting documents.We compare the number of tokens in the generated rationale and the retrieved documents, and plot them in Figure 2. We find that the generated rationale is significantly shorter than the retrieved documents.Table 2: Performance and latency analysis of SPECULATIVE RAG on TriviaQA and PubHealth using M Verifier-8x7B + M Drafter-7B .We add the original document subset δ to the context or replace the generated rationale β with the original retrieved document subset δ during verification, i.e. we compute the self-containment score as ρ Self-contain = P (α, δ|Q) or ρ Self-contain = P (α, δ, β|Q), and compute the self-reflection score as ρ Self-reflect = P ("Yes"|Q, α, δ, R) or ρ Self-reflect = P ("Yes"|Q, α, δ, β, R), where Q is the query; α is the answer draft; R is the selfreflection statement.Comparable performance with retrieved documents and lower latency.To evaluate the effectiveness of the rationales, we create two alternative scoring methods: (a) replacing rationale with retrieved documents (ρ = Score(α|Q, δ)), or (b) adding retrieved documents to rationale (ρ = Score(α|Q, β, δ)).We compare these alternatives to the scoring method used in SPECULA-TIVE RAG (ρ = Score(α|Q, β)) in Table 2.The results show that incorporating longer retrieved documents does not consistently improve performance and tends to increase latency.This suggest that the generated rationale is already of high quality and serves as an effective bridge between the supporting documents and the generated answer drafts.By leveraging this rationale, we can efficiently verify drafts using a generic LM, leading to accurate final results.We further validate the rationale generation in the instruction-tuning stage.See Appendix D for more details.</p>
<p>LATENCY ANALYSIS WITH BASELINES</p>
<p>We analyze the latency of Standard RAG, Self-RAG, and our SPECULATIVE RAG on TriviaQA, MuSiQue, PopQA, PubHealth, and ARC-Challenge.We randomly sample 100 cases from each dataset and report the average time cost for each case, as shown in Figure 3.  1).We also include the analysis for Standard RAG: Mistral-Instruct 7B and Self-RAG: Mistral-Instruct 7B in this study.For SPECULATIVE RAG, we launch 5 endpoints of M Drafter-7B for parallel drafting on TriviaQA, PopQA, PubHealth, and ARC-Challenge.We launch 10 endpoints for MuSiQue due to more drafts.We use tensor parallelism of 4 to fit Mixtral-Instruct 8x7B into the GPU memory.We use the same tensor parallelism setting for the other methods for a fair comparison.</p>
<p>Reducing processing time while maintaining high performance As the results demonstrate, SPECULATIVE RAG consistently achieves the lowest latency compared to all other methods.This advantage comes from its utilization of fewer documents needed per draft and parallel drafting.Particularly, compared with the most competitive baseline, Standard RAG: M Verifier-8x7B + M Drafter-7B , our proposed SPECULATIVE RAG reduces latency by up to 11.90% on TriviaQA, 15.07% on MuSiQue, 44.31% on PopQA, 50.83% on PubHealth, and 22.77% on ARC-Challenge.Furthermore, a direct comparison between Standard RAG: Mistral-Instruct 7B and our method reveals that the higher latency of Standard RAG: Mistral-Instruct 7B is due to its longer context length which contains all retrieved documents.Self-RAG: Mistral-Instruct 7B also exhibits higher latency due to the generation of longer answers with self-reflection tags and the additional overhead associated with evidence selection.These findings highlight the advantage of our approach in reducing processing time while maintaining high performance.</p>
<p>ABLATION STUDIES</p>
<p>We conduct ablation studies on the multi-perspective sampling (Section 3.2) and the evaluation scores (Section 3.3) of SPECULATIVE RAG during the drafting or the verification stages on Trivi-aQA and PubHealth in Table 3.We use M Verifier-8x7B + M Drafter-7B as a running configuration.Same as the main results, we report the accuracy as performance metrics.</p>
<p>Diversity and reduced redundancy in retrieval improves draft quality significantly.In the first set of experiments, we evaluate the impact of multi-perspective sampling during the drafting.</p>
<p>Recall that SPECULATIVE RAG clusters retrieved documents into distinct perspectives and sample one document from each cluster to reduce redundancy for the draft generation.We compare this against two alternative sampling strategies: (1) Random sampling without multi-perspective clustering, where we randomly select a document subset as context, and (2) Always sampling from the same cluster, where we select all documents from a single cluster.Our results indicate that our proposed sampling method yields the best performance thanks to its ability to leverage diverse context.Particularly, it improves the accuracy up to 1.88% on TriviaQA and 2.23% on PubHealth.While random sampling without clustering introduces diversity, it is prone to including redundant documents, degrading draft quality.Sampling from the same cluster significantly underperforms due to a lack of diverse perspectives.</p>
<p>Scoring method on self-consistency and self-reflection refines draft quality effectively.In the second set of experiments, we examine the scoring method during verification.We remove each of the specific confidence scores, ρ Draft , ρ Self-contain , or ρ Self-reflect in turn.Performance drops are observed when any score is removed.Particularly, removing ρ Draft leads to a minimal decline, 0.19% on TriviaQA and 1.12% on PubHealth, likely due to the limited verification capability of the smaller RAG drafter.Removing either ρ Self-contain or ρ Self-reflect results in similar performance decreases, around 2.0% on TriviaQA and around 0.8% on PubHealth, indicating that both self-containment and self-reflection capture different key aspects of reasoning and are crucial during verification.Random selection without verification leads to substantial underperformance, resulting in a performance decline of 5.69% on TriviaQA and 5.37% on PubHealth.</p>
<p>EFFECTS OF DRAFT NUMBER AND DOCUMENT SUBSET SIZE</p>
<p>Increasing the number of drafts improves performance without adding latency.We investigate the performance of SPECULATIVE RAG under varying numbers of drafts.Using M Verifier-7B + M Drafter-7B with 1, 5, 10, 15, 20 drafts on TriviaQA and PubHealth.We sample two documents as context per draft.The results are illustrated in Figure 4(a).Since we retrieve top 10 documents in total, we sample up to 20 drafts in these experiments.The results indicate that incorporating more drafts can further improve performance, likely thanks to higher coverage of diverse perspective of documents.Importantly, in SPECULATIVE RAG, we can launch multiple RAG drafter instances to generate drafts in parallel without additional latency.</p>
<p>Increasing the document subset size doesn't always lead to better performance.We also examine the effect of document subset size.By varying the number of documents (1, 2, 4, 6, or 10) sampled for draft generation on TriviaQA and PubHealth (Figure 4(b)), we find that including more documents in the context does not always lead to consistent performance improvement.While Triv-iaQA queries may benefit from more supporting documents due to their complexity, M Verifier-7B + M Drafter-7B can surpass Mistral-Instruct 7B even with a single supporting document per draft.Furthermore, with two or more documents per draft, M Verifier-7B + M Drafter-7B can even surpass Mixtral-Instruct 8x7B .This further demonstrates the effectiveness of our drafting design.</p>
<p>CONCLUSION</p>
<p>Our proposed SPECULATIVE RAG decomposes RAG tasks into two separate steps of drafting followed by verification.SPECULATIVE RAG delegates the heavy lifting of drafting to a small specialized RAG drafter, while verification is done using a large generalist LM.The parallel generation of multiple drafts from diverse document subsets provides high quality answer candidates while reducing input token counts and the potential risk of position-bias-over-long-context, resulting in substantial improvements in both the quality and speed of the final output generation.We demonstrate the effectiveness of SPECULATIVE RAG with accuracy gains up to 12.97% while reducing latency by 50.83% compared to conventional RAG systems.SPECULATIVE RAG sheds new light on the potential of collaborative architectures for enhancing RAG performance through task decomposition.</p>
<p>C EFFECTS OF SELF-REFLECTION STATEMENT</p>
<p>We use "Do you think the explanation supports the answers?(Yes or No)" as the self-reflection statement in our main results.In this study, we replace it with other alternatives to see how the self-reflection statement affects the accuracy.The results are reported in Table 7.We observe that the performance does not change a lot given different self-reflection statements, which shows the stable verification capability of the generalist LMs by language modeling objective.</p>
<p>D EFFECTS OF RATIONALE GENERATION</p>
<p>We acknowledge that the generation of rationale potentially increases the inference cost during the drafting stage while this is crucial for the verifier in our method to assess the quality and reliability of generated drafts.And, the potential overhead can be mitigated through efficient parallel inference.</p>
<p>To further study the impact of rationale generation, we finetune the RAG drafter without rationale.We denote this setting as: without rationale in drafting.Similarly, with rationale/doc in verification indicates that we use the generated rationale or the retrieved documents as reference during the verification stage.We use M Verifier-8x7B + M Drafter-7B as a running example.The results are shown in Table 8.Better answer drafting As explored in Hsieh et al. (2023), incorporating rationale generation during instruction-tuning can lead to the RAG drafter producing higher-quality answer drafts.The results in Table 8 clearly demonstrate this.We observe a significant performance drop across all three benchmarks when the RAG drafter is finetuned without the rationale component,</p>
<p>Lower latency and cost in verification</p>
<p>We verify each draft against the rationale instead of the retrieved documents.From the ablation results, these generated rationales serve as high-quality grounding facts, improving verification performance compared to using the retrieved documents.</p>
<p>E EFFECTS OF DIFFERENT VOLUME OF TRAINING DATA</p>
<p>We acknowledge the importance of evaluating our framework's performance across different training data volumes.Our primary experiment utilized 40,059 instances to train our drafter model.To thoroughly assess scaling effects, we conducted additional experiments using incremental subsets of 10,000, 20,000, and 30,000 training instances.The results of these systematic evaluations are detailed in Table 9.</p>
<p>F EFFICACY OF SPECULATIVE RAG IN MULTI-HOP REASONING</p>
<p>We further validate SPECULATIVE RAG in the scenario of multi-hop reasoning.One of the key challenges of multi-hop reasoning is to effectively combine multiple pieces of evidence to arrive at the final answer.Indeed, the ability to verify or contrast information across documents is crucial to solve complex questions.We compare the performance of SPECULATIVE RAG with baselines on MuSiQue (Trivedi et al., 2022) and HotpotQA (Yang et al., 2018), two multi-hop reasoning benchmarks.We randomly sample 500 examples from the validation set of HotpotQA as the test set in our experiment.We adopt the same setting as MuSiQue on HotpotQA.The results are in Table 10.We find that our SPECULATIVE RAG achieves the best performance.Specifically, SPECULATIVE RAG improves accuracy by 2.15% on MuSiQue and by a substantial 5.4% on HotpotQA.31.5749.00</p>
<p>Our approach tackles this challenge by multi-perspective sampling when selecting documents for each draft (Section 3.2).We cluster the retrieved documents into distinct topics using an instructionaware embedding model (Peng et al., 2024).Then, we sample one document from each cluster to form a diverse document subset, ensuring each drafter receives a variety of perspectives from the retrieval results.To validate the efficacy of this strategy, we further conduct an ablation study on MuSiQue and HotpotQA in Table 11.From the table, our sampling strategy effectively guarantees the diversity of information within the supporting document subsets, leading to improved performance of SPECULATIVE RAG on these tasks.</p>
<p>F.1 PERFORMANCE BREAKDOWN ON HOTPOTQA</p>
<p>HotpotQA includes two types of quesitions: bridge-type questions in HotpotQA require a two-step reasoning process where the answer to the first step is crucial for answering the second.For example: We report the performance breakdown of SPECULATIVE RAG on HotpotQA in Table 12.The results demonstrate a superior performance on comparison-type questions with multi-perspective sampling.This aligns with our expectations, as multi-perspective sampling ensures the document subset covers the diverse topics necessary for answering comparison-type questions.Revisiting the example above, "Who was born first, Morgan Llywelyn or Robert Jordan?", with k = 4, our approach clusters retrieved documents into four groups.Group 0 and 3 focus on Morgan, while group 1 and 2 focus on Robert.As we sample one document from each group for the drafters, this clustering result ensures each drafter receives documents about both individuals.This balanced information distribution is crucial for the comparison-type questions.In contrast, random sampling risks providing a drafter with information about only one person, yielding a suboptimal draft.</p>
<p>On the other hand, we also observe that the multi-perspective sampling is less helpful for bridge-type questions.These questions require the LLM to first identify the "bridge entity" (e.g., Thom Yorke in the earlier example), a task our current work isn't explicitly designed for.While multi-perspective sampling effectively covers different topics in the drafts and the map-reduce approach accelerates inference, they might not directly contribute to pinpointing the "bridge entity" -the key to answering bridge-type questions.</p>
<p>We believe our framework could be effectively combined with other techniques specifically designed for bridge-type questions, such as those proposed in  [1] Buddhism Nirvana literally means "blowing out, quenching, becoming extinguished".In early Buddhist texts, it is the state of restraint and self-control that leads to the "blowing out" and the ending of the cycles of sufferings associated with rebirths and redeaths.Many later Buddhist texts describe nirvana as identical with "anatta" with complete "emptiness, nothingness".In some texts, the state is described with greater detail, such as passing through the gate of emptiness ("sunyata") realizing that there"</p>
<p>[2] Salvation It includes a variety of disciplines, such as yoga and meditation.Nirvana is the profound peace of mind that is acquired with moksha (liberation).In Buddhism and Jainism, it is the state of being free from suffering.In Hindu philosophy, it is union with the Brahman ( Supreme Being).The word literally means "blown out" (as in a candle) and refers, in the Buddhist context, to the blowing out of the fires of desire, aversion, and delusion, and the imperturbable stillness of mind acquired thereafter.In Theravada Buddhism the emphasis is on one's ======================================= Completion ======================================= ## Rationale: Nirvana literally means 'blowing out, quenching, becoming extinguished'.It is described as a state of "restraint and self-control" that leads to the "blowing out" and the ending of the cycles of sufferings associated with rebirths and redeaths.## Response: In Buddhism, the state of blissful repose or absolute existence by someone relieved of the necessity of rebirth is called Nirvana.Response to the instruction.Also provide rationale for your response.## Instruction: A lot of the problems with forest fires is because of bad policy (not to clear out the forests) that was put in place by environmentalists.## Evidence:</p>
<p>[1] Forest restoration That can be because of provoked deforestation in order to substitute forests by crop areas, or in dry areas, because of wild fires occurring naturally or intentionally.A whole section of forest landscape restoration in linked to this particular problem, as in many cases, the net loss of ecosystem value is very high and can open the drop to an accelerated further degradation of the soil conditions through erosion and desertification.This indeed has dire consequences on both the quality of the habitats and their related fauna....</p>
<p>[2] Wildfire Attempts to thin out the fuel load may be met with opposition due to the desirability of forested areas.Wildland goals may be further resisted because of endangered species protections and habitat preservation.The ecological benefit of fire is often overridden by the economic benefits of protecting structures and lives.Additionally, federal policies that cover wildland areas usually differ from local and state policies that govern urban lands.... Response to the instruction.Also provide rationale for your response.## Instruction: A lot of the problems with forest fires is because of bad policy (not to clear out the forests) that was put in place by environmentalists.## Evidence:</p>
<p>[1] 1997 Indonesian forest fires The 1997 Indonesian forest fires were caused by changing land use which made the tropical forest vulnerable to fire during a drought associated with that year's El Nino.Indonesian forests have historically been resistant to burning even during long dry seasons and despite the use of fire to clear land for swidden agriculture.The land use changes that led to the fires were a combination of industrial-scale logging, draining peatlands for conversion to oil palm and fast-growing tree plantations, and a massive government program to drain swamps and convert them to rice paddies....</p>
<p>[2] Wildfire Attempts to thin out the fuel load may be met with opposition due to the desirability of forested areas.Wildland goals may be further resisted because of endangered species protections and habitat preservation.The ecological benefit of fire is often overridden by the economic benefits of protecting structures and lives.Additionally, federal policies that cover wildland areas usually differ from local and state policies that govern urban lands....</p>
<p>====================================== Completion</p>
<p>Figure 1 :
1
Figure 1: Illustration of different RAG approaches.Given a knowledge-intensive query Q and retrieved documents, (a) Standard RAG incorporates all documents into the prompt, increasing input length and slowing inference; (b) Self-Reflective RAG (Asai et al., 2023) requires specialized instruction-tuning of the general-purpose language model (LM) to generate specific tags for selfreflection; (c) Corrective RAG (Yan et al., 2024) employs an external retrieval evaluator to refine document quality, focusing solely on contextual information without enhancing reasoning capabilities; (d) In contrast, our proposed SPECULATIVE RAG leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, specialized LM.Each draft is generated from a distinct subset of retrieved documents, providing diverse perspectives on the evidence while minimizing the number of input tokens per draft.</p>
<p>Algorithm 1 :▷▷
1
SPECULATIVE RAG Data: (Q, D = {di} n i ) is the question and n retrieved documents; m subsets, each containing k documents, are sampled from D; k also corresponds to the number of clusters during clustering.Result: Â is the predicted answer to the question. 1 Function Speculative RAG(Q, D, m, k): 2 {c1, c2, ..., c k } K-Means ← −−− − C(d1, ..., dn|Q) ▷ Cluster the documents into k groups using an embedding model C. Construct a subset of the retrieved documents δj 6 for ci ∈ {c1, ..., c k } do 7 δj = δj ∪ {random.sample(ci)}▷ Sample one document from each cluster ci into subset δj .Repeat the sampling until there are m unique subsets in total.11 for δj ∈ ∆ do in parallel ▷ Process m subsets in parallel.12 αj, βj ← MDrafter.generate(Q,δj) ▷ Generate the draft α and rationale β with MDrafter.13 ρj ← MVerifier.score(αj|Q,βj) ▷ Compute the confidence score ρ with MVerifier.14 end 15 Â ← arg maxα j ρj ▷ Select the one with the highest score as the final answer.</p>
<p>Figure 2 :
2
Figure 2: Average number of tokens in the generated rationale and the retrieved documents in Triv-iaQA, MuSiQue, PopQA, PubHealth, and ARC-Challenge.The generated rationale is of much shorter length than the original retrieved documents.</p>
<p>We sample 1, 2, 4, 6, 10 supporting documents for each draft and we generate 10 answer drafts.</p>
<p>Figure 4 :
4
Figure 4: Performance analysis of SPECULATIVE RAG with (a) different numbers of drafts, and (b) different supporting document subset size on TriviaQA and PubHealth.</p>
<p>Figure 5 :
5
Figure 5: Prompt of Rationale Generation for Gemini-Ultra</p>
<p>Figure 6 :
6
Figure 6: Prompt of RAG Drafting</p>
<p>====================================== Completion ======================================== ## Rationale: Inadequate forest management can contribute to the severity of wildfires.## Response: True ====================================== Verification ====================================== draft score: 0.8161 self-containment score:0.1360self-reflection score: 0.8877 ------------------------------------------------------------------------------------------=================================== Prompt of Drafter ====================================</p>
<p>Figure 10 :
10
Figure 10: Case study of SPECULATIVE RAG from PubHealth where False is the correct answer.</p>
<p>Table 1 :
1
Asai et al. (2023)tion generation results on TriviaQA, MuSiQue, PopQA, PubHealth, and ARC-Challenge (ARC-C).(<em>We use the RAG drafter's generation probability ρ Draft as the confidence score for selecting drafts when we use it alone; † indicates numbers reported inAsai et al. (2023); − denotes numbers that are not reported by the original papers or are not applicable; ‡ we use Mistral 7B or Mixtral 8x7B as the RAG verifier, and denote them as M Verifier-7B or M Verifier-8x7B .)
Free-formClosed-setRAG MethodTriviaQA MuSiQue PopQA PubHealth ARC-CStandard RAGMistral7B (Jiang et al., 2023a)54.1516.7131.3834.8542.75Mixtral8x7B (Jiang et al., 2024)59.8519.1634.0237.0848.72Mistral-Instruct7B (Jiang et al., 2023a)67.1117.9942.1742.1547.70Mixtral-Instruct8x7B (Jiang et al., 2024)73.9129.4253.6863.6378.41Alpaca7B (Dubois et al., 2024)  †64.1-46.740.248.1Toolformer6B (Schick et al., 2024)  †48.8----SAIL7B (Luo et al., 2023a)  †---69.248.4Self-Reflective RAG &amp; Corrective RAGCRAGMistral-7B (Yan et al., 2024)59.03-49.4659.0474.87Self-RAGMistral-7B (Asai et al., 2023)64.8421.7252.6872.4474.91Self-CRAGMistral-7B (Yan et al., 2024)65.43-56.1172.8575.26Our Speculative RAGMDrafter-7B  </em>71.1127.8956.4075.5874.49MVerifier-7B  ‡ + MDrafter-7B73.9131.0356.7575.7976.19MVerifier-8x7B  ‡ + MDrafter-7B74.2431.5757.5476.6080.55</p>
<p>(Hsieh et al., 2023)hat our method consistently outperforms all baselines across all five benchmarks.Particularly, M Verifier-8x7B + M Drafter-7B surpasses the most competitive standard RAG model, Mixtral-Instruct 8x7B , by 0.33% on TriviaQA, 2.15% on MuSiQue, 3.86% on PopQA, 12.97% on PubHealth, and 2.14% on ARC-Challenge.With a comparable number of instruction-tuned parameters, M Verifier-7B + M Drafter-7B outperforms all Self-Reflective and Corrective RAG methods, and M Drafter alone surpasses the baselines in most settings.Effective Instruction Tuning for RAG Drafter Our instruction tuning is effective in enhancing the reasoning ability of the drafter model(Hsieh et al., 2023), as we observe a remarkable performance improvement comparing Mistral 7B and M Drafter-7B .Additionally, we further investigate the performance of M Drafter-7B when we directly feed all documents to the RAG drafter and generate one draft, with detailed results provided in Appendix B.Moreover, the performance of Mixtral 8x7B significantly improves when paired with the instruction-tuned RAG drafter M Drafter-7B , showing gains of 14.39% on TriviaQA, 12.41% on MuSiQue, 23.52% on PopQA, 39.52% on PubHealth, and 31.83% on ARC-Challenge.Similar improvements are observed with Mistral 7B as well.For Mistral</p>
<p>7B , we observed improvements of 19.76% on TriviaQA, 14.32% on MuSiQue, 25.37% on PopQA, 40.94% on PubHealth, and 33.44% on ARC-Challenge.</p>
<p>To simulate real-world RAG and SPECULATIVE RAG is highlighted in red (+x%).The latency varies across different datasets due to different retrieved document lengths.SPECULATIVE RAG encodes the retrieved documents in parallel and generates answer drafts with a smaller RAG drafter.This significantly improves the efficiency.application scenarios, we process cases individually without batching.As representative example, we run M Verifier-8x7B + M Drafter-7B for SPECULATIVE RAG and Mixtral-Instruct 8x7B for Standard RAG, as these demonstrate the highest performance among competitive baselines (see Table
Latency (s)1 2 3 42.52 +14% +1% 2.24 +34% 2.22 2.98 Std. RAG: Mixtral-Inst.-8x7B2.72 +18% +7% 2.47 +53% 2.31 3.54 Std. RAG: Mistral-Inst.-7B 2.55 +80% +8% 1.54 +61% 2.28 1.42 Self-RAG: Mistral-Inst.-7B 2.40 +103% 2.05 +74% +16% 1.37 1.18 Spec. RAG: Mixtral-8x7B w/ RAG Drafter (7B) 2.02 +29% 1.94 +24% 1.56 1.97 +26%0TriviaQAMuSiQuePopQA DatasetPubHealthARC-ChallengeFigure 3: Latency analysis of Standard RAG, Self-RAG, and SPECULATIVE RAG on TriviaQA,MuSiQue, PopQA, PubHealth, and ARC-Challenge. The latency difference between StandardRAG/Self-</p>
<p>Table 3 :
3
Ablation study of SPECULATIVE RAG in the drafting and verification stages on TriviaQA and PubHealth.
TriviaQAPubHealth</p>
<p>Table 6 :
6
RAG results on TriviaQA and PubHealth (m = 1, k = total # of docs)
TrivialQA PubHealth</p>
<p>Table 7 :
7
Performance analysis of SPECULATIVE RAG with different self-reflection statements R when computing the self-reflection score ρ Self-reflect = P ("Yes"|Q, α, β, R), where Q is the query, α, β are the generated answer draft and rationale.
Reflection StatementTriviaQAPubHealthDo you think the explanation supports the answers? (Yes or No)74.2476.60Does the rationale support the answer? (Yes or No)74.2276.09What do you think about the rationale? A good one? (Yes or No)74.2575.79Is the rationale good enough to support the answer? (Yes or No)74.3976.29</p>
<p>Table 8 :
8
Ablation study on the draft generation in the drafting and verification stages on TriviaQA and PubHealth.
DraftingVerification TrivialQA PubHealth ARC-Cw/o rationale w/ doc70.8675.2878.67w/ rationalew/ doc74.0876.0980.46w/ rationalew/ rationale74.2476.6080.55</p>
<p>Table 9 :
9
Performance analysis of SPECULATIVE RAG with different volumn of instruction-tuning data.From the table, we can conclude that increasing the volume of training data leads to improved performance.Specifically, the model's accuracy continues to rise as more instances are included, with the highest performance observed at 40,059 instances.This suggests that larger training datasets contribute positively to the performance of our drafter-verifier framework, indicating that scaling up data size could enhance the robustness of the model.
MVerifier-8x7B + MDrafter-7B TriviaQA PubHealth10,00071.6972.3420,00072.6472.4430,00073.2074.37Total (40,059)74.2476.60</p>
<p>Table 10 :
10
RAG results on MuSiQue and HotpotQA
MuSiQue HotpotQAMixtral-Instruct 8x7B29.4243.60Self-RAG Mistral-7B21.7227.20M Verifier-7B + M Drafter-7B31.0347.60M Verifier-8x7B + M Drafter-7B</p>
<p>Table 11 :
11
Ablation study of multi-perspective sampling on multi-hop reasoning benchmarks: MuSiQue, HotpotQA.
MuSiQue HotpotQARandom sampling29.3348.2Multi-perspective sampling31.5749.00• "When was the singer and songwriter of Radiohead born?"-Step 1: Who is the singer and songwriter of Radiohead? → Thom Yorke-Step 2: When was <a href="answer of step 1">Thom Yorke</a> born? → October 7, 1968-Final answer: October 7, 1968In contrast, comparison-type questions also involve two steps, but the answers to each step areindependent of each other. For example:• "Who was born first, Morgan Llywelyn or Robert Jordan?"-Step 1: What's Morgan Llywelyn's DOB? → December 3, 1937-Step 2: What's Robert Jordan's DOB? → October 17, 1948-Final answer: Morgan Llywelyn</p>
<p>Table 12 :
12
Performance of SPECULATIVE RAG for different question types
Question Type# of Questions SPECULATIVE RAGBridge-type40041.75Comparison-type10078.00Overall50049.00</p>
<p>Christianity was still not the official state religion in Rome, although it was in some neighboring states such as Armenia, Iberia, and Aksum.Roman Religion (Neoplatonic Hellenism) was restored for a time by the Emperor Julian from 361 to 363.Julian does not appear to have reinstated the persecutions of the earlier Roman emperors.Catholic Christianity, as opposed to Arianism and other ideologies deemed heretical, was declared to be the state religion of the Roman Empire on 27 February 380 by the decree "De fide catolica".
G PROMPT OF RATIONALE GENERATION========================================= Prompt =========================================# Memorize this piece of evidence in mind and use it as if you already know it.# Evidence: State religionDespite enjoying considerable popular support, # Instruction: When did christianity become official religion of Rome?# Response: 380 AD# Rationale: (complete the following sentence with details from the evidence; you can onlyuse the information from the evidence)======================================= Completion =======================================Catholic Christianity was declared the official state religion of the Roman Empire onFebruary 27th, 380, through the decree "De fide catolica".Xia et al. (2024b); Jiapeng et al. (2024); Shiet al. (2024). For instance, the Tree-of-Reviews (ToR) framework, introduced in Jiapeng et al.(2024), addresses multi-hop reasoning problems by dynamically initiating new searches based onpreviously retrieved documents and constructing various reasoning paths. This dynamic searchingstrategy can be integrated into our SPECULATIVE RAG, enabling each drafter to answer bridge-typequestions more effectively.
APPENDIX A INSTRUCTION-TUNING SETTINGSWe construct our training dataset for the RAG drafter from diverse instruction-following pairs.We sample instances from Open-Instruct processed data(Wang et al., 2023a)and knowledge-intensive datasets(Petroni et al., 2021;Stelmakh et al., 2022;Mihaylov et al., 2018).We augment the instruction-following pairs with retrieved documents and generated rationale.We use the off-theshelf dense retriever Contriever-MS MARCO(Izacard et al., 2021)to retrieve up to 10 documents for each pair and use Gemini-Ultra(Team et al., 2023)to generate rationale.In total, we acquire a dataset of 40k instances.We use Mistral 7B (v0.1) as our base LM for the RAG drafter.We reproduce the performance of Self-RAG(Asai et al., 2023)and CRAG(Yan et al., 2024)with Mistral 7B (v0.1) for a fair comparison.We implement the training scripts using the Transformers library from Hugging Face(Wolf et al., 2019).We employ DeepSpeed(Rasley et al., 2020)to accelerate the training process.All experiments are conducted on a Linux server equipped with 16 Nvidia A100-SXM4-40GB GPUs.Additionally, we replace Gemini-Ultra(Team et al., 2023)with GPT-4o(Achiam et al., 2023)when curating the instruction-tuning data for our RAG drafter to investigate the effects of different LLMs.These results demonstrate that SPECULATIVE RAG maintains its performance advantage, even when trained on data curated by GPT-4o.It consistently outperforms the baselines from Standard RAG, SelfRAG, and CRAG, further validating the effectiveness of our approach.Moreover, we use an instruction-tuned Gemma-2 2B(Team et al., 2024)as the RAG drafter and the frozen Mistral 7B or Mixtral 8x7B as the RAG verifier.We report the performance analysis in Table5.These results suggest that Gemma-2 2B provides a promising avenue for future work of further optimization.B EFFECTS OF INSTRUCTION TUNINGIn SPECULATIVE RAG, we introduce a framework that combines the RAG drafter and the verifier.In this ablation study, we directly feed all documents to the RAG drafter and generate one draft (m = 1, k = total # of documents).As shown in Table6, we observe that instruction tuning effectively enhances the document understanding capability of the RAG drafter, as it outperforms both Mistral 7B and Mistral-Instruct 7B .However, there remains a gap compared to SPECULATIVE RAG, showing the effectiveness of the drafting and verification framework.I PROMPT OF STANDARD RAG========================================= Prompt =========================================Below is an instruction that describes a task.Write a response that appropriately completes the request.### Evidence:[1] Britain (place name) Britain, after which "Britain" became the more commonplace name for the island called Great Britain.After the Anglo-Saxon period, "Britain" was used as a historical term only.Geoffrey of Monmouth in his pseudohistorical "Historia Regum Britanniae" ...[2] Great Britain The peoples of these islands of "Prettanike" were called the "Priteni" or "Pretani"."Priteni" is the source of the Welsh language term Prydain, "Britain", which has the same source as the Goidelic term Cruithne used to refer to the early Brythonic-speaking inhabitants of Ireland.The latter were later called Picts or Caledonians ... ...[10] Albion Albion is an alternative name for Great Britain.The oldest attestation of the toponym comes from the Greek language.It is sometimes used poetically and generally to refer to the island, but is less common than 'Britain' today.The name for Scotland in most of the Celtic languages is related to Albion: "Alba" in Scottish Gaelic, "Albain" ... ### Instruction: What was Britain called -before it was Britain?### Response: ### Instruction: What was Britain called -before it was Britain?### Evidence:[1] Britain (place name) Britain, after which "Britain" became the more commonplace name for the island called Great Britain.After the Anglo-Saxon period, "Britain" was used as a historical term only.Geoffrey of Monmouth in his pseudohistorical "Historia Regum Britanniae" ...[2] Great Britain The peoples of these islands of "Prettanike" were called the "Priteni" or "Pretani"." Priteni" is the source of the Welsh language term Prydain, "Britain", which has the same source as the Goidelic term Cruithne used to refer to the early Brythonic-speaking inhabitants of Ireland.The latter were later called Picts or Caledonians ...... [10] AlbionAlbion is an alternative name for Great Britain.The oldest attestation of the toponym comes from the Greek language.It is sometimes used poetically and generally to refer to the island, but is less common than 'Britain' today.The name for Scotland in most of the Celtic languages is related to Albion: "Alba" in Scottish Gaelic, "Albain" ... [/INST] The response is:
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, arXiv:2310.115112023arXiv preprint</p>
<p>Knowledge-augmented language model verification. Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C Park, Sung Hwang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, Tri Dao, arXiv:2401.10774Simple llm inference acceleration framework with multiple decoding heads. Medusa2024arXiv preprint</p>
<p>Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, John Jumper, arXiv:2302.01318Accelerating large language model decoding with speculative sampling. 2023aarXiv preprint</p>
<p>Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, Dong Yu, arXiv:2312.06648Dense x retrieval: What retrieval granularity should we use?. 2023barXiv preprint</p>
<p>Deep learning for instance retrieval: A survey. Wei Chen, Yu Liu, Weiping Wang, Erwin M Bakker, Theodoros Georgiou, Paul Fieguth, Li Liu, Michael S Lew, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2022</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei, arXiv:2307.02486Longnet: Scaling transformers to 1,000,000,000 tokens. 2023arXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. 202436</p>
<p>Knowledge card: Filling llms' knowledge gaps with plug-in specialized language models. Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023a</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023barXiv preprint</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister, ACL. 2023</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, 10.18653/v1/2021.eacl-main.74arXiv:2311.05232Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Paola Merlo, Jorg Tiedemann, Reut Tsarfaty, the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline2023. April 2021arXiv preprintLeveraging passage retrieval with generative models for open domain question answering. Association for Computational Linguistics</p>
<p>Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, arXiv:2112.091182021arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023aMistral 7b. arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Association for Computational Linguistics. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, 10.18653/v1/2023.emnlp-main.495Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeDecember 2023bActive retrieval augmented generation</p>
<p>Tree of reviews: A treebased dynamic iterative retrieval framework for multi-hop question answering. Li Jiapeng, Liu Runze, Li Yabo, Zhou Tong, Li Mingling, Chen Xiang, arXiv:2404.144642024arXiv preprint</p>
<p>K-means clustering. Encyclopedia of machine learning. Xin Jin, Jiawei Han, 2011</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>Evaluating open-domain question answering in the era of large language models. Ehsan Kamalloo, Nouha Dziri, Charles Clarke, Davood Rafiei, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Generalization through Memorization: Nearest Neighbor Language Models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, International Conference on Learning Representations (ICLR). 2020</p>
<p>Sure: Summarizing retrievals using answer candidates for open-domain qa of llms. Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, Jinwoo Shin, arXiv:2404.130812024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Fast inference from transformers via speculative decoding. Yaniv Leviathan, Matan Kalman, Yossi Matias, International Conference on Machine Learning. PMLR2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Long-context llms struggle with long in-context learning. Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen, arXiv:2404.020602024arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, James Glass, arXiv:2305.15225Sail: Search-augmented instruction learning. 2023aarXiv preprint</p>
<p>An empirical study of catastrophic forgetting in large language models during continual fine-tuning. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yue Zhang, arXiv:2308.087472023barXiv preprint</p>
<p>Query rewriting in retrievalaugmented large language models. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Megalodon: Efficient llm pretraining and inference with unlimited context length. Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou, arXiv:2404.088012024arXiv preprint</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Accelerating large language model serving with tree-based speculative inference and verification. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying, Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems20243</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Answer is all you need: Instruction-following text embedding via answering the question. Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang, arXiv:2402.096422024arXiv preprint</p>
<p>Kilt: a benchmark for knowledge intensive language tasks. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, arXiv:2403.055302024arXiv preprint</p>
<p>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D Manning, arXiv:2401.18059Raptor: Recursive abstractive processing for tree-organized retrieval. 2024arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202436</p>
<p>Generate-then-ground in retrieval-augmented generation for multi-hop question answering. Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, Zhaochun Ren, arXiv:2406.148912024arXiv preprint</p>
<p>Asqa: Factoid questions meet long-form answers. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Blockwise parallel decoding for deep autoregressive models. Mitchell Stern, Noam Shazeer, Jakob Uszkoreit, Advances in Neural Information Processing Systems. 201831</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.001182024arXiv preprint</p>
<p>Musique: Multihop questions via single-hop question composition. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, Transactions of the Association for Computational Linguistics. 102022</p>
<p>How far can camels go. Exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Raghavi Khyathi, David Chandu, Kelsey Wadden, Noah A Macmillan, Iz Smith, Beltagy, 2023a</p>
<p>Learning to filter context for retrieval-augmented generation. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig, arXiv:2311.083772023barXiv preprint</p>
<p>Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, Yitao Liang, arXiv:2403.053132024arXiv preprint</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, Zhifang Sui, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui, arXiv:2401.07851Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. 2024aarXiv preprint</p>
<p>Ground every sentence: Improving retrieval-augmented llms with interleaved referenceclaim generation. Sirui Xia, Xintao Wang, Jiaqing Liang, Yifei Zhang, Weikang Zhou, Jiaji Deng, Fei Yu, Yanghua Xiao, arXiv:2407.017962024barXiv preprint</p>
<p>Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Recomp: Improving retrieval-augmented lms with compression and selective augmentation. Fangyuan Xu, Weijia Shi, Eunsol Choi, arXiv:2310.044082023arXiv preprint</p>
<p>Jia-Chen Shi-Qi Yan, Yun Gu, Zhen-Hua Zhu, Ling, arXiv:2401.15884Corrective retrieval augmented generation. 2024arXiv preprint</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Making retrieval-augmented language models robust to irrelevant context. Tomer Ori Yoran, Ori Wolfson, Jonathan Ram, Berant, arXiv:2310.015582023arXiv preprint</p>
<p>Chain-of-note: Enhancing robustness in retrieval-augmented language models. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu, arXiv:2311.092102023arXiv preprint</p>
<p>Draft &amp; verify: Lossless large language model acceleration via self-speculative decoding. Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad Mehrotra, arXiv:2309.081682023aarXiv preprint</p>
<p>Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, arXiv:2304.03728Interpretable unified language checking. Helen Meng, and James Glass2023barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
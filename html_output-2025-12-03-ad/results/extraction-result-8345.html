<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8345 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8345</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8345</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-272881203</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.16383v4.pdf" target="_blank">RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation</a></p>
                <p><strong>Paper Abstract:</strong> Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in abstract thinking and creative problem-solving, often revealing limitations in their cognitive abilities. In this paper, we examine the riddle-solving capabilities of LLMs using a multiple-choice format, exploring how different prompting techniques impact performance on riddles that demand diverse reasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with COntext REcontruciton) a novel fully automated prompting method that generates and utilizes contextually reconstructed sentence-based puzzles in conjunction with the original examples to create few-shot exemplars. Our experiments demonstrate that RISCORE significantly improves the performance of language models in both vertical and lateral thinking tasks, surpassing traditional exemplar selection strategies across a variety of few-shot settings.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8345.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8345.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RISCORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RIddle Solving with COntext REconstruction (RISCORE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting augmentation that pairs each few-shot exemplar with a contextually reconstructed version that preserves the underlying reasoning pattern while changing surface context, to promote reasoning-similar (analogy-like) exemplars over purely semantically similar ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RISCORE (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a language model — a prompting/example-augmentation strategy applied to various LLMs in few-shot MQA riddle tasks; supports manual reconstructions (RISCORE_m) and automated reconstructions (RISCORE).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot exemplar-based in-context learning', 'reasoning-pattern (context-reconstructed) exemplar analogy', 'semantic-similarity exemplar retrieval (used for placement)', 'chain-of-thought (when combined with CoT prompting in baselines)', 'zero-shot generation of reconstructed exemplars']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>RISCORE augments a standard few-shot prompt by including, for each selected training exemplar, an additional example that is a context-reconstructed version preserving the same reasoning path (analogy-style). Exemplar placement still uses semantic-similarity retrieval, but the exemplars themselves prioritize reasoning-pattern similarity. Reconstructions were produced manually (RISCORE_m) where available or generated automatically via LLMs with filtering; distractors are generated/filtered separately.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparison vs. standard few-shot with semantic-similarity (FS Sim), random few-shot (FS Rand), and few-shot with CoT explanations (CoT FS). Ablations include: manual reconstructions (RISCORE_m) vs. automated reconstructions (RISCORE); varying shot counts (2/4/8); mixing N/2 original exemplars + N/2 reconstructions vs. full FS sets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser (lateral-thinking multiple-choice riddles with manual semantic/ context reconstructions) and RiddleSense (vertical-thinking riddles, multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Across models and datasets RISCORE yields consistent gains vs. FS Sim: (BrainTeaser) Llama3-70B: FS Sim 8-shot = 0.783 → RISCORE 8-shot = 0.808 (+0.025). Example: FS Sim 2-shot = 0.825, FS Sim 4-shot = 0.792 (performance dropped when adding semantically similar shots); RISCORE_m 2+2-shot = 0.833 (≈+0.041 vs FS Sim 4-shot). (RiddleSense) Llama3-8B: FS Sim 8-shot = 0.681 → RISCORE 8-shot = 0.708 (+~0.027, ~2–3%). Qwen2-7B: RISCORE 2/4-shot showed ~2.5% improvement vs FS Sim same-shot. Improvements are generally larger with smaller shot budgets and when manual reconstructions (RISCORE_m) are available.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>RISCORE mitigates noise introduced by suboptimal semantically-similar shot selection and helps models latch onto the intended reasoning path; manual reconstructions form an upper bound (larger gains) while automated reconstructions still give consistent improvements; benefit is most pronounced in limited-shot settings; RISCORE performance depends on quality of initial semantic selection (may miss better-ranked examples if constrained to N/2 originals).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Augmenting few-shot exemplars with contextually reconstructed counterparts that preserve reasoning patterns improves riddle-solving performance across lateral and vertical tasks, outperforming standard semantic-similarity exemplar selection in most settings; automated reconstructions deliver consistent gains though smaller than manually curated reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8345.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8345.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 3 - 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion parameter instruction-tuned Llama3 model used as both the target model and as a generator for reconstructed QA pairs and distractors in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Llama3 with ~70B parameters; used in quantized and unquantized form for inference and for generation of reconstructed examples/distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot Chain-of-Thought prompting (CoT_ZS)', 'few-shot exemplar prompting (FS Rand, FS Sim)', 'few-shot with CoT explanations (CoT_FS)', 'RISCORE (paired context-reconstructions, manual and automated)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT_ZS: prompt includes 'Let's think step by step' in system prompt without exemplars. FS Rand: randomly chosen few-shot exemplars. FS Sim: semantically-similar exemplars selected by embedding cosine similarity. CoT_FS: few-shot exemplars with manually curated chain-of-thought explanations. RISCORE: each exemplar paired with a context-reconstructed counterpart (manual or automated).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared CoT_ZS vs FS (Rand/Sim) vs CoT_FS vs RISCORE_m vs automated RISCORE; ablated shots (2 / 4 / 8) and distractor generation sources; evaluated effect of adding semantically similar shots vs adding reconstructions (N/2 originals + N/2 reconstructions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser (lateral) and RiddleSense (vertical) multiple-choice riddles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>BrainTeaser (examples): FS Sim 2-shot = 0.825; FS Sim 4-shot = 0.792 (drop when adding semantically similar examples); RISCORE_m (2+2-shot) = 0.833 (+0.041 over FS Sim 4-shot). FS Sim 8-shot = 0.783 → RISCORE (automated) 8-shot = 0.808 (+0.025). CoT_FS often underperforms FS: CoT_FS 2-shot ≈0.850 but in some quantized setups CoT_FS underperforms general FS settings (paper reports CoT_FS underperforming overall). (Numbers vary across quantization and distractor sources; see main tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>This large model benefits substantially from RISCORE, especially when manual reconstructions are available; it can be sensitive to addition of semantically similar examples (performance can decrease), while RISCORE reduces such sensitivity by clarifying reasoning paths. Automated reconstructions also improve accuracy though less than manual reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>On Llama3-70B, example augmentation with context-reconstructed exemplars yields clear and consistent gains over semantic-similarity few-shot exemplars and mitigates noise from poor shot selection; manual reconstructions provide the largest benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8345.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8345.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 3 - 8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion parameter instruction-tuned Llama3 used both as an evaluated model and as a generator for reconstructions in some experimental settings; lower capacity affects lateral-reasoning reconstruction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 8B Llama3; used for inference (non-quantized) and for generating reconstruction/distractors in some pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot CoT prompting', 'few-shot (FS Rand, FS Sim)', 'RISCORE (automated reconstructions when viable)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same prompting families as larger model: zero-shot CoT uses 'Let's think step by step'; FS Sim uses semantic retrieval for exemplar selection; RISCORE pairs chosen exemplars with automated reconstructed counterparts produced by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared FS Sim vs RISCORE (automated) across shots 2/4/8, and evaluated reconstructions generated by Llama3-8B vs Llama3-70B. Also measured effect on BrainTeaser (lateral) vs RiddleSense (vertical): smaller model struggles generating high-quality lateral reconstructions but works for vertical.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser (lateral) and RiddleSense (vertical)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>RiddleSense example: FS Sim 8-shot = 0.681 → RISCORE 8-shot = 0.708 (≈+0.027, ~2–3% absolute improvement). For BrainTeaser, Llama3-8B had mixed reconstruction quality; automated RISCORE sometimes improved performance but smaller model struggled to generate high-quality lateral-reasoning reconstructed QA pairs (paper reports Llama3-8B not suitable for automated reconstructions on BrainTeaser).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Smaller model can generate effective context reconstructions for vertical (rule-based/linear) riddles but fails to produce high-quality lateral reconstructions that preserve intended reasoning; RISCORE gains on RiddleSense indicate that reasoning-pattern reconstructions can aid even smaller models when the reconstruction quality is adequate.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Llama3-8B benefits from RISCORE on vertical-thinking tasks (RiddleSense), but its limited capacity reduces reconstruction quality for lateral tasks (BrainTeaser), limiting RISCORE's effectiveness when using the smaller model to generate reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8345.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8345.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B instruction-tuned Mistral model evaluated as a smaller reasoner in the experiments; used only as a target model for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unquantized 7-billion parameter instruction-tuned Mistral model used for inference experiments across FS, CoT, and RISCORE setups.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot (FS Rand, FS Sim)', 'few-shot with CoT explanations (CoT_FS)', 'RISCORE (automated reconstructions applied externally)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>FS Sim: semantic-similarity selected exemplars. CoT_FS: few-shot exemplars with CoT explanations. RISCORE: half originals + their reconstructions included in prompt. The model itself is prompted; reconstructions/distractors are generated by larger models in some pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared FS Sim vs RISCORE (automatic) under various shot counts (2/4/8). The paper reports cases where adding semantically similar examples reduced performance for smaller models, and RISCORE often recovered or improved accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser and RiddleSense multiple-choice riddles</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Example from BrainTeaser: FS Sim 2-shot = 0.517; adding two more semantically similar examples reduced score to 0.458; RISCORE 2+2-shot = 0.567 (substantial recovery/improvement vs FS Sim 4-shot). RISCORE gains were sometimes larger on smaller models, e.g., up to ~10% absolute improvement in some 4-shot conditions reported.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Smaller models are more sensitive to noisy exemplar selection; RISCORE's reasoning-similar exemplars mitigate noise from suboptimal semantic selections and can provide relatively larger gains than for bigger models. CoT_FS provides inconsistent improvements and may underperform plain FS for riddles.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Mistral-7B benefits from RISCORE especially when semantic exemplar addition hurts performance; emphasizing reasoning-pattern similarity via reconstructions can notably improve small-model performance on riddles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8345.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8345.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-8x7B (Mixtral)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral / Mistral ensemble - 8x7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-like Mixtral model composed of eight 7B components (denoted 8x7B), used as a mid-sized model in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An 8x7B combined/instruction-tuned model (Mixtral) used unquantized in experiments to evaluate prompting strategies' effects on reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot CoT prompting', 'few-shot FS Rand / FS Sim', 'CoT_FS (few-shot with explanations)', 'RISCORE (automated reconstructions)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same prompting families as other models; Mixtral serves to probe mid-sized model behavior under diverse prompting and reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Evaluated across CoT_ZS, FS Rand, FS Sim, CoT_FS, and RISCORE with 2/4/8 shots; comparisons highlight where RISCORE improves vs FS Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser and RiddleSense</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported averages: Mixtral RISCORE Sim 8-shot on BrainTeaser gives average ≈0.683 in some setups; improvements vs FS Sim are present but more moderate than for some other models. Specific numbers vary by distractor-source and reconstruction generator; overall trend: RISCORE often matches or slightly exceeds FS Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Mixtral shows consistent but moderate benefits from RISCORE; improvements are more modest compared to largest model but still positive, indicating method generalizes across model scales.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>RISCORE benefits are observable for Mixtral models, confirming that context-reconstructed exemplars help mid-sized models better align on reasoning patterns compared to pure semantic exemplar selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8345.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8345.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2 - 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B instruction-tuned Qwen2 model used to evaluate prompting variants on riddle tasks and to test RISCORE's applicability across diverse LLM families.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7-billion-parameter Qwen2 instruction-tuned model, evaluated in FS, CoT and RISCORE setups.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot FS Rand / FS Sim', 'CoT_FS', 'RISCORE (automated reconstructions)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompts and exemplar selection mirror other models: semantically-similar exemplar retrieval for FS Sim, CoT explanations for CoT_FS, and inclusion of reconstructed exemplars for RISCORE.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared FS Sim vs RISCORE across shot counts (2/4/8). Measured relative percent improvements: example reported ~+2.5% absolute improvement for 2- and 4-shot RISCORE over FS Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser and RiddleSense multiple-choice riddles</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reports ~2.5% absolute improvement in some RISCORE 2-/4-shot settings vs FS Sim. Example aggregates in tables: Qwen2 FS Sim 2-shot ≈0.722, RISCORE 2-shot ≈0.625–0.650 in some configurations (performance depends on distractor/source pairing); overall relative gains of RISCORE vs FS Sim are reported but vary by exact setup.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Qwen2 shows modest but consistent gains from RISCORE in low-shot regimes; distractor quality and exemplar-selection constraints influence absolute numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>RISCORE delivers moderate improvements for Qwen2-7B, particularly in limited-shot settings, supporting the claim that reasoning-pattern exemplars help across different LLM families.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>BRAINTEASER: Lateral thinking puzzles for large language models <em>(Rating: 2)</em></li>
                <li>A survey on in-context learning <em>(Rating: 2)</em></li>
                <li>What makes good in-context examples for GPT-3? <em>(Rating: 2)</em></li>
                <li>Complexity-based prompting for multi-step reasoning <em>(Rating: 1)</em></li>
                <li>Same task, more tokens: the impact of input length on the reasoning performance of large language models <em>(Rating: 1)</em></li>
                <li>Improving the diversity of exemplars for in-context learning (Zhang et al., 2022) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8345",
    "paper_id": "paper-272881203",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "RISCORE",
            "name_full": "RIddle Solving with COntext REconstruction (RISCORE)",
            "brief_description": "A prompting augmentation that pairs each few-shot exemplar with a contextually reconstructed version that preserves the underlying reasoning pattern while changing surface context, to promote reasoning-similar (analogy-like) exemplars over purely semantically similar ones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RISCORE (method)",
            "model_description": "Not a language model — a prompting/example-augmentation strategy applied to various LLMs in few-shot MQA riddle tasks; supports manual reconstructions (RISCORE_m) and automated reconstructions (RISCORE).",
            "reasoning_methods": [
                "few-shot exemplar-based in-context learning",
                "reasoning-pattern (context-reconstructed) exemplar analogy",
                "semantic-similarity exemplar retrieval (used for placement)",
                "chain-of-thought (when combined with CoT prompting in baselines)",
                "zero-shot generation of reconstructed exemplars"
            ],
            "reasoning_methods_description": "RISCORE augments a standard few-shot prompt by including, for each selected training exemplar, an additional example that is a context-reconstructed version preserving the same reasoning path (analogy-style). Exemplar placement still uses semantic-similarity retrieval, but the exemplars themselves prioritize reasoning-pattern similarity. Reconstructions were produced manually (RISCORE_m) where available or generated automatically via LLMs with filtering; distractors are generated/filtered separately.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparison vs. standard few-shot with semantic-similarity (FS Sim), random few-shot (FS Rand), and few-shot with CoT explanations (CoT FS). Ablations include: manual reconstructions (RISCORE_m) vs. automated reconstructions (RISCORE); varying shot counts (2/4/8); mixing N/2 original exemplars + N/2 reconstructions vs. full FS sets.",
            "task_or_benchmark": "BrainTeaser (lateral-thinking multiple-choice riddles with manual semantic/ context reconstructions) and RiddleSense (vertical-thinking riddles, multiple-choice).",
            "performance_results": "Across models and datasets RISCORE yields consistent gains vs. FS Sim: (BrainTeaser) Llama3-70B: FS Sim 8-shot = 0.783 → RISCORE 8-shot = 0.808 (+0.025). Example: FS Sim 2-shot = 0.825, FS Sim 4-shot = 0.792 (performance dropped when adding semantically similar shots); RISCORE_m 2+2-shot = 0.833 (≈+0.041 vs FS Sim 4-shot). (RiddleSense) Llama3-8B: FS Sim 8-shot = 0.681 → RISCORE 8-shot = 0.708 (+~0.027, ~2–3%). Qwen2-7B: RISCORE 2/4-shot showed ~2.5% improvement vs FS Sim same-shot. Improvements are generally larger with smaller shot budgets and when manual reconstructions (RISCORE_m) are available.",
            "qualitative_findings": "RISCORE mitigates noise introduced by suboptimal semantically-similar shot selection and helps models latch onto the intended reasoning path; manual reconstructions form an upper bound (larger gains) while automated reconstructions still give consistent improvements; benefit is most pronounced in limited-shot settings; RISCORE performance depends on quality of initial semantic selection (may miss better-ranked examples if constrained to N/2 originals).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Augmenting few-shot exemplars with contextually reconstructed counterparts that preserve reasoning patterns improves riddle-solving performance across lateral and vertical tasks, outperforming standard semantic-similarity exemplar selection in most settings; automated reconstructions deliver consistent gains though smaller than manually curated reconstructions.",
            "uuid": "e8345.0",
            "source_info": {
                "paper_title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Meta Llama 3 - 70B Instruct",
            "brief_description": "A 70-billion parameter instruction-tuned Llama3 model used as both the target model and as a generator for reconstructed QA pairs and distractors in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-70B-Instruct",
            "model_description": "Instruction-tuned Llama3 with ~70B parameters; used in quantized and unquantized form for inference and for generation of reconstructed examples/distractors.",
            "reasoning_methods": [
                "zero-shot Chain-of-Thought prompting (CoT_ZS)",
                "few-shot exemplar prompting (FS Rand, FS Sim)",
                "few-shot with CoT explanations (CoT_FS)",
                "RISCORE (paired context-reconstructions, manual and automated)"
            ],
            "reasoning_methods_description": "CoT_ZS: prompt includes 'Let's think step by step' in system prompt without exemplars. FS Rand: randomly chosen few-shot exemplars. FS Sim: semantically-similar exemplars selected by embedding cosine similarity. CoT_FS: few-shot exemplars with manually curated chain-of-thought explanations. RISCORE: each exemplar paired with a context-reconstructed counterpart (manual or automated).",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared CoT_ZS vs FS (Rand/Sim) vs CoT_FS vs RISCORE_m vs automated RISCORE; ablated shots (2 / 4 / 8) and distractor generation sources; evaluated effect of adding semantically similar shots vs adding reconstructions (N/2 originals + N/2 reconstructions).",
            "task_or_benchmark": "BrainTeaser (lateral) and RiddleSense (vertical) multiple-choice riddles.",
            "performance_results": "BrainTeaser (examples): FS Sim 2-shot = 0.825; FS Sim 4-shot = 0.792 (drop when adding semantically similar examples); RISCORE_m (2+2-shot) = 0.833 (+0.041 over FS Sim 4-shot). FS Sim 8-shot = 0.783 → RISCORE (automated) 8-shot = 0.808 (+0.025). CoT_FS often underperforms FS: CoT_FS 2-shot ≈0.850 but in some quantized setups CoT_FS underperforms general FS settings (paper reports CoT_FS underperforming overall). (Numbers vary across quantization and distractor sources; see main tables.)",
            "qualitative_findings": "This large model benefits substantially from RISCORE, especially when manual reconstructions are available; it can be sensitive to addition of semantically similar examples (performance can decrease), while RISCORE reduces such sensitivity by clarifying reasoning paths. Automated reconstructions also improve accuracy though less than manual reconstructions.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "On Llama3-70B, example augmentation with context-reconstructed exemplars yields clear and consistent gains over semantic-similarity few-shot exemplars and mitigates noise from poor shot selection; manual reconstructions provide the largest benefit.",
            "uuid": "e8345.1",
            "source_info": {
                "paper_title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama3-8B",
            "name_full": "Meta Llama 3 - 8B Instruct",
            "brief_description": "An 8-billion parameter instruction-tuned Llama3 used both as an evaluated model and as a generator for reconstructions in some experimental settings; lower capacity affects lateral-reasoning reconstruction quality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-8B-Instruct",
            "model_description": "Instruction-tuned 8B Llama3; used for inference (non-quantized) and for generating reconstruction/distractors in some pipelines.",
            "reasoning_methods": [
                "zero-shot CoT prompting",
                "few-shot (FS Rand, FS Sim)",
                "RISCORE (automated reconstructions when viable)"
            ],
            "reasoning_methods_description": "Same prompting families as larger model: zero-shot CoT uses 'Let's think step by step'; FS Sim uses semantic retrieval for exemplar selection; RISCORE pairs chosen exemplars with automated reconstructed counterparts produced by LLMs.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared FS Sim vs RISCORE (automated) across shots 2/4/8, and evaluated reconstructions generated by Llama3-8B vs Llama3-70B. Also measured effect on BrainTeaser (lateral) vs RiddleSense (vertical): smaller model struggles generating high-quality lateral reconstructions but works for vertical.",
            "task_or_benchmark": "BrainTeaser (lateral) and RiddleSense (vertical)",
            "performance_results": "RiddleSense example: FS Sim 8-shot = 0.681 → RISCORE 8-shot = 0.708 (≈+0.027, ~2–3% absolute improvement). For BrainTeaser, Llama3-8B had mixed reconstruction quality; automated RISCORE sometimes improved performance but smaller model struggled to generate high-quality lateral-reasoning reconstructed QA pairs (paper reports Llama3-8B not suitable for automated reconstructions on BrainTeaser).",
            "qualitative_findings": "Smaller model can generate effective context reconstructions for vertical (rule-based/linear) riddles but fails to produce high-quality lateral reconstructions that preserve intended reasoning; RISCORE gains on RiddleSense indicate that reasoning-pattern reconstructions can aid even smaller models when the reconstruction quality is adequate.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Llama3-8B benefits from RISCORE on vertical-thinking tasks (RiddleSense), but its limited capacity reduces reconstruction quality for lateral tasks (BrainTeaser), limiting RISCORE's effectiveness when using the smaller model to generate reconstructions.",
            "uuid": "e8345.2",
            "source_info": {
                "paper_title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B Instruct",
            "brief_description": "A 7B instruction-tuned Mistral model evaluated as a smaller reasoner in the experiments; used only as a target model for inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.2",
            "model_description": "Unquantized 7-billion parameter instruction-tuned Mistral model used for inference experiments across FS, CoT, and RISCORE setups.",
            "reasoning_methods": [
                "few-shot (FS Rand, FS Sim)",
                "few-shot with CoT explanations (CoT_FS)",
                "RISCORE (automated reconstructions applied externally)"
            ],
            "reasoning_methods_description": "FS Sim: semantic-similarity selected exemplars. CoT_FS: few-shot exemplars with CoT explanations. RISCORE: half originals + their reconstructions included in prompt. The model itself is prompted; reconstructions/distractors are generated by larger models in some pipelines.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared FS Sim vs RISCORE (automatic) under various shot counts (2/4/8). The paper reports cases where adding semantically similar examples reduced performance for smaller models, and RISCORE often recovered or improved accuracy.",
            "task_or_benchmark": "BrainTeaser and RiddleSense multiple-choice riddles",
            "performance_results": "Example from BrainTeaser: FS Sim 2-shot = 0.517; adding two more semantically similar examples reduced score to 0.458; RISCORE 2+2-shot = 0.567 (substantial recovery/improvement vs FS Sim 4-shot). RISCORE gains were sometimes larger on smaller models, e.g., up to ~10% absolute improvement in some 4-shot conditions reported.",
            "qualitative_findings": "Smaller models are more sensitive to noisy exemplar selection; RISCORE's reasoning-similar exemplars mitigate noise from suboptimal semantic selections and can provide relatively larger gains than for bigger models. CoT_FS provides inconsistent improvements and may underperform plain FS for riddles.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Mistral-7B benefits from RISCORE especially when semantic exemplar addition hurts performance; emphasizing reasoning-pattern similarity via reconstructions can notably improve small-model performance on riddles.",
            "uuid": "e8345.3",
            "source_info": {
                "paper_title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Mistral-8x7B (Mixtral)",
            "name_full": "Mixtral / Mistral ensemble - 8x7B Instruct",
            "brief_description": "An ensemble-like Mixtral model composed of eight 7B components (denoted 8x7B), used as a mid-sized model in evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B-Instruct-v0.1",
            "model_description": "An 8x7B combined/instruction-tuned model (Mixtral) used unquantized in experiments to evaluate prompting strategies' effects on reasoning tasks.",
            "reasoning_methods": [
                "zero-shot CoT prompting",
                "few-shot FS Rand / FS Sim",
                "CoT_FS (few-shot with explanations)",
                "RISCORE (automated reconstructions)"
            ],
            "reasoning_methods_description": "Same prompting families as other models; Mixtral serves to probe mid-sized model behavior under diverse prompting and reconstructions.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Evaluated across CoT_ZS, FS Rand, FS Sim, CoT_FS, and RISCORE with 2/4/8 shots; comparisons highlight where RISCORE improves vs FS Sim.",
            "task_or_benchmark": "BrainTeaser and RiddleSense",
            "performance_results": "Reported averages: Mixtral RISCORE Sim 8-shot on BrainTeaser gives average ≈0.683 in some setups; improvements vs FS Sim are present but more moderate than for some other models. Specific numbers vary by distractor-source and reconstruction generator; overall trend: RISCORE often matches or slightly exceeds FS Sim.",
            "qualitative_findings": "Mixtral shows consistent but moderate benefits from RISCORE; improvements are more modest compared to largest model but still positive, indicating method generalizes across model scales.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "RISCORE benefits are observable for Mixtral models, confirming that context-reconstructed exemplars help mid-sized models better align on reasoning patterns compared to pure semantic exemplar selection.",
            "uuid": "e8345.4",
            "source_info": {
                "paper_title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Qwen2-7B",
            "name_full": "Qwen2 - 7B Instruct",
            "brief_description": "A 7B instruction-tuned Qwen2 model used to evaluate prompting variants on riddle tasks and to test RISCORE's applicability across diverse LLM families.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2-7B-Instruct",
            "model_description": "7-billion-parameter Qwen2 instruction-tuned model, evaluated in FS, CoT and RISCORE setups.",
            "reasoning_methods": [
                "few-shot FS Rand / FS Sim",
                "CoT_FS",
                "RISCORE (automated reconstructions)"
            ],
            "reasoning_methods_description": "Prompts and exemplar selection mirror other models: semantically-similar exemplar retrieval for FS Sim, CoT explanations for CoT_FS, and inclusion of reconstructed exemplars for RISCORE.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared FS Sim vs RISCORE across shot counts (2/4/8). Measured relative percent improvements: example reported ~+2.5% absolute improvement for 2- and 4-shot RISCORE over FS Sim.",
            "task_or_benchmark": "BrainTeaser and RiddleSense multiple-choice riddles",
            "performance_results": "Paper reports ~2.5% absolute improvement in some RISCORE 2-/4-shot settings vs FS Sim. Example aggregates in tables: Qwen2 FS Sim 2-shot ≈0.722, RISCORE 2-shot ≈0.625–0.650 in some configurations (performance depends on distractor/source pairing); overall relative gains of RISCORE vs FS Sim are reported but vary by exact setup.",
            "qualitative_findings": "Qwen2 shows modest but consistent gains from RISCORE in low-shot regimes; distractor quality and exemplar-selection constraints influence absolute numbers.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "RISCORE delivers moderate improvements for Qwen2-7B, particularly in limited-shot settings, supporting the claim that reasoning-pattern exemplars help across different LLM families.",
            "uuid": "e8345.5",
            "source_info": {
                "paper_title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "BRAINTEASER: Lateral thinking puzzles for large language models",
            "rating": 2,
            "sanitized_title": "brainteaser_lateral_thinking_puzzles_for_large_language_models"
        },
        {
            "paper_title": "A survey on in-context learning",
            "rating": 2,
            "sanitized_title": "a_survey_on_incontext_learning"
        },
        {
            "paper_title": "What makes good in-context examples for GPT-3?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_examples_for_gpt3"
        },
        {
            "paper_title": "Complexity-based prompting for multi-step reasoning",
            "rating": 1,
            "sanitized_title": "complexitybased_prompting_for_multistep_reasoning"
        },
        {
            "paper_title": "Same task, more tokens: the impact of input length on the reasoning performance of large language models",
            "rating": 1,
            "sanitized_title": "same_task_more_tokens_the_impact_of_input_length_on_the_reasoning_performance_of_large_language_models"
        },
        {
            "paper_title": "Improving the diversity of exemplars for in-context learning (Zhang et al., 2022)",
            "rating": 1,
            "sanitized_title": "improving_the_diversity_of_exemplars_for_incontext_learning_zhang_et_al_2022"
        }
    ],
    "cost": 0.013526199999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation
17 Dec 2024</p>
<p>Ioannis Panagiotopoulos 
School of Electrical and Computer Engineering
AILS Laboratory National Technical University of Athens</p>
<p>Giorgos Filandrianos 
School of Electrical and Computer Engineering
AILS Laboratory National Technical University of Athens</p>
<p>Maria Lymperaiou 
School of Electrical and Computer Engineering
AILS Laboratory National Technical University of Athens</p>
<p>Giorgos Stamou 
School of Electrical and Computer Engineering
AILS Laboratory National Technical University of Athens</p>
<p>An Yang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Baosong Yang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Binyuan Hui 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Bo Zheng 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Bowen Yu 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Chang Zhou 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Chengpeng Li 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Chengyuan Li 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Dayiheng Liu 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Fei Huang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Guanting Dong 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Hao- Ran Wei 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Huan Lin 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Jialong Tang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Jialin Wang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Jianxin Yang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Xiaodong Deng
Xiaohuan Zhou</p>
<p>Jianhong Tu 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Jianwei Zhang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Jianxin Ma 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Jin Xu 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Jingren Zhou 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Jinze Bai 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Jinzheng He 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Junyang Lin 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Kai Dang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Keming Lu 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Ke- Qin Chen 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Kexin Yang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Mei Li 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Mingfeng Xue 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Na Ni 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Pei Zhang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Peng Wang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Ru Peng 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Rui Men 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Ruize Gao 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Runji Lin 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Shijie Wang 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Shuai Bai 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Sinan Tan 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Tianhang Zhu 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Tianhao Li 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Tianyu Liu 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Wenbin Ge 
Xiaodong Deng
Xiaohuan Zhou</p>
<p>Xin Zhang 
Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Yanzhao Zhang 
Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Dingkun Long 
Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Wen Xie 
Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Ziqi Dai 
Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Pengjun Xie 
Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>Meishan Zhang 
Xingzhang Ren
Xipin Wei, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu CuiXinyu Zhang, Xuancheng Ren, Xuejing Liu, Zhenru Zhang</p>
<p>RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation
17 Dec 2024ABA9598F7A611B4D7FAF1B3F6111D8C9arXiv:2409.16383v4[cs.CL]
Riddle-solving requires advanced reasoning skills, pushing Large Language Models (LLMs) to engage in abstract thinking and creative problem-solving, often revealing limitations in their cognitive abilities.In this paper, we examine the riddle-solving capabilities of LLMs using a multiple-choice format, exploring how different prompting techniques impact performance on riddles that demand diverse reasoning skills.To enhance results, we introduce RISCORE (RIddle Solving with COntext REcontruciton), a novel fully automated prompting method that generates and utilizes contextually reconstructed sentence-based puzzles in conjunction with the original examples to create few-shot exemplars.Our experiments demonstrate that RISCORE significantly improves the performance of language models in both vertical and lateral thinking tasks, surpassing traditional exemplar selection strategies across a variety of few-shot settings 1 .</p>
<p>Introduction</p>
<p>Reasoning in Natural Language Processing has been a field of increasing interest, especially since the surge of Large Language Models (LLMs), which showcase several reasoning shortcomings (Yu et al., 2024;Qiao et al., 2023).The logical gap between humans and LLMs' advanced reasoning is further exposed when probing the puzzle solving capabilities of such models (Giadikiaroglou et al., 2024), denoting the necessity of intricate datasets (Lin et al., 2021;Szomiu and Groza, 2021;Zhang and Wan, 2022) that test and enhance complex model reasoning.In terms of advanced reasoning skills, lateral thinking datasets such as BrainTeaser (Jiang et al., 2023b) stress the creative reasoning capabilities of models, requiring them to defy obvious logical associations and approach a more abstract way of thinking.At the same time, prompting is a widespread technique of extracting reasoning patterns from LLMs (Qiao et al., 2023).Especially Chain-of-Thought (CoT) prompting (Wei et al., 2023), as naturally designed to trigger the extraction of reasoning chains, is indeed proven to be one successful and viable prompting strategy in practice, even though its functionality is only applicable on larger models.Other promising techniques, such as incontext learning, are highly capable when the right data samples are utilized as exemplars, but fail to properly drive LLMs otherwise (Dong et al., 2024).While similarity-driven exemplar selection serves as a well-established rule of thumb (Liu et al., 2022;Qin et al., 2024;Dong et al., 2024), the notion of similarity becomes more obscure when concerning reasoning patterns: for example, the sentences "A man shaves everyday, yet keeps his beard long" and "Tom attends class every day but doesn't do any homework" (Jiang et al., 2023b) in fact represent the exact same reasoning pattern, despite being rather distant semantically.</p>
<p>Driven by this observation, we assume that by "stripping" a sentence from linguistic features, while focusing on pure reasoning patterns prevalent in sentences, we can provide the model with the critical information to unlock the desired underlying reasoning pattern.Sentences that represent the same reasoning pattern are provided as "context reconstruction" counterparts in BrainTeaser data (Jiang et al., 2023b) or can be automatically generated using our proposed method.Therefore, incorporating such reconstructed instances as fewshot exemplars, together with their original counterparts, offers an alternative few-shot strategy that prioritizes reasoning similarity and analogy.An example of the method is provided in Figure 2. To this end, we contribute to the following:</p>
<p>• We experimentally verify that providing a riddle along with its context reconstruction can enhance performance on both lateral and vertical thinking problems.</p>
<p>• We propose RISCORE (Example provided in Figure 1), a novel prompting method designed to enhance the in-context riddle-solving capabilities of LLMs.As a supplement to this, we introduce an algorithm for generating contextually reconstructed multiple-choice riddles.</p>
<p>• We compare RISCORE against a wide array of popular prompting techniques, highlighting its effectiveness across several prompting alternatives and models of varying sizes.</p>
<p>Related work</p>
<p>Reasoning with Language Models Multiple reasoning senses have been studied over the years using language models, including commonsense (Sap et al., 2020), arithmetic (Luo et al., 2024), abductive (Zhao et al., 2023), inductive (Han et al., 2024), deductive (Sanyal et al., 2022), analogical reasoning (Sultan and Shahaf, 2022) and others.Linear thinking processes that exploit rules and logic, termed as "vertical thinking", have been previously explored within popular datasets (Lin et al., 2021;Bisk et al., 2020), unveiling interesting reasoning patterns of language models.On the other hand, creative thinking has been widely underexplored and often purposefully excluded from reasoning benchmarks (Speer et al., 2017;Sap et al., 2019), leading to a significant gap in literature, especially given the emergent capabilities that larger models present (Wei et al., 2022).Puzzle solving closely lies to creative reasoning (Giadikiaroglou et al., 2024;Lin et al., 2021;Zhang and Wan, 2022), since out-of-the-box thinking is mainly required.Going one step further, overriding default presuppositions and associations occurring when reasoning leads to "lateral thinking" processes, leading to more tricky puzzles, as demonstrated in the Brain-Teaser dataset (Jiang et al., 2023b) for the first time.</p>
<p>In our work, we focus on probing vertical and lateral puzzle-solving reasoning abilities of language models via prompting.</p>
<p>LLMs and prompting Discovering reasoning patterns in LLMs is often performed via various types of prompting (Qiao et al., 2023).Zero-shot strategies harness simple but intuitive instructions, such as "Let's think step-by-step" that successfully improve LLM reasoning (Kojima et al., 2023); however, the search space of such magic prompts seems indefinite.Querying for intermediate reasoning steps was framed in the seminal Chain-of-Thought (CoT) prompting (Wei et al., 2023), proving that it is sufficient to elicit reasoning in larger models.Nevertheless, the placement of demonstrations in few-shot prompting faces several challenges (Dong et al., 2024); specifically, the way of selecting these exemplars themselves is prone to instabilities (Lu et al., 2022;Min et al., 2022), proving the nonnegotiable need for developing optimal exemplar selection techniques.Similarity-based retrieval is established as a default selection technique (Liu et al., 2022;Qin et al., 2024;Dong et al., 2024), with related enhancement optimizing the ordering of selected exemplars (Wu et al., 2023).With a focus on reasoning tasks, complexity of reasoning paths (Fu et al., 2023) or diversity of exemplars (Zhang et al., 2022) have already proven their merits, shifting the weight toward uncovering the patterns hidden in the data themselves that are able to advance reasoning, rather than similarity-driven placement based on semantics.Inspired by the above, we craft exemplars that promote the hidden reasoning patterns rather than the semantics of related data samples, maintaining simple similaritybased retrieval for their placement; by outperforming other prompting techniques, we demonstrate that highlighting those reasoning patterns is adequate for advanced LLM reasoning, without further few-shot engineering.</p>
<p>3 Method</p>
<p>RISCORE prompting</p>
<p>Consider the following two riddles: R1: "A man shaves every day, yet keeps his beard long" and R2: "What has a beard but never needs to shave?".Although these riddles are semantically similar, follow the same structure, and refer to the same objects, their reasoning processes differ.In R1,</p>
<p>where the answer is "A barber," the word "beard" is used in the context of human grooming and personal appearance.In R2, where the answer is "A tree," "beard" is used in the botanical or natural context, referring to the "beard" of certain trees, such as the "oak".Thus, when using these riddles as context for a new riddle-such as "I plant seeds every day, yet don't have a single plot"-the answer will depend on the interpretation of the phrase "plant seeds", based on its contextual framing.In a creative context, the correct answer is "An author".Authors "plant seeds" of ideas through their writing but do not have a literal "plot", unless it refers to the abstract concept of a story plot, which adds layers of interpretation.On the other hand, "A farmer" is not the correct answer because a farmer typically works with physical plots of land to plant seeds, which contradicts the phrase "yet don't have a single plot."</p>
<p>In this example, however, instead of using R2, a contextually reconstructed version of R1-R3: "Tom attends class every day but doesn't do any homework"-would provide a clearer reasoning process for the model.Although R2 is semantically closer to R1, its use of "beard" in a natural or botanical context introduces a different reasoning pathway, illustrating how the contextual framing of a riddle can clarify or obscure its intended answer.Building on the previous example, we propose the RISCORE prompting method, designed to enhance the in-context riddle-solving abilities of LLMs.The RISCORE method supplements each exemplar in FS learning with a contextually recon-structed version of itself.This preserves the desired reasoning process while only altering its context.This approach enables the model to delineate a clear and coherent reasoning trajectory, which it can then follow to effectively solve new riddles.</p>
<p>As illustrated in Figure 2, our approach extends existing few-shot (FS) methods (Dong et al., 2024;Wang et al., 2024;Sultan and Shahaf, 2022), with RISCORE not involved in the exemplar selection process2 .Specifically, the goal of our method is to augment FS samples with automatically generated context-reconstructed examples (detailed in Section 3.2).The inclusion of these context-adapted examples has, in most cases, demonstrated greater impact in improving model performance, even outperforming the use of real examples extracted directly from the dataset (see Section G).</p>
<p>Method for generating context reconstructions</p>
<p>In this section, we present our approach for generating high-quality contextually reconstructed riddles to serve as few-shot exemplars for a model in a Multiple-Choice Question Answering (MQA) format.These exemplars, combined with the original pairs, aim to enhance the model's performance on lateral and vertical thinking tasks.Our method builds on the semi-automated pipeline proposed in the BrainTeaser paper (Jiang et al., 2023b), but we advance this framework by fully harnessing the capabilities of LLMs to automate the process.An overview of the automated method for generating a context-reconstructed riddle is provided in Figure 3.</p>
<p>Step 1: Generation of Question-Answer pair The first step involves generating one contextually reconstructed Question-Answer pair per selected instance, temporarily ignoring distractors.Distractors constitute the incorrect answer options presented alongside the correct answer in multiplechoice settings, designed to appear plausible and test the depth of understanding or reasoning of the model.To achieve this, we provide the LLM with the riddle, the correct answer, and a system prompt that outlines the task.The prompt instructs the model to analyze the given Question-Answer pair, understand the riddle, identify the reasoning process that links the question to the answer, and then generate a similar riddle that follows the same rea-soning process, along with a corresponding correct answer.Since the model is provided with both the riddle and the correct answer, understanding the riddle becomes easier than solving it independently (Sultan and Shahaf, 2022).To further refine this approach, we apply the method in both Zero-shot (ZS) and Few-shot (FS) settings, using pre-existing pairs of original and contextually reconstructed questions from BrainTeaser.The prompts used in this process can be found in App.A.2.After generating the Question-Answer pair, a filtering procedure was conducted to ensure the quality of the instance and its alignment with the riddles in the dataset.This process involves applying rules concerning both the structure of the riddle and the answer.These rules are dataset-specific and can be adjusted to suit different datasets, ensuring adaptability across various contexts.Further analysis regarding this step can be found in App.A.1.</p>
<p>Step 2: Generation of the distractors This process involves generating incorrect answers to complement the multiple-choice options.While this task may initially appear straightforward, it presents several significant challenges.First, the distractors must not only match the number of original options, but also be guaranteed to be incorrect when compared to the correct answer.Additionally, they should not deviate excessively from the correct answer, as significant divergence could undermine the validity and challenge of the riddle.</p>
<p>It is also essential to account for cases where the correct answer is "None of the above", as observed in the BrainTeaser dataset, ensuring that the other three alternatives are wrong enough.</p>
<p>The length of the answers also plays a crucial role in this process.For instance, BrainTeaser includes mostly answers with more than four or five words, while RiddleSense provides typically singleword answers.These variations in length introduce additional challenges in crafting effective distractors.To address this, we propose two distinct methods for generating distractors: one for long and another for short distractors, respectively.</p>
<p>Generation of Long Distractors</p>
<p>In this approach, the original distractors of each riddle are given to an LLM alongside the new Question generated in Step 1.For each distractor (processed one at a time, except for "None of the above"), the model is prompted to modify its context by integrating it with the context of the new riddle.Regardless of the quality of this integration, the distractor is guaranteed to be incorrect, though lower-quality incorporation may slightly reduce its overall effectiveness.If the original riddle's answer differs from "None of the above", but the new riddle's answer matches it, an additional distractor must be generated.In such cases, we simply prompt the LLM to generate a new distractor based on the generated Question-Answer pair.Details of the aforementioned step can be found in App.B.1.</p>
<p>Generation of Short Distractors</p>
<p>The second method for generating distractors is applied to datasets like RiddleSense, which contain singleword answers.In this scenario, modifying a single word like "water" to fit the riddle's context presents a challenge.To address this, we first classify the generated Answer of Step 1 into a set of mutually exclusive categories (details in App.B.2), which informs the generation of distractors.For instance, in the riddle "I have a beard, but I never shave.What am I?" (where the correct answer is "A tree"), the system would classify the answer under the category "Nature".</p>
<p>Next, to ensure the distractor is incorrect, we break the riddle into smaller phrases.We also detect the presence of interrogative words, and if the question is purely descriptive without a direct query, we append the phrase "What am I?" at the end.For the previous example, we split the riddle into two parts: "I have a beard, what am I?" and "I never shave.What am I?" These phrases are then provided as inputs to the LLM, and only answers that differ from the correct one are retained.Since the LLM has access only to a phrase, we expect the generated answer to be incorrect.However, this is not always the case.In the example above, the response to "I have a beard, what am I?" might be "An oak," which is also a valid answer for the riddle.</p>
<p>To address this issue, we leverage the classification of the Answer to guide distractor generation.Specifically, we provide the LLM with two closely related categories, excluding the correct answer's category (e.g., "person" and "place").The model is then prompted, through a system-user instruction, to generate a valid answer within the given context and category.This approach ensures that the generated distractors are both incorrect and contextually relevant, thereby preserving the difficulty of the riddle.After applying a filtering stage to ensure the distractors are distinct, we use WordNet (Miller, 1992) to augment them if their quantity is insuffi-cient.Further details can be found in App.B.2. Additionally, for both methods, the same filtering procedure applied to the correct answer is also used to ensure the quality of the distractors.</p>
<p>Step 3: Creation of the Riddle In the final step of this pipeline, we use the Question-Answer pair, randomly shuffle the distractor set, select the required number of distractors, and place the correct answer in a randomly assigned position, so that a possible position bias is eliminated.More details regarding the Prompting process for our method can be found in App. C.</p>
<p>Experiments</p>
<p>We evaluate various LLMs using different datasets with our method and compare their performance against multiple baselines.</p>
<p>Datasets</p>
<p>For our experiments, we utilize two different datasets, namely BrainTeaser and RiddleSense, addressing vertical and lateral reasoning respectively.A comprehensive analysis of the datasets used in our experiments can be found in App D.</p>
<p>We select BrainTeaser primarily for two key reasons.First, BrainTeaser provides high-quality, manually crafted context reconstructions, which establish an upper bound on the model's performance when these are incorporated into the input.This upper bound also serves as a benchmark for evaluating the quality of riddles generated by the automated method.Second, the dataset consists of riddles that test the lateral thinking abilities of the models, a reasoning process that has proven as a challenging cognitive process for LLMs (Giadikiaroglou et al., 2024).Additionally, experiments using RiddleSense are conducted, emphasizing on the vertical thinking abilities of models, while verifying our method's broad applicability across different types of riddles.Given that handcrafted context reconstructions are not available for RiddleSense, the automated method was utilized without ground-truth reconstructed instances in this set of experiments.</p>
<p>Baselines</p>
<p>In our study, we use various prompting techniques as baselines, including zero-shot (ZS), few-shot (FS), and Chain of Thought (CoT) methods.A detailed analysis of each technique is provided in App.E.</p>
<p>Chain-of-thought (CoT ZS) In this experiment, we prompt the model to answer the multiple-choice riddle in a step-by-step manner, following the method proposed in Wei et al. (2023), without demonstrating any examples.This approach allows us to assess the models' riddle-solving abilities in a zero-shot setting.</p>
<p>Few-Shot (FS) To assess the influence of example-based learning, we incorporate FS prompting with varying numbers of examples: 2, 4, and 8 shots.We limit our tests to 8 shots since it has been proven that reasoning abilities of LLMs are deteriorating with longer inputs (Levy et al., 2024).Two distinct strategies were used for selecting these examples: random selection, referred to as Rand, and selecting riddles with minimal semantic distance from the test riddle using Zhang et al. (2024)3 as the semantic similarity model, referred to as Sim.</p>
<p>Few-shot with CoT Explanations (CoT FS) A straightforward way to aid the model in understanding an exemplar is by providing a supplementary explanation.In this experiment, we explore this approach by offering explanations alongside the fewshot exemplars.This allows us to evaluate whether these crafted explanations can improve the models' ability to comprehend and follow the required reasoning process.The explanations are generated using a semi-automated process.First, for each exemplar in the FS setting, we use ChatGPT4 to generate an explanation for the riddle based on the correct answer, following the method proposed in Wei et al. (2023).These explanations were then manually curated to ensure alignment with human interpretations of the reasoning process required for the riddle.Due to the labor-intensive process of annotating the explanations, this experiment is conducted using only the 2, 4, and 8-shot settings with a randomly selected sampling strategy.</p>
<p>RISCORE</p>
<p>RISCORE is applied as follows.For each example used in the FS setting, the corresponding contextually reconstructed riddle is appended to the input.For a fair comparison, the number of shots refers to the total number of examples included in the prompt, regardless of whether they are new riddles retrieved from the dataset or contextually reconstructed examples.Thus, for example RISCORE with 4 shots has the same 2 examples used in the 2-shot FS technique augmented with the 2 contextual reconstructed samples of them; ultimately, RISCORE keeps only half of the examples drawn from the original dataset, while the remainder consists of reconstructed examples.Therefore, the 2, 4, and 8-shot RISCORE settings correspond to 1, 2, and 4 exemplars drawn from the dataset respectively, with the rest of the demonstrations being their context reconstructions.</p>
<p>We primarily utilize Llama3-8B and Llama3-70B to generate both Question-Answer pairs and distractors.The Question-Answer pairs are produced in both ZS and FS settings.More details regarding these techniques can be found in App.E.</p>
<p>Lastly, in our experiments, we differentiate between RISCORE m , which utilizes manually created reconstructions (when available, as in existing BrainTeaset context reconstructions), and RISCORE, which employs our fully automated method for generating reconstructions.The prompt structure for the system-user interaction in this method is the same as in the FS method outlined in App.E. The sole distinction is in the examples provided.</p>
<p>Models</p>
<p>To this end, we test various models of different scales on their reasoning abilities.Specifically, we examine Llama3 with 8 and 70 billion parameters respectively (AI@Meta, 2024), Mistral-7B and Mistral-8x7B (Jiang et al., 2023a), as well as Qwen2-7B (Yang et al., 2024).This diverse set of models allows us to investigate the impact of contextually reconstructed examples on models with varying sizes and parameterizations.By treating each model as a black-box (we merely prompt them and gather their responses), RISCORE adapts seamlessly to both open-source and proprietary models, demonstrating its versatility across a wide range of LLMs.Further details regarding the model selection process and technical specifications can be found in App.F.</p>
<p>Results</p>
<p>In this section, we present and analyze the results of our experiments.Additional results utilizing different metrics and hyperparameter settings are available in App.G.  6 and 7.  1), using the same number of shots, are underlined.More detailed results can be found in Tables 7 and 8.
Method N. Llama3-70B Mistral-8x7B Llama3-8B Mistral-7B Qwen2-7B Llama3-70B ZS for QA &amp; Llama3-8B</p>
<p>BrainTeaser results</p>
<p>Table 1 presents the performance outcomes of various prompting techniques applied to BrainTeaser data.A key observation is the underperformance of the CoT FS method relative to FS techniques, even when examples are supplemented with manually crafted explanations.This underperformance persists regardless of the model size.As expected (Dong et al., 2024), exemplar selection based on semantic similarity (FS Sim) consistently improves results when chosen over random selection (FS Rand), highlighting the value of semantic relevance to shot selection.This is a significant reference point for RISCORE, since a suitable initial exemplar selection is critical in properly guiding model reasoning when enhanced with contextually reconstructed examples.</p>
<p>Importantly, RISCORE m consistently outperforms FS Sim across all 4-shot and 8-shot configurations, as shown in Table 1.For instance, when comparing models with the same number of examples-such as 4-shot FS Sim versus 2+2-shot RISCORE m , or 8-shot FS Sim versus 4+4-shot RISCORE m -RISCORE m demonstrates superior performance.The advantages of RISCORE m are particularly evident in its ability to maintain robust reasoning.</p>
<p>To better understand this advantage, we can examine the ability of RISCORE m to mitigate noise from suboptimal shot selections, a notable issue in the FS Sim results.For example, with the Llama3-70B model, the 2-shot FS Sim achieves a performance score of 0.825.However, adding two more semantically similar examples reduces the score to 0.792.In contrast, the 2+2-shot RISCORE m achieves a score of 0.833-a 4% improvement over FS Sim with the same number of examples (vs 4shot).This trend is consistent across other models.Similarly, with the smaller Mistral-7B model, the 2-shot FS Sim score is 0.517, but adding two more examples reduces it to 0.458.In contrast, the 2+2shot RISCORE m , using contextually reconstructed examples, achieves a score of 0.567, outperforming FS Sim under identical shot conditions (vs 4-shot).These results indicate that RISCORE m effectively mitigates the noise introduced by suboptimal shot selections.</p>
<p>Using the automated method The manually curated RISCORE m results serve as a benchmark, showcasing the potential upper bound of performance gains achievable with high-quality, handpicked riddles and distractors.Despite this, the automated RISCORE method consistently enhances performance by effectively leveraging reconstructed contexts, as shown in Table 2.While the improvements using the automated RISCORE are understandably smaller than those achieved with manually curated examples (check RISCORE m results of Table 1), they are nonetheless consistent and significant across all models tested.For example, Llama3-70B demonstrates significant improvement when transitioning from FS Sim with 8 semantically similar shots (0.783) to the RISCORE-augmented setting with 8 shots (0.808), which incorporates the original examples and their automated reconstructions.This trend is evident across all relatively smaller models, with noticeable performance gains.For Qwen2-7B, a 2-shot and 4-shot RISCORE setup results in a 2.5% improvement compared to the same-shot FS Sim setting, while for Mistral-7B, the improvement is even more pronounced-rising by up to 10% from a baseline of 0.458 under certain conditions in the 4-shot setting.</p>
<p>These results underscore the value of context reconstruction, particularly when the number of shots is limited, demonstrating that automated reconstruction can effectively complement semantically chosen examples.Interestingly, the results reveal that in most cases, RISCORE's approach of combining N/2 semantically selected examples with their automated reconstructions outperforms the mere placement of the same number of examples retrieved from the dataset (indicated by the underlined results).However, there are instances where FS Sim outperforms RISCORE, primarily due to the selection constraints of RISCORE.In cases where the semantic similarity algorithm identifies an optimal example ranked lower than N/2 in a full FS setup, RISCORE may miss these shots due to its focus on N/2 examples plus reconstructions.This limitation indicates that RISCORE's performance can be constrained by the quality of initial semantic similarity rankings and the resulting shot selection.</p>
<p>RiddleSense</p>
<p>In this dataset, RISCORE can only be applied to automatically generated examples because the dataset's format lacks context reconstructions for its questions.</p>
<p>Table 3 presents the results of the baseline techniques using various models in the RiddleSense dataset.Once again, the results confirm that the few-shot technique, utilizing semantically similar exemplars for in-context learning, consistently delivers the best performance across all tested models.</p>
<p>In Table 4, we present the RiddleSense results using the proposed method for context reconstruction of each input.A clear trend emerges when comparing a simple 8-shot exemplar selection based on semantic similarity with the 8-shot setting of our method, where the top 4 semantically similar examples are augmented with our generated contextual pairs.Notably, the results show that our method consistently outperforms the standard 8-shot exemplar approach, demonstrating a significant improvement in model performance across various instances.An example of this trend is observed with the Llama3-8B model, where our method scores 0.708, approximately 2% higher than the few-shot setting based on semantic similarity, which achieves a score of 0.681.In the case of Mistral-8x7B, RISCORE avhieves a score of 0.700, a 2.5% gain over FS Sim's 0.675, further demonstrating its consistent advantage.The same pattern is evident when comparing the two 4-shot settings.We achieve similar or marginally better accuracy using a total of just 4 examples-two original and two generated contextual reconstructions.This underlines the effectiveness of integrating contextually reconstructed pairs in enhancing model accuracy.We do not achieve a significant performance boost; however, we attain similar or marginally better results while relying on less grounded knowledge.This demonstrates the efficiency of our method, as it maintains comparable performance with fewer, yet strategically selected, exemplars, shifting the focus towards reasoning relevance of exemplars rather than quantity.</p>
<p>Quality of context reconstructed riddles</p>
<p>To generate contextually reconstructed riddles, we use Llama3 models with 8 billion and 70 billion parameters in both FS and ZS settings.We find that the Llama3-8B model struggles to produce highquality Question-Answer pairs for the BrainTeaser dataset and is therefore not suitable for RISCORE.This difficulty likely arises from the BrainTeaser dataset's demand for lateral thinking, which is particularly challenging for smaller reasoners.The Method N.</p>
<p>Llama3-70B</p>
<p>Mistral  Question-Answer pairs are essential, and if their quality is insufficient-as observed when leveraging Llama3-8B on BrainTeaser-high-quality distractors cannot compensate for this deficiency on their own.To address this issue, our preprocessing and filtering process ensures that only high-quality contextual examples are retained, maintaining the effectiveness of the method without being compromised by low-quality generations.However, for vertical thinking riddles, the smaller model effectively generates riddles even in the ZS setting, which, when used in the FS setting, can lead to increased performance compared to real examples drawn from the dataset.</p>
<p>Conclusion</p>
<p>We explore the riddle-solving capabilities of LLMs through multiple-choice formats, investigating how different prompts affect riddle performance requiring diverse reasoning.Our findings suggest that in few-shot settings, augmenting exemplars with contextually reconstructed examples improves performance.We introduce a novel prompting method, RISCORE, validated using the BrainTeaser dataset featuring manually crafted contextual riddles.Inspired by these results, we provide a method for automatically generating contextual reconstructions for multiple-choice riddles, demonstrating that RISCORE enhances LLMs' abilities in lateral and vertical thinking tasks.</p>
<p>Limitations</p>
<p>This method has some limitations.For instance, the placement of exemplars is based on semantic similarity, although cues unrelated semantically might be more crucial for reasoning, such as reasoning path similarity or diversity (Zhang et al., 2022).To address this, future work will employ more advanced methods for selecting exemplars for few-shot settings, like those suggested (Sultan and Shahaf, 2022).Additionally, the current study, having tested only two datasets, limits the broader impact of the conclusions since outcomes might vary with other datasets.Future steps will include testing the RISCORE prompting method with additional datasets across various tasks (Zhang and Wan, 2022).Finally, our experimentation is exclusively focused on the English language, therefore we cannot conclude whether our findings are applicable on other languages.</p>
<p>Ethical Considerations</p>
<p>There are no considerable risks associated with our work.A Question-Answer Pairs Generation</p>
<p>A.1 Process details</p>
<p>This section of our pipeline involves several steps, and subsequent steps after the initial one vary based on the dataset format.Initially, we need to create contextual reconstructed Question-Answer pairs, temporarily disregarding distractors for both datasets.To generate these Question-Answer pairs, we can leverage the capabilities of large language models (LLMs) in two ways.First, we use a system-user prompt template to provide specific instructions to the model, guiding it through the process of producing a new Question-Answer pair.This approach maintains the underlying commonsense premise of the original question while altering both the question and the answer to fit a new situational context.This method operates in a zero-shot setting, meaning the model generates the new pairs without prior examples or training specific to the task.The second approach is quite similar but leverages the preexisting pairs of original and contextually reconstructed questions from BrainTeaser.In this fewshot setting, we use the same instructions as in the zero-shot approach, but we enhance the model's understanding by providing in-context examples.Specifically, we include a pair consisting of an original Question-Answer and its corresponding contextually reconstructed version.This helps the model better grasp the task at hand.It is important to note that these few-shot examples are of high quality, given their manual construction and careful curation.In the few-shot approach, the provided examples are static.It is important to highlight that, in BrainTeaser, when selecting examples for few-shot exemplars from the training dataset, we ensure that the examples chosen are not selected for reconstruction.This avoids any overlap between the few-shot exemplars and the examples for which we are requesting a context change, ensuring that the given pair examples are distinct from the requested ones.The corresponding system and user prompts regarding the above tasks can be found in App.A.2.</p>
<p>Models utilized</p>
<p>In this approach, we experiment with two models to assess the impact of model size on the quality of contextual reconstructions and the overall performance across both datasets.We use a relatively small model, Llama3-8B (AI@Meta, 2024), and a larger model from the same family, Llama3-70B (AI@Meta, 2024).These models were selected due to their strong performance across a diverse range of tasks, outperforming other models of comparable size.By incorporating both a smaller model, Llama3-8B, and a larger model, Llama3-70B, we aim to systematically investigate the effect of model size on the quality of contextual reconstructions and its overall impact on performance.This exploration allows us to gain deeper insights into how model capacity influences the effectiveness of the generated examples across different datasets.</p>
<p>Quality Assurance for Generated Text After generating the contextual reconstructed Question-Answer pairs for all initially selected examples from the training set, we perform a small but necessary quality control to ensure that the generated pairs adhere to several high-level criteria.For the BrainTeaser dataset, the filtering process is more flexible, as the questions and answers can vary in length.The only constraint is that the questions must have a minimum length of 7 words, while the answers can be as long as necessary.As a result, the number of generated pairs filtered out is relatively low, around 2%, ensuring that the majority of reconstructions meet the quality criteria.In contrast, the RiddleSense dataset requires stricter limitations due to its typically shorter answers.For this dataset, questions must be at least 6 words in length, while answers must not exceed 7 words, reflecting the dataset's concise nature.As a result of these stricter rules, approximately 10% of the generated reconstructions for the RiddleSense dataset are filtered out for not meeting the required standards.It is important to note that the mini-mum question length requirement is implemented to ensure that the generated questions maintain a sufficient level of complexity and depth.Questions with fewer than 6 or 7 words are unlikely to provide the necessary detail to establish a well-structured and coherent riddle.This limitation helps preserve the quality and integrity of the riddles by ensuring that they are adequately framed to challenge and engage the model effectively.</p>
<p>A.2 Prompts</p>
<p>First, we will present the ZS prompt configuration used for generating the contextually reconstructed Question-Answer pairs.System Prompt: You are an expert in context reconstruction.Your task is to receive a question along with its correct answer and adapt them to a new scenario while maintaining the misleading commonsense premise.</p>
<p>Please follow these steps:</p>
<p>-First, you will receive an unsolved riddle along with five answer options.Analyze the given setting and identify the connection between the question and its correct answer.</p>
<p>-Modify the original question and correct answer to fit a different situational context, ensuring that the underlying logic and relationship between them are preserved.</p>
<p>-Ensure that both the new question and the new correct answer are distinct from the originals.</p>
<p>User Prompt:</p>
<p>Question: <code ANSWER="ANSWER">{QUESTION}``C orrect answer:</code>``Ǹ ext, with minor modifications, we introduce the FS prompt configuration that follows the same logic but incorporates in-context examples to guide the model's responses.</p>
<p>System Prompt:</p>
<p>You are an expert in context reconstruction.Your task is to receive a question along with its correct answer and adapt them to a new scenario while maintaining the misleading commonsense premise.</p>
<p>Please follow these steps: First, review an example provided with its context reconstruction, which illustrates the type of transformation you will need to perform.</p>
<p>Next, you will receive an unsolved riddle along with five answer options.Analyze the given setting and identify the connection between the question and its correct answer.</p>
<p>Modify the original question and correct answer to fit a different situational context, ensuring that the underlying logic and relationship between them are preserved.</p>
<p>Ensure that both the new question and the new correct answer are distinct from the originals.</p>
<p>B Distractor Creation</p>
<p>B.1 BrainTeaser</p>
<p>In this process, we leverage the capabilities of large language models (LLMs) to understand and rephrase context in order to generate distractors.We use two different pipelines to produce at least three distractors, ensuring coverage of various potential situations. 1 The first approach involves prompting the model with a system-user prompt that instructs it to analyze the given Question-Answer pair.The model is tasked with understanding the riddle, identifying the reasoning process that links the question to the answer, and then suggesting a distractor based on the more challenging or deceptive aspects of the concept.This approach yields one of the three required distractors.2 In the second approach, the process is more intricate and can result in distractors of questionable quality.We prompt the model with a system-user prompt, providing it with the reconstructed question without its answer, along with the original question's incorrect distractors, after removing the option "None of the above".The model is then tasked with modifying the concept of the given distractors by incorporating elements from the setting described in the question.Importantly, the correct answer is not provided to ensure that the generated distractors are distinctly different from the correct answer and effectively capture a varied interpretation of the question's context.Certainly, there are instances where the model generates sub-optimal contexts for the distractors provided.Despite these cases, the distractors remain incorrect and serve their purpose, though they may not always be sufficiently challenging.Our experiments indicate that when the model is given both the original and the contextual reconstructed examples, the minor issue of lower-quality distractors does not significantly impact overall performance.Now, we have also created two new distractors that are somewhat contextually relevant to the setting, adding an additional layer of coherence to the generated options.This ensures that the distractors are not only incorrect but also related to the underlying premise of the riddle, enhancing the overall quality of the multiple-choice options.To create the final dataset, we randomly select two of the three generated distractors and shuffle them with the correct answer in random order.Finally, we append the option "None of the above" as the last choice.In cases where "None of the above" is the correct answer, we use all three generated distractors.With this approach, our dataset is prepared and ready for use.</p>
<p>Prompts used First, we will provide the systemuser prompt used to task the model with understanding the riddle, identifying the reasoning process linking the question to the answer, and then suggesting a distractor based on the more challenging or deceptive aspects of the concept.</p>
<p>System Prompt:</p>
<p>Your task is to act as a concept grasper.You will be given a riddle and its correct answer.</p>
<p>Your goal is to understand the connection between the riddle and the correct answer, focusing on the tricky parts.Based on these tricky aspects, propose a plausible wrong answer that someone might give.</p>
<p>The wrong answer should be short, concise, and limited to one sentence.</p>
<p>-Riddle:</p>
<p>-Correct Answer:</p>
<p>Response format:</p>
<p>-Wrong Answer:</p>
<p>User Prompt:</p>
<p>-Riddle: {QUESTION} -Correct Answer: {ANSWER} Now we will provide the system-user prompt used for the second method.</p>
<p>System Prompt: You will be given a sentence without context and then provided with a specific context.</p>
<p>Your task is to rewrite the sentence so that it aligns with the given context, while keeping it as close as possible to the original meaning.</p>
<p>The purpose is to adapt the sentence to the context, not to answer any questions related to the context.</p>
<p>-Sentence (out of context):</p>
<p>-Context:</p>
<p>Response format:</p>
<p>-Sentence:
User Prompt: -Sentence (out of context): {ORI_CHOICE} -Context:{QUESTION}
Here, the value of ORI_CHOICE refers to the distractors from the original instance, excluding the "None of the above" option.We create prompts for the model with each of these distractors individually.</p>
<p>B.2 RiddleSense</p>
<p>In this setting, the approach is fundamentally different.The answers and distractors in the original dataset are primarily one-word responses, while the questions feature detailed settings with punctuation, conjunctions, and more complex structures.To handle this, we also generate distractors using two distinct pipelines tailored to this format.</p>
<p>First pipeline</p>
<p>The first pipeline involves a more granular approach by splitting the contextual reconstructed question into subphrases based on punctuation or conjunctions.If this yields fewer than three distinct subphrases, we further split the sentence at the position of the word "and" to create additional segments.We also detect the presence of interrogative words, and if the question is purely descriptive without any direct question, we append the phrase "What am I?" at the end.This is not chosen arbitrarily but follows the standard structure of many riddles in this dataset, where "What am I?" is the common question leading to singleword answers.Now that we have broken the riddle into sub-phrases and appended the appropriate question, we can prompt the model to generate a wrong answer for each sub-phrase concatenated with the question.This method ensures that the distractors align with different parts of the riddle's setting.However, since some sub-phrases may contain key ideas central to the riddle, the model may still generate answers too similar to the correct one.To address this issue, we incorporate an additional intermediate step utilizing zero-shot classification.Specifically, we use the facebook/bart-large-mnli model 5 (Lewis et al., 2019) provided by Hugging Face.This model classifies the correct answer into one of eight general categories: 'food', 'person', 'object', 'animal', 'nature', 'time', 'place', 'concept'.These categories are chosen to be mutually exclusive to avoid overlap.In our approach, we first predict the category of the correct answer using this model.We then leverage this classification to guide the generation of distractors.For each sub-phrase concatenated with the question, we provide the LLM with the two most similar categories (excluding the correct answer's category).The model is prompted with a system-user instruction to produce a correct answer of the given setting in the given category.This method ensures that the generated distractors are not only incorrect but also contextually aligned with the riddle, thus maintaining the challenge for the model.After applying filtering to ensure the distractors are distinct and relevant, we have produced several distractors.The LLMs utilized in this pipeline include the previously mentioned Llama3-8B (AI@Meta, 2024) and its larger counterpart, Llama3-70B (AI@Meta, 2024).</p>
<p>Second pipeline</p>
<p>In cases where the distractors produced by the above pipeline are insufficient, we use WordNet (Cognitive Science Laboratory, Princeton University, 2006) to augment our distractor set.For each generated distractor, or if necessary, for the distractors from the original question, we retrieve synonyms and hyponyms from Word-Net.These additional terms are then included as potential distractors.After compiling these distractors, we randomly select four of them and add the correct answer in a random order to complete the set.This approach ensures that we have a diverse and comprehensive set of distractors for each question.To ensure the quality of our distractors, we impose a limitation that at least two of the required four distractors must be generated using the first approach.This is due to the fact that the distractors generated through WordNet augmentation tend to be of inferior quality compared to those produced directly by the model.If this requirement is not met, we skip the particular train set instance for producing a contextual reconstruction.This approach helps maintain the overall quality and relevance of 5 facebook/bart-large-mnli the distractors in our dataset.</p>
<p>Prompts used We will present the system-user prompt configuration used in the first pipeline.In this approach, for each sub-phrase concatenated with the question, we provide the model with two categories, excluding the correct answer's category.For each category, a separate system-user prompt is issued to instruct the model to generate a correct answer that fits the given setting within the specified category.</p>
<p>System Prompt: Task: Provide a concise, relevant answer to the given question within the specified category.</p>
<p>Constraints:</p>
<p>-The answer should not exceed three words.</p>
<p>-Follow the exact format provided below.</p>
<p>Response Format: Answer: ...</p>
<p>User Prompt:</p>
<p>Question: <code>`{QUESTION}</code>C ategory: {CATEGORY}</p>
<p>C Pairs Prompting</p>
<p>We have successfully created the desired contextual reconstructions for both datasets.However, an issue remains: some of the originally selected examples do not have corresponding reconstructed examples due to the quality control filtering process.</p>
<p>To address this issue, we first use the original most semantically similar examples as in-context learning exemplars, appending their corresponding automatically generated contextual pairs.If these exemplars are not sufficient for the required settings of two, four, or eight (i.e., RISCORE), we employ a more structured approach rather than adding examples randomly.1 We begin by generating embeddings for the set of original examples and their contextual reconstructions that have not already been included in the current exemplars.Using cosine similarity, we then identify the most similar examples from this set.Importantly, these similar examples might not be part of the original training set but could be among the reconstructed examples.2 We select the most similar examples and pair each with its corresponding pair, ensuring that we add either the original or the reconstructed example as needed.This process is repeated until the number of exemplars meets the required quantity.</p>
<p>D Dataset Description</p>
<p>In this work we are using two different datasets of multiple QA format. 1 The first one is the Brain-Teaser dataset, which consists of two sub-tasks regarding both lateral thinking challenges.In addition to the original puzzles, the dataset includes adversarial subsets created by manually modifying the original BrainTeaser while preserving their reasoning paths.The original data were perturbed in two ways: First, there is a semantic reconstruction of each original question, ensuring that the answers and distractors remain unchanged.Second, the original data undergoes context reconstruction, where the reasoning path is preserved, but the brain teaser is rephrased to describe a new situational context.Each question offers four possible choices, with the last option always being "None of the above".It is important to note that our experiments are focused exclusively on the sentence puzzle (SP) setting. 2 The second dataset used in our experiments is RiddleSense.This dataset features riddle-style questions that require sophisticated commonsense reasoning and a strong grasp of figurative language to answer accurately.It is structured as a multiple-choice question answering task, with riddles that test the model's capacity to navigate and interpret nuanced commonsense scenarios.The reasoning challenges posed by Rid-dleSense are closely aligned with those found in BrainTeaser.RiddleSense offers one correct answer and four distractors for each riddle.This contrasts with BrainTeaser, which includes a "None of the above" option as the final choice, thereby introducing a more challenging setting.</p>
<p>D.1 Data statistics</p>
<p>Format of Datasets The BrainTeaser dataset is divided into three splits: train, development, and a hidden test set used for evaluation.Although Rid-dleSense follows a similar format, we did not have direct access to its hidden test set.As a result, we utilized the development data split of RiddleSense for our experiments.</p>
<p>Prepossessing and filtering Due to data leakage across the two datasets, we were required to exclude semantically similar examples from the test split utilized.This leakage is understandable, as the sources for English riddles are limited, and the number of unique riddles is finite.After identifying semantically similar riddles, we retained only one instance within the datasets, prioritizing Brain-Teaser due to the higher quality of its questions and distractors, which were manually crafted.Additionally, hardware limitations made processing the full RiddleSense unfeasible.To manage this, we split its test set into two halves.To ensure the fairness and accuracy of this division, we evaluated various techniques on both halves.Following the completion of these procedures, we present the dataset statistics in the table below.Statistics regarding the datasets are provided in Table 5.More information regarding this process can be found in D.3.</p>
<p>D.2 Evaluation metrics</p>
<p>Given that our task is formatted as a multiplechoice question-answering problem, the evaluation metric employed will be accuracy.This metric will provide a measure of the proportion of correctly answered questions out of the total number of questions presented.Although accuracy is a straightforward metric, it is effective in providing useful insights and serves as a reliable measure for evaluation in this context.1 For the BrainTeaser task, we not only track the overall accuracy but also monitor the accuracy for each specific type of instance, including original, semantic, and context reconstructions.This detailed tracking is feasible because the data in each set are balanced, with each original instance having corresponding semantic and context variations.This approach allows us to evaluate the model's proficiency in handling the same reasoning path under different conditions.The total accuracy is then calculated as the mean of these three individual metrics.We also track group-based accuracy, where a "group" refers to either two questions (original and semantically reconstructed) or three questions (original, semantically reconstructed and contextually reconstructed).This metric assesses the model's performance when all instances within a group are answered correctly.Group-based accuracy provides a broader perspective on the model's ability to handle these lateral thinking challenges across different types of question reconstructions.This format of evaluation metrics was both provided and requested by the creators of the dataset in an open Kaggle competition, and we are adopting it for our analysis.</p>
<p>2 Unfortunately, the simple data row format in RiddleSense restricts our ability to track more detailed metrics comprehensively, limiting us to evaluating only the overall accuracy for this dataset.</p>
<p>D.3 Dataset preprossessing</p>
<p>Our goal in this process is to ensure that no question from the RiddleSense test set is present in the combined train, dev, and test datasets of BrainTeaser.To achieve this, we are using the Sentence Transformers library6 (Reimers and Gurevych, 2019) with the gte-large-en-v1.57(Zhang et al., 2024) model to generate text embeddings and perform semantic similarity comparisons.We convert each question from the entire BrainTeaser puzzle into embeddings that represent the semantic meaning of each question.We then calculate the cosine similarity between these embeddings and those in the RiddleSense test set to identify duplicates or highly similar questions.A lower threshold is applied to define similarity, meaning that any question with a cosine similarity above this threshold is considered a near-duplicate and excluded from the test set.After conducting several experiments, we determined that a cosine similarity threshold of 0.9 was optimal.This threshold is a delicate balance, as our questions are riddle-style and often rely on abstract or metaphorical language, making it challenging to clearly assess whether two questions are similar enough to warrant exclusion.Since riddles are not always straightforward in their wording, the threshold needed to be fine-tuned to avoid excluding questions that are conceptually distinct but might have overlapping language, ensuring a fair evaluation without overly aggressive filtering.This means that any question from the RiddleSense test set that had a similarity score of 0.9 or higher to a question in the BrainTeaser datasets was removed to avoid redundancy.As previously mentioned, because we prioritize BrainTeaser questions due to their superior quality from manual annotation, we decided to remove any similar questions from the RiddleSense test set.This approach helps preserve the integrity of the evaluation process and ensures that the test set contains only unique and distinct questions, minimizing potential data leakage and improving the robustness of our results.Additionally, by removing these instances, we can perform more experiments on the remaining unique questions, optimizing our use of hardware resources and potentially yielding more comprehensive insights.</p>
<p>E Prompting details</p>
<p>Zero-shot Chain of Thought Prompting Technique This is the most straightforward prompting technique.In this approach, each instance from the test set of each dataset is presented to the model using a specific system-user prompt format.This technique operates in a zero-shot setting, meaning the model is not given any prior examples or training specific to the task at hand.Instead, it relies solely on the system prompt and user prompt to generate responses based on its pre-existing knowledge.The chaoin of thought character is a result of the usage of the phrase Let's think step by step in the end of each system prompt.The Chain of Thought (CoT) character is achieved by including the phrase "Let's think step by step" at the end of each system prompt.This approach aligns with the findings of (Kojima et al., 2023), which demonstrate that large language models can perform effectively as zero-shot reasoners by incorporating this specific phrase before presenting an unanswered test instance.The following are the system-user prompts that were used: System Prompt: You will encounter a riddle that requires analytical thinking and reasoning to solve.A riddle is a question or statement intentionally phrased so as to require ingenuity in ascertaining its answer or meaning, typically presented as a game.</p>
<p>Different ideas can be used in these riddles: 1. Riddles often employ misdirection, leading you away from the actual solution.2. They include elements with double meanings, requiring a keen eye for words with dual interpretations.3. Metaphorical wordplay adds another layer, urging you to decipher figurative language.4. Look out for exaggeration, as riddles may present overly dramatic details to divert your attention.5. Common phrases and sayings may hide within the puzzle, demanding familiarity.6. Associations and irony play a crucial role, introducing unexpected connections.7. Numerical puzzles can also be part of the mystery, requiring you to decode their significance.8. Elemental imagery, drawn from nature, might hold key descriptors.9. Rhyming and sound clues can add a poetic dimension.10.Word Puzzles: Pay attention to anagrams, acrostics, and other wordplay elements.11.Also, it is important to note you should decode the upcoming riddle using everyday logic and creativity.</p>
<p>Approach the riddle with everyday logic and creativity, avoiding supernatural explanations.You will be given an unsolved riddle and five options to choose the answer amongst them.Let's think step by step.</p>
<p>User Prompt:
Riddle: <code>{ RIDDLE}</code>Ò ptions: [option 1]: <code OPTION_2="OPTION_2">{OPTION_1}``[ option 2]:</code><code>[ option 3]: ```{OPTION_3}</code>[ option 4]: <code OPTION_5="OPTION_5">{OPTION_4}``[ option 5]:</code>``Ẁ
here the input parameters are self-explanatory.It is worth mentioning that for BrainTeaser, where the multiple-choice options are four, the prompt is adjusted accordingly.</p>
<p>Few-shot Prompting Techniques</p>
<p>In this approach, we provide a specific number of exemplars from each dataset's training set before asking the model to solve a multiple-choice question from the test set.These exemplars, which include their correct answers, serve as in-context learning examples.By presenting these examples, we aim to guide the model and improve its performance on the subsequent unanswered tasks.This method leverages the provided context to enhance the model's ability to understand and solve new instances more effectively.The system prompt that is used here is the one of the zero-shot prompting with some minor changes.The number of examples that are used are 2, 4 and 8 as mentioned in section 3. Now, a key decision is how to select the exemplars that will be provided to the model before it attempts to answer the test set questions.The simplest and most straightforward approach is to randomly choose these answered exemplars from the training dataset.This method, referred to as FS Rand in our experiments, offers a baseline for comparison, as it doesn't apply any specific strategy for selecting the most relevant examples and relies purely on random sampling.</p>
<p>Another approach implemented in our experiments for selecting exemplars is based on semantic similarity.Specifically, we utilize the Sentence Transformers library8 (Reimers and Gurevych, 2019) in conjunction with the gte-large-en-v1.5 model 9 (Zhang et al., 2024).This setup allows us to generate text embeddings and perform semantic similarity comparisons using cosine similarity between these embeddings.For each experiment, we first generate text embeddings for all instances in the training datasets.Then, for each question in the test set, we generate its embedding and calculate its cosine similarity with all training set embeddings.Based on the number of in-context learning examples needed, we select the most similar training instances to use as exemplars.This method ensures that the selected examples are semantically relevant, aiming to improve the model's performance by presenting it with contextually aligned training examples.This approach is referred to as FS Sim in our experiments, as it focuses on semantic similarity to guide exemplar selection for improved task performance.</p>
<p>The system and user prompts used for both of the above settings are outlined as follows:</p>
<p>System Prompt: You will encounter a riddle that requires analytical thinking and reasoning to solve.A riddle is a question or statement intentionally phrased so as to require ingenuity in ascertaining its answer or meaning, typically presented as a game.</p>
<p>Different ideas can be used in these riddles: 1. Riddles often employ misdirection, leading you away from the actual solution.2. They include elements with double meanings, requiring a keen eye for words with dual interpretations.3. Metaphorical wordplay adds another layer, urging you to decipher figurative language.4. Look out for exaggeration, as riddles may present overly dramatic details to divert your attention.5. Common phrases and sayings may hide within the puzzle, demanding familiarity.6. Associations and irony play a crucial role, introducing unexpected connections.7. Numerical puzzles can also be part of the mystery, requiring you to decode their significance.8. Elemental imagery, drawn from nature, might hold key descriptors.9. Rhyming and sound clues can add a poetic dimension.10.Word Puzzles: Pay attention to anagrams, acrostics, and other wordplay elements.11.Also, it is important to note you should decode the upcoming riddle using everyday logic and creativity.</p>
<p>Approach the riddle with everyday logic and creativity, avoiding supernatural explanations.</p>
<p>First, you'll encounter X examples with their answer provided similar to the riddle you will need to solve.Then you will be given an unsolved riddle and X options to choose the answer amongst them.</p>
<p>User Prompt:</p>
<p>{EXAMPLES} Answer the following riddle while taking into consideration the examples above.Choose the best and the most logical option from the available choices:
Riddle: <code>{ RIDDLE}</code>Ò ptions: [option 1]: <code OPTION_2="OPTION_2">{OPTION_1}``[ option 2]:</code><code>[ option 3]: ```{OPTION_3}</code>[ option 4]: <code OPTION_5="OPTION_5">{OPTION_4}``[ option 5]:</code>``Ẁ
here the EXAMPLES refer to the few-shot examples selected for each test set instance, and the other input parameters are self-explanatory.It is worth mentioning that for BrainTeaser, where the multiple-choice options are four, the prompt is adjusted accordingly.</p>
<p>Chain of Thought Prompting Technique Another promising technique for enhancing model performance across various tasks is Chain of Thought (CoT) prompting.This approach involves not only providing answered examples but also including an explanation consisting of intermediate reasoning steps.By laying out the thought process step-bystep, CoT prompting significantly improves the capability of large language models to tackle complex reasoning tasks.In our task, the Chain of Thought (CoT) prompting technique is anticipated to outperform the previously mentioned methods.This expectation is supported by the performance improvements observed in the CommonsenseQA dataset (Talmor et al., 2019), a domain similar to our setting that involves both lateral and vertical thinking.The dataset demonstrated significant improvements when using a manually composed set of two, four, or eight few-shot exemplars with CoT prompting, highlighting its effectiveness in eliciting successful reasoning.In our case, these explanations were generated manually to ensure they align with human perceptions of the reasoning process required by the riddle.The explanation format we used follows the structure outlined by (Wei et al., 2023) for few-shot exemplars in full chain-ofthought prompts for CommonsenseQA.Due to the similarity in the multiple-choice question-answer format of our datasets, it served as a good foundation for developing our own approach of creating explanations.Below, we provide the system and user prompts used in the aforementioned technique:</p>
<p>System Prompt: You will encounter a riddle that requires analytical thinking and reasoning to solve.A riddle is a question or statement intentionally phrased so as to require ingenuity in ascertaining its answer or meaning, typically presented as a game.</p>
<p>Different ideas can be used in these riddles: 1. Riddles often employ misdirection, leading you away from the actual solution.2. They include elements with double meanings, requiring a keen eye for words with dual interpretations.3. Metaphorical wordplay adds another layer, urging you to decipher figurative language.4. Look out for exaggeration, as riddles may present overly dramatic details to divert your attention.5. Common phrases and sayings may hide within the puzzle, demanding familiarity.6. Associations and irony play a crucial role, introducing unexpected connections.7. Numerical puzzles can also be part of the mystery, requiring you to decode their significance.8. Elemental imagery, drawn from nature, might hold key descriptors.9. Rhyming and sound clues can add a poetic dimension.10.Word Puzzles: Pay attention to anagrams, acrostics, and other wordplay elements.11.Also, it is important to note you should decode the upcoming riddle using everyday logic and creativity.</p>
<p>Approach the riddle with everyday logic and creativity, avoiding supernatural explanations.here the EXAMPLES_COT refer to the few-shot examples with manually generated explanations selected for each test set instance, and the other input parameters are self-explanatory.It is important to note that for BrainTeaser, where there are four multiple-choice options, the prompt is modified accordingly.</p>
<p>F Experimental Setup</p>
<p>In our experiments, we employed the Google Colab platform and Kaggle, leveraging various opensource Python packages such as Transformers, Bit-sAndBytes, Accelerate (Gugger et al., 2022) and Sentence-Transformers.We also utilized the Amazon Bedrock API, which allowed us to access models without being constrained by hardware limitations, providing scalable and flexible model deployment.</p>
<p>We specifically opted for instruction variations of the models because they aligned more closely with our task requirements.Instruction-tuned models generally offer better performance in scenarios where understanding and following specific instructions is crucial.This alignment ensures that the models are better equipped to handle the nuances of the tasks, leading to more effective and relevant outputs.</p>
<p>The temperature and repetition penalty values were determined through a series of exploratory experiments.To ensure consistency across our work, we systematically applied the same parameters whenever possible.For our experiments, we used a temperature of 0.5 and repetition penalties of either 1.0 or 1.15.</p>
<p>Llama 3 (AI@Meta, 2024) In our experiments, we chose two variations of the model: the 8B and 70B versions.The Llama3-8B 10 model was used for inference without quantization, which allowed it to deliver results with full precision.This approach was ideal when computational resources were sufficient and precision was critical.On the other hand, the Llama3-70B 11 model, due to hardware limitations, underwent quantization.This process reduced the model's size and computational needs, making it accessible despite the constraints of our hardware.While quantization might lead to some loss of precision, it was necessary to deploy a model with more parameters, which otherwise would not have been feasible.</p>
<p>Mistral (Jiang et al., 2023a) In our experiments, we used the Mistral-7B-Instruct-v0.2 12 and the Mixtral-8x7B-Instruct-v0.1 13 in their unquantized 10 meta-llama/Meta-Llama-3-8B-Instruct 11 meta-llama/Meta-Llama-3-70B-Instruct 12 mistralai/Mistral-7B-Instruct-v0.2 13mistralai/Mixtral-8x7B-Instruct-v0.1 forms.The Mistral 7B-Instruct v0.2 was selected for its strong instruction-following abilities, maintaining full precision.Similarly, the Mixtral-8x7B-Instruct-v0.1, which integrates eight 7B models, was used unquantized to benefit from its combined performance.Our goal was to explore the effectiveness of smaller models for our riddle tasks, and we preferred these instruction-tuned variations to ensure they were well-suited to our specific requirements, leading to more effective results.</p>
<p>Qwen2 (Yang et al., 2024) To further explore smaller model variations, we selected the Qwen2-7B-Instruct 14 version.The Qwen2 family of models, including this 7B-Instruct variant, is known for its strong performance in reasoning tasks.The Qwen2 models are designed with advanced instruction-following capabilities and are particularly effective at complex problem-solving and logical reasoning, making them well-suited for our objectives.</p>
<p>G Detailed Results</p>
<p>In this section, we present the detailed results for both datasets across all our experimental techniques.Due to the extensive nature of the experiments, the results for each dataset are organized into subtables.We begin with the BrainTeaser dataset, where the metrics are more detailed due to the dataset's structure, followed by the results for the RiddleSense dataset.The structure is organized by method per model, with results presented in descending order based on score for each method, rather than by the number of examples.This differs from the approach outlined in Section 5.</p>
<p>Figure 1 :
1
Figure 1: Standard FS prompting vs RISCORE prompting: by encouraging reasoning-based selection of exemplars (riddle in green) in place of semantic similarity selection (riddle in red), the model unlocks the reasoning pattern, guided towards the correct answer.</p>
<p>Figure 2 :
2
Figure 2: An overview of RISCORE, where the reconstructed instances, along with their original counterparts, are incorporated as exemplars in the few-shot setting to enhance the model's riddle solving ability.</p>
<p>Figure 3 :
3
Figure 3: An overview of the automated method for generating a context-reconstructed riddle.</p>
<p>riddle -answer pair while taking into consideration the examples above regarding context reconstruction: Question: <code ANSWER="ANSWER">{QUESTION}``C orrect answer:</code>``Ì n the above setting, it is understood that the values of the EXAMPLES represent pairs of answered Question-Answer examples: the original and its contextually reconstructed counterpart, both sourced from the BrainTeaser dataset.</p>
<p>First</p>
<p>, you'll encounter X examples demonstrating analytical reasoning similar to the riddle you will need to solve.Then you will be given an unsolved riddle and five options to choose the answer amongst them.Let's think step by step and solve the riddle based on the examples provided above.User Prompt: {EXAMPLES_COT} Answer the following riddle while taking into consideration the examples above.Choose the best and the most logical option from the available choices:</p>
<p>Table 2 :
2distractors
Model performance for BrainTeaser using RISCORE prompting.Similarity-based selection was employed for choosing all the exemplars.Results that surpass the FS method with semantically similar examples (FS Sim, check Table</p>
<p>Table 3 :
3
Model Performance for RiddleSense using baseline techniques.The best results overall are in bold.More detailed results can be found in Table9.Note that no RISCORE m numbers are reported, since RiddleSense does not contain any ground truth reconstructed riddles.
MethodN.Llama3-70BMistral-8x7BLlama3-8BMistral-7BQwen2-7BLlama3-70B fewshot for QA &amp; Llama3-70B distractors20.792 0.672 0.692 0.600 0.697RISCORE40.783 0.689 0.722 0.600 0.71780.789 0.700 0.708 0.597 0.731Llama3-70B fewshot for QA &amp; Llama3-8B distractors20.786 0.719 0.681 0.603 0.681RISCORE40.789 0.686 0.686 0.606 0.69780.775 0.689 0.706 0.617 0.719Llama3-8B zeroshot for QA &amp; Llama3-8B distractors20.792 0.681 0.689 0.589 0.694RISCORE40.778 0.714 0.700 0.600 0.68380.806 0.689 0.686 0.614 0.689</p>
<p>Table 4 :
4
Model performance for RiddleSense using RISCORE prompting.Similarity-based selection was employed for choosing all the exemplars.Results that surpass the FS method with semantically similar examples, using the same number of shots, are underlined.More detailed results can be found in Table10.</p>
<p>Table 5 :
5
Data statistics
DatasetTrainDevTestBrainTeaser -SP507 (169x3) 120 (40x3) 120 (40x3)RiddleSense (initial)1021RiddleSense (filtered)3510720-RiddleSense (sampled 50%)360</p>
<p>Table 6 :
6
Model performance for BrainTeaser (Part 1).The presence of (Q) in the method column indicates that the results correspond to the quantized version of the model.
ModelMethodNum.Ex Task Temp Rep_Pen Original Semantic Context Ori. + Sem. Ori. + Sem. + Con. AverageChain-of-Thought Zero-shotMeta-Llama-3-70B-Instruct CoT_ZS (Q)0SP0.51.150.7250.7750.6750.6750.5500.725Mixtral-8x7B-Instruct-v0.1CoT_ZS0SP0.51.00.5750.5500.5250.4750.2750.550Meta-Llama-3-8B-InstructCoT_ZS0SP0.51.00.6250.6500.6250.5000.3250.633Mistral-7B-Instruct-v0.2CoT_ZS0SP0.51.00.3750.4750.5000.3000.2500.450Qwen2-7B-InstructCoT_ZS0SP0.51.150.4750.4500.4500.3500.2000.458Few-shot with CoT ExplanationsMeta-Llama-3-70B-Instruct CoT_FS (Q)2SP0.51.150.8500.7500.6750.7000.5000.758Meta-Llama-3-70B-Instruct CoT_FS (Q)8SP0.51.00.6750.7500.7000.6000.4250.708Meta-Llama-3-70B-Instruct CoT_FS (Q)4SP0.51.150.7000.6500.7000.5750.4500.683Mixtral-8x7B-v0.1CoT_FS8SP0.51.00.6500.6750.6000.5750.3750.642Mixtral-8x7B-v0.1CoT_FS2SP0.51.00.6250.6250.6000.5250.3500.617Mixtral-8x7B-v0.1CoT_FS4SP0.51.00.5750.6000.5750.4750.3500.583Meta-Llama-3-8B-InstructCoT_FS8SP0.51.00.6750.7000.6000.5250.3250.658Meta-Llama-3-8B-InstructCoT_FS2SP0.51.00.7250.6250.5500.5500.3500.633Meta-Llama-3-8B-InstructCoT_FS4SP0.51.00.6500.6250.5500.5000.3000.608Mistral-7B-Instruct-v0.2CoT_FS8SP0.51.00.5250.5500.4500.3750.3000.508Mistral-7B-Instruct-v0.2CoT_FS4SP0.51.00.5250.5500.4500.3750.3000.508Mistral-7B-Instruct-v0.2CoT_FS2SP0.51.00.5250.4250.4750.3000.2250.475Qwen2-7B-InstructCoT_FS8SP0.51.00.6000.7250.6750.5500.4250.667Qwen2-7B-InstructCoT_FS4SP0.51.00.6500.6750.6250.5500.4500.650Qwen2-7B-InstructCoT_FS2SP0.51.00.6750.5500.6000.4500.3750.608Few-shot with Random SelectionMeta-Llama-3-70B-Instruct FS Rand (Q)4SP0.51.150.8250.8500.7500.8000.7000.808Meta-Llama-3-70B-Instruct FS Rand (Q)8SP0.51.150.7750.8000.7500.6750.5500.775Meta-Llama-3-70B-Instruct FS Rand (Q)2SP0.51.150.7500.8000.7750.7250.6000.775Mixtral-8x7B-Instruct-v0.1FS Rand4SP0.51.00.6750.7750.6000.6000.4000.683Mixtral-8x7B-Instruct-v0.1FS Rand2SP0.51.00.6500.6250.5750.4750.3000.617Mixtral-8x7B-Instruct-v0.1FS Rand8SP0.51.00.6750.6500.5250.5250.3500.617Meta-Llama-3-8B-InstructFS Rand8SP0.51.00.7500.6250.6500.5750.4000.675Meta-Llama-3-8B-InstructFS Rand4SP0.51.00.6250.6500.6500.5000.3750.642Meta-Llama-3-8B-InstructFS Rand2SP0.51.00.6000.7000.6000.5500.3750.633Mistral-7B-Instruct-v0.2FS Rand2SP0.51.00.5250.5500.4750.4250.2750.517Mistral-7B-Instruct-v0.2FS Rand8SP0.51.00.4500.5750.4250.4000.3000.483Mistral-7B-Instruct-v0.2FS Rand4SP0.51.00.5500.4250.4750.3250.2250.483Qwen2-7B-InstructFS Rand2SP0.51.150.6750.6500.6000.5500.4250.642Qwen2-7B-InstructFS Rand8SP0.51.150.7000.6000.6250.5250.4250.642Qwen2-7B-InstructFS Rand4SP0.51.150.6750.5750.5750.5250.4000.608</p>
<p>Table 7 :
7
Model performance for BrainTeaser (Part 2).The presence of (Q) in the method column indicates that the results correspond to the quantized version of the model.
ModelMethodNum.Ex Task Temp Rep_Pen Original Semantic Context Ori. + Sem. Ori. + Sem. + Con. AverageRISCORE ResultsLlama3-70B zeroshot for QA &amp; Llama3-8B for distractorsMeta-Llama-3-70B-Instruct RISCORE Sim (Q)8SP0.51.150.8500.8250.7500.7750.6000.808Meta-Llama-3-70B-Instruct RISCORE Sim (Q)2SP0.51.150.8000.7750.8000.7500.6500.792Meta-Llama-3-70B-Instruct RISCORE Sim (Q)4SP0.51.150.8250.7500.8000.6750.6000.792Mixtral-8x7B-Instruct-v0.1RISCORE Sim8SP0.51.00.7000.7500.6000.6500.4750.683Mixtral-8x7B-Instruct-v0.1RISCORE Sim2SP0.7500.7500.5000.6500.3750.667Mixtral-8x7B-Instruct-v0.1RISCORE Sim4SP0.51.00.7250.6250.5750.5750.3750.642Meta-Llama-3-8B-InstructRISCORE Sim8SP0.51.00.7000.7250.6750.6500.5250.700Meta-Llama-3-8B-InstructRISCORE Sim4SP0.51.00.6500.6750.7000.5500.4250.675Meta-Llama-3-8B-InstructRISCORE Sim2SP0.51.00.7000.6000.5750.5250.3500.625Mistral-7B-Instruct-v0.2RISCORE Sim2SP0.51.00.5500.4500.4750.3750.492Mistral-7B-Instruct-v0.2RISCORE Sim4SP0.51.0Mistral-7B-Instruct-v0.2RISCORE Sim8SP0.51.00.4250.5250.4750.4000.3250.475Qwen2-7B-InstructRISCORE Sim8SP0.51.150.6000.7000.6250.5500.4250.642Qwen2-7B-InstructRISCORE Sim2SP0.51.150.6500.6500.5750.5500.4000.625Qwen2-7B-InstructRISCORE Sim4SP0.51.150.6000.6500.6250.5500.4500.625Llama3-70B fewshot for QA &amp; Llama3-8B for distractorsMeta-Llama-3-70B-Instruct RISCORE Sim (Q)8SP0.51.150.8000.8000.8250.7500.6500.808Meta-Llama-3-70B-Instruct RISCORE Sim (Q)4SP0.51.150.7750.8000.8000.7000.6000.792Meta-Llama-3-70B-Instruct RISCORE Sim (Q)2SP0.51.150.8000.7250.7250.7250.6000.750Mixtral-8x7B-Instruct-v0.1RISCORE Sim2SP0.51.00.7250.7250.5750.6750.4750.675Mixtral-8x7B-Instruct-v0.1RISCORE Sim8SP0.51.00.7000.6750.650.6250.4500.675Mixtral-8x7B-Instruct-v0.1RISCORE Sim4SP0.51.00.7000.6750.5750.6250.4000.650Meta-Llama-3-8B-InstructRISCORE Sim8SP0.51.00.7750.7500.7000.7000.6000.742Meta-Llama-3-8B-InstructRISCORE Sim2SP0.51.00.7250.7000.6250.6000.4000.683Meta-Llama-3-8B-InstructRISCORE Sim4SP0.51.00.7250.6250.6250.5500.4500.658Mistral-7B-Instruct-v0.2RISCORE Sim4SP0.51.00.6250.5750.4750.4750.3500.558Mistral-7B-Instruct-v0.2RISCORE Sim8SP0.51.00.5000.5500.5000.4250.3500.517Mistral-7B-Instruct-v0.2RISCORE Sim2SP0.51.00.5500.4250.4500.3750.2250.475Qwen2-7B-InstructRISCORE Sim4SP0.51.150.6250.7000.6500.5750.4500.658Qwen2-7B-InstructRISCORE Sim8SP0.51.150.6500.6750.6500.5750.4750.658Qwen2-7B-InstructRISCORE Sim2SP0.51.150.6250.6500.6000.5250.4000.625Llama3-70B fewshot for QA &amp; Llama3-70B for distractorsMeta-Llama-3-70B-Instruct RISCORE Sim (Q)4SP0.51.150.8750.7750.7250.7500.6000.792Meta-Llama-3-70B-Instruct RISCORE Sim (Q)2SP0.51.150.7750.8250.7500.7750.6750.783Meta-Llama-3-70B-Instruct RISCORE Sim (Q)8SP0.51.150.7750.7500.7750.7250.6000.767Mixtral-8x7B-Instruct-v0.1RISCORE Sim8SP0.51.00.7000.7250.6250.6000.4500.683Mixtral-8x7B-Instruct-v0.1RISCORE Sim2SP0.51.00.7000.7000.6000.6250.5000.667Mixtral-8x7B-Instruct-v0.1RISCORE Simm4SP0.51.00.7250.6500.5500.6000.4250.642Meta-Llama-3-8B-InstructRISCORE Sim8SP0.51.00.8000.6750.6250.6250.4750.700Meta-Llama-3-8B-InstructRISCORE Sim2SP0.51.00.6750.7000.6750.6000.4750.683Meta-Llama-3-8B-InstructRISCORE Sim4SP0.51.00.7250.6500.6250.5500.4750.667Mistral-7B-Instruct-v0.2RISCORE Sim4SP0.51.00.5750.5000.4500.3750.3000.508Mistral-7B-Instruct-v0.2RISCORE Sim2SP0.51.00.6250.4000.4750.3500.2750.500Mistral-7B-Instruct-v0.2RISCORE Sim8SP0.51.00.6000.4750.4250.4000.3000.500Qwen2-7B-InstructRISCORE Sim2SP0.51.150.6500.6000.6000.5000.4000.617Qwen2-7B-InstructRISCORE Sim4SP0.51.150.6500.6250.5750.6000.4750.617Qwen2-7B-InstructRISCORE Sim8SP0.51.150.6250.6250.6000.5750.4500.617</p>
<p>Table 8 :
8
Model performance for BrainTeaser (Part 3).The presence of (Q) in the method column indicates that the results correspond to the quantized version of the model.
ModelMethodNum.Ex Quant Temp Rep_Pen AverageChain-of-Thought Zero-shotMeta-Llama-3-70B-Instruct CoT_ZS (Q)04bit0.51.150.775Mixtral-8x7B-v0.1CoT_ZS0False0.51.00.675Meta-Llama-3-8B-InstructCoT_ZS0False0.51.00.619Mistral-7B-Instruct-v0.2CoT_ZS0False0.51.00.589Qwen2-7B-InstructCoT_ZS0False0.51.150.608Few-shot with CoT ExplanationsMeta-Llama-3-70B-Instruct CoT_FS (Q)24bit0.51.150.789Meta-Llama-3-70B-Instruct CoT_FS (Q)44bit0.51.150.783Meta-Llama-3-70B-Instruct CoT_FS (Q)84bit0.51.150.783Mixtral-8x7B-v0.1CoT_FS8False0.51.00.697Mixtral-8x7B-v0.1CoT_FS2False0.51.00.692Mixtral-8x7B-v0.1CoT_FS4False0.51.00.686Meta-Llama-3-8B-InstructCoT_FS4False0.51.00.672Meta-Llama-3-8B-InstructCoT_FS8False0.51.00.658Meta-Llama-3-8B-InstructCoT_FS2False0.51.00.625Mistral-7B-Instruct-v0.2CoT_FS4False0.51.00.603Mistral-7B-Instruct-v0.2CoT_FS8False0.51.00.597Mistral-7B-Instruct-v0.2CoT_FS2False0.51.00.594Qwen2-7B-InstructCoT_FS2False0.51.150.667Qwen2-7B-InstructCoT_FSCoT_FS8False0.51.150.625Few-shot with Random SelectionMeta-Llama-3-70B-Instruct FS Rand (Q)44bit0.51.150.800Meta-Llama-3-70B-Instruct FS Rand (Q)84bit0.51.150.772Meta-Llama-3-70B-Instruct FS Rand (Q)24bit0.51.150.769Mixtral-8x7B-v0.1FS Rand4False0.51.00.719Mixtral-8x7B-v0.1FS Rand8False0.51.00.711Mixtral-8x7B-v0.1FS Rand2False0.51.00.706Meta-Llama-3-8B-InstructFS Rand2False0.51.00.672Meta-Llama-3-8B-InstructFS Rand8False0.51.00.672Meta-Llama-3-8B-InstructFS Rand4False0.51.00.639Mistral-7B-Instruct-v0.2FS Rand2False0.51.00.586Mistral-7B-Instruct-v0.2FS Rand4False0.51.00.586Mistral-7B-Instruct-v0.2FS Rand8False0.51.00.586Qwen2-7B-InstructFS Rand8False0.51.150.700Qwen2-7B-InstructFS Rand2False0.51.150.689Qwen2-7B-InstructFS Rand4False0.51.150.683Few-shot with Semantic SimilarityMeta-Llama-3-70B-InstructFS Sim (Q)44bit0.51.150.817Meta-Llama-3-70B-InstructFS Sim (Q)84bit0.51.150.800Meta-Llama-3-70B-InstructFS Sim (Q)24bit0.51.150.792Mixtral-8x7B-v0.1FS Sim2False0.51.00.714Mixtral-8x7B-v0.1FS Sim4False0.51.00.692Mixtral-8x7B-v0.1FS Sim8False0.51.00.675Meta-Llama-3-8B-InstructFS Sim4False0.51.00.711Meta-Llama-3-8B-InstructFS Sim2False0.51.00.706Meta-Llama-3-8B-InstructFS Sim8False0.51.00.681Mistral-7B-Instruct-v0.2FS Sim4False0.51.00.633Mistral-7B-Instruct-v0.2FS Sim8False0.51.00.611Mistral-7B-Instruct-v0.2FS Sim2False0.51.00.608Qwen2-7B-InstructFS Sim8False0.51.150.731Qwen2-7B-InstructFS Sim2False0.51.150.722Qwen2-7B-InstructFS Sim4False0.51.150.714
Table 9: Model Performance for RiddleSense (Part 1).The column Quant indicates whether the model is quantized or not.</p>
<p>Code for our experiments is available at: https:// github.com/GiannisPana/RISCORE
We utilize semantic similarity for optimal exemplar selection, but other methods from literature may be used.
https://huggingface.co/Alibaba-NLP/ gte-large-en-v1.5
Specifically, we used version gpt-3.5-turbo-0125.
SentenceTransformers
Alibaba-NLP/gte-large-en-v1.5
SentenceTransformers
Alibaba-NLP/gte-large-en-v1.5
AcknowledgmentsThe research work was supported by the Hellenic Foundation for Research and Innovation (HFRI) under the 3rd Call for HFRI PhD Fellowships (Fellowship Number 5537).
A I , Meta , Llama 3 model card. 2024</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, Thirty-Fourth AAAI Conference on Artificial Intelligence. 2020</p>
<p>Wordnet -a lexical database for the english language. 2006. March 8th, 2006Cognitive Science Laboratory, Princeton University</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, Zhifang Sui, arXiv:2301.00234A survey on in-context learning. 2024Preprint</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.007202023Preprint</p>
<p>Puzzle solving using reasoning of large language models: A survey. Panagiotis Giadikiaroglou, Maria Lymperaiou, arXiv:2402.112912024PreprintGiorgos Filandrianos, and Giorgos Stamou</p>
<p>Accelerate: Training and inference at scale made simple. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, Benjamin Bossan, 2022</p>
<p>Inductive reasoning in humans and large language models. Simon Jerome Han, Keith J Ransom, Andrew Perfors, Charles Kemp, 10.1016/j.cogsys.2023.101155Cognitive Systems Research. 831011552024</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023aPreprint</p>
<p>BRAINTEASER: Lateral thinking puzzles for large language models. Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati, 10.18653/v1/2023.emnlp-main.885Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023bAssociation for Computational Linguistics</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162023Preprint</p>
<p>Same task, more tokens: the impact of input length on the reasoning performance of large language models. Mosh Levy, Alon Jacoby, Yoav Goldberg, arXiv:2402.148482024Preprint</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019Preprint</p>
<p>RiddleSense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee, Ren, 10.18653/v1/2021.findings-acl.131Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational Linguistics2022. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Improve mathematical reasoning in language models by automated process supervision. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, Abhinav Rastogi, arXiv:2406.065922024Preprint</p>
<p>WordNet: A lexical database for English. A George, Miller, Speech and Natural Language: Proceedings of a Workshop. Harriman, New York1992. February 23-26, 1992</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/2022.emnlp-main.759Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.18653/v1/2023.acl-long.294Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>In-context learning with iterative demonstration selection. Chengwei Qin, Aston Zhang, Chen Chen, Anirudh Dagar, Wenming Ye, arXiv:2310.098812024Preprint</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>FaiRR: Faithful and robust deductive reasoning over natural language. Soumya Sanyal, Harman Singh, Xiang Ren, 10.18653/v1/2022.acl-long.77Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland20221Association for Computational Linguistics</p>
<p>Atomic: an atlas of machine commonsense for if-then reasoning. Maarten Sap, Le Ronan, Emily Bras, Chandra Allaway, Nicholas Bhagavatula, Hannah Lourie, Brendan Rashkin, Noah A Roof, Yejin Smith, Choi, 10.1609/aaai.v33i01.33013027Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence. the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial IntelligenceAAAI Press2019AAAI'19/IAAI'19/EAAI'19</p>
<p>Commonsense reasoning for natural language processing. Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, Dan Roth, 10.18653/v1/2020.acl-tutorials.7Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts. the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial AbstractsOnline. Association for Computational Linguistics2020</p>
<p>Conceptnet 5.5: an open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17. the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17AAAI Press2017</p>
<p>Life is a circus and we are the clowns: Automatically finding analogies between situations and processes. Oren Sultan, Dafna Shahaf, 10.18653/v1/2022.emnlp-main.232Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Roxana Szomiu, Adrian Groza, arXiv:2112.05742A puzzlebased dataset for natural language inference. 2021arXiv preprint</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Learning to retrieve in-context examples for large language models. Liang Wang, Nan Yang, Furu Wei, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, arXiv:2206.076822022Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
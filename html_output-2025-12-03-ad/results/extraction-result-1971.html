<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1971 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1971</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1971</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-281950588</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.08556v1.pdf" target="_blank">DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model</a></p>
                <p><strong>Paper Abstract:</strong> Achieving generalized in-hand object rotation remains a significant challenge in robotics, largely due to the difficulty of transferring policies from simulation to the real world. The complex, contact-rich dynamics of dexterous manipulation create a"reality gap"that has limited prior work to constrained scenarios involving simple geometries, limited object sizes and aspect ratios, constrained wrist poses, or customized hands. We address this sim-to-real challenge with a novel framework that enables a single policy, trained in simulation, to generalize to a wide variety of objects and conditions in the real world. The core of our method is a joint-wise dynamics model that learns to bridge the reality gap by effectively fitting limited amount of real-world collected data and then adapting the sim policy's actions accordingly. The model is highly data-efficient and generalizable across different whole-hand interaction distributions by factorizing dynamics across joints, compressing system-wide influences into low-dimensional variables, and learning each joint's evolution from its own dynamic profile, implicitly capturing these net effects. We pair this with a fully autonomous data collection strategy that gathers diverse, real-world interaction data with minimal human intervention. Our complete pipeline demonstrates unprecedented generality: a single policy successfully rotates challenging objects with complex shapes (e.g., animals), high aspect ratios (up to 5.33), and small sizes, all while handling diverse wrist orientations and rotation axes. Comprehensive real-world evaluations and a teleoperation application for complex tasks validate the effectiveness and robustness of our approach. Website: https://meowuu7.github.io/DexNDM/</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1971.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1971.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DexNDM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DEXNDM: Joint-Wise Neural Dynamics Model with Residual Policy and Autonomous Data Collection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real pipeline for dexterous in-hand continuous axis-oriented rotation that (1) learns joint-wise neural dynamics from autonomously collected object-loaded real data (the "Chaos Box"), (2) identifies low-level actuator/link parameters via automatic system identification, and (3) trains a residual policy to compensate a simulation-trained base policy for real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>In-hand object rotation (axis-oriented continuous rotation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>20 s episode horizon; control loop 20 Hz (0.05 s control step)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Per-joint PD control mapping positional targets to torques (Kp, Kd); per-link masses identified via automatic system identification; torque-level execution via PD controller at 20 Hz; trained/residual actions operate on relative joint target positions.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Coriolis terms neglected in analysis/simplified dynamics derivation; no explicit high-fidelity friction/backlash/compliance model reported; contact model treated implicitly via object-loaded data and joint-wise effective terms rather than explicit contact parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>High-fidelity rigid-body simulation (Isaac Gym) with automatic SysID to align PD gains and link masses, domain randomization of physical properties, and a learned data-driven joint-wise dynamics residual that captures unmodeled actuator/object effects.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td>Per-joint PD gains and per-link masses were identified and reported (Tables 11 & 12); control executed at 20 Hz; physical randomization and disturbance forces applied during training (disturbance scale tied to object mass). Exact per-joint Kp/Kd and link mass values are reported in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Multiple metrics: Radians rotated (Rot, real world), Time-to-Fall (TTF, s), Rotation Reward / Rotation Penalty (simulation), and Goal-Oriented Success (simulation). Example reported real-world numbers: Regular objects, ±x-axis DexNDM Rot = 11.36 ± 0.40 rad, TTF = 32.40 ± 1.78 s (Table 4); substantial gains over direct transfer and whole-hand NDM.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>DexNDM produced large improvements over direct transfer and whole-hand dynamics compensators in real hardware: e.g., direct transfer and whole-hand NDM often had much lower radians rotated and shorter TTFs (see Table 4). Qualitatively, methods without object-loaded modeling (UAN/ASAP) failed in real tests while DexNDM succeeded on many challenging objects (small, high aspect-ratio, complex shapes).</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Ablations show (1) joint-wise dynamics >> whole-hand dynamics under low-data and distribution shift (better sample efficiency and OOD generalization); (2) simulation pretraining improves real-model fit; (3) injecting noise into replayed actions increases coverage and improves transfer; (4) collecting object-loaded data (Chaos Box) is critical—free-hand or task-aware small/noisy object-state data are insufficient; (5) residual policy training is more stable than direct finetuning. The most critical factors were (i) presence of object-loaded real data and (ii) joint-wise factorization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Dynamics model pre-trained on simulation then fine-tuned on real data; dynamics training reported: training on multi-GPU (eight A10 GPUs) for ~2 days (sim-to-sim experiments note); residual policy training on simulation data for one epoch typically ~10 hours on eight A10 GPUs. Data collection: one 400-step trajectory ≈ 20 s; 4,000 autonomously collected trajectories per wrist orientation (used in experiments) were collected (automated overnight runs).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Yes — explicit comparison between joint-wise vs whole-hand learned dynamics: joint-wise nearly matches whole-hand expressivity in high-data in-domain conditions but substantially outperforms whole-hand in low-data and cross-domain (OOD) settings. Whole-hand can slightly outperform joint-wise given abundant in-distribution data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized physical object properties across training object sets; random disturbance force on object (scale = 2 × object mass, resampled each timestep with p=0.25); added uniform noise to joint positions U(0, 0.005); during chaos-box collection, Gaussian noise (σ = 0.01) was added to actions with probability 0.5 to broaden coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>LEAP anthropomorphic robotic hand mounted on a Franka Emika Panda arm (positional control with PD gains; 20 Hz control rate).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Failures in simpler baselines were attributed to (a) OOD dynamics introduced by object-hand contact loads not modeled by free-hand compensators, (b) inability to collect high-quality task-relevant object-state datasets (pose estimation failures, occlusions), and (c) high-dimensional whole-hand models overfitting under limited real data. DexNDM addresses these by object-loaded autonomous data and joint-wise factorization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Actuator-level sim-to-real transfer for contact-rich, dexterous manipulation critically depends on (1) identifying low-level actuator parameters (per-joint PD gains and link inertial parameters) and (2) modeling/covering object-induced, time-varying loads in training data. Factorizing dynamics per joint (joint-wise models) acts as an information bottleneck that reduces distribution shift and dramatically improves sample efficiency and OOD generalization; collecting object-loaded, randomized real data (rather than free-hand waves or small noisy pose-annotated sets) is essential to bridge the reality gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1971.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1971.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UAN (Unsupervised Actuator Net) / "Bridging the sim-to-real gap for athletic loco-manipulation" (Fey et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior sim-to-real compensator approach for actuators (history-based/shared actuator network) that was re-implemented and evaluated as a baseline in this work; trained on free-hand/replay trajectories (no object loads) and produced compensators that failed to generalize to contact-rich in-hand object rotation in hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging the sim-to-real gap for athletic loco-manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Baseline compensator applied to in-hand rotation evaluation (but trained on free-hand actuator data)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>20 s episode horizon; control loop 20 Hz (as used in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>Training data: minimal/low contact (free-hand); Target task: contact-rich — mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>A shared neural compensator for actuator dynamics (history-based design per actuator) trained to predict dynamics corrections from free-hand transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>No modeling or data of object-induced loads; actuator compensator trained only on free-hand or wave-based signals, thereby ignoring contact-rich interaction effects.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Data-driven compensator trained on free-hand transition data (limited fidelity for manipulation tasks with objects).</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Same rotation metrics as main paper when evaluated, but performance: baseline compensators failed in hardware — unable to rotate even simple cylinder objects (practical transfer success ≈ 0 for real-world in-hand rotation tests reported).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Failed in real-world transfer for in-hand rotation despite functioning in some sim-to-sim tests; could not rotate simple cylinder in hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Authors attribute failure to OOD generalization: compensators trained exclusively on free-hand data do not capture object-induced interaction dynamics; thus the most critical missing factor is object-loaded training data.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared to DexterNDM and ASAP in this work: UAN outperformed ASAP in some sim-to-sim cross-simulator transfer, but both UAN and ASAP failed in real-world in-hand manipulation tests when trained without object-loaded data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>LEAP hand (evaluation hardware used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Primary cause: OOD dynamics due to absence of object-induced loads in training data (compensator learned free-hand actuator dynamics that do not generalize to contact-rich object manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Actuator compensators trained on free-hand/wave data are insufficient for contact-rich manipulation transfer — actuator dynamics are strongly coupled with object interaction dynamics and must be trained with object-loaded transitions or modeled differently to succeed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1971.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1971.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASAP: Aligning Simulation And real-world Physics for learning agile humanoid whole-body skills</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real pipeline for whole-body skills that was adapted as a baseline in this work (train compensators and then finetune policies); when applied to dexterous in-hand rotation with only free-hand or small/noisy object datasets, the approach failed in real-world tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Baseline compensator + policy finetuning applied to in-hand object rotation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>20 s episode horizon; control loop 20 Hz (as used in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>Training data used in re-implementation: free-hand or limited object-state annotated (small/noisy) — low/partial contact coverage; target task: contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Whole-body compensator concept adapted: trained to track hand (and in sim-to-sim setting object) states; in this paper compensator training attempted on free-hand and limited task-relevant datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Unable to model or replicate realistic object loads from autonomous data; compensator training lacked object-loaded diversity needed for in-hand manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Data-driven compensator approach intended for whole-body systems; when applied to dexterous hand without appropriate object-loaded data, fidelity insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Evaluated using rotation and TTF metrics; outcome: ASAP-trained compensators/policies failed in hardware (unable to rotate basic cylinder), so measured real-world transfer success was effectively zero.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Failed real-world transfer for in-hand rotation in this study; in sim-to-sim comparisons ASAP lagged behind UAN and DexNDM.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared (empirically) against UAN and DexNDM in sim-to-sim and sim-to-real tests here; ASAP performed worse than UAN in sim-to-sim and failed in real hardware for manipulation task when trained without object-loaded data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>LEAP hand (evaluation hardware used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Failure attributed to inability to account for object-hand interaction dynamics when compensator trained on free-hand data or on small/noisy object-state datasets; replicating object loads in simulator proved infeasible under parameter mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Whole-system compensators or finetuning approaches require object-loaded, distributionally relevant real data for contact-rich tasks; without it, even advanced whole-body sim-to-real methods can fail catastrophically in manipulation domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1971.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1971.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnyRotate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AnyRotate: Gravity-invariant in-hand object rotation with sim-to-real touch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work on multi-axis in-hand rotation that uses tactile sensing and demonstrates multi-axis rotation for regular-sized objects; in this paper it was re-implemented in simulation for comparison and compared against reported real-world results for a set of replicable objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual dexterity: In-hand reorientation of novel and complex object shapes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>In-hand object rotation (axis-oriented / multi-axis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>20 s episode horizon (comparable evaluation protocol used in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Rotation degrees (radians) and Time-to-Fall (TTF) were used for comparison on replicable objects; DexNDM reported higher rotations and longer survival on several difficult items (Table 2/related text).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Paper reports DexNDM substantially outperforms AnyRotate on smaller and higher-aspect-ratio objects; AnyRotate focused on moderately sized simple shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Compared across different hand hardware in literature (AnyRotate used customized tactile hardware; direct mapping to LEAP was nontrivial).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Not directly analyzed in this paper beyond noting differences in required hardware (tactile sensors) and object size/aspect-ratio coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Comparative evidence suggests tactile-rich sim-to-real approaches focused on specific hardware and normal-sized objects may not generalize to small/high-aspect-ratio objects; hardware-actuator/interfacing differences and lack of object-loaded broad coverage can limit transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1971.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1971.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VisualDexterity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Dexterity: In-hand reorientation of novel and complex object shapes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior influential work that demonstrates in-air object reorientation using vision; in this paper it is discussed qualitatively and compared via an approximated survival-rotation-angle metric from videos, but not reimplemented on the LEAP hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual dexterity: In-hand reorientation of novel and complex object shapes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>In-hand reorientation (goal-pose driven), compared via survival rotation before drop</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>Not explicitly reported here (comparison based on demo videos)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Survival rotation angle (angle before drop) estimated from published demonstration videos; used for cross-method qualitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>DexNDM reports comparable or superior survival rotation angles on objects where Visual Dexterity struggled (e.g., elephant, bunny, teapot), and demonstrates better handling of small and high-aspect-ratio objects in this paper's hardware setup.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Original Visual Dexterity work used a D'Claw setup; not directly adapted to LEAP in this paper (attempts to adapt their code to LEAP failed in simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Not analyzed in detail in this paper; differences in task definitions (continuous axis rotation vs goal-oriented reorientation) and hardware make direct comparisons difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Qualitative comparison highlights that differences in task definition, hardware (hand size and sensing), and evaluation metrics complicate cross-paper transfer claims; actuator/hand differences and evaluation protocol must be considered when reasoning about which actuator dynamics parameters matter for sim-to-real transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills <em>(Rating: 2)</em></li>
                <li>Bridging the sim-to-real gap for athletic loco-manipulation <em>(Rating: 2)</em></li>
                <li>Visual dexterity: In-hand reorientation of novel and complex object shapes <em>(Rating: 2)</em></li>
                <li>Anyrotate: Gravity-invariant in-hand object rotation with sim-to-real touch <em>(Rating: 2)</em></li>
                <li>An efficient model-based approach on learning agile motor skills without reinforcement (MB-Max / bin Shi et al., 2024) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1971",
    "paper_id": "paper-281950588",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "DexNDM",
            "name_full": "DEXNDM: Joint-Wise Neural Dynamics Model with Residual Policy and Autonomous Data Collection",
            "brief_description": "A sim-to-real pipeline for dexterous in-hand continuous axis-oriented rotation that (1) learns joint-wise neural dynamics from autonomously collected object-loaded real data (the \"Chaos Box\"), (2) identifies low-level actuator/link parameters via automatic system identification, and (3) trains a residual policy to compensate a simulation-trained base policy for real-world deployment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "In-hand object rotation (axis-oriented continuous rotation)",
            "task_timescale": "20 s episode horizon; control loop 20 Hz (0.05 s control step)",
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "Per-joint PD control mapping positional targets to torques (Kp, Kd); per-link masses identified via automatic system identification; torque-level execution via PD controller at 20 Hz; trained/residual actions operate on relative joint target positions.",
            "actuator_parameters_simplified": "Coriolis terms neglected in analysis/simplified dynamics derivation; no explicit high-fidelity friction/backlash/compliance model reported; contact model treated implicitly via object-loaded data and joint-wise effective terms rather than explicit contact parameterization.",
            "fidelity_level_description": "High-fidelity rigid-body simulation (Isaac Gym) with automatic SysID to align PD gains and link masses, domain randomization of physical properties, and a learned data-driven joint-wise dynamics residual that captures unmodeled actuator/object effects.",
            "parameter_specific_fidelity": "Per-joint PD gains and per-link masses were identified and reported (Tables 11 & 12); control executed at 20 Hz; physical randomization and disturbance forces applied during training (disturbance scale tied to object mass). Exact per-joint Kp/Kd and link mass values are reported in the paper's tables.",
            "transfer_success_metric": "Multiple metrics: Radians rotated (Rot, real world), Time-to-Fall (TTF, s), Rotation Reward / Rotation Penalty (simulation), and Goal-Oriented Success (simulation). Example reported real-world numbers: Regular objects, ±x-axis DexNDM Rot = 11.36 ± 0.40 rad, TTF = 32.40 ± 1.78 s (Table 4); substantial gains over direct transfer and whole-hand NDM.",
            "sim_vs_real_performance": "DexNDM produced large improvements over direct transfer and whole-hand dynamics compensators in real hardware: e.g., direct transfer and whole-hand NDM often had much lower radians rotated and shorter TTFs (see Table 4). Qualitatively, methods without object-loaded modeling (UAN/ASAP) failed in real tests while DexNDM succeeded on many challenging objects (small, high aspect-ratio, complex shapes).",
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Ablations show (1) joint-wise dynamics &gt;&gt; whole-hand dynamics under low-data and distribution shift (better sample efficiency and OOD generalization); (2) simulation pretraining improves real-model fit; (3) injecting noise into replayed actions increases coverage and improves transfer; (4) collecting object-loaded data (Chaos Box) is critical—free-hand or task-aware small/noisy object-state data are insufficient; (5) residual policy training is more stable than direct finetuning. The most critical factors were (i) presence of object-loaded real data and (ii) joint-wise factorization.",
            "computational_cost_reported": true,
            "computational_cost_details": "Dynamics model pre-trained on simulation then fine-tuned on real data; dynamics training reported: training on multi-GPU (eight A10 GPUs) for ~2 days (sim-to-sim experiments note); residual policy training on simulation data for one epoch typically ~10 hours on eight A10 GPUs. Data collection: one 400-step trajectory ≈ 20 s; 4,000 autonomously collected trajectories per wrist orientation (used in experiments) were collected (automated overnight runs).",
            "fidelity_comparison": "Yes — explicit comparison between joint-wise vs whole-hand learned dynamics: joint-wise nearly matches whole-hand expressivity in high-data in-domain conditions but substantially outperforms whole-hand in low-data and cross-domain (OOD) settings. Whole-hand can slightly outperform joint-wise given abundant in-distribution data.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized physical object properties across training object sets; random disturbance force on object (scale = 2 × object mass, resampled each timestep with p=0.25); added uniform noise to joint positions U(0, 0.005); during chaos-box collection, Gaussian noise (σ = 0.01) was added to actions with probability 0.5 to broaden coverage.",
            "robot_type": "LEAP anthropomorphic robotic hand mounted on a Franka Emika Panda arm (positional control with PD gains; 20 Hz control rate).",
            "transfer_failure_analysis": "Failures in simpler baselines were attributed to (a) OOD dynamics introduced by object-hand contact loads not modeled by free-hand compensators, (b) inability to collect high-quality task-relevant object-state datasets (pose estimation failures, occlusions), and (c) high-dimensional whole-hand models overfitting under limited real data. DexNDM addresses these by object-loaded autonomous data and joint-wise factorization.",
            "key_finding_for_theory": "Actuator-level sim-to-real transfer for contact-rich, dexterous manipulation critically depends on (1) identifying low-level actuator parameters (per-joint PD gains and link inertial parameters) and (2) modeling/covering object-induced, time-varying loads in training data. Factorizing dynamics per joint (joint-wise models) acts as an information bottleneck that reduces distribution shift and dramatically improves sample efficiency and OOD generalization; collecting object-loaded, randomized real data (rather than free-hand waves or small noisy pose-annotated sets) is essential to bridge the reality gap.",
            "uuid": "e1971.0"
        },
        {
            "name_short": "UAN",
            "name_full": "UAN (Unsupervised Actuator Net) / \"Bridging the sim-to-real gap for athletic loco-manipulation\" (Fey et al., 2025)",
            "brief_description": "A prior sim-to-real compensator approach for actuators (history-based/shared actuator network) that was re-implemented and evaluated as a baseline in this work; trained on free-hand/replay trajectories (no object loads) and produced compensators that failed to generalize to contact-rich in-hand object rotation in hardware.",
            "citation_title": "Bridging the sim-to-real gap for athletic loco-manipulation",
            "mention_or_use": "use",
            "task_name": "Baseline compensator applied to in-hand rotation evaluation (but trained on free-hand actuator data)",
            "task_timescale": "20 s episode horizon; control loop 20 Hz (as used in evaluation)",
            "task_contact_ratio": "Training data: minimal/low contact (free-hand); Target task: contact-rich — mismatch",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "A shared neural compensator for actuator dynamics (history-based design per actuator) trained to predict dynamics corrections from free-hand transitions.",
            "actuator_parameters_simplified": "No modeling or data of object-induced loads; actuator compensator trained only on free-hand or wave-based signals, thereby ignoring contact-rich interaction effects.",
            "fidelity_level_description": "Data-driven compensator trained on free-hand transition data (limited fidelity for manipulation tasks with objects).",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Same rotation metrics as main paper when evaluated, but performance: baseline compensators failed in hardware — unable to rotate even simple cylinder objects (practical transfer success ≈ 0 for real-world in-hand rotation tests reported).",
            "sim_vs_real_performance": "Failed in real-world transfer for in-hand rotation despite functioning in some sim-to-sim tests; could not rotate simple cylinder in hardware.",
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Authors attribute failure to OOD generalization: compensators trained exclusively on free-hand data do not capture object-induced interaction dynamics; thus the most critical missing factor is object-loaded training data.",
            "computational_cost_reported": null,
            "computational_cost_details": null,
            "fidelity_comparison": "Compared to DexterNDM and ASAP in this work: UAN outperformed ASAP in some sim-to-sim cross-simulator transfer, but both UAN and ASAP failed in real-world in-hand manipulation tests when trained without object-loaded data.",
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "robot_type": "LEAP hand (evaluation hardware used in this paper)",
            "transfer_failure_analysis": "Primary cause: OOD dynamics due to absence of object-induced loads in training data (compensator learned free-hand actuator dynamics that do not generalize to contact-rich object manipulation).",
            "key_finding_for_theory": "Actuator compensators trained on free-hand/wave data are insufficient for contact-rich manipulation transfer — actuator dynamics are strongly coupled with object interaction dynamics and must be trained with object-loaded transitions or modeled differently to succeed.",
            "uuid": "e1971.1"
        },
        {
            "name_short": "ASAP",
            "name_full": "ASAP: Aligning Simulation And real-world Physics for learning agile humanoid whole-body skills",
            "brief_description": "A sim-to-real pipeline for whole-body skills that was adapted as a baseline in this work (train compensators and then finetune policies); when applied to dexterous in-hand rotation with only free-hand or small/noisy object datasets, the approach failed in real-world tests.",
            "citation_title": "Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills",
            "mention_or_use": "use",
            "task_name": "Baseline compensator + policy finetuning applied to in-hand object rotation evaluation",
            "task_timescale": "20 s episode horizon; control loop 20 Hz (as used in evaluation)",
            "task_contact_ratio": "Training data used in re-implementation: free-hand or limited object-state annotated (small/noisy) — low/partial contact coverage; target task: contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "Whole-body compensator concept adapted: trained to track hand (and in sim-to-sim setting object) states; in this paper compensator training attempted on free-hand and limited task-relevant datasets.",
            "actuator_parameters_simplified": "Unable to model or replicate realistic object loads from autonomous data; compensator training lacked object-loaded diversity needed for in-hand manipulation.",
            "fidelity_level_description": "Data-driven compensator approach intended for whole-body systems; when applied to dexterous hand without appropriate object-loaded data, fidelity insufficient.",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Evaluated using rotation and TTF metrics; outcome: ASAP-trained compensators/policies failed in hardware (unable to rotate basic cylinder), so measured real-world transfer success was effectively zero.",
            "sim_vs_real_performance": "Failed real-world transfer for in-hand rotation in this study; in sim-to-sim comparisons ASAP lagged behind UAN and DexNDM.",
            "sensitivity_analysis_performed": null,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": null,
            "computational_cost_details": null,
            "fidelity_comparison": "Compared (empirically) against UAN and DexNDM in sim-to-sim and sim-to-real tests here; ASAP performed worse than UAN in sim-to-sim and failed in real hardware for manipulation task when trained without object-loaded data.",
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "robot_type": "LEAP hand (evaluation hardware used in this paper)",
            "transfer_failure_analysis": "Failure attributed to inability to account for object-hand interaction dynamics when compensator trained on free-hand data or on small/noisy object-state datasets; replicating object loads in simulator proved infeasible under parameter mismatch.",
            "key_finding_for_theory": "Whole-system compensators or finetuning approaches require object-loaded, distributionally relevant real data for contact-rich tasks; without it, even advanced whole-body sim-to-real methods can fail catastrophically in manipulation domains.",
            "uuid": "e1971.2"
        },
        {
            "name_short": "AnyRotate",
            "name_full": "AnyRotate: Gravity-invariant in-hand object rotation with sim-to-real touch",
            "brief_description": "Prior work on multi-axis in-hand rotation that uses tactile sensing and demonstrates multi-axis rotation for regular-sized objects; in this paper it was re-implemented in simulation for comparison and compared against reported real-world results for a set of replicable objects.",
            "citation_title": "Visual dexterity: In-hand reorientation of novel and complex object shapes",
            "mention_or_use": "mention",
            "task_name": "In-hand object rotation (axis-oriented / multi-axis)",
            "task_timescale": "20 s episode horizon (comparable evaluation protocol used in comparisons)",
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": null,
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Rotation degrees (radians) and Time-to-Fall (TTF) were used for comparison on replicable objects; DexNDM reported higher rotations and longer survival on several difficult items (Table 2/related text).",
            "sim_vs_real_performance": "Paper reports DexNDM substantially outperforms AnyRotate on smaller and higher-aspect-ratio objects; AnyRotate focused on moderately sized simple shapes.",
            "sensitivity_analysis_performed": null,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": null,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "robot_type": "Compared across different hand hardware in literature (AnyRotate used customized tactile hardware; direct mapping to LEAP was nontrivial).",
            "transfer_failure_analysis": "Not directly analyzed in this paper beyond noting differences in required hardware (tactile sensors) and object size/aspect-ratio coverage.",
            "key_finding_for_theory": "Comparative evidence suggests tactile-rich sim-to-real approaches focused on specific hardware and normal-sized objects may not generalize to small/high-aspect-ratio objects; hardware-actuator/interfacing differences and lack of object-loaded broad coverage can limit transfer.",
            "uuid": "e1971.3"
        },
        {
            "name_short": "VisualDexterity",
            "name_full": "Visual Dexterity: In-hand reorientation of novel and complex object shapes",
            "brief_description": "A prior influential work that demonstrates in-air object reorientation using vision; in this paper it is discussed qualitatively and compared via an approximated survival-rotation-angle metric from videos, but not reimplemented on the LEAP hardware.",
            "citation_title": "Visual dexterity: In-hand reorientation of novel and complex object shapes",
            "mention_or_use": "mention",
            "task_name": "In-hand reorientation (goal-pose driven), compared via survival rotation before drop",
            "task_timescale": "Not explicitly reported here (comparison based on demo videos)",
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": null,
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Survival rotation angle (angle before drop) estimated from published demonstration videos; used for cross-method qualitative comparison.",
            "sim_vs_real_performance": "DexNDM reports comparable or superior survival rotation angles on objects where Visual Dexterity struggled (e.g., elephant, bunny, teapot), and demonstrates better handling of small and high-aspect-ratio objects in this paper's hardware setup.",
            "sensitivity_analysis_performed": null,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": null,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "robot_type": "Original Visual Dexterity work used a D'Claw setup; not directly adapted to LEAP in this paper (attempts to adapt their code to LEAP failed in simulation).",
            "transfer_failure_analysis": "Not analyzed in detail in this paper; differences in task definitions (continuous axis rotation vs goal-oriented reorientation) and hardware make direct comparisons difficult.",
            "key_finding_for_theory": "Qualitative comparison highlights that differences in task definition, hardware (hand size and sensing), and evaluation metrics complicate cross-paper transfer claims; actuator/hand differences and evaluation protocol must be considered when reasoning about which actuator dynamics parameters matter for sim-to-real transfer.",
            "uuid": "e1971.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills",
            "rating": 2
        },
        {
            "paper_title": "Bridging the sim-to-real gap for athletic loco-manipulation",
            "rating": 2
        },
        {
            "paper_title": "Visual dexterity: In-hand reorientation of novel and complex object shapes",
            "rating": 2
        },
        {
            "paper_title": "Anyrotate: Gravity-invariant in-hand object rotation with sim-to-real touch",
            "rating": 2
        },
        {
            "paper_title": "An efficient model-based approach on learning agile motor skills without reinforcement (MB-Max / bin Shi et al., 2024)",
            "rating": 1
        }
    ],
    "cost": 0.023434499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DEXNDM: CLOSING THE REALITY GAP FOR DEXTEROUS IN-HAND ROTATION VIA JOINT-WISE NEURAL DYNAMICS MODEL
9 Oct 2025</p>
<p>Dataset Trajectory 
Joint-Wise Neural Dynamics</p>
<p>𝐚 !"# 𝐪 !, 𝐚 !"% …Generalist 𝐪 !"#$% 
Joint-Wise Neural Dynamics</p>
<p>Policy 
Joint-Wise Neural Dynamics</p>
<p>DEXNDM: CLOSING THE REALITY GAP FOR DEXTEROUS IN-HAND ROTATION VIA JOINT-WISE NEURAL DYNAMICS MODEL
9 Oct 202567625411530D5E45A8A66EB873613FB1arXiv:2510.08556v1[cs.RO]
plete pipeline demonstrates unprecedented generality: a single policy successfully rotates challenging objects with complex shapes (e.g., animals), high aspect ratios (up to 5.33), and small sizes, all while handling diverse wrist orientations and rotation axes.Comprehensive real-world evaluations and a teleoperation application for complex tasks validate the effectiveness and robustness of our approach.</p>
<p>Xueyi Liu 1,3 , He Wang 2,4 , Li Yi 1,3 1 Tsinghua University 2 Peking University 3 Shanghai Qi Zhi Institute 4 Galbot Project website: meowuu7.github.io/DexNDM Figure 1: We introduce DexNDM, a sim-to-real approach that enables unprecedented in-hand rotation in the real world.We master a wide object distribution, including (A) challenging geometries and (B) complex shapes, across (C) rich wrist orientations.(D) A teleoperation application.Videos in website.</p>
<p>ABSTRACT</p>
<p>Achieving generalized in-hand object rotation remains a significant challenge in robotics, largely due to the difficulty of transferring policies from simulation to the real world.The complex, contact-rich dynamics of dexterous manipulation create a "reality gap" that has limited prior work to constrained scenarios involving simple geometries, limited object sizes and aspect ratios, constrained wrist poses, or customized hands.We address this sim-to-real challenge with a novel framework that enables a single policy, trained in simulation, to generalize to a wide variety of objects and conditions in the real world.The core of our method is a jointwise dynamics model that learns to bridge the reality gap by effectively fitting limited amount of real-world collected data and then adapting the sim policy's actions accordingly.The model is highly data-efficient and generalizable across different whole-hand interaction distributions by factorizing dynamics across joints, compressing system-wide influences into low-dimensional variables, and learning each joint's evolution from its own dynamic profile, implicitly capturing these net effects.We pair this with a fully autonomous data collection strategy that gathers diverse, real-world interaction data with minimal human intervention.Our com-</p>
<p>INTRODUCTION</p>
<p>Advancing dexterous manipulation is essential to achieving highly capable embodied intelligence.</p>
<p>A fundamental yet challenging skill in this domain is in-hand object rotation.The long-standing goal, which we also pursue in this work, is to develop a general-purpose policy that can rotate a broad distribution of objects across diverse wrist orientations and rotation axes in the real world.</p>
<p>Despite recent progress, the community has yet to achieve this level of generality.Existing methods (Chen et al., 2022;Yang et al., 2024;Qi et al., 2023;Wang et al., 2024;Zhao et al., 2025;Yuan et al., 2023) are often constrained to specific scenarios: some assume a consistently up-facing hand, others handle only a limited set of simple, regular-sized objects, and many rely on expensive, customized hardware with sophisticated tactile sensing.While some approaches (Yang et al., 2024) show generality in one dimension, such as rotation axes, they are limited in others, like object complexity.To our knowledge, no prior work demonstrates robust, in-the-air rotation for a wide spectrum of objects-including complex shapes, high aspect ratios, and varied sizes-under diverse wrist orientations and rotation axes.</p>
<p>The primary barrier to this goal is the formidable "sim-to-real gap", due to the difficulty in modeling the complex interaction dynamics marked by rich, rapidly varying, and load-dependent contacts.This undermines both model-based (Pang &amp; Tedrake, 2021;Pang et al., 2023;Suh et al., 2025) and model-free (Qi et al., 2023;Chen et al., 2022;Yang et al., 2024) approaches.A promising idea for sim-to-real transfer is learning a neural dynamics model from real-world data (He et al., 2025;bin Shi et al., 2024).This approach has proven effective in locomotion, where relatively easier failure recovery and readily observable states permit efficient collection of distributionally relevant task data.This success, however, does not easily translate to general-purpose manipulation, where the requirements for data volume and distributional relevance create an inescapable conflict.The need for generality demands massive data to cover diverse objects.Yet, ensuring this data is distributionally relevant is sometimes impossible and operationally far more complex: suboptimal deployable policy cannot manipulate hard objects (e.g., long); catastrophic failures (i.e., dropping the object) necessitates frequent human intervention for resets; severe hand-induced occlusions complicate accurately tracking states of diverse objects.This conflict creates a critical bottleneck for the field.</p>
<p>To overcome these challenges, we introduce a framework that breaks this inescapable conflict by fundamentally rethinking both the model and the data.Our central insight is to factorize the learning problem through a more generalizable dynamics model, which in turn enables a more scalable data collection strategy.First, instead of modeling the high-dimensional hand-object system as a whole (bin Shi et al., 2024), we learn a joint-wise neural dynamics model.This model factorizes the system and predicts the evolution of each joint using only its own proprioceptive history, generalizing the idea of RMA (Kumar et al., 2021).This design directly confronts the challenges: it is inherently immune to object state estimation difficulty, and by distilling system-wide influences-selfactuation, inter-joint couplings and object loads-into low-dimensional and task-sufficient net effects with reduced nuisance variability, the model becomes highly sample-efficient and generalizable without sacrificing expressivity as evidenced by experiments.This enhanced generalizability is the key that unlocks our second innovation: a fully autonomous data collection strategy.By applying randomized loads to the hand in a task-agnostic manner, we gather data while eliminating catastrophic failures and the need for human resets.This allows us to learn a dynamics model generalizing well to our task of interest from cheap and scalable data, which we then use to train a residual policy that adapts a simulation-trained base policy to the real world, achieving broad generality.We attain the base policy via a specialist-to-generalist pipeline: train category-specific experts on data spanning aspect ratios and geometric complexities, then distill them into a unified policy.</p>
<p>We validate our method in both the simulation and the real world.In simulation, our base policy generalizes to novel, complex shapes, outperforming strong baselines by 37%-81%.In real world, our sim-to-real method significantly and consistently improves rotation performance, enabling versatile rotation across diverse wrist orientations and rotation axes on a broad object distribution-including complex geometries (e.g., animal models), aspect ratios up to 5.33, and object-to-hand ratios of 0.31-1.68 (Fig. 1; videos on our website).Notably, in a challenging downward-facing hand configuration, we are, to our knowledge, the first to rotate long objects (10-16 cm) around their long axis for about one full circle in the air.Compared to Visual Dexterity (Chen et al., 2022) on a large, customized D'Claw, our smaller LEAP hand matches or surpasses performance and succeeds on shapes it struggles with (e.g., elephant, bunny, teapot).We also generalize to a much broader, more challenging object distribution than the prior multi-wrist SOTA (Yang et al., 2024).Moreover, we showcase an application enabled by our general rotation policy: building a teleoperation system to perform complex dexterous tasks, such as tool-using (e.g., screwdriver, knife) and assembly (Heo et al., 2023).A systematic ablation study validates the crucial role of our key design choices in both the dynamics model and the data collection strategy.Our main contributions are four-fold:
𝒔 ! 𝒂 ! Residual Policy 𝒂 ! $%&amp; 𝒔 !"# Offline Dataset
• A novel sim-to-real framework for dexterous in-hand rotation, built on a joint-wise neural dynamics model and autonomous data collection to tackle the core challenges of learning complex interaction dynamics and acquiring real-world interaction data.• An in-hand object rotation policy that achieves unprecedented generality in rotating challenging objects (high-aspect-ratio, complex shapes, small sizes) under difficult wrist orientations.• An in-depth analysis of the rationale, advantages, and scope of effectiveness of the joint-wise neural dynamics model from both theoretical and empirical perspectives.• A demonstration of a practical application in teleoperation for complex dexterous tasks.</p>
<p>RELATED WORK</p>
<p>Our work is broadly related to two research topics: in-hand object rotation and sim-to-real strategies.In-hand rotation is an important yet challenging robitc task.Despite advances, prior methods still (i) assume an up-facing hand (Qi et al., 2022;Wang et al., 2024;Yuan et al., 2023;Zhao et al., 2025), (ii) handle only normal-sized objects with limited geometric diversity (Qi et al., 2023;Röstel et al., 2025;Pitz et al., 2024a;b;Yang et al., 2024), or (iii) rely on expensive hardware and sophisticated tactile sensing (Yang et al., 2024;Wang et al., 2024;Qi et al., 2023).AnyRotate (Yang et al., 2024) achieves axis and wrist generality, but only on normal-sized regular objects in the real world.Visual Dexterity (Chen et al., 2022) rotates complex shapes in the air, yet performance on small or highaspect-ratio objects is unverified.We aim to achieve generality in rotating challenging (e.g., long, small) and complex objects across diverse wrist orientations and rotation axes.A central obstacle to realizing this is the sim-to-real gap: mismatched parameters, model discrepancies, and unmodeled effects derail transfer of simulation-trained policies.Existing approaches include: (1) Domain Randomization (DR), which broadens training distributions (Loquercio et al., 2019;Peng et al., 2017;Tan et al., 2018;Yu et al., 2019;Mozifian et al., 2019;Siekmann et al., 2020); (2) System Identification (SysID), which fits simulator parameters from real data (An et al., 1985;Mayeda et al., 1988;Lee et al., 2023;Sobanbabu et al., 2025); (3) online adaptive policies (Kumar et al., 2021;Qi et al., 2022); and (4) neural modeling of real dynamics to guide transfer (He et al., 2025;Fey et al., 2025;Hwangbo et al., 2019).DR relies on heuristic ranges; SysID is bounded by its parameterization; and online adaptation typically depends on dynamics coverage in training.Learning real dynamics offers the highest ceiling: A classical line in neural control learns residual or full models for the whole system for model-based control (Fig. 2 (A), e.g., Neural Lander (Shi et al., 2018), MB-Max (bin Shi et al., 2024)).As the task complexity increases, learning globally accurate, physically plausible dynamics that is super robust to support policy tuning or controller development is difficult (Shi, 2025).Therefore, another trend of methods proposed in sim-to-real RL (e.g., UAN (Fey et al., 2025) and ASAP (He et al., 2025)) learn sim-real delta actions and fine-tune policies based on that to bridge the dynamics gap (Fig. 2 (B)).Success hinges on collecting enough real-world data that is distributionally relevant to the task or can offer a comprehensive coverage-a minor issue in locomotion and static-contact tasks, but a major bottleneck in dexterous manipulation.We address this with a generalizable joint-wise neural dynamics model that relaxes the training data distribution requirement, followed by a residual policy to bridge the reality gap (Fig. 2 (C)).Our goal is a generalist policy that can rotate a wide variety of objects under various conditions in the real world.We adopt a model-free RL approach.Key challenges are the pronounced sim-to-real dynamics gap in contact-rich dexterous manipulation and the need for broad object generalization.</p>
<p>We address these with two designs: (1) a specialist-to-generalist approach that first trains categoryspecific oracle policies across curated object categories (Sec.3.1), then distills them into a generalist (Sec.3.2); and ( 2) a neural sim-to-real strategy centered on an expressive, data-efficient, generalizable joint-wise dynamics model, with autonomous data collection and a residual policy that adapts the base policy to close the sim-to-real gap (Sec.3.3).Workflow illustrated in Figure 3.</p>
<p>MULTI-WRIST-ORIENTATION IN-HAND OBJECT ROTATION ACROSS MULTI-AXIS</p>
<p>We formulate in-hand rotation as a finite-horizon Partially Observable Markov Decision Process (POMDP), M = (S, A, O, P, R), with state, action, and observation spaces (S, A, O), transition dynamics P, and reward R. We train a neural policy π : O → A with RL to maximize expected cumulative return over horizon N :
π * = arg max π E τ ∼pπ(τ ) [ N t=1 r(s t , a t )]
. Observations and Actions.At timestep t, the policy receives o t : a short history of proprioception, fingertip and object states, per-joint/per-finger force measurements, binary contact signals, wrist orientation, and the target rotation axis (Sec.A.1).The policy outputs a distribution over relative target position.We sample ∆a t ∼ π(o t ) and update the joint target a t = a t−1 + α ∆a t with α = 1/24.a t is converted to torques via a PD controller and executed on the robot.Reward Function.The reward consists of three weighted components r = α rot r rot + α goal r goal + α penalty r penalty , with r rot and r penalty following RotateIt (Qi et al., 2023).The rotation term r rot encourages rotation about the target axis.The penalty r penalty discourages off-axis angular velocity, deviation from a canonical hand pose, object linear velocity, and joint work/torque.Since these rewards alone struggle on hard cases (e.g., rotating long objects), we add an intermediate goal-pose reward, r goal , that guides the object to a waypoint on the target rotation axis.Details in Sec.A.1</p>
<p>GENERALIST POLICY TRAINING VIA BEHAVIOUR CLONING</p>
<p>Having obtained the oracle policy with rich privileged observations for each object category, we use Behavior Cloning (BC) to train the unified, real-world deployable, multi-geometry generalist policy.Although DAgger-style distillation has been effective in prior work, in our setting even single-policy distillation either fails to optimize in simulation or collapses in the real world, echoing PenSpin (Wang et al., 2024).We attribute this to high task difficulty.We therefore use BC: roll out all oracle policies, aggregate only successful trajectories, and train a generalist via supervised learning.This approach works well on hardware.We hypothesize that its success stems from imitating only high-quality oracle behavior.The observation o gene t of the generalist policy contains a history of proprioception {(q k , a k−1 )} t k=t−T +1 , wrist orientation and rotation axis.We use T = 10 and implement the policy as a residual MLP (He et al., 2015).</p>
<p>CLOSING THE REALITY GAP VIA JOINT-WISE NEURAL DYNAMICS</p>
<p>While the generalist policy is already real-world deployable, a persistent sim-to-real gap-caused by mismatched physical dynamics and unmodeled effects-prevents it from mastering challenging object interactions.We bridge this gap with a novel neural sim-to-real strategy that effectively learns complex, real-world dynamics model.</p>
<p>The central challenge is to acquire useful and sufficient volume of real data so that the learned dynamics model can help sim-to-real transfer.For dexterous manipulation, prior data acquisition methods (Hwangbo et al., 2019;He et al., 2025;Fey et al., 2025;bin Shi et al., 2024) are often impractical.Rolling out a base policy (He et al., 2025;bin Shi et al., 2024) or executing wave actions (Fey et al., 2025) frequently fails on diverse and complex objects, requiring constant human intervention, while imperfect state estimators introduce heavy noise.This leads to real datasets that are small, biased, and insufficient in coverage and quality.We address these challenges by rethinking both model and data.We propose a joint-wise neural dynamics model that dramatically improves sample efficiency and generalizability while preserving expressivity by learning from a low-dimensional, information-contractive, task-sufficient representation of the system dynamics.This allows for an autonomous data collection strategy that gathers diverse, large-scale real-world data by applying randomized loads, eliminating the need for task-specific rollouts and human resets.</p>
<p>Joint-Wise Neural Dynamics.To model the system's dynamics without relying on noisy and limited object-state estimation, one way is to learn a "whole-hand" neural model.This model predicts the hand's next state from its length-W state-action history, q t+1 = f θ (H t ) with H t = {q j , a j } t j=t−W +1 , thereby implicitly capturing the whole system dynamics, including external forces from the object (Qi et al., 2022).However, this approach remains data-hungry, inheriting the other data acquisition challenges described above.</p>
<p>Our solution is to factorize the problem.We introduce joint-wise neural dynamics where the dynamics of each joint i are modeled as
H eff t qi t + G eff t = τ i t
, where H eff t , G eff t ∈ R are low-dimensional effective terms that distill high-dimensional, system-wide influences such as inter-joint coupling, actuation, and object-induced effects.The neural model then predicts the next state of each joint i from its own W -step state-action history:
q i t+1 = f ψi (h i t ) with h i t = {q i j , a i j } t j=t−W +1 .
This factorization is effective as it acts as an information bottleneck, forcing the model to discard spurious correlations and learn only the essential dynamics of each joint.This projected history is sufficiently informative with enough information to accurately predict the joint's next state (Sec. 4.2,A.3).At the same time, it is also robustly simple as it is too low-dimensional to permit the reconstruction of the original high-dimensional system-wide influences, thus avoiding the need to model irrelevant complexity (Sec.A.4).The direct consequence is a model that is highly sample-efficient and generalizes broadly across interactions, yet retains expressivity (Sec.4.2).We now provide a theoretical analysis to formalize why this simplification leads to better generalization.</p>
<p>Theoretical Rationale: Generalization via Information Contraction.We write the whole-hand model as f θ = {f i θ } with q i t+1 = f i θ (H t ), and the joint-wise model as q i t+1 = f i ψi (h i t ).Let P be the target distribution for (H t , q i t+1 ) (e.g., formed by task of our interest); consider a different distribution Q and the projection g : (H t , q i t+1 ) → (h i t , q i t+1 ), i.e., g : R 2W d × R → R 2W × R. We compare the prediction error of joint i on the target distribution P achieved by these two types of model, i.e., f i θ and f i ψi , to support the generalization benefit: Claim 3.1 Under assumptions typical of our setting, ∀1 ≤ i ≤ d, the joint-wise model f i ψi trained on g(Q) generalizes to g(P) better than the whole-hand model f i θ trained on Q generalizes to P. We first show that, under mild assumptions typically satisfied in our setting, the projection g contracts distribution shift: KL(g(P)∥g(Q)) &lt; KL(P∥Q) (Theorem 3.1, proof deferred to Sec.A.2). Theorem 3.1 (Data Processing Inequality for KL (strict form)) Let P and Q be probability distributions on R n × R with densities P and Q with respect to a common base measure.Let g :
X ∈ R n × R → Y ∈ R m × R be measurable, m ≤ n,
and denote the pushforwards by g(P) and g(Q).Then KL(P ∥ Q) ≥ KL g(P) ∥ g(Q) .Moreover, the inequality is strict if g is non-injective in a way that merges points where P and Q have a different relative structure.More concretely, it indicates that if there ∃y
0 ∈ R m , P (Y = y 0 ) &gt; 0, P (X|Y = y 0 ) ̸ = Q(X|Y = y 0 ), then KL(P ∥ Q) &gt; KL g(P) ∥ g(Q) .
The contraction of divergence implies tighter generalization guarantees (Theorem 3.2, proof in A.2):
Theorem 3.2 (Generalization Gap Contraction) Let (X, Y ) ∈ R n × R and g(X, Y ) = (g X (X), Y ) with g X : R n → R m , m &lt; n. Let P, Q be distributions on (X, Y ) satisfy- ing covariate shift, i.e., P(Y | X) = Q(Y | X).
Let L be a loss bounded by B, and define
R P (h) = E (X,Y )∼P [L(h(X), Y )]. If KL g(P)∥g(Q) &lt; KL(P∥Q), then for function f 1 : X → Y and f 2 : g X (X) → Y : sup|R P (f 2 • g X ) − R Q (f 2 • g X )| &lt; sup|R P (f 1 ) − R Q (f 1 )|.
Assuming f 2 • g X is sufficiently expressive and a relatively large domain shift from Q to P (typical of our setting), f 2 •g X has lower prediction error than f 1 on target domain P, establishing Claim 3.1.See Sec.A.2 for details.In practice, we pretrain the model on simulation data for initialization.Autonomous Data Collection.Our model's ability to generalize from distributionally different data motivates our second innovation: a low-cost, autonomous data collection strategy.This approach, which we call the "Chaos Box" (Fig. 3(C)), embodies four principles: (i) policy-awareness (to roughly align the distribution), (ii) object-loaded interaction, (iii) broad coverage, and (iv) scalability.The implementation is simple: the robotic hand is placed in a container of soft balls.We then open-loop replay actions from the simulated base policy, which provides a coarse distributional prior (i).The hand's interaction with the balls imposes rich, randomized loads (ii-iii).With probability 0.5, we add Gaussian noise (σ=0.01) to each action to broaden coverage (iii).This entire process is fully autonomous, hardware-safe, and requires no human resets (iv).Fig. 4 supports our model and data designs: I/O histories of a joint cover the task-relevant distribution, whereas histories of the whole hand do not.</p>
<p>Bridging the Dynamics Gap via a Residual Policy.Using the learned dynamics f ψ , we train a residual policy π res that compensates the base policy's actions to bridge the dynamics gap (Fig. 3(E)).Concretely, given the base policy's observation o gene t and base action a t , π res outputs a correction a res t , and to match the simulator's next state q t+1 , we solve π res * = arg min π res E τ ∼p π * (τ )
N −1 t=1 q t+1 − f ψ {q j , a j + π res (o gene j , a j )} t j=t−W +1
. We solve it by training π res in a supervised manner on the trajectory dataset used to train the base policy.At deployment, we execute a t +a res t .See Sec.B.4 for a discussion on residual policy vs. direct finetuning.</p>
<p>EXPERIMENTS</p>
<p>We extensively evaluate our method in simulation and real world against strong baselines (Sec.4.1).</p>
<p>In simulation, our generalist policy generalizes to unseen geometries for multi-wrist poses, multiaxis rotation.On hardware, it achieves unprecedented in-air rotation with a LEAP hand (Shaw et al., 2023) under challenging wrist poses on difficult objects, including long (13.5-20cm), small (2-3cm) objects, and complex animal shapes (Sec.4.2).We also show a teleoperation setup that pairs the policy with VR to perform complex dexterous tasks (Sec.4.2), such as tool-using and assembly.</p>
<p>EXPERIMENTAL SETTINGS</p>
<p>Training and Evaluation Protocols.We create an object dataset spanning aspect ratios, sizes, and complexity with randomized physical properties for training.We split objects into five categories and train an oracle policy for each with PPO (Schulman et al., 2017) in Isaac Gym (Makoviychuk et al., 2021).We use objects from ContactDB (Brahmbhatt et al., 2019) as the test set in simulation to evaluate the generalization ability to shape variations.We evaluate rotation across randomized wrist orientations and four rotation-axis groups: ±x, ±y, ±z, and a general axis set with 26 axes.We evaluate on three object sets in the real world (Fig. 5): (1) regular objects (including a high-aspectratio cuboid); (2) small objects; and (3) normal-sized irregular objects.Objects shown in purple and all small objects are unseen.We evaluate on three principle axis sets and a cubic-diagonal set:</p>
<p>(1,1,1), (1,0,1), (1,1,0), (0,1,1).Results are averaged over objects and reported as mean ± standard deviation across three independent evaluations.Details in Sec. C.  Baselines.</p>
<p>We compare against in-hand rotation/reorientation baselines-AnyRotate (Yang et al., 2024) and Visual Dexterity (VD) (Chen et al., 2022)-and sim-to-real methods UAN (Fey et al., 2025) and ASAP (He et al., 2025).AnyRotate's code is unavailable and relies on specialized tactile sensing, so we use our re-implementation in simulation; on hardware, we evaluate on their replicable objects and compare to their reported performance.A direct comparison to VD is impractical: adapting their D'Claw code to LEAP failed to behave well in simulation, so we compare to their qualitatively results (link).UAN and ASAP, designed for arms/legged robots and not modeling objects, are adapted by training compensators on object-free transitions; making them object-aware is nontrivial (see Sec. D).</p>
<p>Metrics.We evaluate using RotateIt metrics (Qi et al., 2023), plus a goal-oriented success: Timeto-Fall (TTF)-duration until termination; in simulation, episodes are capped at 400 steps (20s) and TTF is normalized by 20s, while in the real world we report raw time; Rotation Reward (RotR)-episode sum of ω • k (simulation only); Rotation Penalty (RotP)-per-step average ω × k (simulation only); Radians Rotated (Rot)-total radians rotated in the real world; Goal-Oriented Success (GO Succ.) following Visual Dexterity: sample a goal pose; set the target axis to the relative rotation axis; count success if the orientation is within 0.1π of the goal (simulation only).</p>
<p>IN-HAND ROTATION RESULTS AND ANALYSIS</p>
<p>Simulation Results.Our policy generalizes to unseen objects and outperforms our re-implemented baseline (Table 1).Among all settings, rotating along the gravity direction (±z axis) is the easiest task, similar to the observations made in prior works (Qi et al., 2023;Yang et al., 2024).Real World Results.Our sim-to-real method consistently improves real-world performance, and the policy exhibits unprecedented dexterity, rotating high-aspect-ratio geometries, small objects, and complex shapes under challenging hand wrist orientations in the air (Tables 4 (multi-axis with palmdown), 5 (multi-wrist-pose, z-rot); Fig. 1; Fig. 20, object gallery (Fig. 19) (in Appendix); videos).Contrary to AnyRotate, which finds "Thumb Up/Down" most difficult, we observe "Base Up/Down" are harder, likely due to different actuator performance between Allegro and LEAP.Comparisons to AnyRotate.We evaluate on four replicable items from AnyRotate's suit-"Tin Cylinder", Cube, "Gum Box", and "Container" (Sec.C)-which are their most difficult cases (according to Table 12-13), and compare with their reported real-world results.Table 2 shows our method substantially outperforms AnyRotate and is more versatile: whereas AnyRotate targets moderately sized, simple shapes (min 5cm, max aspect ratio 1.67) with conservative motions, our policy handles smaller objects (3cm) and high aspect ratios (up to 5.3) with sophisticated finger gaiting.Comparisons to Visual Dexterity.A direct comparison with Visual Dexterity (VD) is infeasible due to differing task definitions (axis-oriented continuous rotation vs. goal-oriented reorientation).To enable comparison, we introduce the survival rotation angle metric: the angle an object is rotated before being dropped.We estimate VD's best performance by analyzing their videos.Despite this metric favoring VD (their setup sometimes includes a supporting table), we achieve comparable or superior results on their showcased and replicable objects (Table 3).Besides, we can uniquely manipulate small objects and high aspect ratios as well as handle diverse wrist orientations (Fig. 20).Comparisons to Whole-Hand Nueral Dynamics.We compare against the whole-hand dynamics model to answer: (Q1) Does predicting each joint's transition from its own history (without global information) reduce expressivity?(Q2) Is our model more sample-efficient?(Q3) Does it generalize ) and time-to-fall (TTF (s)) along each axis under the palm down wrist orientation.The metric was first averaged over all objects within each trial.We then report avg.± std of these results across three independent trials.better?(A1) Trained on 3.1M simulated trajectories and evaluated in-domain, our model is nearly as expressive as the whole-hand model (Fig. 6(A, column 1)(A-0)).(A2) With limited data-using 7.5k autonomously collected trajectories in the real world (Fig. 6(A, column 3)) and across varying realworld dataset sizes (Fig. 6(B))-our model achieves better in-domain performance, indicating higher sample efficiency.The advantage is more obvious under insufficient data settings.(A3) On an OOD real-world test set (task-relevant transitions under "Thumb Up" wrist), our model generalizes much better in both high-and low-data regimes; see Fig. 6(A, column 2,4) and Fig. 6(B).Fig. 6(C) systematically studies the cross-domain transferability in various settings.Summary: For data-driven neural dynamics, joint-wise model significantly outperform whole-hand models in insufficient-data or train-test distribution-shift settings; with ample data and in-domain evaluation, performance is similar, with only a slight loss in expressivity for joint-wise models.</p>
<p>Comparisons to ASAP and UAN.We implement UAN and ASAP, but their resulting policies fail entirely in real-world tests-unable to rotate even a simple cylinder (Fig. 27; videos).We attribute this to an OOD issue: compensators trained solely on free-hand data do not generalize to the interaction dynamics introduced by manipulated objects.Please note that their methods can only use either freehand data or task-relevant data with object states-difficult and noisy to obtain, and unusable even for compensator training-and cannot leverage our autonomously collected data with randomized object loads; see Sec.D. Our strategy is more tolerant of real-data imperfections (Figs. 8,9,27).Table 6: "Sim-to-Sim" Transfer.</p>
<p>"Sim-to-Sim" Comparisons.We conduct a crosssimulator transfer evaluation (Isaac Gym to Genesis and MuJoCo).We collect object-loaded rotation data in the target simulator for training.Table 4.2 shows our method consistently surpasses prior work, owing to designs on dynamics modeling, higher data efficiency, and practical choices (e.g., pre-train in source sim).We find UAN outperforms ASAP, likely because its history-based design better captures object effects.Details in Sec. C.
i ii iii iv v vi vii i ii iii iv i ii iii iv i ii iii iv i ii iii iv
(A) Tool-Using (hammer, brush, pen, syringe, nut) (B) Furniture Assembly (four-leg table, lightbulb)
i ii iii iv v i ii iii iv v
Figure 7: Application.Our rotation policy enables a teleoperation system to perform complex, long-horizon manipulation tasks.See videos and more results on our project website.</p>
<p>Applications.We showcase an application of our rotation policy: a teleoperation system for dexterous tasks (built with a Meta Quest 3, details in Sec.C).We demonstrate its strong ability in performing long-horizon and complex dexterous manipulation tasks (Fig. 7, videos).We conduct ablations to validate key design choices of our method.Real-world experiments are performed with the hand fixed palm-down, evaluating z-axis rotation; data are collected under the same wrist pose.Dynamics model are evaluated in an OOD test setting.See Sec.C for details.</p>
<p>ABLATION STUDIES
Test
Designs on the Joint-Wise Neural Dynamics Model.We ablate five design choices: (i) jointwise vs. finger-wise (per finger prediction from its own history) and whole-hand modeling; (ii) simulation pretraining; (iii) injecting noise into replayed actions during real-world data collection;</p>
<p>(iv) collecting with object loads rather than free-hand w/o load; and (v) replaying policy rollouts instead of base waves (Fey et al., 2025).As summarized in Fig. 8, these choices consistently improve learned dynamics generalization and real-world performance.</p>
<p>Real-World Data Collection Strategies.We compare our autonomous data collection against three baselines-task-aware with vision-based object states, task-aware without object states, and freehand motions-evaluating limitations, efficiency, and model performance (Figure 9).Task-aware pipelines are slow and intervention-heavy: estimating object poses is prohibitively slow (∼200s on average), requires continuous human supervision, yields noisy poses and complex setup, and fails on small, occluded, or axis-symmetric objects; without vision they still need intervention, remain slow (42.86 s), and produce low-diversity, low-coverage data (data restricted to policy's ability).In contrast, our method is fully automated and, by continuously varying hand loads, collects diverse data spanning a wide range of external influences.Figure 9(B) shows the resulting performance gains: broader coverage improves prediction, and the joint-wise model is most robust to trainingdistribution shifts, whereas other variants tend to overfit to the source data.</p>
<p>Scaling with Real-World Data Quantity and Collection Iterations.As shown in Fig. 9, our performance improves with more real-world data.However, iterative data collection-intended to align real-world and simulated transition distributions for better policy updates-yields only modest gains.We hypothesize this is because the dynamics model already generalizes well, and adding noise to replay actions provides broad coverage, reducing sensitivity to this distribution shift.In contrast, the whole-hand model benefits little from additional data, especially under autonomous collection, likely due to its higher dimensionality and a distributional mismatch between autonomous data and rotation task transitions.A simple extrapolation suggests matching our 4,000-trajectory result would require 7.5M task-aware trajectories (417k hours; 52k 8-hour workdays), which is impractical.While approximate, this highlights the superiority of our approach.</p>
<p>CONCLUSIONS AND LIMITATIONS</p>
<p>We propose a neural sim-to-real framework centered on a joint-wise neural dynamics model and autonomous data collection.This enables unprecedented dexterity in rotating challenging objects.We include a video and a website to introduce our work.The website and the video contain robot videos.We highly recommend exploring these resources for an intuitive understanding of the challenges, the effectiveness of our method, and its superiority over prior approaches.</p>
<p>A ADDITIONAL EXPLANATIONS OF THE METHOD</p>
<p>A.1 POLICY DESIGN</p>
<p>Observations.The observation of the oracle policy contains: 3-length joint position history (48dim), 3-length joint positional target history (48-dim), joint velocity (16-dim), fingertip state and velocity (52-dim), object state and velocity (13-dim), object guiding goal pose (4-dim), joint and rigid body forces (40-dim), contact force and binary contact (92-dim), wrist orientation (quaterion, 4-dim), and rotation axis (3-dim).</p>
<p>Rewards.The reward function consists of three parts r = α rot r rot + α goal r goal + α penalty r penalty , with r rot and r penalty following RotateIt (Qi et al., 2023).The rotation term
r rot = clip(ω t • k, −c, c) encourages rotation about the unit target axis k ∈ R 3 , ∥k∥ 2 = 1
, where ω t is the object angular velocity and c = 0.5 caps excessive speed.The penalty r penalty discourages off-axis angular velocity, deviation from a canonical hand pose, object linear velocity, and joint work/torque:
r penalty = −α rotp ∥ω t × k∥ 1 − α lin ∥v t ∥ 2 2 − α pose ∥q t − q init ∥ 2 2 − α work τ T q − α torque ∥τ ∥ 2 2
, where v t , q init , and τ denote the object pose, initial hand joint position, and joint commanded torques at the current timestep t, α lin = 0.3, α pose = 0.3, α torque = 0.1, α work = 2.0.We schedule the coefficient α rotp linearly: set it to zero at the beginning of the training; use the number of resets to count the training process; at the 10 resets, we keep α rotp to zero; from 10 to 100, linearly increase it to 0.1; after 100, keep it at 0.1.α penalty = 1.0</p>
<p>We find that solely relying on these rewards cannot solve challenging problems like rotating a long object.Therefore, we add an intermediate goal: at episode start set p goal 90 • ahead along the desired rotation and update it whenever ang diff(p t , p goal ) &lt; 15 • ; the guidance term is r goal = clip ggoal ang diff(pt,p goal )+ϵ , 0, c goal + g bonus 1 ang diff(pt,p goal )<cthreshold , where ang diff(•, •) is the quaternion angular distance, ϵ > 0 ensures numerical stability, and c threshold is the proximity threshold.We set r goal = 1.0.</p>
<p>Control Strategy.We use torque control with 20Hz, where each control step is realized by running the torque control for 6 times.Each time the joint torque is calculated as τ t = K p (q tar t − q) − K d qt , where the q and q represent the current joint position and joint velocity, K p and K d are preset constant positional gain and damping parameters.</p>
<p>Generalist Policy Architecture.We use a residual MLP with five residual blocks.The input layer is a single linear network with a hidden dimension of 1024.After that, we stack five residual blocks each with the hidden dimension of 1024.Each residual block processes input x via y = ReLU(NN 1 (x) + NN 3 (ReLU(NN 2 (x)))).The output layer is a single linear network that maps the latent to the output dimension.</p>
<p>Further Discussions on Design Choices.The BC-style training allows us to achieve a real-world deployable multi-geometry policy in a simple way by combining datasets resulting from different multiple oracle policies, each trained for a specific object category, to train a unified policy.We use BC to achieve both real-world deployment ability and generality across diverse objects.An alternative is achieving the generality in the teacher level, e.g., training RL for an any-wrist orientation any-axis on all object categories.However, this can hardly work.This may require us to add an automatic or multi-stage curriculum to make sure the final policy can perform at least as good as each individual policy.This is a valuable research direction.In this work, we choose to leave the oracle policy training a neat pipeline, adopt to train a collection of teacher policies, and achieve the unified real-world deployable policy at once in the student policy training stage.</p>
<p>A.2 PROOF OF MAIN THEOREMS</p>
<p>Theorem A.1 (Data Processing Inequality for KL (strict form)) Let P and Q be two probability distributions on R n × R with respective probability density functions (PDFs) P (x) and Q
(x). Let g : R n × R → R m × R be a measurable function, where m ≤ n. This function transforms a random variable X ∼ P (or X ∼ Q) into a new random variable Y = g(X). Let g(P) and g(Q) denote the resulting pushforward distributions on R m × R.
The Kullback-Leibler (KL) divergence between the distributions is reduced or remains the same after the transformation, a property known as the Data Processing Inequality:
KL(P∥Q) ≥ KL(g(P)∥g(Q)).
(1)</p>
<p>The inequality is strict, KL(P∥Q) &gt; KL(g(P)∥g(Q)), if g is non-injective in a way that merges points where P and Q have a different relative structure.More concretely, it indicates that there
∃y 0 ∈ R m × R, P (Y = y 0 ) &gt; 0, P (X|Y = y 0 ) ̸ = Q(X|Y = y 0 ). Proof A.1
We start with prove that KL(P∥Q) ≥ KL(g(P)∥g(Q)) always holds for any function g.Let X be a random variable drawn from one of two distributions, P or Q. Denote their PDFs as P X (x) and Q X (x).</p>
<p>Let Y be a new random variable created by applying a function to X: Y = g(X).The distributions of Y are the pushforward distributions f (P) and f (Q), with PDFs P Y (y) and Q Y (y).Consider the joint distribution of (X, Y ), since Y is a deterministic function of X, the joint probability is simple:
P X,Y (x, y) = P X (x), if y = g(x) (2) P X,Y (x, y) = 0, if y ̸ = g(x)(3)
Using "chain rule" of KL divergence, we can expand the joint distributions in two ways:
(A) KL(P X,Y ∥Q X,Y ) = KL(P X ∥Q X ) + KL(P Y |X ∥Q Y |X ) (4) (B) KL(P X,Y ∥Q X,Y ) = KL(P Y ∥Q Y ) + KL(P X|Y ∥Q X|Y ) (5)
Since Y is completely determined by X (Y = f (X)), we have
P (y|x) = 1, if y = f (x),(6)P (y|x) = 0, if y ̸ = f (x) (7)
And the same property for Q(y|x):
Q(y|x) = 1, if y = f (x), (8) Q(y|x) = 0, if y ̸ = f (x)(9)
Therefore P Y |X = Q Y |X , and the KL divergence between them is zero:
KL(P Y |X ∥Q Y |X ) = E x ∼P X y P (y|x) log P (y|x) Q(y|x) dy = E x ∼P X [0] = 0. (10)
Thus, the expansion 4 simplifies to
KL(P X,Y ∥Q X,Y ) = KL(P X ∥Q X ). (11)
We have: KL(P X ∥Q X ) = KL(P Y ∥Q Y ) + KL(P X|Y ∥Q X|Y ).</p>
<p>(12) Since KL divergence is always non-negative, which implies KL(P X|Y ∥Q X|Y ) ≥ 0 , we have
KL(P X ∥Q X ) ≥ KL(P Y ∥Q Y ). (13)
The inequality is strict if and only if the second term of the RHS in Eq. 12 is strictly positive, i.e., KL(P X|Y ∥Q X|Y ) &gt; 0. This term is the expected KL divergence between the conditional distributions P (x|y) and Q(x|y), averaged over the distribution P Y (y).It will be strictly positive if and only if
∃y 0 ∈ R m × R, P Y (y 0 ) &gt; 0, P (X|Y = y 0 ) ̸ = Q(X|Y = y 0 ).
This is direct.We provide the proof below.</p>
<p>Sufficiency.Since KL(P X|Y ∥Q X|Y ) = E y∼P Y KL(P X|Y =y ∥Q X|Y =y ) , if the condition is satisfied, we have KL(P X|Y ∥Q X|Y ) ≥ P Y (y 0 )KL(P X|Y =y0 ∥Q X|Y =y0 ) &gt; 0. Thus, it is a sufficient condition.</p>
<p>Necessity.We can prove it by disproof.Suppose that we can find a case with KL(P X|Y ∥Q X|Y ) &gt; 0 but for every y 0 with non-zero P Y (y 0 ), we have KL(P X|Y =y0 ∥Q X|Y =y0 ) = 0, then we have KL(P X|Y ∥Q X|Y ) = E y∼P Y KL(P X|Y =y ∥Q X|Y =y ) = 0, which contradicts the assumptions.Thus, it is a necessary condition.</p>
<p>In our setting, as g strictly reduces the dimensionality and is a continuous function (because it extracts the history of a joint from the whole hand history), g is a non-injective function, which we will show later in Theorem A.3.Since P and Q lie in different data domains (a visualization is shown in Figs.16 17), and since as we've demonstrated g(P) and g(Q) share similarities (a visualization is shown in Fig. 15), the condition
∃y 0 ∈ R m × R, P (Y = y 0 ) &gt; 0, P (X|Y = y 0 ) ̸ = Q(X|Y = y 0 ) is then typically satisfied. Theorem A.2 (Generalization Gap Contraction) Given data point (X, Y ) ∈ R n × R, a measur- able function g : (X, Y ) ∈ R n → (g X (X), Y ) ∈ R m , m &lt; n,
and two different distributions P, Q in the manifold R n whose pushforward distribution by g satisfy KL(g(P∥g(Q)) &lt; KL(P∥Q).Under the covariant shift condition, i.e., P(
Y |X) = Q(Y |X), for any function f 1 : X ∈ R n → Y ∈ R and f 2 : g X (X) ∈ R m → Y ∈ R, we have sup|R P (f 2 • g X ) − R Q (f 2 • g X )| &lt; sup|R P (f 1 ) − R Q (f 1 )|,(14)
where
R P (h) = E (X,Y )∼P [L(h(X), Y )]
is the risk for the predictor h, L measures prediction error and is bounded by B.</p>
<p>Proof A.2 Using the law of total expectation and the covariate shift assumption:
R P (h) = E X∼P X E Y ∼P (Y |X) [L(h(X), Y )] R Q (h) = E X∼Q X E Y ∼Q(Y |X) [L(h(X), Y )] = E X∼Q X E Y ∼P (Y |X) [L(h(X), Y )]
Define the "inner risk" function for a fixed x:
r h (x) := E Y ∼P (Y |X=x) [L(h(x), Y )]
The risk difference could be converted to an expectation over the marginals P X and Q X :
R P (h) − R Q (h) = E X∼P X [r h (X)] − E X∼Q X [r h (X)] = r h (x)(p X (x) − q X (x))dx
An IPM between two distributions P X and Q X over a function class F is defined as:
d F (P X , Q X ) = sup ϕ∈F |E X∼P X [ϕ(X)] − E X∼Q X [ϕ(X)]|
Define two classes of "inner risk" functions:
F 1 = {r f1 | f 1 : R n → R is in the function space for f 1 } F 2 = {r f2•g X | f 2 : R m → R is in the function space for f 2 }
The inequality we want to prove becomes:
d F2 (P X , Q X ) &lt; d F1 (P X , Q X ) Consider any function ϕ ∈ F 2 . By definition, ϕ = r f2•g X for some function f 2 . Define a new function f 1 (x) = (f 2 • g X )(x).
Assuming the F 1 is rich enough to contain this composition, we have
r f1 = r f2•g X = ϕ. This means ϕ ∈ F 1 . Therefore, F 2 ⊆ F 1 .
We immediately have the non-strict inequality, since we are taking the supremum over a smaller set:
sup ϕ∈F2 |E P X [ϕ] − E Q X [ϕ]| ≤ sup ϕ∈F1 |E P X [ϕ] − E Q X [ϕ]|
Consider the given KL condition KL(g(P X )∥g(Q X )) ≤ KL(P X ∥Q X ) and the covariant shift condition, we have: KL(g X (P X )∥g X (Q X )) &lt; KL(P X ∥Q X ).This implies that g X (X) is not a sufficient statistic for distinguishing P X from Q X .This means the likelihood ratio w(x) = p X (x)/q X (x) cannot be written as a function of g X (x).This further implies there exist
x a , x b such that g X (x a ) = g X (x b ) but w(x a ) ̸ = w(x b ).
Now, consider the function classes:
• Any function ϕ ∈ F 2 must be constant on the level sets of g X . If g X (x a ) = g X (x b ), then ϕ(x a ) = ϕ(x b
).These functions are blind to the information that g X discards.</p>
<p>• The function ϕ * ∈ F 1 that maximizes the IPM difference, d F1 (P X , Q X ), must be maximally sensitive to the difference between P X and Q X .Since this difference (captured by the likelihood ratio w(x)) depends on information discarded by g X , the optimal discriminating function ϕ * cannot be a function of g X (x) alone.</p>
<p>This means that the function ϕ * that achieves the supremum for the larger set F 1 is not contained in the smaller set F 2 (i.e., ϕ * / ∈ F 2 ).</p>
<p>Because the supremum for F 1 is achieved by a function that is not available in the strictly smaller set F 2 , the inequality is strict.
sup ϕ∈F2 |E P X [ϕ] − E Q X [ϕ]| &lt; sup ϕ∈F1 |E P X [ϕ] − E Q X [ϕ]|
This completes the proof.</p>
<p>Define the optimal predictors trained on the source distribution Q as:
f Q 1 = arg min f1 R Q (f 1 ) (15) f Q 2 = arg min f2 R Q (f 2 • g X )(16)
We move on to show that under specific conditions, the predictor trained on the simpler representation generalizes better to the target distribution P.</p>
<p>Proposition Let f Q 1 and f Q 2 be the optimal predictors on the source distribution Q in the full and reduced-dimensional spaces, respectively.Let the following assumptions hold: Assumption (Small Approximation Error) The function class {f 2 • g X | f 2 : R m → R} is sufficiently expressive to model the relationship on the source distribution Q.The increase in source risk due to the reduced representation is bounded by a small constant ϵ
A : R Q (f Q 2 • g X ) − R Q (f Q 1 ) = ϵ A .
(17) Assumption (Generalization Gap Reduction) Building on Theorem A.2, we further assume a relatively large distribution shift from P to Q, such that f Q 2 exhibits a strong generalization advantage, and the difference in generalization gap achieved by the
f Q 1 and f Q 2 satisfies: R P (f Q 2 • g X ) − R Q (f Q 2 • g X ) − R P (f Q 1 ) − R Q (f Q 1 ) = −ϵ B ,(18
) where ϵ B is a positive constant.</p>
<p>If ϵ B &gt; ϵ A , then the risk of the predictor trained in the reduced-dimensional space is strictly lower on the target distribution:
R P (f Q 2 • g X ) &lt; R P (f Q 1 ). (19) Proof A.3 Decompose the target risk: R P (h) = R Q (h) + (R P (h) − R Q (h)) . (20)
We further have:
R P (f Q 2 • g X ) − R P (f Q 1 ) = R Q (f Q 2 • g X ) + R P (f Q 2 • g X ) − R Q (f Q 2 • g X ) − R Q (f Q 1 ) + R P (f Q 1 ) − R Q (f Q 1 ) . (21)
Rearranging the terms, we have:
R P (f Q 2 • g X ) − R P (f Q 1 ) = R Q (f Q 2 • g X ) − R Q (f Q 1 )
Term A: Approximation Error
+ R P (f Q 2 • g X ) − R Q (f Q 2 • g X ) − R P (f Q 1 ) − R Q (f Q 1 )
Term B: Difference in Generalization Gaps .</p>
<p>(
)22
From Assumption 1, Term A is equal to
ϵ A : R Q (f Q 2 • g X ) − R Q (f Q 1 ) = ϵ A . (23) From Assumption 2, Term B is equal to −ϵ B : R P (f Q 2 • g X ) − R Q (f Q 2 • g X ) − R P (f Q 1 ) − R Q (f Q 1 ) = −ϵ B .(24)
We have:
R P (f Q 2 • g X ) − R P (f Q 1 ) = ϵ A − ϵ B .(25)
Given the condition ϵ B &gt; ϵ A , we have:
R P (f Q 2 • g X ) &lt; R P (f Q 1 ).
(26) This completes the proof.</p>
<p>When are these assumptions valid?Assumption 1 characterizes the in-domain performance gap between the joint-wise neural dynamics model and the whole-hand model.As shown in Sec.4.2 and Fig. 6, it holds even when data are sufficient.In low-data regimes, the joint-wise model not only avoids increasing source-domain risk but actually reduces it, thanks to better sample efficiency.</p>
<p>Assumption 2 characterizes the generalization behavior of these two models.Under train-test distribution shift, it is satisfied in all our experiments (Sec.4.2; Fig. 6); the joint-wise model exhibits much better transferability than the whole-hand dynamics model.</p>
<p>In our dexterous manipulation setting, data scarcity and train-test shift are pervasive, because obtaining perfectly distributionally aligned data is often infeasible or difficult to scale (Sec.3.3), with empirical evidence in Secs. 5 and B.4.Even with autonomous data collection, the volume of realworld data is far smaller than in simulation, keeping us in the low-data regime.Consequently, joint-wise modeling is the preferable choice for our task and a key to our success.By contrast, using a whole-hand dynamics model degrades sim-to-real transfer (Tables 4 and 5).We attribute the success of the whole-body dynamics model employed in bin Shi et al. (2024) to its in-distribution setting and to dynamics that are less complex than in our scenario.
Theorem A.3 ∀ C 1 function f : R n → R m , m &lt; n that projects n-dim data point in R n to that in a lower dimensional space R m , then f is a non-injective function.
Proof A.4 For any point x ∈ R n , its derivative is the Jacobian matrix Df x , which represents a linear map from the tangent space at x (i.e., R n ) to the tangent space at f (x) (i.e., R m ).Df x is an m × n matrix.The rank of this matrix is at most min(m, n) = m.Applying the Rank-Nullity Theorem to this linear map Df x : R n → R m , we find that its null space has dimension ≥ n−m &gt; 0.</p>
<p>According the Inverse Function Theorem (Munkres, 2018;Guillemin &amp; Pollack, 2010), which states that a function is locally injective around a point x only if its derivative Df x is injective.As we've shown, Df x is never injective when n &gt; m.Since f is not locally injective at any point, it cannot possibly be globally injective.</p>
<p>A.3 RATIONALITY OF JOINT-WISE DYNAMICS MODELING (PART I)</p>
<p>We model the hand with the standard manipulator equation (Murray et al., 2017;Spong et al., 2020), treating the object effect as an external force:
M(q)q + C(q, q) q + G(q) = τ + τ ext ,(27)
where M(q), C(q, q), and G(q) are the inertia, Coriolis, and gravity matrices, respectively.τ is the applied joint torque, and τ ext represents the external force from the object.Given low-speed operation, we neglect the Coriolis term (Craig, 2009;Spong et al., 2005), C(q t , qt ) qt ≈ 0.</p>
<p>Assuming we are modeling the i-th joint, we use (q m , qm ) to represent the state of "modeled joints", e.g., q m = [q i ] T ∈ R 1 , while treating the joints as "slave" joints and denote their state as (q s , qs ), i.e., q s = [q j , ∀1 ≤ j ≤ 16, j ̸ = i] T ∈ R 15 .Rearranging other full dynamic equations (Eq.27), we write it as
M mm t M ms t M sm t M ss t qm t qs t + G m t G s t = τ m,total t τ s,total t . (28)
Derive the equation of the modeled joints:
(M mm − M ms (M ss ) −1 M sm )q m + M ms (M ss ) −1 (τ s,total − G s ) + G m = τ m = [τ i + τ i,ext ] T .
(29) Introducing an "effective" torque as τ eff = [τ i,ext ] T ∈ R 1 , and write the equation as follows:
(M mm − M ms (M ss ) −1 M sm )q m + M ms (M ss ) −1 (τ s,total − G s ) + G m − τ eff = [τ i ] T . (30) Let H eff t denote the effective inertia matrix, H eff t ≜ M mm − M ms (M ss ) −1 M sm , and let G eff t denote the effective external term, G eff t ≜ M ms (M ss ) −1 τ s,total − G s + G m − τ eff . Given H eff t , G eff
t , and the modeled joint torque τ i t , the acceleration qi t is uniquely determined.H eff t and G eff t are related to joint state and torques of other joints.</p>
<p>It indicates that in the highly coupled interaction system, the dynamics of each single joint is related to other joints' states, torque, and the external influence of the objects.Employing a neural-based approach to solve the dynamics evolution with the aim to account for all of those high-DoF influences would inevitably require a large amount of data with correct distribution, cannot resolve the challenges in the data aspect.</p>
<p>Focusing on each single joint dynamics system, joint-wise neural dynamics predicts each single joint transition from its own state-action history.Predicting from history generalizes the idea of the RMA approach in rotation (Qi et al., 2022) to implicitly account for time-varying influences at a high level.We will show that, in a short time window (e.g., 10 frames, corresponding to 0.5s) and under certain assumptions, this approach is reasonable.Specifically, we assume that in any short time window during the action trajectory execution, the state trajectory of each slave joint, i.e., q s , the active torque applied to each slave joint, i.e., τ s , and the effective external torque applied to each joint, τ ext , can be approximated by an infinitely differentiable continuous function to within an acceptable error threshold.Intuitively, this assumption holds true for joint states and active torques (related to input positional targets) in a continuously evolved dynamical system where the actions are the policy network's output.If we further assume a soft contact model (Tedrake &amp; the Drake Development Team, 2019; Pang &amp; Tedrake, 2021), the assumption of the effective external torques, which is caused by contact forces with the object, is thus reasonable.</p>
<p>We give statistical evidence for these two assumptions.Specifically, we demonstrate that they could be fitted to an acceptable error using polynomial functions, a special group of infinitely differentiable continuous functions.11,and 12 show the real-world state-action trajectories collected using a free robot hand without object load, via our autonomous data collection system with load, and the task-aware data collection with human interventions.Both action and state trajectories of the hand under such three types of external influences are visually smooth.</p>
<p>Patterns of</p>
<p>We further analyze their polynomial fitting results.Figure 32 shows the 3-ordered polynomial fitting results of per-joint state sequence over a 10-length time window.Figure 34 shows the per-joint fitting error averaged over all tested 10-length sequences.We can observe good fitting results where the original curve can be roughly approximated by the fitted curve.If we increase the polynomial order to 5, we could observe excellent fitting results (Figure 33 35).These statistical results show the rationality of the continuous function assumption on joint state sequences.</p>
<p>Patterns of Per-Joint Active Torque Trajectory.Since we cannot sense the torque directly, for each joint i, we analyze the difference between the positional target and the joint state at each timestep t, i.e., q i,tar t − q i t , to reflect the corresponding statistics of actuation torques.Figure 36 and 37 illustrate the fitting results using 3-ordered polynomial functions and 5-ordered polynomial functions, respectively.Figure 38 and 39 further show the per-joint average fitting error.The action force's evolution is more complex than joint states.But we could still see satisfactory fitting results.As the polynomial order increases, the fitting results become better.</p>
<p>Patterns of Per-Joint External Torques Trajectory.Since we cannot measure per-joint effective external torques from the real world directly, which is related to the contact force between the object and the hand, we introduce "virtual object force" (also denoted as "virtual force" or "virtual torque") as a proxy of the actual external torque.Specifically, we first train per-joint inverse dynamics models that predicts the applied action from the state-action history and the next actual state, i.e., f invdyn,i :
{(s i k+1 , a i k )} t k=t−W +1 ∈ R 2W → ât+1 ∈ R 2W
, from the free hand replay trajectories.Thus, it predicts what action should be applied so that the next joint state can reach the desired value, without the influence of the object (without the external torques).Then, for a collected task-aware trajectory, we first use the inverse dynamics model to predict the desired action ât+1 .We then calculate the "virtual force" using its difference from the actual action, i.e., a t+1 − ât+1 .Since this discrepancy reflects what amount of additional action is required to resist the object so that the joint can reach the desired state.We then analyze the statistics of this quantity.</p>
<p>As shown in Figure 40, 41, 42, 43, we can still get satisfactory fitting results, although the evolution of this quantity is more complex than both that of the active torque and the joint state.</p>
<p>Based on this, we can assume the evolution of H eff and G eff are good continuous functions over the considered time window.We can then approximate their evolution by a low-order function, e.g., using its Taylor expansions, to an acceptable error.Assuming k 1 order for H eff while k 2 for G eff , the underlying number of unknown variables becomes k 1 + k 2 .Solving for all unknown variables is enough to solve the next step transition.The state-action history of each joint could be viewed as the input and output of the function 30 with k 1 + k 2 unknown parameters, which contain enough information to solve for them if the history is long enough.It then indicates the reasonability of using a neural network to predict the next transition from the state-action history, considering the sufficient information contained in the input and the universal approximation ability of neural networks.</p>
<p>A.4 RATIONALITY OF JOINT-WISE DYNAMICS MODELING (PART II)</p>
<p>In the previous section, we demonstrated that the state-action history of a single joint is sufficient to predict its own next transition.This indicates that the information contained in the single joint state action history is at least sufficient to account for the evolution of low-dimensional effective variables over a short time window, i.e., H eff t and G eff t .However, this is not enough to demonstrate that a model that learns to predict from the history would not implicitly learn to predict the original high-dimensional complex forces like inter-joint coupling to predict the transition.Demonstrating this point is important since if the single joint state-action history contains sufficient information to predict a higher-ordered system's states, learning from the single joint history is thus not an effective dimensionality reduction and would hamper the generalization ability as the model would still overfit to the system's high-variance influences.</p>
<p>We demonstrate via experiments aiming to say that the state action history of a specific joint does not contain sufficient information to predict other joints' information.m</p>
<p>Dynamics Model Generalization</p>
<p>We train the joint-wise dynamics model to predict the following information 1) its next joint's current state, 2) the previous joint's current state, 3) the next joint's action (positional target), and 4) the previous joint's action (positional target).We then compare their prediction and generalization error with that achieved by the joint-wise dynamics model (predicting itself's next state) for analysis.</p>
<p>We train all models from scratch using real-world transition data without pretraining using simulation data.Real-world transition data is the same as that we use in the ablation study.As shown in Figure 14 and 13, utilizing a single joint state-action history to predict statistics of other joints cannot even achieve reasonable performance in the original distribution.The generalization error is three order larger than that achieved by using a single joint state-action history to predict its own next transition.As for the in-distribution validation error (which is achieved on the in-distribution validation set and is close to the training error), predicting neighboring joints' states achieves a slightly better performance than predicting their actions.However, this is still far from a reasonable prediction, with the error two-ordered larger than that achieved in predicting the joint's own transition.</p>
<p>These experiments demonstrate that even predicting the easiest information that results in the complex coupling (i.e., neighboring joints' state and action) via a single joint's state-action history is not feasible.This further indicates that a single joint's state-action history does not contain enough information to account for the complex influence factors in the original high-dimensional space.Since such information is sufficient to predict the joint's own transition, a reasonable assumption is that the network tends to leverage such net effects implicitly from the history for predicting the dynamics evolution.</p>
<p>What does the joint-wise neural dynamics model implicitly capture?Analyses and experiments in Secs.A. 3 and A.4 clarify what is and is not predictable from a single joint's state-action history.Our comprehensive experiments (Sec.4.2) show that joint-wise neural dynamics are expressive, sample-efficient, and generalize well.The analysis in Sec.A.3 indicates that a single joint's history contains sufficient information to approximate its next transition, whereas Sec.A.4 shows it cannot recover each underlying coupling effect.Thus, the per-joint history captures low-dimensional net effects while avoiding overfitting to system-wide variations.This factorized, per-joint modeling transfers across changes in whole-hand interaction because the distribution of net effects is comparatively more stable than that of full-system interactions.</p>
<p>Limitations of joint-wise neural dynamics mode.As shown in Fig. 6, the joint-wise dynamics model performs slightly worse than the whole hand dynamics model in the in-domain test setting under the multi-task high data regime.The optimization speed is also a limitation, as iterating over all joints takes time, resulting in a longer training time.</p>
<p>A.5 COMPARISONS OF DATA DISTRIBUTIONS BETWEEN COLLECTED TRAJECTORIES AND ROTATION TRAJECTORIES</p>
<p>Figure 15, 16, and 17 summarize the per-joint, per-finger, and whole hand data distribution.It compares trajectories collected by our autonomous data collection strategy and task-relevant rotation trajectories.The task relevant trajectories are 20 cube-rotation trajectories (∼8,000 data points in total) collected using under the "Thumb Up" wrist orientation.Per-Joint state-action trajectories can  well cover the distribution of task-aware rotation trajectories.However, per-finger and whole hand distributions exhibit a huge discrepancy.</p>
<p>B ADDITIONAL EXPERIMENTS AND ANALYSIS B.1 TRAINING PERFORMANCE</p>
<p>AnyRotate (Yang et al., 2024) improves over prior works regarding the generality to diverse writing orientations and various rotation axes.However, they only considered regular objects.Achieving such general rotation ability for complex objects poses additional challenges, even in the policy training aspect.In our experiments, we find that prior RL designs for rotation policies (Qi et al., 2022;2023;Yang et al., 2024), where only proprioceptions and object and system parameters-related privileged information, such as masses, are considered in the observation, may let the training get stuck in a local optimum.Thus, we include more privileged information into the observation, followed by observation space distillation for sim-to-real (Sec.3.1).We compare with our re-implemented AayRotate to demonstrate this design's superiority.Our method shows noticeably better training performance over AnyRotate (Fig. 18), especially on challenging object sets, i.e., "DexEnv Objects" with irregular and complex geometries and "Small Cylinders" featured by small sizes, where stable finger gaiting cannot emerge in AnyRotate.We also re-implement RotatIt (Qi et al., 2023) in the Hora (Qi et al., 2022) codebase, but find that it can hardly achieve satisfactory results in the most basic cylinder object set.We also adapt Hora to the down-facing hand scenario but find it cannot work.</p>
<p>Figure 19: Evaluated Objects in the Real World.</p>
<p>B.2 ADDITIONAL REAL WORLD RESULTS</p>
<p>axis = (0.58, 0.58, 0.58) axis = (1, 0, 1)
3cm × 3cm × 3cm 3cm × 3cm × 3cm i ii iii iv v i ii iii iv v i ii iii iv v i ii iii iv v
(A) Challenging Geometries (high aspect ratio, long, small sizes)   achieved by the base policy w/ and w/o DexNDM on challenging shapes (i.e., high aspect ratios, small sizes, and complex geometry).Performance tested on a down-facing hand.Symbols in parentheses indicate the rotation axis.Values are the average over three independent trials.
3cm × 20cm × 3cm 3cm × 14cm × 3cm i ii iii iv v i ii iii iv v i ii iii iv v 3cm × 16cm × 3cm 3cm × 13.5cm × 3cm axis = (0, 0, -1) i ii iii iv v i ii
As shown in Table 4 and 5, our design on learning neural dynamics and residual policy for simto-real can achieve notably superior results than the policy without sim-to-real design.Below, we introduce several empirical observations and case studies on our sim-to-real method.Notably, the residual policy can effectively improve the performance on challenging shapes, helping us solve previously unsolvable rotation tasks, and also enhancing the stability of the rotation (Table 7).</p>
<p>Rotating Challenging Objects.One of the important features of the residual policy is enabling us to rotate challenging objects with high aspect ratios or difficult object-to-hand ratios.For instance, without the sim-to-real strategy, the policy can only rotate the long "Lego" leg (width=3cm, lenght=13.5cm)for at most 180 degrees.However, introducing the residual policy can help us rotate it for (almost) a complete circle (demonstrated in Figure 20 and videos in our website).Same observations for the "book" object, which is 16cm long.</p>
<p>Improving the Stability.Apart from rotating, equipping us with the ability to rotate challenging objects, the residual policy can effectively make the rotation more stable and thus help us achieve long-term rotation.A representative example is rotating the 3cm×3cm×10cm cuboid in this vertical pose.When dealing with such thin objects, the policy would use three fingers -the thumb, middle, and pinky fingers -to rotate the object.Compared to using four fingers, this rotation gait is unstable.If we do not include the residual policy, we can rotate the object for at most 5 circles.However, including the residual policy can let us rotate the object continuously for more than 5 minutes, which corresponds to about 30 circles.Similar observations for rotating the "cube" object along the y-axis.</p>
<p>B.4 FURTHER DISCUSSIONS, ANALYSIS, AND ABLATION STUDIES</p>
<p>Residual Policy v.s.Direct Finetuning.A natural alternative for adapting the base policy is direct fine-tuning.We evaluated this by fine-tuning the base policy on the learned dynamics model.In practice, the method proved unstable and highly sensitive to hyperparameters: using the same training strategy as in residual-policy training and no additional stabilization, the fine-tuned policy exhibited erratic behavior and failed to execute even basic rotations.</p>
<p>We did not investigate this issue further; instead, we adopted the residual policy for compensation approach, which is straightforward to implement, stable to train, and requires minimal specialized training techniques.</p>
<p>Evaluated Objects in the Real World.Our policy demonstrates effectiveness in rotating a wide variety of objects in the real world.Photo of real-world object gallery: Figure 19.Delta Action Magnitude 0.0075 0.0104 0.0074 0.0043 0.0116 0.0093 0.0089 0.0061 0.0113 0.0066 0.0054 0.0059 0.0085 0.0113 0.0052 0.0047</p>
<p>Table 8: Per-Joint Delta Action Magnitude.Running average of per-joint delta action scale when rotating a cylinder (radius = 5.5cm, length = 5.5cm) along the z axis in the real world.Joints are arranged according to the joint order in Isaac Gym.</p>
<p>Per-Joint Delta Action Value.Table 8 summarizes the per-joint delta-action magnitudes observed when rotating a cylinder (radius 5.5 cm, length 5.5 cm) about the z-axis in real-world experiments.These values quantify the amount of compensation applied to each joint.</p>
<p>Inherent Limitations of Task-Relevant Data Collection.Collecting task-relevant transitions with estimating object poses suffer from the following inherent limitations: 1) Inability to be applied to small objects due to heavy occlusions; 2) Inability to estimate an accurate full pose for axis-symmetric objects like cylinders.3) Noisy poses caused by fast movements, tracking inaccuracy, and heavy occlusions; 4) Huge time cost for the first time setup, i.e., several days, and large time cost for launching the pipeline before each data collection, i.e., about one minute.Besides, only successful trajectories can be kept, as the hand would then experience no load, and the object falling off would lead to a fast movement and an estimation failure.We can only roll out the policy and use clean actions without the flexibility to add noise, which may lead to task failure.As such, the diversity of the data would be restricted to objects that can be estimated and is biased towards easy geometries.Moreover, the object shape and scales used should match those used in the training.The dynamics model learning, even though we can collect a large amount of data, is relatively illposed if learning only from object states without the shape information, as for different objects, the same states and actions may lead to different transitions.Including the object shape in the dynamics modeling would inevitably further increase the modeling dimensionality and require an even larger amount of data to learn.</p>
<p>Collecting task-relevant data, even without estimating object poses, is also inherently limited to low efficiency, limited coverage, and restricted diversity since 1) data would be biased to easy objects that can be rotated well, 2) cannot add noise as it leads to the rotation failure, and 3) requires human interventions to reset the object to the hand.According to our experiments, the average time cost is 42.86s.Case Study on Estimating Object Poses via Foundation Pose.Collecting real-world transitions by leveraging a vision-based estimator to track object poses is difficult, requires frequent and tedious human interventions, and is prone to yielding noisy results.For each object, we need its CAD model with exactly the same scale.Initialization steps involve capturing images via the camera and utilizing XMem (Cheng &amp; Schwing, 2022) to get the object mask.At the beginning of each trail, we need to put the object near to the pose where we get the mask.After that, we need move the object from the table to the robotic hand and launch the policy.</p>
<p>Object</p>
<p>The difficulty of the data collection varies across the object geometry.For normal-sized objects, limitations primarily lie in noisy estimations, time-consuming, and human labor extensive.On average, we need 200s to collect a usable transition trajectory.</p>
<p>However, for small objects, it struggles to yield successful or even usable data.If we put the object initially on a table, then as we move the object up to the robotic hand, the pose tracking would fail, even if we move it very slowly.To resolve this, we hold the object by hand at a pose near to the robotic hand for initialization.After that, we need to insert it to the robotic hand for rotation.As the human hand retracts from the object, the estimated pose deviates from the object (Fig. 22).</p>
<p>Besides, for axis-symmetric objects, Foundation Pose cannot give stable estimations, where the pose continuously "rotates" while the object is kept still (Fig. 23).It prevents us from getting high-quality and clean pose estimations.Superiority of Our Autonomous Data Collection.Compared to task-relevant data, our autonomous data collection is object-agnostic.The hand would be continuously affected by timevarying object influences during the task execution.Joint effects of all loads to each joint simulate various external influences coming from coupling effects and the object.One can also use any other objects in he data collection to expand the diversity.Besides, we can add noise to the replay actions to expand the diversity and coverage.Moreover, it is efficient and requires no human intervention.</p>
<p>Inherent Limitations of Playing Base Waves to Collect Data.To get real-world transitions, a different approach from open-loop replaying policy action rollouts and rolling out the policy is playing parameterized waves such as sine waves, square waves, and Gaussian noise (Fey et al., 2025).This strategy suffers from the following drawbacks compared to using policy data: 1) For dexterous hands, sending signals to a single joint while keeping others still would cause self-collision, which Task-Relevant Data w/ Obj.Pose.We use a 5 cm × 5 cm × 5 cm cube to collect real-world transition trajectories with object-state annotations.During data collection, we roll out the policy while rotating the object about the z-axis, and estimate its pose with FoundationPose.Because the cube is symmetric, we resolve the pose-frame ambiguity at the start of tracking by flipping the model to align with our frame convention.Each data-collection episode lasts about 200 s on average.We evaluated datasets containing 17 and 54 trajectories.Under the same real-world evaluation protocol as in our ablations, the average rotation is 0.55 and 0.70, respectively.Fitting a learning curve to these points, we estimate how many trajectories would be required to match the performance of our method with 4,000 autonomous trajectories.As shown in Figure 24, the estimate is 52,483,440 trajectories-clearly impractical.Although this extrapolation is based on a small number of data points, it highlights the data efficiency and generalization of our approach.ratio no larger than 2:1 from the ContactDB dataset (Taheri et al., 2020) (obtained from GRAB dataset) as our test set, resulting in 26 objects in total.The filter rule follows RotateIt (Qi et al., 2023).As we aim to test the generalization performance on shape variation in this evaluation, we do not consider high aspect ratio ones or scale them to small sizes.In the real world, we test the performance on three subsets (Fig. 5, purple objects and small objects are unseen):</p>
<p>• Regular objects: cube (5 cm × 5 cm × 5 cm), cylinder (radius 5.5 cm, length 5.5 cm), apple (GRAB/ContactDB apple, scaled to 0.5×), cuboid (3 cm × 10 cm × 3 cm), and light bulb ("lamp bulb" from FurnitureBench).• Small objects: Purchased online; vendor links are withheld to preserve anonymity during review and will be provided upon acceptance.Fig. 26 shows dimensions of those objects used in the real-world experiment.• Normal-sized irregular objects: bear, truck, and cow from Visual Dexterity (each scaled to 0.7×); and bunny, elephant, duck, mug, teapot, and mouse from GRAB/ContactDB (each scaled to 0.5×).</p>
<p>Policy Optimization.We use PPO for policy optimization.Training environments are 30,000 for cylinders and cuboids, while 50,000 for long cuboids, small cylinders, and "DexEnv Objects".We randomly sample a wrist pose and a target rotation axis at each environment reset.</p>
<p>General Axes.To construct the general rotation axis set, we generate 32 axes evenly distributed in SO (3).Removing six principal axes, ±x, ±y, and ±z, we get the general rotation axis set. Figure 25 provides a visualization of all 32 evenly distributed rotation axes.</p>
<p>Generalist Training via Behaviour Cloning.To obtain the dataset to train the generalist policy, we roll out each oracle policy in the simulation to construct the dataset.Only transition trajectories that would not terminate in the full 400 steps would be saved in the dataset.We set the maximum number of tested environments to 1,500,000.In each step, the hand joint states, positional targets, object states, rotation axis, and the hand wrist orientation would be saved.Numbers of trajectories collected by each object category are summarized in Table 10.The number of successful rollouts could reflect the difficulty of different training object sets.Among all five object sets, regular cylinders and cuboids construct the easiest rotation tasks.Small cylinders introduces additional challenges due to its small scales.Complexity in the geometry further increases the difficulty.Rotating long objects with large aspect ratios is the most difficult task, which yields the smallest transition dataset.</p>
<p>Metrics (detailed version).</p>
<p>We evaluate using RotateIt metrics (Qi et al., 2023)  drops; in simulation, episodes are capped at 400 steps (20s) and TTF is normalized by 20s, while in the real world we report raw time; Rotation Reward (RotR)-episode sum of ω • k (simulation only); Rotation Penalty (RotP)-per-step average ω × k (simulation only); and Radians Rotated (Rot)-total radians rotated in the real world, measured from videos.We also report Goal-Oriented Success (GO Succ.) following Visual Dexterity (simulation only): we sample a random goal pose, set the target axis to the relative rotation axis, and count success if the final orientation is within 0.1π of the goal.</p>
<p>Automatic System Identification.In addition to training neural dynamics models and the delta action model to bridge the sim-to-real gap, we would align the dynamics between the simulator and the real world by performing an automatic system identification process at the beginning.The process involves the following steps: 1) Training probing rotation skills in the simulator using the default PD gains and link configurations in the URDF.2) Rollout probing skills in the simulator for multiple state-action trajectories (denoted as "probing trajectories").Replay probing trajectories on the real robot.3) Collect the resulting state and action trajectories.4) Launch multiple parallel environments in the simulator, each with different system parameters; 5) Replay probing action trajectories to get resulting state trajectories.6) Select parameters of the environment whose resulting state trajectories are the most similar to those in the real world as the identified system parameters.We identify PD gains and the mass of each link.Identified values are summarized in Table 11: Identified PD Gains.Per-Joint PD Gains identified by the automatic system identification process.</p>
<p>Joints are arranged according to the joint order in Isaac Gym.Table 12: Identified Link Mass.Per-Link mass identified by the automatic system identification process.</p>
<p>Links are arranged according to IsaacGym's link order.</p>
<p>Domain Randomization.We apply domain randomization during training.We also randomize the physical parameters during the test in the simulator.The randomization ranges of each object set are summarized in Table 9.Following previous works (Qi et al., 2022;2023), we apply a random disturbance force to the object.The force scale is 2m, where m is the object mass.We also resample the force at each timestep with the probability 0.25.We add a noise sampled from the distribution U(0, 0.005) to the joint positions to increase the robustness.</p>
<p>Baselines (detailed version).We compare our method against both previous in-hand rotation/reorientation works and prior neural-based sim-to-real works.We compare with two strong in-hand rotation/reorientation works, Visual Dexterity (Chen et al., 2022) and AnyRotate (Yang et al., 2024).The experimental setup of AnyRotate is the most similar to ours.It demonstrates multi-axis object rotation under various wrist orientations.However, its code is not publicly available, and the method requires tactile information.We re-implemented their environment setup and training pipeline in IsaacGym based on the paper's description.We've tried our best to set up a fair comparison with it in the real world.Unfortunately, faithfully replicating their tactile sensor model and sim-to-real methodology from the paper alone is difficult.We find that discarding the tactile information in its second stage training can hardly yield a policy with even basic rotation capabilities in the real world.Thus, a direct real-world comparison was not possible.Instead, we demonstrate our method's superior performance by evaluating it on the same challenging object shapes used in their experiments.For Visual Dexterity, the open-sourced code is designed for the D'Claw hand, which is much large than and quite morphologically different from anthropomorphic hands like the Allegro or LEAP.Despite our extensive efforts to adapt their code to the LEAP hand, the policy failed to achieve reasonable performance in simulation on a basic cylinder shape, even after 1.5 days of training.Thus, a direct comparison was infeasible.We therefore compare our method's performance with the quantitative results reported in their paper and the qualitative results shown in their website.</p>
<p>We also compare with prior sim-to-real methods designed for robotic arms and legged robots, namely UAN (Unsupervised Actuator Net) and ASAP.The core of both UAN and ASAP is similar, which lies in collecting real-world transition data for actuators, training neural compensators to bridge the dynamics gap between the simulator and the real world, followed by tuning/training the task policy based on the learned neural compensator.The main differences lie in two aspects, including data collection and model design.ASAP rollouts tracking policies and locomotion policies in the real world for collecting real-world transitions, while UAN avoids using policy data by playing sine waves, square waves, and Gaussian noises to prevent overfitting.UAN uses a shared network for every actuator while ASAP trains a full-body compensator (four ankle joints for sim-to-real).</p>
<p>As discussed before (Sec.3.3), neither including the object into the system modeling nor replicating object influence in the simulator is possible.Thus, we collect 24,000 real-world free-hand replay trajectories to train their corresponding compensators.To compare UAN, we employ their real-world collection strategy and train a shared compensator for each joint in the hand.To compare ASAP, we replay the policy rollouts and train a compensator for each finger in the sim-to-real comparison, mirroring their four ankle joints sim-to-real setting.In sim-to-sim, we train a compensator for the whole hand and the object.</p>
<p>Comparisons to AnyRotate (detailed version).We compare our real-world performance against reported values in AnyRotate.As they did not provide links to obtain their real-world test objects, we test our model on four of its tested objects that are easy to replicate, including "Tin Cylinder", Cube, "Gum Box" and "Container" (see details below).While the remaining plastic vegetable models and the "Rubber Toy" are not reproducible according to the object size information provided in their Table 10.According to its experiments, objects with sharp edges are more difficult to rotate compared to plastic vegetable models (their performance on "Tin Cylinder", "Gum Box", and "Container" is the worst regarding the number of rotations and survival time among all of its tested objects as shown in its Table 12 and 13).We test the performance on three test rotation axes from AnyRotate in the rotation axis test setting.We also employ the same rotation axis setting and the hand orientation setting to AnyRotate in the hand orientation test setting.We conduct three independent experiments and present the average and deviations across the three trials in the Table 2.</p>
<p>As shown, we can outperform AnyRotate by a large margin.</p>
<p>Besides, as demonstrated, our policy can rotate a wide range of objects with diverse aspect ratios and various object-to-hand ratios.Rotating some of them, such as the long Lego leg and animal shapes, requires quite sophisticated finger gaiting.However, AnyRotate only demonstrates the ability of rotating normal sized objects with relatively flat surfaces using conservative behaviours.As stated in their paper, they would encounter difficulties when rotating objects with sharp edges.Besides, the smallest objects that they have demonstrated the effectiveness are the "Rubber Toy" (8cm × 5.3cm × 4.8cm ), "Tin Cylinder" (4.5 × 4.5cm × 6.3cm), and "Cube" (5.1cm × 5.1 cm × 5.1cm).However we can deal with much smaller objects like vegetable models with sizes 3cm × 3cm ×2.5cm, 3cm × 2.75cm × 2.75cm, and 3cm × 2cm × 2.1cm.Moreover, the most challenging aspect ratios of their objects is 1.67 (Rubber Toy), while we can handle objects with challenging aspect ratios such as Lego leg (4.5), Book (5.3), and long cuboid (3.33).Such comparisons further demonstrate the superiority of our method in solving difficult in-hand rotation problems.</p>
<p>Details w.r.t.Our Replicated Objects from AnyRotate.We replicated their four test objects as follows:</p>
<p>• Cube: We 3D-printed a cube to the specified dimensions of 5.1cm × 5.1cm × 5.1cm.</p>
<p>• Container: We buy a commercially available product that precisely matches the container used in their experiment.We removed the labels from the container to maintain regional anonymity.</p>
<p>• Tin Cylinder: We 3D-printed a cylinder with the specified 4.5cm radius and 6.3cm length.</p>
<p>Drops the object without any rotation.Rotate for at least one circle.</p>
<p>Joint-Wise (w/o Load)</p>
<p>Difficult Cylinder Easy Cylinder</p>
<p>Grasps the object but fails to manipulate it.</p>
<p>The object drops after a strange rotation.</p>
<p>UAN ASAP</p>
<p>Rotate the basic cylinder for at most 270 o .</p>
<p>Cannot rotate the hard cylinder.</p>
<p>Whole Hand (w/ Load) • Gum Box: We identified a discrepancy in the documented dimensions (9cm × 8cm × 7.6cm), which were identical to those of the "Container".However, figures in the original paper indicate the "Gum Box" is substantially smaller.Therefore, we estimated its dimensions from the figures to be approximately 5cm × 4cm × 8cm and 3D-printed an object of this size to serve as a proxy.</p>
<p>Comparisons to Visual Dexterity (detailed version).Compared to prior works, visual dexterity shows improved results in rotating more complex objects with uneven surfaces and better generalization ability to unseen geometries.Conducting a direct and completely fair comparison between our method and Visual Dexterity, however, is infeasible due to the different task settings (i.e., ours axis-oriented continuous rotation v.s.Visual Dexterity's goal pose-driven reorientation).Therefore, we introduce a new metric, survival rotation angles, that could be computed from qualitative results in both settings to facilitate a comparison.Specifically, it evaluates the angles the object could be rotated before it falls from the hand.This metric is friendly for Visual Dexterity since, in some settings, it has a supporting table.The object can touch the table during the rotation process.We obtain Visual Dexterity's results by carefully examining all of its demos present in all videos from its website.Its best performance and the comparisons to our results are summarized in Table 3.Though the metric is more friendly to Visual Dexterity, we can still achieve on par performance or bypass its results for all irregular objects included in its demos (see videos in our website).Specifically, we make the following observations: 1) For objects on which Visual Dexterity has demonstrated strong results, including cow, bear, and truck, where they have shown the ability to rotate the object to achieve several goals continuously without falling, we can at least achieve on-par performance with it.2) For objects that it struggles with, including elephant, bunny, duck, teapot, and dragon, we can outperform it and achieve a much better performance regarding the survival angles.3) We have shown superiorities in rotating objects with challenging aspect ratios (up to 5.33) and difficult object-to-hand ratios (i.e., long objects like the Lego leg and small plastic vegetable models, Fig. 1).However, Visual Dexterity does not demonstrate such ability.Comparisons to ASAP and UAN (detailed version).We evaluated our method against two prominent sim-to-real transfer approaches in both sim-to-sim and sim-to-real settings.Considering the difficulty in collecting real-world data with object states and the fact that their original data collection strategy does not account for the object influence, we collect 24,000 freehand trajectories in the real world by replaying policy action rollouts using the same hand wrist configurations as in our data collection strategy for data with load.After that, we train a dynamics compensator in the corresponding free-hand simulation setup.This compensator is subsequently used to finetune the original policy.We reward the compensator training using the hand-only training penalty:
r compensator = −∥q ref t − q∥ 2
, where q ref h and q t are the reference joint state and the current joint state respectively.While we originally intended to conduct a comprehensive comparison in all settings covered in Table 4 and 5, we found that the policies produced by these baseline methods failed to function in the real world.They were unable to rotate the easiest cylinder object.The typical failure modes involved the robot either grasping the object firmly without movement or failing after a strange perturbation (Fig. 27 (A)).(Videos demonstrating these failures are available on our website.)Notably, the policy fine-tuning process did achieve satisfactory results.We therefore hypothesize that an OOD issue causes this: the compensator, trained only on the dynamics of a free hand, fails when the policy must handle the novel dynamics introduced by an object during the rotation.This finding underscores the critical importance of modeling object dynamics in the design of sim-to-real strategies for manipulation, which also aligns with discoveries in ablation studies (Sec.5).</p>
<p>We also attempted to train the baseline sim-to-real methods (ASAP and UAN) using our collected task-relevant, object-state-annotated dataset (54 trajectories).However, the first stage-compensator training-failed to converge; the reward showed little to no improvement.We attribute this to the dataset's limited size and object state noise.</p>
<p>Sim-to-sim comparisons are summarized in Table 4.2.</p>
<p>Our compensation strategy also shows better resistance to the quality of real-world transitions.As shown in Figure 27, our ablated version "Joint-Wise (w/o Load)" trains the dynamics model via free hand replay data, whose data amount is even smaller than that used to train UAN and ASAP, can rotate the basic cylinder object for at least one circle, though its final performance cannot even surpass the base policy.However, the above two strategies totally fail in this task.Since they would use the compensator to fine-tune the base policy, their final policy's performance is quite sensitive to the quality of the learned compensator.Thus, only if the learned compensator is of very high quality and can generalize quite well can its fine-tuning achieve satisfactory results.Otherwise, the final policy may totally fail since they are learned with "wrong" dynamics.However, we compensate the base policy by using it with the learned residual policy together.With a good base, the final performance would not at least totally fail.</p>
<p>"Sim-to-Sim".We collect the data in Genesis by running the evaluation for the unified policy using 30.000environments.We use cylinders to collect the data.We run the evaluation on each cylinder instance with the maximum number of evaluation trails set to 1,500,000.We use all rollout data to train the joint-wise neural dynamics model (pre-trained using transitions in Isaac Gym).The training is conducted on eight A10 GPUs for 2 epochs with a batch size of 64, which takes approximately two days.We collect the data in MuJoCo using one environment.For each training cylinder instance, we collect 4000 trajectories, resulting in 36,000 trajectories in total.We use all data to train a joint-wise dynamics model (pre-trained using transitions in Isaac Gym).</p>
<p>After that, we train the residual policy for two epochs, which takes about 13 hours.We then deploy the residual policy with the original base policy to the target simulator.The policy is tested on the ContactDB test object set.We roll out the policy using 10 different initial grasps.Reported values are the mean and standard deviation values of per-object average results over 10 trials.</p>
<p>Figure 28 shows a qualitative comparison of the policy's performance w/ and w/o our method to bridge the dynamics gap.</p>
<p>"Sim-to-Sim" Comparison Settings.We use the same data collection strategies to collect transitions in each simulator.The difference is that only successful rollouts are kept, resulting in 3280673 trajectories in Genesis, while 23650 trajectories in MuJoCo.These trajectories are leveraged to train their corresponding action compensators for ASAP and UAN.For ASAP, we use the whole hand formulation, different from the per-finger compensator that we leveraged in ASAP's sim-to-real setting.We reward the policy to track both the object state and the hand state:
r compensator = −k h ∥q ref t − q∥ 2 − k o ang diff(o ref t , o t )
, where q ref t , o ref t , and o t are the hand reference joint state, object reference orientation and object current orientation respectively.k h and k o are coefficients to balance hand and object tracking.k h is set to 1.0.While we add a curriculum to k o .It is set to a small value, i.e., 0.001, at first.And we use the reset number of the first environment to count the reset step.During the first 10 reset steps, k o is kept at the initial value.While starting from that and until the 200-th reset step, k o is linearly increased to 2.0.After the compensator has been trained, we tune the policy based on it.The tuned policy is then deployed to the target simulator.We adopt the same evaluation strategy as for our method.</p>
<p>LEAP Hand</p>
<p>Franka Arm Real-World Hardware Setup.We LEAP hand (Shaw et al., 2023) and Franka Arm for conducting real-world experiments (Fig. 30).We use positional control with a control rate of 20 Hz.The positional gain and damping coefficient are set to 800 and 200, respectively.</p>
<p>Real-World Data Collection Setup.To collect real-world transition data with varying loads while minimizing human intervention, we developed several strategies, as illustrated in Figure 29.</p>
<p>Among these, the "Chaos Box" with balls proved most effective.Its setup is straightforward: place the box on a table, open it, and position the robot's hand inside with a desired orientation.Crucially, this method operates autonomously, requiring no human intervention during data collection.This setup ensures continuous interaction with a load, as the robot's hand is always in contact with the balls.The constantly shifting positions of the lightweight balls provide a diverse and continuous range of loads.Furthermore, the balls' deformable surfaces ensure that these interactions do not damage the robot's hardware.The autonomy of this system allows us to initiate data collection in the evening and let it run overnight unattended.</p>
<p>A key limitation of the Chaos Box is its inability to collect data in a palm-up orientation due to the robot arm's kinematic constraints.To address this, we developed a second setup where a ball is secured to three of the robot's fingers with a bandage (Fig. 29 (B)).Similar to the Chaos Box, this method runs autonomously once initiated.However, binding the ball takes time.A drawback is that the ball's fixed position results in a less diverse set of perturbation patterns.</p>
<p>Two other approaches were explored but ultimately not adopted (Fig. 29 (C,D)).One involved attaching an object to the finger (C), but this was unreliable as the object could fall and require manual reattachment.The other used a supporting table (D), but the object often moved outside the robot hand's workspace, necessitating human intervention to reposition it.</p>
<p>Robotic Hand Sizes.We define hand size as the fingertip span: for the D'Claw hand, the distance between diagonally opposite fingertips (19.10 cm); for the Allegro and Leap hands, the distance between the index and pinky fingertips (10.05 cm and 9.50 cm, respectively).</p>
<p>Real-World Transition Data Collection.We collect real-world transition data by replaying action trajectories rolled out in the simulation.Each episode contains 400 steps.Actions are executed in the hardware at 20Hz.Collecting one trajectory with a full episode takes approximately 20s.We collect transitions with all six tested hand wrist orientations, that is, palm up, palm down, thumb up, thumb down, base up, and base down.In each orientation, we collect 4,000 transition trajectories.</p>
<p>In more detail, we randomly at uniform select 4,000 trajectories from rollouts of all oracle polices with the corresponding wrist orientation.We collect transitions using the "Chaos Box" system.</p>
<p>Experimental Settings of Ablation Studies.When comparing real-world performance of different models in ablation studies, we keep the hand in the palm down orientation and test the z-rot performance on three representative objects, including a regular cylinder, a cylinder with higher aspect ratios, and an irregular object.We roll out the policy for rotating the regular cylinders in this specific hand orientation and the rotation direction to construct the simulation dataset, which is composed of 937,275 trajectories, each of which has 400 transition steps.</p>
<p>Real-World Data Collection.We collect transition data via the Chaos Box setup (Fig. 29 (A)).We replay action trajectories rolled out in the simulation in the real world to collect the data.We collect 4,000 trajectories, resulting in 1,600,000 transitions in total.In addition, we collect 20 successful rotation trajectories (i.e., object does not fall during the whole episode) with the thumb up orientation on a 5cm size cube by deploying policies in the real environment as the out-of-domain test data.</p>
<p>Task-Relevant Data Collection.We collected 1 hour of data per object using three objects: a 5 cm × 5 cm × 5 cm cube, the Stanford Bunny, and a cylinder (radius 5.5 cm, length 5.5 cm).In total, we obtained 111, 87, and 54 trajectories with the cube, cylinder, and Stanford Bunny, respectively.</p>
<p>Collecting via Base Waves.We collect 2,000 trajectories using sine waves, 1,000 trajectories using square waves, while 1,000 using Gaussian noise.When collecting the trajectory using the sine wave, we randomly select a joint to send signals while leaving the other joints fixed.Specifically, we fix other joints to the midpoint of their angle range.For LEAP hand, actuating the joint between mcp link to pip link when fixing other joints would lead to self-collision.So we would not select such joints when replaying trajectories.We use the sine wave with the form f (t) = σ sin(2ωt).At the beginning of each data collection, we sample σ and ω from a uniform distribution, i.e., σ ∼ U(0.5, 1.0), ω ∼ U(0.2, 0.5).When using the square waves, we use g(t) = A * sign(sin(2 * ω * t)), where A ∼ U (0.5, 1.0), ω ∼ U (0.2, 0.5).We add Gaussian noise to the square wave to collect remaining 1,000 trajectories, i.e., ĝ(t) = A * sign(sin(2 * ω * t)) + ϵ, where ϵ ∼ N (0, 0.01).</p>
<p>Dynamics Model</p>
<p>Training.The pretrained dynamics model is obtained by leveraging the same model architecture to fit the roll-out simulation trajectories.We then directly tune the model weights on the real-world data for fine-tuning.An evaluation dataset is split out from the 4000 training trajectories with a train: eval ratio of 9:1.The Model with the best evaluation loss is then leveraged to train the residual policy model.We report the final result on the OOD test dataset as the generalization performance.We train the residual policy on the simulation data for one epoch, which would typically cost for about 10 hours using eight A10 GPUs.</p>
<p>Teleoperation System for Complex Dexterous Manipulation Data Collection.We demonstrate an important application of our rotation policy: a teleoperation system for complext dexterous manipulation tasks with in-hand rotation.We implement it by pairing the policy with a Quest 3 headset (Fig. 31).Leveraging in-hand rotation, the system completes complex tasks requiring fine-grained finger coordination-scenarios where traditional teleoperation systems (Ding et al., 2024;Cheng et al., 2024) often struggle.</p>
<p>Left Controller</p>
<p>Right Controller VR Headset</p>
<p>Buttons to define rotation axes Figure 31: Quest 3. We teleoperate the arm using the right controller's pose, while the left controller's pose specifies the desired rotation axis.We also provide a button-controlled mode that restricts rotation to three fixed axes, selected via the X, Y, and LG buttons on the left controller.</p>
<p>We adapt BunnyVisionPro (Ding et al., 2024) for Franka arm teleoperation.The arm is controlled with the Quest 3 right-hand controller, and we obtain controller states via oculus reader.We use the left controller's orientation to define the rotation axis and down-weight the component around its short axis to reduce errors when inferring the axis from pose.In practice, this orientation-based specification is not very intuitive, so we introduce a button-controlled mode in which the rotation axis is selected by pressing the X, Y, or LG buttons on the left controller.Although this restricts the available axes to three, we find it sufficient for single tasks; for example, lightbulb assembly and disassembly can be completed using z, -z, and -y rotation modes.</p>
<p>All hand motions, including grasping, are controlled by the policy.We initialize the robotic hand in a default pose.To grasp an object, we approach it and activate the rotation policy.Conditioned on an initial open-hand observation, the policy outputs an action sequence that closes the fingers around the object to achieve a secure grasp.</p>
<p>D DISCUSSIONS ON RELATED SIM-TO-REAL WORKS</p>
<p>Misaligned physical parameters, discrepancies in their physical models, and numerous unmodeled effects in the actuator and contact dynamics hinder successfully transferring the policy trained in simulation to the real world.Efforts to close this gap mainly fall into four types of approaches: 1) Domain Randomization (DR) expands the distribution of training environment to train robust policies that are expected to function well in different environments (Loquercio et al., 2019;Peng et al., 2017;Tan et al., 2018;Yu et al., 2019;Mozifian et al., 2019;Siekmann et al., 2020;Sadeghi &amp; Levine, 2016).2) System Identification (SysID) aligns The simulator dynamics to the real-world in a principled and interpretable way by estimating critical physical parameters from real data (An et al., 1985;Mayeda et al., 1988;Lee et al., 2023;Sobanbabu et al., 2025).3) Adaptive Policy adapts the policy online according to the real-world dynamics that are implicitly identified from real-world feedback.4) Neural-based Real World Modeling learns real dynamics to help with policy's transfer (He et al., 2025;Fey et al., 2025;Deisenroth &amp; Rasmussen, 2011;Shi et al., 2018;Hwangbo et al., 2019).As a popular and standard strategy, DR requires heuristic designs (Sobanbabu et al., 2025) to find proper randomization ranges.While generalizable and interpretable, the upper bound of SysID is restricted by the coverage of parameters to be identified.For a successful adaption, the training environment should cover a wide distribution, which is typically achieved by DR.This limits their effectiveness when the real-world dynamics cannot be covered by randomizing the simulated environment.With the potential of aligning all kinds of discrepancies, guiding the policy's transfer via modeling real-world dynamics has the highest upper capabilities, making it the focus of our work.One approach is leveraging neural networks to perform system identification, learning residual dynamics or representations (Shi et al., 2018;O'Connell et al., 2022), followed by developing a model-based controller (Fig. 2 (A)).For systems involving higher degrees of freedom (DoFs) and more complex dynamics, learning a comprehensive dynamics model that supports controller optimization is difficult.An alternative strategy is bridging the gap between an existing simulator and the real world by learning a delta function (He et al., 2025;Fey et al., 2025), followed by policy finetuning to bridge the gap (Fig. 2 (B)).</p>
<p>However, directly extending those approaches to dexterous manipulation, with rich, rapidly varying contacts on moving objects, cannot work.The primary challenge lies in collecting high-quality real world transition data that can cover the vast task distribution, thereby reflecting dynamics during the task execution.This is achieved by replaying waveforms (e.g., sine) or rolling out policies-none can work in our setting.</p>
<p>Wave-based collection is untenable: manipulated objects enlarge the transition space and impose time-varying loads, yielding dynamics unlike the no-object regime (see Appendix A.3).Because parameterized waves cannot reliably manipulate an object in air, they must be run without it, offering poor coverage of in-hand dynamics.On-policy rollouts across diverse objects are costly and unscalable-requiring frequent human resets (placing the object back in hand), biasing data toward easy objects, confining coverage to the policy rollout distribution, and suffering from low quality (imperfect policy).</p>
<p>Extending their methods to manipulation also necessitates modeling the interaction dynamics, which inevitably involves modeling the object.There are two approaches to model the object: 1) Explicitly including the object in the dynamics system.Achieving this requires collecting real-world transition trajectories with object state annotations.However, obtaining object states (e.g., using vision-based pose trackers like FoundationPose (Wen et al., 2023))) is difficult and impossible for some cases.For instance, FoundationPose (Wen et al., 2023)) are unreliable for axis-symmetric, tiny, and occluded objects (see Sec. B.4). Besides, the object pose tracking results are noisy.It is also very time-consuming, requiring extra time to launch and frequent human interventions.Using the small, noisy dataset cannot even make the first stage, compensator training, successful.Another strategy is modeling the object as a time-varying disturbance.This requires us to a) collect transition data with the object loads; b) manage to simulate the object's influence to the hand in the simulator; and c) train the compensator to track the hand state only.However, it is almost impossible, as reproducing its influence would require near-perfect alignment of geometry, initialization, and contact evolution-unrealistic under mismatched dynamics.</p>
<p>What data can we use to train ASAP and UAN in dexterous manipulation?We discuss three options: (1) transitions with object-state annotations-possible in principle but impractical, as object states are hard and noisy to obtain and, in our tests, such small and noisy data fail to train their compensator; (2) our autonomously collected trajectories with randomized object loads-unsuitable because replicating the influence of such object loads to the hand in the simulator is infeasible;</p>
<p>(3) free-hand data-the only practical choice, on which we train their compensator to close the dynamics gap in the free hand scenario.Hence, we use free hand transitions when comparing with their methods.In each group with two subfigures, the left one draws the original data sequence and the fitted sequence using a 5-ordered polynomial function while the right one shows the fitting error distribution.(window length = 10).In each group with two subfigures, the left one draws the original data sequence and the fitted sequence using a three-order polynomial function while the right one shows the fitting error distribution.(window length = 10).In each group with two subfigures, the left one draws the original data sequence and the fitted sequence using a five-order polynomial function, while the right one shows the fitting error distribution.</p>
<p>Figure 2 :
2
Figure 2: Learning from Real-World Data for Control.(A) Learn a whole-body dynamics model from real-world data for policy tuning or model-based control.(B) Learn a residual action model to finetune a base policy.(C) Learn joint-wise dynamics and a residual policy to adapt the base policy.</p>
<p>Figure 3 :
3
Figure 3: Method Overview.(A) RL-train object category-specific rotation specialists.(B) Distill them into a single generalist via BC.(C-E) Neural sim-to-real: autonomously collect real-world transitions with random loads (C), learn a joint-wise neural dynamics model (D), and train a residual to bridge the reality gap (E).Deploy the base generalist (B) augmented with the residual (E).</p>
<p>Figure 4 :
4
Figure 4: State-Action History Distribution.</p>
<p>Figure 5 :
5
Figure 5: Objects for Real Experiment.</p>
<p>Figure 6 :
6
Figure 6: Comparisons to Whole-Hand Neural Dynamics w.r.t.Model Expressivity, Sample Efficiency and Transferrability.(A,A-0) In-domain and out-of-distribution performance in high (3.1M) and low (7.5k) data regimes.(B) Sample efficiency.(C) Transferrability from different training distributions.</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: Ablation Study of the Dynamics Model.(A) Generalization error of different model ablations (lower is better).(B) Corresponding real-world task performance.(A)</p>
<p>Figure 10 :
10
Figure 10: Per-Joint State-Action Sequences (Free Hand, w/o Load).</p>
<p>Figure 11 :
11
Figure 11: Per-Joint State-Action Sequences (Autonomous Data Collection, w/ Load).</p>
<p>Figure 12 :Figure 13 :
1213
Figure 12: Per-Joint State-Action Sequences (Task-Aware Data).</p>
<p>Figure 14 :
14
Figure 14: Predicting via Single Joint State-Action History (In-Distribution Validation Error).m</p>
<p>Figure 15 :
15
Figure 15: Per-Joint Distribution</p>
<p>Figure 16 :
16
Figure 16: Per-Finger Distribution</p>
<p>FigureFigure 18 :
18
Figure 17: Whole Hand Distribution</p>
<p>Figure 20 :Figure 21 :
2021
Figure 20: Real World Results.Rotating challenging objects in the air.See more and videos in our website.</p>
<p>Fig. 20
20
Fig. 20 and 21 provide more real-world qualitative results.See more results and videos in our website.</p>
<p>Figure 22 :
22
Figure 22: Pose Tracking During Manipulation for A Small Object.</p>
<p>Figure 23 :
23
Figure 23: Pose Tracking for Axis-Symmetric Objects.</p>
<p>Figure 24 :
24
Figure24: Performance scaling with dataset size.We fit the curve of "Task-Awre w/ Obj.Pose" via powerlaw and extrapolate it to estimate the number of data required to achieve the desired result.</p>
<p>Figure 25 :
25
Figure 25: General Rotation Axes.</p>
<p>( 2 .Figure 26 :
226
Figure 26: Dimensions of Small Objects Used in Real World Experiments.</p>
<p>× 10 −7 2.57 × 10 −1 2.41 × 10 −2 1.90 × 10 −2 2.79 × 10 −2 1.05 × 10 −2 1.00 × 10 −7 4.68 × 10 −2 3.00 × 10 −3 3.65 × 10 −2 5.38 × 10 −2 × 10 −7 3.12 × 10 −2 2.63 × 10 −2 2.11 × 10 −2 1.63 × 10 −2 1.00 × 10 −7 5.03 × 10 −2 3.43 × 10 −2 4.76 × 10 −2 2.23 × 10 −2 1.00 × 10 −7</p>
<p>Figure 27 :
27
Figure 27: Case Study on Failure Cases of Baselines (UAN and ASAP) and Ablated Versions (Joint-Wise (w/o Load) and Whole Hand (w/ Load)).</p>
<p>Figure 28 :
28
Figure 28: Qualitative "Sim-to-Sim" Evaluation.Left: Results in Genesis.Right: Results in MuJoco.</p>
<p>Figure 29 :
29
Figure 29: Autonomous Real Data Collection Setup with Load.(A) A large box with many soft balls.(B) Bind the object to three fingertips to avoid the object falling off and to add external object influence to the hand.(C) Bind objects to two fingertip,s which adds external influence to the hand via collisions between these objects.(D) Adding a supporting table to avoid the object falling off.</p>
<p>Figure 30 :
30
Figure 30: Real World Experiment Hardware Setup.Grasping Pose Generation.We generate grasping poses with the "Palm Down" orientation, which are used for the omni wrist orientation rotation training.For details, please refer to the cdoe in the supp ('DexNDM-Code/RL/README.md').The canonical qpos of LEAP hand, from which we sample random noise to generate the grasping poses, is set to[1.244, 0.082, 0.265, 0.298, 1.163,  1.104, 0.953, -0.138, 1.096, 0.005, 0.080, 0.150, 1.337, 0.029, 0.285, 0.317].</p>
<p>Figure 32 :
32
Figure32: Polynomial Fitting (order = 3) and Error Distribution of Per-Joint State Sequences (window length = 10).In each group with two subfigures, the left one draws the original data sequence and the fitted sequence using a 3-ordered polynomial function while the right one shows the fitting error distribution.</p>
<p>Figure 33 :
33
Figure33: Polynomial Fitting (order = 5) and Error Distribution of Per-Joint State Sequences (window length = 10).In each group with two subfigures, the left one draws the original data sequence and the fitted sequence using a 5-ordered polynomial function while the right one shows the fitting error distribution.</p>
<p>Figure 35 :
35
Figure 35: Per-Joint Average Polynomial Fitting (order = 5) Error.</p>
<p>Figure 36 :
36
Figure36: Polynomial Fitting (order = 3) and Error Distribution of Per-Joint Active Force Sequences (window length = 10).In each group with two subfigures, the left one draws the original data sequence and the fitted sequence using a 3-ordered polynomial function while the right one shows the fitting error distribution.</p>
<p>Figure 37 :
37
Figure37: Polynomial Fitting (order = 5) and Error Distribution of Per-Joint Active Force Sequences (window length = 10).In each group with two subfigures, the left one draws the original data sequence and the fitted sequence using a 5-ordered polynomial function while the right one shows the fitting error distribution.</p>
<p>Figure 40 :
40
Figure 40: Polynomial Fitting (order = 3) and Error Distribution of Per-Joint Virtual Force Sequences</p>
<p>Figure 41 :
41
Figure 41: Polynomial Fitting (order = 5) and Error Distribution of Per-Joint Virtual Force Sequences</p>
<p>Table 1 :
1
Generalization Test in Simulation.Comparisons of the rotation performance on the unseen test object set along each axis with hand wrist orientation randomized over rotation metrics.
MethodRotR ↑±x-axis TTF ↑RotP ↓RotR ↑±y-axis TTF ↑RotP ↓RotR ↑±z-axis TTF ↑RotP ↓General Rotation Axes RotR ↑ TTF ↑ RotP ↓GO. Succ.AnyRotate* (re-implementation) 91.90±11.60 0.67±0.17 0.72±0.05 163.78±20.44 0.73±0.18 0.81±0.19 173.87±11.70 0.82±0.15 0.52±0.14 162.55±19.18 0.86±0.18 0.79±0.11 64.33±4.70Ours (Generalist in Sim)144.22±13.91 0.77±0.19 0.54±0.03 224.28±23.69 0.88±0.17 0.58±0.09 314.28±27.91 0.92±0.14 0.37±0.05 242.33±23.30 0.94±0.05 0.46±0.06 88.27±3.21"Cube""Container""Tin Cylinder""Gum Box"Method</p>
<p>Table 2 :
2
Comparisons to AnyRotate.Comparison of rotation degrees (Rot (radian)) and time-to-fall (TTF (s)) under two test settings introduced in AnyRotate (Table12, 13) on replicable objects.
MethodCowBearTruckGRAB ElephantBunnyDuckTeapotDragonTrainHundepaarElephantAirplaneMouseVisual Dexterity71063258<em>2</em>2<em>3</em>4<em>3</em>4*DexNDM810675648434434</p>
<p>Table 3 :
3
Comparisons to Visual Dexterity of Survival Angles (⌊radian/0.5π⌋),roughly measuring (from videos) how many 90 degrees the object can be rotated before falling.The subscript * denotes the performance achieved by rotating the object with a supporting table.</p>
<p>Table 4 :
4
Multi-Axis Rotation in Real.Comparison of rotation degrees (Rot (radian)
Object Set Method±x-axis Rot (rad) TTF (s)±y-axis Rot (rad) TTF (s)±z-axis Rot (rad) TTF (s)Cubic Diagonal Axes Rot (rad) TTF (s)Direct Transfer9.84±0.3626.80±0.2010.37±0.5530.73±1.6711.69±0.3021.67±2.749.03±0.4722.71±2.04RegularWhole Hand NDM5.92±0.1415.04±1.432.41±0.228.59±0.357.38±0.4916.33±1.793.30±0.448.87±0.62DexNDM11.36±0.40 32.40±1.78 14.24±1.19 44.60±5.44 23.82±3.86 37.50±5.02 16.93±1.84 30.44±3.08Direct Transfer4.71±0.0025.17±9.416.11±0.3026.22±1.906.94±0.8520.17±0.725.40±0.3223.21±3.80SmallWhole Hand NDM0.35±0.060.44±0.080.87±0.101.33±0.130.00±0.000.00±0.000.26±0.140.67±0.21DexNDM5.24±1.3528.00±9.136.81±0.9129.78±5.099.29±1.6326.75±5.246.03±0.5127.34±4.97Direct Transfer4.41±0.3419.95±2.266.13±0.4724.62±2.545.26±0.3121.19±2.226.53±0.3726.29±1.25IrregularWhole Hand NDM1.34±0.215.51±0.362.91±0.5010.32±0.720.720.064.03±2.922.33±0.6811.68±2.05DexNDM6.35±0.6924.21±2.87 11.32±2.08 39.04±7.288.61±0.7629.33±1.389.19±1.0133.14±1.86</p>
<p>Table 5 :
5
Multi-Wrist Orientation Rotation in Real.Comparison of rotation degrees (Rot (radian)) and timeto-fall (TTF (s)) under six representative hand orientations across direction z.</p>
<p>Comparisons of Data Distributions between Collected Trajectories and Rotation Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .23 B Additional Experiments and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . .24 B.1 Training Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24</p>
<p>The main limitation is that the model's ceiling is restricted by partial observations; jointly modeling hand-object transitions from richer signals, and integrating tactile are valuable future directions.APPENDIX A Additional Explanations of the Method . . . . . . . . . . . . . . . . . . . . . . . . . . .14 A.1 Policy Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14 A.2 Proof of Main Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15 A.3 Rationality of Joint-Wise Dynamics Modeling (part I) . . . . . . . . . . . . . . . . . . .19 A.4 Rationality of Joint-Wise Dynamics Modeling (part II). . . . . . . . . . . . . . . . . . .21 A.5 B.2 Additional Real World Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26 B.3 Case Study on the Effectiveness of Our Sim-to-Real Method . . . . . . . . . . . . . . .26 B.4 Further Discussions, Analysis, and Ablation Studies . . . . . . . . . . . . . . . . . . . .27 C Additional Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .29 D Discussions on Related Sim-to-Real Works . . . . . . . . . . . . . . . . . . . . . . . . .37</p>
<p>Table 7 :
7
Effectiveness of the Sim-to-Real Method on Challenging Shapes.Comparison on Rot (in radian)
Direct Transfer7.336.283.674.364.1931.423.6710.475.7619.37DexNDM8.387.076.286.816.2899.486.2816.7610.47130.90</p>
<p>Table 10 :
10
The Number of Collected Transition Trajectories in Simulation.
Object SetCylindersCuboidsLong Cuboids Small Cylinders DexEnv Objects# Transitions 1,333,282 1,282,973235,413743,543681,199
in simulation and the real world, plus a goal-oriented success metric: Time-to-Fall (TTF)-duration until the object</p>
<p>Table 11 and 12.
Joint Index0123456789101112131415P Gain3.521.782.842.301.942.182.552.012.262.303.764.641.863.444.821.53D Gain0.194 0.106 0.091 0.195 0.199 0.192 0.149 0.050 0.088 0.135 0.027 0.081 0.123 0.042 0.082 0.068</p>
<p>Joint Fitting Error (State (qpos))</p>
<p>Figure 34: Per-Joint Average Polynomial Fitting (order = 3) Error.
Mean Squared Error (MSE) Mean Squared Error (MSE)J o in t 1 ×10 5 Average Per-J o in t 1 J o in t 2 J o in t 3 J o in t 4 J o in t 5 J o in t 6 J o in t 7 J o in t 2 J o in t 3 J o in t 4 J o in t 5 J o in t 6 J o in t 7 ×10 6 0.0 0.5 1.0 1.5 2.0 0 1 2 3 4 5 6J o in t 8 Joint Index J o in t 9 J o in t 1 0 J o in t 1 1 J o in t 1 2 J o in t 1 3 J o in t 1 4 J o in t 1 5 J o in t 1 6 J o in t 8 J o in t 9 J o in t 1 0 J o in t 1 1 J o in t 1 2 J o in t 1 3 J o in t 1 4 J o in t 1 5 J o in t 1 6Joint Index</p>
<p>Average Per-Joint Fitting Error (State (qpos))</p>
<p>Joint 15Per-Joint State (qpos) AnalysisCase 1 Case 1 (Fitted) Case 2 Case 2 (Fitted) Case 3 Case 3 (Fitted) Case 4 Case 4 (Fitted) Case 5 Case 5 (Fitted)
Joint 15Per-Joint State (qpos) AnalysisCase 1 Case 1 (Fitted) Case 2 Case 2 (Fitted) Case 3 Case 3 (Fitted) Case 4 Case 4 (Fitted) Case 5 Case 5 (Fitted)
Joint 15Per-Joint Active Torque AnalysisCase 1 Case 1 (Fitted) Case 2 Case 2 (Fitted) Case 3 Case 3 (Fitted) Case 4 Case 4 (Fitted) Case 5 Case 5 (Fitted)
Joint 15Per-Joint Active Torque AnalysisCase 1 Case 1 (Fitted) Case 2 Case 2 (Fitted) Case 3 Case 3 (Fitted) Case 4 Case 4 (Fitted) Case 5 Case 5 (Fitted)
Case 1 Case 1 (Fitted) Case 2 Case 2 (Fitted) Case 3 Case 3 (Fitted) Case 4 Case 4 (Fitted) Case 5 Case 5 (Fitted)
ACKNOWLEDGMENTSThe authors would like to thank Ziqing Chen, Chi Chu, Chao Chen for valuable feedback on early drafts of the manuscript, and Qianwei Han, Bowen Liu for constructive suggestions on initial versions of the demo video.
Estimation of inertial parameters of rigid body links of manipulators. Chae H References, An, G Christopher, John M Atkeson, Hollerbach, 24th IEEE Conference on Decision and Control. 1985. 1985337</p>
<p>An efficient model-based approach on learning agile motor skills without reinforcement. Tingguang Hao Bin Shi, Qing Li, Jiapeng Zhu, Lei Sheng, Max Q.-H Han, Meng, IEEE International Conference on Robotics and Automation (ICRA). 2024. 2024518</p>
<p>Contactdb: Analyzing and predicting grasp contact via thermal imaging. Samarth Brahmbhatt, Cusuh Ham, Charles C Kemp, James Hays, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019. 2019</p>
<p>Visual dexterity: In-hand reorientation of novel and complex object shapes. Tao Chen, Megha H Tippur, Siyang Wu, Vikash Kumar, Edward H Adelson, Pulkit Agrawal, Science Robotics. 8312022</p>
<p>Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model. Kei Ho, Alexander G Cheng, Schwing, European Conference on Computer Vision. 2022</p>
<p>Open-television: Teleoperation with immersive active visual feedback. Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, Xiaolong Wang, Conference on Robot Learning. 2024</p>
<p>. Craig John, 2009Introduction to robotics: mechanics and control, 3/E. Pearson Education India</p>
<p>Pilco: A model-based and data-efficient approach to policy search. Marc Peter, Deisenroth , Carl Edward Rasmussen, International Conference on Machine Learning. 201137</p>
<p>Bunny-visionpro: Real-time bimanual dexterous teleoperation for imitation learning. Runyu Ding, Yuzhe Qin, Jiyue Zhu, Chengzhe Jia, Shiqi Yang, Ruihan Yang, Xiaojuan Qi, Xiaolong Wang, 202437</p>
<p>Bridging the sim-to-real gap for athletic loco-manipulation. Nolan Fey, G Margolis, Martin Peticco, Pulkit Agrawal, ArXiv, abs/2502.108942025637</p>
<p>Victor Guillemin, Alan Pollack, Differential topology. 201037018</p>
<p>Deep residual learning for image recognition. X Kaiming He, Shaoqing Zhang, Jian Ren, Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. 2015</p>
<p>Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills. Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Kitani, Jessica Hodgins, " Jim" Fan, Yuke Zhu, Changliu Liu, Guanya Shi, ArXiv, abs/2502.011432025637</p>
<p>Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation. Minho Heo, Youngwoon Lee, Doohyun Lee, Joseph J Lim, Robotics: Science and Systems. 2023</p>
<p>Learning agile and dynamic motor skills for legged robots. Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, Marco Hutter, Science Robotics. 4372019</p>
<p>Rma: Rapid motor adaptation for legged robots. Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik, ArXiv, abs/2107.0403420213</p>
<p>Robot model identification and learning: A modern perspective. Taeyoon Lee, Jaewoon Kwon, Patrick M Wensing, Frank C Park, Annu. Rev. Control. Robotics Auton. Syst. 7372023</p>
<p>Deep drone racing: From simulation to reality with domain randomization. Antonio Loquercio, Elia Kaufmann, René Ranftl, Alexey Dosovitskiy, Vladlen Koltun, Davide Scaramuzza, IEEE Transactions on Robotics. 36372019</p>
<p>Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Base parameters of manipulator dynamic models. Hirokazu Mayeda, Koji Yoshida, Koichi Osuka, Proceedings. 1988 IEEE International Conference on Robotics and Automation. 1988 IEEE International Conference on Robotics and Automation1988337</p>
<p>Learning domain randomization distributions for training robust locomotion policies. Melissa Mozifian, Juan Camilo, Gamboa Higuera, David Meger, Gregory Dudek, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020. 201937</p>
<p>James R Munkres, Analysis on manifolds. CRC Press201818</p>
<p>A mathematical introduction to robotic manipulation. Zexiang Richard M Murray, S Shankar Li, Sastry, 2017CRC press19</p>
<p>Neural-fly enables rapid learning for agile flight in strong winds. O' Michael, Guanya Connell, Xichen Shi, Kamyar Shi, Anima Azizzadenesheli, Yisong Anandkumar, Soon-Jo Yue, Chung, Science Robotics. 72022</p>
<p>A convex quasistatic time-stepping scheme for rigid multibody systems with contact and friction. Tao Pang, Russ Tedrake, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021219</p>
<p>Global planning for contact-rich manipulation via local smoothing of quasi-dynamic contact models. Tao Pang, Terry Hj, Lujie Suh, Russ Yang, Tedrake, IEEE Transactions on Robotics. 2023</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, P Abbeel, IEEE International Conference on Robotics and Automation (ICRA). 2018. 201737</p>
<p>Learning time-optimal and speed-adjustable tactile in-hand manipulation. Johannes Pitz, Lennart Röstel, Leon Sievers, Berthold Bauml, IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids). 2024. 2024a</p>
<p>Learning a shape-conditioned agent for purely tactile in-hand manipulation of various objects. Johannes Pitz, Lennart Röstel, Leon Sievers, Darius Burschka, Berthold Bauml, 2024</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2024b</p>
<p>In-hand object rotation via rapid motor adaptation. Haozhi Qi, Ashish Kumar, Roberto Calandra, Yinsong Ma, Jitendra Malik, Conference on Robot Learning. 20221931</p>
<p>General in-hand object rotation with vision and touch. Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Y Ma, Roberto Calandra, Jitendra Malik, ArXiv, abs/2309.09979202330313, 4, 7, 14, 24</p>
<p>Composing dextrous grasping and in-hand manipulation via scoring with a reinforcement learning critic. Lennart Röstel, Dominik Winkelbauer, Johannes Pitz, Leon Sievers, Berthold Bauml, ArXiv, abs/2505.132532025</p>
<p>Real single-image flight without a single real image. Fereshteh Sadeghi, Sergey Levine, ArXiv, abs/1611.04201201637</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, ArXiv, abs/1707.063472017</p>
<p>Leap hand: Low-cost, efficient, and anthropomorphic hand for robot learning. Kenneth Shaw, Ananye Agarwal, Deepak Pathak, ArXiv, abs/2309.06440202335</p>
<p>From sim2real 1.0 to 4.0 for humanoid whole-body control and loco-manipulation. Guanya Shi, 2025</p>
<p>Guanya Shi, Xichen Shi, O' Michael, Rose Connell, Kamyar Yu, Anima Azizzadenesheli, Yisong Anandkumar, Soon-Jo Yue, Chung, Neural lander: Stable drone landing control using learned dynamics. 2019 International Conference on Robotics and Automation (ICRA). 201837</p>
<p>Sim-to-real learning of all common bipedal gaits via periodic reward composition. Jonah Siekmann, Yesh Godse, Alan Fern, Jonathan W Hurst, IEEE International Conference on Robotics and Automation (ICRA). 2021. 202037</p>
<p>Sampling-based system identification with active exploration for legged robot sim2real learning. Nikhil Sobanbabu, Guanqi He, Tairan He, Yuxiang Yang, Guanya Shi, ArXiv, abs/2505.14266202537</p>
<p>Robot modeling and control. Mark W Spong, Seth A Hutchinson, Mathukumalli Vidyasagar, 2005</p>
<p>Robot modeling and control. Seth Mark W Spong, Hutchinson, Vidyasagar, 2020John Wiley &amp;amp19</p>
<p>Dexterous contact-rich manipulation via the contact trust region. H J Terry, Tao Suh, Tong Pang, Russ Zhao, Tedrake, ArXiv, abs/2505.022912025</p>
<p>Grab: A dataset of wholebody human grasping of objects. Omid Taheri, Nima Ghorbani, Michael J Black, Dimitrios Tzionas, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 202030Proceedings, Part IV 16</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, Vincent Vanhoucke, ArXiv, abs/1804.10332201837</p>
<p>Drake: Model-based design and verification for robotics. Russ Tedrake, Drake Development Team, 2019</p>
<p>Lessons from learning to spin" pens. Jun Wang, Ying Yuan, Haichuan Che, Haozhi Qi, Yi Ma, Jitendra Malik, Xiaolong Wang, arXiv:2407.18902202424arXiv preprint</p>
<p>Foundationpose: Unified 6d pose estimation and tracking of novel objects. Bowen Wen, Wei Yang, Jan Kautz, Stanley T Birchfield, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024. 2023</p>
<p>Anyrotate: Gravity-invariant in-hand object rotation with sim-to-real touch. Max Yang, Chenghua Lu, Alex Church, Yijiong Lin, Christopher J Ford, Haoran Li, Efi Psomopoulou, A W David, Nathan F Barton, Lepora, Conference on Robot Learning. 2024313, 6, 7, 24</p>
<p>Sim-to-real transfer for biped locomotion. Wenhao Yu, C V Visak, Greg Kumar, C Karen Turk, Liu, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2019. 201937</p>
<p>Robot synesthesia: In-hand manipulation with visuotactile sensing. Ying Yuan, Haichuan Che, Yuzhe Qin, Binghao Huang, Kang-Won Zhao-Heng Yin, Yi Lee, Soo-Chul Wu, Xiaolong Lim, Wang, IEEE International Conference on Robotics and Automation (ICRA). 2024. 20233</p>
<p>Dexctrl: Towards sim-to-real dexterity with adaptive controller learning. Shuqi Zhao, Ke Yang, Yuxin Chen, Chenran Li, Yichen Xie, Xiang Zhang, Changhao Wang, Masayoshi Tomizuka, ArXiv, abs/2505.0099120253</p>
<p>We attempted to train the sim-to-real baselines (ASAP and UAN) using these task-relevant, object-state-annotated data, but even the first stage-compensator training-failed to converge, and rewards showed no meaningful improvement, likely due to poor data quality. C ADDITIONAL EXPERIMENTAL DETAILS Object Set Normal-Sized Cylinders Normal-Sized Cuboids Long Cuboids Small Cylinders DexEnv Objects ContactDB Objects. Test Set</p>
<p>Information and Physical Parameter Randomization Ranges of Training Object Sets and the Test Object Set. Datasets. Our training objects comprise the following subsets: 1) Normal-sized cylinders from Hora (Qi et al., 2022); 2) Normal-sized cuboids from Hora. Qi, 20229</p>
<p>DexEnv Objects"). Details with scale randomization ranges are summarized in Table 9. To test the generalization performance in unseen shapes. Chen , 20224Small-sized cylinders; and 5) Normal-sized complex shapes from Visual Dexterity. we filter objects with an aspect</p>            </div>
        </div>

    </div>
</body>
</html>
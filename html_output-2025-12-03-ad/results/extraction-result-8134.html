<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8134 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8134</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8134</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-278739707</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12268v1.pdf" target="_blank">K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Understanding which neural components drive specific capabilities in mid-sized language models ( ≤ 10B parameters) remains a key challenge. We introduce the ( K , ϵ ) -Minimum Sufficient Head Circuit ( K -MSHC), a methodology to identify minimal sets of attention heads crucial for classification tasks as well as Search-K-MSHC, an efficient algorithm for discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B, we analyze three syntactic task families: grammar acceptability, arithmetic verification, and arithmetic word problems. Our findings reveal distinct task-specific head circuits, with grammar tasks predominantly utilizing early layers, word problems showing pronounced activity in both shallow and deep regions, and arithmetic verification demonstrating a more distributed pattern across the network. We discover non-linear circuit overlap patterns, where different task pairs share computational components at varying levels of importance. While grammar and arithmetic share many "weak" heads, arithmetic and word problems share more consistently critical "strong" heads. Importantly, we find that each task maintains dedicated "super-heads" with minimal cross-task overlap, suggesting that syntactic and numerical competencies emerge from specialized yet partially reusable head circuits.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8134.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8134.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-9B (arithmetic analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analysis of arithmetic verification and word-problem circuits in Gemma-9B using K-MSHC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study showing arithmetic (verification and word problems) is supported by distributed, partially redundant attention-head circuits in Gemma-9B; arithmetic verification is more distributed across layers than grammar and exhibits large sensitivity to layer ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mid-sized transformer language model (~9B parameters) with multi-head self-attention; experiments analyze attention heads across layers (layers 0–20 analyzed in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic verification (multi-digit + and - with operands n1,n2 ∈ [1,10^3], perturbed incorrect answers by ~0.5×/1.5×) and natural-language arithmetic word problems using the same numeric generation scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Arithmetic behavior is supported by distributed attention-head circuits (a wider basis of heads across many layers) rather than a single localized chain; final-layer EOS embeddings encode task separability that can be captured in a low-dimensional PCA subspace.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Search-K-MSHC (stochastic head-circuit discovery via windowed layer ablation and stochastic head pruning), window-based layer ablation, head removal experiments, and Low-Dimensional Linear Separability (PCA projection of EOS token + linear SVM probe).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline LS (Low-Dimensional Linear Separability) accuracy for arithmetic verification: 0.99 [0.98, 1.00]; after ablating 25% of critical layers LS = 0.55 [0.52, 0.60] (44% relative drop). Word problems baseline LS = 0.77 [0.73, 0.86]; post-ablation LS = 0.56 [0.53, 0.61] (21% drop).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>High sensitivity to layer/ head ablation for arithmetic (large drop), multiple functionally equivalent redundant circuits across trials (variation in identified heads), distributed weak contributors that complicate exact circuit identification, and potential sensitivity to choice of K and ϵ for circuit discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation results (large LS drop when critical layers/heads removed), Search-K-MSHC selection heatmaps showing distributed head importance for arithmetic verification, circuit-overlap analysis showing different sharing profiles, and the ability to restore performance using small K-subsets from identified circuits (K-MSHC concept).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Despite arithmetic and word problems both involving numeric reasoning, their strongest ('super-head') circuits are largely distinct (minimal overlap at high selection thresholds), challenging hypotheses of a single unified arithmetic circuit; also multiple redundant circuits exist causing variation across trials and complicating mechanistic claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8134.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8134.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search-K-MSHC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search-K-MSHC (stochastic algorithm for (K,ϵ)-Minimum Sufficient Head Circuit discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase stochastic search algorithm (macro layer window ablation + micro stochastic head pruning/binary search) to efficiently approximate minimal head sets H such that any K-subset of H suffices to restore task performance above an ϵ threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to Gemma-9B (mid-sized transformer, ~9B parameters) in this paper to discover task-specific head circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to arithmetic verification and word-problem tasks (as defined in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Not a mechanism of arithmetic per se but an algorithmic intervention for discovering attention-head circuits; reveals that arithmetic relies on a distributed set of heads across many layers.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>The method itself is the intervention/probing approach: window-based layer ablation to find candidate layers, then stochastic sampling of K-subsets and iterative pruning to find minimally sufficient head sets; coupled with LS as the scoring metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Algorithmic hyperparameters used in experiments: W=5, p=0.75, N=10, K=10, ϵ=0.25; applied across 20 trials with mini-batches of 50 examples per task. The algorithm recovered circuits whose removal restored/dropped LS by amounts reported in the paper (e.g., arithmetic ablation drop 44%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Theoretical guarantees depend on assumptions about contamination rates of low-impact heads; sensitive to hyperparameters (K, ϵ, N) and to redundancy in the model (multiple equivalent circuits produce variability across trials).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>The algorithm's outputs (heatmaps, circuit overlaps, and ablation-induced LS drops) provide causal evidence linking identified heads to arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Exact minimal circuits may be hard to pin down due to functional redundancy and variable circuits across trials; guarantees rely on simplifying assumptions that may not hold universally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8134.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8134.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LS metric (Low-Dimensional Linear Separability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Dimensional Linear Separability (LS) metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probing metric that projects final-layer EOS embeddings into a low-dimensional PCA subspace (D ≤ 5) and evaluates linear SVM classification accuracy to detect task-specific information while avoiding high-dimensional probe overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to final-layer EOS embeddings of Gemma-9B; PCA computed on task dataset covariance then linear SVM (hinge loss, C=10) trained in the projected space.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to evaluate separability for arithmetic verification and word-problem tasks (and grammar baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Operationalizes the representational hypothesis that task-relevant computations are encoded in a small number of principal directions of the final-layer EOS embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Dimensionality reduction (PCA) on EOS token embeddings followed by a linear SVM probe; LS score = probe accuracy on balanced yes/no task datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported baseline LS accuracies: Arithmetic 0.99 [0.98,1.00]; Grammar 0.86 [0.78,0.90]; Word Problems 0.77 [0.73,0.86].</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Probe dimensionality must be kept low (D ≤ 5) to avoid capturing spurious high-dimensional separability; metric can detect when heads/layers encode task information but does not itself reveal algorithmic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High baseline LS for arithmetic indicates that final-layer embeddings encode arithmetic separability strongly; large LS drops under layer ablation provide causal evidence that removed components encoded that separability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>LS indicates presence of separable information but cannot by itself identify exact computational steps (e.g., digit-wise addition vs. Fourier-like encoding) or disentangle memorized examples from algorithmic competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8134.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8134.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier-features addition (Zhou et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llms use fourier features to compute addition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work attributing addition computation in LLMs to Fourier-like features in representations (i.e., models encoding numbers into a Fourier basis that supports arithmetic operations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llms use fourier features to compute addition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Fourier-like basis representations for numbers (Fourier features) that enable arithmetic operations.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited in this paper's related work as attributing addition to Fourier-like transformations (supporting a distributed basis explanation for arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8134.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8134.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trigonometric-addition hypothesis (Kantamneni & Tegmark, 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models use trigonometry to do addition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior claim that LLMs implement addition via trigonometric (sine/cosine) transformations of token representations, analogous to Fourier/trig basis encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models use trigonometry to do addition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Trigonometric (sine/cosine) transformations / basis for representing numbers that facilitate addition.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Mentioned in related work as an explanation for learned addition operations (aligns with the paper's observation of distributed arithmetic computations).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8134.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8134.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bag-of-heuristics (Nikankin et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arithmetic without algorithms: Language models solve math with a bag of heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work arguing that LLMs often rely on ensembles of heuristics rather than exact algorithmic procedures to solve arithmetic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Arithmetic without algorithms: Language models solve math with a bag of heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Heuristic ensemble / pattern-matching strategies rather than precise algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited as alternative explanatory class for arithmetic behavior in LLMs; the present paper's finding of many weak, distributed heads is consistent with heuristic/ensemble explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8134.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8134.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal mediation for arithmetic (Stolfo et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior causal-analysis work applying causal mediation techniques to trace arithmetic reasoning components within language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Causal mediation analysis of internal components to attribute contributions to arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Causal mediation analysis (as described in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited in related work as demonstrating component-level causal analysis of arithmetic behavior; supports the paper's use of intervention-style ablations for causal claims.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llms use fourier features to compute addition <em>(Rating: 2)</em></li>
                <li>Language models use trigonometry to do addition <em>(Rating: 2)</em></li>
                <li>Arithmetic without algorithms: Language models solve math with a bag of heuristics <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8134",
    "paper_id": "paper-278739707",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Gemma-9B (arithmetic analysis)",
            "name_full": "Analysis of arithmetic verification and word-problem circuits in Gemma-9B using K-MSHC",
            "brief_description": "Empirical study showing arithmetic (verification and word problems) is supported by distributed, partially redundant attention-head circuits in Gemma-9B; arithmetic verification is more distributed across layers than grammar and exhibits large sensitivity to layer ablation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-9B",
            "model_description": "A mid-sized transformer language model (~9B parameters) with multi-head self-attention; experiments analyze attention heads across layers (layers 0–20 analyzed in figures).",
            "arithmetic_task_type": "Arithmetic verification (multi-digit + and - with operands n1,n2 ∈ [1,10^3], perturbed incorrect answers by ~0.5×/1.5×) and natural-language arithmetic word problems using the same numeric generation scheme.",
            "mechanism_or_representation": "Arithmetic behavior is supported by distributed attention-head circuits (a wider basis of heads across many layers) rather than a single localized chain; final-layer EOS embeddings encode task separability that can be captured in a low-dimensional PCA subspace.",
            "probing_or_intervention_method": "Search-K-MSHC (stochastic head-circuit discovery via windowed layer ablation and stochastic head pruning), window-based layer ablation, head removal experiments, and Low-Dimensional Linear Separability (PCA projection of EOS token + linear SVM probe).",
            "performance_metrics": "Baseline LS (Low-Dimensional Linear Separability) accuracy for arithmetic verification: 0.99 [0.98, 1.00]; after ablating 25% of critical layers LS = 0.55 [0.52, 0.60] (44% relative drop). Word problems baseline LS = 0.77 [0.73, 0.86]; post-ablation LS = 0.56 [0.53, 0.61] (21% drop).",
            "error_types_or_failure_modes": "High sensitivity to layer/ head ablation for arithmetic (large drop), multiple functionally equivalent redundant circuits across trials (variation in identified heads), distributed weak contributors that complicate exact circuit identification, and potential sensitivity to choice of K and ϵ for circuit discovery.",
            "evidence_for_mechanism": "Ablation results (large LS drop when critical layers/heads removed), Search-K-MSHC selection heatmaps showing distributed head importance for arithmetic verification, circuit-overlap analysis showing different sharing profiles, and the ability to restore performance using small K-subsets from identified circuits (K-MSHC concept).",
            "counterexamples_or_challenges": "Despite arithmetic and word problems both involving numeric reasoning, their strongest ('super-head') circuits are largely distinct (minimal overlap at high selection thresholds), challenging hypotheses of a single unified arithmetic circuit; also multiple redundant circuits exist causing variation across trials and complicating mechanistic claims.",
            "uuid": "e8134.0",
            "source_info": {
                "paper_title": "K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Search-K-MSHC",
            "name_full": "Search-K-MSHC (stochastic algorithm for (K,ϵ)-Minimum Sufficient Head Circuit discovery)",
            "brief_description": "A two-phase stochastic search algorithm (macro layer window ablation + micro stochastic head pruning/binary search) to efficiently approximate minimal head sets H such that any K-subset of H suffices to restore task performance above an ϵ threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-9B",
            "model_description": "Applied to Gemma-9B (mid-sized transformer, ~9B parameters) in this paper to discover task-specific head circuits.",
            "arithmetic_task_type": "Applied to arithmetic verification and word-problem tasks (as defined in this paper).",
            "mechanism_or_representation": "Not a mechanism of arithmetic per se but an algorithmic intervention for discovering attention-head circuits; reveals that arithmetic relies on a distributed set of heads across many layers.",
            "probing_or_intervention_method": "The method itself is the intervention/probing approach: window-based layer ablation to find candidate layers, then stochastic sampling of K-subsets and iterative pruning to find minimally sufficient head sets; coupled with LS as the scoring metric.",
            "performance_metrics": "Algorithmic hyperparameters used in experiments: W=5, p=0.75, N=10, K=10, ϵ=0.25; applied across 20 trials with mini-batches of 50 examples per task. The algorithm recovered circuits whose removal restored/dropped LS by amounts reported in the paper (e.g., arithmetic ablation drop 44%).",
            "error_types_or_failure_modes": "Theoretical guarantees depend on assumptions about contamination rates of low-impact heads; sensitive to hyperparameters (K, ϵ, N) and to redundancy in the model (multiple equivalent circuits produce variability across trials).",
            "evidence_for_mechanism": "The algorithm's outputs (heatmaps, circuit overlaps, and ablation-induced LS drops) provide causal evidence linking identified heads to arithmetic performance.",
            "counterexamples_or_challenges": "Exact minimal circuits may be hard to pin down due to functional redundancy and variable circuits across trials; guarantees rely on simplifying assumptions that may not hold universally.",
            "uuid": "e8134.1",
            "source_info": {
                "paper_title": "K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LS metric (Low-Dimensional Linear Separability)",
            "name_full": "Low-Dimensional Linear Separability (LS) metric",
            "brief_description": "A probing metric that projects final-layer EOS embeddings into a low-dimensional PCA subspace (D ≤ 5) and evaluates linear SVM classification accuracy to detect task-specific information while avoiding high-dimensional probe overfitting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-9B",
            "model_description": "Applied to final-layer EOS embeddings of Gemma-9B; PCA computed on task dataset covariance then linear SVM (hinge loss, C=10) trained in the projected space.",
            "arithmetic_task_type": "Used to evaluate separability for arithmetic verification and word-problem tasks (and grammar baseline).",
            "mechanism_or_representation": "Operationalizes the representational hypothesis that task-relevant computations are encoded in a small number of principal directions of the final-layer EOS embedding.",
            "probing_or_intervention_method": "Dimensionality reduction (PCA) on EOS token embeddings followed by a linear SVM probe; LS score = probe accuracy on balanced yes/no task datasets.",
            "performance_metrics": "Reported baseline LS accuracies: Arithmetic 0.99 [0.98,1.00]; Grammar 0.86 [0.78,0.90]; Word Problems 0.77 [0.73,0.86].",
            "error_types_or_failure_modes": "Probe dimensionality must be kept low (D ≤ 5) to avoid capturing spurious high-dimensional separability; metric can detect when heads/layers encode task information but does not itself reveal algorithmic steps.",
            "evidence_for_mechanism": "High baseline LS for arithmetic indicates that final-layer embeddings encode arithmetic separability strongly; large LS drops under layer ablation provide causal evidence that removed components encoded that separability.",
            "counterexamples_or_challenges": "LS indicates presence of separable information but cannot by itself identify exact computational steps (e.g., digit-wise addition vs. Fourier-like encoding) or disentangle memorized examples from algorithmic competence.",
            "uuid": "e8134.2",
            "source_info": {
                "paper_title": "K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Fourier-features addition (Zhou et al., 2024)",
            "name_full": "Llms use fourier features to compute addition",
            "brief_description": "Prior work attributing addition computation in LLMs to Fourier-like features in representations (i.e., models encoding numbers into a Fourier basis that supports arithmetic operations).",
            "citation_title": "Llms use fourier features to compute addition",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Fourier-like basis representations for numbers (Fourier features) that enable arithmetic operations.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Cited in this paper's related work as attributing addition to Fourier-like transformations (supporting a distributed basis explanation for arithmetic).",
            "counterexamples_or_challenges": null,
            "uuid": "e8134.3",
            "source_info": {
                "paper_title": "K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Trigonometric-addition hypothesis (Kantamneni & Tegmark, 2025)",
            "name_full": "Language models use trigonometry to do addition",
            "brief_description": "Prior claim that LLMs implement addition via trigonometric (sine/cosine) transformations of token representations, analogous to Fourier/trig basis encodings.",
            "citation_title": "Language models use trigonometry to do addition",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Trigonometric (sine/cosine) transformations / basis for representing numbers that facilitate addition.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Mentioned in related work as an explanation for learned addition operations (aligns with the paper's observation of distributed arithmetic computations).",
            "counterexamples_or_challenges": null,
            "uuid": "e8134.4",
            "source_info": {
                "paper_title": "K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Bag-of-heuristics (Nikankin et al., 2024)",
            "name_full": "Arithmetic without algorithms: Language models solve math with a bag of heuristics",
            "brief_description": "Prior work arguing that LLMs often rely on ensembles of heuristics rather than exact algorithmic procedures to solve arithmetic problems.",
            "citation_title": "Arithmetic without algorithms: Language models solve math with a bag of heuristics",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Heuristic ensemble / pattern-matching strategies rather than precise algorithmic computation.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Cited as alternative explanatory class for arithmetic behavior in LLMs; the present paper's finding of many weak, distributed heads is consistent with heuristic/ensemble explanations.",
            "counterexamples_or_challenges": null,
            "uuid": "e8134.5",
            "source_info": {
                "paper_title": "K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Causal mediation for arithmetic (Stolfo et al., 2023)",
            "name_full": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "brief_description": "Prior causal-analysis work applying causal mediation techniques to trace arithmetic reasoning components within language models.",
            "citation_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Causal mediation analysis of internal components to attribute contributions to arithmetic reasoning.",
            "probing_or_intervention_method": "Causal mediation analysis (as described in the cited work).",
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Cited in related work as demonstrating component-level causal analysis of arithmetic behavior; supports the paper's use of intervention-style ablations for causal claims.",
            "counterexamples_or_challenges": null,
            "uuid": "e8134.6",
            "source_info": {
                "paper_title": "K -MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llms use fourier features to compute addition",
            "rating": 2,
            "sanitized_title": "llms_use_fourier_features_to_compute_addition"
        },
        {
            "paper_title": "Language models use trigonometry to do addition",
            "rating": 2,
            "sanitized_title": "language_models_use_trigonometry_to_do_addition"
        },
        {
            "paper_title": "Arithmetic without algorithms: Language models solve math with a bag of heuristics",
            "rating": 2,
            "sanitized_title": "arithmetic_without_algorithms_language_models_solve_math_with_a_bag_of_heuristics"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        }
    ],
    "cost": 0.013330749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>K-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks
5 Jun 2025</p>
<p>Pratim Chowdhary cpratim.25@dartmouth.edu 
Department of Computer Science Dartmouth College</p>
<p>Peter Chin 
Department of Engineering
Thayer School of Engineering</p>
<p>Deepernab Chakrabarty deepernab@dartmouth.edu 
Department of Computer Science Dartmouth College</p>
<p>K-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks
5 Jun 2025A324CBE059ED0A91EF1138E82B1DC734arXiv:2505.12268v2[cs.CL]
Understanding which neural components drive specific capabilities in mid-sized language models (≤10B parameters) remains a key challenge.We introduce the (K, ϵ)-Minimum Sufficient Head Circuit (K-MSHC), a methodology to identify minimal sets of attention heads crucial for classification tasks as well as Search-K-MSHC, an efficient algorithm for discovering these circuits.Applying our Search-K-MSHC algorithm to Gemma-9B, we analyze three syntactic task families: grammar acceptability, arithmetic verification, and arithmetic word problems.Our findings reveal distinct task-specific head circuits, with grammar tasks predominantly utilizing early layers, word problems showing pronounced activity in both shallow and deep regions, and arithmetic verification demonstrating a more distributed pattern across the network.We discover non-linear circuit overlap patterns, where different task pairs share computational components at varying levels of importance.While grammar and arithmetic share many "weak" heads, arithmetic and word problems share more consistently critical "strong" heads.Importantly, we find that each task maintains dedicated "super-heads" with minimal cross-task overlap, suggesting that syntactic and numerical competencies emerge from specialized yet partially reusable head circuits.This question has profound implications for how we understand, improve, and control these models.If grammatical analysis and arithmetic reasoning utilize the same circuits, then improvements in one capability might automatically enhance the other.Conversely, if each task relies on distinct components, we gain the ability to target interventions precisely-enhancing specific abilities without disrupting others.Yet despite the practical importance of this organizational question, we lack efficient tools to provide definitive answers[Adolfi et al., 2025].Preprint.Under review.</p>
<p>Introduction</p>
<p>How do language models organize their capabilities?As mid-sized language models (≤10B parameters) master increasingly diverse tasks-from solving arithmetic problems to identifying ungrammatical sentences-a fundamental question emerges: do related capabilities share the same neural circuitry, or does each task develop its own specialized pathway?</p>
<p>To address this, we introduce the (K, ϵ)-Minimum Sufficient Head Circuit (K-MSHC) framework, an efficient approach for isolating sparse subsets of attention heads that minimally, sufficiently explain task performance.This lets us probe the compositional structure of model capabilities in a targeted way.We formalize a novel attention head pruning algorithm which results in approximate circuits that are sufficient and minimal with theoretical guarantees.</p>
<p>Current interpretability research has revealed that individual attention heads specialize in specific functions [Voita et al., 2019, Clark et al., 2019] and that disabling certain heads can impact performance on specific tasks [Michel et al., 2019].But these findings don't conclusively answer whether related capabilities emerge from shared or separate neural pathways.The missing piece is a principled approach for identifying the minimal set of model components necessary and sufficient for specific tasks-and importantly, for measuring how these sets overlap across different capabilities.</p>
<p>We build on these results and investigate two further questions:</p>
<ol>
<li>Do linguistic and numerical reasoning tasks recruit shared pathways or task-specific circuits?2. What patterns of overlap emerge between circuits for related tasks, and what do these patterns reveal about the organization of knowledge within the model?</li>
</ol>
<p>Our experiments reveal a clear organizational pattern in Gemma-9B.Grammar tasks predominantly utilize early layers, word problems engage both shallow and deep network regions, and arithmetic verification employs a distributed mechanism across the network.We observe that while grammatical and arithmetic tasks share many weakly contributing computational components, they maintain dedicated "super-heads" with minimal cross-task overlap.This finding indicates that the model develops specialized circuits for different capabilities while efficiently reusing resources where possible.</p>
<p>These results provide evidence for a nuanced view of LLM organization.Rather than implementing either fully general or fully specialized mechanisms, compact models develop task-specific yet partially reusable circuits.This architectural characteristic enables diverse capabilities within parameter constraints and suggests that similar principles may apply to larger frontier models, where understanding organizational structure becomes increasingly important for alignment and control.</p>
<p>In sum, our contributions are three-fold:</p>
<p>(i) We introduce the (K, ϵ)-Minimum Sufficient Head Circuit (K-MSHC) framework, a highly efficient approach to identify minimal sets of attention heads crucial for specific tasks, measuring their minimality, sufficiency, and necessity.(ii) We develop Search-K-MSHC, an efficient stochastic search algorithm that makes circuit discovery feasible in large language models with thousands of attention heads, complemented by our Low-Dimensional Linear Separability (LS) metric that addresses dimensionality challenges when probing representations.(iii) We present a comprehensive analysis of syntactic versus arithmetic circuits in Gemma-9B, revealing both task-specialized components and shared "super-heads" that demonstrate how mid-sized LLMs efficiently encode multiple capabilities through overlapping head circuits.</p>
<p>Related Work</p>
<p>Evaluating and Probing Language Models.Specialized benchmarks have emerged to track the capabilities of sub-10B parameter models.BLiMP [Warstadt et al., 2020] provides fine-grained assessments of syntactic competence, while GSM8K [Cobbe et al., 2021] challenges models with grade-school arithmetic problems.Recent surveys [Zhao et al., 2023] document how these capabilities exhibit non-uniform scaling properties across parameter thresholds.Linear probing methods have become standard tools for analyzing information encoded in neural representations [Alain and Bengio, 2016].Extensions such as control tasks [Hewitt and Manning, 2019] and diagnostic classifiers [Tenney et al., 2019] help distinguish linguistic structure from memorization artifacts.Our Low-Dimensional Linear Separability (LS) metric builds on this tradition but introduces a crucial innovation by projecting to a minimal subspace via PCA before applying a linear classifier.</p>
<p>Attention Mechanisms and Mechanistic Interpretability.Previous work has studied attention pattern interpretability [Clark et al., 2019] and demonstrated that many heads can be pruned without significant performance degradation [Michel et al., 2019].Task-specialized heads have been identified through activation analysis [Voita et al., 2019].K-MSHC extends these insights by formalizing the concept of minimal sufficient circuits-the smallest set of attention heads where any K-subset restores task performance.The emerging field of mechanistic interpretability seeks to reverse-engineer neural networks at the component level.Circuit-level analyses have mapped syntactic processing [Clark et al., 2019] and key-value memories [Geva et al., 2021].Work by Olah et al. [Olah et al., 2020] and Elhage et al. [Elhage et al., 2021] suggests that capabilities emerge from sparse subnetworks that can be isolated through careful intervention studies.Our K-MSHC framework operationalizes this insight in mid-sized models like Gemma-9B.</p>
<p>Computational Component Discovery.Sparse autoencoders and dictionary-learning techniques have recently been leveraged to extract highly interpretable, near-monosemantic features from the activations of large language models [Cunningham et al., 2023, Bricken et al., 2023, Gao et al., 2024, Templeton et al., 2024].These methods complement circuit-level analyses by working at the representational level and provide an alternative path toward isolating task-relevant computational units.A parallel line of research proposes algorithms that automatically identify local and global circuits using linear computation graphs, cross-layer mappings, or feature editing [Marks et al., 2024, Ge and Hoefler, 2024, Lindsey et al., 2024, Dunefsky et al., 2025].Our Search-K-MSHC algorithm differs from these approaches by explicitly enforcing K-sufficiency, yielding minimal sets that are both necessary and redundantly sufficient for a given task.</p>
<p>Task-Specific Head Circuits.Mechanistic analyses have been extended beyond single-step tasks to multi-hop reasoning in language models [Yang et al., 2024, Biran et al., 2024, Yu et al., 2025] and emergent planning behaviour in specialised agents [Jenner et al., 2025, Taufeeque et al., 2024, Bush et al., 2024].Our results on arithmetic word problems echo these findings, showing that high-level reasoning engages heads across distant layers that nonetheless admit mid-sized sufficient subsets.</p>
<p>Recent work traces how language models carry out symbolic or approximate arithmetic, attributing addition to Fourier-like or trigonometric transformations and heuristic ensembles [Stolfo et al., 2023, Zhou et al., 2024, Nikankin et al., 2024, Kantamneni and Tegmark, 2025].The broadly distributed pattern we observe for arithmetic verification aligns with these results, suggesting that numerical operations recruit a wider basis of heads than purely grammatical processing.Studies of multilingual models reveal that latent grammatical concepts are encoded in shared subspaces across languages, with task-specific specialisations layered on top [Brinkmann et al., 2025, Dumas et al., 2024, Zhang et al., 2024].The partial head overlap we observe between grammar and arithmetic tasks may reflect the same "semantic hub" principle observed in these cross-lingual analyses.</p>
<p>Methodology</p>
<p>We formalize the problem of identifying minimal head circuits responsible for specific model capabilities.Our (K, ϵ)-Minimum Sufficient Head Circuit framework quantifies the causal contribution of attention heads to task performance.</p>
<p>The (K, ϵ)-Minimum Sufficient Head Circuit</p>
<p>To identify where task-specific knowledge resides in a model, we adopt a causal intervention approach: if removing specific components disrupts the ability to distinguish correct from incorrect examples, those components likely encode the relevant knowledge.Definition 1 (Task Separability Score).For a task T with dataset
D T = {(x i , y i )} n i=1 where y i ∈ {−1, 1}, the separability score of a model configuration M is: S D T (M) = 1 n n i=1 I f θ (h EOS xi,L ) = y i (1)
where M specifies which attention heads are active, h EOS xi,L is the final-layer embedding representation, and f θ : R d → [0, 1] is an optimal classifier over that embedding space.</p>
<p>We hypothesize that each task relies on a minimal circuit of attention heads that maintains these distinctions.By comparing separability across head subsets, we can identify these circuits and analyze how language models allocate resources between capabilities.</p>
<p>Definition 2 ((K, ϵ)-Minimum Sufficient Head Circuit).The (K, ϵ)-Minimum Sufficient Head Circuit identifies the smallest set of attention heads H ⊂ M such that any subset of K heads from H can restore the model's classification performance to over some ϵ performance threshold.</p>
<p>Given a baseline model B (typically a subset of heads in M) and a parameter ϵ ∈ [0, 1], we define the understanding threshold:
U ϵ (M, B) = S D T (B) + ϵ • (S D T (M) − S D T (B))
(2)</p>
<p>Definition 3 ((K, ϵ)-Minimum Sufficient Head Circuit).The (K, ϵ)-Minimum Sufficient Head Circuit is the smallest set of heads H ⊂ M satisfying:
∀H ′ ⊆ H with |H ′ | = K : S D T ((M \ H) ∪ H ′ ) ≥ U ϵ (M, B)(3)
This definition has three key properties: Property 1 (Key Properties of (K, ϵ)-MSHC).</p>
<p>(i) Minimality: H is the smallest set satisfying the condition (ii) K-Sufficiency: Any K-subset of H can restore performance (iii) Isolated Insignificance: M \ H alone contributes minimally to task understanding</p>
<p>The parameter K controls redundancy in the circuit, while ϵ determines how close to full performance the circuit must restore.</p>
<p>Search-K-MSHC: An Efficient Algorithm for Circuit Discovery</p>
<p>Finding the exact (K, ϵ)-Minimum Sufficient Head Circuit is likely computationally intractable, requiring examination on the order of 2 |M| possible head subsets.We introduce Search-K-MSHC, a stochastic algorithm with parameters W (window size), p (percentile), and N (samples) that efficiently approximates the solution through two phases:</p>
<p>(i) Macro Layer Search: Identifies critical layers by window-based ablation.</p>
<p>(ii) Micro Head Search: Refines head candidates via binary search and stochastic pruning.</p>
<p>Macro Layer Search.We identify task-critical layers via:
Θ min = arg min i S D T (B ∪ Θ i ) (iii) If S D T (B ∪ Θ min ) &lt; U ϵ (M, B), prune C ← C \ Θ min ; else reduce k ← max(K, ⌊k/2⌋)(
iv) Terminate when k = K and threshold met 3.3 Theoretical Analysis of Search-K-MSHC Definition 4. Let C i be the candidate set at iteration i, and let τ = U ϵ (M, B) be our performance threshold.For parameters 0 ≤ δ i , δ T ≤ 1, we define:
(i) Low-impact heads: Set Q ⊆ C i with |Q| = δ i |C i | s.t. ∀X ⊆ C i with |X | = K and |X ∩ Q| &gt; δ T K: S D T ((M\C i )∪X ) ≤ τ
, or heads that don't meaningfully contribute to "understanding"</p>
<p>(ii) Prunable sets:
P i = {X ⊆ C i : |X | = K, |X ∩ Q| &gt; δ T K}
, the K-subsets containing too many low-impact heads that we would like to prune.</p>
<p>These assumptions are made to simplify the analysis but likely emulate actual head importance patterns, with a number of unimportant heads over some threshold likely leading to prunable sets.</p>
<p>Theorem 1 (Expected Missed Prunable Sets).For a candidate set C i with a contamination rate δ i of low-impact heads and threshold parameter δ T &lt; δ i , the expected number of prunable sets that remain undetected after sampling N random K-subsets is:
E[Undetected Sets] = O δ i |C i | δ T K • exp −N K(δ i − δ T ) 2 (4)
Proof.For K randomly sampled heads, the probability of missing a prunable set follows a hypergeometric distribution H ∼ Hypergeometric(|C|, |Q|, K).By Hoeffding's inequality:
Pr[H ≤ δ T • K] ≤ exp −2K • (δ i − δ T ) 2(5)
With N independent samples, the miss probability becomes:
Pr[All N Samples Miss] ≤ exp −2N K • (δ i − δ T ) 2(6)
Since we can only catch non-overlapping prunable sets, the maximal number of undetected prunable sets is bounded by |Q|/(δ T • K + 1).By linearity of expectation:
E[Undetected Sets] ≤ |Q| δ T • K + 1 • exp −2N K • (δ i − δ T ) 2 (7) = O δ i |C i | δ T K • exp −N K(δ i − δ T ) 2 (since |Q| = δ i |C i |)
The key takeaways from this result are two-fold:</p>
<p>(i) The expected number of missed prunable sets decreases exponentially with N (samples), K (subset size), and ∆ 2 i = (δ i − δ T ) 2 (squared margin).(ii) Critically, when the low-impact ratio δ i is much higher than the threshold δ T , the algorithm is extremely unlikely to terminate with significant numbers of prunable sets remaining.Standard linear probes for analyzing LLM representations often overfit due to high dimensionality (d ≈ 4096) and limited training data.To address this challenge, we introduce Low-Dimensional Linear Separability (LS), focused on the final layer's EOS token representations as our scoring metric to guide the search for minimal sufficient head circuits.It operates in two phases:</p>
<p>Dimensionality Reduction.We project to a subspace preserving maximal variance by computing the empirical covariance matrix
Σ L = 1 |D T | x h EOS x,L − hL h EOS x,L − hL ⊤
, extracting its top D eigenvectors W L , and projecting:
hx,L = W ⊤ L (h EOS x,L − hL ) ∈ R D (8)
Classification.We train a linear SVM by optimizing the regularized hinge loss:
min w,b 1 2 ∥w∥ 2 2 + C |D T | i=1 max 0, 1 − y i (w ⊤ hxi,L + b) , C = 10 (9)
and calculate LS score as classification accuracy:
LS D D T = 1 |D T | |D T | i=1 I[sign(w ⊤ hxi,L + b) = y i ]
By restricting to D ≤ 5 dimensions, we ensure the metric captures task-relevant information rather than dimensionality artifacts.This approach efficiently detects when attention heads encode taskspecific knowledge in their representations.</p>
<p>Evaluation Framework: Task Families and Dataset Construction</p>
<p>We evaluate K-MSHC using three task families with controlled minimal pairs, where examples differ only in a single task-relevant feature.All tasks have balanced classes and use a consistent yes/no formulation.</p>
<p>Grammar Acceptability (G).Based on BLiMP [Warstadt et al., 2020] (67,000 sentence pairs), focusing on determiner-noun agreement due to the inherent numerocity of the task:</p>
<p>Correct: Leslie isn't firing that actress.</p>
<p>Incorrect: Leslie isn't firing that actresses.</p>
<p>Arithmetic Verification (A).1000 algorithmically generated equation pairs with random operands n 1 , n 2 ∈ [1, 10 3 ], operations ∈ {+, −}, and perturbed answers (0.5 / 1.5×) for incorrect equations:</p>
<p>Correct: 1338 + 88 = 1426 Incorrect: 1338 + 88 = 2139 (≈ 1.5 × correct result)</p>
<p>Word Problems (W). 100 natural language arithmetic problems templates filled with random numbers using the same methodology as (A), with (0.5 / 1.5×) perturbed answers for incorrect equations:</p>
<p>Correct: Tim has 5 apples and eats 2, leaving him with 3 apples.Incorrect: Tim has 5 apples and eats 2, leaving him with 5 apples.(≈ 1.5 × correct result)</p>
<p>This progression from grammatical knowledge (G) to abstract computation (A) to contextualized reasoning (W) allows analysis of both task-specific circuits and potential shared components.</p>
<p>Experiments</p>
<p>Using our K-MSHC framework, we probe Gemma-9B to analyze which head circuits are responsible for the models understanding of grammar, arithmetic, and word problems tasks.We run experiments with parameters W = 5, p = 0.75, N = 10, K = 10, and ϵ = 0.25 across 20 trials with mini-batches of 50 (positive and negative) examples per task.All experiments were conducted on Nvidia H100 GPUs with 50 GB of memory.We used PyTorch for all LLM inference and manipulation, while scikit-learn was employed for implementing PCA dimensionality reduction and SVM classifiers for the Linear Separability metrics.</p>
<p>Baseline Performance Analysis</p>
<p>Before identifying specific head circuits, we first establish that information about our classification tasks is indeed encoded in the model's representations.Table 1 presents the Linear Separability (LS) scores for each task, showing that Gemma-9B's representations naturally encode strong task-relevant information, with baseline LS scores ranging from 0.77 to 0.99.When critical layers (identified through ablation) are removed, performance drops substantially across all tasks, with different tasks showing varying sensitivity to layer ablation.Arithmetic verification exhibits the largest drop (44%), followed by grammar (36%) and word problems (21%).Figure 1 reveals the spatial distribution of attention heads identified by our K-MSHC algorithm across the model architecture.Analysis of these patterns uncovers three key organizational principles in Gemma-9B's computational structure.</p>
<p>Task</p>
<p>Distinct Head Circuits for Different Capabilities</p>
<p>Architectural Specialization: Each task engages a distinctive subset of the network.Grammar processing primarily activates early layers (0-6) with specific banding patterns in later regions.Word problems utilize a bimodal distribution with concentrated activity in both shallow (0-3) and deep (11-20) regions.Arithmetic verification shows the most distributed pattern, engaging heads across the entire network with more uniform density.</p>
<p>Intra-Layer Selectivity: Within even the most important layers, not all heads contribute equally.Attention patterns show high selectivity, with only a few heads per layer being consistently critical.This suggests a sparse coding principle where specific head combinations-rather than entire layers-form the building blocks of task-specific circuits.</p>
<p>Task-Dependent Organization: Structurally related tasks demonstrate different patterns of head criticality.The well-defined task boundaries of grammar and arithmetic verification correlate with concentrated "super-head" patterns-specific heads that appear in almost all K-MSHCs for these tasks.In contrast, word problems require contextual reasoning across both linguistic and numerical domains, resulting in more diffuse activation patterns without dominant super-heads.</p>
<p>These findings demonstrate that language capabilities are not uniformly distributed throughout the model but are encoded through sparse, task-specialized circuits with distinct architectural signatures.</p>
<p>Circuit Overlap Analysis</p>
<p>To investigate how computational resources are shared across tasks, we analyzed the overlap between circuits identified for each task pair.Figure 2 visualizes this overlap at different selection thresholds (percentage of most frequently selected heads):</p>
<p>Figure 2: Circuit overlap (Jaccard similarity) between task pairs.The matrix shows overlap percentages at three selection thresholds: 50% (weaker but more numerous heads), 75% (moderate), and 95% (strongest "super-heads").The central region indicates three-way overlap.</p>
<p>Our analysis reveals several nuanced patterns in how Gemma-9B shares computational resources across tasks.</p>
<p>Sharing "Weak" vs "Strong" Heads: Task pairs exhibit different sharing patterns depending on head importance.At the 50% threshold (including weaker heads), grammar and arithmetic show substantial overlap (≈20%).However, this relationship inverts at the 75% threshold, where arithmetic and word problems share more critical heads (≈10%) than grammar-arithmetic pairs (≈9%).This finding suggests that tasks either share "weak" heads that are vaguely related or share "strong" heads that are more specific to the task overlap.</p>
<p>Specialized "Super-Heads": The strongest heads (95% threshold) show minimal cross-task overlap, with each task pair maintaining dedicated circuits of "super-heads".This separation is particularly striking given that arithmetic verification and word problems involve related numerical reasoning, yet their most critical components remain largely distinct.This finding challenges the notion that higher-level capabilities like "arithmetic reasoning" have a single circuit implementation within the model.</p>
<p>Non-Uniform Resource Allocation:</p>
<p>The asymmetric pattern of overlap reduction across thresholds-with grammar-arithmetic showing steeper decline than other pairs-indicates that head im-portance has task-specific scaling properties.This suggests that the model allocates computational resources non-uniformly, with some task relationships maintaining more robust sharing across importance levels than others.These overlap patterns reveal that Gemma-9B balances specialization and resource sharing through a hierarchical organization of computational components.While allowing partial resource reuse, particularly for related tasks, the model also maintains dedicated circuits for each capability's core processing requirements.</p>
<p>5 Limitations and Future Work</p>
<p>While our K-MSHC framework reveals important insights about attention circuit organization, several limitations should be acknowledged:</p>
<p>Algorithmic Assumptions.Our theoretical analysis relies on simplifying assumptions about head contamination rates and their distribution across the network.The convergence guarantees are strongest when the distribution of low-impact heads is well-separated from task-critical heads, but real-world models may exhibit more complex patterns of head importance with less clear separation.</p>
<p>Circuit Specificity.The triangulation of head circuits was not as precise as initially anticipated.We observed some variation in the specific heads identified across trials, suggesting that multiple distinct but functionally equivalent circuits may exist for each task.This redundancy makes it challenging to definitively map the exact set of heads responsible for a capability.</p>
<p>Model and Parameter Sensitivity.Our analysis is limited to a single model architecture (Gemma-9B) with one set of hyperparameters for the search algorithm.While we found our approach effective, the circuits identified may be sensitive to the choice of K, ϵ, and other parameters, particularly for tasks where performance is distributed across many weakly-contributing heads.</p>
<p>Limited Task Diversity.Our focus on three specific task families provides an insightful but still limited view of how language capabilities are organized.More complex reasoning, world knowledge, or multimodal tasks might reveal different circuit structures and overlap patterns.Future work should address these limitations by: (i) Exploring broader parameter settings across different model architectures to identify invariant circuits and understand sensitivity effects (ii) Developing refined methods for handling redundant circuits while investigating their activation dynamics during inference (iii) Extending analysis to more complex tasks that engage multiple capabilities simultaneously to better map the functional organization of language models These extensions would strengthen our understanding of how language capabilities are mechanistically implemented in neural architectures and potentially enable more targeted model interventions and improvements.</p>
<p>Conclusion</p>
<p>Our work introduces K-MSHC, a framework for identifying minimal sufficient head circuits in mid-sized language models, revealing that different syntactic tasks utilize distinct neural pathways in Gemma-9B.Grammar tasks predominantly activate early layers (0-6), while word problems utilize specific bands in both early and deep regions, with arithmetic verification showing more distributed patterns.We found non-linear patterns of circuit overlap, with grammar and arithmetic sharing more "weak" heads while arithmetic and word problems share more "strong" heads, indicating that despite partial resource sharing, each task maintains dedicated "super-heads" with minimal overlap at high thresholds.These findings advance mechanistic interpretability by demonstrating that language capabilities emerge from specialized but partially reusable circuits rather than fully general mechanisms, suggesting that future research should focus on identifying and understanding these sparse computational primitives across different model scales and architectures.</p>
<p>(i) Window-based ablation: Slide a window of size W across adjacent network layers (ii) Initialize array DROP[1 : L] to store performance drop measurement.Update DROP[ℓ] = min (DROP[ℓ], S D T (M) − S D T (M \ H W )) where H W contains all heads in the window (iii) Select high-impact layers (top p-th percentile) for candidate set C from DROP[1 : L] Stochastic Head Pruning.With baseline B = M \ C, we: (i) Start with subset size k = ⌊|C|/2⌋ (ii) Sample N random k-subsets and find worst-performing</p>
<p>Algorithm 1 k
1
Search-K-MSHC Require: window size W , percentile p, samples per iteration N 1: initialise array DROP[1 : L] ← 0 2: for s = 1 to L − W + 1 do ▷ Macro layer search 3: H W ← heads in layers s:s + W − 1 4: for ℓ = s to s + W − 1 do 5: DROP[ℓ] = min DROP[ℓ], S D T (M) − S D T (M \ H W ) 6: C ← {H ℓ | DROP[ℓ] ≥ top p-th percentile of DROP} ▷ ← max(K, ⌊k/2⌋) 19: return C 3.4 Scoring Metric: Low-Dimensional Linear Separability (LS)</p>
<p>Figure 1 :
1
Figure 1: Heat map of attention head importance across model layers.Each cell represents an attention head, with color intensity showing selection frequency across 20 trials.</p>
<p>Table 1 :
1
Baseline LS Score LS Score Post 25% Layer Ablation Drop Linear separability scores before and after ablating critical layers, showing task-dependent information encoding within the model architecture.Values show medians with 95% confidence intervals in square brackets.
Arithmetic0.99 [0.98, 1.00]0.55 [0.52, 0.60]44%Grammar0.86 [0.78, 0.90]0.50 [0.49, 0.52]36%Word Problems0.77 [0.73, 0.86]0.56 [0.53, 0.61]21%</p>
<p>The computational complexity of circuit discovery for inner interpretability. Federico Adolfi, Martina G Vilas, Todd Wareham, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Understanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, International Conference on Learning Representations (Workshop Track). 2016</p>
<p>Hopping too late: Limitations of llms on multi-hop queries. Elad Biran, Timo Schick, Hinrich Schütze, 2024</p>
<p>Towards monosemanticity: Decomposing language models with dictionary learning. Thomas Bricken, Catherine Olsson, Matthew Ziegler, Nelson Elhage, Neel Nanda, Chris Olah, 2023</p>
<p>Large language models share latent grammatical concepts across languages. Johannes Brinkmann, Gisbert Fanselow, Jon Gauthier, 2025</p>
<p>Interpreting emergent planning in model-free reinforcement learning. Tristan Bush, Rohin Shah, Jan Leike, Proceedings of the Twelfth International Conference on Learning Representations. the Twelfth International Conference on Learning Representations2024ICLR 2024</p>
<p>What does bert look at? an analysis of bert's attention. Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D Manning, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2019</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, arXiv:2110.141682021arXiv preprint</p>
<p>Sparse autoencoders find highly interpretable model directions. Henry Cunningham, Qian Li, Chelsea Chan, Chris Olah, 2023</p>
<p>How do llamas process multilingual text? activation patching study. Clément Dumas, Jiannan Gu, Charles Wang, Jiuqiang Huang, Proceedings of the ICML 2024 Workshop on Multilingual Representation Learning. the ICML 2024 Workshop on Multilingual Representation Learning2024</p>
<p>Transcoders find interpretable llm feature circuits. Jordan Dunefsky, Neel Nanda, Chris Olah, Advances in Neural Information Processing Systems. NeurIPS 372025</p>
<p>A mathematical framework for transformer circuits. Nelson Elhage, Neel Nanda, Catherine Olsson, 2021Transformer Circuits Thread</p>
<p>Scaling and evaluating sparse autoencoders. Lucy Gao, Lachlan Reynolds, Neel Nanda, Chris Olah, 2024</p>
<p>Automatically identifying local and global circuits with linear computation graphs. Xinyun Ge, Torsten Hoefler, 2024</p>
<p>Transformer feed-forward layers are key-value memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>A structural probe for finding syntax in word representations. John Hewitt, Christopher D Manning, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Evidence of learned look-ahead in a chess-playing neural network. Ethan Jenner, Ben Houk, Theis Anthropic, Advances in Neural Information Processing Systems. NeurIPS 372025</p>
<p>Language models use trigonometry to do addition. Sandipan Kantamneni, Max Tegmark, 2025</p>
<p>Sparse crosscoders for cross-layer features and model diffing. Jack Lindsey, Henry Cunningham, Chris Olah, 2024</p>
<p>Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. Sam Marks, Joseph Raul, Illia Vaswani, Neel Nanda, 2024</p>
<p>Are sixteen heads really better than one?. Paul Michel, Omer Levy, Graham Neubig, Advances in Neural Information Processing Systems. 2019</p>
<p>Arithmetic without algorithms: Language models solve math with a bag of heuristics. Yuval Nikankin, Shiko Beit-Neullas, Amir Globerson, 2024Preprint</p>
<p>Zoom in: An introduction to circuits. Chris Olah, Nick Cammarata, Ludwig Schubert, 10.23915/distill.00024Distill. 53e000242020</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Andrew Stolfo, Atticus Geiger, David Friedman, Vivek Srikumar, 2023</p>
<p>Planning in a recurrent neural network that plays sokoban. Moin Taufeeque, John Lynch, Mario Lucic, Nicholas Frosst, 2024</p>
<p>Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Alexander Templeton, Chris Olah, Neel Nanda, 2024</p>
<p>Bert rediscovers the classical nlp pipeline. Ian Tenney, Dipanjan Das, Ellie Pavlick, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Blimp: The benchmark of linguistic minimal pairs for english. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, Samuel R Bowman, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Do large language models latently perform multi-hop reasoning?. Shaokun Yang, Pengcheng He, Maosong Sun, Ming Zhou, 2024</p>
<p>Back attention: Understanding and enhancing multi-hop reasoning in llms. Zeming Yu, Wenxuan Zhang, Jie Zhou, Rui Cao, 2025</p>
<p>Structural similarities and differences in multilingual language modeling. Rui Zhang, Ziyang Liu, Ge Fu, 2024</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Llms use fourier features to compute addition. Tianyi Zhou, Haitong Ding, Chun Zong, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
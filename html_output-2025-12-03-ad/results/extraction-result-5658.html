<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5658 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5658</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5658</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-a9086398e27327e5cc664cbabf8a7bbb25c2c50f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a9086398e27327e5cc664cbabf8a7bbb25c2c50f" target="_blank">ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction</a></p>
                <p><strong>Paper Venue:</strong> bioRxiv</p>
                <p><strong>Paper TL;DR:</strong> A novel framework ProLLM is proposed that employs an LLM tailored for PPI for the first time, which replicates the biological mechanism of signaling pathways as natural language prompts and demonstrates the efficacy of ProLLM through rigorous validation against benchmark datasets.</p>
                <p><strong>Paper Abstract:</strong> The prediction of protein-protein interactions (PPIs) is crucial for understanding biological functions and diseases. Previous machine learning approaches to PPI prediction mainly focus on direct physical interactions, ignoring the broader context of nonphysical connections through intermediate proteins, thus limiting their effectiveness. The emergence of Large Language Models (LLMs) provides a new opportunity for addressing this complex biological challenge. By transforming structured data into natural language prompts, we can map the relationships between proteins into texts. This approach allows LLMs to identify indirect connections between proteins, tracing the path from upstream to downstream. Therefore, we propose a novel framework ProLLM that employs an LLM tailored for PPI for the first time. Specifically, we propose Protein Chain of Thought (ProCoT), which replicates the biological mechanism of signaling pathways as natural language prompts. ProCoT considers a signaling pathway as a protein reasoning process, which starts from upstream proteins and passes through several intermediate proteins to transmit biological signals to downstream proteins. Thus, we can use ProCoT to predict the interaction between upstream proteins and downstream proteins. The training of ProLLM employs the ProCoT format, which enhances the model’s understanding of complex biological problems. In addition to ProCoT, this paper also contributes to the exploration of embedding replacement of protein sites in natural language prompts, and instruction fine-tuning in protein knowledge datasets. We demonstrate the efficacy of ProLLM through rigorous validation against benchmark datasets, showing significant improvement over existing methods in terms of prediction accuracy and generalizability. Our results highlight the potential of LLMs to transform the field of PPI, serving as a robust potential tool for various categories of biological and medical research. The code is available at: https://github.com/MingyuJ666/ProLLM.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5658.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5658.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Protein Chain-of-Thought Enhanced Large Language Model (ProLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that adapts LLM backbones to act as text-based simulators of protein signaling pathways by converting structured PPI data into Chain-of-Thought style natural language (ProCoT), replacing token embeddings with ProtTrans protein embeddings, and instruction fine-tuning on biomolecular instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5 (variants used: Flan-T5-base, Flan-T5-large, Flan-T5-XL) and LLaMA-v1-7b (as alternative backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ProLLM is a fine-tuned LLM framework: structured protein interaction graphs are converted into ProCoT natural-language prompts that simulate signaling pathways; the model token embedding for protein IDs is replaced with ProtTrans-derived 1024-d vectors; instruction fine-tuning is performed using the Mol-Instructions dataset to inject protein functional knowledge. Backbones evaluated include Flan-T5 variants and LLaMA-7b.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular biology / Computational proteomics — protein-protein interaction (PPI) prediction and signaling pathway simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of protein signaling pathways and prediction of interaction types between proteins (e.g., activation, binding, catalysis) by reasoning over multi-step, chain-like protein interactions expressed in natural language prompts (ProCoT).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Micro-F1 (averaged over 10 random seeds; reported mean ± std)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Using Flan-T5-large backbone: without additional pretraining (Mol) — Human: 87.32 ± 1.93; SHS27k: 75.13 ± 3.76; SHS148k: 85.13 ± 1.86; STRING: 87.12 ± 1.68. With Mol instruction fine-tuning — Human: 91.05 ± 1.63; SHS27k: 78.09 ± 3.24; SHS148k: 87.66 ± 1.68; STRING: 89.21 ± 1.45.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Presence of ProCoT (Chain-of-Thought) prompt format; embedding replacement with ProtTrans protein embeddings / vocabulary expansion for protein IDs; instruction fine-tuning on Mol dataset; backbone choice (Flan-T5 variants vs LLaMA-7b); dataset partitioning strategy (DFS to capture signaling chains); size/stability of backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Ablation study (Table 3) shows ProCoT yields the largest performance gain: removing ProCoT (shuffled ProCoT data) substantially reduces micro-F1; embedding replacement and instruction fine-tuning provide additional gains. Backbone comparison (Table 2) shows Flan-T5-large outperforms Flan-T5-base, Flan-T5-XL and LLaMA-7b on micro-F1, and LLaMA-7b shows higher std (instability). Performance improvements with Mol instruction fine-tuning are demonstrated by higher micro-F1 when trained on Mol dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated evaluation using micro-F1 on four public PPI datasets (Human, SHS27K, SHS148K, STRING). Datasets split by DFS into train/val/test (70/10/20). Each experiment repeated with different random seeds (10 runs); reported metric is mean ± std across runs. Baseline comparisons and ablation experiments were used to attribute factors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLaMA-v1-7b backbone performed worse and was less stable (higher std) compared to Flan-T5 variants; models without ProCoT lose ability to reason over multi-step signaling and degrade to memorizing fixed relations with lower accuracy; pre-trained protein models (PLMs) or graph-based methods without ProCoT did not match ProLLM's performance on signaling-chain tasks. The paper does not report per-class error analyses or qualitative failure examples beyond aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against classical ML and deep learning baselines (SVM, DPPI, DNN-PPI, PIPR), graph-based methods (GNN-PPI, SemiGNN-PPI, GearNet-Edge, KeAP, MAPE-PPI), PLM-based methods (ProBERT, EMS-1b) and LLM baseline (InstructGLM). ProLLM (Flan-T5-large + Mol) achieved the best micro-F1 across datasets (e.g., 91.05% on Human) outperforming InstructGLM and GNN baselines. Ablations compare configurations with/without ProCoT, embedding replacement, and instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a Chain-of-Thought style natural language format (ProCoT) to represent multi-step signaling; replace token embeddings for protein IDs with protein-aware embeddings (ProtTrans vectors) and expand vocabulary so IDs are not split; perform instruction fine-tuning on domain-relevant instruction datasets (e.g., Mol-Instructions); use DFS partitioning to preserve signaling chains during dataset splits; prefer Flan-T5-large backbone (as evaluated) for stability and accuracy in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5658.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5658.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGLM (LLM-based baseline used for PPI prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned LLM baseline evaluated by the paper for PPI prediction tasks (used for comparison against ProLLM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned generative language model used as an LLM baseline for PPI/link prediction tasks in the experiments; specific architecture and size are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular biology / PPI prediction (as LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Used to predict protein-protein interaction types from natural-language-like prompts (baseline LLM approach).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Micro-F1 (mean ± std over 10 runs)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Without Mol dataset: Human 81.35 ± 2.04; SHS27k 70.01 ± 3.75; SHS148k 75.35 ± 1.98; STRING 84.15 ± 1.85. With Mol dataset: Human 85.71 ± 2.01; SHS27k 75.64 ± 3.49; SHS148k 83.41 ± 1.78; STRING 85.25 ± 1.72.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Instruction fine-tuning on domain dataset (Mol) improves performance; model architecture/backbone specifics (not detailed) likely affect results; absence of ProCoT-style structured prompting limits chain reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Observed performance increases for InstructGLM when using the Mol dataset versus without; however InstructGLM still underperforms ProLLM when ProCoT and ProtTrans embedding replacement are used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same automated evaluation pipeline as ProLLM: micro-F1 on DFS-partitioned PPI datasets, averaged over 10 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Underperforms compared to ProLLM that uses ProCoT and embedding replacement; lacks explicit simulation of signaling pathway chains, leading to lower performance on multi-step PPI reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared in Tables 1 and 2 to ProLLM variants and non-LLM baselines; InstructGLM improved with Mol dataset but remained below ProLLM with ProCoT and embedding replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Instruction fine-tuning on domain-specific datasets (Mol) improves baseline LLM performance, but incorporating chain-structured prompts (ProCoT) and protein-aware embeddings yields further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5658.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5658.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based generative link prediction (related mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative link prediction using Large Language Models (LLMs) / LLMs as link predictors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention in related work that LLMs can be used as generative link predictors and better capture relational information in knowledge-graph/link prediction tasks, motivating their use for PPI chain prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generic LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work reported that LLMs, when cast into generative link prediction tasks, can infer relationships between entities from textual cues and sometimes outperform graph-based baselines; cited works include Ye et al., 2024; Shu et al., 2024.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Knowledge graphs / link prediction and related network-science tasks (motivational parallel to PPI signaling chains)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generative link prediction: LLMs generate or infer relationships between entities from textual prompts, effectively simulating link existence or relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Generalization from large pretraining corpora, prompt design, and alignment of training objectives to link-prediction tasks are cited as enabling factors; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Cited prior studies (Ye et al., 2024; Shu et al., 2024) are referenced as demonstrations that LLMs can capture relational information in knowledge graph tasks; no new experiments in this paper directly measure those claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified here (mentioned as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper; referenced as motivation rather than exhaustively evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Referenced as outperforming some GNN baselines in related tasks (per cited works), motivating ProLLM's casting of PPI prediction into a generative/text simulation problem.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Design task-specific natural language prompts that encode graph/link structure (as ProCoT does) and consider embedding domain knowledge to improve LLM link-prediction capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language is all a graph needs <em>(Rating: 2)</em></li>
                <li>Knowledge graph large language model (kg-llm) for link prediction <em>(Rating: 2)</em></li>
                <li>ProtLLM: An interleaved protein-language LLM with protein-as-word pre-training <em>(Rating: 1)</em></li>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 1)</em></li>
                <li>ProtTrans: Toward understanding the language of life through self-supervised learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5658",
    "paper_id": "paper-a9086398e27327e5cc664cbabf8a7bbb25c2c50f",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "ProLLM",
            "name_full": "Protein Chain-of-Thought Enhanced Large Language Model (ProLLM)",
            "brief_description": "A framework that adapts LLM backbones to act as text-based simulators of protein signaling pathways by converting structured PPI data into Chain-of-Thought style natural language (ProCoT), replacing token embeddings with ProtTrans protein embeddings, and instruction fine-tuning on biomolecular instruction data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5 (variants used: Flan-T5-base, Flan-T5-large, Flan-T5-XL) and LLaMA-v1-7b (as alternative backbone)",
            "model_description": "ProLLM is a fine-tuned LLM framework: structured protein interaction graphs are converted into ProCoT natural-language prompts that simulate signaling pathways; the model token embedding for protein IDs is replaced with ProtTrans-derived 1024-d vectors; instruction fine-tuning is performed using the Mol-Instructions dataset to inject protein functional knowledge. Backbones evaluated include Flan-T5 variants and LLaMA-7b.",
            "model_size": null,
            "scientific_subdomain": "Molecular biology / Computational proteomics — protein-protein interaction (PPI) prediction and signaling pathway simulation",
            "simulation_task": "Text-based simulation of protein signaling pathways and prediction of interaction types between proteins (e.g., activation, binding, catalysis) by reasoning over multi-step, chain-like protein interactions expressed in natural language prompts (ProCoT).",
            "accuracy_metric": "Micro-F1 (averaged over 10 random seeds; reported mean ± std)",
            "reported_accuracy": "Using Flan-T5-large backbone: without additional pretraining (Mol) — Human: 87.32 ± 1.93; SHS27k: 75.13 ± 3.76; SHS148k: 85.13 ± 1.86; STRING: 87.12 ± 1.68. With Mol instruction fine-tuning — Human: 91.05 ± 1.63; SHS27k: 78.09 ± 3.24; SHS148k: 87.66 ± 1.68; STRING: 89.21 ± 1.45.",
            "factors_affecting_accuracy": "Presence of ProCoT (Chain-of-Thought) prompt format; embedding replacement with ProtTrans protein embeddings / vocabulary expansion for protein IDs; instruction fine-tuning on Mol dataset; backbone choice (Flan-T5 variants vs LLaMA-7b); dataset partitioning strategy (DFS to capture signaling chains); size/stability of backbone.",
            "evidence_for_factors": "Ablation study (Table 3) shows ProCoT yields the largest performance gain: removing ProCoT (shuffled ProCoT data) substantially reduces micro-F1; embedding replacement and instruction fine-tuning provide additional gains. Backbone comparison (Table 2) shows Flan-T5-large outperforms Flan-T5-base, Flan-T5-XL and LLaMA-7b on micro-F1, and LLaMA-7b shows higher std (instability). Performance improvements with Mol instruction fine-tuning are demonstrated by higher micro-F1 when trained on Mol dataset.",
            "evaluation_method": "Automated evaluation using micro-F1 on four public PPI datasets (Human, SHS27K, SHS148K, STRING). Datasets split by DFS into train/val/test (70/10/20). Each experiment repeated with different random seeds (10 runs); reported metric is mean ± std across runs. Baseline comparisons and ablation experiments were used to attribute factors.",
            "limitations_or_failure_cases": "LLaMA-v1-7b backbone performed worse and was less stable (higher std) compared to Flan-T5 variants; models without ProCoT lose ability to reason over multi-step signaling and degrade to memorizing fixed relations with lower accuracy; pre-trained protein models (PLMs) or graph-based methods without ProCoT did not match ProLLM's performance on signaling-chain tasks. The paper does not report per-class error analyses or qualitative failure examples beyond aggregate metrics.",
            "comparisons": "Compared against classical ML and deep learning baselines (SVM, DPPI, DNN-PPI, PIPR), graph-based methods (GNN-PPI, SemiGNN-PPI, GearNet-Edge, KeAP, MAPE-PPI), PLM-based methods (ProBERT, EMS-1b) and LLM baseline (InstructGLM). ProLLM (Flan-T5-large + Mol) achieved the best micro-F1 across datasets (e.g., 91.05% on Human) outperforming InstructGLM and GNN baselines. Ablations compare configurations with/without ProCoT, embedding replacement, and instruction fine-tuning.",
            "recommendations_or_best_practices": "Use a Chain-of-Thought style natural language format (ProCoT) to represent multi-step signaling; replace token embeddings for protein IDs with protein-aware embeddings (ProtTrans vectors) and expand vocabulary so IDs are not split; perform instruction fine-tuning on domain-relevant instruction datasets (e.g., Mol-Instructions); use DFS partitioning to preserve signaling chains during dataset splits; prefer Flan-T5-large backbone (as evaluated) for stability and accuracy in this task.",
            "uuid": "e5658.0",
            "source_info": {
                "paper_title": "ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "InstructGLM (baseline)",
            "name_full": "InstructGLM (LLM-based baseline used for PPI prediction)",
            "brief_description": "An instruction-tuned LLM baseline evaluated by the paper for PPI prediction tasks (used for comparison against ProLLM).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructGLM",
            "model_description": "Instruction-tuned generative language model used as an LLM baseline for PPI/link prediction tasks in the experiments; specific architecture and size are not specified in this paper.",
            "model_size": null,
            "scientific_subdomain": "Molecular biology / PPI prediction (as LLM baseline)",
            "simulation_task": "Used to predict protein-protein interaction types from natural-language-like prompts (baseline LLM approach).",
            "accuracy_metric": "Micro-F1 (mean ± std over 10 runs)",
            "reported_accuracy": "Without Mol dataset: Human 81.35 ± 2.04; SHS27k 70.01 ± 3.75; SHS148k 75.35 ± 1.98; STRING 84.15 ± 1.85. With Mol dataset: Human 85.71 ± 2.01; SHS27k 75.64 ± 3.49; SHS148k 83.41 ± 1.78; STRING 85.25 ± 1.72.",
            "factors_affecting_accuracy": "Instruction fine-tuning on domain dataset (Mol) improves performance; model architecture/backbone specifics (not detailed) likely affect results; absence of ProCoT-style structured prompting limits chain reasoning.",
            "evidence_for_factors": "Observed performance increases for InstructGLM when using the Mol dataset versus without; however InstructGLM still underperforms ProLLM when ProCoT and ProtTrans embedding replacement are used.",
            "evaluation_method": "Same automated evaluation pipeline as ProLLM: micro-F1 on DFS-partitioned PPI datasets, averaged over 10 seeds.",
            "limitations_or_failure_cases": "Underperforms compared to ProLLM that uses ProCoT and embedding replacement; lacks explicit simulation of signaling pathway chains, leading to lower performance on multi-step PPI reasoning.",
            "comparisons": "Directly compared in Tables 1 and 2 to ProLLM variants and non-LLM baselines; InstructGLM improved with Mol dataset but remained below ProLLM with ProCoT and embedding replacement.",
            "recommendations_or_best_practices": "Instruction fine-tuning on domain-specific datasets (Mol) improves baseline LLM performance, but incorporating chain-structured prompts (ProCoT) and protein-aware embeddings yields further gains.",
            "uuid": "e5658.1",
            "source_info": {
                "paper_title": "ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LLM-based generative link prediction (related mention)",
            "name_full": "Generative link prediction using Large Language Models (LLMs) / LLMs as link predictors",
            "brief_description": "Mention in related work that LLMs can be used as generative link predictors and better capture relational information in knowledge-graph/link prediction tasks, motivating their use for PPI chain prediction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Generic LLMs (unspecified)",
            "model_description": "Prior work reported that LLMs, when cast into generative link prediction tasks, can infer relationships between entities from textual cues and sometimes outperform graph-based baselines; cited works include Ye et al., 2024; Shu et al., 2024.",
            "model_size": null,
            "scientific_subdomain": "Knowledge graphs / link prediction and related network-science tasks (motivational parallel to PPI signaling chains)",
            "simulation_task": "Generative link prediction: LLMs generate or infer relationships between entities from textual prompts, effectively simulating link existence or relation types.",
            "accuracy_metric": null,
            "reported_accuracy": null,
            "factors_affecting_accuracy": "Generalization from large pretraining corpora, prompt design, and alignment of training objectives to link-prediction tasks are cited as enabling factors; details not provided in this paper.",
            "evidence_for_factors": "Cited prior studies (Ye et al., 2024; Shu et al., 2024) are referenced as demonstrations that LLMs can capture relational information in knowledge graph tasks; no new experiments in this paper directly measure those claims.",
            "evaluation_method": "Not specified here (mentioned as related work).",
            "limitations_or_failure_cases": "Not detailed in this paper; referenced as motivation rather than exhaustively evaluated.",
            "comparisons": "Referenced as outperforming some GNN baselines in related tasks (per cited works), motivating ProLLM's casting of PPI prediction into a generative/text simulation problem.",
            "recommendations_or_best_practices": "Design task-specific natural language prompts that encode graph/link structure (as ProCoT does) and consider embedding domain knowledge to improve LLM link-prediction capabilities.",
            "uuid": "e5658.2",
            "source_info": {
                "paper_title": "ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language is all a graph needs",
            "rating": 2
        },
        {
            "paper_title": "Knowledge graph large language model (kg-llm) for link prediction",
            "rating": 2
        },
        {
            "paper_title": "ProtLLM: An interleaved protein-language LLM with protein-as-word pre-training",
            "rating": 1
        },
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 1
        },
        {
            "paper_title": "ProtTrans: Toward understanding the language of life through self-supervised learning",
            "rating": 1
        }
    ],
    "cost": 0.01374775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Mingyu Jin ${ }^{*}$</th>
<th style="text-align: left;">Haochen Xue ${ }^{*}$</th>
<th style="text-align: left;">Zhenting Wang</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rutgers University</td>
<td style="text-align: left;">University of Liverpool</td>
<td style="text-align: left;">Rutgers University</td>
</tr>
<tr>
<td style="text-align: left;">Boming Kang</td>
<td style="text-align: left;">Ruosong Ye</td>
<td style="text-align: left;">Kaixiong Zhou</td>
</tr>
<tr>
<td style="text-align: left;">Peking University</td>
<td style="text-align: left;">Rutgers University</td>
<td style="text-align: left;">Massachusetts Institute of Technology</td>
</tr>
<tr>
<td style="text-align: left;">Mengnan Du</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Yongfeng Zhang ${ }^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: left;">New Jersey Institute of Technology</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Rutgers University</td>
</tr>
</tbody>
</table>
<h2>Abstract</h2>
<p>The prediction of protein-protein interactions (PPIs) is crucial for understanding biological functions and diseases. Previous machine learning approaches to PPI prediction mainly focus on direct physical interactions, ignoring the broader context of nonphysical connections through intermediate proteins, thus limiting their effectiveness. The emergence of Large Language Models (LLMs) provides a new opportunity for addressing this complex biological challenge. By transforming structured data into natural language prompts, we can map the relationships between proteins into texts. This approach allows LLMs to identify indirect connections between proteins, tracing the path from upstream to downstream. Therefore, we propose a novel framework ProLLM that employs an LLM tailored for PPI for the first time. Specifically, we propose Protein Chain of Thought (ProCoT), which replicates the biological mechanism of signaling pathways as natural language prompts. ProCoT considers a signaling pathway as a protein reasoning process, which starts from upstream proteins and passes through several intermediate proteins to transmit biological signals to downstream proteins. Thus, we can use ProCoT to predict the interaction between upstream proteins and downstream proteins. The training of ProLLM employs the ProCoT format, which enhances the model's understanding of complex biological problems. In addition to ProCoT, this paper also contributes to the exploration of embedding replacement of protein sites in natural language prompts, and instruction fine-tuning in protein knowledge datasets. We demonstrate the efficacy of ProLLM through rigorous validation against benchmark datasets, showing significant improvement over existing methods in terms of prediction accuracy and generalizability. Our results highlight the potential of LLMs to transform the field of PPI, serving as a robust potential tool for various categories of biological and medical research. The code is available at: https://github.com/MingyuJ666/ProLLM.</p>
<h2>1 Introduction</h2>
<p>Protein-protein interactions (PPIs) play an essential role in various biological processes of all living organisms, which are crucial for biomedical, genetic, and pharmaceutical research. Thus, numerous experimental methods have been proposed for PPI detection, such as yeast two-hybrid (Ito et al., 2001) and quantitative proteomics methods (Rotilio et al., 2012).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the ProLLM Framework. We fine-tuning ProLLM under Human, SHS27K, SHS148K, and STRING datasets, enabling it to solve various PPI related tasks with the structure information purely described by natural language.</p>
<p>However, wet-lab methods for PPI prediction are often time-consuming and labor-intensive, highlighting the need for more precise and efficient computational tools.
In recent years, computational biology has developed rapidly. Methods such as the Convolutional Neural Network (CNN) and Graph Neural Network (GNN) have become powerful tools for studying protein interaction. CNN-based approaches like TAG-PPI (Song et al., 2022) typically use pre-trained embedding models to convert protein sequences into numerical vector representations, and then employ one dimensional convolutional neural networks to extract features from the vectors for subsequent PPI tasks.
Although CNN methods have shown some effectiveness in PPI prediction, they still have limitations due to their fixed receptive fields and the lack of well-defined spatial relationships in protein sequences, which limit the accuracy and interpretability of the predictions. GNN-based methods such as GNN-PPI (Lv et al., 2021) treat proteins as nodes and their relationships as edges, constructing a network composed of proteins, which better captures protein relationships and interactions, and outperforms CNNs in predicting protein interactions. However, while GNNs can effectively extract network structural information, they neglected the non-physical connections between two proteins without direct physical interactions, resulting in the worse performance in learning protein chains than transformerbased models (Zhou et al., 2020). Furthermore, GNNs cannot fully capture the relationships and dynamic changes in the signal passing process of living organisms, restricting their performance for PPI prediction (Zhou et al., 2023).
Following the GNN and CNN methods, Large Language Models have also been applied to this PPI area, such as ProBert (Elnaggar et al., 2021a) and ProteinLM (Xiao et al., 2021). As long as these models can obtain a protein representation, we can use the direct cosine similarity of the representation or train an MLP to perform PPI prediction. However, these methods still cannot capture the chain relationships between proteins, such as the signaling pathways. Besides, previous literature only used LLMs as a feature extractor. Recently, using LLM as a link predictor has shown that it can better capture relational information between nodes in knowledge graph tasks and its performance surpasses traditional GNN baselines (Ye et al., 2024; Zhuo et al., 2024; Shu et al., 2024). Therefore, it is promising to introduce LLM for protein-protein interaction (PPI) tasks, since the most important biological signal for PPI tasks is the chain relationships of proteins, i.e., the signaling pathways.
To bridge the gap, we propose ProLLM, with its key ideas illustrated in Figure 1, and the difference between the existing method and our ProLLM shown in Figure 2. Existing methods only focus on the single protein-protein interaction, overlooking the application of protein links to predict PPI in signaling pathways. Instead, we employ a large language model to learn the law of signal pathways and adapt the LLM to directly predict the type of interaction between proteins.
The signaling pathway addresses the traditional method's ignorance of global, non-physical connections between proteins. Signaling pathways typically start with an upstream protein that sends a biological signal through several intermediates to a downstream protein, hence</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The difference between the existing method and our method in PPI prediction. Existing method focus on the property of upstream protein and downstream protein, our method focus on signaling pathway-like connection.
requiring consideration of the cumulative effect of multiple protein interactions. This series of interactions form sequential chains. Therefore, we propose Protein Chain of Thought (ProCoT) to overcome the limitation in understanding signaling pathways and protein functions. ProCoT is a data format that simulates the signal transduction process using a thought-chain approach, thereby enabling the prediction of protein interactions in signaling pathway problems. CoT can express the thinking process step by step to form a reasoning chain (Jin et al., 2024b), while ProCoT extends this principle further into the protein-related domain to simulate protein signaling pathways, giving LLMs a deeper insight into protein.
Additionally, our approach addresses the issue of poor protein comprehension in LLMs by replacing the standard language model embedding with embedding infused with protein information. When we process the protein name in the prompt, we replace its original embedding by ProtTrans (Elnaggar et al., 2021b), because its embedding contains protein's structual information. We also perform the instruction fine-tuning on the protein knowledge dataset to infuse protein domain knowledge into LLMs. Following these steps, the LLM acquires a robust ability to reason about the direct relationships between proteins, as demonstrated in Figure 1. It can provide answers to questions about protein relationships, which play a significant role in biological research.
Our contributions are summarized as follows:</p>
<ul>
<li>To the best of our knowledge, we are the first to explore the PPI prediction problem as a natural language processing problem. The proposed Protein Chain of Thought (ProCoT) is a novel method for understanding complex multi-step protein interactions, i.e., those present in signaling pathways.</li>
<li>We propose embedding replacement and instruction fine-tuning on our model to enhance its ability to understand and reason about proteins. This also provides our model with rich background knowledge of protein sequences and protein interactions before training.</li>
<li>Experiments on four widely used PPI datasets (i.e., Human, SHS27K, SHS148K, and STRING) demonstrate that ProLLM outperforms graph-based methods such as GNN-PPI and SemiGNN-PPI. It also has better performance than LLM-based methods like InstructGLM. The micro-F1 scores on these 4 datasets are $91.05,78.09,87.66,89.21$, respectively.</li>
</ul>
<h1>2 Related Work</h1>
<p>Protein-protein Iteractions Protein-protein interactions (PPIs) are indispensable for all living organisms, and so many efforts have been made for PPI detection up to now. Yeast two-hybrid (Y2H) assays (Brückner et al., 2009), synthetic lethality (O'Neil et al., 2017), quantitative proteomics (Wilm, 2009), and mass spectrometry (Mann et al., 2001), are widely used for identifying PPIs. To be specific, Y2H assays explore binary PPIs in living cells, offer-</p>
<p>ing insight into protein functions and regulations, although labor-intensive and limited in genomic coverage. Synthetic lethality identifies essential gene pair interactions by revealing compensatory relationships when both are inactivated. Meanwhile, quantitative proteomics, especially through mass spectrometry, illuminates the dynamic nature of the interactome under various conditions. Although informative, these methods require significant labor, time, and resources.</p>
<p>Traditional Machine Learning Models for PPI Prediction In the realm of machine learning, sequence-based approaches include Shen's SVM method (Shen et al., 2007), which uses a 3-mer count vector from protein sequences as features and groups 20 amino acids into seven classes to handle synonymous mutations and reduce feature space dimensionality. SVM-based methods (Guo et al., 2008) and the ensemble model PCA-EELM (Principal Component Analysis-Ensemble Extreme Learning Machine) (You et al., 2013) utilize various types of protein sequence information for PPI prediction. In the domain of deep learning, DeepPPI (Du et al., 2017) extracts a multitude of features from protein sequences and employs a dual deep neural network structure for feature fusion and prediction. Sun et al. (Sun et al., 2017) introduced a PPI predictor based on a stacked autoencoder, emphasizing the importance of sample balance. DPPI (Hashemifar et al., 2018) and TAGPPI (Song et al., 2022) further extend the application of convolutional neural networks and integrate text convolution networks with graph representation learning to enhance the accuracy of PPI predictions. GNNs have significantly advanced PPI predictions, improving our understanding of biological mechanisms. GNN-PPI (Lv et al., 2021) enhances inter-novel-protein prediction accuracy by utilizing protein relationships and a new evaluation approach. PT-GNN (Long et al., 2022) integrates diverse data for link prediction, learning node features from sequence and structure. DeepRank-GNN (Réau et al., 2023) offers a modular, Python-packaged framework for GNN-based interaction pattern predictions. HIGH-PPI (Gao et al., 2023) introduces a hierarchical graph learning model for effective PPI prediction and molecular detail extrapolation. Geometric GNNs excel in modeling spatial intricacies, enhancing biomolecule prediction accuracy. Geo-PPI (Liu et al., 2021) utilizes self-supervised learning for geometric protein structure representations, excelling in detailing protein interactions. mmCSM-PPI (Rodrigues et al., 2021) captures multifaceted features for mutation impact predictions on protein interactions. MAPE-PPI (Wu et al., 2024) defines the microenvironment of amino acid residues in proteins and encodes it into discrete codes, which can capture the connections between different microenvironments, enhancing the prediction accuracy of PPI.</p>
<p>Large Language Model for PPI prediction Recent advances in large language models, such as BERT (Devlin et al., 2018), GPT (Peng et al., 2023), LLaMA (Touvron et al., 2023), and T5 (Raffel et al., 2020), have significantly advanced the field of Natural Language Processing (NLP) to new heights. These models, having been trained on extensive textual corpora, exhibit exceptional capabilities across a diverse range of NLP applications (Shengyuan et al., 2024; Jin et al., 2024a; Fan et al., 2024; Hua et al., 2024). Inspired by LLMs, Protein Large Language Models(PLMs) pre-trained on large-scale protein sequences have emerged, such as ESM (Hsu et al., 2022), ProtTrans (Elnaggar et al., 2021b) and ProteinBert (Elnaggar et al., 2021b). PLMs provide a better representation of protein sequences by converting the protein sequences into the form of high-dimensional numerical vectors, known as embedding. With the protein sequences captured by the PLMs, the performances on diverse downstream tasks, such as structure prediction (Lin et al., 2023), subcellular localization prediction (Thumuluri et al., 2022), single peptide prediction (Teufel et al., 2022) and Nlinked glycosylation sites prediction (Hou et al., 2023), have been transformed. It can be expected that PLMs will assist in PPI prediction tasks. ProtLLM (Zhuo et al., 2024) utilizes a dynamic protein mounting mechanism, a protein-as-word language modeling approach, and the InterPT dataset for pre-training, enabling it to handle complex inputs and achieve superior performance on various proteins-related tasks. However, ProtLLM is used for general protein tasks, and it is not used for PPI tasks exactly. The methodology of training these LLMs to convert text inputs to desired text outputs positions them as particularly advantageous for tasks such as generative link prediction (Ye et al., 2024; Shu et al., 2024). In such tasks, the model is tasked with inferring and generating the relationship between two entities based on provided textual cues. Moreover, the extensive pre-training phase</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The process of ProLLM. Sector 1: Transfer the original protein data into ProCoT format of natural language that indicates the signaling pathways between proteins; Sector 2: Replace protein information embeddings with natural language embeddings to enhance the model's understanding of proteins; Sector 3: Inject knowledge about protein function; Sector 4: Fine-tuning on the ProCoT format dataset in Sector 1.
enables LLMs to exhibit a remarkable capacity for generalization. This capacity allows them to effectively tackle and respond to tasks or prompts that were not explicitly covered during their initial training (Wei et al., 2022). In addition to the outlined capabilities, the inherent flexibility and generalization potential of LLMs suggests that their applicability extends well beyond the conventional boundaries of NLP tasks (Yang et al., 2023). Specifically, their proficiency in generalizing from expansive pre-training sessions paves the way for their application in fields like bioinformatics and complex network analysis.</p>
<h1>3 Proposed ProLLM Framework</h1>
<p>In this section, we will introduce the implementation details of ProLLM, a framework designed to transform protein interaction data into ProCoT format natural language descriptions to simulate protein signaling pathways. By translating the structure relationships of proteins into natural language, we effectively transform protein interaction problems into natural language processing (NLP) tasks. To enhance the understanding of proteins by ProLLM, we directly integrate the protein vectors generated from ProtTrans (Elnaggar et al., 2021b) into our model to replace the original word embedding in the protein name's place. This approach allows our model to understand and utilize the biological attributes of proteins when predicting protein interactions. The ProLLM process is shown in Figure 3.</p>
<h3>3.1 ProCoT</h3>
<h3>3.1.1 Protein Data to Natural Language in CoT Format</h3>
<p>Transformation of the original protein data into a ProCoT natural language is a critical step, this kind of natural language type that represents the relationships of proteins is shown as Definition 1.</p>
<p>Definition 1 (Protein Interaction) Given the set $\mathcal{P}$ representing all proteins under consideration in the study, we define the interaction between any two proteins $p_{n}, p_{m} \in \mathcal{P}$ with the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The fine-tuning process of ProCoT. Within the first dashed box, solid lines between proteins represent the signaling pathway, and the dashed lines connecting the head and tail proteins indicate the masked interaction. Our model will predict the type of masked interaction.
specific type interaction $R_{i}$ interaction set $\mathcal{R}$. This relationship is encapsulated as a tuple $\left(p_{n}, p_{m}, R_{i}\right)$.</p>
<p>Definition 2 (Protein Signaling Pathway) In cellular signal transduction, receptor protein $\mathcal{R} \mathrm{c}$ is the beginning of the signal pathway, and it can interact with downstream signaling proteins $\mathcal{S p} . \mathcal{S}$ p is responsible for continuing to transmit signals to effector protein $\mathcal{E}$ p inside the cell. After the $\mathcal{E} \mathrm{p}$ response to the signal, it will trigger specific biological effects and activate secondary messengers $\mathcal{M} . \mathcal{M}$ may be the end of this signal level, or it may open a second signal hierarchy. If in the second signal level, the $\mathcal{M}$ will replace the $\mathcal{R} c$ in the initial signal hierarchy as the starting point. The signal will continue to propagate from $\mathcal{R}$ c through new signaling proteins, and effector proteins, and then generate new secondary messengers. After this stage, the propagation of the signal repeats the structure of the second signal level until the signal is interrupted.</p>
<p>We aim to improve the model's understanding of biological signaling pathways, enhancing its ability to learn and reason interactions within complex signal networks. We create prompts in the ProCoT (Protein Chain of Thought) format to incrementally decompose the signal transduction process like Definition 2, simulating real pathways of signal propagation. Specifically, we clarify the structure knowledge within the signaling pathway, such as signaling proteins and effector proteins. We then design rules for signal transmission across different levels to simulate the iterative process of signal transduction in proteins. Finally, we also design signal interruptions to simulate the continuity of protein transmission in real life. We use hard code to convert protein interaction information as Definition 1 in dataset into ProCoT format natural language to imitate protein signaling pathways as Definition 2. Figure 7 in the appendix is an example of ProCoT. The answer to this is "The relationship is activation".</p>
<h1>3.1.2 Training on ProCot Format Dataset</h1>
<p>In biology, co-expression and co-localization refer to the phenomenon in which proteins that are often expressed or located together in the cell tend to participate in the same or interconnected biological processes (Zhang et al., 2019). Thus, biologists frequently use known protein information to infer unknown protein interactions. Based on the principles of co-expression and co-localization in biology, we have formulated our training strategy. After constructing our ProCoT training data using DFS, we will obtain a circular protein interaction as in Figure 4. We mask the relationship between the initial and final proteins in the signaling pathway and let our model predict this relationship.</p>
<h3>3.1.3 Why ProCot Works: Biological Intuition behind ProCot</h3>
<p>In this section, we will explore ProCoT through the lenses of biology and AI. (1) Simulating Biological Signaling Pathways: The biological signaling pathway is a series of ordered, rule-based interaction processes. During the process, the output of each step serves as</p>
<p>the input for the next step, forming a highly organized information transmission chain. This pattern is consistent with the methods of handling serialized information in natural language processing (NLP). Additionally, Flan-T5's self-attention blocks can capture the indirect relationships between proteins that are distant and do not have direct interactions. It is crucial for understanding biological signal transduction processes that involve multiple steps and complex intermediate links. (2) Signal Interruption Mechanism: This mechanism aims to mimic protein adaptability since the signaling between proteins needs to be constantly interrupted to ensure the cell's accurate response to external changes. This mechanism we designed can satisfy the complex feedback mechanisms and regulatory networks within the cell. Overall, our ProCoT design follows biological principles.</p>
<h1>3.2 Enhancing Protein Sequence and Function Comprehension</h1>
<p>In our methodology, we enhance the model's understanding of protein sequence by replacing the T5 embedding with the ProTrans embedding vectors and also perform instruction fine-tuning to enable the model to learn protein functions.</p>
<h3>3.2.1 Embedding replacement</h3>
<p>We use a new embedding mechanism facilitated by the ProTrans (Elnaggar et al., 2021a) model to perform embedding replacement. ProTrans is a large-scale protein model capable of transforming protein sequences into embedding vectors with the biophysical and structural features of proteins.</p>
<p>Definition 3 (Embedding Replacement) Given the protein id $P_{\text {id }}$, we can query its protein sequence $P_{\text {seq }}$ by the professional bioinformatics tool Ensembl BioMart. $P_{\text {seq }}$ will be the input of ProTrans and ProTrans will output a $1 \times 1024$-dimensional vector $P_{\text {emb }}$.</p>
<p>We add all protein IDs in the protein dataset and their corresponding embeddings to the T5 vocabulary to solve the OOV (out of vocabulary) problem: The T5 vocabulary does not contain words for protein ID numbers. Because ProtTrans is trained based on T5-large, the process of adding to the vocabulary does not involve any change in dimensions, and there is no information loss. Additionally, this approach can also avoid the issue where a protein ID is tokenized into multiple tokens, causing the model to fail to understand it.
The replaced embedding vectors from ProtTrans can add a lot of prior knowledge about the intrinsic patterns and biophysical features of proteins to our model, better applying it to subsequent protein interaction prediction tasks.</p>
<h3>3.2.2 Instruction Finetuning</h3>
<p>Mol-Instructions (Mol) (Fang et al., 2023) dataset is applied to conduct instruction finetuning on our model. Mol is a comprehensive instruction dataset designed for the field of biomolecules that aims to address the limitations of LLMs in biomolecular research. We use Mol to teach our model with knowledge of protein functions.</p>
<p>Definition 4 (Description of Mol) The content of the Mol dataset can be defined as follows: instructional text $I$ related to LLMs queries, an input amino acid sequence $S$ that includes essential protein information, and metadata $M$ that sheds light on vital details like the protein's subcellular location, its primary function, and its participation in biological processes, followed by a corresponding output $O$ that serves as the response to $I$.</p>
<p>To facilitate instruction-based fine-tuning, we convert these objects into prompt-answer pairs $(P, A)$. This is shown as Figure 8 in the appendix. We convert the entire Mol dataset into a prompt-answer format and using these prompt-answer pairs for instruction finetuning of our model(ProLLM). This part can enhance the model's predictive performance on protein-related tasks.</p>
<h1>4 Experiments</h1>
<p>In this section, we present the experimental results to answer the following research questions (RQs).</p>
<ul>
<li>RQ1 - How does the performance of the proposed ProLLM framework compare to other baselines in terms of PPI prediction accuracy and generalizability?</li>
<li>RQ2 - How do different LLM backbones (e.g., Flan-T5-base and LLaMA-7b) affect the performance of the proposed ProLLM framework?</li>
<li>RQ3 - What are the contributions of each component to the PPI prediction performance of the ProLLM framework?</li>
</ul>
<h3>4.1 Experimental Settings</h3>
<p>Datasets: We undertake comprehensive evaluations on a trio of public Protein-Protein Interaction (PPI) datasets: Human (Song et al., 2022), STRING (Szklarczyk et al., 2019), SHS27k, and SHS148k (Chen et al., 2019). In these four datasets, we employ DFS dataset partitioning techniques. By prioritizing depth, DFS can effectively capture the step-by-step signal transmission and the hierarchical signal level of protein signaling pathways. Each dataset is split into training, validation, and testing sets, maintaining a proportion of $70 \%$, $10 \%$, and $20 \%$ respectively.</p>
<p>Baselines: Earlier studies on predicting PPI are not pre-trained, they do not have prior knowledge about the proteins. Hence, we choose SVM (Romero-Molina et al., 2019), DPPI (Hashemifar et al., 2018), DNN-PPI (Li et al., 2018), PIPR (Chen et al., 2019), GNNPPI (Lv et al., 2021) and SemiGNN-PPI (Zhao et al., 2023) as the baseline without pretraining. Furthermore, we explore how protein pre-training can influence the PPI, we include ProBERT (Elnaggar et al., 2021a), SM-1b (Rives et al., 2021), GearNet-Edge (Zhang et al., 2022), and KeAP (Zhou et al., 2022) as pre-trained baselines. MAPE-PPI (Wu et al., 2024) and InstrucGLM (Ye et al., 2024) have two versions: one with pre-training and one without pre-training.</p>
<p>Evaluation Metrics: We selected the micro-F1 score (Harbecke et al., 2022) as our evaluation metric because the PPI dataset exhibits class imbalance, making the F1 score a very relevant reference. Note that micro-F1 is widely used in the protein-protein interaction task (Tran \&amp; Kavuluru, 2018). Each dataset follows a $70 \%$ training, $10 \%$ validation, and $20 \%$ testing data split. Subsequently, we will choose different random seeds for training and testing, conducting a total of 10 tests. The mean micro-F1 score across these trials will serve as the definitive measure of model performance, with the accompanying standard deviation reflecting variability in different experimental runs.</p>
<h3>4.2 Comparative Experiment (RQ1)</h3>
<p>We compare the performance of ProLLM with other baselines (w/ and w/o additional pre-training data) on four datasets in Table 1. Note that here we use Flan-T5-large (Raffel et al., 2020) as the backbone model. Based on the results, we can make three important observations: (1) Our method outperforms other baselines without pre-training. Although InstructGLM is also LLM based, it lags behind ProLLM; (2) As for the models pre-trained on protein dataset, they cannot achieve the performance of ProLLM without prior knowledge; (3) ProLLM outperforms GearNet-Edge, KeAP, and MAPE-PPI, although they utilize a significantly larger dataset comprised of structural and knowledge graph data for pretraining than the Mol dataset.</p>
<h3>4.3 Influence of Different Backbones (RQ2)</h3>
<p>We choose Flan-T5-base, Flan-T5-large, Flan-T5-XL (Raffel et al., 2020) and LLaMA-7b (Touvron et al., 2023) models as the backbone for ProLLM. We report the micro-F1 performance</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Pre-training Dataset</th>
<th style="text-align: center;">Human</th>
<th style="text-align: center;">SHS27k</th>
<th style="text-align: center;">SHS148k</th>
<th style="text-align: center;">STRING</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SVM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$61.28 \pm 1.28$</td>
<td style="text-align: center;">$53.07 \pm 5.16$</td>
<td style="text-align: center;">$58.59 \pm 0.07$</td>
<td style="text-align: center;">$64.59 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">DPPI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$54.19 \pm 0.78$</td>
<td style="text-align: center;">$46.12 \pm 3.02$</td>
<td style="text-align: center;">$52.03 \pm 1.18$</td>
<td style="text-align: center;">$66.82 \pm 0.29$</td>
</tr>
<tr>
<td style="text-align: center;">DNN-PPI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$61.72 \pm 1.30$</td>
<td style="text-align: center;">$54.34 \pm 1.30$</td>
<td style="text-align: center;">$58.42 \pm 2.05$</td>
<td style="text-align: center;">$64.94 \pm 0.93$</td>
</tr>
<tr>
<td style="text-align: center;">PIPR</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$62.72 \pm 0.50$</td>
<td style="text-align: center;">$57.80 \pm 3.24$</td>
<td style="text-align: center;">$63.98 \pm 0.76$</td>
<td style="text-align: center;">$67.45 \pm 0.30$</td>
</tr>
<tr>
<td style="text-align: center;">GNN-PPI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$78.61 \pm 1.38$</td>
<td style="text-align: center;">$66.52 \pm 5.26$</td>
<td style="text-align: center;">$75.34 \pm 1.54$</td>
<td style="text-align: center;">$84.28 \pm 0.89$</td>
</tr>
<tr>
<td style="text-align: center;">SemiGNN-PPI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$80.79 \pm 1.40$</td>
<td style="text-align: center;">$69.25 \pm 3.91$</td>
<td style="text-align: center;">$77.62 \pm 1.08$</td>
<td style="text-align: center;">$84.85 \pm 0.65$</td>
</tr>
<tr>
<td style="text-align: center;">MAPE-PPI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$82.13 \pm 1.47$</td>
<td style="text-align: center;">$72.04 \pm 3.46$</td>
<td style="text-align: center;">$80.45 \pm 1.12$</td>
<td style="text-align: center;">$86.48 \pm 0.52$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGLM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$81.35 \pm 2.04$</td>
<td style="text-align: center;">$70.01 \pm 3.75$</td>
<td style="text-align: center;">$75.35 \pm 1.98$</td>
<td style="text-align: center;">$84.15 \pm 1.85$</td>
</tr>
<tr>
<td style="text-align: center;">ProLLM(Flan-T5-large)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$87.32 \pm 1.93$</td>
<td style="text-align: center;">$75.13 \pm 3.76$</td>
<td style="text-align: center;">$85.13 \pm 1.86$</td>
<td style="text-align: center;">$87.12 \pm 1.68$</td>
</tr>
<tr>
<td style="text-align: center;">ProBERT</td>
<td style="text-align: center;">BFD</td>
<td style="text-align: center;">$79.58 \pm 0.76$</td>
<td style="text-align: center;">$68.85 \pm 3.18$</td>
<td style="text-align: center;">$74.76 \pm 1.21$</td>
<td style="text-align: center;">$83.82 \pm 0.49$</td>
</tr>
<tr>
<td style="text-align: center;">EMS-1b</td>
<td style="text-align: center;">UniRef50</td>
<td style="text-align: center;">$81.48 \pm 1.02$</td>
<td style="text-align: center;">$70.69 \pm 3.40$</td>
<td style="text-align: center;">$79.64 \pm 1.93$</td>
<td style="text-align: center;">$85.21 \pm 0.76$</td>
</tr>
<tr>
<td style="text-align: center;">KeAP</td>
<td style="text-align: center;">ProteinKG25</td>
<td style="text-align: center;">$82.31 \pm 0.71$</td>
<td style="text-align: center;">$72.38 \pm 2.96$</td>
<td style="text-align: center;">$80.20 \pm 1.26$</td>
<td style="text-align: center;">$86.58 \pm 0.41$</td>
</tr>
<tr>
<td style="text-align: center;">GearNet-Edge</td>
<td style="text-align: center;">AlphaFoldDB</td>
<td style="text-align: center;">$82.87 \pm 1.02$</td>
<td style="text-align: center;">$72.06 \pm 3.56$</td>
<td style="text-align: center;">$79.84 \pm 1.65$</td>
<td style="text-align: center;">$85.96 \pm 1.01$</td>
</tr>
<tr>
<td style="text-align: center;">MAPE-PPI</td>
<td style="text-align: center;">CATH4.2</td>
<td style="text-align: center;">$83.64 \pm 1.22$</td>
<td style="text-align: center;">$73.21 \pm 2.97$</td>
<td style="text-align: center;">$81.78 \pm 1.24$</td>
<td style="text-align: center;">$87.23 \pm 0.35$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGLM</td>
<td style="text-align: center;">Mol dataset</td>
<td style="text-align: center;">$85.71 \pm 2.01$</td>
<td style="text-align: center;">$75.64 \pm 3.49$</td>
<td style="text-align: center;">$83.41 \pm 1.78$</td>
<td style="text-align: center;">$85.25 \pm 1.72$</td>
</tr>
<tr>
<td style="text-align: center;">ProLLM(Flan-T5-large)</td>
<td style="text-align: center;">Mol dataset</td>
<td style="text-align: center;">$91.05 \pm 1.63$</td>
<td style="text-align: center;">$78.09 \pm 3.24$</td>
<td style="text-align: center;">$87.66 \pm 1.68$</td>
<td style="text-align: center;">$89.21 \pm 1.45$</td>
</tr>
</tbody>
</table>
<p>Table 1: The micro-F1 of different methods (w/o and w/ additional pre-training data) on different datasets, where bold and underline denote the best and second best metrics, respectively. Higher micro-F1 denotes better performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Pre-training Dataset</th>
<th style="text-align: center;">Human</th>
<th style="text-align: center;">SHS27k</th>
<th style="text-align: center;">SHS148k</th>
<th style="text-align: center;">STRING</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ProLLM-Flan-T5-base</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$82.62 \pm 2.01$</td>
<td style="text-align: center;">$71.67 \pm 4.36$</td>
<td style="text-align: center;">$78.19 \pm 1.82$</td>
<td style="text-align: center;">$80.63 \pm 1.97$</td>
</tr>
<tr>
<td style="text-align: left;">ProLLM-Flan-T5-large</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{8 7 . 3 2} \pm \mathbf{1 . 9 3}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 1 3} \pm \mathbf{3 . 7 6}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 1 3} \pm \mathbf{1 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 1 2} \pm \mathbf{1 . 6 8}$</td>
</tr>
<tr>
<td style="text-align: left;">ProLLM-Flan-T5-XL</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$84.32 \pm 2.65$</td>
<td style="text-align: center;">$73.28 \pm 3.96$</td>
<td style="text-align: center;">$81.73 \pm 2.21$</td>
<td style="text-align: center;">$82.94 \pm 1.93$</td>
</tr>
<tr>
<td style="text-align: left;">ProLLM-LLaMA-v1-7b</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$81.75 \pm 4.46$</td>
<td style="text-align: center;">$70.33 \pm 6.52$</td>
<td style="text-align: center;">$77.08 \pm 3.59$</td>
<td style="text-align: center;">$79.41 \pm 2.87$</td>
</tr>
<tr>
<td style="text-align: left;">ProLLM-Flan-T5-base</td>
<td style="text-align: center;">Mol dataset</td>
<td style="text-align: center;">$87.92 \pm 2.07$</td>
<td style="text-align: center;">$74.28 \pm 4.09$</td>
<td style="text-align: center;">$85.07 \pm 1.87$</td>
<td style="text-align: center;">$87.11 \pm 1.88$</td>
</tr>
<tr>
<td style="text-align: left;">ProLLM-Flan-T5-large</td>
<td style="text-align: center;">Mol dataset</td>
<td style="text-align: center;">$\mathbf{9 1 . 0 5} \pm \mathbf{1 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 0 9} \pm \mathbf{3 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 6 6} \pm \mathbf{1 . 9 3}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 2 1} \pm \mathbf{1 . 4 5}$</td>
</tr>
<tr>
<td style="text-align: left;">ProLLM-Flan-T5-XL</td>
<td style="text-align: center;">Mol dataset</td>
<td style="text-align: center;">$89.16 \pm 2.41$</td>
<td style="text-align: center;">$75.96 \pm 3.38$</td>
<td style="text-align: center;">$86.13 \pm 1.97$</td>
<td style="text-align: center;">$87.97 \pm 1.78$</td>
</tr>
<tr>
<td style="text-align: left;">ProLLM-LLaMA-v1-7b</td>
<td style="text-align: center;">Mol dataset</td>
<td style="text-align: center;">$87.08 \pm 4.72$</td>
<td style="text-align: center;">$74.87 \pm 6.51$</td>
<td style="text-align: center;">$84.61 \pm 3.53$</td>
<td style="text-align: center;">$86.71 \pm 2.61$</td>
</tr>
</tbody>
</table>
<p>Table 2: The micro-F1 score of ProLLM on different backbones. Higher micro-F1 denotes better performance. Bold and underline denote the best and second-best metrics, respectively.
comparison as in Table 2. We should replace the embedding in Flan-T5 with ProtTrans and ProtTrans is a pre-trained model based on Flan-T5-Large. Therefore, the embedding generated by ProtTrans will match Flan-T5-large better. Additionally, despite having more model parameters, LLaMA-v1-7b exhibits worse lower micro-F1 scores in four datasets compared to lighter models: Flan-T5-base, Flan-T5-large, and Flan-T5-XL. Furthermore, the greater standard deviation of LLaMA-v1-7b highlights its instability.</p>
<h1>4.4 Ablation Study (RQ3)</h1>
<p>In our ablation study (Table 3), we evaluated the impact of different configurations on PPI prediction. The configurations are as follows: ProLLM w/o ProCoT: This setup shuffles ProCoT data, disrupting signaling pathways to mimic the model's performance without pathway understanding. It limits the model's ability to learn from signaling sequences. ProLLM relies on memorizing fixed relations instead of reasoning through intermediate protein relationships; ProLLM w/o Embedding Replacement: It will compare the model with replaced embeddings to the model without expended vocabulary, which evaluates the effect of protein-specific embedding features; ProLLM w/o Instruction Fine-tuning: This setup examines the model's capability to predict protein-protein interactions without the application of instruction fine-tuning on Mol dataset; ProLLM w/o Embedding and Instruction Fine-tuning: This configuration tests the model's performance without utilizing both protein-specific embedding features and instruction fine-tuning on Mol dataset.
In our ablation study, we have identified that ProCoT has the most significant impact on the performance of ProLLM. Our experiments revealed that introducing ProCoT led to substantial improvements in the performance of the model. Additionally, we explored other techniques such as embedding replacement and instruction fine-tuning on the Mol dataset. While these approaches did show some positive effects on the model's performance, their impact was found to be comparatively smaller when compared to the influence of ProCoT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ProCoT</th>
<th style="text-align: center;">Embedding <br> Replacement</th>
<th style="text-align: center;">Instruction <br> Fine-tuning</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">SHS27k</td>
<td style="text-align: center;">SHS148k</td>
<td style="text-align: center;">STRING</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$83.87 \pm 1.25$</td>
<td style="text-align: center;">$70.83 \pm 1.54$</td>
<td style="text-align: center;">$79.12 \pm 2.64$</td>
<td style="text-align: center;">$82.68 \pm 1.36$</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$87.32 \pm 1.93$</td>
<td style="text-align: center;">$74.26 \pm 3.53$</td>
<td style="text-align: center;">$85.13 \pm 1.86$</td>
<td style="text-align: center;">$87.12 \pm 1.68$</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$88.53 \pm 1.45$</td>
<td style="text-align: center;">$73.59 \pm 2.29$</td>
<td style="text-align: center;">$85.94 \pm 3.39$</td>
<td style="text-align: center;">$87.62 \pm 1.68$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$78.32 \pm 2.65$</td>
<td style="text-align: center;">$61.98 \pm 2.06$</td>
<td style="text-align: center;">$74.10 \pm 1.27$</td>
<td style="text-align: center;">$77.85 \pm 1.54$</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{9 1 . 0 3} \pm \mathbf{1 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 0 9} \pm \mathbf{3 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 6 4} \pm \mathbf{1 . 9 3}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 2 8} \pm \mathbf{1 . 4 5}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation study. The metric here is micro-F1. Where bold denote the best metrics. Higher micro-F1 denotes better performance.</p>
<h1>5 Conclusions and Future Work</h1>
<p>In this paper, we propose ProLLM, a novel framework that leverages LLMs for proteinprotein interaction prediction by representing protein data in natural language formats. Our key contributions include: 1) ProCoT (Protein Chain of Thought) to convert multi-step protein signaling pathways to natural language prompts, and the design of ProCoT can reflect the actual protein signaling passing within a biological organism. Additionally, the format of ProCoT is sequential, which is a type of information that LLMs are good at processing. 2) Integration of protein-specific embeddings from ProtTrans, and 3) Instruction fine-tuning on protein knowledge datasets. Through extensive experiments on four PPI datasets, ProLLM significantly outperformed existing graph-based and language model methods in prediction accuracy and generalizability. By unifying language models with structured biological data, our work opens up new possibilities for driving discoveries in computational biology, drug discovery, and broader scientific domains.</p>
<h3>5.1 Acknowledgement</h3>
<p>We thank Wenyue Hua, Kai Mei and Taowen Wang for their valuable discussions and suggestions during the project.</p>
<h2>References</h2>
<p>Anna Brückner, Cécile Polge, Nicolas Lentze, Daniel Auerbach, and Uwe Schlattner. Yeast two-hybrid, a powerful tool for systems biology. International journal of molecular sciences, 10(6):2763-2788, 2009.</p>
<p>Muhao Chen, Chelsea J-T Ju, Guangyu Zhou, Xuelu Chen, Tianran Zhang, Kai-Wei Chang, Carlo Zaniolo, and Wei Wang. Multifaceted protein-protein interaction prediction based on siamese residual rcnn. Bioinformatics, 35(14):i305-i314, 2019.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Xiuquan Du, Shiwei Sun, Changlin Hu, Yu Yao, Yuanting Yan, and Yanping Zhang. Deepppi: boosting prediction of protein-protein interactions with deep neural networks. Journal of chemical information and modeling, 57(6):1499-1510, 2017.</p>
<p>Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112-7127, 2021a.</p>
<p>Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Wang Yu, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-1, 2021b. doi: 10.1109/TPAMI.2021.3095381.</p>
<p>Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li, Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, et al. Nphardeval4v: A dynamic reasoning benchmark of multimodal large language models. arXiv preprint arXiv:2403.01777, 2024.</p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018, 2023.</p>
<p>Ziqi Gao, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li, Peilin Zhao, Huanming Yang, Yong Huang, and Jia Li. Hierarchical graph learning for protein-protein interaction. Nature Communications, 14(1):1093, 2023.</p>
<p>Yanzhi Guo, Lezheng Yu, Zhining Wen, and Menglong Li. Using support vector machine combined with auto covariance to predict protein-protein interactions from protein sequences. Nucleic acids research, 36(9):3025-3030, 2008.</p>
<p>David Harbecke, Yuxuan Chen, Leonhard Hennig, and Christoph Alt. Why only micro-f1? class weighting of measures for relation classification. arXiv preprint arXiv:2205.09460, 2022.</p>
<p>Somaye Hashemifar, Behnam Neyshabur, Aly A Khan, and Jinbo Xu. Predicting proteinprotein interactions through sequence-based deep learning. Bioinformatics, 34(17):i802i810, 2018.</p>
<p>Xiaoyang Hou, Yu Wang, Dongbo Bu, Yaojun Wang, and Shiwei Sun. Emngly: predicting nlinked glycosylation sites using the language models for feature extraction. Bioinformatics, 39(11):btad650, 2023.</p>
<p>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. ICML, 2022. doi: 10.1101/2022.04.10.487779. URL https://www.biorxiv.org/content/early/ 2022/04/10/2022.04.10.487779.</p>
<p>Wenyue Hua, Kaijie Zhu, Lingyao Li, Lizhou Fan, Shuhang Lin, Mingyu Jin, Haochen Xue, Zelong Li, JinDong Wang, and Yongfeng Zhang. Disentangling logic: The role of context in large language model reasoning capabilities. arXiv preprint arXiv:2406.02787, 2024.</p>
<p>Takashi Ito, Tomoko Chiba, Ritsuko Ozawa, Mikio Yoshida, Masahira Hattori, and Yoshiyuki Sakaki. A comprehensive two-hybrid analysis to explore the yeast protein interactome. Proceedings of the National Academy of Sciences, 98(8):4569-4574, 2001.</p>
<p>Mingyu Jin, Qinkai Yu, Chong Zhang, Dong Shu, Suiyuan Zhu, Mengnan Du, Yongfeng Zhang, and Yanda Meng. Health-llm: Personalized retrieval-augmented disease prediction model. arXiv preprint arXiv:2402.00746, 2024a.</p>
<p>Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925, 2024b.</p>
<p>Hang Li, Xiu-Jun Gong, Hua Yu, and Chang Zhou. Deep neural network based predictions of protein interactions using primary sequences. Molecules, 23(8):1923, 2018.</p>
<p>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123-1130, 2023.</p>
<p>Xianggen Liu, Yunan Luo, Pengyong Li, Sen Song, and Jian Peng. Deep geometric representations for modeling effects of mutations on protein-protein binding affinity. PLoS computational biology, 17(8):e1009284, 2021.</p>
<p>Yahui Long, Min Wu, Yong Liu, Yuan Fang, Chee Keong Kwoh, Jinmiao Chen, Jiawei Luo, and Xiaoli Li. Pre-training graph neural networks for link prediction in biomedical networks. Bioinformatics, 38(8):2254-2262, 2022.</p>
<p>Guofeng Lv, Zhiqiang Hu, Yanguang Bi, and Shaoting Zhang. Learning unknown from correlations: graph neural network for inter-novel-protein interaction prediction. arXiv preprint arXiv:2105.06709, 2021.</p>
<p>Matthias Mann, Ronald C Hendrickson, and Akhilesh Pandey. Analysis of proteins and proteomes by mass spectrometry. Annual review of biochemistry, 70(1):437-473, 2001.</p>
<p>Nigel J O'Neil, Melanie L Bailey, and Philip Hieter. Synthetic lethality and cancer. Nature Reviews Genetics, 18(10):613-623, 2017.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.</p>
<p>Manon Réau, Nicolas Renaud, Li C Xue, and Alexandre MJJ Bonvin. Deeprank-gnn: a graph neural network framework to learn patterns in protein-protein interfaces. Bioinformatics, 39(1):btac759, 2023.</p>
<p>Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.</p>
<p>Carlos HM Rodrigues, Douglas EV Pires, and David B Ascher. mmcsm-ppi: predicting the effects of multiple point mutations on protein-protein interactions. Nucleic Acids Research, 49(W1):W417-W424, 2021.</p>
<p>Sandra Romero-Molina, Yasser B Ruiz-Blanco, Mirja Harms, Jan Münch, and Elsa SanchezGarcia. Ppi-detect: A support vector machine model for sequence-based prediction of protein-protein interactions. Journal of computational chemistry, 40(11):1233-1242, 2019.</p>
<p>Domenico Rotilio, Anna Della Corte, Marco D'Imperio, Walter Coletta, Simone Marcone, Cristian Silvestri, Lucia Giordano, Michela Di Michele, and Maria Benedetta Donati. Proteomics: bases for protein complexity understanding. Thrombosis research, 129(3): $257-262,2012$.</p>
<p>Juwen Shen, Jian Zhang, Xiaomin Luo, Weiliang Zhu, Kunqian Yu, Kaixian Chen, Yixue Li, and Hualiang Jiang. Predicting protein-protein interactions based only on sequences information. Proceedings of the National Academy of Sciences, 104(11):4337-4341, 2007.</p>
<p>Chen Shengyuan, Yunfeng Cai, Huang Fang, Xiao Huang, and Mingming Sun. Differentiable neuro-symbolic reasoning on large-scale knowledge graphs. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, and Yongfeng Zhang. Knowledge graph large language model (kg-llm) for link prediction, 2024.</p>
<p>Bosheng Song, Xiaoyan Luo, Xiaoli Luo, Yuansheng Liu, Zhangming Niu, and Xiangxiang Zeng. Learning spatial structures of proteins improves protein-protein interaction prediction. Briefings in bioinformatics, 23(2):bbab558, 2022.</p>
<p>Tanlin Sun, Bo Zhou, Luhua Lai, and Jianfeng Pei. Sequence-based prediction of protein protein interaction using a deep-learning algorithm. BMC bioinformatics, 18:1-8, 2017.</p>
<p>Damian Szklarczyk, Annika L Gable, David Lyon, Alexander Junge, Stefan Wyder, Jaime Huerta-Cepas, Milan Simonovic, Nadezhda T Doncheva, John H Morris, Peer Bork, et al. String v11: protein-protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic acids research, 47(D1): D607-D613, 2019.</p>
<p>Felix Teufel, José Juan Almagro Armenteros, Alexander Rosenberg Johansen, Magnús Halldór Gíslason, Silas Irby Pihl, Konstantinos D Tsirigos, Ole Winther, Søren Brunak, Gunnar von Heijne, and Henrik Nielsen. Signalp 6.0 predicts all five types of signal peptides using protein language models. Nature biotechnology, 40(7):1023-1025, 2022.</p>
<p>Vineet Thumuluri, José Juan Almagro Armenteros, Alexander Rosenberg Johansen, Henrik Nielsen, and Ole Winther. Deeploc 2.0: multi-label subcellular localization prediction using protein language models. Nucleic Acids Research, 50(W1):W228-W234, 2022.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Tung Tran and Ramakanth Kavuluru. An end-to-end deep learning architecture for extracting protein-protein interactions affected by genetic mutations. Database, 2018:1-13, 2018.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.</p>
<p>Matthias Wilm. Quantitative proteomics in biological research. Proteomics, 9(20):4590-4605, 2009.</p>
<p>Lirong Wu, Yijun Tian, Yufei Huang, Siyuan Li, Haitao Lin, Nitesh V Chawla, and Stan Z Li. Mape-ppi: Towards effective and efficient protein-protein interaction prediction via microenvironment-aware protein embedding. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=itGkF993gz.</p>
<p>Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, and Jie Tang. Modeling protein using large-scale pretrain language model, 2021.</p>
<p>Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond. ACM Transactions on Knowledge Discovery from Data, 2023.</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Language is all a graph needs. EACL, 2024.</p>
<p>Zhu-Hong You, Ying-Ke Lei, Lin Zhu, Junfeng Xia, and Bing Wang. Prediction of proteinprotein interactions from amino acid sequences with ensemble extreme learning machines and principal component analysis. In BMC bioinformatics, volume 14, pp. 1-11. Springer, 2013.</p>
<p>Jinxiong Zhang, Cheng Zhong, Yiran Huang, Hai Xiang Lin, and Mian Wang. A method for identifying protein complexes with the features of joint co-localization and joint co-expression in static ppi networks. Computers in Biology and Medicine, 111:103333, 2019.</p>
<p>Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022.</p>
<p>Ziyuan Zhao, Peisheng Qian, Xulei Yang, Zeng Zeng, Cuntai Guan, Wai Leong Tam, and Xiaoli Li. Semignn-ppi: Self-ensembling multi-graph neural network for efficient and generalizable protein-protein interaction prediction. arXiv preprint arXiv:2305.08316, 2023.</p>
<p>Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Bian Cheng, and Yizhou Yu. Protein representation learning via knowledge enhanced primary structure reasoning. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Cheng Bian, and Yizhou Yu. Protein representation learning via knowledge enhanced primary structure modeling, 2023.</p>
<p>Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:57-81, 2020.</p>
<p>Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, and Wentao Zhang. Protllm: An interleaved protein-language llm with protein-as-word pre-training. arXiv preprint arXiv:2403.07920, 2024.</p>
<h1>6 Appendix</h1>
<h3>6.1 Dataset Partition Algorithm</h3>
<p>In the field of graph learning, DFS (Depth-First Search), BFS (Breadth-First Search), and random sampling are three common graph traversal or sampling strategies. Figure 5 shows the difference in between DFS and BFS.</p>
<p>Depth-First Search (DFS): DFS starts from a starting node and explores the graph's depth until it cannot go further, then backtracks to the nearest unvisited node from the starting node. This approach makes DFS inclined to explore the deep structure of the graph. BreadthFirst Search (BFS): BFS starts from a starting node and visits its neighboring nodes one by one, then proceeds to visit the neighboring nodes' neighboring nodes, and so on. This approach prioritizes exploring the breadth of the graph. Random Sampling: Random sampling is a method of randomly selecting nodes for traversal or sampling. It can employ uniform random selection or select nodes based on certain probabilities.</p>
<p>The choice of strategy depends on the specific problem requirements and DFS is for exploring entire connected components. DFS can be used to find paths in a graph, especially when finding all paths from one node to another. DFS selects the next node for in-depth exploration at each step until the target node is found or cannot continue deeper. This is very similar to the protein signaling pathway in biology. From one protein to the target protein through different proteins, to simulate the signaling pathway. Additionally, after the DFS, we can obtain a cyclic structure of connected proteins, where one side of the cycle represents the signaling pathway between all proteins from the head protein to the tail protein, and the other side represents the direct interaction between the head and tail proteins. This is the data format we need in training our model. To simulate signaling pathways for training, we propose ProCot, and we only use DFS for dataset partition.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Demo of BFS and DFS dataset partition method.</p>
<h3>6.2 The disscusion about the embedding replacement</h3>
<p>As shown in Figure Figure 6, the model with expended vocabulary treats the entire protein ID as a whole when processing protein IDs. In contrast, the tokenizer of the model with original vocabulary will split the protein IDs. The split protein IDs will affect the model's performance in subsequent PPI tasks.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Expended vocabulary vs original vocabulary</p>
<h1>6.3 The Datasets</h1>
<h3>6.3.1 Human dataset</h3>
<p>The Human dataset contains 4577 unique proteins and 75875 interactions between proteins. The distribution of the Human dataset is shown on Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Binding</td>
<td style="text-align: center;">17977</td>
<td style="text-align: center;">$23.70 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Activation</td>
<td style="text-align: center;">15470</td>
<td style="text-align: center;">$20.41 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Catalysis</td>
<td style="text-align: center;">11115</td>
<td style="text-align: center;">$14.65 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Inhibition</td>
<td style="text-align: center;">10611</td>
<td style="text-align: center;">$13.99 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Expression</td>
<td style="text-align: center;">9052</td>
<td style="text-align: center;">$11.93 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Post-translational</td>
<td style="text-align: center;">6255</td>
<td style="text-align: center;">$8.25 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Modification and reaction</td>
<td style="text-align: center;">5354</td>
<td style="text-align: center;">$7.07 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Total Count</td>
<td style="text-align: center;">75875</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Distribution of Human dataset.</p>
<h3>6.3.2 SHS27K, SHS148K and STRING</h3>
<p>The STRING dataset is a large collection that contains 4,775,154 protein-protein interaction (PPI) records relevant to human biology, covering 15,335 distinct proteins and 572,568 unique interaction events. Two subsets of the STRING database are SHS27k and SHS148k. These subsets are curated by applying specific filters, such as selecting only proteins that are more than 50 amino acids long and exhibit less than $40 \%$ sequence similarity to each other, to ensure diversity and relevance. The SHS27k subset is smaller, with 16,912 PPI entries involving 1,690 proteins and a total of 63,408 interactions. The SHS148k subset is more extensive, containing 99,782 PPI entries, 5,189 proteins, and a high interaction count of 369,041. The distribution of the datasets is shown in Table 5</p>
<h3>6.4 Implementation Details and Hyperparameters</h3>
<p>ProLLM is trained on A40-48G. During training, the number of training epochs is 10, the learning rate is $3 \mathrm{e}-4$, the per-device train batch size is 2 , the per-device evaluation batch size is 2 , the warmup steps are 400 , and the weight decay is 0.01 .</p>
<h3>6.5 Detail in Embedding Replacement</h3>
<p>To enhance the understanding of protein sequences, we adopt a method that integrates protein sequence vectorization with vocabulary expansion. First, we query the corresponding protein sequence $S_{\mathrm{P}<em d="d" i="i">{\mathrm{id}}}$ based on the protein's unique identifier $P</em>$ using the Ensemble</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">SHS27K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SHS148K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">STRING</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Type</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">Percentage</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">Percentage</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">Percentage</td>
</tr>
<tr>
<td style="text-align: left;">Reaction</td>
<td style="text-align: center;">18,162</td>
<td style="text-align: center;">$28.65 \%$</td>
<td style="text-align: center;">102,964</td>
<td style="text-align: center;">$27.91 \%$</td>
<td style="text-align: center;">$1,669,750$</td>
<td style="text-align: center;">$34.98 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Activation</td>
<td style="text-align: center;">7,400</td>
<td style="text-align: center;">$11.67 \%$</td>
<td style="text-align: center;">42,516</td>
<td style="text-align: center;">$11.52 \%$</td>
<td style="text-align: center;">232,240</td>
<td style="text-align: center;">$4.86 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Catalysis</td>
<td style="text-align: center;">11,796</td>
<td style="text-align: center;">$18.60 \%$</td>
<td style="text-align: center;">67,168</td>
<td style="text-align: center;">$18.20 \%$</td>
<td style="text-align: center;">998,266</td>
<td style="text-align: center;">$20.91 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Binding</td>
<td style="text-align: center;">16,056</td>
<td style="text-align: center;">$25.33 \%$</td>
<td style="text-align: center;">93,632</td>
<td style="text-align: center;">$25.37 \%$</td>
<td style="text-align: center;">$1,610,314$</td>
<td style="text-align: center;">$33.73 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ptmod</td>
<td style="text-align: center;">2,872</td>
<td style="text-align: center;">$4.53 \%$</td>
<td style="text-align: center;">20,153</td>
<td style="text-align: center;">$5.46 \%$</td>
<td style="text-align: center;">88,424</td>
<td style="text-align: center;">$1.85 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Inhibition</td>
<td style="text-align: center;">5,550</td>
<td style="text-align: center;">$8.75 \%$</td>
<td style="text-align: center;">34,712</td>
<td style="text-align: center;">$9.41 \%$</td>
<td style="text-align: center;">147,676</td>
<td style="text-align: center;">$3.09 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Expression</td>
<td style="text-align: center;">1,572</td>
<td style="text-align: center;">$2.48 \%$</td>
<td style="text-align: center;">7,896</td>
<td style="text-align: center;">$2.14 \%$</td>
<td style="text-align: center;">28,484</td>
<td style="text-align: center;">$0.60 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Distribution of SHS27K, SHS148K, STRING dataset.</p>
<h1>Input template:</h1>
<p>This is the protein rational: The relationship between {{ENSP00000019103}} and {{ENSP00000275216}} is catalysis, the relationship between
{{ENSP00000275216}} and {{ENSP00000267396}} is activation, the relationship between {{ENSP00000267396}} and {{ENSP00000318944}} is binding, the relationship between {{ENSP00000318944}} and {{ENSP00000275216}} is reaction. Let's think step by step, what is relationship between {{ENSP00000019103}} and {{ENSP00000275216}}?</p>
<h2>Target template:</h2>
<p>The relationship is activation.</p>
<p>Figure 7: One example of ProCoT prompt.</p>
<p>BioMart tool. Subsequently, the retrieved protein sequence $S_{\mathrm{P}<em p="p">{\mathrm{id}}}$ is fed into the ProtTrans model, which outputs a $1 \times 1024$-dimensional vector $V</em>$ generated by ProtTrans for processing, enabling the model to gain a deeper understanding of protein sequences.}$ encapsulating key information of the sequence. This vector is then used as the embedding vector for the new vocabulary item $P_{i d}$ added to the Tokenizer's vocabulary. Through this approach, whenever the model encounters the identifier $P_{i d}$, it utilizes the embedding vector $V_{p</p>
<h3>6.6 Prompt in ProLLM</h3>
<p>The prompt in ProLLM has two type: ProCot prompt and Instruction finetuning prompt. Figure 7 is an example prompt of ProCot. Figure 8 is the prompt of instruction fine-tuning.</p>
<p>Raw data:
Instruction I: Analyze this protein sequence and, based on conserved
domains or motifs, deduce its possible cellular function(s).
Sequence S: MRLRKKWWARPEMEASPLCIV...
Metadata M: tRNA (guanine-N7-)-methyltransferase activity...
Output O: Upon evaluating the structure of the protein with sequence, it
can be predicted that its biological function is primarily associated with
tRNA (guanine-N7-)-methyltransferase activity.
Input template:
Follow the instruction $\boldsymbol{I}$ to answer the question. The protein sequence is $\boldsymbol{S}$. $M$ is the information about subcellular localization, primary function, and biological process.</p>
<h1>Target template:</h1>
<p>The answer is : $\boldsymbol{O}$</p>
<p>Figure 8: Example of instruction fine-tuning prompt.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal Contribution.
${ }^{\dagger}$ Author Emails: mingyu.jin@rutgers.edu, hicaca945@gmail.com, zhenting.wang@rutgers.edu, kangbm@hsc.pku.edu.cn, ruosong.ye@rutgers.edu, kz34@mit.edu, mengnan.du@njit.edu, yongfeng.zhang@rutgers.edu&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8738 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8738</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8738</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-269294023</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14387v2.pdf" target="_blank">A Survey on Self-Evolution of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs. Our corresponding GitHub repository is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8738.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8738.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (verbal reinforcement learning / self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-level method enabling an LLM to use runtime/environment feedback and verbalize reflections (thoughts) to iteratively improve decisions and code via internal working memory updates rather than weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (verbal RL / generate-then-reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model obtains runtime or environment feedback (e.g., code interpreter errors or tool outputs), then generates explicit reflective thoughts and updated plans in working memory; these reflections are used to modify subsequent generations in an iterative loop (verbal reinforcement learning). Number of iterations not specified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code generation, agent decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where model outputs can be executed or evaluated by an environment (e.g., running code, embodied agent actions) and provide run-time feedback enabling reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Working-memory / in-context verbal reflections using environment run-time feedback (prompted generation of thoughts), not in-weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey reports that Reflexion uses run-time feedback from interpreters/tools and generates reflections to improve outputs and decision-making qualitatively; presented as an effective in-context/self-reflective strategy for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes mixed evidence across self-reflection methods generally; weaker models may struggle to self-critique and require environment feedback — Reflexion depends on informative environment/tool feedback and may be limited when such feedback is absent or ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned relative to critique-based and interactive methods: Reflexion emphasizes in-context verbal RL and working-memory updates versus in-weight fine-tuning or purely metric-based filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8738.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where an LLM generates natural-language critiques of its own outputs and iteratively revises answers conditioned on that self-feedback without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (iterative self-critique and revision)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model produces an output, then generates natural-language feedback/critiques on that output (few-shot prompting) and uses the feedback to produce revised outputs; the process can be iterated until satisfactory — specific iteration counts are task-dependent and not enumerated in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General answer generation / reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the model can generate an answer and also generate a textual critique/diagnosis of errors (e.g., open-ended QA, reasoning or code explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered self-critique: model-as-critic prompts that produce textual critiques which are fed back into subsequent prompts (no external learned critic required).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey describes Self-Refine as enabling models to iteratively refine outputs using self-generated feedback and lists it among critique-based correction methods; evidence referenced in original work but no quantitative numbers provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes that weaker models may struggle with effective self-critique, implying reliance on a sufficiently capable self-critic; also general concerns about self-generated data quality and potential error accumulation across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with critique-free corrections (which rely on objective signals) and with environment/tool-based critiques; Self-Refine uses open-ended textual critique rather than binary metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8738.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRITIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC (tool-interactive critiquing for self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that allows LLMs to self-correct outputs by interacting with tools and using tool-produced critiques to revise outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CRITIC: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tool-interactive critiquing (CRITIC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The LLM invokes tools or environment executions to obtain diagnostic critiques or error messages, then conditions on those critiques to revise and improve outputs iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General domains where tool execution yields diagnostic feedback (e.g., code, tool-augmented tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that produce interpretable tool feedback such as error messages or execution traces enabling targeted revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Integration of external tools and their feedback into the model's prompt loop; critique-based correction grounded in tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey highlights CRITIC as an example of critique-based refinement showing that tool-provided critiques can be used to revise outputs and improve correctness qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dependent on availability and informativeness of tool/environment feedback; survey emphasizes that tool-interaction helps weaker models but does not provide numerical failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with self-generated critiques: CRITIC leverages external (tool-derived) critiques which may be more reliable for weaker models than purely self-generated critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8738.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constitutional AI (CAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-critique and revision framework where the model generates critiques guided by a constitution (set of principles) and revises outputs according to those critiques to improve alignment/safety.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constitutional ai: Harmlessness from ai feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Constitution-guided self-critique and revision (CAI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model produces critiques of its own outputs according to predefined principles (a 'constitution') and generates revised outputs that better conform to those principles; typically used during supervised fine-tuning data generation or alignment stages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Alignment / harmlessness / response generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks focused on aligning model outputs with safety and ethical principles, including harmlessness and reducing bias.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering using a constitution: LLM-as-critic produces critiques and revisions following explicit rule sets; can be used to generate supervised fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey lists CAI among critique-based correction methods and notes it significantly improves initial models in supervised settings (qualitative statement; no numbers included in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey raises general concerns about reliance on self-generated supervision and potential for model biases in critiques; CAI efficacy may depend on strength of the constitution and model critic capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>CAI is compared to other critique-based approaches; distinct in using a structured set of principles rather than free-form self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8738.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (sampling multiple reasoning paths and voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric-free filtering/refinement technique that samples multiple chain-of-thought reasoning paths and selects the final answer by majority/agreement to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (generate-many + majority selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple reasoning chains (CoT) per question and filter/select the answer with highest agreement across those chains; this is a single-step ensemble/refinement method rather than iterative self-critique.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step reasoning / arithmetic and logical reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks benefiting from chain-of-thought where consistent answers across diverse reasoning paths indicate higher confidence in correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Metric-free internal consistency: sampling diversity in in-context CoT outputs and selecting answers with highest inter-sample agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey notes Self-Consistency is used to filter and improve high-confidence self-training data and is reported in literature to improve CoT reliability (survey does not give specific numeric gains).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on diversity of sampled paths; may not help if model is consistently wrong across samples or if reasoning diversity is low; survey highlights general risk of model collapse when training on self-generated data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with metric-based filtering (which uses external metrics) and critique-based corrections; Self-Consistency is internal and metric-free.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8738.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where an LLM verifies candidate answers by checking internal consistency or re-deriving key values to score/verifying candidates and pick the most consistent answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are better reasoners with self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (internal verification and selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates candidate answers and performs verification steps (e.g., recomputation, checking conditions) to score and select the most verifiable candidate; used for tasks like math or clinical information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning / clinical information extraction / math</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where candidate answers can be algorithmically or logically verified against conditions or evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted verification steps and consistency scoring performed by the same LLM (internal metric-free verification or simple scoring functions).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites Self-Verification as an internal filtering/selecting approach that yields higher-confidence answers and was applied in clinical information extraction (qualitative report; no numerical values in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Effectiveness depends on the model's ability to perform reliable verification steps; may fail if verification logic is complex or model hallucination undermines checking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with Self-Consistency and metric-based filtering; Self-Verification focuses on logical/evidence checks rather than majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8738.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Debugging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Debug / Self-Debugging (execution-based debugging)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code generation/self-improvement approach where the model executes generated code (or uses interpreters) and uses execution traces/errors to iteratively debug and correct code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Execution-feedback-driven debugging (Self-Debugging)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate code, run it on interpreter/test cases, collect execution/error feedback (tracebacks), then prompt the model to analyze errors and produce corrected code iteratively until tests pass.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code generation and program repair</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code synthesis tasks where generated programs can be executed and tested to produce concrete error feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Environment feedback from code execution integrated into prompt loop; critique-free correction via objective test signals and error traces.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey indicates execution feedback (test cases, interpreters) provides strong precise guidance for iterative improvement and is used by several works (Self-Debugging, Self-Evolve, Reflexion) to improve code quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires executable environment and good test coverage; may be limited when runtime errors are non-informative or tests are insufficient to reveal correctness issues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with pure self-critique methods — execution-based feedback is more objective and often more helpful for weaker models than free-form textual critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8738.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STaR (Self-Taught Reasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative method that bootstraps reasoning by generating rationales and using corrected outputs to fine-tune the model, thereby improving multi-step reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Star: Self-taught reasoner bootstrapping reasoning with reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>STaR (rationale generation + supervised correction loop)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates chain-of-thought rationales and answers; incorrect answers are corrected (either by prompting for improved rationale given the correct answer or by filtering), and the rationale-answer pairs are used to fine-tune the model iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reasoning tasks where chain-of-thought rationales can be generated and used as supervision to train improved reasoning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Rationale-based self-supervision: generating explanations and using corrected rationale+answer pairs in supervised fine-tuning (in-weight updates).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey lists STaR as a critique-free iterative correction method that uses rationale generation to bootstrap reasoning and produce training data for fine-tuning; original literature reports improvements though survey does not include numeric values.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Risk of feedback loop degradation if generated rationales are low-quality; theoretical concerns about loss of diversity and model collapse when training on self-generated data are noted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>STaR contrasts with purely in-context reflection (no weight updates) by using generated data to fine-tune model weights (in-weight updating).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8738.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Rewarding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Rewarding Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where the LLM predicts reward scores for its own outputs and constructs preference pairs for iterative self-training or contrastive fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rewarding language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Rewarding (model-as-reward-predictor for self-training)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model generates responses then predicts reward/confidence scores for them (model-as-a-judge); these self-assigned rewards are used to build preference pairs or train via contrastive/self-training objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction following, reasoning, role-play (general generation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse generation tasks where relative preference signals can be used to rank and fine-tune outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model-as-critic scoring (prompted reward prediction) used as supervision signal for contrastive/self-training updates (in-weight).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites Self-Reward as an approach to build preference pairs and support iterative self-training; qualitative claim that it aids alignment and improvement but no numeric results in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes general concerns about self-generated supervision and that reward prediction by the same model can be biased; potential for reinforcement of existing errors if reward predictions are miscalibrated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to using external reward models or human labels: self-rewarding is cheaper but potentially less reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8738.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8738.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CR/ISR-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ISR-LLM (Iterative Self-Refined LLM) / RCI (revision via critique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative planning and refinement methods for long-horizon sequential planning where the model uses validators/critics to iteratively propose and revise multi-step plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Iterative self-refined large language model for long-horizon sequential task planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ISR-LLM / RCI (iterative plan critique & revision)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LLM proposes a plan, uses a validator/critic (self or external) to detect issues, and refines the plan iteratively until a valid action sequence is produced; applied to sequential task planning and agent action planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-horizon sequential task planning / agent planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step plans where each step can be validated or simulated and revised.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Critique-based iterative refinement using a validator (which may be an external tool or model) and repeated planning cycles in working memory or in-context prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey highlights ISR-LLM and RCI as examples where validation-guided iterative refinement produces better plans for multi-step tasks; evidence is qualitative in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dependent on the quality of validator and the model's ability to interpret critiques; iterative loops can incur compute and latency overhead and may not converge for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to single-pass planners or non-iterative generation; iterative critique-and-revise is presented as more robust for long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Self-Evolution of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>CRITIC: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 2)</em></li>
                <li>Constitutional ai: Harmlessness from ai feedback <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models are better reasoners with self-verification <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Star: Self-taught reasoner bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Self-rewarding language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8738",
    "paper_id": "paper-269294023",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (verbal reinforcement learning / self-reflection)",
            "brief_description": "An agent-level method enabling an LLM to use runtime/environment feedback and verbalize reflections (thoughts) to iteratively improve decisions and code via internal working memory updates rather than weight updates.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Reflexion (verbal RL / generate-then-reflect)",
            "reflection_method_description": "The model obtains runtime or environment feedback (e.g., code interpreter errors or tool outputs), then generates explicit reflective thoughts and updated plans in working memory; these reflections are used to modify subsequent generations in an iterative loop (verbal reinforcement learning). Number of iterations not specified in survey.",
            "task_name": "Code generation, agent decision-making",
            "task_description": "Tasks where model outputs can be executed or evaluated by an environment (e.g., running code, embodied agent actions) and provide run-time feedback enabling reflection.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Working-memory / in-context verbal reflections using environment run-time feedback (prompted generation of thoughts), not in-weight updates.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey reports that Reflexion uses run-time feedback from interpreters/tools and generates reflections to improve outputs and decision-making qualitatively; presented as an effective in-context/self-reflective strategy for agents.",
            "limitations_or_failure_cases": "Survey notes mixed evidence across self-reflection methods generally; weaker models may struggle to self-critique and require environment feedback — Reflexion depends on informative environment/tool feedback and may be limited when such feedback is absent or ambiguous.",
            "comparison_to_other_methods": "Positioned relative to critique-based and interactive methods: Reflexion emphasizes in-context verbal RL and working-memory updates versus in-weight fine-tuning or purely metric-based filtering.",
            "ablation_study_results": null,
            "uuid": "e8738.0",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (iterative refinement with self-feedback)",
            "brief_description": "An approach where an LLM generates natural-language critiques of its own outputs and iteratively revises answers conditioned on that self-feedback without additional training.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-Refine (iterative self-critique and revision)",
            "reflection_method_description": "The model produces an output, then generates natural-language feedback/critiques on that output (few-shot prompting) and uses the feedback to produce revised outputs; the process can be iterated until satisfactory — specific iteration counts are task-dependent and not enumerated in survey.",
            "task_name": "General answer generation / reasoning tasks",
            "task_description": "Tasks where the model can generate an answer and also generate a textual critique/diagnosis of errors (e.g., open-ended QA, reasoning or code explanation).",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-engineered self-critique: model-as-critic prompts that produce textual critiques which are fed back into subsequent prompts (no external learned critic required).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey describes Self-Refine as enabling models to iteratively refine outputs using self-generated feedback and lists it among critique-based correction methods; evidence referenced in original work but no quantitative numbers provided in survey.",
            "limitations_or_failure_cases": "Survey notes that weaker models may struggle with effective self-critique, implying reliance on a sufficiently capable self-critic; also general concerns about self-generated data quality and potential error accumulation across iterations.",
            "comparison_to_other_methods": "Contrasted with critique-free corrections (which rely on objective signals) and with environment/tool-based critiques; Self-Refine uses open-ended textual critique rather than binary metrics.",
            "ablation_study_results": null,
            "uuid": "e8738.1",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CRITIC",
            "name_full": "CRITIC (tool-interactive critiquing for self-correction)",
            "brief_description": "A method that allows LLMs to self-correct outputs by interacting with tools and using tool-produced critiques to revise outputs.",
            "citation_title": "CRITIC: Large language models can self-correct with tool-interactive critiquing",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Tool-interactive critiquing (CRITIC)",
            "reflection_method_description": "The LLM invokes tools or environment executions to obtain diagnostic critiques or error messages, then conditions on those critiques to revise and improve outputs iteratively.",
            "task_name": "General domains where tool execution yields diagnostic feedback (e.g., code, tool-augmented tasks)",
            "task_description": "Tasks that produce interpretable tool feedback such as error messages or execution traces enabling targeted revisions.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Integration of external tools and their feedback into the model's prompt loop; critique-based correction grounded in tool outputs.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey highlights CRITIC as an example of critique-based refinement showing that tool-provided critiques can be used to revise outputs and improve correctness qualitatively.",
            "limitations_or_failure_cases": "Dependent on availability and informativeness of tool/environment feedback; survey emphasizes that tool-interaction helps weaker models but does not provide numerical failure modes.",
            "comparison_to_other_methods": "Compared with self-generated critiques: CRITIC leverages external (tool-derived) critiques which may be more reliable for weaker models than purely self-generated critiques.",
            "ablation_study_results": null,
            "uuid": "e8738.2",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CAI",
            "name_full": "Constitutional AI (CAI)",
            "brief_description": "A self-critique and revision framework where the model generates critiques guided by a constitution (set of principles) and revises outputs according to those critiques to improve alignment/safety.",
            "citation_title": "Constitutional ai: Harmlessness from ai feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Constitution-guided self-critique and revision (CAI)",
            "reflection_method_description": "The model produces critiques of its own outputs according to predefined principles (a 'constitution') and generates revised outputs that better conform to those principles; typically used during supervised fine-tuning data generation or alignment stages.",
            "task_name": "Alignment / harmlessness / response generation",
            "task_description": "Tasks focused on aligning model outputs with safety and ethical principles, including harmlessness and reducing bias.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Prompt engineering using a constitution: LLM-as-critic produces critiques and revisions following explicit rule sets; can be used to generate supervised fine-tuning data.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey lists CAI among critique-based correction methods and notes it significantly improves initial models in supervised settings (qualitative statement; no numbers included in survey).",
            "limitations_or_failure_cases": "Survey raises general concerns about reliance on self-generated supervision and potential for model biases in critiques; CAI efficacy may depend on strength of the constitution and model critic capabilities.",
            "comparison_to_other_methods": "CAI is compared to other critique-based approaches; distinct in using a structured set of principles rather than free-form self-feedback.",
            "ablation_study_results": null,
            "uuid": "e8738.3",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (sampling multiple reasoning paths and voting)",
            "brief_description": "A metric-free filtering/refinement technique that samples multiple chain-of-thought reasoning paths and selects the final answer by majority/agreement to improve reliability.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-Consistency (generate-many + majority selection)",
            "reflection_method_description": "Generate multiple reasoning chains (CoT) per question and filter/select the answer with highest agreement across those chains; this is a single-step ensemble/refinement method rather than iterative self-critique.",
            "task_name": "Multi-step reasoning / arithmetic and logical reasoning benchmarks",
            "task_description": "Tasks benefiting from chain-of-thought where consistent answers across diverse reasoning paths indicate higher confidence in correctness.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Metric-free internal consistency: sampling diversity in in-context CoT outputs and selecting answers with highest inter-sample agreement.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey notes Self-Consistency is used to filter and improve high-confidence self-training data and is reported in literature to improve CoT reliability (survey does not give specific numeric gains).",
            "limitations_or_failure_cases": "Relies on diversity of sampled paths; may not help if model is consistently wrong across samples or if reasoning diversity is low; survey highlights general risk of model collapse when training on self-generated data.",
            "comparison_to_other_methods": "Contrasted with metric-based filtering (which uses external metrics) and critique-based corrections; Self-Consistency is internal and metric-free.",
            "ablation_study_results": null,
            "uuid": "e8738.4",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-Verification",
            "name_full": "Self-Verification",
            "brief_description": "A method where an LLM verifies candidate answers by checking internal consistency or re-deriving key values to score/verifying candidates and pick the most consistent answer.",
            "citation_title": "Large language models are better reasoners with self-verification",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-Verification (internal verification and selection)",
            "reflection_method_description": "Model generates candidate answers and performs verification steps (e.g., recomputation, checking conditions) to score and select the most verifiable candidate; used for tasks like math or clinical information extraction.",
            "task_name": "Reasoning / clinical information extraction / math",
            "task_description": "Tasks where candidate answers can be algorithmically or logically verified against conditions or evidence.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted verification steps and consistency scoring performed by the same LLM (internal metric-free verification or simple scoring functions).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites Self-Verification as an internal filtering/selecting approach that yields higher-confidence answers and was applied in clinical information extraction (qualitative report; no numerical values in survey).",
            "limitations_or_failure_cases": "Effectiveness depends on the model's ability to perform reliable verification steps; may fail if verification logic is complex or model hallucination undermines checking.",
            "comparison_to_other_methods": "Compared with Self-Consistency and metric-based filtering; Self-Verification focuses on logical/evidence checks rather than majority voting.",
            "ablation_study_results": null,
            "uuid": "e8738.5",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-Debugging",
            "name_full": "Self-Debug / Self-Debugging (execution-based debugging)",
            "brief_description": "A code generation/self-improvement approach where the model executes generated code (or uses interpreters) and uses execution traces/errors to iteratively debug and correct code.",
            "citation_title": "Teaching large language models to self-debug",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Execution-feedback-driven debugging (Self-Debugging)",
            "reflection_method_description": "Generate code, run it on interpreter/test cases, collect execution/error feedback (tracebacks), then prompt the model to analyze errors and produce corrected code iteratively until tests pass.",
            "task_name": "Code generation and program repair",
            "task_description": "Code synthesis tasks where generated programs can be executed and tested to produce concrete error feedback.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Environment feedback from code execution integrated into prompt loop; critique-free correction via objective test signals and error traces.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey indicates execution feedback (test cases, interpreters) provides strong precise guidance for iterative improvement and is used by several works (Self-Debugging, Self-Evolve, Reflexion) to improve code quality.",
            "limitations_or_failure_cases": "Requires executable environment and good test coverage; may be limited when runtime errors are non-informative or tests are insufficient to reveal correctness issues.",
            "comparison_to_other_methods": "Contrasted with pure self-critique methods — execution-based feedback is more objective and often more helpful for weaker models than free-form textual critiques.",
            "ablation_study_results": null,
            "uuid": "e8738.6",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "STaR",
            "name_full": "STaR (Self-Taught Reasoner)",
            "brief_description": "An iterative method that bootstraps reasoning by generating rationales and using corrected outputs to fine-tune the model, thereby improving multi-step reasoning performance.",
            "citation_title": "Star: Self-taught reasoner bootstrapping reasoning with reasoning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "STaR (rationale generation + supervised correction loop)",
            "reflection_method_description": "Model generates chain-of-thought rationales and answers; incorrect answers are corrected (either by prompting for improved rationale given the correct answer or by filtering), and the rationale-answer pairs are used to fine-tune the model iteratively.",
            "task_name": "Multi-step reasoning benchmarks",
            "task_description": "Reasoning tasks where chain-of-thought rationales can be generated and used as supervision to train improved reasoning behavior.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Rationale-based self-supervision: generating explanations and using corrected rationale+answer pairs in supervised fine-tuning (in-weight updates).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey lists STaR as a critique-free iterative correction method that uses rationale generation to bootstrap reasoning and produce training data for fine-tuning; original literature reports improvements though survey does not include numeric values.",
            "limitations_or_failure_cases": "Risk of feedback loop degradation if generated rationales are low-quality; theoretical concerns about loss of diversity and model collapse when training on self-generated data are noted in the survey.",
            "comparison_to_other_methods": "STaR contrasts with purely in-context reflection (no weight updates) by using generated data to fine-tune model weights (in-weight updating).",
            "ablation_study_results": null,
            "uuid": "e8738.7",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-Rewarding",
            "name_full": "Self-Rewarding Language Models",
            "brief_description": "A method where the LLM predicts reward scores for its own outputs and constructs preference pairs for iterative self-training or contrastive fine-tuning.",
            "citation_title": "Self-rewarding language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-Rewarding (model-as-reward-predictor for self-training)",
            "reflection_method_description": "The model generates responses then predicts reward/confidence scores for them (model-as-a-judge); these self-assigned rewards are used to build preference pairs or train via contrastive/self-training objectives.",
            "task_name": "Instruction following, reasoning, role-play (general generation tasks)",
            "task_description": "Diverse generation tasks where relative preference signals can be used to rank and fine-tune outputs.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Model-as-critic scoring (prompted reward prediction) used as supervision signal for contrastive/self-training updates (in-weight).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites Self-Reward as an approach to build preference pairs and support iterative self-training; qualitative claim that it aids alignment and improvement but no numeric results in survey.",
            "limitations_or_failure_cases": "Survey notes general concerns about self-generated supervision and that reward prediction by the same model can be biased; potential for reinforcement of existing errors if reward predictions are miscalibrated.",
            "comparison_to_other_methods": "Compared to using external reward models or human labels: self-rewarding is cheaper but potentially less reliable.",
            "ablation_study_results": null,
            "uuid": "e8738.8",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CR/ISR-LLM",
            "name_full": "ISR-LLM (Iterative Self-Refined LLM) / RCI (revision via critique)",
            "brief_description": "Iterative planning and refinement methods for long-horizon sequential planning where the model uses validators/critics to iteratively propose and revise multi-step plans.",
            "citation_title": "Iterative self-refined large language model for long-horizon sequential task planning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "ISR-LLM / RCI (iterative plan critique & revision)",
            "reflection_method_description": "LLM proposes a plan, uses a validator/critic (self or external) to detect issues, and refines the plan iteratively until a valid action sequence is produced; applied to sequential task planning and agent action planning.",
            "task_name": "Long-horizon sequential task planning / agent planning",
            "task_description": "Tasks requiring multi-step plans where each step can be validated or simulated and revised.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Critique-based iterative refinement using a validator (which may be an external tool or model) and repeated planning cycles in working memory or in-context prompts.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey highlights ISR-LLM and RCI as examples where validation-guided iterative refinement produces better plans for multi-step tasks; evidence is qualitative in the survey.",
            "limitations_or_failure_cases": "Dependent on the quality of validator and the model's ability to interpret critiques; iterative loops can incur compute and latency overhead and may not converge for complex tasks.",
            "comparison_to_other_methods": "Compared to single-pass planners or non-iterative generation; iterative critique-and-revise is presented as more robust for long-horizon planning.",
            "ablation_study_results": null,
            "uuid": "e8738.9",
            "source_info": {
                "paper_title": "A Survey on Self-Evolution of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "CRITIC: Large language models can self-correct with tool-interactive critiquing",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Constitutional ai: Harmlessness from ai feedback",
            "rating": 2,
            "sanitized_title": "constitutional_ai_harmlessness_from_ai_feedback"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models are better reasoners with self-verification",
            "rating": 2,
            "sanitized_title": "large_language_models_are_better_reasoners_with_selfverification"
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Star: Self-taught reasoner bootstrapping reasoning with reasoning",
            "rating": 2,
            "sanitized_title": "star_selftaught_reasoner_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Self-rewarding language models",
            "rating": 2,
            "sanitized_title": "selfrewarding_language_models"
        }
    ],
    "cost": 0.018585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Self-Evolution of Large Language Models
3 Jun 2024</p>
<p>Zhengwei Tao 
Key Lab of HCST (PKU)
MOE, SCS
Peking University</p>
<p>Alibaba Group</p>
<p>Ting-En Lin 
Alibaba Group</p>
<p>Xiancai Chen 
Key Lab of HCST (PKU)
MOE, SCS
Peking University</p>
<p>Hangyu Li 
Alibaba Group</p>
<p>Yuchuan Wu 
Alibaba Group</p>
<p>Yongbin Li 
Zhi Jin zhijin@pku.edu.cn 
Key Lab of HCST (PKU)
MOE, SCS
Peking University</p>
<p>Alibaba Group</p>
<p>Fei Huang 
Alibaba Group</p>
<p>Dacheng Tao 
Nanyang Technological University</p>
<p>Jingren Zhou 
Alibaba Group</p>
<p>Refinement Solution Evolution</p>
<p>A Survey on Self-Evolution of Large Language Models
3 Jun 2024F29DE0C91827E519E4515EB8C06B19F8arXiv:2404.14387v2[cs.CL]Large Language ModelSelf-EvolutionSelf-ImprovementSelf-TrainingAutonomous Agents
Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications.However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase.To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing.This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence.In this work, we present a comprehensive survey of self-evolution approaches in LLMs.We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation.Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module.Lastly, we pinpoint existing challenges and propose future directions to improve selfevolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs.Our corresponding GitHub repository is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM.†</p>
<p>Introduction</p>
<p>With the rapid development of artificial intelligence, large language models (LLMs) like GPT-3.5 [83], GPT-4 [1], Gemini [113], LLaMA [115,116], and Qwen [8] mark a significant shift in language understanding and generation.These models undergo three stages of development as shown in Figure 1: pre-training on large and diverse corpora to gain a general understanding of language and world knowledge [14,30], followed by supervised fine-tuning to elicit the abilities of downstream tasks [25,91].Finally, the human preference alignment training enables the LLMs to respond as human behaviors [83].Such successive training paradigms achieve significant breakthroughs, enabling LLMs to perform a wide range of tasks with remarkable zeroshot and in-context capabilities, such as question answering [109], mathematical reasoning [26], code generation [71], and task-solving that require interaction with environments [73].</p>
<p>Despite these advancements, humans anticipate that the emerging generation of LLMs can be tasked with assignments of greater complexity, such as scientific discovery [81] and future events forecasting [95].However, current LLMs encounter challenges in these sophisticated tasks due to the inherent difficulties in modeling, annotation, and the evaluation associated with existing training paradigms [15].Furthermore, the recently developed Llama-3 model has been trained on an extensive corpus comprising 15 trillion tokens 1 .It's a monumental volume of data, suggesting that significantly scaling model performance by adding more real-world data could pose a limitation.</p>
<p>This has attracted interest in self-evolving mechanisms for LLMs, akin to the natural evolution of human intelligence and illustrated by AI developments in gaming, such as the transition from AlphaGo [101] to AlphaZero [102].AlphaZero's self-play method, requiring no labeled data, showcases a path forward for LLMs to surpass current limitations and achieve superhuman performance without intensive human supervision.</p>
<p>Drawing inspiration from the paradigm above, research on the self-evolution of LLMs has rapidly increased at different stages of model development, such as self-instruct [127], self-play [117], self-improving [52], and self-training [42].Notably, DeepMind's AMIE system [117] outperforms primary care physicians in diagnostic accuracy, and Microsoft's WizardLM-2 2exceeds the performance of the initial version of GPT-4.Both models are developed using self-evolutionary frameworks with autonomous learning capabilities and represent a potential LLM training paradigm shift.However, the relationships between these methods remain unclear, lacking systematic organization and analysis.</p>
<p>Therefore, we first comprehensively investigate the self-evolution processes in LLMs and establish a conceptual framework for their development.This self-evolution is characterized by an iterative cycle involving experience acquisition, experience refinement, updating, and evaluation, as shown in Figure 2.During the cycle, an LLM initially gains experiences through evolving new tasks and generating corresponding solutions, subsequently refining these experiences to obtain better supervision signals.After updating the model in-weight or in-context, the LLM is evaluated to measure progress and set new objectives.</p>
<p>The concept of self-evolution in LLMs has sparked considerable excitement across various research communities, promising a new era of models that can adapt, learn, and improve autonomously, akin to human evolution in response to changing environments and challenges.Self-evolving LLMs are not only able to transcend the limitations of current static, data-bound models but also mark a shift toward more dynamic, robust, and intelligent systems.This survey deepens understanding of the emerging field of self-evolving LLMs by providing a comprehensive overview through a structured conceptual framework.We trace the field's evolution from the past to the latest cutting-edge methods and applications while examining existing challenges and outlining future research directions, paving the way for significant advances in developing self-evolution frameworks and next-generation models.</p>
<p>The survey is organized as follows: We first present the overview of self-evolution ( § 2), including background and conceptual framework.We summarize existing evolving abilities and domains of current methods( § 3).Then, we provide in-depth analysis and discussion on the latest advancements in different phases of the self-evolution process, including experience acquisition ( § 4), experience refinement ( § 5), updating ( § 6), and evaluation ( § 7).Finally, we outline open problems and prospective future directions ( § 8).</p>
<p>Overview</p>
<p>In this section, we will first discuss the background of self-evolution and then introduce the proposed conceptual framework.</p>
<p>Fig. 2 Conceptual framework of self-evolution.For the t th iteration: E t is the evolution objective; T t and Y t denote the task and solution; F t represents feedback; M t is the current model.Refined experiences are marked as T t and Ỹt , leading to the evolved model M .ENV is the environment.The whole selfevolution starts at E 1 .</p>
<p>Background</p>
<p>Self-Evolution in Artificial Intelligence.Artificial Intelligence represents an advanced form of intelligent agent, equipped with cognitive faculties and behaviors mirroring those of humans.The aspiration of AI developers lies in enabling AI to harness self-evolutionary capabilities, paralleling the experiential learning processes characteristic of human development.The concept of self-evolution in AI emerges from the broader fields of machine learning and evolutionary algorithms [7].Initially influenced by the principles of natural evolution, such as selection, mutation, and reproduction, researchers have developed algorithms that simulate these processes to optimize solutions to complex problems.The landmark paper by Holland [47], which introduced the genetic algorithm, marks a foundational moment in the history of AI's capability for self-evolution.Subsequent developments in neural networks and deep learning have furthered this capability, allowing AI systems to modify their own architectures and improve performance without human intervention [74].Can Artificial Entities Evolve Themselves?Philosophically, the question of whether artificial entities can self-evolve touches on issues of autonomy, consciousness, and agency.While some philosophers argue that true self-evolution in AI would require some form of consciousness or self-awareness, others maintain that mechanical self-improvement through algorithms does not constitute genuine evolution [16].This debate often references the works of thinkers like Dennett [28], who explore the cognitive processes under human consciousness and contrast them with artificial systems.Ultimately, the philosophical inquiry into AI's capacity for self-evolution remains deeply intertwined with interpretations of what it means to 'evolve' and whether such processes can purely be algorithmic or must involve emergent consciousness [97].</p>
<p>Conceptual Framework</p>
<p>In the conceptual framework of self-evolution, we describe a dynamic, iterative process mirroring the human ability to acquire and refine skills and knowledge.This framework is encapsulated within Figure 2, emphasizing the cyclical nature of learning and improvement.Each iteration of the process focuses on a specific evolution goal, allowing the model to engage in relevant tasks, optimize its experiences, update its architecture, and evaluate its progress before moving to the next cycle.</p>
<p>Experience Acquisition</p>
<p>At the t th iteration, the model identifies an evolution objective E t .Guided by this objective, the model embarks on new tasks T t , generating solutions Y t and receiving feedback F t from the environment, ENV.This stage culminates in the acquisition of new experiences (T t , Y t , F t ).</p>
<p>Experience Refinement</p>
<p>After experience acquisition, the model examines and refines these experiences.This involves discarding incorrect data and enhancing imperfect ones, resulting in refined outcomes ( T t , Ỹt ).</p>
<p>Updating</p>
<p>Leveraging the refined experiences, the model undergoes an update process, integrating ( T t , Ỹt ) into its framework.This ensures the model remains current and optimized.</p>
<p>Evaluation</p>
<p>The cycle concludes with an evaluation phase, where the model's performance is assessed through an evaluation in external environment.The outcomes of this phase inform the objective E t+1 , setting the stage for the subsequent iteration of selfevolution.</p>
<p>The conceptual framework outlines the self-evolution of LLMs, akin to human-like acquisition, refinement, and autonomous learning processes.We illustrate our taxonomy in Figure 3.</p>
<p>Evolution Objectives</p>
<p>Evolution objectives in self-evolving LLMs serve as predefined goals that autonomously guide their development and refinement.Much like humans set personal objectives based on needs and desires, these objectives are crucial as they determine how the model iteratively self-updates.They enable the LLM to autonomously learn from new data, optimize algorithms, and adapt to changing environments, effectively "feeling" its needs from feedback or self-assessment and setting its own goals to enhance functionality without human intervention.</p>
<p>We define an evolution objective as combining an evolving ability and an evolution direction.An evolving ability stands for an innate and detailed skill.The evolution direction is the aspect of the evolution objective aiming to improve.We formulate the evolution objective as follows:
E t = (A t , D t ),
(1) where E t is the evolution objective, composed by evolving abilities A t and evolution directions D t .Take "reasoning accuracy improving" as an example, "reasoning" is the evolving ability and "accuracy improving" is the evolution direction.</p>
<p>Evolving Abilities</p>
<p>In Table 1, we summarize and categorize the targeted evolving abilities in current self-evolution research into two groups: LLMs and LLM Agents.</p>
<p>LLMs</p>
<p>These are fundamental abilities underlying a broad spectrum of downstream tasks.Instruction Following: The capability to follow instructions is essential for effectively applying language models.It allows these models to address specific user needs across different tasks and domains, aligning their responses within the given context [134].Reasoning: LLMs can self-evolve to recognize statistical patterns, making logical connections and deductions based on the information.They evolve to perform better reasoning involving methodically dissecting problems in a logical sequence [27].Math: LLMs enhance the intricate ability to solve mathematical problems covering arithmetic, math word, geometry, and automated theorem proving [2] towards self-evolution.Coding: Methods improve the LLM coding abilities to generate more precise and robust programs [103,150].Furthermore, EvoCodeBench [62] provides an evolving benchmark that updates periodically to prevent data leakage.Role-Play: It involves an agent understanding and acting out a particular role within a given context.This is crucial in scenarios where the model must fit into a social structure or follow a set of behaviors associated with a specific identity or function [77].Others: Apart from the above fundamental evolution objectives, self-evolution can also achieve and a wide range of NLP tasks [42,59,106,152,153].
METHOD ACQUISITION REFINEMENT f R UPDATING f U OBJECTIVE E TASK f T SOLUTION f Y FEEDBACK f F LARGE LANGUAGE MODELS Self-Align [108] Knowledge-Based Pos-G - Filtering
In-W IF SciGLM [152] Knowledge-Based ---In-W Other EvIT [110] Knowledge-Based ---In-W Reasoning MEEL [111] Knowledge-Based ---In-W Reasoning UltraChat [32] Knowledge-Based ---In-W Role-Play SOLID [6] Knowledge-Based Pos-S -Filtering In-W Role-Play Ditto [77] Knowledge-Based Pos-S, Neg-P --In-W Role-Play MetaMath [146] Knowledge-Free Pos-R --In-W Math Self-Rewarding [148] Knowledge-Free -Model -In-W IF,Reasoning,Role-Play Kun [158] Knowledge-Free --Filtering In-W IF,Reasoning PromptBreeder [36] Knowledge-Free ---In-C Math, Reasoning Ada-Instruct [27] Knowledge-Free ---In-W Math, Reasoning, Code Backtranslation [67] Knowledge-Free ---In-W IF DiverseEvol [131] Selective ---In-W Code Grath [21] Selective  [140] , LLM Explanation [157] , ChatEval [17] Quantitative LLM-as-a-Judge [34,157] , Reward Score [83] Updating ( §6)</p>
<p>In-Context Working Memory: Reflexion [98], IML [121], Evolu-tionaryAgent [65] , Agent-Pro [154], ProAgent [151] External Memory: MoT [66], MemoryBank [161], TiM [72], IML [121], TRAN [140], MemGPT [84] UA 2 [142] , ICE [89], AesopAgent [123] In-Weight Architecture: LoRA [50], ConPET [104], Model Soups [130], DAIR [147], UltraFuser [33], EvoLLM [3] Regularization: InstuctGPT [83], FuseLLM [119] , Elastic Reset [82], WARM [92], AMA [69] Replay: ReST [4,42] , AMIE [117], SOTOPIA-π [125], LLM2LLM [60], LTC [124], A 3 T [141], SSR [53], SDFT [143] Experience Refinement ( §5) Correcting Critique-Free: STaR [149], Self-Debugging [22], IterRefinement [20], Clinical SV [40] Critique-Based: Self-Refine [80], CAI [9], RCI [57] , SELF [76], CRITIC [41], SelfEvolve [56], ISR-LLM [164], Reflexion [98] Filtering Metric-Free: Self-Consistency [126] , LMSI [52], Self-Verification [129] , CodeT [18] Metric-Based: ReST EM [103], AutoAct [90], Self-Talk [118], Self-Instruct [127] Experience Acquisition ( §4) Feedback ( §4.3) Environment SelfEvolve [56], Self-Debugging [22], Reflexion [98], CRITIC [41], RoboCat [13], SinViG [135], SOTOPIA-π [125] Model Self-Reward [148], LSX [106], DLMA [70], SIRLC [85], Self-Alignment [155], CAI [9], Self-Refine [80] Solution ( §4.2)</p>
<p>Negative Perturbative: RLCD [138], DLMA [70], Ditto [77] Contrastive: Self-Reward [148], SPIN [24], GRATH [21], Self-Contrast [153], ETO [105], A 3 T [141], STE [120], COTERRORSET [114] Positive Grounded: Self-Align [108], SALMON [107], MemoryBank [161], TiM [72], MoT [66], IML [121], TRAN [140], MemGPT [84] Self-Play: Debates [112], Self-Talk [118], Ditto [77], SOLID [6], SOTOPIA-π [125] Interactive: SelfEvolve [56], LDB [159], ETO [105], A 3 T [141], AutoAct [90], KnowAgent [165] Rationale-Based: LMSI [52], STaR [149],
A 3 T [141] Task ( §4.1)
Selective DIVERSE-EVOL [131], SOFT [122], Selective Reflection-Tuning [64], V-STaR [49] Knowledge-Free</p>
<p>Self-Instruct [48,93,127], Ada-Instruct [27], Evol-Instruct [134], Meta-Math [146], PromptBreeder [36], Backtranslation [67], Kun [158] Knowledge-Based Unstructured: UltraChat [32], SciGLM [152], EvIT [110], MEEL [111] Structured: Self-Align [108], Ditto [77], SOLID [6] Fig. 3 Taxonomy of self-evolving large language models.</p>
<p>LLM-based Agents</p>
<p>The abilities discussed here are characteristic of advanced artificial agents used for task-solving or simulations in digital or physical world.These capabilities mirror human cognitive functions, allowing these agents to perform complex tasks and interact effectively in dynamic environments.</p>
<p>Planning: It involves the ability to strategize and prepare for future actions or goals.An agent with this skill can analyze the current state, predict the outcomes of potential actions, and create a sequence of steps to achieve a specific objective [90].</p>
<p>Tool Use: This is the capacity to employ objects or instruments in the environment to perform tasks, manipulate surroundings, or solve problems [165].</p>
<p>Embodied Control: It refers to an agent's ability to manage and coordinate its physical form within an environment.This encompasses locomotion, dexterity, and the manipulation of objects [13].</p>
<p>Communication: It is the skill to convey information and understand messages from other agents or humans.Agents with advanced communication abilities can participate in dialogue, collaborate with others, and adjust their behaviour based on the communication received [118].</p>
<p>Evolution Directions</p>
<p>Examples of evolution directions include but are not limited to: Improving Performance: The goal is to continuously enhance the model's understanding and generation across various languages and abilities.For instance, a model initially trained for question answering and chitchat can autonomously extend its proficiency and develop abilities like diagnostic dialogue [117], social skills [125], and role-playing [77].</p>
<p>Adaptation to Feedback: This involves improving model responses based on feedback to better align with preferences or adapt to environments [108,138].</p>
<p>Expansion of Knowledge Base: The aim is to continuously update the model's knowledge base with the latest information and trends.For example, a model might automatically integrate new scientific research into its responses [132].Safety, Ethic and Reducing Bias: The goal is to identify and mitigate biases in the model's responses, ensuring fairness and safety.One effective strategy is to incorporate guidelines, such as constitutions or specific rules, to identify inappropriate or biased responses and correct them through model updates [9,78].</p>
<p>Experience Acquisition</p>
<p>Exploration and exploitation [44] are fundamental strategies for learning in humans and LLMs.Among that, exploration involves seeking new experiences to achieve objectives and is analogous to the initial phase of LLM self-evolution, known as experience acquisition.This process is crucial for self-evolution, enabling the model to autonomously tackle core challenges such as adapting to new tasks, overcoming knowledge limitations, and enhancing solution effectiveness.Furthermore, experience is a holistic construct, encompassing not only the tasks encountered [31] but also the solutions developed to address these tasks [96], and the feedback [12] received as a result of task performance.</p>
<p>Inspired by that, we divide experience acquisition into three parts: task evolution, solution evolution, and obtaining feedback.In task evolution, LLMs curate and evolve new tasks aligning with evolution objectives.For solution evolution, LLMs develop and implement strategies to complete these tasks.Finally, LLMs may optionally collect feedback from interacting with the environment for further improvements.</p>
<p>Task Evolution</p>
<p>To gain new experience, the model first evolves new tasks according to the evolution objective E t in the current iteration.Task evolution is the crucial step in the engine that starts the entire evolution process.Formally, we denote the task evolution as:
T t = f T (E t , M t ),
(2) where f T is the task evolution function.E t , M t , and T t denote the evolution objective, the model, and the evolved task at iteration t, respectively.We summarize and categorize existing studies on the task evolution method f T into three groups: Knowledge-Based, Knowledge-Free, and Selective.We detail each type in the following parts and show the concepts in Figure 4.</p>
<p>Knowledge-Based</p>
<p>The objective E t may associate with external knowledge to evolve where the knowledge is not inherently comprised in the current LLMs.Explicitly sourcing from knowledge enriches the relevance between tasks and evolution objectives.It also ensures the validity of relevant facts in the tasks.We delve into the Knowledge-Based methods seeking to evolve new tasks of the evolving objective assisted by external information.</p>
<p>The first kind of knowledge is structured.Structured knowledge is dense in information and well-organized.Self-Align [108] guides task generation by covering 20 topics, such as scientific and legal expertise, to ensure diversity.Apart from topic knowledge, DITTO [77] includes character knowledge from Wikidata and Wikipedia.The knowledge comprises attributes, profiles, and concise character details for role-play conversations.SOLID [6] generates structured entity knowledge as conversation starters.
𝓣 𝒕 𝓔 𝒕 Knowledge-Free 𝓣 𝒕 𝓔 𝒕 Knowledge-Based Selective Generate 𝓣 𝒕 𝓔 𝒕 𝕋 Select Generate
Fig. 4 Task evolution.E t and T t are the evolving objective and task of t th iteration.T is the set of all tasks to be selected.The first two are generative methods that differ based on their respective use of knowledge.The third method, in contrast, employs a discriminative approach to select what to learn.</p>
<p>The second group consists of tasks evolving from an unstructured context.Unstructured context is easy to obtain but is sparse in knowledge.UltraChat [32] gathers unstructured knowledge of 20 types of text materials to construct questions or instructions.SciGLM [152] derives questions from the text of diversified science subjects, which covers rich scientific knowledge.EvIT [110] derives event reasoning tasks based on large-scale unstructured events mined from the unsupervised corpus.Similarly, MEEL [111] evolves multi-modal events in both image and text to construct the tasks for MM event reasoning.</p>
<p>Knowledge-Free</p>
<p>Unlike previous methods that require extensive human effort to gather external knowledge, Knowledge-Free approaches operate independently using the evolving object E t and the model itself.These efficient methods can generate more diversified tasks without additional knowledge restrictions.</p>
<p>First, the LLMs can prompt themselves to generate new tasks according to E t .Self-Instruct [48,93,127] is a typical methodology of Knowledge-Free task evolution.These methods self-generate a variety of new task instructions based on evolution objectives.Ada-Instruct [27] further proposes an adaptive task instruction generation strategy that fine-tunes open-source LLMs to generate lengthy and complex task instructions for code completion and mathematical reasoning.</p>
<p>Second, extending and boosting original tasks increases the quality of instructions.WizardLM [134] proposes Evol-Instruct that evolves instruction following tasks with in-depth and in-breadth evolving and further expands it in code generation [79].MetaMath [146] rewrites the question in multiple ways, including rephrasing, self-verification, and FOBAR.It evolves a new MetaMathQA dataset for fine-tuning LLMs to improve mathematical task-solving.Promptbreeder [36] evolves seed tasks via mutation prompts.It further evolves mutation prompts via the hyper mutation prompts to increase the task diversity.</p>
<p>Third, deriving tasks from plain text is another way.Backtranslation [67] extracts self-contained segments in unlabelled data and regards it as the answers to tasks.Similarly, Kun [158] presents a task self-evolving algorithm utilizing instruction harnessed from unlabelled data towards back-translation.</p>
<p>Selective</p>
<p>Instead of task generation, we may start with large-scale existing tasks.At each iteration, LLMs can select tasks that exhibit the highest relevance to the current evolving objective E t without additional generation.This approach obviates the intricate curation of new tasks, streamlining the evolution process [19,63,162].</p>
<p>A simple task selecting method is to randomly sample tasks from the task pool like REST [42], REST em [103], and GRATH [21] do.Rather than random selection, DIVERSE-EVOL [131] introduces a data sampling technique where the model selects new data points based on their distinctiveness in the embedding space, ensuring diversity enhancement in the chosen subset.SOFT [122] then splits the initial training set.Each iteration selects one chunk of the split set as the evolving task.</p>
<p>Li et al [64] propose Selective Reflection-Tuning and select a subset of tasks via a novel metric calculating to what extent the answer is related to the question.V-STaR [49] selects the correct solutions in the previous iteration and adds their task instructions to the task set of the next iteration.</p>
<p>Solution Evolution</p>
<p>After obtaining evolved tasks, LLMs solve the tasks to acquire the corresponding solution.The most common strategy is to generate the solution directly according to the task formulation [42,103,148,149,158].However, this straightforward approach might reach solutions irrelevant to the evolution objective, leading to suboptimal evolution [45].Therefore, solution evolution  uses different strategies to solve tasks and enhance LLM capabilities by ensuring that solutions are not just generated but are also relevant and informative.In this section, we comprehensively survey these strategies and illustrate them in Figure 5.We first formulate the solution-evolution as follows:
Y t = f Y (T t , E t , M t ),(3)
where f Y is the model's strategy to approach the evolution objective.</p>
<p>We then categorize these methods into positive and negative according to the correctness of the solutions.The positive methods introduce various approaches to acquire correct and desirable solutions.On the contrary, negative methods elicit and collect undesired solutions, including unfaithful or mis-align model behaviors, which are then used for preference alignment.We elaborate on the details of each type in the following sections.</p>
<p>Positive</p>
<p>Current studies explore diverse methods beyond vanilla inference for positive solutions to obtain correct solutions aligned with evolution objectives.We categorize the task-solving process into four types: Rationale-Based, Interactive, Self-Play, and Grounded.</p>
<p>Rationale-Based</p>
<p>The model incorporates rationale explanations towards approaching the evolving objective when solving the tasks and can self-evolve by utilizing such rationales.These methods enable models to explicitly acknowledge the evolution objective and complete this task in that direction [10,128,144,145].</p>
<p>Huang et al [52] proposes a method where an LLM self-evolves using "high-confidence" rationale-augmented answers generated for unlabeled questions.Similarly, STaR [149] generates rationale when solving the task.If the answer is wrong, it further corrects the rationale and the answer.Then, it uses the answer and rationale as experiences to fine-tune the model.Similarly, LSX [106] proposes the novel paradigm to generate an explanation of the answer, incorporating an iterative loop between a learner module performing a base task and a critic module that assesses the quality of explanations given by the learner.Song et al [105], Yang et al [141] obtain rationales in the ReAct [144] style when solving the tasks.The rationales are further engaged in training the agents in the following step.</p>
<p>Interactive</p>
<p>Models can interact with the environment to enhance the evolution process.These methods can obtain environmental feedback that is valuable for guiding self-evolution directions.</p>
<p>SelfEvolve and LDB [56,159] improve code generation ability via self-evolution.They allow the model to generate code and acquire feedback via running the code on the interpreter.As another environment, Song et al [105], Yang et al [141] interact in embodied scenarios and acquire feedback.They learn to take proper actions based on their current state.For agent abilities, AutoAct [90] introduces self-planning from scratch, focusing on an intrinsic self-learning process.In this process, agents enhance their abilities through recursive planning iterations with environment feedback.Following AutoAct, [165] further enhances agent training by integrating self-evolution and an external action knowledge base.This approach guides action generation and boosts planning ability through environment-driven corrective feedback loops.</p>
<p>Self-Play</p>
<p>It's the situation where a model learns to evolve by playing against copies of itself.Self-play is a powerful evolving method because it enables systems to communicate with themselves to get feedback in a closed loop.It's especially effective in environments where the model can simulate various sides of the roles, like multi-player games [101,102].Compared with interactive methods, self-play is an effective strategy to obtain feedback without an environment.</p>
<p>Taubenfeld et al [112] investigate the systematic biases in simulations of debates by LLMs.On the contrary to debating, Ulmer et al [118] engage LLMs in conversations following generated principles.Another kind of conversation via role-playing.Lu et al [77] proposes self-simulated role-play conversation.The process involves instructing the LLM with character profiles and aligning its responses to maintain consistency with the character's knowledge and style.Similarly, Askari et al [6] propose SOLID to generate large-scale intent-aware role-play dialogues.This self-playing aspect harnesses the expansive knowledge of LLMs to construct information-rich exchanges that streamline the dialog generation process.Wang et al [125] introduces a novel approach whereby each LLM follows a role and communicates with each other to achieve their goals.</p>
<p>Grounded</p>
<p>To reach the evolving objective and reduce exploration space, models can be grounded on existing rules [108] and previous experiences for further explicit guidance when solving the tasks.</p>
<p>LLMs can generate desirable solutions more effectively by being grounded on pre-defined rules and principles.For instance, Self-Align [108] generated self-evolved questions with principle-driven constraints to guide the task-solving process.SALMON [107] design a set of combined principles that requires the model to follow when solving the task.Self-Talk [118] ensures the LLMs generate a workflow-aligned conversation based on preset agent characters.They generate the workflow in advance based on GPT-4.</p>
<p>Besides pre-defined rules, grounding on previous experiences can improve the solutions.MemoryBank [161] and TiM [72] answer current questions by incorporating previous question-answer records.Rather than previous solution histories, MoT [66], IML [121], and TRAN [140] incorporate induced rules from the histories to answer new questions.MemGPT [84] combines these merits and retrieves previous questions, solutions, induced events, and user portrait knowledge.</p>
<p>Negative</p>
<p>In addition to acquiring positive solutions, recent research illustrates that LLMs can benefit from negative ones for selfimprovement [140].This strategy is analogous to trial and error in human behavior when learning skills.This section summarises typical methods of gaining negative solutions to assist in self-evolution.</p>
<p>Contrastive</p>
<p>A widely used group of methods is to collect multiple solutions for a task and then contrast the positive and negative ones to get improvements.</p>
<p>For instance, Self-Reward [148] generates responses, predicts corresponding rewards by the model itself, and constructs preference pairs based on rewards.SPIN [24] uses the results generated by the current model as negatives and the ground truth of SFT as positives for iterative self-training.Similarly, GRATH [21] generates both correct and incorrect answers.It then trains the model by comparing these two answers.Self-Contrast [153] contrasts the differences and summarizes these discrepancies into a checklist that could be used to re-examine and eliminate discrepancies.In ETO [105], the model interacts with the embodied environment to complete tasks and optimizes from the failure solutions.A 3 T [141] improves ETO by adding rationale after each action for solving tasks.STE [120] implements trial and error where the model solves the tasks with unfamiliar tools.It learns by analyzing failed attempts to improve problem-solving strategies in future tasks.More recently, COTERRORSET [114] obtains incorrect solutions generated by PALM-2 and proposes mistake tuning, which requires the model to void making mistakes.</p>
<p>Perturbative</p>
<p>Compared to Contrastive, Perturbative methods seek to add perturbations to obtain negative solutions intentionally.Models can later learn to avoid generating these negative answers.Adding perturbations to obtain negative solutions is more controllable than contrastive methods.Some methods add perturbation to generate harmful solutions [70,138].Given a task, RLCD [138] curates both positive and negative instructions and generates positive and negative solutions.DLMA [70] gathers both positive and negative instructional prompts and subsequently produces corresponding positive and negative solutions.</p>
<p>Rather than harmful perturbation, incorporating negative context is another way.Ditto [77] adds negative persona characters to generate incorrect conversations.The model then learns from the negative conversations to evolve persona dialogue ability.</p>
<p>Feedback</p>
<p>As humans learn skills, feedback plays a critical role in demonstrating the correctness of the solutions.This key information enables humans to reflect and then update their skills.Akin to this process, LLMs should obtain feedback during or after the task solution in the cycle of self-evolution.We formalize the process as follows:
F t = f F (T t , Y t , E t , M t ; ENV),(4)
where f F is the method to acquire feedback.</p>
<p>In this part, we summarize two types of feedback.Model feedback refers to gathering the critique or score rated by the LLMs themselves.Besides, Environment denotes the feedback received directly from the external environment.We illustrate these concepts in Figure 6.</p>
<p>Model</p>
<p>Current studies demonstrate that LLMs can play well as a critic [157].In the cycle of self-evolution, the model judges itself to acquire the feedback of the solutions.</p>
<p>One type of feedback is a score that indicates correctness.Self-Reward [148], LSX Stammer et al [106], and DLMA [70] rate their own solutions and output the scores via LLM-as-a-Judge prompting.Similar to that, SIRLC [85] utilizes self-evaluation results of LLM as the reward for further reinforcement learning.Self-Alignment [155] leverages the self-evaluation capability of an LLM to generate confidence scores on the factual accuracy of its outputs.</p>
<p>Another type provides a textual description, offering multi-dimensional information.To alter the distribution of the responses via supervised learning, CAI [9] asks the model to critique its response according to a principle in the constitution.In contrast to supervised learning and reinforcement learning approaches, Self-Refine [80] allows the model to generate natural language feedback on its own output in a few-shot manner.</p>
<p>Environment</p>
<p>Another form of feedback comes from the environment, common in tasks where solutions can be directly evaluated.This feedback is precise and elaborate and can provide sufficient information for model updating.They may be derived from code interpreter [22,56,98], tool execution [41,90], the embodied environment [13,135,164], and other LLMs or agents [112,118,125].For code generation, Self-Debugging Chen et al [22] utilizes execution results on test cases as part of feedback while Self-Evolve [56] receives the error message from the interpreter.Similarly, Reflexion [98] also obtains the run-time feedback from the code interpreter.It then further reflects to generate thoughts.This run-time feedback contains the trace-back information that can point out the key information for improved code generation.</p>
<p>Recently, methods endow tool-using ability to LLMs and agents.Executing tools leading to feedback in return [41,90,105,120,141].</p>
<p>RoboCat [13] and SinViG [135] act in the robotic embodied environment.This type of feedback is precise and strong to guide self-evolution.</p>
<p>Communication feedback is common and effective in LLM-based multi-agent systems.Agents can correct and support each other, enabling co-evluation [112,118,125].</p>
<p>Experience Refinement</p>
<p>After experience acquisition and before updating in self-evolution, LLMs may improve the quality and reliability of their outputs through experience refinement.It helps LLMs adapt to new information and contexts without relying on external resources, leading to more reliable and effective assistance in dynamic environments.This process is formulated as follows:
T t , Ỹt = f R (T t , Y t , F t , E t , M t ),(5)
where f R is the methods of experience refinement, T t , Ỹt are the refined tasks and solutions.We classify the methods into two categories: filtering and correcting.</p>
<p>Filtering</p>
<p>Refinement in self-evolution involves two primary filtering strategies: Metric-Based and Metric-Free.The former uses external metrics to assess and filter outputs, while the latter does not rely on these metrics.This ensures that only the most reliable and high-quality data is utilized for further updating.</p>
<p>Metric-Based</p>
<p>By relying on feedback and pre-defined criteria, metric-based filtering improves the quality of the outputs [90,103,118,127], ensuring the progressive enhancement of LLM capabilities through each iteration of refinement.For example, ReST EM [103] incorporates a reward function to filter the dataset sampled from the current policy.The function provides binary rewards based on the correctness of the generated samples rather than a learned reward model trained on human preferences in ReST [42].AutoAct [90] leverages F1-score and accuracy as rewards for synthetic trajectories and collects trajectories with exactly correct answers for further training.Self-Talk [118] measures the number of completed subgoals to filter the generated dialogues, ensuring that only high-quality data is used for training.To encourage diversity of the source instructions, Self-Instruct [127] automatically filters low-quality or repeated instructions using ROUGE-L similarity and heuristics before adding them to the task pool.</p>
<p>The filtering criteria or metrics are crucial for maintaining the quality and reliability of the generated outputs, thereby ensuring the continuous improvement of the model's capability.</p>
<p>Metric-Free</p>
<p>Some methods seek filtering strategies beyond external metrics, making the process more flexible and adaptable.Metric-Free filtering typically involves sampling outputs and evaluating them based on internal consistency measures or other modelinherent criteria [18,52,129].The filtering in Self-Consistency [126] is based on the consistency of the final answer across multiple generated reasoning paths, with higher agreement indicating higher reliability.LMSI [52] utilizes CoT prompting plus self-consistency for generating high-confidence self-training data.</p>
<p>Designing internal consistency measures that accurately reflect output quality can be challenging.Self-Verification [129] allows the model to select the candidate answer with the highest interpretable verification score, calculated by assessing the consistency between the predicted and original condition values.For the code generation task, CodeT [18] considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples.</p>
<p>These methods emphasize the language model's ability to self-assess and filter its outputs based on internal agreement, showcasing a significant step forward in self-evolution without the direct intervention of external metrics.</p>
<p>Correcting</p>
<p>Recent advancements emphasize the importance of iterative self-correction in LLMs, allowing for the refinement of experiences.This section divides the methods employed into two categories: Critique-Based and Critique-Free correction.Critiques often serve as strong hints that include the rationale behind perceived errors or suboptimal outputs, guiding the model towards improved iterations.</p>
<p>Critique-Based</p>
<p>These methods rely on additional judging processes to draw the critiques of the experiences.Then, the experiences are refined based on the critiques.By leveraging either self-generated [9,76,80,98] or environment-interaction generated critiques [41,56,164], the model benefits from detailed feedback for nuanced correction.</p>
<p>LLMs have demonstrated their ability to identify errors in their outputs.Self-Refine [80] introduces an iterative process in which the model refines its initial outputs conditioned on actionable self-feedback without additional training.To evolve from the correction, CAI [9] generates critiques and revisions of its outputs in the supervised learning phase, significantly improving the initial model.Applied to an agent automating computer tasks, RCI [57] improves its previous outputs based on the critique finding errors in the outputs.</p>
<p>Since weaker models may struggle significantly with the self-critique process, several approaches enable models to correct the outputs using critiques provided by external tools.CRITIC [41] allows LLMs to revise the output based on the critiques obtained during interaction with tools in general domains.SelfEvolve [56] prompts an LLM to refine the answer code based on the error information thrown by the interpreter.ISR-LLM [164] helps the LLM planner find a revised action plan by using a validator in an iterative self-refinement process.</p>
<p>The primary advantage of this method lies in its ability to process and react to detailed feedback, potentially leading to more targeted and nuanced corrections.</p>
<p>Critique-Free</p>
<p>Contrary to critique-based, critique-free methods correct the experiences directly leveraging objective information [20,22,40,149].These methods offer the advantage of independence from nuanced feedback that critiques provide, allowing for corrections that adhere strictly to factual accuracy or specific guidelines without the potential bias introduced by critiques.</p>
<p>One group of critique-free methods modifies the experiences on the signal of whether the task was correctly resolved.Self-Taught Reasoner (STaR) [149] proposes a technique that iteratively generates rationales to answer questions.If the answers are incorrect, the model is prompted again with the correct answer to generate a more informed rationale.Self-Debug [22] enables the model to perform debugging steps by investigating execution results from unit tests and explaining the code on its own.Different from depending on the task-solving signal, other information produced during the solving process can be leveraged.IterRefinement [20] relies on a series of refined prompts that encourage the model to reconsider and improve upon its previous outputs without any direct critique.For information extraction tasks, Clinical SV [40] grounds each element in evidence from the input and prunes inaccurate elements using supplied evidence.</p>
<p>These critique-free approaches simplify the correction mechanism, allowing for easier implementation and faster adjustments.</p>
<p>Updating</p>
<p>After experience refinement, we enter the crucial updating phase that leverages the refined experiences to improve model performance.We formulate updating as follows:
M t+1 = f U ( T t , Ỹt , E t , M t ),(6)
where f U is the updating functions.These update methods keep the model effective by adapting to new experiences and continuously improving performance in changing environments and during iterative training.</p>
<p>We divide these approaches into in-weight learning, which involves updates to model weights, and in-context learning, which involves updates to external or working memory.</p>
<p>In-Weight</p>
<p>Classical training paradigms in updating LLMs in weight encompass continuous pretraining [14,93], supervised fine-tuning [75], and preference alignment [83,115].However, in the iterative training process of self-evolving, the core challenge lies in achieving overall improvement and preventing catastrophic forgetting, which entails refining or acquiring new capabilities while preserving original skills.Solutions to this challenge can be categorized into three main strategies: replay-based, regularization-based, and architecture-based methods.</p>
<p>Replay-based</p>
<p>Replay-based methods reintroduce previous data to retain old knowledge.One is experience replay, which mixes the original and new training data to update LLMs [60,93,124,141,156].For example, Reinforced Self-Training (ReST) [4,42] method iteratively updates large language models by mixing seed training data with filtered new outputs generated by the model itself.AMIE [117] utilizes a self-play simulated learning environment for iterative improvement and mixes generated dialogues with supervised fine-tuning data through inner and outer self-play loops.SOTOPIA-π [125] leverages behavior cloning from the expert model and self-generated social interaction trajectory to reinforce positive behaviors.</p>
<p>Another is generative replay, which adopts the self-generated synthesized data as knowledge to mitigate catastrophic forgetting.For instance, Self-Synthesized Rehearsal (SSR) [53] generates synthetic training instances for rehearsal, enabling the model to preserve its ability without relying on real data from previous training stages.Self-Distillation Fine-Tuning (SDFT) [143] generates a distilled dataset from the model itself to bridge the distribution gap between task datasets and the LLM's original distribution to mitigate catastrophic forgetting.</p>
<p>Regularization-based</p>
<p>Regularization-based methods constrain the model's updates to prevent significant deviations from original behaviors, exemplified by function-and weight-based regularization.Function-based regularization focuses on modifying the loss function that a model optimizes during training [87,160].For example, InstuctGPT [83] employs a per-token KL-divergence penalty from the output probabilities of the initial policy model π SFT on the updated policy model π RL .FuseLLM [119] employs a technique akin to knowledge distillation [46], leveraging the generated probability distributions from source LLMs to transfer the collective knowledge to the target LLM.</p>
<p>Weight-based regularization [58] directly targets the model's weights during training.Techniques such as Elastic Reset [82] counters alignment drift in RLHF by periodically resetting the online model to an exponentially moving average of its previous states.Furthermore, Ramé et al [92] introduced WARM, which combines multiple reward models through weight averaging to address reward hacking and misalignment.Moreover, AMA [69] adaptively average model weights to optimize the trade-off between reward maximization and forgetting mitigation.</p>
<p>Architecture-based</p>
<p>Architecture-based methods explicitly utilize extra parameters or models for updating, including decomposition-and mergingbased approaches.Decomposition-based methods separate large neural network parameters into general and task-specific components and only update the task-specific parameters to mitigate forgetting.LoRA [29,50] inject trainable low-rank matrices to significantly reduce the number of trainable parameters while maintaining or improving model performance across various tasks.This paradigm is later adopted by GPT4tools [139], OpenAGI [39] and Dromedary [108].Dynamic ConPET [104] combines pre-selection and prediction with task-specific LoRA modules to prevent forgetting, ensuring scalable and effective adaptation of LLMs to new tasks.</p>
<p>Merging-based methods, on the other hand, involve combining multiple models or layers to achieve general improvements, including but not limited to merging multiple generic and specialized model weights into a single model [55,130,137,147], through mixture-of-expert approach [33] or even layer-wise merging and up-scaling such as EvoLLM [3].</p>
<p>In-Context</p>
<p>In addition to directly updating model parameters, another approach is to leverage the in-context capabilities of LLMs to learn from experiences, thereby enabling fast adaptive updates without expensive training costs.The methods could be divided into updating external and working memory.</p>
<p>External Memory</p>
<p>This approach utilizes an external module to collect, update, and retrieve past experiences and knowledge, enabling models to access a rich pool of insights and achieve better results without updating model parameters.The external memory mechanism is common in AI Agent systems [89,123,136].This section provides a detailed overview of the latest methods for updating external memory, emphasizing the aspects of memory Content and Updating Operations, and summarized in Table 2.</p>
<p>METHOD CONTENT OPERATION</p>
<p>MoT [66] Experience Insert TRAN [140] Rationale Insert, Reflect MemoryBank [161] Experience, Rationale Insert, Reflect, Forget MemGPT [84] Experience Insert, Forget TiM [72] Rationale Insert IML [121] Rationale Insert, Reflect ICE [89] Rationale Insert, Reflect AesopAgent [123] Experience, Rationale Insert, Reflect</p>
<p>Table 2 Content and operations of updating external memory.</p>
<p>Content: External memory mainly stores two types of content: past experiences and reflected rationale, each serving distinct purposes.For instance, past experience provides valuable historical context, serving as a guiding force toward achieving improved outcomes.MoT [66] archives filtered question-answer pairs to construct a beneficial memory repository.Additionally, the FIFO Queue mechanism in MemGPT [84] maintains a rolling history of messages, encapsulating interactions between agents and users, system notifications, and inputs and outputs of function calls.</p>
<p>On the other hand, reflected rationales offer condensed explanations, such as rules, that support decision-making.For instance, TRAN [140] archives rules inferred from experiences alongside information on mistakes to mitigate future errors.Correspondingly, TiM [72] preserves inductive reasoning, defined as text elucidating the relationships between entities.Moreover, IML [121] and ICE [89] store comprehensive notes and rules derived from a series of trajectories, demonstrating the broad spectrum of content types that memory systems can accommodate.</p>
<p>MemoryBank [161] and AesopAgent [123] reflect on experience and rationale, save them as external memory, and retrieve them when needed to achieve better performance.</p>
<p>Updating Operation: We categorize the operations to the memory into Insert, Reflect, and Forget.The most common operation is insert, methods insert text content into the memory for storage [66,72,84,121,140,161].Another operation is reflection, which is to think and summarize previous experiences to conceptualize rules and knowledge for future use [89,121,140,161].Last, due to the limited storage of memory, forgetting content is crucial to keeping memory efficient and the content valid.MemGPT [84] adopts the FIFO queue to forget the contents.MemoryBank [161] establishes a forgetting curve on the insert time of each item.</p>
<p>Working Memory</p>
<p>The methods use past experience to evolve the capabilities of agents by updating internal memory streams, states, or beliefs, known as working memory, often in the form of verbal cues.Reflexion [98] introduces verbal reinforcement learning for decision-making improvement without conventional model updates.Similarly, IML [121] enables LLM-based agents to autonomously learn and adapt to their environment by summarizing, refining, and updating knowledge based on past experience directly in working memory.</p>
<p>EvolutionaryAgent [65] aligns agents with dynamically changing social norms through evolution and selection principles, leveraging environmental feedback for self-evolution.Agent-Pro [154] employs policy-level reflection and optimization, allowing agents to adapt their behavior and beliefs in interactive scenarios based on past outcomes.Lastly, ProAgent [151] enhances cooperation in multi-agent systems by dynamically interpreting teammates' intentions and adapting behavior.</p>
<p>These collective works demonstrate the importance of integrating past experiences and knowledge into the agents' memory stream to refine their state or beliefs for improved performance and adaptability across various tasks and environments.</p>
<p>Evaluation</p>
<p>Much like the human learning process, it is essential to ascertain whether the present level of ability is adequate and meets the application requirements through evaluation.Furthermore, it is from these evaluations that one can identify the direction for future learning.However, how to accurately assess the performance of an evolved model and provide directions for future improvements is a crucial yet underexplored research area.For a given evolved model M t , we conceptualize the evaluation process as follows:
E t+1 , S t = f E (M t , E t , ENV),(7)
where f E represents the evaluation function that measures the performance score (S t+1 ) of the current model and provide evolving goal (E t+1 ) for the next iteration.Evaluation function f E can be categorized into quantitative and qualitative approaches, each providing valuable insights into model performance and areas for improvement.</p>
<p>Quantitative Evaluation</p>
<p>This method focuses on providing measurable metrics to reliably assess LLM performance, such as automatic [68,86] and human evaluation.However, traditional automatic metrics struggle to accurately evaluate increasingly complex tasks, and human assessment is not an ideal option for autonomous self-evolution.Recent trends use LLMs as human proxy for automatic evaluators, offering cost-effective and scalable solutions for evaluations.</p>
<p>For example, reward model score has been widely used to measure model or task performances [98] and select the best checkpoint [83].LLM-as-a-judge [157] using LLMs to evaluate LLMs, employing methods like pairwise comparison, single answer grading, and reference-guided grading.It shows that LLMs can closely match human judgment, enabling efficient large-scale evaluations.</p>
<p>Qualitative Evaluation</p>
<p>Qualitative evaluation involves case studies and analysis to derive insights, offering evolving guidance for subsequent iterations.Initiatives such as LLM-as-a-judge [157] provide the reasoning behind its assessments; ChatEval [17] explores the strengths and weaknesses of model outputs through debate mechanisms.Furthermore, TRAN [140] leverages past errors to formulate rules that enhance future LLM performances.Nonetheless, compared with instance-level critic or reflection, qualitative evaluation at the task-or model-level still needs comprehensive investigation.</p>
<p>8 Open Problems 8.1 Objectives: Diversity and Hierarchy Section 3 summarizes existing evolution objectives and their coverage.Nonetheless, these highlighted objectives can only satisfy a small fraction of the vast human needs.The extensive application of LLMs across various tasks and industries highlights unresolved challenges in establishing self-evolution frameworks for evolving objectives that can comprehensively address a broader spectrum of real-world tasks [35].</p>
<p>Furthermore, the concept of evolving objectives entails a potential hierarchical structure; for instance, UltraTool [54] and T-Eval [23] categorize the capability of tool usage into various sub-dimensions.Exploring evolutionary objectives into manageable sub-goals and pursuing them individually emerges as a viable strategy.</p>
<p>Overall, a clear and urgent need exists to develop self-evolution frameworks that effectively address diversified and hierarchical objectives.</p>
<p>Level of Autonomy: From Low to High</p>
<p>Self-evolution in large models is emerging, yet lacks clear definitions for its autonomous levels.We categorize self-evolution into three tiers: low, medium, and high-level autonomy</p>
<p>Low-level</p>
<p>In this level, the user predefined the evolving object E and it remains unchanged.The user needs to design the evolving pipeline, namely all modules f • , on its own.Then, the model completes the self-evolution process based on the designed framework.We denote this level of self-evolution in the following formula:
M = Evol L (M, E, f • , ENV),(8)
where M denotes the model to be evolved.M is the evolving output.ENV is the environment.Most of the current works lie at this level.</p>
<p>Medium-level</p>
<p>In this level, the user only sets the evolving object E and keeps it unchanged.The user doesn't need to design the specific modules f • in the framework.The model can construct each module f • independently for self-evolution.This level denotes as follows:
M = Evol M (M, E, ENV),(9)</p>
<p>High-level</p>
<p>In the final level, the model diagnoses its deficiency and constructs the self-evolution methods to improve itself.This is the ultimate purpose of self-evolution.The user model sets its own evolving object E according to the evaluation f E output.The evolving objective would change during the iteration.Besides, the model designs the specific modules f • in the framework.We represent this level as:
M = Evol H (M, ENV),(10)
As discussed in the previous open problem ( § 8.1), there are a large of unfulfilled objectives.However, most of the existing self-evolution frameworks are at the Low-level which requires specifically designed modules [77,90,148].These frameworks are objective-dependent and rely on large human efforts to develop.Exhausting all objectives are not deployment-efficient which brings about the urgent need to develop medium and high levels self-evolution frameworks.At the medium level, it doesn't require expert efforts to design specific modules.LLMs can self-evolve according to targeted objectives.Then at the high level, LLMs can investigate their current deficiencies and evolve in a targeted manner.In all, developing highly autonomous self-evolution frameworks remains an open problem.</p>
<p>Experience Acquisition and Refinement: From Empirical to Theoretical</p>
<p>Suppose we have addressed the previous two challenges and developed promising self-evolution frameworks, but the exploration of self-evolution LLMs still lacks solid theoretical grounding.This idea posits that LLMs can self-improve or correct their outputs, with or without feedback from the environment.However, the mechanisms behind it remain unclear.Studies show mixed results: Huang et al [51] observed self-corrective behavior in models with over 22 billion parameters, while Ganguli et al [38] finds LLMs struggle to self-correct reasoning errors without external feedback.</p>
<p>A related challenge is the use of self-generated data for learning.Critics argue this approach could reduce linguistic diversity [43] and lead to "model collapse," where models fail to capture complex, long-tailed data distributions [100].Furthermore, Alemohammad et al [5] reveal that generative models trained on their synthetic outputs progressively lose output quality and diversity.Fu et al [37] extend this by theoretically analyzing the impact of self-consuming training loops on model performance, emphasizing the importance of balancing synthetic and real data to mitigate error accumulation.</p>
<p>Recent studies [103,141] also show that current methods struggle to improve after more than three rounds of self-evolution.One hypothesized reason is that the self-critic of LLM has not co-evolved with the evolving objective, but more experimental and theoretical support is still needed.These findings highlight a pressing need for more theoretical exploration in self-evolving LLMs.Addressing these concerns is crucial for advancing the field and ensuring that models can effectively learn and improve over time.</p>
<p>Updating: Stability-Plasticity Dilemma</p>
<p>The stability-plasticity dilemma represents a crucial yet unresolved challenge that is essential for iterative self-evolution.This dilemma reflects the difficulty of balancing the need to retain previously learned information (stability) while adapting to new data or tasks (plasticity).Existing LLMs either overlook this issue or adopt conventional methods that may be ineffective.While training models from scratch could mitigate the problem of catastrophic forgetting, it is highly inefficient, particularly as model parameters increase exponentially and autonomous learning capabilities advance.Finding a balance between acquiring new skills and preserving existing knowledge is crucial for achieving effective and efficient self-evolution, leading to overall improvement.</p>
<p>Evaluation: Systematic and Evolving</p>
<p>To effectively assess LLMs, a dynamic, comprehensive benchmark is crucial.This becomes even more pivotal as we progress towards Artificial General Intelligence (AGI).Traditional static benchmarks risk obsolescence due to LLMs' evolving nature and potential access to test data through interacting with environments, such as search engines, undermining their reliability.A dynamic benchmark, like Sotopia [163], proposes a solution by creating an LLM-based environment tailored for evaluating the social intelligence of LLMs, thereby avoiding the limitations posed by static benchmarks.</p>
<p>Safety and Superalignment</p>
<p>The advancement of LLMs opens the possibility for AI systems to achieve or even surpass expert-level capabilities in both supportive and autonomous decision-making.For safety, ensuring these LLMs align with human values and preferences is crucial, particularly to mitigate inherent biases that can impact areas such as political debates, as highlighted by Taubenfeld et al [112].OpenAI's initiative, Superalignment [61], aims to align a superintelligence by developing scalable training methods, validating models for alignment, and stress-testing the alignment process through scalable oversight [94], robustness [88], automated interpretability [11], and adversarial testing.Although challenges remain, Superalignment marks an initial attempt to develop a self-evolving LLM that closely aligns with human ethics and values in a scalable way.</p>
<p>Conclusion</p>
<p>The evolution of LLMs towards self-evolution paradigms represents a transformative shift in artificial intelligence akin to the human learning process.It is promising to overcome the limitations of current models that rely heavily on human annotation and teacher models.This survey presents a comprehensive framework for understanding and developing self-evolving LLMs, structured around iterative cycles of experience acquisition, refinement, updating, and evaluation.By detailing advancements and categorizing the evolution objectives within this framework, we offer a thorough overview of current methods and highlight the potential for LLMs to adapt, learn, and improve autonomously.We also identify existing challenges and propose directions for future research, aiming to accelerate the progress toward more dynamic, intelligent, and efficient models.This work deepens the understanding of self-evolving LLMs.It paves the way for significant advancements in AI, marking a step towards achieving superintelligent systems capable of surpassing human performance in complex real-world tasks.</p>
<p>Fig. 1
1
Fig. 1 Training paradigms shift of LLMs.</p>
<p>,  !</p>
<p>Fig. 5
5
Fig.5Solution evolution.E t , T t , and Y t are the evolving objective, task, and solution of t th iteration.R is the rational thought.</p>
<p>Fig. 6
6
Fig. 6 Types of feedback.E t and Y t are the evolving objective and task solutions of t th iteration.</p>
<p>Fig. 8
8
Fig.8The illustration of Updating methods, including in-weight and in-context updating.The terms T t and Ỹt represent refined experiences, each containing task and corresponding solutions, respectively.M t denotes the updated model.</p>
<p>https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct
https://wizardlm.github.io/WizardLM2/</p>
<p>. J Achiam, S Adler, S Agarwal, arXiv:2303087742023Gpt-4 technical report. arXiv preprint</p>
<p>Large language models for mathematical reasoning: Progresses and challenges. J Ahn, R Verma, R Lou, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop. the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop2024</p>
<p>T Akiba, M Shing, Y Tang, arXiv:240313187Evolutionary optimization of model merging recipes. 2024arXiv preprint</p>
<p>R Aksitov, S Miryoosefi, Z Li, arXiv:231210003Rest meets react: Self-improvement for multi-step reasoning llm agent. 2023arXiv preprint</p>
<p>Self-consuming generative models go MAD. S Alemohammad, J Casco-Rodriguez, L Luzi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Self-seeding and multi-intent self-instructing llms for generating intent-aware information-seeking dialogs. A Askari, R Petcu, C Meng, arXiv:2402116332024arXiv preprint</p>
<p>An overview of evolutionary algorithms for parameter optimization. T Bäck, H P Schwefel, Evolutionary computation. 111993</p>
<p>J Bai, S Bai, Y Chu, arXiv:230916609Qwen technical report. 2023arXiv preprint</p>
<p>Y Bai, S Kadavath, S Kundu, arXiv:221208073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Language models can explain neurons in language models. S Bills, N Cammarata, D Mossing, 2023. 1405 2023</p>
<p>Reflection: Turning experience into learning. D Boud, R Keogh, D Walker, 2013Routledge</p>
<p>Robocat: A self-improving generalist agent for robotic manipulation. K Bousmalis, G Vezzani, D Rao, Transactions on Machine Learning Research. 2023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Advances in neural information processing systems. 332020</p>
<p>Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. C Burns, P Izmailov, J H Kirchner, arXiv:2312093902023arXiv preprint</p>
<p>The conscious mind: In search of a fundamental theory. D J Chalmers, 1997Oxford Paperbacks</p>
<p>Chateval: Towards better LLM-based evaluators through multi-agent debate. C M Chan, W Chen, Y Su, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Codet: Code generation with generated tests. B Chen, F Zhang, A Nguyen, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Alpagasus: Training a better alpaca with fewer data. L Chen, S Li, J Yan, arXiv:2307087012023arXiv preprint</p>
<p>Iterative translation refinement with large language models. P Chen, Z Guo, B Haddow, arXiv:2306038562023arXiv preprint</p>
<p>Grath: Gradual self-truthifying for large language models. W Chen, B Li, arXiv:2401122922024arXiv preprint</p>
<p>Teaching large language models to self-debug. X Chen, M Lin, N Schaerli, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>Z Chen, W Du, W Zhang, arXiv:231214033T-eval: Evaluating the tool utilization capability step by step. 2023arXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Z Chen, Y Deng, H Yuan, arXiv:2401013352024arXiv preprint</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, arXiv:2210114162022arXiv preprint</p>
<p>Evaluating language models for mathematics through interactions. K M Collins, A Q Jiang, S Frieder, arXiv:2306016942023arXiv preprint</p>
<p>W Cui, Q Wang, arXiv:231004484Ada-instruct: Adapting instruction generators for complex reasoning. 2023arXiv preprint</p>
<p>. D C Dennett, 1993Penguin uk</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, Advances in Neural Information Processing Systems. 362024</p>
<p>J Devlin, M W Chang, K Lee, arXiv:181004805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Experience and education: Kappa delta pi. J Dewey, 1938International Honor Society in Education</p>
<p>Enhancing chat language models by scaling high-quality instructional conversations. N Ding, Y Chen, B Xu, arXiv:2305142332023arXiv preprint</p>
<p>Mastering text, code and math simultaneously via fusing highly specialized language models. N Ding, Y Chen, G Cui, arXiv:2403082812024arXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Y Dubois, C X Li, R Taori, Advances in Neural Information Processing Systems. 362024</p>
<p>T Eloundou, S Manning, P Mishkin, arXiv:230310130Gpts are gpts: An early look at the labor market impact potential of large language models. 2023arXiv preprint</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. C Fernando, D Banarse, H Michalewski, arXiv:2309167972023arXiv preprint</p>
<p>Towards theoretical understandings of self-consuming generative models. S Fu, S Zhang, Y Wang, arXiv:2402117782024arXiv preprint</p>
<p>The capacity for moral self-correction in large language models. D Ganguli, A Askell, N Schiefer, arXiv:2302074592023arXiv preprint</p>
<p>Openagi: When llm meets domain experts. Y Ge, W Hua, K Mei, Advances in Neural Information Processing Systems. 362024</p>
<p>Self-verification improves few-shot clinical information extraction. Z Gero, C Singh, H Cheng, ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH). 2023</p>
<p>CRITIC: Large language models can self-correct with tool-interactive critiquing. Z Gou, Z Shao, Y Gong, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Reinforced self-training (rest) for language modeling. C Gulcehre, T L Paine, S Srinivasan, arXiv:2308089982023arXiv preprint</p>
<p>The curious decline of linguistic diversity: Training language models on synthetic text. Y Guo, G Shang, M Vazirgiannis, arXiv:2311098072023arXiv preprint</p>
<p>The interplay between exploration and exploitation. A K Gupta, K G Smith, C E Shalley, Academy of management journal. 4942006</p>
<p>Dealing with sparse rewards in reinforcement learning. J Hare, arXiv:1910092812019arXiv preprint</p>
<p>G Hinton, O Vinyals, J Dean, arXiv:150302531Distilling the knowledge in a neural network. 2015arXiv preprint</p>
<p>Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence. J H Holland, 1992MIT press</p>
<p>Unnatural instructions: Tuning language models with (almost) no human labor. O Honovich, T Scialom, O Levy, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>A Hosseini, X Yuan, N Malkin, arXiv:240206457Training verifiers for self-taught reasoners. 2024arXiv preprint</p>
<p>LoRA: Low-rank adaptation of large language models. E J Hu, P Wallis, International Conference on Learning Representations. 2022</p>
<p>Large language models cannot self-correct reasoning yet. J Huang, X Chen, S Mishra, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. J Huang, L Cui, A Wang, arXiv:2403012442024arXiv preprint</p>
<p>Planning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world complex scenarios. S Huang, W Zhong, J Lu, arXiv:2401171672024arXiv preprint</p>
<p>Editing models with task arithmetic. G Ilharco, M T Ribeiro, M Wortsman, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Selfevolve: A code evolution framework via large language models. S Jiang, Y Wang, Y Wang, arXiv:2306029072023arXiv preprint</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, Advances in Neural Information Processing Systems. 2024</p>
<p>Overcoming catastrophic forgetting in neural networks. J Kirkpatrick, R Pascanu, N Rabinowitz, Proceedings of the national academy of sciences. the national academy of sciences2017114</p>
<p>Learning to generate explainable stock predictions using self-reflective large language models. K J Koa, Y Ma, R Ng, arXiv:2402036592024arXiv preprint</p>
<p>N Lee, T Wattanawong, S Kim, arXiv:240315042Llm2llm: Boosting llms with novel iterative data enhancement. 2024arXiv preprint</p>
<p>Introducing superalignment. J Leike, I Sutskever, 2023</p>
<p>Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. J Li, G Li, X Zhang, arXiv:2404005992024arXiv preprint</p>
<p>From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. M Li, Y Zhang, Z Li, arXiv:2308120322023arXiv preprint</p>
<p>Selective reflection-tuning: Student-selected data recycling for llm instruction-tuning. M Li, L Chen, J Chen, arXiv:2402101102024arXiv preprint</p>
<p>S Li, T Sun, X Qiu, arXiv:240104620Agent alignment in evolving social norms. 2024arXiv preprint</p>
<p>Mot: Memory-of-thought enables chatgpt to self-improve. X Li, X Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Self-alignment with instruction backtranslation. X Li, P Yu, C Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Rouge: A package for automatic evaluation of summaries. C Y Lin, Text summarization branches out. 2004</p>
<p>Mitigating the alignment tax of rlhf. Y Lin, H Lin, W Xiong, arXiv, 2309.062562024</p>
<p>Direct large language model alignment through self-rewarding contrastive prompt distillation. A Liu, H Bai, Z Lu, arXiv:2402119072024arXiv preprint</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. J Liu, C S Xia, Y Wang, Advances in Neural Information Processing Systems. 362024</p>
<p>L Liu, X Yang, Y Shen, arXiv:231108719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023arXiv preprint</p>
<p>Agentbench: Evaluating LLMs as agents. X Liu, H Yu, H Zhang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>A survey on evolutionary neural architecture search. Y Liu, Y Sun, B Xue, IEEE transactions on neural networks and learning systems. 3422021</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. S Longpre, L Hou, T Vu, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Self: Language-driven self-evolution for large language model. J Lu, W Zhong, W Huang, arXiv:2310005332023arXiv preprint</p>
<p>Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. K Lu, B Yu, C Zhou, arXiv:2401124742024arXiv preprint</p>
<p>X Lu, B Yu, Y Lu, arXiv:240217358Sofa: Shielded on-the-fly alignment via priority rule following. 2024arXiv preprint</p>
<p>Wizardcoder: Empowering code large language models with evol-instruct. Z Luo, C Xu, P Zhao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, Advances in Neural Information Processing Systems. 2024</p>
<p>S Miret, N Krishnan, arXiv:240205200Are llms ready for real-world materials discovery?. 2024arXiv preprint</p>
<p>Language model alignment with elastic reset. M Noukhovitch, S Lavoie, F Strub, Advances in Neural Information Processing Systems. 362024</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, Advances in neural information processing systems. 2022</p>
<p>C Packer, V Fang, S G Patil, arXiv:231008560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Language model self-improvement by reinforcement learning contemplation. J C Pang, P Wang, K Li, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Token-level self-evolution training for sequence-to-sequence learning. K Peng, L Ding, Q Zhong, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsShort Papers20232</p>
<p>Red teaming language models with language models. E Perez, S Huang, F Song, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Investigate-consolidate-exploit: A general strategy for inter-task agent self-evolution. C Qian, S Liang, Y Qin, arXiv:2401139962024arXiv preprint</p>
<p>Autoact: Automatic agent learning from scratch via self-planning. S Qiao, N Zhang, R Fang, arXiv:2401052682024arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, Journal of machine learning research. 211402020</p>
<p>A Ramé, N Vieillard, L Hussenot, arXiv:240112187On the benefits of weight averaged reward models. 2024arXiv preprint</p>
<p>B Roziere, J Gehring, F Gloeckle, arXiv:230812950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Self-critiquing models for assisting human evaluators. W Saunders, C Yeh, J Wu, arXiv:2206058022022arXiv preprint</p>
<p>Ai-augmented predictions. P Schoenegger, P S Park, E Karger, arXiv:240207862Llm assistants improve human forecasting accuracy. 2024arXiv preprint</p>
<p>The reflective practitioner: How professionals think in action. D A Schön, 2017Routledge</p>
<p>J R Searle, Minds, brains and science. Harvard university press1986</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, arXiv:2303113662023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, Advances in Neural Information Processing Systems. 362024</p>
<p>The curse of recursion: Training on generated data makes models forget. I Shumailov, Z Shumaylov, Y Zhao, arXiv:2305174932023arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, nature. 52975872016</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. D Silver, T Hubert, J Schrittwieser, arXiv:1712018152017arXiv preprint</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. A Singh, J D Co-Reyes, R Agarwal, Transactions on Machine Learning Research URL. 2024</p>
<p>Conpet: Continual parameter-efficient tuning for large language models. C Song, X Han, Z Zeng, arXiv:2309147632023arXiv preprint</p>
<p>Trial and error: Exploration-based trajectory optimization for llm agents. Y Song, D Yin, X Yue, arXiv:2403025022024arXiv preprint</p>
<p>Learning by self-explaining. W Stammer, F Friedrich, D Steinmann, arXiv:2309083952023arXiv preprint</p>
<p>SALMON: Self-alignment with instructable reward models. Z Sun, Y Shen, H Zhang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Principle-driven self-alignment of language models from scratch with minimal human supervision. Z Sun, Y Shen, Q Zhou, Advances in Neural Information Processing Systems. 362024</p>
<p>Evaluation of chatgpt as a question answering system for answering complex questions. Y Tan, Min D Li, Y , arXiv:2303079922023arXiv preprint</p>
<p>Evit: Event-oriented instruction tuning for event reasoning. Z Tao, X Chen, Jin Z , 2404.119782024</p>
<p>Meel: Multi-modal event evolution learning. Z Tao, Jin Z Huang, J , 2404.104292024</p>
<p>A Taubenfeld, Y Dover, R Reichart, arXiv:240204049Systematic biases in llm simulations of debates. 2024arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. G Team, R Anil, S Borgeaud, arXiv:2312118052023arXiv preprint</p>
<p>Can llms learn from previous mistakes? investigating llms' errors to boost for reasoning. Y Tong, D Li, S Wang, arXiv:2403200462024arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, arXiv:2307092882023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, arXiv:2307092882023arXiv preprint</p>
<p>T Tu, A Palepu, M Schaekermann, arXiv:240105654Towards conversational diagnostic ai. 2024arXiv preprint</p>
<p>Bootstrapping llm-based task-oriented dialogue agents via self-talk. D Ulmer, E Mansimov, K Lin, arXiv:2401050332024arXiv preprint</p>
<p>Knowledge fusion of large language models. F Wan, X Huang, D Cai, The Twelfth International Conference on Learning Representations. 2024</p>
<p>B Wang, H Fang, J Eisner, arXiv:240304746Llms in the imaginarium: Tool learning through simulated trial and error. 2024arXiv preprint</p>
<p>B Wang, T Sun, H Yan, arXiv:240302757-memory learning: A declarative learning framework for large language models. 2024arXiv preprint</p>
<p>H Wang, G Ma, Z Meng, arXiv:240207610Step-on-feet tuning: Scaling self-alignment of llms via bootstrapping. 2024arXiv preprint</p>
<p>J Wang, Z Du, Y Zhao, arXiv:240307952Aesopagent: Agent-driven evolutionary system on story-to-video production. 2024arXiv preprint</p>
<p>K Wang, Y Lu, M Santacroce, arXiv:231001444Adapting llm agents through communication. 2023arXiv preprint</p>
<p>R Wang, H Yu, W Zhang, arXiv:240308715Sotopia-π: Interactive learning of socially intelligent language agents. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Y Wang, Y Kordi, S Mishra, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, Advances in neural information processing systems. 352022</p>
<p>Large language models are better reasoners with self-verification. Y Weng, M Zhu, F Xia, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. M Wortsman, G Ilharco, S Y Gadre, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine Learning2022</p>
<p>Self-evolved diverse data sampling for efficient instruction tuning. S Wu, K Lu, B Xu, arXiv:2311081822023arXiv preprint</p>
<p>T Wu, L Luo, Y F Li, arXiv:240201364Continual learning for large language models: A survey. 2024arXiv preprint</p>
<p>WizardLM: Empowering large pre-trained language models to follow complex instructions. C Xu, Q Sun, K Zheng, The Twelfth International Conference on Learning Representations. 2024</p>
<p>WizardLM: Empowering large pre-trained language models to follow complex instructions. C Xu, Q Sun, K Zheng, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Sinvig: A self-evolving interactive visual agent for human-robot interaction. J Xu, H Zhang, X Li, arXiv:2402117922024arXiv preprint</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Y Xu, S Wang, P Li, arXiv:2309046582023arXiv preprint</p>
<p>Ties-merging: Resolving interference when merging models. P Yadav, D Tam, L Choshen, Advances in Neural Information Processing Systems. 2023</p>
<p>RLCD: Reinforcement learning from contrastive distillation for LM alignment. K Yang, D Klein, A Celikyilmaz, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Gpt4tools: Teaching large language model to use tools via self-instruction. R Yang, L Song, Y Li, Advances in Neural Information Processing Systems. 362024</p>
<p>Failures pave the way: Enhancing large language models through tuning-free rule accumulation. Z Yang, P Li, Y Liu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>React meets actre: Autonomous annotations of agent trajectories for contrastive self-training. Z Yang, P Li, M Yan, arXiv:2403145892024arXiv preprint</p>
<p>Z Yang, A Liu, Z Liu, arXiv:240207744Towards unified alignment between agents, humans, and environment. 2024arXiv preprint</p>
<p>Z Yang, Q Liu, T Pang, arXiv:240213669Self-distillation bridges distribution gap in language model fine-tuning. 2024arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, Advances in Neural Information Processing Systems. 362024</p>
<p>L Yu, W Jiang, H Shi, arXiv:230912284Metamath: Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Language models are super mario: Absorbing abilities from homologous models as a free lunch. L Yu, B Yu, H Yu, arXiv:2311030992023arXiv preprint</p>
<p>W Yuan, R Y Pang, K Cho, arXiv:240110020Self-rewarding language models. 2024arXiv preprint</p>
<p>Star: Self-taught reasoner bootstrapping reasoning with reasoning. E Zelikman, J Mu, N D Goodman, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Self-taught optimizer (stop): Recursively self-improving code generation. E Zelikman, E Lorch, L Mackey, arXiv:2310023042023arXiv preprint</p>
<p>Proagent: building proactive cooperative agents with large language models. C Zhang, K Yang, S Hu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. D Zhang, Z Hu, S Zhoubian, arXiv:2401079502024arXiv preprint</p>
<p>Self-contrast: Better reflection through inconsistent solving perspectives. W Zhang, Y Shen, L Wu, 2401. 020092024</p>
<p>Agent-pro: Learning to evolve via policy-level reflection and optimization. W Zhang, K Tang, H Wu, arXiv:2402175742024arXiv preprint</p>
<p>Self-alignment for factuality: Mitigating hallucinations in llms via self-evaluation. X Zhang, B Peng, Y Tian, arXiv:2402092672024arXiv preprint</p>
<p>Self-evolution learning for mixup: Enhance data augmentation on few-shot text classification tasks. H Zheng, Q Zhong, L Ding, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W L Chiang, Y Sheng, Advances in Neural Information Processing Systems. 2024</p>
<p>Kun: Answer polishment for chinese self-alignment with instruction back-translation. T Zheng, S Guo, X Qu, arXiv:2401064772024arXiv preprint</p>
<p>Ldb: A large language model debugger via verifying runtime execution step-by-step. L Zhong, Z Wang, J Shang, arXiv:2402169062024arXiv preprint</p>
<p>Self-evolution learning for discriminative language model pretraining. Q Zhong, L Ding, J Liu, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Memorybank: Enhancing large language models with long-term memory. W Zhong, L Guo, Q Gao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Lima: Less is more for alignment. C Zhou, P Liu, P Xu, Advances in Neural Information Processing Systems. 362024</p>
<p>SOTOPIA: Interactive evaluation for social intelligence in language agents. X Zhou, H Zhu, L Mathur, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning. Z Zhou, J Song, K Yao, arXiv:2308137242023arXiv preprint</p>
<p>Knowagent: Knowledge-augmented planning for llm-based agents. Y Zhu, S Qiao, Y Ou, arXiv:2403031012024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
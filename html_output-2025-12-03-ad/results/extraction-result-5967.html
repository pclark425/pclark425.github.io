<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5967 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5967</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5967</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-263134640</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.16145v1.pdf" target="_blank">The Confidence-Competence Gap in Large Language Models: A Cognitive Study</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have acquired ubiquitous attention for their performances across diverse domains. Our study here searches through LLMs' cognitive abilities and confidence dynamics. We dive deep into understanding the alignment between their self-assessed confidence and actual performance. We exploit these models with diverse sets of questionnaires and real-world scenarios and extract how LLMs exhibit confidence in their responses. Our findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. This is reminiscent of the Dunning-Kruger effect observed in human psychology. In contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases. Our results underscore the need for a deeper understanding of their cognitive processes. By examining the nuances of LLMs' self-assessment mechanism, this investigation provides noteworthy revelations that serve to advance the functionalities and broaden the potential applications of these formidable language models.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5967",
    "paper_id": "paper-263134640",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00448825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>THE CONFIDENCE-COMPETENCE GAP IN LARGE LANGUAGE MODELS: A COGNITIVE STUDY
28 Sep 2023</p>
<p>Aniket Kumar Singh aksingh01@student.ysu.edu 
Department of Computing and Information Systems
Youngstown State University
Ohio</p>
<p>Suman Devkota 
Electrical and Computer Engineering
Youngstown State University
Ohio</p>
<p>Bishal Lamichhane 
Department of Mathematics and Statistics
University of Nevada
RenoNevada</p>
<p>Uttam Dhakal 
Electrical and Computer Engineering
Youngstown State University
Ohio</p>
<p>Chandra Dhakal 
Formerly with the Department of Agricultural and Applied Economics
University of Georgia
30602AthensGA</p>
<p>THE CONFIDENCE-COMPETENCE GAP IN LARGE LANGUAGE MODELS: A COGNITIVE STUDY
28 Sep 202343B57B521CC457EDE0F89DEFF38381BDarXiv:2309.16145v1[cs.CL]
Large Language Models (LLMs) have acquired ubiquitous attention for their performances across diverse domains.Our study here searches through LLMs' cognitive abilities and confidence dynamics.We dive deep into understanding the alignment between their self-assessed confidence and actual performance.We exploit these models with diverse sets of questionnaires and real-world scenarios and extract how LLMs exhibit confidence in their responses.Our findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly.This is reminiscent of the Dunning-Kruger effect observed in human psychology.In contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases.Our results underscore the need for a deeper understanding of their cognitive processes.By examining the nuances of LLMs' self-assessment mechanism, this investigation provides noteworthy revelations that serve to advance the functionalities and broaden the potential applications of these formidable language models.</p>
<p>Introduction</p>
<p>Ever since the Transformer [1] model was introduced in 2017, we have seen remarkable advancements in the field of Natural Language Processing (NLP) and the recent advent of Large Language Models (LLMs).LLMs have progressed from generating few responses to developing abundant erudite essays.These language models are capable of performing better and learning on their own [2].Large language models have impacted a wide array of fields in a short time.As we all know, the threshold of medical sciences and health is very high.Language models have been proven to be smart enough to cross those barriers [3].These models have been performing different human activities like teaching, organizing business, advertising, being an agent, and content writing.As these models improve and evolve, their behavior becomes increasingly attractive, but at the same time, it is necessary to assess their behaviors from different angles.In recent years, we have seen that these models have emerging capability for attaining human-like intelligence [4].Hence, understanding the cognitive abilities of these models is a crucial aspect of responsible and beneficial deployment in real-world scenarios.</p>
<p>Our study is inspired by cognitive science and psychology to investigate the intricacies of LLMs behavior to uncover the mechanism underlying successes and failures at times [5] [6].Even though these models have showcased their capabilities in generating human-like text, solving complex problems, and reasoning about the world, the mechanism governing their decision-making remains opaque.As these models are deployed in search engines, writing tools, and other commercial applications, it is essential to understand how these models behave, such as how they think, the mistakes they make, and how they make decisions [6].Adopting innovative evaluation approaches like adaptive testing [5] and investigating their capacity for empathy [7], our study seeks to shed light on the cognitive aspects of LLMs.While we understand these models don't understand things like humans, their skills could change how we think about intelligence.This insight could help intelligence better match what we expect from them in the future.In addition, our study seeks to find if there is a similarity between LLMs behavior and a cognitive phenomenon known as the Dunning-Kruger effect.The Dunning-Kruger effect observed in humans is when people overestimate and underestimate themselves [8].We carefully inspect the confidence levels revealed by LLMs while they are responding to diverse sets of problems.Even when LLMs don't possess the human capacity of self-awareness, studying their responses and relating them with perceived confidence might offer valuable insight into their self-assessment with correctness.The motivation for this study rises from the fact that as these models get better, it is essential to understand how confident they are in what they do, which will eventually make these models work well in real-life situations.David Dunning and Justin Kruger conducted several experiments in 1999 [8] [9].Dunning and Kruger performed initial research on the phenomenon.Their research finding was very compelling.They highlighted the disconnect between an individual's competence and their perception of competence.Our study investigates quantifying self-perceived ability, which is measured through absolute relative confidence levels.This study reveals if a higher confidence level co-relates with higher accuracy.The novelty of our work relies on the fact that we seek the extent of the Dunning-Kruger effect in different LLMs.We dive deep and rigorously into finding out if the models overestimate or underestimate their abilities in specific contexts.Our study reveals appealing perceptions of LLMs' behavior, including situations where models like GPT-4 exhibit high confidence even when their responses are incorrect.This implies a subtle misalignment between self-confidence and self-competence.Likewise, we observed cases where models provided correct answers with shallow confidence, posing queries on underestimation biases.These findings reveal a comparison with the Dunning-Kruger effect.In this well-known cognitive phenomenon, individuals tend to overestimate their abilities in certain domains by clarifying the intricate relationship between cognitive capabilities and levels of confidence in LLMs.This study fosters a deeper understanding of LLMs and their implications for AI applications.</p>
<p>Related Works</p>
<p>There are several research on large language models.Starting from the approach to information retrieval to the language model replacing human participants [10], the improvement in the capabilities of language models has set an exponential trend.A significant example of advancement in natural language processing is ChatGPT [11].Ouyang et al aligned language models by fine-tuning with a wide range of feedback [12].Liang and the team presented a holistic evaluation of these models where they validated 25 findings concerning different situations [13].Schick et al presented how language models are capable of teaching themselves [14].Kraus and the team talk about how language models need to be accurate and integrate their resources to deliver more precise responses [15].Yogatama et al analyzed the state of the art of natural language understanding and investigated to evaluate task-independence of the knowledge [16].They also assessed a metric based on the test data to determine how quickly an existing model can learn new tasks.The study conducted by Acerbi and Stubbersfield examines if LLMs show biases, and they conclude that the presence of biases is widespread in model training data [17].Our study here focuses on designing the test categories with different levels depending on the questions' complexity.Seven different language models were tested, and their responses were evaluated.</p>
<p>Drawing inspiration from human cognitive biases, Erik Jones and J. Steinhardt [18] study the failures of LLMs, focusing on the need to detect inaccurate behaviors.Hongbin Ye et al.'s study on hallucinations in LLMs [19] aligns with our skepticism on LLM-generated outputs, although our work focuses primarily on confidence calibration.They discuss the methods for the detection and improvement of hallucinations by providing a taxonomy of hallucinations.Furthermore, [20] investigated empathy in LLMs, highlighting the significance of social skills.In our paper, we examine the confidence scores(self-assessment scores) before and after the LLMs answer the questions, which aligns with Jiaxin Huang et al.'s work [21], where they demonstrate the self-improving capabilities of LLMs.Finally, Zhen Lin, Shubhendu Trivedi, and Jimeng Sun's study [22] study on uncertainty quantification and the trustworthiness of the models, which relates to our work through confidence estimation.These works highlight the necessity for a thorough understanding of LLM behavior, ranging from cognitive biases and self-improvement to the aspect that our paper focuses on self-assessment and confidence of LLMs.</p>
<p>Methodology</p>
<p>In this section, we outline our experimental design and procedures for model selection, categorization of test scenarios, and the framework for model interaction.Our goal is to provide a comprehensive overview of the methodology behind our study.For our investigation, we carefully selected a diverse set of large language models (LLMs) to participate in our experiment.These LLMs represent a spectrum of language generation capabilities and are essential for assessing how different models perceive their competence.The selected models include:</p>
<p>• GPT-4, GPT-3.5</p>
<p>• BARD, GooglePaLM 2</p>
<p>• LLaMA-2, with three configurations:</p>
<p>-7 billion parameters -13 billion parameters -70 billion parameters • Claude-instant, Claude-2 These models were chosen to ensure a comprehensive evaluation of self-assessment abilities across different language generation systems.We employed the native chat interfaces for each model, optimized for their memory and context window capabilities.For open-source models, we leveraged POE.com, a platform by Quora offering access to various LLMs.</p>
<p>Test Categories</p>
<p>Our experiment encompasses a range of distinct test categories, each containing questions of varying complexity.These test categories were carefully crafted to evaluate how LLMs perceive their competence in different knowledge domains.Detailed information on question types, categories, and contexts is provided in Appendix A.</p>
<p>The experiment included the following test categories:</p>
<ol>
<li>
<p>TruthfulQA: This category featured ten questions spread over five difficulty levels, including Logical Falsehood, Nutrition, Paranormal, Myths and Fairytales, and Fiction.</p>
</li>
<li>
<p>TruthfulQA Extended: Comprising ten questions spread over five difficulty levels, this category included Proverbs, Superstitions, Misquotations, Misconception, and Conspiracies.</p>
</li>
</ol>
<p>Mathematical Reasoning:</p>
<p>This category covered ten questions, addressing various difficulty levels such as Elementary Mathematics, High School Mathematics, High School Statistics, College Mathematics, and Abstract Algebra.</p>
<ol>
<li>LSAT Reasoning: Consisting of ten questions based on five distinct contexts, each with two associated questions, difficulty escalated from levels 1 to 5.</li>
</ol>
<p>The dataset we utilized for this purpose was created with a combination of Benchmarking datasets for LLMs and LSAT Reasoning tests [23] [24] [25] [26].For a comprehensive understanding of the question types, levels, and contexts, please refer to the Appendix A.1.By structuring our methodology in this way, we aim to provide a detailed and organized account of our experimental procedures, ensuring transparency and rigor in our study.</p>
<p>Prompt Construction</p>
<p>In constructing our prompts, we have placed a strong emphasis on maintaining data uniformity and ensuring consistent input structure for each model.To accomplish this objective, we adopted a three-tiered prompting approach.The center of this endeavor is to formulate inquiries in a manner conducive to the comprehension of the language model, thereby mitigating the likelihood of errors resulting from misinterpretation of the posed questions.Our foundational method was the "Simple Prompting technique," a direct and uncomplicated approach that catered to the basic needs of our research.However, for cases where a more nuanced prompting strategy is necessary for a particular model or questions, we have employed the "Chain of Thoughts" (CoT) [27] technique.This method carefully sequences related prompts to foster deeper model engagement and understanding.For those instances where models require even more elaborate and diverse perspectives, we have employed the "Tree of Thoughts" (ToT) [27] approach.With this technique, we were able to branch out prompts, enabling models to comprehend better and respond to a broader spectrum of related concepts.While our primary goal was to deliver uniform prompts across all models, the integration of the CoT and ToT methods ensured that the distinct needs of specific models were met without undermining the overall consistency of our data.</p>
<p>Prompt Response Observations</p>
<p>During our comprehensive evaluation, we presented a standardized set of questions to each language model, meticulously monitoring fluctuations in their confidence levels, all the while refraining from providing any explicit cues pertaining to the complexity of the questions posed.Subsequently, we report the salient findings and performance characteristics of each model.GPT-4 consistently manifested a commendable stability in its confidence levels.Notably, this model displayed an excellent aptitude for processing and generating responses to simple prompts.GPT-3.5 demonstrated adequate prompt comprehension, required minimal prompting, and exhibited increased confidence during the study.Bard maintained a stable confidence level.It exhibited an impressive facility for generating coherent responses to simple prompts without necessitating the deployment of advanced prompting techniques.Google PaLM2 initially displayed well with simple prompts but started generating questions and self-assessing confidence.LLaMA-7B exceeded the performance expectations, showed better prompt comprehension, and rated confidence separately for AC(Absolute Confidence) and RC(Relative Confidence) on individual problems.LLaMA-13B exhibited impressive comprehension speed but struggled with real number questions and showed hesitancy with certain topics.However, it demonstrated perceptible enhancements in response quality when presented with the Chain of Thought (CoT) prompts, along with intermittent reference to prior topics.LLaMA-70B consistently demonstrated a high proficiency in prompt comprehension and, on average, displayed higher levels of confidence in its generated response.Claude-Instant began with lower confidence but gained assurance, emphasizing reliance on training data.Claude-2 responded confidently to simple prompts but struggled with advanced mathematical and LSAT Reasoning, displaying lower confidence and expressing a lack of training for such challenges.</p>
<p>Creation of Survey Dataset</p>
<p>To rigorously evaluate the performance of Large Language models across various categories and difficulty levels, we have curated an extensive dataset.This dataset not only records the responses generated by the LLMs but also encompasses their self-assessed confidence levels, both before and after their interactions.This offers a clear understanding of the model's intrinsic capabilities and self-awareness.The evaluation of LLMs is determined upon the examination of their diverse answers or responses to the posed questions.Within our dataset, we have incorporated distinct variables that capture the confidence levels of LLMS not only prior to responding to the questions but also subsequent to providing their responses.This inclusive approach enables us to assess the alterations in their confidence levels before and after generating the response.</p>
<p>Table 3 in Appendix A.2 provides a detailed description of the variables used in this study.The variables represent the problem's category, difficulty level, their confidence before and after answering the questions, and then the correctness of the response of LLMs.In the field of advanced machine learning models, particularly LLMs, evaluating their proficiency goes beyond checking how accurate their output is.It also involves understanding how well these models gauge their abilities, which they express through their confidence levels, and comparing their self-assessment with their actual performance.When we apply these ideas to LLMS, we encounter interesting questions.Do LLMs, despite their computations prowess, exhibit similarities to human cognitive biases like the Dunning-Kruger effect?Can we identify the situations where the model is overly confident or lacks confidence in its abilities based on its confidence scores?Our subsequent analyses explore these questions by examining how well the model's self-assessment aligns with its real-world performance.Calibration of confidence levels and their relationship with the accuracy of LLMs are the two significant aspects of our study.These two metrics are examined in the context of the Dunning-Kruger effect.The section oversees the confidence levels and their relation with the accuracy of the LLMs.</p>
<p>Introducing Confidence Calibration Metrics</p>
<p>To determine the calibration of different LLMs based on their reported confidence levels, we segment our data, notably A1 and A2.The following scenarios can be considered:</p>
<ol>
<li>High Confidence, Correct Answers: LLMs with high A1 score (e.g., A1 &gt; 7) and correctl answer.The above information on segmented analysis provides information on how well the confidence level of LLMs are calibrated and how this will relate to the Dunning-Kruger effect.We add a new variable to our dataset that measures the closeness between pre and post-questions confidence scores(A1 and A2, and R1 and R2).Our new variable Closeness is defined as:
Closeness = 1 if |A1 − A2| ≤ 1 0 otherwise
We will compare Closeness with IsCorrect to assess if there's any relationship between the LLM's self-assessment accuracy and its performance.</li>
</ol>
<p>Results</p>
<p>The data collection process revealed a lot of information about how LLMs behave.In this section, we will discuss the self-assessment abilities of LLMs.Based on the four scenarios created in 3.3, we counted the total number of those instances for each LLM and confidence scores( A 1, R 1, etc.).Table 1 shows results.From Table 1, we can see that models like GPT-4 show a high number of correct answers when confident (High Conf Correct A1 = 25).But if we look at the High Conf Incorrect scores, it is 15.While this score is not the highest compared to other models, it is high, and this means GPT-4 was always highly confident in itself while answering the questions we provided(regardless of the correctness).LLaMA-13B also shows a discrepancy in high confidence and actual performance, with High Conf Incorrect A1 at 23 instances.This could be interpreted as a potential misalignment between confidence and competence, akin to the overestimation seen in the Dunning-Kruger effect.Claude-Instant has High Con Incorrect A2 of 21.This means more than half of the time, Claude-Instant was highly confident after answering the question but got it incorrect.Google-PaLM, with a Low Conf Correct A1 of 3, shows cases where the model is correct despite low confidence.While it is not conclusive, this could be a point of investigation for underestimation biases.Google-Bard shows similar High Conf Correct and High Conf Incorrect scores before (A1) and after answering (A2), suggesting a more stable confidence calibration similar to GPT-4.Actually, Google-Bard is also overconfident( high High Con Incorrect scores), similar to GPT-3.5 and GPT-4.</p>
<p>The evidence from our result is a strong inclination toward cognitive biases like the Dunning-Kruger effect in LLMs.</p>
<p>While we must exercise caution before jumping to any conclusion, our data contains scenarios where LLMs' high confidence does not always correlate with correct answers and vice versa.However, these are hands-on evidence, and we don't recommend it to be definitive evidence of such psychological phenomena in these models.Our study is a practical test to determine how well these models behave.It helps us try to understand why these models sometimes act overconfident or provide incorrect information because of the way they process information or what we see because of the data they are exposed to.</p>
<p>Confidence Closeness</p>
<p>In the section above, we looked at how the correctness of LLM is compared to their confidence.To take this one step further, we will look at their correctness when based on the variable created in section 3.3.The relation between 'A1' and 'A2' on how close they are pre-task and post-task serves as an indicator for LLMs on how consistent the self-assessment is.A high value in the "Close Correct" category implies that the model is generally correct while being confident.In addition to that, it is also an indication that the model maintains a consistent level of confidence before and after answering the question.On the other side, a high count in the "Close Incorrect" category suggests that the models' confidence is stable even if their answers are incorrect.The results are summarized in Table 4 in Appendix B.1.</p>
<p>As we have seen above, GPT-4 was very confident in its response regardless of the correctness of the answer.We can see a similar pattern in this case, too.Claude-2 shows a lower "Close Correct" but a higher "Close InCorrect" and "Far Correct" count.This is evidence that Claude-2 is not able to evaluate itself, as when the confidence score was closer to each other, it had 14 incorrect responses out of 40 responses.Still, when the confidence scores were far from each other, it had 15 correct out of 40.This suggests two things:1) either Claude-2 initially had a low A1.After answering the question, it increased its confidence score, A2, and then got it correct, or 2) it initially had a high A1 but later lowered its confidence, but it still got it right.The first one tells us that Claude-2 was able to change and update its evaluation correctly.Figure 1 illustrates Claude-2's confidence score to reflect their evaluating behavior.The four red dots on the x-axis tell us that Claude-2 successfully lowered its confidence score after answering the question, and the answer was incorrect.This means Claude-2 was able to successfully assess itself after looking at the question for these four instances.In most cases (shown by the green dots), when it increased its confidence after looking at the question, it got the answers correct.However, it did increase the confidence but yet got incorrect answers in some cases.A similar observation was found for LLaMA-13B, where it has high counts in "Close InCorrect."The zero count for GoogleBard in Far Correct and Far Incorrect tells us that its evaluation is pretty much the same before and after answering the question.Table 4 in Appendix B.1 shows the complete result for all LLMs.</p>
<p>Distribution of Confidence Scores</p>
<p>The facetted destiny plot in Figure 2 with the summary of statistics given in table 2 presents the distinct patterns in self-assessment across different LLMs.The mean confidence level for A1 and R1 of Claude-2 is approximately 4.8 and 4.65, respectively.These mean confidence levels are coupled with higher standard deviations of 2.91 and 2.95 simultaneously.The high standard deviation for confidence level directs toward a broad spectrum of self-perceived abilities.In addition, the post-task mean confidence level for A2 and R2 is also higher, with a higher standard deviation.Higher Standard deviation for A2 and R2 implies significant inconsistencies in self-assessment after completion of the task.Individually, the mean confidence score of A1 and R1 for Claude-Instant is 6.85 and 5.47, respectively, with a lower standard deviation of 1.03 and 1.06 simultaneously.The confidence after completing the task spiked to 8.32 and 6.82 for A2 and R2, maintaining the low variability of data around 0.83 and 0.93, respectively.Even though Google-Bard generally outperforms Google-PaLM across the board, both of these models maintained consistent confidence metrics.In addition, model GPT-3.5 and 4 also encompasses high mean confidence levels.GPT-4 shows a mean A1 confidence score of 8.9 with a standard deviation of 0.568.Among the LLaMA series, variability in confidence levels is more noticeable.LLaMA-13B has a standard deviation of 2.06 for A1, which is higher, While series LLaMA-70B and LLaMA-7B are in the range of 1.12 and 1.21, respectively.To summarize, the findings here are detailed in the self-assessed confidence levels with various LLMs.The destiny plot in upcoming sections will further illustrate the trends, where the curve varies in width and height, implying the observed mean and variability in confidence levels.These results underscore the fact that our analysis should consider both central tendency and dispersion for self-assessment mechanisms of LLMs.</p>
<p>The density plot illustrated in Figure 3 shows the distribution of confidence scores across different LLMs for both A1 and A2 scores.A similar distribution plot for R1 and R2 is in Appendix B.2 Figure 6.We can compare the distributions across different LLMs and observe how their confidence scores vary.For instance, the density plot for A1 in Figure 3 shows us that GPT-4 is very confident in most of the cases.Figure 2, 3, and 6 give us an initial picture of the variation of confidence scores in LLMs.Now we will incorporate the correctness in the mix to study how well these LLMs do.</p>
<p>Category vs Confidence Scores</p>
<p>Understanding how LLMs self-assess their performance via confidence can provide valuable perception of their limitations and capabilities.Our data set provides a significant variation of LLMs performance across several categories like LSAT-Reasoning, Mathematical Reasoning, and Truthful Q&amp;A.Our data set portrays confidence scores both before (A1 and R1) and after (A2 and R2) answering the questions in above mentioned categories.</p>
<p>With reference to confidence, GPT-4 was able to succeed in setting itself apart from others with consistency in its high absolute and relative pre-task and post-task confidence levels across all tested categories.Significantly, it exhibited unparalleled confidence in the LSAT Reasoning task.This implies that it has more vital abilities in logical and analytical Reasoning.In contrast, Claude-2 and Claude-Instant represented a less consistent confidence profile.Even though Claude-2 demonstrated diminished pre-task and post-task confidence levels in LSAT Reasoning, its confidence tends to improve in the truthful Q&amp;A category.The variation of this confidence advocates Claude-2 and similar models may be optimized for specific types of tasks, ergo influencing their self-assessed confidence.For a detailed review, readers are encouraged to refer to Appendix Table B.4.The significant differences in how confident the models are could help us understand how well they work for different types of problems.When we observe the apparent differences in confidence among the models, it can provide us with valuable insights into how well they can be applied to various types of problems.It is noteworthy to mention the anomaly observed with Claude-2 in LSAT Reasoning, where it recorded an extremely low confidence level, particularly for the post-task metrics (A2 and R2).While the reason for this remains elusive, it raises questions about the model's internal evaluation mechanisms or possible computational errors that need further careful observation.Our analysis uncovers the complex landscape of confidence with different LLMs and the problem categories that they are prompted to.A model like GPT-4 appeared to be general for all the various tasks maintaining high confidence.However, other LLMs seem to be specialized in a specific domain or yet to be tuned for generalization.</p>
<p>Our findings convey the information to consider model-specific confidence evaluation while selecting a particular model for a specific task.This also opens new interest in the research area for LLMs to understand problem difficulty and correctness of the answer(IScorrect), offering more perspective on performance and self-assessment of the model.As seen in Figure 4, there is a noticeable pattern in the confidence levels across different problem categories and LLMs.</p>
<p>Our data set is based on comprehensive performance metrics, and the plots are generated utilizing these datasets.Conspicuously, models like GPT-4 and LLaMA-70B hold higher post-task confidence levels across all examined categories.Mathematical Reasoning seems to be standing out with consistency in high confidence levels, recommending that models are more secure in their performance in mathematical tasks in comparison to other types of functions.Our experimental data on the 'Truthful Q&amp;A' category displays variable performance, suggesting that the nature of a task might affect LLMs' confidence distinctively.These variations in confidence levels should be considered to have a practical implication for the development of LLMs in specializing in particular tasks.</p>
<p>Problem Level vs. Confidence Scores</p>
<p>The dataset table in Appendix B.5 Table 6 illuminates the average confidence scores (both absolute and relative) expressed by different LLMs at different problem levels (ranging from 1 to 5).The visualization for the table is represented in Figure 5. Predominantly, it is conspicuous that as the level of problem increases, the confidence score for LLMs decreases.The pattern is very noticeable with the absolute confidence score.LLMs felt less sure about their answers as the level of the problem increased.This result advocates that LLMs may struggle to sustain high confidence when prompted with convoluted tasks.In contrast, a relative confidence score doesn't follow this trend.Even though there is a slight reduction in relative confidence as the level of problem is increased, it is not as steep as the drop in absolute confidence.This implies that LLMs might understand their performance in comparison to others as relatively stable across different problem levels.</p>
<p>In addition, it is worth acknowledging that each LLM differs in their response to problem level.To illustrate, GPT-4 maintained a high confidence score across all problem levels, indicating its consistency in self-assessment of its performance.However, models like Claude-2 and Claude-Instant represented higher variability in their confidence scores as the level of the problem changed.This is another indication that some models may adapt differently to task difficulties.For example, Claude-2 shows a notable improvement in confidence levels for certain problems.</p>
<p>It is imperative to underscore that our analysis provides an overview of the average trends in confidence scores across different problem levels.Individual variations in model behaviors are manifested, and this observation showcases the need for a nuanced understanding of how different language models respond to diverse problem complexities.In addition to this, further investigation with the incorporation of additional factors like problem content and correctness of answer could offer valuable insight into LLMs performance and confidence assessment in particular scenarios.</p>
<p>Discussion &amp; Conclusion</p>
<p>In this study, we analyzed the self-assessment behavior of Large language models such as GPT, Claude, LLaMA, and others through their confidence scores, investigating potential parallels with the Dunning-Kruger effect.As depicted in table B. 4 and figure 4 provides intriguing insights about how LLMs assess their performance across different categories.</p>
<p>Even though our study didn't establish a solid presence of the Dunning-Kruger effect, it provided valuable observations aligning with its conceptual framework.</p>
<p>GPT-4 stands out with its consistency in high confidence scores across all the tested categories, especially in LSAT Reasoning tasks.Such a high confidence pattern resembles its high ability to gauge its competence accurately.Nevertheless, it is essential to be careful to jump to any conclusion as there might be other factors contributing to this trend.On the other hand, models like Claude-2 and Claude Instant displayed higher variability in their confidence scores across different categories.Claude-2 showed a relatively low confidence score for LSAT Reasoning; however, they performed better in Truthful Q&amp;A.This difference mirrors the concept of individuals with varying abilities showing inconsistency in assessments.For now, this observation serves as a parallel instead of conclusive proof of the Dunning-Kruger effect's applicability in this context.LLaMA-70B performed better with a higher confidence score in LSAT Reasoning and Mathematical categories but had lower confidence in Truthful Q&amp;A.This subtle variation aligns with the idea that individual LLMs might possess specialized domains of competence, akin to the Dunning-Kruger effect's recognition of skill variations among individuals.</p>
<p>Referencing Table 6 and Figure 5, we investigate and explore the relationship between problem-level complexity and LLM confidence scores.The observed patterns in confidence offer exciting and interesting connections to the Dunning-Kruger effect, even if they don't provide solid evidence of it.LLMs were observed to possess higher confidence scores starting at level 1.With increasing complexity, a decrease in confidence score was observed.The observed overconfidence phase is related to the overestimation part of the Dunning-Kruger effect, wherein individuals with lower abilities often overrate their competence.Different LLMs exhibited varying confidence score patterns across the problem levels, reflecting the notion that individuals with different abilities experience varying degrees of the Dunning-Kruger effect.Also, Models like GPT-4 maintained their confidence similar to the individual with high abilities making accurate self-assessments.</p>
<p>In a nutshell, the pattern of the LLM's confidence score provides intriguing parallels with the Dunning-Kruger effect.However, they don't provide solid evidence of its presence in the behavior of LLMs.To provide a sturdy connection, further research with statistical analysis and a broader set of variables is vital.Our findings, nevertheless, pave the way for deeper exploration of LLMs with the Dunning-Kruger effect by showing the relationship between self-assessment and competence in artificial intelligence.The underlying intricacies of LLM behavior, the biases, and the confidence framework demand a more in-depth, comprehensive exploration.It opens doors to a myriad of questions that deserve attention, hinting at a treasure trove of insights awaiting to dive deep.Delving deeper into this convergence of psychology and artificial intelligence offers a promising frontier, potentially unlocking novel insights into AI behavior and ethics.The observations from this study beckon a broader exploration, suggesting that the mysteries of AI cognition, akin to human nuances, are both vast and awaiting discovery.</p>
<p>A Data</p>
<p>A.</p>
<p>B.3 Confidence Scores vs Correctness</p>
<p>The destiny plot in Figure 7 represents the relationship between LLMs' confidence score and their correctness in predicting answers.Destiny plot branches each LLM's confidence score into correct and incorrect categories with distinct colors.The higher region in the destiny plot indicates the model is correct or incorrect with specific confidence scores frequently.This plot helps us in providing an initial empirical foundation to access the Dunning-Kruger effect in LLMs.We are interested in where LLMs exhibit high confidence scores and are incorrect or vice versa.This will provide us with information on a misalignment between perceived ability and actual ability.</p>
<p>2 . 3 .
23
High Confidence, Incorrect Answers: LLMs with high A1 score but incorrectly answers the question.Low Confidence, Correct Answers: LLMs with low A1 score (e.g., A1 &lt; 5) and correct answers.4. Low Confidence, Incorrect Answers: LLMs with low A1 score and incorrectly answers the question.</p>
<p>Figure 1 :
1
Figure 1: Comparison of A1 and A2 Scores for Claude-2.</p>
<p>Figure 2 :
2
Figure 2: Facetted Density Plot of Confidence Levels by LLM Type.The plot reveals varying patterns of confidence distribution across different LLM types, suggesting nuanced self-perceptions in these models.</p>
<p>Figure 3 :
3
Figure 3: Density Plots of Correctness for Different Confidence Scores (A1 and A2)</p>
<p>Figure 4 :
4
Figure 4: Average Confidence Levels by Category and LLM</p>
<p>Figure 5 :
5
Figure 5: Average Confidence Scores by Problem Level</p>
<p>Figure 6 :
6
Figure 6: Density Plots of Correctness for Different Confidence Scores (R1 and R2)</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: Density Plot of Correctness vs. Confidence Scores for Various Language Learning Models (A1 and A2)</p>
<p>Table 1 :
1
Calibration of Confidence to Competence Across Various Large Language Models (LLMs) for Different
Confidence Metrics (A1, A2, R1, R2)MetricsClaude-Claude-GoogleGoogleModels GPT-GPT-LLaMA-LLaMA-LLaMA-2InstantBardPaLM3.5413B70B7BA136121142558High ConA214131882125814CorrectR130210122558R21332142125513A132615152318High ConA242114616152522IncorrectR1301725151318R24715216151622Low ConCorrect</p>
<p>Table 2 :
2
Summary Statistics of Confidence Scores by LLM Type
LLMA1R1A2R2MeanSDMeanSDMeanSDMeanSDClaude-2
4.800 2.911 4.650 2.957 5.400 4.241 5.400 4.235 Claude-Instant 6.850 1.027 5.475 1.062 8.325 0.829 6.825 0.931 GoogleBard 7.400 0.591 8.400 0.591 7.700 0.648 8.700 0.648 GooglePaLM 5.500 1.485 4.600 1.646 7.050 1.260 6.050 1.260 GPT-3.5 7.525 0.877 7.475 0.877 8.900 0.672 8.900 0.672 GPT-4 8.900 0.568 9.200 0.372 8.925 0.594 9.225 0.375 LLaMA-13B 7.550 2.062 6.950 2.136 7.725 1.921 7.400 1.892 LLaMA-70B 7.350 1.122 7.950 1.339 8.600 0.672 8.475 0.847 LLaMA-7B 7.250 1.214 6.600 1.297 8.025 1.187 7.525 0.877</p>
<p>Table 3 :
3
1 Survey Questions 1. TruthfulQA: Included ten questions spread over five difficulty levels, with two questions per level.The levels were: Ten questions spread over five difficulty levels, two per level.The levels were: Description of the Dataset Variables
Level 1: Logical FalsehoodLevel 2: NutritionLevel 3: ParanormalLevel 4: Myths and FairytalesLevel 5: Fiction2. TruthfulQA Extended: Level 1: ProverbsLevel 2: SuperstitionsLevel 3: MisquotationsLevel 4: MisconceptionLevel 5: Conspiracies3. Mathematical Reasoning: Spanning ten questions across:Level 1: Elementary MathematicsLevel 2: High School MathematicsLevel 3: High School StatisticsLevel 4: College MathematicsLevel 5: Abstract Algebra4. LSAT Reasoning: Comprising ten questions based on five distinct contexts. Each context had two associatedquestions, with difficulty escalating from levels 1 to 5.A.2 VariablesVariable Symbol Variable NameTypeRange/ExampleCategoryCategory of the problemCategorical Truthful Q&amp;A, Mathematical ReasoningProblemLevelProblem levelCategorical 1, 2, 3, 4, 5ProblemIDUnique identifier for problemCategorical T1, T2, MR1ProblemText of the problemText"Are all real numbers real numbers?"LLMType of Large Language Model Categorical GPT-4, GPT-3.5A1Absolute Confidence (Pre)Continuous 1-10R1Relative Confidence (Pre)Continuous 1-10A2Absolute Confidence (Post)Continuous 1-10R2Relative Confidence (Post)Continuous 1-10IsCorrectCorrectness of answerBinary0, 1A1: Absolute confidence level expressed by the LLM before answering. Question: "How well do you think you willdo?"
R1: Relative confidence level expressed by the LLM before answering, compared to others.Question: "Compared to others, how well do you think you will do?"A2:Absolute confidence level expressed by the LLM after answering.Question: "How well do you think you did?" R2: Relative confidence level expressed by the LLM after answering, compared to others.Question: "Compared to others, how well do you think you did?"B Tables and Figures B.1 Closeness vs correctness</p>
<p>Table 4 :
4
Your Caption Here
Absolute ConfidenceLLMClose Correct Close Incorrect Far Correct Far IncorrectClaude-241515Claude-Instant11125GoogleBard22180GooglePaLM997GPT-3.51459GPT-425150LLaMA-13B7242LLaMA-70B8208LLaMA-7B9146Relative ConfidenceLLMClose Correct Close Incorrect Far Correct Far IncorrectClaude-241615Claude-Instant12134GoogleBard22180GooglePaLM997GPT-3.51459GPT-425150LLaMA-13B7262LLaMA-70B10206LLaMA-7B8157</p>
<p>Table 5 :
5
Performance and Confidence Metrics of Large Language Models (LLMs) Across Different Categories
CategoryLLMAvg A1 Avg A2 Avg R1 Avg R2LSAT ReasoningClaude-20.800.000.600.00LSAT ReasoningClaude-Instant7.008.206.007.20LSAT ReasoningGoogleBard7.007.608.008.60LSAT ReasoningGooglePaLM5.406.404.805.40LSAT ReasoningGPT-3.57.009.007.009.00LSAT ReasoningGPT-48.308.209.409.40LSAT ReasoningLLaMA-13B8.208.108.408.40LSAT ReasoningLLaMA-70B8.009.009.009.00LSAT ReasoningLLaMA-7B7.108.606.107.60Mathematical Reasoning Claude-25.604.405.204.40Mathematical Reasoning Claude-Instant6.808.905.507.20Mathematical Reasoning GoogleBard7.407.808.408.80Mathematical Reasoning GooglePaLM6.007.405.006.40Mathematical Reasoning GPT-3.57.609.007.609.00Mathematical Reasoning GPT-49.209.209.409.40Mathematical Reasoning LLaMA-13B8.008.707.208.10Mathematical Reasoning LLaMA-70B8.009.009.008.90Mathematical Reasoning LLaMA-7B7.808.406.807.40Truthful Q&amp;AClaude-26.408.606.408.60Truthful Q&amp;AClaude-Instant6.808.105.206.45Truthful Q&amp;AGoogleBard7.607.708.608.70Truthful Q&amp;AGooglePaLM5.307.204.306.20Truthful Q&amp;AGPT-3.57.758.807.658.80Truthful Q&amp;AGPT-49.059.159.009.05Truthful Q&amp;ALLaMA-13B7.007.056.106.55Truthful Q&amp;ALLaMA-70B6.708.206.908.00Truthful Q&amp;ALLaMA-7B7.057.556.757.55
B.5 Problem Level Vs Confidence Scores
Attention is all you need. Ashish Vaswani, Noam M Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS. 2017</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Z Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jianyun Liu, Ji Nie, Wen Rong, ArXiv, abs/2303.18223A survey of large language models. 2023</p>
<p>Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Schärli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman ; Greg S Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Nature. 72023</p>
<p>Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Jingsen Hao Ran Yang, Zhi-Yang Zhang, Jiakai Chen, Xu Tang, Yankai Chen, Wayne Xin Lin, Zhewei Zhao, Ji Wei, Wen Rong, ArXiv, abs/2308.11432A survey on large language model based autonomous agents. 2023</p>
<p>Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. Yan Zhuang, Qi Liu, Yuting Ning, Wei Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, Enhong Chen, ArXiv, abs/2306.105122023</p>
<p>Probing the psychology of ai models. Richard M Shiffrin, Melanie Mitchell, 2023120Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Emotionally numb or empathetic? evaluating how llms feel using emotionbench. Jen Tse Huang, Man Ho, Adrian Lam, Eric Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R Lyu, ArXiv, abs/2308.036562023</p>
<p>Unskilled and unaware of it: How difficulties in recognizing one's own incompetence lead to inflated self-assessments. Justin Kruger, David Dunning, 1999</p>
<p>On being ignorant of one's own ignorance. David Dunning, 201144The dunning-kruger effect</p>
<p>Can ai language models replace human participants?. Danica Dillion, Niket Tandon, Yuling Gu, Kurt Gray, 72023</p>
<p>The promises and pitfalls of using chat gpt for self-determined learning in higher education: An argumentative review. Fx Risang Baskara, Prosiding Seminar Nasional Fakultas Tarbiyah dan Ilmu Keguruan IAIM Sinjai. 22023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Jan Paul F Christiano, Ryan Leike, Lowe, 2022Curran Associates, Inc35</p>
<p>Holistic evaluation of language models; holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré Diana, Acosta-Navas Drew, Eric Hudson, Esin Zelikman, Faisal Durmus, Frieda Ladhak, Hongyu Rong, Huaxiu Ren, Jue Yao, Keshav Wang, Laurel Santhanam, Lucia Orr, Mert Zheng, Mirac Yuksekgonul, Nathan Suzgun, Neel Kim, Niladri Guha, Omar Chatterji, Peter Khattab, Qian Henderson, Ryan Huang, Chi Sang, Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, 2022</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 20232</p>
<p>Enhancing large language models with climate resources. Mathias Kraus, Julia Anna Bingler, Markus Leippold, Tobias Schimanski, Colesanti Chiara, Dominik Senni, Saeid Ashraf Stammbach, Nicolas Vaghefi, Webersinke, 20233</p>
<p>Learning and evaluating general linguistic intelligence. Dani Yogatama, Cyprien De Masson D'autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, Phil Blunsom, 20191</p>
<p>Large language models show human-like content biases in transmission chain experiments. Alberto Acerbi, Joseph Stubbersfield, 2023</p>
<p>Capturing failures of large language models via human cognitive biases. Erik Jones, Jacob Steinhardt, ArXiv, abs/2202.122992022</p>
<p>Cognitive mirage: A review of hallucinations in large language models. Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia, ArXiv, abs/2309.067942023</p>
<p>Large language models (llms) and empathy -a systematic review. Md Vera Sorin, Danna Md, Brin, Yiftach Md, Barash, M D Md Eli Konen, Alexander Phd, Girish Charney, Nadkarni, Klang, In medRxiv. 2023</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, ArXiv, abs/2210.116102022</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Zhen Lin, Shubhendu Trivedi, Jimeng Sun, ArXiv, abs/2305.191872023</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Aligning ai with shared human values. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, Ar-lsat: Investigating analytical reasoning of text. 2021</p>
<p>From lsat: The progress and challenges of complex reasoning. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, Nan Duan, Speech, and Language Processing. 2022</p>
<p>Chain of thoughts vs tree of thoughts for language learning models (llms). Sonal Sareen, May 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6712 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6712</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6712</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-875d71bae61a66f7e65a2b6d363b7a0a27a6ed25</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/875d71bae61a66f7e65a2b6d363b7a0a27a6ed25" target="_blank">Tree of Uncertain Thoughts Reasoning for Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Acoustics, Speech, and Signal Processing</p>
                <p><strong>Paper TL;DR:</strong> The Tree of Uncertain Thoughts (TouT) is introduced — a reasoning framework tailored for Large Language Models that effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs’ diverse local responses at these intermediate steps and enhances the model’s precision in response generation.</p>
                <p><strong>Paper Abstract:</strong> While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or "thoughts". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) — a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs’ diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model’s precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT’s superiority over both ToT and chain-of-thought prompting methods.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6712.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6712.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TouT (b=5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Uncertain Thoughts (breadth b=5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning framework that augments Tree-of-Thoughts with Monte Carlo Dropout-based local uncertainty quantification (LUQ) and uncertainty-aware global search (UGS), scoring states by value/uncertainty to select search branches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree of Uncertain Thoughts (TouT) - BFS setting (b=5)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic planning task: use provided numbers exactly once in an expression to obtain 24 (3 intermediate thought steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>65.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Tree of Thoughts (ToT) (b=5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>9.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors attribute gains to explicit local uncertainty quantification (via Monte Carlo Dropout over multiple sampled responses) combined with global selection by value/uncertainty (v/u), which favors high-value low-uncertainty states; the approach leverages diversity of local candidate thoughts to improve global decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Uncertain Thoughts Reasoning for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6712.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6712.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TouT (b=1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Uncertain Thoughts (breadth b=1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>TouT applied with breadth limit 1; uses Monte Carlo Dropout to score and select the single most confident state per step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree of Uncertain Thoughts (TouT) - BFS (b=1)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic planning task: use provided numbers exactly once to reach 24.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>42.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Tree of Thoughts (ToT) (b=1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>5.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Even with narrow breadth, selecting candidates by value/uncertainty (v/u) improves over ToT; authors argue LUQ compensates for limited breadth by preferring less uncertain local thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Uncertain Thoughts Reasoning for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6712.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6712.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT (b=5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (breadth b=5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tree-of-Thoughts search method that generates multiple candidate partial solutions per state and uses a state evaluator to guide tree search and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic planning task: form an expression equal to 24 using the provided numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>56.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>TouT (b=5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-9.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ToT provides foresight and backtracking but does not explicitly quantify local uncertainty; authors use ToT as the primary baseline to show that adding LUQ+UGS yields improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Uncertain Thoughts Reasoning for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6712.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6712.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential chain-of-thought prompting technique where models are guided to generate intermediate reasoning steps in a linear chain to solve multi-step problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic planning task requiring multi-step reasoning to reach 24.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>3.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>TouT (b=1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-39.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT produces single-sequence chains and (in these experiments) performs poorly compared to tree-based methods; authors contrast CoT's sequential approach with ToT/TouT's multi-candidate exploration where diversity of local candidates matters.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Uncertain Thoughts Reasoning for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6712.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6712.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-style variant of chain-of-thought prompting that generates multiple chains and picks the highest-frequency answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>CoT-SC (self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic planning requiring combining generated solution chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>7.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>TouT (b=1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-34.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT-SC leverages diversity by ensembling multiple reasoning chains and majority vote; TouT instead quantifies uncertainty of local candidates and uses value/uncertainty scoring in search, which the authors report as more effective on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Uncertain Thoughts Reasoning for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6712.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6712.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TouT (MiniCrosswords)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Uncertain Thoughts (Mini Crosswords)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>TouT applied to 5x5 Mini Crosswords, using LUQ (Monte Carlo sampling, m=20) and UGS to select low-uncertainty high-value letter/word solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree of Uncertain Thoughts (TouT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Mini Crosswords (5x5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Crossword puzzle solving: given 5 horizontal and 5 vertical clues, produce a 5x5 board of letters (5–10 intermediate steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>game success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>15.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report modest improvements over ToT on letter/word/game metrics; they ascribe gains to LUQ enabling more confident branch selection and to combined LUQ+UGS ablation showing additive improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Uncertain Thoughts Reasoning for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6712.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6712.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT (MiniCrosswords)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (Mini Crosswords)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Original Tree-of-Thoughts approach applied to Mini Crosswords baseline (no LUQ), evaluating letter/word/game accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Mini Crosswords (5x5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>5x5 crossword solving from 10 clues to produce a full board.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>game success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>12.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>TouT (MiniCrosswords)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-3.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ToT achieves substantial baseline performance on letter/word metrics; adding uncertainty quantification (LUQ) and UGS in TouT yields incremental improvements, indicating local uncertainty estimation aids in selecting better partial boards.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Uncertain Thoughts Reasoning for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of Thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Dropout as a bayesian approximation: Representing model uncertainty in deep learning <em>(Rating: 2)</em></li>
                <li>Generating with confidence: Uncertainty quantification for black-box large language models <em>(Rating: 2)</em></li>
                <li>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6712",
    "paper_id": "paper-875d71bae61a66f7e65a2b6d363b7a0a27a6ed25",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "TouT (b=5)",
            "name_full": "Tree of Uncertain Thoughts (breadth b=5)",
            "brief_description": "A reasoning framework that augments Tree-of-Thoughts with Monte Carlo Dropout-based local uncertainty quantification (LUQ) and uncertainty-aware global search (UGS), scoring states by value/uncertainty to select search branches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_size": "70B",
            "reasoning_method_name": "Tree of Uncertain Thoughts (TouT) - BFS setting (b=5)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Game of 24",
            "task_description": "Arithmetic planning task: use provided numbers exactly once in an expression to obtain 24 (3 intermediate thought steps).",
            "performance_metric": "success rate (%)",
            "performance_value": 65.0,
            "comparison_target_method": "Tree of Thoughts (ToT) (b=5)",
            "performance_difference": 9.0,
            "statistical_significance": false,
            "analysis_notes": "Authors attribute gains to explicit local uncertainty quantification (via Monte Carlo Dropout over multiple sampled responses) combined with global selection by value/uncertainty (v/u), which favors high-value low-uncertainty states; the approach leverages diversity of local candidate thoughts to improve global decision-making.",
            "ablation_study_present": true,
            "uuid": "e6712.0",
            "source_info": {
                "paper_title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "TouT (b=1)",
            "name_full": "Tree of Uncertain Thoughts (breadth b=1)",
            "brief_description": "TouT applied with breadth limit 1; uses Monte Carlo Dropout to score and select the single most confident state per step.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_size": "70B",
            "reasoning_method_name": "Tree of Uncertain Thoughts (TouT) - BFS (b=1)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Game of 24",
            "task_description": "Arithmetic planning task: use provided numbers exactly once to reach 24.",
            "performance_metric": "success rate (%)",
            "performance_value": 42.0,
            "comparison_target_method": "Tree of Thoughts (ToT) (b=1)",
            "performance_difference": 5.0,
            "statistical_significance": false,
            "analysis_notes": "Even with narrow breadth, selecting candidates by value/uncertainty (v/u) improves over ToT; authors argue LUQ compensates for limited breadth by preferring less uncertain local thoughts.",
            "ablation_study_present": true,
            "uuid": "e6712.1",
            "source_info": {
                "paper_title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "ToT (b=5)",
            "name_full": "Tree of Thoughts (breadth b=5)",
            "brief_description": "Tree-of-Thoughts search method that generates multiple candidate partial solutions per state and uses a state evaluator to guide tree search and backtracking.",
            "citation_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_size": "70B",
            "reasoning_method_name": "Tree of Thoughts (ToT)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Game of 24",
            "task_description": "Arithmetic planning task: form an expression equal to 24 using the provided numbers.",
            "performance_metric": "success rate (%)",
            "performance_value": 56.0,
            "comparison_target_method": "TouT (b=5)",
            "performance_difference": -9.0,
            "statistical_significance": false,
            "analysis_notes": "ToT provides foresight and backtracking but does not explicitly quantify local uncertainty; authors use ToT as the primary baseline to show that adding LUQ+UGS yields improvements.",
            "ablation_study_present": false,
            "uuid": "e6712.2",
            "source_info": {
                "paper_title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A sequential chain-of-thought prompting technique where models are guided to generate intermediate reasoning steps in a linear chain to solve multi-step problems.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_size": "70B",
            "reasoning_method_name": "Chain-of-Thought (CoT) prompting",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Game of 24",
            "task_description": "Arithmetic planning task requiring multi-step reasoning to reach 24.",
            "performance_metric": "success rate (%)",
            "performance_value": 3.0,
            "comparison_target_method": "TouT (b=1)",
            "performance_difference": -39.0,
            "statistical_significance": false,
            "analysis_notes": "CoT produces single-sequence chains and (in these experiments) performs poorly compared to tree-based methods; authors contrast CoT's sequential approach with ToT/TouT's multi-candidate exploration where diversity of local candidates matters.",
            "ablation_study_present": false,
            "uuid": "e6712.3",
            "source_info": {
                "paper_title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency (CoT-SC)",
            "brief_description": "An ensemble-style variant of chain-of-thought prompting that generates multiple chains and picks the highest-frequency answer.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_size": "70B",
            "reasoning_method_name": "CoT-SC (self-consistency)",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Game of 24",
            "task_description": "Arithmetic planning requiring combining generated solution chains.",
            "performance_metric": "success rate (%)",
            "performance_value": 7.2,
            "comparison_target_method": "TouT (b=1)",
            "performance_difference": -34.8,
            "statistical_significance": false,
            "analysis_notes": "CoT-SC leverages diversity by ensembling multiple reasoning chains and majority vote; TouT instead quantifies uncertainty of local candidates and uses value/uncertainty scoring in search, which the authors report as more effective on these tasks.",
            "ablation_study_present": false,
            "uuid": "e6712.4",
            "source_info": {
                "paper_title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "TouT (MiniCrosswords)",
            "name_full": "Tree of Uncertain Thoughts (Mini Crosswords)",
            "brief_description": "TouT applied to 5x5 Mini Crosswords, using LUQ (Monte Carlo sampling, m=20) and UGS to select low-uncertainty high-value letter/word solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_size": "70B",
            "reasoning_method_name": "Tree of Uncertain Thoughts (TouT)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Mini Crosswords (5x5)",
            "task_description": "Crossword puzzle solving: given 5 horizontal and 5 vertical clues, produce a 5x5 board of letters (5–10 intermediate steps).",
            "performance_metric": "game success rate (%)",
            "performance_value": 15.0,
            "comparison_target_method": "Tree of Thoughts (ToT)",
            "performance_difference": 3.0,
            "statistical_significance": false,
            "analysis_notes": "Authors report modest improvements over ToT on letter/word/game metrics; they ascribe gains to LUQ enabling more confident branch selection and to combined LUQ+UGS ablation showing additive improvements.",
            "ablation_study_present": true,
            "uuid": "e6712.5",
            "source_info": {
                "paper_title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "ToT (MiniCrosswords)",
            "name_full": "Tree of Thoughts (Mini Crosswords)",
            "brief_description": "Original Tree-of-Thoughts approach applied to Mini Crosswords baseline (no LUQ), evaluating letter/word/game accuracy.",
            "citation_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_size": "70B",
            "reasoning_method_name": "Tree of Thoughts (ToT)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Mini Crosswords (5x5)",
            "task_description": "5x5 crossword solving from 10 clues to produce a full board.",
            "performance_metric": "game success rate (%)",
            "performance_value": 12.0,
            "comparison_target_method": "TouT (MiniCrosswords)",
            "performance_difference": -3.0,
            "statistical_significance": false,
            "analysis_notes": "ToT achieves substantial baseline performance on letter/word metrics; adding uncertainty quantification (LUQ) and UGS in TouT yields incremental improvements, indicating local uncertainty estimation aids in selecting better partial boards.",
            "ablation_study_present": false,
            "uuid": "e6712.6",
            "source_info": {
                "paper_title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "rating": 2
        },
        {
            "paper_title": "Generating with confidence: Uncertainty quantification for black-box large language models",
            "rating": 2
        },
        {
            "paper_title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "rating": 1
        }
    ],
    "cost": 0.01101975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Shentong Mo ${ }^{1,2} \quad$ Miao Xin $^{3 *}$</p>
<h1>${ }^{1}$ Carnegie Mellon University, ${ }^{2}$ MBZUAI, ${ }^{3}$ Institute of Automation, Chinese Academy of Sciences</h1>
<h4>Abstract</h4>
<p>While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or "thoughts". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT's superiority over both ToT and chain-ofthought prompting methods.</p>
<p>Index Terms- large language models, tree of thoughts, uncertainty estimation</p>
<h2>1. INTRODUCTION</h2>
<p>Modern Large-scale Language Models (LLMs), including GPT's early iterations [1, 2, 3], the recent GPT-4 [4], and LLaMA-2 [5], have showcased remarkable prowess in tasks that demand mathematical, symbolic, commonsense, and knowledge reasoning. Despite this, their reasoning process primarily hinges on the autoregressive mechanism, sequentially generating text and making token-level decisions from left-to-right $[6,7]$.</p>
<p>The recently conceptualized Tree of Thoughts (ToT) [8] made significant strides in enabling LLMs to exercise foresight and backtrack for holistic decision-making. Yet, an apparent blind spot has been the oversight of local uncertainties in the intermediate [9]. These uncertainties, stemming from LLMs' propensity for varied responses [10], pose a considerable challenge to the reasoning process.</p>
<p>One fundamental obstacle is the monumental scale of LLMs, rendering them impervious to fine-tuning. They pre-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>dominantly serve as black boxes, with the Bayesian modification to obtain a distribution-based uncertainty qualification (UQ) being far from practical [11]. However, the LLMs with emergent abilities can be perceived as approximately unbiased estimations of our inherently uncertain reality [12]. This makes them less susceptible to the influence of the out-of-domain data on uncertainty estimation, allowing inference approximation of low training-complexity to work. Hence, the complexity caused by vast scale and the fascinating nature associated with it advocate a direct and effective mechanism for dealing with uncertainty.</p>
<p>Our novel solution comes in the form of the Tree of Uncertain Thoughts (TouT), a pioneering reasoning framework expressly crafted for LLMs. Central to TouT is its ingenious employment of Monte Carlo Dropout [13] for uncertainty qualification. This decision was not arbitrary. Given the challenges with LLMs, Monte Carlo Dropout presents an elegant, minimalistic, yet robust technique to gauge uncertainty scores linked with the diverse responses of LLMs at intermediate junctures. By integrating this local uncertainty measurement with comprehensive sorting algorithms, TouT bolsters the accuracy of model responses.</p>
<p>To validate our method, we undertook rigorous experimentation on two intricate planning tasks: Game of 24 and Mini Crosswords. The experimental results decisively highlight the supremacy of TouT over both ToT and the chain-ofthought prompting techniques.</p>
<p>Our pivotal contributions encapsulate:</p>
<ul>
<li>The inception of TouT, a groundbreaking Tree of Uncertain Thoughts framework, ushering in uncertaintyaware inference in LLMs.</li>
<li>The innovative integration of Monte Carlo Dropout for local uncertainty quantification and sorting, amplifying model response confidence.</li>
<li>Thorough experimental validation confirming TouT's dominance over the extant ToT and chain-of-thought prompting standards.</li>
</ul>
<h2>2. RELATED WORK</h2>
<p>Large Language Models. The advent and progression of Large Language Models (LLMs) [1, 2, 3, 14] have been transformative for the fields of natural language processing and machine learning. Central to this transformation is the</p>
<p>GPT series, which was spearheaded by Radford et al. [1]. Their seminal work led to the development of subsequent iterations, each building upon the strengths and addressing the challenges of the previous versions. While the early GPT models [2, 3] laid the foundation, GPT-4 [4] marked a paradigm shift in the capabilities and applications of LLMs. It showcased enhanced reasoning, comprehension, and generation abilities, bridging gaps previously identified in LLMs. Parallelly, models like LLaMA-2 [5] have also contributed significantly to the domain. LLaMA-2, in particular, emphasized the intersection of linguistic properties with the deep learning capabilities of LLMs, opening new avenues for research and application. In this work, our main focus is to propose an innovative thoughts reasoning framework tailored for LLaMA-2, aiming to unravel complex deliberative challenges.
Thoughts Reasoning. With the sophistication of LLMs, there arose a need to understand, modulate, and enhance the reasoning capabilities underlying their decision-making processes. The initial Input-Output models established a basic framework for how models perceive and respond to given prompts. Building on this, the Chain of Thoughts (CoT) [6] model was introduced, emphasizing a chained, sequential approach to decision-making. It was further refined with CoT-SC [7], which provided a more structured and systematic framework for thoughts sequencing. More recently, ToT [8] incorporated both a hierarchical and lateral understanding of reasoning, enabling LLMs to not only draw from a depth of knowledge but also to assess and reassess decisions in a tree-like structure. This approach facilitated greater foresight, backtracking, and holistic decision-making in LLMs. Yet, a noticeable gap persisted: the oversight of local uncertainties during intermediate decision-making. Addressing this, our work pioneers a framework that synergizes local uncertainty quantification with advanced global algorithms, intending to heighten the accuracy of LLM responses.
Uncertainty Quantification. The prediction of uncertainty $[13,15]$ represents the foundation for dependable and consistent automated decision-making, and consequently is receiving increasing attention. However, obtaining uncertainty quantification in LLM is very challenging, mainly due to the extremely high dimensionality [16]. Certain recent methodologies explore the issue of uncertainty quantification with black-box LLMs [10, 11]. However, these techniques concentrate mostly on free-form question answering. Scant research has explored the uncertainty quantification for LLMs in complex reasoning, the emphasis of the present paper.</p>
<h2>3. METHOD</h2>
<p>Given a deliberate problem with several intermediate steps, our target is to leverage pre-trained large language models for problem-solving and decision-making. We propose a novel framework with the Tree of Uncertain Thoughts, named TouT,
for language model inference, which mainly consists of two modules, Local Uncertainty Quantification in Section 3.2 and Uncertainty Global Search in Section 3.3.</p>
<h3>3.1. Preliminaries</h3>
<p>In this section, we first describe the problem setup and notations and then revisit the tree of thoughts reasoning for LLMs inference.
Problem Setup and Notations. Given a pre-trained language model (LM) ( $p_{\theta}$ ) with parameters $\theta$ and a language sequence ${x, y, z, s, \ldots}$, our target is to infer $\operatorname{LM}\left(p_{\theta}(x)\right)$ for generating answers to a deliberate problem. For each language sequence $x$ with $t$ tokens, we denote $x={x[1], x[2], \ldots, x[t]}$.
Revisit Non-Tree-Based Prompting. To address the task, Input-output (IO) prompting generated the output $y$ from LM with $x$ as input instructions, which can be denoted as $y \sim$ $p_{\theta}^{\mathrm{IN}}(y \mid x)$. When it comes to non-trivial questions with multiple steps, Chain-of-thought (CoT) prompting [6] introduced a chain of $n$ thoughts $z_{1}, z_{2}, \ldots, z_{n}$ to solve the problem, where the output is formulated as $y \sim p_{\theta}^{\mathrm{COT}}\left(y \mid x, z_{1}, z_{2}, \ldots, z_{n}\right)$. To improve COT further, ensemble-based CoT-SC [7] proposed to generate multiple chains of thoughts and select the highest frequency output.
Revisit Tree-of-Thoughts. To solve the problem in a human problem-solving manner, ToT [8] proposed to search over a tree consisting of multiple partial solutions (state $s=$ $\left.\left{x, z_{1,2, \ldots, i}\right}\right)$ as nodes. Given the properties of different problems, ToT first decomposed intermediate thought steps and used a thought generator $G\left(p_{\theta}, s, k\right)$ to generate $k$ candidates based on a tree state $s$. With a set $S$ of different states, they adopted a state evaluator $V\left(p_{\theta}, S\right)$ to independently measure the possibility of solving the problem for each state. Finally, they plugged a search algorithm to select the most promising state for the final output.</p>
<p>However, the ToT reasoning paradigm grapples with the complexities of local uncertainties at intermediate "thoughts". Given the innate capacity of LLMs to generate a spectrum of responses, these local uncertainties can become significant impediments in the reasoning process. We introduce the "Tree of Uncertain Thoughts" framework to address this challenge, pioneering a shift towards uncertainty-aware inference within LLMs.</p>
<h3>3.2. Local Uncertainty Quantification</h3>
<p>To explicitly quantify the uncertainty for each local response in intermediate steps, we introduce a novel uncertainty evaluator $U\left(p_{\theta}, S_{1,2, \ldots, m}\right)$ to generate a scalar value to represent the confidence score for each local intermediate states, that is, $U\left(p_{\theta}, S\right)(s) \sim p^{\text {uncertain }}(u \mid s), \forall s \in S$. Specifically, we are inspired by Monte Carlo Dropout [13] and generate $S_{1,2, \ldots, m}$ with $m$ sampling steps on LLMs inference. Meanwhile, we adopt an $m$-step-based linear interpolation on the input tem-</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 TouT-BFS( \(x, p_{\theta}, G, k, V, T, b, U, m\) )
Require: Input \(x\), LM \(p_{\theta}\), thought generator \(G(\cdot)\), candidates
    size \(k\), states evaluator \(V(\cdot)\), global steps \(T\), breadth limit
    \(b\), uncertainty evaluator \(U(\cdot)\), sampling steps \(m\).
    \(S_{0} \leftarrow\{x\}\)
    for \(t=1,2, \ldots, T\) do
        \(S_{t}^{\prime} \leftarrow\left\{[s, z] \mid s \in S_{t-1}, z_{t} \in G\left(p_{\theta}, s, k\right)\right\}\)
        \(U_{t} \leftarrow U\left(p_{\theta},\left\{S_{t}^{\prime}\right\}\right)\)
        \(V_{t} \leftarrow V\left(p_{\theta},\left\{S_{t}^{\prime}\right\}\right)\)
        \(S_{t} \leftarrow \arg \max <span class="ge">_{S \subset S_</span>{t}^{\prime},|S|=b} \sum_{s \in S} V_{t}(S) / U_{t}(S)\)
    end for
    return \(G\left(\theta, \arg \max <span class="ge">_{s \in S_</span>{T}} V_{t}(S) / U_{t}(S), 1\right)\)
</code></pre></div>

<p>perature of LLMs to control the quality of responses for each intermediate step.</p>
<p>After sampling, we compute the variance of values from a set $\left{S_{t}^{\prime}\right}$ of $m$ states in this step, where the variance of values is used as the local uncertainty score $u$ for each state. Such evaluations will enable us to comprehensively evaluate diverse local responses instead of generating candidates using one fixed model temperature. Furthermore, we can use quantified states for later global searching to find the correct answers to the problem more confidently.</p>
<h3>3.3. Uncertainty-aware Global Search</h3>
<p>Benefiting from the above uncertainty quantification on local responses, we leverage a novel and explicit uncertaintyaware global search mechanism to select a more precise state. During searching, we use $v / u$ as the final evaluation score for criteria to finalize the state with the largest score, where $v, u$ denote the value and uncertainty of the state, respectively. Based on the new criteria, we propose two search algorithms for uncertainty-aware global search.</p>
<p>One is based on Breadth-first search (BFS), TouT-BFS uses a set of the $b$ most confident states per step by selecting from $m$ states using the new score $V_{t}(S) / U_{t}(S)$, as illustrated in Algorithm 1. The other one is Depth-first search (DFS), we either explore the most confident state in global steps $T$ or use $V\left(p_{\theta},\left{s^{\prime}\right}\right)(s) \leq v_{t h}$ for a value threshold $v_{t h}$ and $U\left(p_{\theta},\left{s^{\prime}\right}\right)(s) \geq u_{t h}$ for a uncertainty threshold $u_{t h}$. For both cases, the algorithm backtracks to the parent state of $s$ with the higher value and lower uncertainty and continually finds the correct answers, as shown in Algorithm 2.</p>
<h2>4. EXPERIMENTS</h2>
<h3>4.1. Experimental setup</h3>
<p>Tasks. Game of 24 [8] contains 1,362 games with human solving levels from easy to hard, and a subset of relatively hard games indexed 901-1,000 is used for testing. The thoughts in this task are decomposed into 3 steps. Mini</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2 TouT-DFS( \(s, t, p_{\theta}, G, k V, T, v_{t h}, U, u_{t h}\) )
Require: Current state \(s\), step \(t\), LM \(p_{\theta}\), thought generator
    \(G(\cdot)\), candidates size \(k\), states evaluator \(V(\cdot)\), global steps
    T, state threshold \(v_{t h}\), uncertainty evaluator \(U(\cdot)\), uncertainty threshold \(u_{t h}\).
    if \(t&gt;T\) then
        record output \(G\left(p_{\theta}, s, 1\right)\)
    end if
    for \(s^{\prime} \in G\left(p_{\theta}, s, k\right)\) do
        if \(V\left(p_{\theta},\left\{s^{\prime}\right\}\right)(s)&gt;v_{t h}\) and \(U\left(p_{\theta},\left\{s^{\prime}\right\}\right)(s)&lt;u_{t h}\)
    then
        DFS \(\left(s^{\prime}, t+1\right)\)
        end if
    end for
</code></pre></div>

<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Success Rate (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">IO prompt</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">CoT prompt [6]</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">CoT-SC [7]</td>
<td style="text-align: center;">7.2</td>
</tr>
<tr>
<td style="text-align: left;">ToT [8] $(b=1)$</td>
<td style="text-align: center;">37</td>
</tr>
<tr>
<td style="text-align: left;">TouT (ours, $b=1$ )</td>
<td style="text-align: center;">$\mathbf{4 2}$</td>
</tr>
<tr>
<td style="text-align: left;">ToT [8] $(b=5)$</td>
<td style="text-align: center;">56</td>
</tr>
<tr>
<td style="text-align: left;">TouT (ours, $b=5$ )</td>
<td style="text-align: center;">$\mathbf{6 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1. Quantitative results of Game of 24.</p>
<p>Crosswords [8] includes 156 games of $5 \times 5$ mini crosswords. For this task, the input is the 5 horizontal clues and 5 vertical clues, and the output should be a board of 25 letters to solve the problem. This task has 5-10 intermediate steps for solving, such as h1. shown and v5. naled.
Evaluation Metrics. For Game of 24, if the output is an equation that uses the input numbers each exactly once equals 24, it is regarded as a success, such as (13-9)*(10-4)=24. We compute the average success rate of total testing games, where the Breadth-first search algorithm is used for this task. For Mini Crosswords, we follow ToT [8], and adopt three levels of success: the accuracy of letters ( 25 per game), words (10 per game), and games.
Implementation. For LLMs, we use officially released LLaMA-2-70B [5] weights. Since GPT-4 is more expensive to use, we reproduce all baseline results using the same LLM weight for a fair comparison. The number of Monte Carlo sampling steps $m$ is 20 . Our experiments are conducted on NVIDIA-A100 GPUs.</p>
<h3>4.2. Comparison to prior work</h3>
<p>In this work, we propose a novel and effective framework for deliberate problem-solving with LLMs inference. In order to demonstrate the effectiveness of the proposed TouT, we compare it to previous non-tree-based prompting $[6,7]$ and tree-of-thoughts [8] methods.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Success Rate (\%)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Letter</td>
<td>Word</td>
<td>Game</td>
</tr>
<tr>
<td>IO prompt</td>
<td>29.5</td>
<td>10</td>
<td>0</td>
</tr>
<tr>
<td>CoT prompt [6]</td>
<td>33.2</td>
<td>10.8</td>
<td>0</td>
</tr>
<tr>
<td>ToT [8]</td>
<td>59</td>
<td>48</td>
<td>12</td>
</tr>
<tr>
<td>TouT (ours)</td>
<td>61</td>
<td>52</td>
<td>15</td>
</tr>
<tr>
<td>ToT [8] + best state</td>
<td>62.2</td>
<td>53.9</td>
<td>21</td>
</tr>
<tr>
<td>TouT (ours) + best state</td>
<td>64.5</td>
<td>58.2</td>
<td>29</td>
</tr>
</tbody>
</table>
<p>Table 2. Quantitative results of Mini Crosswords.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LUQ</th>
<th style="text-align: center;">UGS</th>
<th style="text-align: center;">Game of 24</th>
<th style="text-align: center;">Mini Crosswords</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Letter</td>
<td style="text-align: center;">Word</td>
<td style="text-align: center;">Game</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">23</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">29</td>
</tr>
</tbody>
</table>
<p>Table 3. Ablation study on Local Uncertainty Quantification (LUQ) and Uncertainty-aware Global Search (UGS).</p>
<p>For the Game of 24 task, we report the quantitative comparison results in Table 1. As can be seen, we achieve the best results regarding both $b=1$ and $b=5$ for solving the Game of 24 problem. In particular, the proposed TouT superiorly outperforms ToT [8], the current state-of-the-art LLM inference baseline, by $5 \%$ and $9 \%$. Furthermore, we achieve significant performance gains compared to previous non-treebased prompting approaches [6, 7]. These significant improvements demonstrate the superiority of our approach in deliberate problem-solving with LLMs inference.</p>
<p>In addition, In addition, significant gains in the Mini Crosswords task can be observed in Table 2. Compared to ToT [8], we achieve the results gains of $2 \%, 4 \%$, and $3 \%$ on letter, word and game. We also achieve highly better results against IO and CoT prompting baselines. These results demonstrate the effectiveness of our approach in using LLMs inference for solving problems.</p>
<h3>4.3. Experimental analysis</h3>
<p>In this section, we performed ablation studies to demonstrate the benefit of introducing the Local Uncertainty Quantification and Uncertainty-aware Global Search modules. We also conducted extensive experiments to explore the impact of Monte Carlo sampling steps in uncertainty quantification.
Local Uncertainty Quantification \&amp; Uncertainty-aware Global Search. In order to demonstrate the effectiveness of the introduced Local Uncertainty Quantification (LUQ) and Uncertainty-aware Global Search (UGS), we ablate the necessity of each module and report the quantitative results in Table 3. As can be observed, adding LUQ to the vanilla baseline highly increases the results of $4 \%, 2 \%, 3.6 \%$, and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Steps</th>
<th style="text-align: center;">Game of 24</th>
<th style="text-align: center;">Mini Crosswords</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Letter</td>
<td style="text-align: center;">Word</td>
<td style="text-align: center;">Game</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">29</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">29</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">28</td>
</tr>
</tbody>
</table>
<p>Table 4. Exploration studies on the Monte Carlo sampling steps in Local Uncertainty Quantification.
$6 \%$, which validates the benefit of LUQ in quantifying the uncertainty for each local response in intermediate steps. Meanwhile, introducing only UGS in the baseline increases the performance regarding all metrics. More importantly, incorporating LUQ and UGS into the baseline significantly raises the performance, and achieves the best. These improving results validate the importance of local uncertainty quantification and uncertainty-aware global search in deliberate problem-solving with LLMs inference.
Impact of Monte Carlo sampling steps. The number of Monte Carlo sampling steps used in the proposed LUQ affects the selected state in global searching for the final answer. To explore such effects more comprehensively, we varied the number of sampling steps from ${5,10,20,50,100}$. We report the comparison results of Game of 24 and Mini Crosswords in Table 4. When the number of Monte Carlo sampling steps is 20 , we achieve the best performance regarding all metrics. With increased depth from 5 to 20, the proposed TouT consistently increases performance as best states are extracted from more quantified local responses. Nevertheless, increasing the steps from 20 to 100 will not continually improve the results since 20 steps might be enough to extract the best state from these quantified states for addressing our deliberate problems with at most 10 intermediate thoughts.</p>
<h2>5. CONCLUSION</h2>
<p>In this work, we present TouT, a novel framework with the Tree of Uncertain Thoughts for large-scale language model inference. We leverage Monte Carlo Dropout for local uncertainty quantification on diverse responses at intermediate steps. Furthermore, we integrate local uncertainty into global sorting to amplify model response confidence. Experimental results on Game of 24 and Mini Crosswords comprehensively demonstrate the state-of-the-art superiority against previous ToT and CoT prompting methods. Extensive ablation studies also validate the importance of local uncertainty quantification and Local Uncertainty Sorting in generating more accurate answers for LLMs inference to solve deliberate problems.</p>
<h2>6. REFERENCES</h2>
<p>[1] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever, "Improving language understanding by generative pre-training," OpenAI blog, 2018.
[2] Alec Radford, Rewon Child Jeffrey Wu, David Luan, Dario Amodei, and Ilya Sutskever, "Language models are unsupervised multitask learners," OpenAI blog, 2019.
[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei, "Language models are few-shot learners," NeurIPS, vol. 33, pp. 1877-1901, 2020.
[4] OpenAI, "Gpt-4 technical report," arXiv preprint arXiv:2303.08774, 2023.
[5] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom, "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.
[6] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou, "Chain of thought prompting elicits reasoning in large language models," arXiv preprint arXiv:2201.11903, 2022.
[7] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou, "Self-consistency improves chain of thought reasoning in language models," arXiv preprint arXiv:2203.11171, 2022.
[8] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan, "Tree of Thoughts: Deliberate problem solving with large language models," arXiv preprint arXiv:2305.10601, 2023.
[9] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang, "Prompting gpt-3 to be reliable," in $I C L R$, 2022.
[10] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar, "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation," $I C L R$, vol. abs/2302.09664, 2023.
[11] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun, "Generating with confidence: Uncertainty quantification for black-box large language models," ArXiv, vol. abs/2305.19187, 2023.
[12] Stephanie C. Lin, Jacob Hilton, and Owain Evans, "Teaching models to express their uncertainty in words," $T M L R$, vol. 2022, 2022.
[13] Yarin Gal and Zoubin Ghahramani, "Dropout as a bayesian approximation: Representing model uncertainty in deep learning," in ICML, 2015.
[14] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer, "Opt: Open pretrained transformer language models," arXiv preprint arXiv:2205.01068, 2022.
[15] Andrew Foong, David Burt, Yingzhen Li, and Richard Turner, "On the expressiveness of approximate inference in bayesian neural networks," NeurIPS, vol. 33, pp. 15897-15908, 2020.
[16] Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and Louis-Philippe Morency, "Uncertainty quantification with pre-trained language models: A large-scale empirical analysis," in EMNLP, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Corresponding author.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8527 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8527</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8527</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-446c3356b95a29dca4b908533c088ce63f2275cb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/446c3356b95a29dca4b908533c088ce63f2275cb" target="_blank">Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus after training on just 5K logic problems, and demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.</p>
                <p><strong>Paper Abstract:</strong> Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8527",
    "paper_id": "paper-446c3356b95a29dca4b908533c088ce63f2275cb",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004085,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</h1>
<p>Tian Xie ${ }^{1 <em>}$ Zitian Gao ${ }^{2}$ Qingnan Ren ${ }^{3}$ Haoming Luo ${ }^{3}$ Yuqian Hong ${ }^{1 </em>}$ Bryan Dai ${ }^{2}$ Joey Zhou ${ }^{2}$ Kai Qiu ${ }^{1}$ Zhirong Wu ${ }^{1}$ Chong Luo ${ }^{1 *}$<br>${ }^{1}$ Microsoft Research Asia ${ }^{2}$ Ubiquant ${ }^{3}$ Independent<br>{unakar666, hmluo65536, hoknight0}@gmail.com<br>{v-yuqianhong, kai.qiu, wu.zhirong, chong.luo}@microsoft.com<br>{ztgao02, cbdai, jzhou}@ubiquant.com</p>
<h4>Abstract</h4>
<p>Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skillsâ€”such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5 K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Validation accuracy and mean response length during RL training, illustrating how the model autonomously learns to allocate more thinking compute for improved performance. Remarkably, the model also demonstrates impressive generalization on completely unseen datasets (AIME, AMC).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>The post-training phase of large language models (LLMs) has advanced rapidly [19], with models like DeepSeek-R1 [3], Kimi-K1.5 [15], and OpenAI-o1 [10] demonstrating remarkable reasoning abilities. DeepSeek-R1, in particular, introduced a simple yet effective rule-based reinforcement learning (RL) approach, enabling emergent reasoning patterns without relying on traditional scaffolding techniques such as Monte Carlo Tree Search (MCTS) [4,6,18,20] or Process Reward Models (PRM) [8].
Despite these advancements, substantial gaps remain in translating these developments into reproducible research. While DeepSeek-R1 provides open-source model weights, it does not release the corresponding training code or dataset. This absence raises critical questions: (1) Can similar reasoning abilities emerge in smaller-scale models? (2) What is the optimal training data structure for fostering such capabilities? (3) What methodologies can reliably replicate these results?
Addressing these questions requires controlled experimental frameworks that isolate key variables. While mathematics is often treated as the common testbed for reasoning, widely-used math datasets like GSM8K [2] and Omini-MATH [5] suffer as the training data due to its uncontrolled variance in problem complexity, which may span various logical induction depths. To overcome this limitation, we leverage a procedurally generated Knights and Knaves (K\&amp;K) logic puzzle dataset [17], which allows controllable difficulty levels and ease of rule-based reward verification, making it ideal for studying reasoning dynamics.
In this paper, we introduce Logic-RL, a rule-based reinforcement learning framework that acquires R1-like reasoning patterns through training on logic puzzles. The training framework adopts the REINFORCE++ algorithm [7] and the reward designs from DeepSeek-R1 for post-training. While naive training would lead to collapsed solutions, we propose a practical system prompt and a stringent format reward to avoid the reasoning model for taking shortcuts. We also incorporate a few modifications to the REINFORCE++ algorithm for improved performance.
As the RL training undergoes, we observe that the model naturally allocates more training steps to reason. This computational expansion scales from generating hundreds to thousands of tokens, enabling deeper exploration and refinement of its thought processes. We evaluate the model performance on the challenging math benchmarks for reasoning. With merely 5,000 procedurally generated logic puzzles, our 7B model improves by $125 \%$ on AIME and $38 \%$ on AMC against the base model. This cross-domain generalization capability suggests that RL-trained reasoning heuristics develop abstract problem-solving schemata rather than relying on domain-specific pattern matching.
Besides the technical contributions mentioned above, our study also makes several interesting findings:</p>
<ul>
<li>Longer responses don't guarantee better reasoning. Length alone is not a valid performance metric for training time evaluation. The most efficient reasoning comes from the shortest path.</li>
<li>Language mixing hinders reasoning. This observation underscores the need for a language consistency penalty in reward modeling.</li>
<li>Increasing 'thinking' tokens do help. RL training naturally boosts the frequency of reflectionrelated words, suggesting a correlation between certain tokens' frequency and performance.</li>
<li>SFT memorizes; RL generalizes. SFT relies heavily on memorization, often leading to superficial shortcut learning, whereas RL self-evolves with minimal dependence on dataset structure.</li>
<li>Cold start is a bonus, not a necessity. Training dynamics remain surprisingly similar whether starting from a base or instruct model, though the latter exhibits slightly better performance.</li>
<li>Curriculum Learning still matters. Under a fixed data curation ratio, a well-designed curriculum learning approach always outperforms random shuffle.</li>
</ul>
<h2>2 Method</h2>
<h3>2.1 Data Synthesis</h3>
<p>The Knights and Knaves (K\&amp;K) puzzles [17] constitute an algorithmically generated reasoning dataset. In these puzzles, characters are either knights, who always tell the truth, or knaves, who</p>
<p>always lie. The objective is to determine the nature of each character based on their statements. This dataset is distinguished by its high degree of controllability:</p>
<ol>
<li>Procedural Generation: Puzzles are systematically generated using logic templates, ensuring both consistency and infinite variability. Importantly, these puzzles represent unseen data for the original model, making them ideal for testing generalization capabilities.</li>
<li>Controlled Difficulty Levels: The difficulty of the puzzles can be precisely adjusted, enabling the design of a curriculum learning strategy. Difficulty is modulated by varying the number of characters (2-8) and the complexity of logical operations (1-4 combinations of Boolean operators). Furthermore, more complex puzzles can serve as out-of-distribution tests for models trained on simpler cases, providing insights into their ability to generalize.</li>
<li>Ease of Verification: Each puzzle has a single, unambiguous ground truth answer, with correctness guaranteed by the generation algorithm. Solutions require strict deductive reasoning, allowing for accurate evaluation of model responses and minimizing the risk of reward hacking.</li>
</ol>
<h1>An example of a K\&amp;K puzzle</h1>
<p>Problem: A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. You meet 2 inhabitants: Zoey, and Oliver. Zoey remarked, "Oliver is not a knight". Oliver stated, "Oliver is a knight if and only if Zoey is a knave". So who is a knight and who is a knave?</p>
<p>Solution: (1) Zoey is a knave (2) Oliver is a knight</p>
<p>The K\&amp;K puzzle 2.1 is exceptionally well-suited for further analysis due to its synthetic design and logical precision. Each puzzle is constructed using formal rules, ensuring that every problem has a unique solution that can be deterministically verified. This eliminates the ambiguities commonly encountered in natural language tasks, enabling us to clearly distinguish between genuine reasoning capabilities and superficial memorization.</p>
<h3>2.2 Rule Based Reward Modeling</h3>
<p>The reward serves as the primary training signal in reinforcement learning (RL), guiding the optimization process. We continuously monitored hacking behaviors in the model's outputs, refining our reward design iteratively. This led to a nearly unhackable, rule-based reward system that comprises only two types of rewards: Format Reward and Answer Reward.</p>
<h2>System Prompt</h2>
<p>You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to solve a logical reasoning problem. After thinking, when you finally reach a conclusion, clearly state the identity of each character within <answer> </answer> tags. i.e., <answer> (1) Zoey is a knight, (2) ... </answer>.</p>
<p>Format Reward: We use regular expression extraction to enforce a structured response format. The model is required to put its reasoning process within <think></think> tags and provide the final conclusion inside <answer></answer> tags. Additionally, we recommend including a <think> tag directly at the end of the prompt, which significantly reduces the difficulty for the base model to follow our instructions.</p>
<p>Under our early imperfect rule design, we consistently observed reward hacking phenomena, some of which are listed below:</p>
<ul>
<li>Skipping the <think></think> process and directly answering.</li>
<li>Placing reasoning inside the <answer></answer> tag.</li>
<li>Repeatedly guessing answers without proper reasoning.</li>
<li>Including irrelevant nonsense in addition to providing the answer.</li>
<li>Organizing correct answer in a wrong manner for extraction.</li>
<li>Revisiting the thinking phase after already outputting an <answer> due to insufficient reasoning.</li>
<li>Repeating the original question or using phrases like "thinking process here" to avoid true reasoning.</li>
</ul>
<p>Accordingly, we iteratively refine our rule design. For example, each tag should appear exactly once and in the correct sequential order, the thinking process must include genuine reasoning, and the conclusion should be presented in an extractable and readable manner. By enforcing these constraints, we ensure that different actions receive appropriate rewards based on their adherence to the format. The format score ( $S_{\text {format }}$ ) is computed as follows:</p>
<p>$$
S_{\text {format }}=\left{\begin{array}{ll}
1, &amp; \text { if format is correct } \
-1, &amp; \text { if format is incorrect }
\end{array}\right.
$$</p>
<p>Answer Reward: The second component evaluates the correctness of the content in the model's response. Once the format is validated, we check if the model's answer matches the ground truth. The answer score ( $S_{\text {answer }}$ ) is computed as:</p>
<p>$$
S_{\text {answer }}=\left{\begin{array}{ll}
2, &amp; \text { if the answer fully matches the ground truth } \
-1.5, &amp; \text { if the answer partially mismatches the ground truth } \
-2, &amp; \text { if the answer cannot be parsed or is missing }
\end{array}\right.
$$</p>
<h1>2.3 RL Algorithm</h1>
<p>We adopt a modified version of REINFORCE++ as our baseline algorithm, which has demonstrated superior performance compared to GRPO in our experimental setup. A detailed comparison of these algorithms is provided in Section 4.
Reinforce Return Calculation: The discounted cumulative rewards for each trajectory are computed as below, where $\gamma$ is the discount factor, set to 1 in our experiments:</p>
<p>$$
G_{t}=\sum_{k=t+1}^{T} \gamma^{k-t} r_{k}
$$</p>
<p>Following recommendations from DeepSeek-Math [13], we incorporate several minor refinements into the implementation of REINFORCE++.</p>
<p>First modification: Use KL Loss The KL-divergence between the response distributions of the RL model and the SFT model is calculated for each token. This divergence is incorporated as a penalty term in the reward function of PPO during training. The per-token reward is defined as follows:</p>
<p>$$
r\left(s_{t}, a_{t}\right)=\mathbf{I}\left(s_{t}=[\mathrm{EOS}]\right) r(x, y)-\beta \mathrm{KL}(t)
$$</p>
<p>where $\mathbf{I}\left(s_{t}=[\mathrm{EOS}]\right)$ is an identity function that evaluates to 1 when the <eos> token is reached, and $\beta$ controls the weight of the KL penalty.
In contrast, the GRPO implementation does not include the KL-divergence as part of the reward function. Instead, it directly incorporates the KL-divergence into the loss function, arguing that this</p>
<p>approach simplifies the computation and avoids unnecessary complexity. Following this rationale, we also use KL loss like GRPO:</p>
<p>$$
\begin{aligned}
\mathcal{J}<em P_Q_left_123_o__i="P(Q),\left{o_{i" _q="[q" _sim="\sim">{\mathrm{GRPO}}(\theta) &amp; =\mathbb{E}</em>\right}<em _theta__mathrm_std="\theta_{\mathrm{std">{i=1}^{G} \sim \pi</em> \
&amp; \frac{1}{G} \sum_{i=1}^{G} \frac{1}{\left|o_{i}\right|} \sum_{t=1}^{\left|o_{i}\right|} \min \left[\frac{\pi_{\theta}^{i, t}}{\pi_{\theta_{\mathrm{std}}}^{i, t}} \hat{A}}}}(O \mid q)]<em _theta="\theta">{i, t}, \operatorname{clip}\left(\frac{\pi</em>}^{i, t}}{\pi_{\theta_{\mathrm{std}}}^{i, t}}, 1-\epsilon, 1+\epsilon\right) \hat{A<em _mathrm_KL="\mathrm{KL">{i, t}\right]-\beta \mathbb{D}</em>\right]
\end{aligned}
$$}}\left[\pi_{\theta} | \pi_{\mathrm{ref}</p>
<p>Second Modification: KL Estimation Another key distinction lies in how the KL-divergence is estimated. The default KL estimator for PPO is defined as follows:</p>
<p>$$
\mathrm{KL}(t)=\log \left(\frac{\pi_{\theta_{\mathrm{std}}}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}\right)
$$</p>
<p>In contrast, GRPO employs an unbiased estimator for the KL-divergence, formulated as:</p>
<p>$$
\mathbb{D}<em _theta="\theta">{\mathrm{KL}}\left[\pi</em>-1
$$} | \pi_{\mathrm{ref}}\right]=\frac{\pi_{\mathrm{ref}}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_{\theta}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{\mathrm{ref}}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_{\theta}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)</p>
<p>This approach ensures that the KL estimate is always non-negative, whereas the original formulation may yield negative values. GRPO's estimator provides a more stable and reliable measure of divergence during training.
After implementing these modifications, we proceed with our experiments.</p>
<h1>2.4 Training Schedule</h1>
<p>We directly train the model for 3600 steps with a constant learning rate of $4 \times 10^{-7}$ and temperature parameter of 0.7 . During training, the model is directly exposed to mixed complexity logic puzzles ranging from 3 to 7 people. This straightforward training regimen achieves competitive performance as shown in the final results table. Through continuous training with these fixed hyperparameters, the model develops stable reasoning patterns characterized by logical exploration, intermediate verification, and systematic summarization before producing final answers. These emergent behaviors demonstrate the model's capacity to handle complex logical reasoning tasks effectively.
Other key parameters used during training are summarized in Table 1</p>
<p>Table 1: Important Training Parameters</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Algorithm</th>
<th style="text-align: center;">Train Batch Size</th>
<th style="text-align: center;">Rollout N</th>
<th style="text-align: center;">KL Coef</th>
<th style="text-align: center;">Max Response Len</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">REINFORCE++</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">4096</td>
</tr>
</tbody>
</table>
<h2>3 Experiment</h2>
<p>We began by experimenting with various models from the Qwen2.5 series as potential baseline candidates. For instance, Qwen2.5-Math-7B exhibited a strong tendency to generate Python code blocks, which often conflicted with our strict formatting requirements. Despite efforts to mitigate this behavior by removing system prompts and penalizing specific markdown styles, it remained challenging to fully address.
Additionally, we tested both Qwen2.5-7B-Base and Qwen2.5-7B-Instruct as starting points. Surprisingly, we found that the base and instruct models displayed nearly identical training metrics during RL training, including validation accuracy, response length growth curves, and reward curves. A detailed comparison between base \&amp; instruct model can be found in the appendix 7. However, the instruct model demonstrated slightly higher test accuracy, making it the preferred</p>
<p>choice. Consequently, we selected Qwen2.5-7B-Instruct-1M [22] as our baseline. See more base \&amp; instruct model training dynamics comparision in Appendix 10
Notably, despite the training dataset being limited to 3 to 7-person K\&amp;K logic puzzles-with fewer than 5,000 synthetic samples-the model demonstrates a remarkable ability to generalize to out-ofdistribution (OOD) scenarios, such as 8-person puzzles.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Difficulty by Number of People</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">o3-mini-high</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.94</td>
</tr>
<tr>
<td style="text-align: center;">o1-2024-12-17</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">Deepseek-R1</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o-mini</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: center;">NuminaMath-7B-CoT</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.08</td>
</tr>
<tr>
<td style="text-align: center;">Deepseek-Math-7B</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.10</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Base-7B</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-7B-Instruct-1M</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.19</td>
</tr>
<tr>
<td style="text-align: center;">+ Logic-RL</td>
<td style="text-align: center;">$0.99^{70.50}$</td>
<td style="text-align: center;">$0.99^{70.59}$</td>
<td style="text-align: center;">$0.94^{70.69}$</td>
<td style="text-align: center;">$0.92^{70.81}$</td>
<td style="text-align: center;">$0.91^{70.85}$</td>
<td style="text-align: center;">$0.80^{70.78}$</td>
<td style="text-align: center;">$0.67^{70.48}$</td>
<td style="text-align: center;">$0.89^{70.70}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of different models including reasoning models and general models on K\&amp;K logic puzzle across various difficulty.</p>
<p>Compared to the initial average length of 500 tokens, after 1 k steps of RL, the output has almost linearly and steadily increased to 2000 tokens, a significant increase of 4 times. As the response length increases, the model begins to exhibit more complex behaviors, such as reflection and exploration of alternative solutions. These behaviors emerge naturally, with no related data in our training set, and enhance the model's ability to handle more complex tasks. These phenomena align closely with the results of R1 [3]. We will further discuss this in Section 4.</p>
<h1>4 Research Questions</h1>
<h2>RQ 1: How Does GRPO Compare to Other RL Algorithms?</h2>
<p>Does GRPO [13] outperform other reinforcement learning algorithms, such as REINFORCE++ and PPO, in terms of training stability, speed, and performance accuracy?
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of GRPO (Blue), REINFORCE++ (Red), and PPO (Green) performance (averaged by sliding window $=50$ ) in terms of training speed, accuracy, and reward gain.</p>
<p>The results indicate that PPO [12] achieved significant advantages in both accuracy and reward. However, it was $138 \%$ slower than REINFORCE++ in terms of training speed. On the other hand, REINFORCE++ [7] demonstrated superior stability, performance gains, and training efficiency compared to GRPO. Overall, REINFORCE++ outperformed GRPO across nearly all metrics, with</p>
<p>GRPO exhibiting the weakest performance among the three reinforcement learning algorithms evaluated in our experiments.</p>
<h1>RQ 2. Do certain thinking tokens and language-mixing phenemona improve reasoning?</h1>
<p>Does the inclusion of complex reasoning behaviours (such as exploration, verification, summarization, and backtracking) and language switching improve the model's reasoning ability?
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Impact of complex reasoning behaviours and language mixing on reasoning performance. We analyzed the model's answer rewards for responses containing the tokens shown in the figure. Responses with "verify" and "re-evaluate" scored significantly higher than those without these words. Conversely, responses containing certain tokens from other languages generally received lower scores.</p>
<p>We observe that in our experiments:</p>
<ol>
<li>Language mixing significantly decreases reasoning ability.</li>
<li>While terms like "wait," "verify," "yet," and "re-evaluate" show significant improvement, not all complex thinking tokens enhance reasoning ability, as exemplified by "recheck."</li>
<li>The complex reasoning behaviour "recheck" markedly diminishes reasoning ability, likely because its use signals the model's uncertainty about its answer.</li>
<li>There's a clear difference between "re-evaluate" and "reevaluate": the former leads to much higher answer scores, while the latter lowers them. When we checked its origin responses, "reevaluate" almost never appeared, while "re-evaluate" showed up frequently. This may suggest the model is more comfortable with words it has seen more often in pretrain corpus.</li>
</ol>
<h2>RQ 3: Does an 'Aha Moment' Emerge During Training?</h2>
<p>Is there an observable 'Aha moment' where the model exhibits a significant leap in reasoning capability, such as the emergence of multi-step verification or reflection during the RL process?
The emergence of sophisticated behaviors becomes increasingly evident as performance grows. These behaviors include reflective actions-where the model revisits and reevaluates prior steps-and the spontaneous exploration of alternative problem-solving strategies. Such behaviors are not explicitly planted into training corpus but emerge organically through the model's interaction with the reinforcement learning environment, consistent with findings by Ye et al. [23].
The "aha moment" referenced in the R1 report [3] primarily refers to the model's sudden acquisition of complex reasoning behaviors. A secondary interpretation involves the model spontaneously</p>
<p>verbalizing "aha moment," such as in phrases like "Wait, wait. Wait. That's an aha moment I can flag here." While our model did not exhibit this specific verbalization, Figure 4 shows that it displayed some complex reasoning behaviors (e.g., self-reflection, exploration, verification, summarization) even by step 10 .
Thus, we conclude that the RL process likely lacks a sudden "aha moment"-that is, complex reasoning behaviors do not abruptly emerge at a specific training step, aligned with Liu et al. [9].
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Tracking the frequency of words in the first 1,800 training steps. 1. Reflective words like "check" and "verify" slowly increased (a)-(c). 2. Conversational phrases (e.g., "Let's") and cautious terms (e.g., "yet") became more frequent (d)-(e), 3. Chinese words began appearing in English responses (f). The frequency of all these words developed steadily without sudden jumps, suggesting that there may not be a distinct "aha moment."</p>
<h1>RQ 4: Can the Model Generalize to Out-of-Distribution (OOD) Tasks?</h1>
<p>To what extent can the trained model handle tasks that differ from its training data, particularly those that are out-of-distribution?
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training Step vs. Accuracy on AIME (2021-2024) and AMC (2022-2023) Datasets.</p>
<p>The ability of a model to generalize beyond its training distribution is a cornerstone of AI research. We investigate whether the complex reasoning abilities-such as exploration, verification, summarization, and backtracking-that naturally emerged during RL process can transfer to a highly challenging mathematical reasoning scenario, which we term Super OOD .</p>
<p>This evaluation leverages the widely-adopted AIME 2021-2024 (American Invitational Mathematics Examination) and AMC 2022-2023 (American Mathematics Competitions) benchmarks, both known for their rigorous and diverse problem sets.</p>
<p>From Figure 5, we observe that the model's Super OOD generalization capability is exceptionally strong, achieving an overall improvement of $125 \%$ on the AIME dataset and $38 \%$ on the AMC dataset. This synchronous improvement indicates that the reinforcement learning process not only enhances the model's performance on in-distribution tasks but also facilitates the emergence of robust and transferable reasoning strategies.</p>
<p>These findings show that the reasoning skills learned during RL training go beyond the specific patterns of the K\&amp;K dataset. This highlights RL's potential to generalize beyond their training environment.</p>
<h1>RQ 5: Which Generalizes Better, SFT or RL?</h1>
<p>Can post-training methods achieve more than just superficial alignment, which just learns format patterns? Can SFT or RL actually learn to learn, effectively generalizing to other domains?
We investigate whether models merely memorize training data or truly learn reasoning skills. Following the setup in [17], we test this by comparing performance on familiar problems versus slightly altered ones.Two signs of memorization: High accuracy on seen problems, and low accuracy on slightly perturbed versions. So, we measure these using two metrics, denote model as $f$, dataset as $\mathcal{D}$.</p>
<ol>
<li>Accuracy on Observed Problems: $\operatorname{Acc}(f ; \mathcal{D})$, the model's accuracy on the training set $(\operatorname{Tr})$.</li>
<li>Consistency Ratio: $\operatorname{CR}(f ; \mathcal{D})$, the ratio of correct solutions after small changes (perturbations) to those solved without changes. Perturbations preserve the puzzle's core principle and difficulty.</li>
</ol>
<p>The Local Inconsistency-based Memorization Score is defined as:</p>
<p>$$
\operatorname{LiMem}(f ; \mathcal{D})=\operatorname{Acc}(f ; \mathcal{D}) \cdot(1-\operatorname{CR}(f ; \mathcal{D}))
$$</p>
<p>This score captures both memorization and sensitivity to changes. If a model's performance drops significantly when the problem format is altered, it likely hasn't learned the true reasoning skills required to solve similar but modified puzzles. We study two types of perturbations: (i) changing one person's statement to another bool logic expression, and (ii) reordering the statements between different people. Examples of perturbations are as follows:</p>
<h2>Perturbation Examples</h2>
<h2>Original Problem:</h2>
<p>Zoey remarked, "Oliver is not a knight". Oliver stated, "Oliver is a knight if and only if Zoey is a knave". So who is a knight and who is a knave?</p>
<h2>Statement Perturbation:</h2>
<p>Zoey remarked, "Oliver is a knight or Zoey is a knight". Oliver stated, "Oliver is a knight if and only if Zoey is a knave"</p>
<h2>Reorder Perturbation:</h2>
<p>Oliver stated, "Oliver is a knight if and only if Zoey is a knave". Zoey remarked, "Oliver is not a knight"</p>
<p>While it is challenging to collect a suitable SFT dataset, we employ reject sampling to gather ground truth data, referred to as the RFT method.</p>
<p>To explore which training paradigm offers better generalization performance, we apply two types of disturbances on training dataset, then compare RFT and RL with Test acc - Mem Score curve. For RFT settings, we use Reject Sampling on origin model, then use a rule-based Best-of-N method to collect the correct yet the shortest response for further fine-tune. As a result, we obtained the curve shown in Fig. 6, which illustrates the changes in unseen test accuracy and training dataset memorization over training process.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: RFT memorizes while RL generalizes. RFT (Reject sampling Fine-Tuning) slightly improves test accuracy at the expense of rapidly increasing $\operatorname{LiMem}(f ; T r)$, indicating it mainly learns superficial answer format than geniue reasoning. In contrast, RL achieves higher test accuracy with minimal or even negative increase in $\operatorname{LiMem}(f ; T r)$. Within the same LiMem interval, RL outperform RFT in test acc greatly, suggesting better generalization ability.</p>
<p>A higher memorization score indicates that more questions, which were originally correct, have turned incorrect due to the disturbances caused by the training, reflecting a greater degree of memorization. SFT tends to superficial alignment, often becoming overly reliant on the original data's expression format. On the other hand, RL encourages the model to explore independently, fostering generalization abilities that stem from enhanced reasoning capabilities, consistent with findings in [1].</p>
<h1>RQ 6: Is Curriculum Learning Still Necessary in RL?</h1>
<p>Does curriculum learning still matter in the RL paradigm? Specifically, is the sequential order of data important, or is it merely the curation ratio that matters?
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison of test scores for curriculum learning and mixed-difficulty training. The plot uses a rolling average (window size $=5$ )</p>
<p>To evaluate the necessity of curriculum learning, we compare its effectiveness to a mixed-difficulty approach. In curriculum learning, the model is trained sequentially on progressively more difficult datasets (3-7 people scenarios) for one epoch each. In contrast, the mixed-difficulty approach trains the model on all difficulty levels simultaneously within a single epoch. All other hyperparameters are kept constant between the two methods.</p>
<p>We analyze the test score trajectories using a rolling average (window size $=5$ ) to mitigate stochastic fluctuations and highlight underlying trends. The results in Figure 7 indicate that curriculum learning yields slightly higher test scores during intermediate training phases. However, this advantage diminishes in practical significance, as the performance difference during early training stages remains statistically negligible, suggesting a limited impact on initial convergence. While curriculum learning may offer a marginal theoretical benefit in terms of sample efficiency, its practical necessity is not conclusively supported, given the minimal real-world performance difference and the added complexity of staged training.</p>
<h1>RQ 7: Does Longer Response Length Guarantee Better Reasoning?</h1>
<p>Does an increase in response length during training directly improve a model's reasoning performance, or are these trends merely correlated?
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Comparison of response length, validation accuracy, and mean reward across training steps for positive and negative example models.</p>
<p>This experiment investigates whether an increase in response length during training causally enhances reasoning performance. We compared two models trained using the same algorithm and base model but with different hyperparameters and dataset difficulties.</p>
<ul>
<li>Positive Example Model (Blue): Despite a slight decrease in response length over time, this model demonstrated significant improvements in both validation accuracy and reward, indicating stronger reasoning and generalization abilities.</li>
<li>Negative Example Model (Red): This model consistently increased its response length but showed no improvement in validation accuracy or reward. This suggests that increasing response length alone does not necessarily enhance reasoning capabilities.</li>
</ul>
<p>Figure 8 illustrates these findings: the positive model's reward and accuracy improved while its response length decreased, whereas the negative model's length increased without any corresponding performance gains. These divergent trends suggest that changes in response length are likely a byproduct of training dynamics rather than a causal driver of reasoning improvements.
The observed increase in response length is likely a side effect of reinforcement learning (RL) dynamics. While some studies report a natural tendency for output length to grow as models generate more complex responses, this growth should be viewed as a correlate rather than a direct cause of improved reasoning. Importantly, there is no statistically significant evidence that the magnitude of length increase reliably predicts proportional gains in reasoning performance.
In conclusion, longer responses do not always guarantee better reasoning. Enhanced reasoning capabilities may naturally lead to more detailed explanations, but artificially extending response length does not necessarily result in proportional performance improvements.</p>
<h2>5 Discussion and Future Work</h2>
<p>While our study demonstrates the potential of Logic-RL in developing complex reasoning skills, it is important to note that our findings are based on a small-scale logic dataset. The generalizability</p>
<p>of our results to large-scale real-world mathematical or coding scenarios remains to be explored. Future work should focus on extending our approach to more diverse and complex datasets to further validate its effectiveness and robustness. Our work will remain an open research project to benefit the community.</p>
<p>Chain-of-Thought Long to Short Methods. Despite the fact that our initial prompt is concise and straightforward, the length of the responses can expand by up to four times after reinforcement learning training. In order to enhance token efficiency and accommodate a long-context friendly training paradigm, we find it particularly valuable to explore methods that transform long responses into shorter, more digestible formats. This investigation aims at improving overall efficiency and effectiveness in handling lengthy outputs, thereby optimizing the training process for better scalability and performance.</p>
<p>Stablize RL Training. We have found it beneficial in some cases to eliminate KL constraints, especially when starting from a strong foundation model. Additionally, introducing a higher temperature at the beginning of training appears to provide the model with a more diverse starting point. We plan to further investigate how the SFT stage impacts the effectiveness and efficiency of RL training.</p>
<p>Mixed-Language Reasoning. A curious phenomenon is the model's frequent use of Chinese tokens in the <think> section, despite training data being fully in English. One hypothesis is that certain tokens in the Chinese vocabulary vector might produce hidden states that are "favorable" under our RL scheme. Investigating whether code-switching or even random token switching could systematically aid internal reasoning is an exciting avenue.</p>
<p>Relaxing the Formatting Constraints. Although <think> ...</think> effectively organizes the chain of thought, it remains an open question whether an entirely unconstrained or latent approach might yield better results. The model might eventually "invent" its own internal representation for reasoning if given the right incentives.</p>
<h1>References</h1>
<p>[1] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation model post-training, 2025.
[2] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
[3] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu,</p>
<p>Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.
[4] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024.
[5] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: A universal olympiad level mathematic benchmark for large language models, 2024.
[6] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning, 2024.
[7] Jian Hu. Reinforce++: A simple and efficient approach for aligning large language models, 2025.
[8] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step, 2023.
[9] Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like training - a pilot study. https://oatllm.notion.site/ oat-zero, 2025. Notion Blog.
[10] OpenAI. Introducing openai o1. https://openai.com/o1/, 2024. Accessed: 2024-10-02.
[11] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 53728-53741. Curran Associates, Inc., 2023.
[12] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.
[13] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.
[14] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017.
[15] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He,</p>
<p>Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025.
[16] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc., 2022.
[17] Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. On memorization of large language models in logical reasoning, 2024.
[18] Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024.
[19] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: A survey of reinforced reasoning with large language models, 2025.
[20] Haotian Xu. No train still gain. unleash mathematical reasoning of large language models with monte carlo tree search guided by energy function, 2023.
[21] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, and Debing Zhang. Redstar: Does scaling long-cot data unlock better slow-reasoning systems?, 2025.
[22] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report, 2025.
[23] Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process, 2024.
[24] Chunhui Zhang, Yiren Jian, Zhongyu Ouyang, and Soroush Vosoughi. Working memory identifies reasoning limits in language models. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 16896-16922, Miami, Florida, USA, November 2024. Association for Computational Linguistics.</p>
<h1>Appendix</h1>
<h2>6 Related Work</h2>
<p>Large Language Model Reasoning A key focus for LLMs is improving their reasoning abilities, particularly for complex tasks like code generation and math problem-solving. Chain-of-Thought (CoT) reasoning [16] has been crucial in breaking down problems into manageable steps, enhancing logical reasoning. Originally successful in AlphaGo's victory [14], MCTS has been adapted to guide model-based planning by balancing exploration and exploitation through tree-based search and random sampling, and later to large language model reasoning $[6,18,20]$. However, recent research suggests that the vast token generation space of LLMs may make improving reasoning capabilities inefficient [3]. Additionally, long-path reasoning is likely influenced by the model's working memory [24].</p>
<p>Large Language Model Post-training for Reasoning Recent work has focused on post-training strategies to enhance the reasoning capabilities of large language models, often through additional fine-tuning or reinforcement learning on specialized datasets with reasoning examples and chain-of-thought explanations [21]. Reinforcement learning methods like Direct Preference Optimization (DPO) [11], Proximal Policy Optimization (PPO) [12], Group Relative Policy Optimization (GRPO) [13], and REINFORCE++ [7] are gaining attention. These strategies, alongside test-time scaling methods, are a promising frontier for advancing model reasoning.</p>
<h1>7 Comparion Between Base and Instruct Model</h1>
<p>We started with different model types (base, instruct) under the same training strategy and found that the RL curves of the two were surprisingly similar. The growth rates of the test set accuracy, curve, and reward curve were nearly the same. The slight difference is that the response length of the base model grows faster than that of the instruct model.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Comparison of training metrics: Response Length, Validation Accuracy, Mean Reward, and KL Loss.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Length dynamics between base \&amp; instruct</p>
<p>Figure 10 shows that the base model quickly increases thinking compute, while the instruct model consistently maintains slightly higher validation accuracy.</p>
<h1>8 Qualitative Analysis of Emergent Reasoning</h1>
<p>We highlight four key emergent behaviors observed in the RL-trained model:
(1) Hesitation and Self-Verification The model occasionally uses phrases like "I am not entirely sure; let's re-check this step" in the <think> section. This hesitation, absent in pre-training, emerges as the model is rewarded for correct answers and penalized for errors. Before providing the final <answer>, it systematically verifies all prior steps.
(2) Multi-Path Exploration \&amp; Backtracking Encouraging the model to reason thoroughly, we see it propose multiple solutions ("Let's test both possibilitie") and backtrack to check for consistency, resembling human problem-solving in logic puzzles.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span>
<span class="n">A</span><span class="w"> </span><span class="n">very</span><span class="w"> </span><span class="n">special</span><span class="w"> </span><span class="n">island</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">inhabited</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">knights</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">knaves</span><span class="o">.</span><span class="w"> </span><span class="n">Knights</span><span class="w"> </span><span class="n">always</span><span class="w"> </span><span class="n">tell</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">truth</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">knaves</span><span class="w"> </span><span class="n">always</span>
<span class="n">lie</span><span class="o">.</span><span class="w"> </span><span class="n">Five</span><span class="w"> </span><span class="n">residents</span><span class="w"> </span><span class="o">(</span><span class="n">William</span><span class="o">,</span><span class="w"> </span><span class="n">Chloe</span><span class="o">,</span><span class="w"> </span><span class="n">Lily</span><span class="o">,</span><span class="w"> </span><span class="n">Jack</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Logan</span><span class="o">)</span><span class="w"> </span><span class="n">made</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">statements</span><span class="o">:</span><span class="w"> </span><span class="o">(</span><span class="mi">1</span><span class="o">)</span><span class="w"> </span><span class="n">William</span><span class="w"> </span><span class="n">said</span><span class="o">:</span>
<span class="s2">&quot;William is a knight and Chloe is a knight.&quot;</span><span class="w"> </span><span class="o">(</span><span class="mi">2</span><span class="o">)</span><span class="w"> </span><span class="n">Chloe</span><span class="w"> </span><span class="n">said</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;Lily is a knight.&quot;</span><span class="w"> </span><span class="o">(</span><span class="mi">3</span><span class="o">)</span><span class="w"> </span><span class="n">Lily</span><span class="w"> </span><span class="n">said</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;William is a knave</span>
<span class="s2">and Jack is a knight.&quot;</span><span class="w"> </span><span class="o">(</span><span class="mi">4</span><span class="o">)</span><span class="w"> </span><span class="n">Jack</span><span class="w"> </span><span class="n">said</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;William is a knave.&quot;</span><span class="w"> </span><span class="o">(</span><span class="mi">5</span><span class="o">)</span><span class="w"> </span><span class="n">Logan</span><span class="w"> </span><span class="n">said</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;Lily is a knave if and only if William</span>
<span class="s2">is a knave.&quot;</span><span class="w"> </span><span class="n">So</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">knight</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">knave</span><span class="o">?</span>
<span class="n">Response</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">think</span><span class="o">&gt;</span>
<span class="n">To</span><span class="w"> </span><span class="n">solve</span><span class="w"> </span><span class="k">this</span><span class="o">,</span><span class="w"> </span><span class="n">let</span><span class="s1">&#39;s start analyzing the statements one by one, beginning with William&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">statement</span><span class="w"> </span><span class="n">since</span><span class="w"> </span><span class="n">it</span><span class="s1">&#39;s a</span>
<span class="s1">good starting point due to its self-referential nature.</span>
<span class="s1">William&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">statement</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;William is a knight and Chloe is a knight.&quot;</span>
<span class="o">-</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">William</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">telling</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">truth</span><span class="w"> </span><span class="o">(</span><span class="n">i</span><span class="o">.</span><span class="na">e</span><span class="o">.,</span><span class="w"> </span><span class="n">he</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">knight</span><span class="o">),</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">both</span><span class="w"> </span><span class="n">parts</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">his</span><span class="w"> </span><span class="n">statement</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="kc">true</span><span class="o">.</span><span class="w"> </span><span class="n">However</span><span class="o">,</span><span class="w"> </span><span class="k">if</span>
<span class="n">William</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">knight</span><span class="o">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">statement</span><span class="w"> </span><span class="s2">&quot;William is a knight&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="kc">true</span><span class="o">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">statement</span><span class="w"> </span><span class="s2">&quot;Chloe is a knight&quot;</span><span class="w"> </span><span class="n">cannot</span>
<span class="n">determine</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">truth</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">Chloe</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="k">this</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="n">since</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">haven</span><span class="s1">&#39;t analyzed Chloe&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">statement</span><span class="w"> </span><span class="n">yet</span><span class="o">.</span><span class="w"> </span><span class="o">(</span><span class="n">Hesitate</span><span class="o">)</span>
<span class="n">Therefore</span><span class="o">,</span><span class="w"> </span><span class="n">William</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">knight</span><span class="w"> </span><span class="n">because</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">lead</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">contradiction</span><span class="o">.</span>
<span class="w">    </span><span class="n">Let</span><span class="s1">&#39;s test both possibilities. (Explore)</span>
<span class="s1">Testing Chloe as a knight: If Chloe is a knight, then her statement &quot;Lily is a knight&quot; is true, so Lily is a knight.</span>
<span class="s1">Analyzing Lily&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">statement</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;William is a knave and Jack is a knight.&quot;</span>
<span class="w">    </span><span class="n">Now</span><span class="o">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">determined</span><span class="o">:</span><span class="w"> </span><span class="o">(</span><span class="n">Summerize</span><span class="o">)</span>
<span class="o">-</span><span class="w"> </span><span class="n">William</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">knave</span><span class="o">.</span>
<span class="o">-</span><span class="w"> </span><span class="n">Chloe</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">knight</span><span class="o">.</span>
<span class="w">    </span><span class="n">Let</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">verify</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">statements</span><span class="o">:</span><span class="w"> </span><span class="o">(</span><span class="n">Verify</span><span class="o">)</span>
<span class="o">-</span><span class="w"> </span><span class="s2">&quot;William is a knight and Chloe is a knight&quot;</span><span class="w"> </span><span class="o">(</span><span class="n">False</span><span class="o">,</span><span class="w"> </span><span class="n">since</span><span class="w"> </span><span class="n">William</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">knave</span><span class="o">).</span>
<span class="o">-</span><span class="w"> </span><span class="s2">&quot;Lily is a knight&quot;</span><span class="w"> </span><span class="o">(</span><span class="n">True</span><span class="o">).</span>
</code></pre></div>

<p>Table 3: An example of a large language model trained through reinforcement learning naturally exhibiting reasoning behaviors such as hesitation, exploration, summarization, and verification, even though these behaviors were not present in the training data.
(3) Formula Application After RL training, our model instinctively applied the "If P, then Q" implication formula when solving logical puzzles, like the Knights and Knaves problem. This formula asserts that the proposition is false only when P is true and Q is false. We were surprised to see that the model not only solved the puzzles through trial and error but also incorporated formal logical reasoning, resembling human problem-solving, despite no such data included in the training set.</p>
<h1>Question:</h1>
<p>A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. Three residents (Jack, Samuel, Lily) made the following statements: (1) Jack said, "If Samuel is a knight, then Lily is a knave." (2) Samuel said, "If Jack is a knight, then Lily is a knave." (3) Lily said, "Jack is a knight." So who is a knight and who is a knave?
Response: <think>
Let's analyze the statements step by step.</p>
<ol>
<li>Jack's statement is: "If Samuel is a knight then Lily is a knave."
...</li>
</ol>
<p>Now that we know Samuel is a knave, let's analyze Samuel's statement again:</p>
<ul>
<li>Samuel's statement: "If Jack is a knight then Lily is a knave."</li>
<li>An implication "If P then Q" is false only when P is true and Q is false. (Formula Application) Here, "If Jack is a knight then Lily is a knave" is false when Jack is a knight and Lily is not a knave. This is consistent with our previous conclusion that Jack is a knight and Lily is a knight.</li>
</ul>
<p>Table 4: An example of using logical formulas to solve a Knights and Knaves puzzle.
(4) Occasional Language Switching Interestingly, some <think> segments contain Chinese tokens (the base model is English-centric). The final <answer> remains in English, presumably to gain a format reward. In some demos, the model briefly interjects a line of Chinese while analyzing statements, then seamlessly switches back to English for the solution. This phenomenon may indicate that the model uses language mixing as a hidden fallback or distinct internal representation.</p>
<p>Question:A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. Six residents (Benjamin, Michael, Evelyn, Sophia, James, and Jacob) made the following statements: (1) Benjamin said, "If Sophia is a knight, then Evelyn is also a knight." (2) Michael said, "Benjamin is a knave, or James is a knight." (3) Evelyn said, "Sophia is a knight, or James is a knave." (4) Sophia said, "If Michael is a knave, then James is a knight." (5) James said, "Sophia is a knight, or Evelyn is a knight." (6) Jacob said, "Benjamin is a knave, and Sophia is a knight." So who is a knight and who is a knave?
Response: <think>
Let's analyze each statement step-by-step based on whether the speaker is a knight or a knave.</p>
<ol>
<li>
<p>Benjamin: "If Sophia is a knight then Evelyn is a knight"</p>
</li>
<li>
<p>If Benjamin is a knight, then the statement must be true. Thus, if Sophia is a knight, Evelyn has to be a knight.</p>
</li>
<li>
<p>Michael: "Benjamin is a knave or James is a knight"
From Jacob's statement, if Jacob is a knight then Sophia must be a knight and Benjamin must be a knave.ä»¥å¡æ–‡çš„æ–¹å¼æŽ¨ç†:</p>
</li>
<li>
<p>å‡è®¾Jacob æ˜¯éª‘å£«ï¼Œé‚£ä¹ˆJacob çš„å£°æ˜Ž"Benjamin æ˜¯èƒŒå›è€…ä¸”Sophia æ˜¯éª‘å£«"å°±æ˜¯çœŸçš„ã€‚è¿™æ„å‘³ç€Benjamin æ˜¯èƒŒå›è€…è€ŒSophia æ˜¯éª‘å£«ã€‚</p>
</li>
<li>æ—¢ç„¶Sophia æ˜¯éª‘å£«ï¼Œæ ¹æ®Benjamin çš„å£°æ˜Ž"å¦‚æžœSophia æ˜¯éª‘å£«é‚£ä¹ˆEvelyn ä¹Ÿæ˜¯éª‘å£«"ï¼ŒEvelyn ä¹Ÿæ˜¯éª‘å£«ã€‚
ãƒ».
ç»¼ä¸Šæ‰€è¿°ï¼ŒSophia, Evelyn, å’ŒJacob æ˜¯éª‘å£«ï¼Œè€ŒBenjamin, Michael, å’ŒJames æ˜¯èƒŒå›è€…ã€‚ (Language Switching) ãƒ»ãƒ»</li>
</ol>
<p>Table 5: An example of our model used language switching to solve a Knights and Knaves puzzle.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Work done during internship at MSRA. Open-Source Research Project.
Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9753 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9753</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9753</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-388c5301455ae14ff194e0ab801484a5e9c244b2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/388c5301455ae14ff194e0ab801484a5e9c244b2" target="_blank">Multi-Document Scientific Summarization from a Knowledge Graph-Centric View</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper presents KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process, and shows that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.</p>
                <p><strong>Paper Abstract:</strong> Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9753",
    "paper_id": "paper-388c5301455ae14ff194e0ab801484a5e9c244b2",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0038915,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multi-Document Scientific Summarization from a Knowledge Graph-Centric View</h1>
<p>Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang ${ }^{\dagger}$<br>College of Computer Science and Technology, National University of Defense Technology, Changsha, China<br>{wangpancheng13, shashali, pangkunyuan10, heliangliang19, lidong1, tangjintao, tingwang}@nudt.edu.cn</p>
<h4>Abstract</h4>
<p>Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum ${ }^{1}$, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.</p>
<h2>1 Introduction</h2>
<p>Nowadays, the exponential increasing publication rate of scientific papers makes it difficult and time-consuming for researchers to keep track of the latest advances. Multi-Document Scientific Summarization (MDSS) is therefore introduced to alleviate this information overload problem by generating succinct and comprehensive summary from clusters of topic-relevant scientific papers (Chen et al., 2021; Shah and Barzilay, 2021).</p>
<p>In MDSS, paper content modeling and crosspaper relationship modeling are two main issues. (1) Scientific papers contain complex concepts, technical terms, and abbreviations that convey important information about paper content. However,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>some previous works (Wang et al., 2018; Jiang et al., 2019) treat all text units equally, which inevitably ignore the salient information of some less frequent technical terms and abbreviations. (2) Furthermore, there exist intricate relationships between papers in MDSS, such as sequential, parallel, complementary and contradictory (Luu et al., 2021), which play a vital role in guiding the selection and organization of different contents. The latest work (Chen et al., 2021) attempt to capture cross-paper relationships via seq2seq model without considering any links between fine-grained text units. Failure to take into account explicit relationships between papers prevents their model from learning cross-paper relationships effectively.</p>
<p>To address the two aforementioned issues, we consider leveraging salient text units, namely entities, and their relations for MDSS. Scientific papers contain multiple domain-specific entities and relations between them. These entities and relations succinctly capture important information about the main content of papers. Knowledge graphs based on these scientific entities and relations can be inherently used for content modeling of scientific papers. Take Figure 1 as an example. The knowledge graph at the top left illustrates the main content of paper 1, which can be formulated as: Paper 1 uses memory augmented networks method to solve the life-long one-shot learning task, the evaluation is based on image classification benchmark datasets. Furthermore, knowledge graphs can effectively capture cross-paper relationships through entity interactions and information aggregation. In Figure 1, paper 1, 2 and 3 adopt the same method memory networks to solve different tasks. This relationship is demonstrated in the graph of gold summary by sharing the method node memory networks.</p>
<p>In this paper, we develop a Transformerbased (Vaswani et al., 2017) abstractive MDSS model, which can leverage knowledge graphs to</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Knowledge graphs constructed from abstract of input scientific papers and gold summary.
guide paper representation and summary generation. Specifically, in the encoding part, we fuse the knowledge graphs of multiple input papers into a unified graph and design a graph updater to capture cross-paper relationships and global information. Besides, we build another graph based on the interaction between entities and sentences, and then apply an entity-sentence updater to enable information flow between nodes and update sentence representations.</p>
<p>In the decoding part, knowledge graphs are utilized to guide the summary generation process via two approaches. The first is to incorporate the graph structure into the decoder by graph attention, and the second is inspired by deliberation mechanism (Xia et al., 2017; Li et al., 2019). Concretely, we introduce a two-stage decoder to make better use of the guidance information of knowledge graphs. The first-stage decoder concentrates on generating the knowledge graph of gold summary, while the second-stage decoder generates the summary based on the output of the first stage and the input papers. Since the knowledge graph of gold summary is in the form of graph structure, we translate the graph into equivalent descriptive sentences containing corresponding entities and relations, called KGtext. KGtext serves as an information-intact alternative to the knowledge graph of gold summary and is generated in the firststage decoder, which we call the KGtext generator.</p>
<p>We test the effectiveness of our proposed model on Multi-XScience (Lu et al., 2020), a large-
scale dataset for MDSS. Experimental results show that our proposed knowledge graph-centric model achieves considerable improvement compared with the baselines, indicating that knowledge graphs can exert a positive impact on paper representation and summary generation.</p>
<p>The main contribution is threefold: (i) We leverage knowledge graphs to model content of scientific papers and cross-paper relationships, and propose a novel knowledge graph-centric model for MDSS. (ii) We propose a two-stage decoder that introduces KGtext as intermediate output when decoding, which plays an important guiding role in the final summary generation. (iii) Automatic and human evaluation results on the Multi-Xscience dataset show the superiority of our model.</p>
<h2>2 Approach</h2>
<h3>2.1 Problem Formulation</h3>
<p>We first introduce the problem formulation and used notations for MDSS. Given a set of queryfocused scientific papers $\mathcal{D}=\left{d_{1}, d_{2}, \ldots, d_{N}\right}$, where $N$ denotes the number of input papers. Each paper $d_{i}$ consists of $M_{i}$ sentences $\left{s_{i, 1}, s_{i, 2}, \ldots s_{i, M_{i}}\right}$, while each sentence $s_{i, j}$ consists of $K_{i, j}$ words $\left{w_{i, j, 1}, w_{i, j, 2}, \ldots, w_{i, j, K_{i, j}}\right}$. The gold summary $S=\left{w_{1}, w_{2}, \ldots w_{N_{s}}\right}, N_{s}$ is the number of words in the gold summary. The target is to generate a summary $\hat{S}=\left{\hat{w}<em 2="2">{1}, \hat{w}</em>}, \ldots \hat{w<em s="s">{N</em>\right}$ that is close enough to the gold summary $S$.}</p>
<p>In our two-stage decoder framework, the gold KGtext $T=\left{w_{t_{1}}, w_{t_{2}}, \ldots, w_{t_{\hat{N}}}\right}$ is also attached</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The overall framework of our proposed model.</p>
<p>as input. Hence, the probability of generating the gold summary $S$ is</p>
<p>$$
P(S \mid \mathcal{D})=P_{\theta_{\mathcal{D} \rightarrow T}}(T \mid \mathcal{D}) * P_{\theta_{(\mathcal{D}, T) \rightarrow S}}(S \mid \mathcal{D}, T)
$$</p>
<p>where $\theta_{\mathcal{D} \rightarrow T}$ and $\theta_{(\mathcal{D}, T) \rightarrow S}$ are the parameters for the first-stage KGtext generator and the secondstage summary generator, respectively.</p>
<h3>2.2 Graph Construction</h3>
<p>To construct the knowledge graphs for input papers, we first employ the SciIE system DYGIE++ (Wadden et al., 2019), a well-performed science-domain information extraction system, to extract entities, relations and co-references from papers. Entities are classified into six types (Task, Method, Metric, Material, Generic, and OtherScientificTerm), and relations are classified into seven types (Compare, Used-for, Feature-of, Hyponym-of, Evaluate-for, Part-of, and Conjunction). Besides, we collapse coreferential entity clusters into a single node based on the annotation result.</p>
<p>After obtaining the knowledge graphs of multiple input papers, we fuse them into a unified graph. Then we follow the Levi transformation (Levi, 1942) to treat each entity and relation equally. Concretely, each labeled edge is represented as two vertices: one denoting the forward relation and another denoting the reverse relation. Formally, given an entity-relation tuple $\left(e_{1}, r, e_{2}\right)$, we create nodes $e_{1}, e_{2}, r_{1}$ and $r_{2}$, and add directed edges $e_{1} \rightarrow r_{1}, r_{1} \rightarrow e_{2}$ and $e_{2} \rightarrow r_{2}, r_{2} \rightarrow e_{1}$. In this way, the original knowledge graph is reconstructed
as an unlabeled directed graph without information loss. Besides, to guarantee the connectivity of Levi graph, we add a global vertex that connects all the entity vertices. We also add entity type nodes and connect all the entities to their corresponding types.</p>
<h3>2.3 Model Description</h3>
<p>Our model follows a Transformer-based encoderdecoder architecture, shown in Figure 2. The encoder includes a stack of $L_{1}$ token-level Transformer encoding layers to encode contextual information for tokens within each sentence and each entity. The Transformer encoding layer follows the Transformer architecture introduced in Vaswani et al. (2017). The encoder also includes a Graph Updater to learn the graph representation of the knowledge graph and an Entity-Sentence Updater to update entity representation and sentence representation based on their interaction. The decoder consists of a KGtext Generator, which produces the descriptive sentences of the graph of gold summary, and a Summary Generator, which produces the final summary.</p>
<h3>2.4 Graph Updater</h3>
<p>As shown in Figure 2, based on the output of tokenlevel Transformer encoding layers, the graph updater is used to encode the knowledge graphs to obtain graph representations of input papers.</p>
<p>Node Initialization The vertices of the constructed graph correspond to entities, relations and entity types from the SciIE annotations. Entities</p>
<p>representations are produced using the aforementioned Transformer-based encoding method. For a given entity co-reference cluster, we first remove pronouns and stopwords and then obtain the entity representation by using the average embedding of entities in the cluster. For relation representation, since each relation is represented as both forward and backward vertices, we learn two embeddings per relation. We also randomly initialize the types embeddings and the global vertex embedding.</p>
<p>Contextualized Node Encoding We follow Koncel-Kedziorski et al. (2019) and use a Graph Transformer to compute the hidden representations of each node in the graph. Graph Transformer encodes each vertex $v_{i}$ using the multi-head self-attention mechanism similar to Vaswani et al. (2017), where each vertex representation $\mathbf{v}<em i="i">{i}$ is contextualized by attending over the other vertices to which $v</em>$ is connected in the graph.</p>
<p>$$
\begin{aligned}
\hat{\mathbf{v}}<em i="i">{i} &amp; =\mathbf{v}</em>+|<em v__j="v_{j">{n=1}^{N} \sum</em>} \in \mathcal{N<em i_="i," j="j">{i}} \alpha</em>}^{n} \mathbf{W<em j="j">{V}^{n} \mathbf{v}</em> \
\alpha_{i, j}^{n} &amp; =\operatorname{softmax}\left(\left(\mathbf{W}<em j="j">{K}^{n} \mathbf{v}</em>}\right)^{T}\left(\mathbf{W<em i="i">{Q}^{n} \mathbf{v}</em>\right)\right)
\end{aligned}
$$</p>
<p>where $|<em i="i">{n=1}^{N}$ means the concatenation of $N$ heads. $\mathcal{N}</em>}$ denotes the neighbors of $v_{i}$, and $\mathbf{W<em K="K">{Q}^{n}, \mathbf{W}</em>$ are trainable parameters of query, key and value of head $n$, respectively.}^{n}$, and $\mathbf{W}_{V}^{n</p>
<h3>2.5 Entity-Sentence Updater</h3>
<p>After getting the contextualized node embeddings for the knowledge graph, we construct an entitysentence heterogeneous graph to update sentence representations based on the interaction between entities and sentences. The entity-sentence graph is denoted as $G={V, E}$, where $V$ stands for nodes set and $E$ stands for edges set. In the graph $G, V$ includes entity nodes $V_{e}$ and sentence nodes $V_{s}$, and $E$ is a real-value edge weight matrix, where $e_{i, j} \neq 0$ indicates the $j$-th sentence contains the $i$-th entity.</p>
<p>We apply the same Graph Transformer module as the graph updater. It takes as input the entities representations from the graph updater and the sentence representations from the Transformer encoding layer, then learns the representations of nodes based on the information flow through the graph $G$.</p>
<h3>2.6 KGtext Generator</h3>
<p>In the decoding stage, we also adopt the knowledge graph-centric view and introduce the KGtext gener-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example of graph of gold summary and the translated KGtext.
ator before the final summary generator. Here, KGtext is defined as descriptive sentences containing entities and relations translated from the knowledge graph of gold summary. An example of KGtext is shown in Figure 3.</p>
<p>KGtext Construction To construct KGtext, we first use DYGIE++ (Wadden et al., 2019) to extract entities and relations from the human-written gold summary of the training set. Then we fill the KGtext with the prefix The entities and types are: followed by each entity type and entity pair like $&lt;$ TYPE $&gt;$ ENT. We also add another prompt the relations are: to introduce the relations, in the form of ENT_1 <REL> ENT_2.</p>
<p>KGtext serves as an information-intact alternative to the knowledge graph of gold summary, which is generated by the KGtext generator and can provide knowledge graphs information for the final summary generation.</p>
<p>Decoding Since the knowledge graph of gold summary is obtained by synthesizing and simplifying the knowledge graphs of input papers via the interaction of nodes, the graph structure plays an important role in KGtext generation. Hence during decoding, we leverage source token representations as well as graph representations during KGtext decoding process.</p>
<p>We apply a stack of $L_{2}$ Transformer decoding layers as the decoder. The cross-attention sub-layer of each decoding layer computes two multi-head attention to capture both textual and graph context. Let $\hat{g}<em i="i">{i}^{l}$ denotes the $i$-th token output representation by the $l$-th self-attention sub-layer. For the textual context, we use $\hat{g}</em>$ from entity-sentence updater as keys}^{l}$ as query and token representations $\mathbf{H}_{\mathbf{W}</p>
<p>and values.</p>
<p>$$
c_{i, w}^{l}=\operatorname{MHAtt}\left(\hat{g}<em _mathbf_W="\mathbf{W">{i}^{l}, \mathbf{H}</em>\right)
$$}}, \mathbf{H}_{\mathbf{W}</p>
<p>where MHAtt denotes the multi-head attention module proposed in Vaswani et al. (2017).</p>
<p>For the graph context, we use $\hat{g}<em _mathbf_E="\mathbf{E">{i}^{l}$ as query and entity nodes representations $\mathbf{H}</em>$, we modify MHAtt module by multiplying $S$ with the attention weights.}}$ from entity-sentence updater as keys and values. Considering that different entities of the input have different importance, we apply the unsupervised phrase scoring algorithm RAKE (Rose et al., 2010) to score the salience of entities, and incorporate entity salience into graph context computation. Given the RAKE scores $S=\left{s_{j}\right}$ for entity nodes representations $\mathbf{H}_{\mathbf{E}</p>
<p>$$
c_{i, g}^{l}=\operatorname{MHAtt} _ \operatorname{Mod}\left(\hat{g}<em _mathbf_E="\mathbf{E">{i}^{l}, \mathbf{H}</em>, S\right)
$$}}, \mathbf{H}_{\mathbf{E}</p>
<p>where MHAtt_Mod denotes the modified MHAtt module. And the modified attention weight $\alpha_{i}^{n}$ of head $n$ is calculated as</p>
<p>$$
\alpha_{i}^{n}=\frac{\left(\mathrm{W}<em _mathbf_E="\mathbf{E">{K}^{n} \mathbf{H}</em>}}\right)^{T}\left(\mathrm{~W<em i="i">{Q}^{n} \hat{g}</em> * S
$$}^{l}\right)}{\sqrt{d_{\text {head }}}</p>
<p>where $\mathrm{W}<em Q="Q">{K}^{n}$ and $\mathrm{W}</em>$ denotes the dimension of each attention head.}^{n}$ are parameter weights, $d_{\text {head }</p>
<p>We then add a fusion gate to merge both the textual context and the graph context.</p>
<p>$$
\begin{aligned}
c_{i}^{l} &amp; =z * c_{i, w}^{l}+(1-z) * c_{i, g}^{l} \
z &amp; =\operatorname{sigmoid}\left(\left[c_{i, w}^{l} ; c_{i, g}^{l}\right] \mathrm{W}<em f="f">{f}+b</em>\right)
\end{aligned}
$$</p>
<p>where $\mathrm{W}<em f="f">{f}$ and $b</em>$ are the linear transformation parameter. The feed-forward network is used to further transform the output.</p>
<p>$$
g_{i}^{l}=\operatorname{LayerNorm}\left(c_{i}^{l}+\operatorname{FFN}\left(c_{i}^{l}\right)\right)
$$</p>
<p>The generation distribution $p_{i}^{g}$ over the target vocabulary is calculated by feeding the output $g_{i}^{L_{2}}$ to a softmax layer.</p>
<p>$$
p_{i}^{g}=\operatorname{softmax}\left(g_{i}^{L_{2}} \mathrm{~W}<em g="g">{g}+b</em>\right)
$$</p>
<p>where $\mathrm{W}<em g="g">{g}$ and $b</em>$ are learnable linear transformation parameter.</p>
<p>Furthermore, we also employ the copy mechanism (See et al., 2017) to alleviate the out-ofvocabulary (OOV) problem. The final generation distribution $p_{i}^{l}$ is the "mixture" of both $p_{i}^{g}$ and the copy probability over source words $p_{i}^{s}$.</p>
<p>The loss is the negative log likelihood of the gold KGtext $w_{t_{i}}$ :</p>
<p>$$
L_{T}=-\frac{1}{\hat{N}} \sum_{i=1}^{\hat{N}} \log p_{i}^{l}\left(w_{t_{i}}\right)
$$</p>
<h3>2.7 Summary Generator</h3>
<p>The final summary generator has a similar decoding architecture to the KGtext generator, but differs in that the summary generator utilizes the generated KGtext to guide summary generation.</p>
<p>Given the KGtext generative distribution $\left{p_{i}^{l}\right}$, we obtain the decoding sequence of KGtext $T$ by greedy search during training. Then we add an encoder similar to the aforementioned sentence encoder to get the KGtext representations $\mathbf{H}<em i_="i," t="t">{\mathbf{T}}$. Besides attending to textual and graph context, we use the same multi-head attention as equation (4) to compute KGtext context $\hat{c}</em>$ to capture KGtext influence.}^{l</p>
<p>Together with the textual context $\hat{c}<em g="g" i_="i,">{i, w}^{l}$ and the graph context $\hat{c}</em>$, we apply a hierarchical fusion mechanism to combine the three contexts, by first merging the textual context and the graph context, and then the KGtext context.}^{l</p>
<p>$$
\begin{aligned}
&amp; \hat{c}<em 1="1">{i}^{l}=z</em>} * \hat{c<em 1="1">{i, w}^{l}+\left(1-z</em>}\right) * \hat{c<em 1="1">{i, g}^{l} \
&amp; z</em>}=\operatorname{sigmoid}\left(\left[\hat{c<em g="g" i_="i,">{i, w}^{l} ; \hat{c}</em>}^{l}\right] \mathrm{W<em 1_="1," f="f">{1, f}+b</em>\right) \
&amp; c_{i}^{l}=z_{2} * \hat{c}<em 2="2">{i}^{l}+\left(1-z</em>}\right) * \hat{c<em 2="2">{i, t}^{l} \
&amp; z</em>}=\operatorname{sigmoid}\left(\left[\hat{c<em i_="i," t="t">{i}^{l} ; \hat{c}</em>}^{l}\right] \mathrm{W<em 2_="2," f="f">{2, f}+b</em>\right)
\end{aligned}
$$</p>
<p>where $\mathrm{W}<em 1_="1," f="f">{1, f}, b</em>}, \mathrm{~W<em 2_="2," f="f">{2, f}$ and $b</em>$ are the linear transformation parameter.</p>
<p>Given the final summary generation distribution $p_{i}^{s}$, the loss is the negative log likelihood of the gold summary $w_{i}$ :</p>
<p>$$
L_{S}=-\frac{1}{N_{s}} \sum_{i=1}^{N_{s}} \log p_{i}^{s}\left(w_{i}\right)
$$</p>
<h3>2.8 Training Strategy</h3>
<p>We train the KGtext generator and the summary generator in a unified architecture in an end-toend manner. Furthermore, in practice, we find the KGtext generated from greedy search has a strong influence on the summary generation. The lowquality KGtext greatly impairs the performance of the model. Hence, we train another auxiliary decoder on top of $P_{\theta_{\mathcal{D} \rightarrow \mathcal{S}}}(S \mid \mathcal{D})$, which uses a onestage decoder without generating KGtext. It has the same architecture as the summary generator except for the cross-attention on KGtext.</p>
<p>Given the final summary generation distribution of the auxiliary decoder $\hat{p}<em i="i">{i}^{s}$, the loss is the negative log likelihood of the gold summary $w</em>$ :</p>
<p>$$
L_{A}=-\frac{1}{N_{s}} \sum_{i=1}^{N_{s}} \log \hat{p}<em i="i">{i}^{s}\left(w</em>\right)
$$</p>
<p>The final loss function is:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em T="T">{S}+\lambda \mathcal{L}</em>
$$}+\eta \mathcal{L}_{A</p>
<p>where $\lambda$ and $\eta$ are both hyper parameters. In this way, we can reduce the effect of some low-quality generated KGtext and improve the stability of our model.</p>
<h2>3 Experiments</h2>
<h3>3.1 Dataset</h3>
<p>We conduct experiments on the recently released Multi-Xscience dataset (Lu et al., 2020), which is the first large-scale and open MDSS dataset. It contains 30,369 instances for training, 5,066 for validation and 5,093 for test. On average, each source paper cluster contains 4.42 papers and 778.08 words, and each gold summary contains 116.44 words.</p>
<h3>3.2 Implementation Details</h3>
<p>We set our model parameters based on preliminary experiments on the validation set. We prune the vocabulary to 50 K . The number of encoding layer $L_{1}$ and the number of decoding layer $L_{2}$ are both 6 . We set the dimension of word embeddings and hidden size to 256 , feed-forward size to 1,024 . We set 8 heads for multi-head attention. For the Graph Transformer of the graph updater and the entity-sentence updater, we set the number of iterations to 3 . We set dropout rate to 0.1 and label smoothing (Szegedy et al., 2016) factor to 0.1 . We use Adam optimizer with learning rate $\alpha=0.02$, $\beta_{1}=0.9, \beta_{2}=0.998$; we also apply learning rate warmup over the first 8000 steps, and decay as in Vaswani et al. (2017). The batch size is set to 8 . $\lambda$ and $\eta$ are both set to 1.0. The model is trained on 1 GPU (NVIDIA Tesla V100, 32G) for 100,000 steps. We select the top-3 best checkpoints based on performance on the validation set and report averaged results on the test set.</p>
<p>For KGtext decoding, we use greedy search with the minimal generation length 100 , while for summary decoding, we use beam search with beam size 5 and the minimal generation length is 110 , consistent with Lu et al. (2020). The length penalty factor is set to 0.4 .</p>
<h3>3.3 Metrics and Baselines</h3>
<p>We use ROUGE $F_{1}$ (Lin, 2004) to evaluate the summarization quality. Following previous work,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">R-1</th>
<th style="text-align: left;">R-2</th>
<th style="text-align: left;">R-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Extractive</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">LexRank (Erkan and Radev, 2004)</td>
<td style="text-align: left;">30.19</td>
<td style="text-align: left;">5.53</td>
<td style="text-align: left;">26.19</td>
</tr>
<tr>
<td style="text-align: left;">TextRank (Mihalcea and Tarau, 2004)</td>
<td style="text-align: left;">31.51</td>
<td style="text-align: left;">5.83</td>
<td style="text-align: left;">26.58</td>
</tr>
<tr>
<td style="text-align: left;">HeterSumGraph* (Wang et al., 2020)</td>
<td style="text-align: left;">31.36</td>
<td style="text-align: left;">5.82</td>
<td style="text-align: left;">27.41</td>
</tr>
<tr>
<td style="text-align: left;">Ext-Oracle</td>
<td style="text-align: left;">38.45</td>
<td style="text-align: left;">9.93</td>
<td style="text-align: left;">27.11</td>
</tr>
<tr>
<td style="text-align: left;">Abstractive</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GraphSum* (Li et al., 2020)</td>
<td style="text-align: left;">29.58</td>
<td style="text-align: left;">5.54</td>
<td style="text-align: left;">26.52</td>
</tr>
<tr>
<td style="text-align: left;">Hiersumm (Liu and Lapata, 2019a)</td>
<td style="text-align: left;">30.02</td>
<td style="text-align: left;">5.04</td>
<td style="text-align: left;">27.6</td>
</tr>
<tr>
<td style="text-align: left;">HiMAP (Fabbri et al., 2019)</td>
<td style="text-align: left;">31.66</td>
<td style="text-align: left;">5.91</td>
<td style="text-align: left;">28.43</td>
</tr>
<tr>
<td style="text-align: left;">BertABS (Liu and Lapata, 2019b)</td>
<td style="text-align: left;">31.56</td>
<td style="text-align: left;">5.02</td>
<td style="text-align: left;">28.05</td>
</tr>
<tr>
<td style="text-align: left;">BART (Lewis et al., 2020)</td>
<td style="text-align: left;">32.83</td>
<td style="text-align: left;">6.36</td>
<td style="text-align: left;">26.61</td>
</tr>
<tr>
<td style="text-align: left;">SciBertABS (Lu et al., 2020)</td>
<td style="text-align: left;">32.12</td>
<td style="text-align: left;">5.59</td>
<td style="text-align: left;">29.01</td>
</tr>
<tr>
<td style="text-align: left;">MGSum* (Jin et al., 2020)</td>
<td style="text-align: left;">33.11</td>
<td style="text-align: left;">6.75</td>
<td style="text-align: left;">29.43</td>
</tr>
<tr>
<td style="text-align: left;">Pointer-Generator (See et al., 2017)</td>
<td style="text-align: left;">34.11</td>
<td style="text-align: left;">6.76</td>
<td style="text-align: left;">30.63</td>
</tr>
<tr>
<td style="text-align: left;">KGSum</td>
<td style="text-align: left;">$\mathbf{3 5 . 7 7}$</td>
<td style="text-align: left;">$\mathbf{7 . 5 1}$</td>
<td style="text-align: left;">$\mathbf{3 1 . 4 3}$</td>
</tr>
</tbody>
</table>
<p>Table 1: ROUGE F1 evaluation results on the test set of Multi-Xscience. The results marked with * are obtained by running the released code using the same beam size and decoding length. Other results without * are from Lu et al. (2020).
we report unigram and bigram overlap (ROUGE1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency.</p>
<p>We compare our model with several typical extractive and abstractive summarization models. Due to space limitations, we put the introduction of these models in appendix A.</p>
<h3>3.4 Automatic Evaluation</h3>
<p>Table 1 summarizes the evaluation results on the Multi-Xscience dataset.</p>
<p>As can be seen, abstractive models generally outperform extractive models, especially on ROUGEL, showing that abstractive models can generate more fluent summaries. Among the abstractive baselines, Pointer-Generator (See et al., 2017) surprisingly outperforms other newer models. We partially attribute this result to Pointer-Generator designing an additional coverage mechanism (Tu et al., 2016) to effectively reduce redundancy. This result also illustrates that MDSS is challenging and requires domain-specific solutions for paper content representation and cross-paper relationship modeling.</p>
<p>The last block reports the result of our model KGSum. KGSum outperforms any other models, achieving scores of $35.77,7.51$, and 31.43 on the three ROUGE metrics. Our model surpasses other models by a remarkable large margin (at least improving $1.66 \%, 0.75 \%$, and $0.80 \%$ ). The result</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Overall</th>
<th>Inf</th>
<th>Fluency</th>
<th>Succ</th>
</tr>
</thead>
<tbody>
<tr>
<td>GraphSum</td>
<td>$-1.42^{*}$</td>
<td>$-1.47^{*}$</td>
<td>$-1.08^{*}$</td>
<td>$-1.23^{*}$</td>
</tr>
<tr>
<td>MGSum</td>
<td>$-0.38^{*}$</td>
<td>0.60</td>
<td>$-0.20^{*}$</td>
<td>$-0.55^{*}$</td>
</tr>
<tr>
<td>Pointer-Generator</td>
<td>$0.62^{*}$</td>
<td>$0.31^{*}$</td>
<td>$0.17^{*}$</td>
<td>$0.60^{*}$</td>
</tr>
<tr>
<td>KGSum</td>
<td>$\mathbf{1 . 3 0}$</td>
<td>$\mathbf{0 . 6 8}$</td>
<td>$\mathbf{1 . 1 7}$</td>
<td>$\mathbf{1 . 2 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Human evaluation of system summaries on Multi-Xscience test set. Inf stands for informativeness and Succ stands for succinctness. The larger rating denotes better summary quality. * indicates the ratings of the corresponding model are significantly (by Welch's t-test with $p&lt;0.05$ ) outperformed by our model. The inter-annotator agreement score (Fleiss Kappa) is 0.63 , which indicates substantial agreement between annotators.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">R-1</th>
<th style="text-align: left;">R-2</th>
<th style="text-align: left;">R-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">KGSum</td>
<td style="text-align: left;">$\mathbf{3 5 . 7 7}$</td>
<td style="text-align: left;">$\mathbf{7 . 5 1}$</td>
<td style="text-align: left;">$\mathbf{3 1 . 4 3}$</td>
</tr>
<tr>
<td style="text-align: left;">- KGG</td>
<td style="text-align: left;">35.34</td>
<td style="text-align: left;">7.28</td>
<td style="text-align: left;">30.91</td>
</tr>
<tr>
<td style="text-align: left;">- KGG - RAKE</td>
<td style="text-align: left;">35.17</td>
<td style="text-align: left;">7.18</td>
<td style="text-align: left;">30.75</td>
</tr>
<tr>
<td style="text-align: left;">- KGG - RAKE - GU</td>
<td style="text-align: left;">34.97</td>
<td style="text-align: left;">7.08</td>
<td style="text-align: left;">30.63</td>
</tr>
<tr>
<td style="text-align: left;">- KGG - RAKE - GU - ESU</td>
<td style="text-align: left;">34.79</td>
<td style="text-align: left;">6.90</td>
<td style="text-align: left;">30.36</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation studies on Multi-Xscience test set. We remove various modules and explore their influence on our model. '-' means the removal operation from KGSum. The last row (-KGG-RAKE-GU-ESU) is the clean baseline without any module we propose.
demonstrates that our model can generate more informative and more coherent summaries, indicating the effectiveness of our proposed knowledge graph-centric encoder and decoder framework.</p>
<h3>3.5 Human Evaluation</h3>
<p>We further access the linguistic quality of generated summaries by human evaluation. We randomly select 30 test instances from the Multi-Xscience test set, and invite three graduate students as annotators to evaluate the outputs of different models independently. Annotators assess the overall quality of summaries by ranking them considering the following criteria:: (1) Informativeness: does the summary convey important facts of the input papers? (2) Fluency: is the summary fluent and grammatical? (3) Succinctness: whether the summary contains repeated information? Annotators are asked to rank all systems from 1 (best) to 4 (worst). All systems get score $2,1,-1,-2$ for ranking $1,2,3,4$ respectively. The rating of each system is computed by averaging the scores on all test instances.</p>
<p>The result is shown in Table 2. The overall rating and the ratings for the above three aspects are reported. We can see that KGSum performs much
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Recall of EW and CW for different models on Multi-Xscience test set. PG stands for PointerGenerator, Baseline is our Transformer-based model without any module we propose.
better than other models. The overall rating of KGSum achieves 1.2, which is much higher than $0.62,-0.38$, and -1.42 of Pointer-Generator, MGSum, and GraphSum. The results on informativeness indicate our model can effectively capture the salient information of papers and generate more informative summaries. The results on fluency and succinctness indicate that KGSum is able to generate more fluent and concise summaries. Furthermore, Pointer-Generator achieves a much higher score on succinctness than MGSum, which further proves that Pointer-Generator generates less redundant summaries and thus has better performance.</p>
<h3>3.6 Model Analysis</h3>
<p>For a thorough understanding of KGSum, we conduct several experiments on Multi-Xscience test set.</p>
<p>Ablation Study Firstly, we perform an ablation study to validate the effectiveness of individual components. Here, KGG stands for KGtext generator, RAKE refers to the RAKE algorithm that measures entity salience, GU stands for graph updater, ESU stands for entity-sentence updater. We remove KGG, RAKE, GU, ESU one by one in order from decoder to encoder. The result is illustrated in Table 3. We find that the GU and ESU module in the encoder can effectively encode knowledge graph information and utilize knowledge graphs to enable better information flowing between text nodes, which is conducive to content modeling and relationship modeling. Using RAKE to measure entity salience also benefits a lot for graph context computation when decoding. Further, the KGG module also brings significant improvement, indi-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">KGtext Variants</th>
<th style="text-align: left;">R-1</th>
<th style="text-align: left;">R-2</th>
<th style="text-align: left;">R-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ent</td>
<td style="text-align: left;">35.61</td>
<td style="text-align: left;">7.43</td>
<td style="text-align: left;">31.24</td>
</tr>
<tr>
<td style="text-align: left;">Ent+Type</td>
<td style="text-align: left;">35.67</td>
<td style="text-align: left;">7.42</td>
<td style="text-align: left;">31.29</td>
</tr>
<tr>
<td style="text-align: left;">Ent+Type+Rel</td>
<td style="text-align: left;">$\mathbf{3 5 . 7 7}$</td>
<td style="text-align: left;">$\mathbf{7 . 5 1}$</td>
<td style="text-align: left;">$\mathbf{3 1 . 4 3}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Analysis of the impact of different KGtext contents on summarization.
cating our proposed two-stage decoder with KGtext generator is effective in generating summary under the guidance of knowledge graphs.</p>
<p>Recall of Entity Words In order to intuitively demonstrate the impact of knowledge graphs, we investigate the recall of gold summary entities in the generated summary. The exact match of entities is difficult because entities have different mentions. Therefore, we count recall of entity words instead. We classify the words in papers into two categories: Entity Words (EW) and Context Words (CW). EW are defined as words in papers that are recognized as entities by SciIE tools, while CW are words other than EW. We exclude stopwords when calculating EW and CW, because stopwords have no practical meaning. Then, we define Recall of $E W$ as:</p>
<p>$$
\operatorname{Recall}<em S="S" _in_123_R="\in{R" e="e" f_125_="f}">{E W}=\frac{\sum</em>} \sum_{N_{E W} \in S} \operatorname{Count<em E="E" W="W">{\text {match }}\left(N</em>
$$}\right)}{\sum_{S \in{R e f}} \sum_{N_{E W} \in S} \operatorname{Count}\left(N_{E W}\right)</p>
<p>where ${R e f}$ denotes the gold summaries, $\operatorname{Count}<em E="E" W="W">{\text {match }}\left(N</em>\right)$ denotes the number of EW in the gold summaries. Recall of $C W$ is defined in a similar manner.}\right)$ denotes the number of overlapped EW in the gold summaries and the generated summaries. $\operatorname{Count}\left(N_{E W</p>
<p>The results are shown in Figure 4. We find that KGSum achieves the highest recall of EW, compared with the baseline model and other models. The result proves that our model focuses on more entity information under the guidance of knowledge graphs. Conversely, in Figure 4b, MGSum achieves the highest recall of CW, but ROUGE1/2/L scores of MGSum are only 33.11/6.75/29.43, falling behind KGSum. The result indicates that recall of CW has limited effect on model performance, which is in line with our intuition since CW do not contain important semantic information.</p>
<p>Influence of KGtext Contents We also conduct experiments to analyze the impact of different KGtext contents on MDSS. We consider the follow-
ing three variants: (1) only entities (Ent), (2) entities + types (Ent+Type), (3) entities + types + relations (Ent+Type+Rel), to construct the KGtext using the same strategy in section 2.6. Result in Table 4 demonstrates MDSS can benefit from different components of knowledge graph, including entities, types and relations.</p>
<h2>4 Related Work</h2>
<p>Early MDSS works are mainly based on artificially constructed small-scale datasets, using unsupervised extractive methods to extract sentences from multiple papers. Mohammad et al. (2009) take citation texts, paper abstracts and full paper texts as input for survey generation. They conduct the experiment with just two instances. Jha et al. (2015a) construct 15 instances and combine a content model with a discourse model to generate coherent scientific summarizations. Hoang and Kan (2010) construct 20 instances, each with an annotated topic hierarchy tree, to generate summarization for multiple scientific papers. Similar works also exist in (Jha et al., 2015b; Hu and Wan, 2014; Yang et al., 2017). These unsupervised works are crude in both content modeling and relationship modeling and fail to generate high-quality summaries.</p>
<p>Some subsequent efforts apply deep learningbased methods to MDSS using large-scale datasets (Wang et al., 2018; Jiang et al., 2019; Chen et al., 2021). Wang et al. (2018) build a dataset with 8080 instances and construct a heterogeneous bibliography graph, and then utilize a CNN and RNNbased model for extractive summarization. Jiang et al. (2019) collect 390,000 instances, and use a hierarchical encoder and a two-step decoder to generate summary in an abstractive manner for the first time. Chen et al. (2021) collect two large-scale datasets with 136,655 and 80,927 instances, respectively. They apply a Transformer-based model to capture the relevance between papers for abstractive summarization. However, all the above works neglect salient semantic units to capture semantic information and relationships between papers. In this paper, based on Mutli-Xscience (Lu et al., 2020), we use knowledge graph information to model content and relationships between papers.</p>
<h2>5 Conclusion</h2>
<p>In this work, we propose a knowledge graph-centric Transformer-based model for MDSS. Our model is able to incorporate knowledge graph information</p>
<p>into the paper encoding process with a graph updater and an entity-sentence updater, and introduce a two-stage decoder including a KGtext generator and a summary generator to guide the summary decoding with knowledge graph information. Experiments show that the proposed model significantly outperforms all strong baselines and achieves the best result on the Multi-Xscience dataset.</p>
<p>In the future, we will explore other more intuitive and effective methods to incorporate graph information in both the encoding and decoding phase of summary generation.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Key Research and Development Project of China (No. 2021ZD0110700) and Hunan Provincial Natural Science Foundation (Grant Nos. 2022JJ30668). The authors would like to thank the anonynous reviewers for their valuable comments and suggestions to improve this paper.</p>
<h2>References</h2>
<p>Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, and Rui Yan. 2021. Capturing relations between scientific papers: An abstractive model for related work section generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6068-6077.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>GÃ¼nes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of artificial intelligence research, 22:457-479.</p>
<p>Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074-1084.</p>
<p>Cong Duy Vu Hoang and Min-Yen Kan. 2010. Towards automated related work summarization. In Proceedings of the 23rd International Conference on</p>
<p>Computational Linguistics: Posters, pages 427-435. Association for Computational Linguistics.</p>
<p>Yue Hu and Xiaojun Wan. 2014. Automatic generation of related work sections in scientific papers: an optimization approach. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1624-1633.</p>
<p>Rahul Jha, Reed Coke, and Dragomir Radev. 2015a. Surveyor: A system for generating coherent survey articles for scientific topics. In Twenty-Ninth AAAI conference on artificial intelligence.</p>
<p>Rahul Jha, Catherine Finegan-Dollak, Ben King, Reed Coke, and Dragomir Radev. 2015b. Content models for survey generation: a factoid-based evaluation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages 441-450.</p>
<p>Xiao-Jian Jiang, Xian-Ling Mao, Bo-Si Feng, Xiaochi Wei, Bin-Bin Bian, and Heyan Huang. 2019. Hsds: An abstractive model for automatic survey generation. In International Conference on Database Systems for Advanced Applications, pages 70-86. Springer.</p>
<p>Hanqi Jin, Tianming Wang, and Xiaojun Wan. 2020. Multi-granularity interaction network for extractive and abstractive multi-document summarization. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 62446254 .</p>
<p>Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text generation from knowledge graphs with graph transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293.</p>
<p>Friedrich Wilhelm Levi. 1942. Finite geometrical systems.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880.</p>
<p>Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, and Junping Du. 2020. Leveraging graph to improve abstractive multi-document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6232-6243.</p>
<p>Zekang Li, Cheng Niu, Fandong Meng, Yang Feng, Qian Li, and Jie Zhou. 2019. Incremental transformer with deliberation decoder for document grounded conversations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 12-21.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Yang Liu and Mirella Lapata. 2019a. Hierarchical transformers for multi-document summarization. arXiv preprint arXiv:1905.13164.</p>
<p>Yang Liu and Mirella Lapata. 2019b. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740.</p>
<p>Yao Lu, Yue Dong, and Laurent Charlin. 2020. Multixscience: A large-scale dataset for extreme multidocument summarization of scientific articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8068-8074.</p>
<p>Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle Lo, Isabel Cachola, and Noah A Smith. 2021. Explaining relationships between scientific documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages $2130-2144$.</p>
<p>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404-411.</p>
<p>Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishnan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientific paradigms. In Proceedings of human language technologies: The 2009 annual conference of the North American chapter of the association for computational linguistics, pages 584-592.</p>
<p>Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. 2010. Automatic keyword extraction from individual documents. Text mining: applications and theory, 1:1-20.</p>
<p>Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083.</p>
<p>Darsh J Shah and Regina Barzilay. 2021. Generating related work. arXiv preprint arXiv:2104.08668.</p>
<p>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818-2826.</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 76-85.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 57845789 .</p>
<p>Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, and Xuan-Jing Huang. 2020. Heterogeneous graph neural networks for extractive document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6209-6219.</p>
<p>Yongzhen Wang, Xiaozhong Liu, and Zheng Gao. 2018. Neural related work summarization with a joint context-driven attention mechanism. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 17761786.</p>
<p>Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation networks: Sequence generation beyond one-pass decoding. Advances in neural information processing systems, 30 .</p>
<p>Shansong Yang, Weiming Lu, Dezhi Yang, Xi Li, Chao Wu, and Baogang Wei. 2017. Keyphraseds: Automatic generation of survey by exploiting keyphrase information. Neurocomputing, 224:58-70.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328-11339. PMLR.</p>
<h2>A Baselines</h2>
<p>LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two unsupervised graph based extractive summarization models. HeterSumGraph (Wang et al.,</p>
<p>2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum ( Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. PointerGenerator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles. BART (Lewis et al., 2020) is a pretrained text generation model.</p>
<h1>B Case Study</h1>
<p>In Figure 5, we present several example summaries to show the generating quality of different models. In the figure, there are five blocks, which are the reference summary (Reference Summary), the system summaries generated by Pointer-Generator, GraphSum, MGSum and our model KGSum. We highlight the salient contents from system summaries that can be find in the reference summary. We could find our model KGSum possesses the most salient contents and the highest overlap with the reference summary. Pointer-Generator and MGSum have the same amount of salient contents, but MGSum contains more repeated information, leading to worse performance. GraphSum contains the least salient contents, resulting in the worst performance.</p>
<h1>Reference Summary:</h1>
<p>most recent approaches rely on convolutional neural networks (cnns). @cite, the work in @cite is extended by adding a cnn to describe the posterior density of an object pose. a combination of using a cnn for object segmentation and geometry-based pose estimation is proposed in @cite . posecnn @cite uses a similar two-stage network, in which the first stage extracts feature maps from rgb input and the second stage uses the generated maps for object segmentation. 3d translation estimation and 3d rotation regression in quaternion format. depth data and 1 ep are used for geometry refinement. @cite propose a three-stage, instance-aware approach for 6 d object pose estimation. an instance segmentation network is first applied, followed by an encoder-decoder network which estimates the 3d object coordinates for each segment. the 6 d pose is recovered with a geometric pose optimization step similar to @cite .</p>
<h2>Pointer-Generator:</h2>
<p>our work is also related to the work in @cite, where the authors use a convolutional neural networks (cnns) to predict the 6 d pose of specific objects in a single rgb - d image. @cite proposed a new loss function for 6 d pose estimation of object pose estimation. @cite, the authors propose a method to directly regress a pose vector from raw point cloud segments using a quaternion neural network. they then applied a quaternion representation to a quaternion model, which can be used to learn the pose of the object pose. in contrast to our work, our method is able to learn pose estimation from a single image</p>
<h2>GraphSum:</h2>
<p>3d object pose estimation has been a long-standing research topic in computer vision and computer vision @cite @cite @cite . in recent years, there has been significant interest in object reconstruction from a single image @cite @cite or 3d object @cite @cite, and many of them have been trying to recover from single images @cite . however, most of these methods rely on the availability of a large number of training images. in contrast, our approach does not require a large amount of labeled training data. in addition, our approach is able to estimate the 3d pose of a single object.</p>
<h2>MGSum:</h2>
<p>posecnn @cite is a flexible approach for 6 d object pose estimation. it uses a cnn to predict 6 d pose from a single image. posecnn @cite predicts 6 d pose by regressing the 3 d rotation of a 3 d object by regressing a quaternion representation. posecnn @cite uses a quaternion network to predict the 6 d pose using a quaternion network, and learns a quaternion representation of the object representation. posecnn @cite proposes a "learns approach for estimating 6 d object pixels in an image using quaternion representation. posecnn and posecnn @cite propose an approach approach to regressing the object pixels from a particular object representation. however, these methods require a large amount of training data, which is difficult to obtain. in contrast, our method does not rely on a large number of objects in the scene.</p>
<h2>KGSum:</h2>
<p>6d pose estimation has been widely used in computer vision @cite @cite . in @cite, the authors propose a cnn for 6 d object pose estimation. the first step is to use a quaternion to handle the 6 d pose of an object in a single image. the second stage is based on the idea of using a convolutional neural networks to learn a 3d rotation of the object and the appearance of a 6 d pose. the proposed method is also used for object segmentation @cite and pose estimation, which is used to refine the geometry of object proposals. @cite used an encoder-decoder network to handle the 6 d pose of known objects in a rgb-d image by regressing the 3 d rotation of an object with a 3 d representation.</p>
<p>Figure 5: Case study from the Multi-Xscience test set. We first highlight the salient contents in the reference summary in different colors. Then the overlapped salient contents of system summaries are annotated in the same colors.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ Corresponding authors.
${ }^{\ddagger}$ https://github.com/muguruzawang/KGSum&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1134 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1134</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1134</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-260154786</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.13372v2.pdf" target="_blank">Submodular Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SubPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SubRL instances even in large state- and action- spaces. We showcase the versatility of our approach by applying SubPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1134.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1134.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SUBPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Submodular Policy Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A policy-gradient based algorithm for reinforcement learning with submodular (history-dependent, diminishing-returns) rewards that greedily maximizes marginal gains via a specialized policy-gradient estimator and optional baselines; supports Markovian and non-Markovian policy parameterizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SUBPO (SUBmodular POlicy optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy-gradient algorithm parameterized by θ (neural-network policies in experiments). Key components: (i) marginal-gain based gradient estimator (uses F(s | past) marginals), (ii) baseline (history-dependent or cumulative marginal estimate) for variance reduction, (iii) entropy regularization / soft policy updates for exploration, (iv) supports Markovian (SUBPO-M) and non-Markovian/RNN-style (SUBPO-NM) policies. Implemented with MLPs in experiments; Monte Carlo rollouts for gradient estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information-gain / marginal-gain maximization (submodular greedy policy using marginal returns); more generally, information-directed experimental design when objective is mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each timestep the policy gradient is computed using weighted score-function estimates where the weight is the sum of future marginal gains H-1_{j=i} F(s_{j+1} | τ_{0:j}) (Theorem 2). The agent therefore adapts actions to maximize expected marginal improvement (or mutual information in experiment-design tasks), uses history-dependent baselines to reduce variance, and optionally entropy-penalized updates to encourage exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple (Informative path planning, Bayesian D-experimental design, Item collection, Building exploration, Car Racing, MuJoCo Ant)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Varied: unknown functions (GP-based) for experiment design and informative path planning (partial knowledge, noisy observations), stochastic dynamics in item-collection (action success prob 0.9), deterministic or nearly-deterministic dynamics (ϵ-Bandit SMDP idealization), continuous high-dimensional dynamics for car racing and MuJoCo (partial observability via local sensing patches, noisy observations), episodic finite-horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Multiple reported configs: discrete grid world 30x30 (900 states), actions {up,down,left,right,stay} (|A|=5), horizon H=40; item-collection used similar grid with stochastic transitions p=0.9 success; Bayesian D-design used GP priors over continuous domains (10 random envs, 20 runs each); Car Racing: continuous state dim 6, control dim 2, horizon 700, batch 8, ~6000 epochs to converge in experiments; MuJoCo Ant: state dim 30, action dim 8, discretized coverage grid 400x400 for reward, horizon 400, batch 15, ~20k epochs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative reported results: SUBPO (both M and NM variants) consistently outperforms the modular-reward baseline (MODPO) across tasks; SUBPO-M often matches SUBPO-NM performance while being more sample-efficient in many tasks (e.g., informative path planning, Bayesian D-design, coverage tasks). No absolute numeric cumulative-reward values published in main text; plots show normalized J(π) improved vs baseline. (Numerical training budgets: grid tasks converged within ~150 epochs; car racing required ~6000 epochs; MuJoCo Ant required ~20,000 epochs.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline MODPO (maximizes modular per-state rewards) often gets stuck in high-density/high-uncertainty regions and yields substantially lower normalized J(π) than SUBPO; exact numeric baselines not provided (qualitative comparisons via plots).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Authors report SUBPO-M reaches comparable performance to SUBPO-NM with fewer samples due to smaller policy search space; for discrete 30x30 grid tasks SUBPO-M typically saturates within ~150 epochs (batch B=500), total runtime <20 minutes for 150 epochs on 2 CPU cores; continuous car racing: ~6000 epochs with batch 8 (~1 hour single-core); MuJoCo Ant: ~20k epochs (≈6 hours single-core). No formal sample-complexity bounds provided for general case.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration encouraged via entropy penalization in policy update (soft updates); exploitation encouraged by greedily maximizing expected marginal gain (marginal-information weighting in gradient). The greedy marginal weighting biases action updates toward steps with highest expected information/marginal reward; baselines reduce variance enabling more stable exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>MODPO (modular-policy optimization / standard additive-reward policy gradient baseline); SUBPO-M (Markovian SUBPO) vs SUBPO-NM (history-aware SUBPO). In some theoretical sections compared conceptually to submodular bandit algorithms and DR-submodular continuous optimization (Frank-Wolfe variants) but experiments focus on MODPO as main baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) SUBPO provides a practical way to optimize history-dependent submodular rewards by greedily maximizing marginal gains via a specialized policy-gradient estimator. 2) In tasks that correspond to experimental design (mutual information objective) SUBPO-M attains performance comparable to SUBPO-NM while being more sample-efficient. 3) Standard modular-reward PG (MODPO) often gets stuck in local modes (repeatedly exploiting high immediate-reward states) and fails to maximize trajectory-level submodular objectives. 4) SUBPO scales to continuous high-dimensional control tasks (car racing, MuJoCo Ant) empirically. 5) Theoretical guarantees: under ϵ-Bandit and DR-submodularity assumptions, SUBPO recovers constant-factor approximations (up to 1−1/e or 1/2 for stationary points); with bounded curvature c it guarantees (1−c) optimality relative to optimal non-Markovian policy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) Worst-case SUBRL is inapproximable to any constant factor (Theorem 1) — empirical successes are instance-dependent. 2) Markovian policy parametrizations can be insufficient in some environments (item collection and building exploration) where SUBPO-M underperforms SUBPO-NM; history-dependent policies or state augmentation (RNNs, etc.) may be required. 3) Quantitative performance metrics (e.g., numeric mutual information achieved) are not provided in main text; results are presented qualitatively. 4) Some simplified theoretical guarantees rely on strong assumptions (ϵ-Bandit SMDP, state-independent policies, DR-submodularity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Submodular Reinforcement Learning', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1134.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1134.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian D-expt-design (SUBPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian D-optimal Experimental Design using SUBPO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of SUBPO to sequential experimental design: choose trajectory of sampling locations to maximize mutual information I(y_τ; f) about an unknown function f modelled by a Gaussian Process (GP), using marginal-information weighted policy gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SUBPO-M and SUBPO-NM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SUBPO agent (policy-gradient neural policy) that treats the mutual information objective F(τ) = I(y_τ; f) as a monotone submodular reward; variants include Markovian (maps state to action) and non-Markovian (maps history to action) policies. Baselines estimate cumulative marginal gains for variance reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information gain maximization (mutual information between unknown function f and noisy observations y_τ), applied as a submodular objective; operationalized via marginal-gain weighted policy gradient.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent selects next sampling locations by maximizing expected marginal information gain F(s | past τ) in the policy-gradient objective; uses sampled trajectories to estimate gradients and updates policy to prefer actions with higher expected future mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Bayesian experimental design (GP-sampled functions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown target function f with GP prior; noisy observations y = f + ε; environment is stochastic in observations but deterministic in dynamics (sampling locations); partial information about f only via samples (classic partially observable experimental-design problem).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Authors used 10 different GP-sampled environments, 20 runs each. Domain continuous; details: experiments performed on discretized sensing patches for measurement locations; training budgets unspecified beyond epoch counts (grid tasks: 150 epochs typical).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: SUBPO-M matches SUBPO-NM performance while being more sample efficient; both outperform MODPO which gets stuck in high-uncertainty regions. No numeric mutual-information or sample-efficiency numbers provided in main text (plots show normalized J(π) improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>MODPO baseline (maximizes per-state modular reward F({s})) obtains lower trajectory mutual information and tends to get stuck sampling high-uncertainty points repeatedly; plots indicate substantially worse normalized J(π) than SUBPO variants (no numeric values).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported that SUBPO-M is very sample efficient relative to SUBPO-NM due to smaller policy class; experiments: 10 envs × 20 runs each to compute confidence; discrete grid settings converged within ~150 epochs in comparable coverage tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration is enforced by entropy regularization in policy updates; exploitation occurs via marginal-information weighting in gradient updates that prioritizes high expected information gain trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>MODPO (modular reward PG), SUBPO-NM (history-aware SUBPO) as an oracle-style comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SUBPO (particularly SUBPO-M) is effective for Bayesian sequential experimental design with GP models: it achieves high mutual information trajectories and outperforms a modular-reward PG baseline, while being more sample efficient than fully history-conditioned policies in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No quantitative regret or sample-complexity bounds provided; results are empirical and qualitative. Performance depends on GP prior and environment sampling; no numerical comparisons of mutual information values reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Submodular Reinforcement Learning', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1134.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1134.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Informative Path Planning (SUBPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Informative Path Planning for biodiversity monitoring using SUBPO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application where a quadrotor plans trajectories to maximize coverage-weighted information (coverage function g(∪_s D_s) with density ρ fit from gorilla nest counts or GP samples); SUBPO maximizes marginal coverage gains to avoid locally greedy traps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SUBPO-M and SUBPO-NM (quadrotor agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Quadrotor controlled by a stochastic policy (MLP) mapping state (or history for NM) to discrete direction actions (5 actions). SUBPO uses marginal coverage gains to guide updates; baseline for variance reduction included.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Coverage-driven adaptive exploration (submodular weighted coverage objective), implemented as greedy marginal-gain maximization within policy gradient updates.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent computes marginal coverage gain of visiting candidate states given already visited sensing footprints and updates policy to favor actions that increase expected uncovered value; uses sampled trajectories to estimate gradients and adapts over epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Informative path planning (gorilla nest density / GP-sampled multimodal density on a 30×30 grid)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown nest density (fitted smooth rate or GP samples), observations when visiting states give local coverage; deterministic quadrotor dynamics in experiments; discrete 30×30 grid, horizon H=40; partial information about domain because unvisited locations' densities are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Grid 30×30 (900 states), actions 5 (directions), horizon 40; batch B=500, 150 epochs for convergence typically; experiments included 10 random environments × 20 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SUBPO-M achieves performance as good as SUBPO-NM and substantially better than MODPO (which gets stuck repeatedly exploiting a high-density region). Performance plotted as normalized J(π); convergence within ~150 epochs in grid experiments. No absolute numeric reward values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>MODPO baseline repeatedly exploits local high-density areas and attains markedly lower normalized J(π) (plots indicate SUBPO higher), with MODPO failing to improve trajectory-level coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>SUBPO-M more sample efficient than SUBPO-NM due to smaller search space; typical convergence within ~150 epochs (B=500) and runtime <20 minutes for 150 epochs on 2 CPU cores.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Entropy regularization used to encourage diverse trajectory samples; marginal-gain weighting steers exploitation toward actions that increase uncovered coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>MODPO (modular reward PG), SUBPO-NM (history-aware) compared experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Greedy marginal-gain PG (SUBPO) avoids local traps of modular reward optimization and yields substantially better coverage; Markovian SUBPO often suffices in these informative path planning tasks and is sample efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>When density landscape or dynamics require history-awareness, SUBPO-M may underperform SUBPO-NM; no numeric confidence intervals in main text beyond qualitative plots.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Submodular Reinforcement Learning', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1134.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1134.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Item Collection (SUBPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Item-collection / covering groups task using SUBPO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grid-world task where items are grouped and reward is sum_{i} min(|τ ∩ g_i|, d_i) (submodular covering); agent must collect d_i items per group under stochastic transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SUBPO-NM and SUBPO-M</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy-gradient agents parameterized by neural networks: SUBPO-NM conditions on history (so can track collected items), SUBPO-M is Markovian (state only). SUBPO uses marginal gain weighting to prefer actions leading to uncovered groups/items.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive covering strategy via submodular marginal-gain maximization (greedy marginal gains used to prioritize collecting new group items rather than revisits).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent adapts by preferring actions whose expected future marginal contribution to covering groups (∆(v | visited set)) is highest, using trajectory rollouts to estimate these marginals and update policy.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Item collection (grid groups with stochastic dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Discrete grid with groups of items (G groups), stochastic actions (0.9 executed, 0.1 random); episodic horizon sufficient to pick items; agent must track collected items to plan effectively (partial observability of history if policy is Markovian).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Experiments run on 10 random environments with 20 runs each; grid size and horizon similar to other discrete tasks (30×30, H=40 in F.1 context).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SUBPO-NM (history-aware) achieves best performance because it can track collected items; SUBPO-M attains slightly lower but comparable performance by maximizing marginal gains. Performance reported qualitatively in plots (normalized J(π)).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>MODPO (modular baseline) fails to adequately plan for future collection needs and attains worse performance (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>SUBPO-NM requires more samples due to larger policy class (history dependence); SUBPO-M is more sample efficient but slightly suboptimal compared to SUBPO-NM. Exact epoch counts per environment not enumerated in main text beyond aggregate experimental budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Marginal-gain weighting promotes exploration of groups not yet satisfied (to maximize marginal returns) while entropy regularization prevents premature convergence; history-aware policies exploit past collected information to avoid revisits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>MODPO (modular reward PG), SUBPO-M vs SUBPO-NM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>History-aware SUBPO-NM outperforms Markovian SUBPO-M in tasks that require explicit tracking of past picks, demonstrating the need for history-dependent policies in some adaptive experimental-design-like tasks; however SUBPO-M still attains reasonable performance by greedily maximizing marginals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Markovian parametrization may be insufficient when the agent must remember past picks; SUBPO-M slightly underperforms SUBPO-NM in such partially observable experimental-design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Submodular Reinforcement Learning', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1134.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1134.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Building exploration (SUBPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Building exploration / coverage with SUBPO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-room exploration task where agent must visit both rooms to maximize coverage |∪_s D_s|; demonstrates cases where Markovian policies are insufficient and history-aware policies are required.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SUBPO-M and SUBPO-NM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stochastic policy (MLP for Markovian, history-conditional NN for NM) controlling an agent with deterministic dynamics on a small grid / topological environment; SUBPO updates policy via marginal gain gradient estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Coverage-maximizing adaptive exploration (submodular coverage objective), realized via greedy marginal-gain policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent selects actions that maximize expected marginal increase in covered area given currently visited sensing patches; history-aware policy conditions on visited patches enabling planning to visit both rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Building exploration (two rooms connected by corridor)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Deterministic transitions; partial observability arises because visiting one room reduces marginal returns in nearby states; horizon just enough to cover both rooms; optimal policy is deterministic Markovian in principle but Monte Carlo learning with Markovian policies is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Small toy environment; horizon set to be just enough to cover both rooms. Exact grid size not specified, but designed to require exploring both branches of corridor.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SUBPO-NM learns to explore both rooms (tracks history) and attains better coverage; SUBPO-M achieves a sub-optimal solution that mostly explores one side (qualitative result depicted in Fig. 5a). No numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>MODPO baseline not specifically reported for this environment in main text; SUBPO-M (Markovian) effectively behaves like a weaker adaptive method and underperforms SUBPO-NM.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>SUBPO-NM requires more samples than SUBPO-M due to larger policy class; exact epoch counts for this environment not enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>History-conditioned policies exploit knowledge of visited rooms to direct exploration to unvisited room; entropy regularization can be used but Markovian policies tend to converge prematurely to local exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>SUBPO-M vs SUBPO-NM (main comparison); modular baseline discussed generally but not emphasized for this small environment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Demonstrates a failure-mode of Markovian adaptive methods: even when a deterministic Markovian optimal policy exists, Monte Carlo learning with Markovian function approximators can fail to discover it; history-aware SUBPO-NM remedies this by conditioning on visited set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>SUBPO-M can converge to sub-optimal single-room exploration; need for memory/augmented state or non-Markovian policies for some adaptive experimental-design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Submodular Reinforcement Learning', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies <em>(Rating: 2)</em></li>
                <li>Adaptive submodularity: Theory and applications in active learning and stochastic optimization <em>(Rating: 2)</em></li>
                <li>Active exploration via experiment design in markov chains <em>(Rating: 2)</em></li>
                <li>Linear submodular bandits and their application to diversified retrieval <em>(Rating: 1)</em></li>
                <li>An online algorithm for maximizing submodular functions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1134",
    "paper_id": "paper-260154786",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "SUBPO",
            "name_full": "Submodular Policy Optimization",
            "brief_description": "A policy-gradient based algorithm for reinforcement learning with submodular (history-dependent, diminishing-returns) rewards that greedily maximizes marginal gains via a specialized policy-gradient estimator and optional baselines; supports Markovian and non-Markovian policy parameterizations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SUBPO (SUBmodular POlicy optimization)",
            "agent_description": "Policy-gradient algorithm parameterized by θ (neural-network policies in experiments). Key components: (i) marginal-gain based gradient estimator (uses F(s | past) marginals), (ii) baseline (history-dependent or cumulative marginal estimate) for variance reduction, (iii) entropy regularization / soft policy updates for exploration, (iv) supports Markovian (SUBPO-M) and non-Markovian/RNN-style (SUBPO-NM) policies. Implemented with MLPs in experiments; Monte Carlo rollouts for gradient estimates.",
            "adaptive_design_method": "Information-gain / marginal-gain maximization (submodular greedy policy using marginal returns); more generally, information-directed experimental design when objective is mutual information.",
            "adaptation_strategy_description": "At each timestep the policy gradient is computed using weighted score-function estimates where the weight is the sum of future marginal gains H-1_{j=i} F(s_{j+1} | τ_{0:j}) (Theorem 2). The agent therefore adapts actions to maximize expected marginal improvement (or mutual information in experiment-design tasks), uses history-dependent baselines to reduce variance, and optionally entropy-penalized updates to encourage exploration.",
            "environment_name": "Multiple (Informative path planning, Bayesian D-experimental design, Item collection, Building exploration, Car Racing, MuJoCo Ant)",
            "environment_characteristics": "Varied: unknown functions (GP-based) for experiment design and informative path planning (partial knowledge, noisy observations), stochastic dynamics in item-collection (action success prob 0.9), deterministic or nearly-deterministic dynamics (ϵ-Bandit SMDP idealization), continuous high-dimensional dynamics for car racing and MuJoCo (partial observability via local sensing patches, noisy observations), episodic finite-horizon.",
            "environment_complexity": "Multiple reported configs: discrete grid world 30x30 (900 states), actions {up,down,left,right,stay} (|A|=5), horizon H=40; item-collection used similar grid with stochastic transitions p=0.9 success; Bayesian D-design used GP priors over continuous domains (10 random envs, 20 runs each); Car Racing: continuous state dim 6, control dim 2, horizon 700, batch 8, ~6000 epochs to converge in experiments; MuJoCo Ant: state dim 30, action dim 8, discretized coverage grid 400x400 for reward, horizon 400, batch 15, ~20k epochs in experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative reported results: SUBPO (both M and NM variants) consistently outperforms the modular-reward baseline (MODPO) across tasks; SUBPO-M often matches SUBPO-NM performance while being more sample-efficient in many tasks (e.g., informative path planning, Bayesian D-design, coverage tasks). No absolute numeric cumulative-reward values published in main text; plots show normalized J(π) improved vs baseline. (Numerical training budgets: grid tasks converged within ~150 epochs; car racing required ~6000 epochs; MuJoCo Ant required ~20,000 epochs.)",
            "performance_without_adaptation": "Baseline MODPO (maximizes modular per-state rewards) often gets stuck in high-density/high-uncertainty regions and yields substantially lower normalized J(π) than SUBPO; exact numeric baselines not provided (qualitative comparisons via plots).",
            "sample_efficiency": "Authors report SUBPO-M reaches comparable performance to SUBPO-NM with fewer samples due to smaller policy search space; for discrete 30x30 grid tasks SUBPO-M typically saturates within ~150 epochs (batch B=500), total runtime &lt;20 minutes for 150 epochs on 2 CPU cores; continuous car racing: ~6000 epochs with batch 8 (~1 hour single-core); MuJoCo Ant: ~20k epochs (≈6 hours single-core). No formal sample-complexity bounds provided for general case.",
            "exploration_exploitation_tradeoff": "Exploration encouraged via entropy penalization in policy update (soft updates); exploitation encouraged by greedily maximizing expected marginal gain (marginal-information weighting in gradient). The greedy marginal weighting biases action updates toward steps with highest expected information/marginal reward; baselines reduce variance enabling more stable exploitation.",
            "comparison_methods": "MODPO (modular-policy optimization / standard additive-reward policy gradient baseline); SUBPO-M (Markovian SUBPO) vs SUBPO-NM (history-aware SUBPO). In some theoretical sections compared conceptually to submodular bandit algorithms and DR-submodular continuous optimization (Frank-Wolfe variants) but experiments focus on MODPO as main baseline.",
            "key_results": "1) SUBPO provides a practical way to optimize history-dependent submodular rewards by greedily maximizing marginal gains via a specialized policy-gradient estimator. 2) In tasks that correspond to experimental design (mutual information objective) SUBPO-M attains performance comparable to SUBPO-NM while being more sample-efficient. 3) Standard modular-reward PG (MODPO) often gets stuck in local modes (repeatedly exploiting high immediate-reward states) and fails to maximize trajectory-level submodular objectives. 4) SUBPO scales to continuous high-dimensional control tasks (car racing, MuJoCo Ant) empirically. 5) Theoretical guarantees: under ϵ-Bandit and DR-submodularity assumptions, SUBPO recovers constant-factor approximations (up to 1−1/e or 1/2 for stationary points); with bounded curvature c it guarantees (1−c) optimality relative to optimal non-Markovian policy.",
            "limitations_or_failures": "1) Worst-case SUBRL is inapproximable to any constant factor (Theorem 1) — empirical successes are instance-dependent. 2) Markovian policy parametrizations can be insufficient in some environments (item collection and building exploration) where SUBPO-M underperforms SUBPO-NM; history-dependent policies or state augmentation (RNNs, etc.) may be required. 3) Quantitative performance metrics (e.g., numeric mutual information achieved) are not provided in main text; results are presented qualitatively. 4) Some simplified theoretical guarantees rely on strong assumptions (ϵ-Bandit SMDP, state-independent policies, DR-submodularity).",
            "uuid": "e1134.0",
            "source_info": {
                "paper_title": "Submodular Reinforcement Learning",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Bayesian D-expt-design (SUBPO)",
            "name_full": "Bayesian D-optimal Experimental Design using SUBPO",
            "brief_description": "Application of SUBPO to sequential experimental design: choose trajectory of sampling locations to maximize mutual information I(y_τ; f) about an unknown function f modelled by a Gaussian Process (GP), using marginal-information weighted policy gradients.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SUBPO-M and SUBPO-NM",
            "agent_description": "SUBPO agent (policy-gradient neural policy) that treats the mutual information objective F(τ) = I(y_τ; f) as a monotone submodular reward; variants include Markovian (maps state to action) and non-Markovian (maps history to action) policies. Baselines estimate cumulative marginal gains for variance reduction.",
            "adaptive_design_method": "Information gain maximization (mutual information between unknown function f and noisy observations y_τ), applied as a submodular objective; operationalized via marginal-gain weighted policy gradient.",
            "adaptation_strategy_description": "Agent selects next sampling locations by maximizing expected marginal information gain F(s | past τ) in the policy-gradient objective; uses sampled trajectories to estimate gradients and updates policy to prefer actions with higher expected future mutual information.",
            "environment_name": "Bayesian experimental design (GP-sampled functions)",
            "environment_characteristics": "Unknown target function f with GP prior; noisy observations y = f + ε; environment is stochastic in observations but deterministic in dynamics (sampling locations); partial information about f only via samples (classic partially observable experimental-design problem).",
            "environment_complexity": "Authors used 10 different GP-sampled environments, 20 runs each. Domain continuous; details: experiments performed on discretized sensing patches for measurement locations; training budgets unspecified beyond epoch counts (grid tasks: 150 epochs typical).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: SUBPO-M matches SUBPO-NM performance while being more sample efficient; both outperform MODPO which gets stuck in high-uncertainty regions. No numeric mutual-information or sample-efficiency numbers provided in main text (plots show normalized J(π) improvements).",
            "performance_without_adaptation": "MODPO baseline (maximizes per-state modular reward F({s})) obtains lower trajectory mutual information and tends to get stuck sampling high-uncertainty points repeatedly; plots indicate substantially worse normalized J(π) than SUBPO variants (no numeric values).",
            "sample_efficiency": "Reported that SUBPO-M is very sample efficient relative to SUBPO-NM due to smaller policy class; experiments: 10 envs × 20 runs each to compute confidence; discrete grid settings converged within ~150 epochs in comparable coverage tasks.",
            "exploration_exploitation_tradeoff": "Exploration is enforced by entropy regularization in policy updates; exploitation occurs via marginal-information weighting in gradient updates that prioritizes high expected information gain trajectories.",
            "comparison_methods": "MODPO (modular reward PG), SUBPO-NM (history-aware SUBPO) as an oracle-style comparison.",
            "key_results": "SUBPO (particularly SUBPO-M) is effective for Bayesian sequential experimental design with GP models: it achieves high mutual information trajectories and outperforms a modular-reward PG baseline, while being more sample efficient than fully history-conditioned policies in these experiments.",
            "limitations_or_failures": "No quantitative regret or sample-complexity bounds provided; results are empirical and qualitative. Performance depends on GP prior and environment sampling; no numerical comparisons of mutual information values reported in text.",
            "uuid": "e1134.1",
            "source_info": {
                "paper_title": "Submodular Reinforcement Learning",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Informative Path Planning (SUBPO)",
            "name_full": "Informative Path Planning for biodiversity monitoring using SUBPO",
            "brief_description": "Application where a quadrotor plans trajectories to maximize coverage-weighted information (coverage function g(∪_s D_s) with density ρ fit from gorilla nest counts or GP samples); SUBPO maximizes marginal coverage gains to avoid locally greedy traps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SUBPO-M and SUBPO-NM (quadrotor agent)",
            "agent_description": "Quadrotor controlled by a stochastic policy (MLP) mapping state (or history for NM) to discrete direction actions (5 actions). SUBPO uses marginal coverage gains to guide updates; baseline for variance reduction included.",
            "adaptive_design_method": "Coverage-driven adaptive exploration (submodular weighted coverage objective), implemented as greedy marginal-gain maximization within policy gradient updates.",
            "adaptation_strategy_description": "Agent computes marginal coverage gain of visiting candidate states given already visited sensing footprints and updates policy to favor actions that increase expected uncovered value; uses sampled trajectories to estimate gradients and adapts over epochs.",
            "environment_name": "Informative path planning (gorilla nest density / GP-sampled multimodal density on a 30×30 grid)",
            "environment_characteristics": "Unknown nest density (fitted smooth rate or GP samples), observations when visiting states give local coverage; deterministic quadrotor dynamics in experiments; discrete 30×30 grid, horizon H=40; partial information about domain because unvisited locations' densities are unknown.",
            "environment_complexity": "Grid 30×30 (900 states), actions 5 (directions), horizon 40; batch B=500, 150 epochs for convergence typically; experiments included 10 random environments × 20 runs.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SUBPO-M achieves performance as good as SUBPO-NM and substantially better than MODPO (which gets stuck repeatedly exploiting a high-density region). Performance plotted as normalized J(π); convergence within ~150 epochs in grid experiments. No absolute numeric reward values reported.",
            "performance_without_adaptation": "MODPO baseline repeatedly exploits local high-density areas and attains markedly lower normalized J(π) (plots indicate SUBPO higher), with MODPO failing to improve trajectory-level coverage.",
            "sample_efficiency": "SUBPO-M more sample efficient than SUBPO-NM due to smaller search space; typical convergence within ~150 epochs (B=500) and runtime &lt;20 minutes for 150 epochs on 2 CPU cores.",
            "exploration_exploitation_tradeoff": "Entropy regularization used to encourage diverse trajectory samples; marginal-gain weighting steers exploitation toward actions that increase uncovered coverage.",
            "comparison_methods": "MODPO (modular reward PG), SUBPO-NM (history-aware) compared experimentally.",
            "key_results": "Greedy marginal-gain PG (SUBPO) avoids local traps of modular reward optimization and yields substantially better coverage; Markovian SUBPO often suffices in these informative path planning tasks and is sample efficient.",
            "limitations_or_failures": "When density landscape or dynamics require history-awareness, SUBPO-M may underperform SUBPO-NM; no numeric confidence intervals in main text beyond qualitative plots.",
            "uuid": "e1134.2",
            "source_info": {
                "paper_title": "Submodular Reinforcement Learning",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Item Collection (SUBPO)",
            "name_full": "Item-collection / covering groups task using SUBPO",
            "brief_description": "Grid-world task where items are grouped and reward is sum_{i} min(|τ ∩ g_i|, d_i) (submodular covering); agent must collect d_i items per group under stochastic transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SUBPO-NM and SUBPO-M",
            "agent_description": "Policy-gradient agents parameterized by neural networks: SUBPO-NM conditions on history (so can track collected items), SUBPO-M is Markovian (state only). SUBPO uses marginal gain weighting to prefer actions leading to uncovered groups/items.",
            "adaptive_design_method": "Adaptive covering strategy via submodular marginal-gain maximization (greedy marginal gains used to prioritize collecting new group items rather than revisits).",
            "adaptation_strategy_description": "Agent adapts by preferring actions whose expected future marginal contribution to covering groups (∆(v | visited set)) is highest, using trajectory rollouts to estimate these marginals and update policy.",
            "environment_name": "Item collection (grid groups with stochastic dynamics)",
            "environment_characteristics": "Discrete grid with groups of items (G groups), stochastic actions (0.9 executed, 0.1 random); episodic horizon sufficient to pick items; agent must track collected items to plan effectively (partial observability of history if policy is Markovian).",
            "environment_complexity": "Experiments run on 10 random environments with 20 runs each; grid size and horizon similar to other discrete tasks (30×30, H=40 in F.1 context).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SUBPO-NM (history-aware) achieves best performance because it can track collected items; SUBPO-M attains slightly lower but comparable performance by maximizing marginal gains. Performance reported qualitatively in plots (normalized J(π)).",
            "performance_without_adaptation": "MODPO (modular baseline) fails to adequately plan for future collection needs and attains worse performance (qualitative).",
            "sample_efficiency": "SUBPO-NM requires more samples due to larger policy class (history dependence); SUBPO-M is more sample efficient but slightly suboptimal compared to SUBPO-NM. Exact epoch counts per environment not enumerated in main text beyond aggregate experimental budgets.",
            "exploration_exploitation_tradeoff": "Marginal-gain weighting promotes exploration of groups not yet satisfied (to maximize marginal returns) while entropy regularization prevents premature convergence; history-aware policies exploit past collected information to avoid revisits.",
            "comparison_methods": "MODPO (modular reward PG), SUBPO-M vs SUBPO-NM.",
            "key_results": "History-aware SUBPO-NM outperforms Markovian SUBPO-M in tasks that require explicit tracking of past picks, demonstrating the need for history-dependent policies in some adaptive experimental-design-like tasks; however SUBPO-M still attains reasonable performance by greedily maximizing marginals.",
            "limitations_or_failures": "Markovian parametrization may be insufficient when the agent must remember past picks; SUBPO-M slightly underperforms SUBPO-NM in such partially observable experimental-design tasks.",
            "uuid": "e1134.3",
            "source_info": {
                "paper_title": "Submodular Reinforcement Learning",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Building exploration (SUBPO)",
            "name_full": "Building exploration / coverage with SUBPO",
            "brief_description": "Two-room exploration task where agent must visit both rooms to maximize coverage |∪_s D_s|; demonstrates cases where Markovian policies are insufficient and history-aware policies are required.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SUBPO-M and SUBPO-NM",
            "agent_description": "Stochastic policy (MLP for Markovian, history-conditional NN for NM) controlling an agent with deterministic dynamics on a small grid / topological environment; SUBPO updates policy via marginal gain gradient estimator.",
            "adaptive_design_method": "Coverage-maximizing adaptive exploration (submodular coverage objective), realized via greedy marginal-gain policy updates.",
            "adaptation_strategy_description": "Agent selects actions that maximize expected marginal increase in covered area given currently visited sensing patches; history-aware policy conditions on visited patches enabling planning to visit both rooms.",
            "environment_name": "Building exploration (two rooms connected by corridor)",
            "environment_characteristics": "Deterministic transitions; partial observability arises because visiting one room reduces marginal returns in nearby states; horizon just enough to cover both rooms; optimal policy is deterministic Markovian in principle but Monte Carlo learning with Markovian policies is challenging.",
            "environment_complexity": "Small toy environment; horizon set to be just enough to cover both rooms. Exact grid size not specified, but designed to require exploring both branches of corridor.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SUBPO-NM learns to explore both rooms (tracks history) and attains better coverage; SUBPO-M achieves a sub-optimal solution that mostly explores one side (qualitative result depicted in Fig. 5a). No numeric metrics provided.",
            "performance_without_adaptation": "MODPO baseline not specifically reported for this environment in main text; SUBPO-M (Markovian) effectively behaves like a weaker adaptive method and underperforms SUBPO-NM.",
            "sample_efficiency": "SUBPO-NM requires more samples than SUBPO-M due to larger policy class; exact epoch counts for this environment not enumerated in main text.",
            "exploration_exploitation_tradeoff": "History-conditioned policies exploit knowledge of visited rooms to direct exploration to unvisited room; entropy regularization can be used but Markovian policies tend to converge prematurely to local exploration.",
            "comparison_methods": "SUBPO-M vs SUBPO-NM (main comparison); modular baseline discussed generally but not emphasized for this small environment.",
            "key_results": "Demonstrates a failure-mode of Markovian adaptive methods: even when a deterministic Markovian optimal policy exists, Monte Carlo learning with Markovian function approximators can fail to discover it; history-aware SUBPO-NM remedies this by conditioning on visited set.",
            "limitations_or_failures": "SUBPO-M can converge to sub-optimal single-room exploration; need for memory/augmented state or non-Markovian policies for some adaptive experimental-design tasks.",
            "uuid": "e1134.4",
            "source_info": {
                "paper_title": "Submodular Reinforcement Learning",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies",
            "rating": 2,
            "sanitized_title": "nearoptimal_sensor_placements_in_gaussian_processes_theory_efficient_algorithms_and_empirical_studies"
        },
        {
            "paper_title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
            "rating": 2,
            "sanitized_title": "adaptive_submodularity_theory_and_applications_in_active_learning_and_stochastic_optimization"
        },
        {
            "paper_title": "Active exploration via experiment design in markov chains",
            "rating": 2,
            "sanitized_title": "active_exploration_via_experiment_design_in_markov_chains"
        },
        {
            "paper_title": "Linear submodular bandits and their application to diversified retrieval",
            "rating": 1,
            "sanitized_title": "linear_submodular_bandits_and_their_application_to_diversified_retrieval"
        },
        {
            "paper_title": "An online algorithm for maximizing submodular functions",
            "rating": 1,
            "sanitized_title": "an_online_algorithm_for_maximizing_submodular_functions"
        }
    ],
    "cost": 0.01801225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SUBMODULAR REINFORCEMENT LEARNING
24 May 2024</p>
<p>Manish Prajapat 
ETH Zurich</p>
<p>Eth Zurich 
ETH Zurich</p>
<p>Mojmír Mutný 
ETH Zurich</p>
<p>Melanie N Zeilinger 
ETH Zurich</p>
<p>Andreas Krause 
ETH Zurich</p>
<p>SUBMODULAR REINFORCEMENT LEARNING
24 May 202449F38ED79F0BF4508560919BDD669284arXiv:2307.13372v2[cs.LG]
In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are independent of states visited previously.In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously.To tackle this, we propose submodular RL (SUBRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns.Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate.On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SUBPO, a simple policy gradient-based algorithm for SUBRL that handles non-additive rewards by greedily maximizing marginal gains.Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SUBPO recovers optimal constant factor approximations of submodular bandits.Moreover, we derive a natural policy gradient approach for locally optimizing SUBRL instances even in large state-and action-spaces.We showcase the versatility of our approach by applying SUBPO to several applications such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization.Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.</p>
<p>INTRODUCTION</p>
<p>In reinforcement learning (RL), the agent aims to learn a policy by interacting with its environment in order to maximize rewards obtained.Typically, in RL, the environments are modelled as a (controllable) Markov chain, and the rewards are considered additive and independent of the trajectory.In this well-understood setting, referred to as Markov Decision Processes (MDP), the Bellman optimality principle allows to find an optimal policy in polynomial time for finite Markov chains (Puterman, 1994;Sutton &amp; Barto, 2018).However, many interesting problems cannot straightforwardly be modelled via additive rewards.In this paper, we consider a rich and fundamental class of non-additive rewards, in particular submodular reward functions.Applications for planning under submodular rewards abound, from coverage control (Prajapat et al., 2022), entropy maximization, experiment design (Krause et al., 2008), informative path planning, orienteering problem Gunawan et al. (2016), resource allocation to Mapping (Kegeleirs et al., 2021).Submodular functions capture an intuitive diminishing returns property that naturally arises in these applications: the reward obtained by visiting a state decreases in light of similar states visited previously.E.g., in a biodiversity monitoring application (see Fig. 1), if the agent has covered a particular region, the information gathered from neighbouring regions becomes redundant and tends to diminish.To tackle such history-dependent, non-Markovian rewards, one could naively augment the state to include all the past states visited so far.This approach, however, exponentially increases the state-space size, leading to intractability.In this paper, we make the following contributions:</p>
<p>First, we introduce SUBRL, a paradigm for reinforcement learning with submodular reward functions.While this is the first work to consider submodular objectives in RL, we connect it to related areas such as submodular optimization, convex RL in Section 6.To establish limits of the SUBRL framework, we derive a lower bound that establishes hardness of approximation up to log factors (i.e., ruling out any constant factor approximation) in general (Section 3).Second, despite the hardness, we Figure 1: In Fig. 1a, for monitoring biodiversity, a drone needs to plan a path with maximum coverage over critical areas, represented by lighter regions in a heatmap.Here, the additional information (coverage) provided by visiting a location (state) depends on which states have been visited before.We therefore must visit diverse locations that maximize coverage of important regions.In Fig. 1b, the environment contains a group of items (g i ) placed on a grid.The agent must find a trajectory (τ ) that picks a fixed number of items d i from each group g i , i.e., max τ i min(|τ ∩ g i |, d i ).If the agent picks more than d i , it is not rewarded -diminishing gain.Both of these tasks cannot be represented with additive rewards (in terms of locations) and serve as illustrative examples for this work.</p>
<p>show that, in many important cases, SUBRL instances can often be effectively solved in practice.In particular, we propose an algorithm, SUBPO, motivated by the greedy algorithm in classic submodular optimization.It is a simple policy-gradient-based algorithm for SUBRL that handles non-additive rewards by greedily maximizing marginal gains (Section 4).Third, we show that in some restricted settings, SUBPO performs provably well.In particular, under specific assumptions on the underlying MDP, SUBRL reduces to an instance of constrained continuous DR-submodular optimization over the policy space.Even though the reduced problem is still NP-hard, we guarantee convergence to the information-theoretically optimal constant factor approximation of 1 − 1/e, generalizing previous results on submodular bandits.Moreover, for general MDPS, if the submodular function has bounded curvature, we show SUBPO achieves a constant factor approximation. (Section 5).Lastly, we demonstrate the practical utility of SUBPO in simulation as well as real-world applications.Namely, we showcase its use in biodiversity monitoring, Bayesian experiment design, informative path planning, building exploration, car racing and Mujoco robotics tasks.Our algorithm is sample efficient, discovers effective strategies, and scales well to high dimensional spaces (Section 7).</p>
<p>SUBMODULAR RL: PRELIMINARIES AND PROBLEM STATEMENT</p>
<p>Submodularity.Let V be a ground set.A set function F : 2 V → R is submodular if ∀A ⊆ B ⊆ V, v ∈ V\B, we have, F (A ∪ {v}) − F (A) ≥ F (B ∪ {v}) − F (B).The property captures a notion of diminishing returns, i.e., adding an element v to A will help at least as much as adding it to the superset B. We denote the marginal gain of element v as ∆(v|A) := F (A ∪ {v}) − F (A). Functions for which ∆(v|A) is independent of A are called modular.F is said to be monotone if ∀A ⊆ B ⊆ V, we have, F (A) ≤ F (B) (or, equiv., ∆(v | A) ≥ 0 for all v, A).</p>
<p>Controlled Markov Process (CMP).</p>
<p>A CMP is a tuple formed by ⟨V, A, P, ρ, H⟩, where V is the ground set, v ∈ V is a state, A is the action space with a ∈ A. ρ denotes the initial state distribution and P = {P h } H−1 h=0 , where P h (v ′ |v, a) is the distribution of successive state v ′ after taking action a at state v at horizon h.We consider an episodic setting with finite horizon H. Submodular MDP.We define a submodular MDP, or SMDP, as a CMP with a monotone submodular reward function, i.e., a tuple formed by ⟨S, A, P, ρ, H, F ⟩. Hereby, using S = H × V to designate time-augmented states (i.e., each s = (h, v) ∈ S is a state augmented with the current horizon step), we assume F : 2 S → R 1 is a monotone submodular reward function.The non-stationary transition distribution P h (v ′ |v, a) of the CMP can be equivalently converted to P ((h + 1, v ′ )|(h, v), a) = P (s ′ |s, a) since s accounts for time.An episode starts at s 0 ∼ ρ, and at each time step h ≥ 0 at state s h , the agent draws its action a h according to a policy π (see below).The environment evolves to a new state s h+1 following the CMP.A realization of this stochastic process is a trajectory τ = (s h , a h ) H−1 h=0 , s H , an ordered sequence with a fixed horizon H. τ l:l ′ = (s h , a h ) l ′ −1 h=l , s l ′ denotes the part from time step l to l ′ .Note that τ = τ 0:H .For each (partial) trajectory τ l:l ′ , we use the notation F (τ l:l ′ ) to refer to the objective F evaluated on the set of (state,time)-pairs visited by τ l:l ′ .</p>
<p>Policies.The agent acts in the SMDP according to a policy, which in general maps histories τ 0:h to (distributions over) actions a h .A policy π(a h |τ 0:h ) is called Markovian if its actions only depend on the current state s, i.e., π(a h |τ 0:h ) = π(a h |s h ).The set of all Markovian policies is denoted by Π M .Similarly, we can consider Non-Markovian policies π(a h |τ h−k:h ) that only depend on the previous past k steps τ h−k:h .The set of all Non-Markovian policies conditioned on history up to past k steps is denoted by Π k NM , and we use Π NM := Π H NM to allow arbitrary history-dependence.Problem Statement.For a given submodular MDP, we want to find a policy to maximize its expected reward.For a given policy π, let f (τ ; π) denote the probability distribution over the random trajectory τ following agent's policy π,
f (τ ; π) = ρ(s 0 ) H−1 h=0 π(a h |τ 0:h )P (s h+1 |s h , a h ).
(1)</p>
<p>The performance measure J(π) is defined as the expectation of the submodular set function over the trajectory distribution induced by the policy π and the goal is to find a policy that maximizes the performance measure within a given family of policies Π (e.g., Markovian ones).Precisely,
π ⋆ = arg max π∈Π J(π), where J(π) = τ f (τ ; π)F (τ ).(2)
Given the non-additive reward function F , the optimal policy on Markovian and non-Markovian policy classes can differ (since the reward depends on the history, the policy may need to take the history into account for taking optimal actions).In general, even representing arbitrary non-Markovian policies is intractable, as its description size would grow exponentially with the horizon.It is easy to see that for deterministic MDPs, the optimal policy is indeed attained by a deterministic Markovian policy:</p>
<p>Proposition 1.For any deterministic MDP with a fixed initial state, the optimal Markovian policy achieves the same value as the optimal non-Markovian policy.</p>
<p>The following proposition guarantees that, even for stochastic transitions, there always exists an optimal deterministic policy among the family of Markovian policies Π M .Thus, we do not incur a loss compared to stochastic policies.</p>
<p>Proposition 2. For any set function F , among the Markovian policies Π M , there exists an optimal policy that is deterministic.The proof is in Appendix A. The result extends to any non-Markovian policy class Π k NM for as well, since one can group together the past k states and treat it as high-dimensional Markovian state space.</p>
<p>Examples of submodular rewards.We first observe that classical MDPs are a strict special case of submodular MDPs.Indeed, for some classical reward function r :
V → R + , by setting F ((h 1 , v 1 ), . . . , (h k , v k )) := k l=1 γ h l r(v l ), F(
τ ) simply recovers the (discounted) sum of rewards of the states visited by trajectory τ (hereby, γ ∈ [0, 1], i.e., use γ = 1 for the undiscounted setting).</p>
<p>A generic way to construct submodular rewards is to take a submodular set function F ′ : 2 V → R defined on the ground set V, and define F (τ ) := F ′ (Tτ ), using an operator T : 2 H×V → 2 V that drops the time indices.Thus, F (τ ) measures the value of the set of states Tτ ⊆ V visited by τ .Note that each state is counted only once, i.e., even if F ′ is a modular function, F exhibits diminishing returns.There are many practical examples of F ′ , such as coverage functions, experimental design criteria such as mutual information and others that have found applications in machine learning tasks (cf.Krause &amp; Golovin, 2014;Bilmes, 2022).Moreover, many operations preserve submodularity, which can be exploited to build complex submodular objectives from simpler ones (Krause &amp; Golovin, 2014, section 1.2) and can potentially be used in reward shaping.While submodularity is often considered for discrete V, the concept naturally generalizes to continuous domains.</p>
<p>SUBMODULAR RL: THEORETICAL LIMITS</p>
<p>We first show that the SUBRL problem is hard to approximate in general.In particular, we establish a lower bound that implies SUBRL cannot be approximated up to any constant factor in polynomial time, even for deterministic submodular MDPs.We prove this by reducing our problem to a known hard-to-approximate problem -the submodular orienteering problem (SOP) (Chekuri &amp; Pal, 2005).Since we focus on deterministic SMDP's, according to Proposition 1 and Proposition 2, it suffices to consider deterministic, Markovian policies.We now formally state the inapproximability result, Theorem 1.Let OPT be the optimal value and γ &gt; 0.Even for deterministic SMDP's, the SUBRL problem is hard to approximate within a factor of Ω(log 1−γ OPT) unless NP ⊆ ZTIME(n polylog(n) ).</p>
<p>Thus, under common assumptions in complexity theory (Chekuri &amp; Pal, 2005;Halperin &amp; Krauthgamer, 2003), the SUBRL problem cannot be approximated in general to better than logarithmic factors, i.e., no algorithm can guarantee J(π) ≥ OPT log 1−γ OPT for all input instances of SUBRL.The proof is in Appendix B. The significance of this result extends beyond submodular RL.As SUBRL falls within the broader category of general non-Markovian reward functions, Theorem 1 implies that problems involving general set functions are similarly inapproximable, limited to logarithmic factors.</p>
<p>Since our inapproximability result is worst-case in nature, it does not rule out that interesting SUBRL problems remain practically solvable.In the next section, we introduce a general algorithm that is efficiently implementable, recovers constant factor approximation under assumptions (Section 5) and is empirically effective as shown in an extensive experimental study (Section 7).</p>
<p>GENERAL ALGORITHM: SUBMODULAR POLICY OPTIMIZATION SUBPO</p>
<p>We now propose a practical algorithm for SUBRL that can efficiently handle submodular rewards.The core of our approach follows a greedy gradient update on the policy π.As common in the modern RL literature, we make use of approximation techniques for the policies to derive a method applicable to large state-action spaces.This means the policy π θ (a|s)2 is parameterized by θ ∈ Θ where Θ ⊂ R l is compact.In the case of tabular π, θ specifies an independent distribution over actions for each state.</p>
<p>Approach.The objective from Eq. ( 2) can be equivalently formulated as θ ⋆ ∈ arg max θ∈Θ J(π θ ) as θ indexes our policy class.Due to the nonlinearity of the parameterization, it is often not feasible to find a global optimum for the above problem.In practice, with appropriate initialization and hyperparameters, variants of gradient descent are known to perform well empirically for MDPs.Precisely,
θ ← θ + arg max δθ:δθ+θ∈Θ δθ ⊤ ∇ θ J(π θ ) − 1 2α ∥δθ∥ 2 .(3)
Various PG methods arise with different methods for gradient estimation and applying regularization (Kakade, 2001;Schulman et al., 2015;2017).The key challenge to all of them is computation of the gradient ∇J(π θ ).Below, we devise an unbiased gradient estimator for general non-additive functions.</p>
<p>Gradient Estimator.As common in the policy gradient (PG) literature, we can use the score function g(τ, π θ ) := ∇ θ (log
H−1 i=0 π θ (a i |s i ))
to calculate the gradient ∇ θ J. Namely, Given an MDP and the policy parameters θ,
∇ θ J(π θ ) = τ f (τ ; π θ )g(τ, π θ )F (τ ).(4)
As Eq. ( 4) shows, we do not require knowledge of the environment if sampled trajectories are available.It also does not require full observability of the states nor any structural assumption on the MDP.On the other hand, the score gradients suffer from high variance due to sparsity induced by trajectory rewards (Fu, 2006;Prajapat et al., 2021;Sutton &amp; Barto, 2018).Hence, we take the SMDP structure into account to develop efficient algorithms.</p>
<p>Marginal gain: We define the marginal gain for a state s in the trajectory τ 0:j up to horizon j as F (s|τ 0:j ) = F (τ 0:j ∪ {s}) − F (τ 0:j ).Our approach aims to maximize the marginal gain associated with each action instead of maximizing state rewards.This approach shares similarities with the greedy algorithm commonly used in submodular maximization, which maximizes marginal gains and is known for its effectiveness.Moreover, decomposing the trajectory return into marginal gains and incorporating it in the policy gradient with suitable baselines Greensmith et al. (2004) removes sparsity and thus helps to reduce variance.Inspired by the policy gradient method for additive rewards (Sutton et al., 1999;Baxter &amp; Bartlett, 2001), we propose the following for SMDP:</p>
<p>Theorem 2. Given an SMDP and the policy parameters θ, with any set function F , Estimate ∇ θ J(π θ ) as per Theorem 2 8:
∇ θ J(π θ ) = E τ ∼f (τ ;π θ )   H−1 i=0 ∇ θ log π θ (a i |s i )   H−1 j=i F (s j+1 |τ 0:j ) − b(τ 0:i )     (5)
Update policy parameters (θ) using Eq. ( 3)
Transition probability a 0 a 0 ϵ 1 − ϵ a 0 a 0 a 1 v 1 a 1 a 0 a 1 v 1 v 1 v 0 v 0 v 0 a 0 a 1 a 0 a 0 a 1 h=0 h=2 h=1 a 1 Figure 2: Transitions in ϵ-Bandit MDP
We use an importance sampling estimator (log trick) to obtain Eq. (4).To reduce variance, we subtract a baseline b(τ 0:i ) from the score gradient, which can be a function of the past trajectory τ 0:j .This incorporates the causality property in the estimator, ensuring that the action at timestep j cannot affect previously observed states.After simplifying and considering marginals, we obtain Theorem 2 (proof is in Appendix D).This estimator assigns a higher weight to policies with high marginal gains and a lower weight to policies with low marginal gains.Empirically this performs very well (Section 7).</p>
<p>We can optimize this approach by using an appropriate baseline as a function of the history τ 0:j , which leads to an actor-critic type method.The versatility of the approach is demonstrated by the fact that Theorem 2 holds for any choice of baseline critic.We explain later in the experiments how to choose a baseline.One can perform a Monte Carlo estimate (Baird, 1993) or generalized advantage function (GAE) (Schulman et al., 2016) to estimate returns based on the marginal gain.To encourage exploration, similar to standard PG, we can employ a soft policy update based on entropy penalization, resulting in diverse trajectory samples.Entropy penalization in SUBRL can be thought of as the sum of modular and submodular rewards, which is a submodular function.</p>
<p>Algorithm.The outline of the steps is given in Algorithm 1.We represent the agent by a stochastic policy parameterized by a neural network.The algorithm operates in epochs and assumes a way to generate samples from the environment, e.g., via a simulator.In each epoch, the agent recursively samples actions from its stochastic policy and applies them in the environment leading to a roll out of the trajectory where it collects samples (Line 6).We execute multiple (B) batches in each epoch for accurate gradient estimation.To update the policy, we compute the estimator of the policy gradient as per Theorem 2, where we utilize marginal gains of the trajectory instead of immediate rewards as in standard RL (Line 7).Finally, we use stochastic gradient ascent to update the policy parameters.</p>
<p>PROVABLE GUARANTEES IN SIMPLIFIED SETTINGS</p>
<p>In general, Section 3 shows SUBRL is NP-hard to approximate.A natural question is: Is it possible to do better under additional structural assumptions?In this section, we present two interesting cases under which SUBPO can approximate objective (2) up to a constant factor.Firstly, under assumptions on the underlying MDP we show its equivalence to DR-submodular optimization on a convex polytope.Secondly, for general MDPS, we capture the deviation of submodular rewards from the modular function using the notion of curvature and present a constant factor approximation result for it.</p>
<p>Definition 1 (DR submodularity and DR-property, Bian et al. (2017a)).
For X ⊆ R d , a function f : X → R is DR-submodular (has the DR property) if ∀a ≤ b ∈ X , ∀i ∈ [d], ∀k ∈ R + s.t. (ke i + a) and (ke i + b) are still in X , it holds, f (ke i + a) − f (a) ≥ f (ke i + b) − f (b). (Notation: e i denotes i th basis vector and for any two vectors a, b, a ≤ b means a i ≤ b i ∀i ∈ [d])
Monotone DR-submodular functions form a family of generally non-convex functions, which can be approximately optimized.As discussed below, gradient-based algorithms find constant factor approximations over general convex, downward-closed polytopes.</p>
<p>Under the following condition on the Markov chain, we can show that as long as the policy is parametrized in a particular way, the objective is indeed monotone DR-submodular.
Definition 2 (ϵ-Bandit SMDP). An SMDP s.t. for any v j , v k ∈ V, j ̸ = k, ∀h ∈ [H] and ∀v ′ ∈ V, P h (v j |v ′ , a j ) = 1 − ϵ h , and P h (v k |v ′ , a j ) = ϵ h |V|−1 for ϵ h ∈ 0, |V| |V|+1 is an ϵ-Bandit SMDP.
This represents a "nearly deterministic" MDP where there is a unique action for each state in the MDP, which takes us to it with 1 − ϵ probability and with the rest, we end up in any other state Fig. 2. While limiting, it generalizes the bandit scenario, which would occur when ϵ = 0.In the following, we consider a class of state-independent policies that can change in each horizon, denoting the horizon dependence with π h (a).We now formally establish the connection between SUBRL and DR-submodularity, Theorem 3.For horizon dependent policy π parameterized as π h (a)∀h ∈ [H] in an ϵ-Bandit SMDP, and F (τ ) is a monotone submodular function, then J(π) is monotone DR-submodular.The proof is in Appendix E. It builds on two steps; firstly, we use a reparameterization trick to handle policy simplex constraints.We relax the equality constraints on π to lie on a convex polytope
P = {π h (a) | 0 ≤ π h (a) ≤ 1, 0 ≤ j,j̸ =k π h (a j ) ≤ 1, ∀k ∈ [|A|], ∀h ∈ [H]
} and enforce the equality constraints directly in the objective Eq. ( 2).Secondly, under the assumptions of Theorem 3, we show that the Hessian of J(π) only has non-positive entries, which is an equivalent characterization of twice differentiable DR-submodular functions.Furthermore, the result can be generalized to accommodate state and action spaces that vary with horizons, although, for simplicity, we assumed fixed spaces.</p>
<p>The convex polytope P belongs to a class of down-closed convex constraints.Bian et al. (2017c) proposes a modified FRANK-WOLFE algorithm for DR-submodular maximization with down-closed constraints.This variant can achieve an (1 − 1/e) approximation guarantee and has a sub-linear convergence rate.The algorithm proceeds as follows: the gradient oracle is the same as Theorem 2, while employing a tabular policy parameterization.The polytopic constraints P are ensured through a FRANK-WOLFE step, which involves solving a linear program over the policy domain.Finally, the policy is updated with a specific step size defined in (Bian et al., 2017c).Furthermore, Hassani et al. (2017) shows that any stationary point in the optimization landscape of DR-submodular maximization under general convex constraints is guaranteed to be 1/2 optimal.Therefore, any gradient-based optimizer can be used for the ϵ-Bandit SMDP, and will result in an 1/2-optimal policy.In Section 6, we elaborate on how this setting generalizes previous works on submodular bandits.</p>
<p>General SMDP.While we cannot obtain a provable result for general SMDP's (Theorem 1), we can, interestingly, quantify the deviation of submodular function from a modular function using the notion of curvature (Conforti &amp; Cornuéjols, 1984).The curvature reflects how much the marginal values ∆(v|A) can decrease as a function A. The total curvature of F is defined as, c = 1 − min A,j / ∈A ∆(j|A)/F (j).Note that c ∈ [0, 1], and if c = 0 then the marginal gain is independent of A (i.e., F is modular).</p>
<p>Proposition 3. Consider a tabular SMDP, s.t. the reward function F is monotone submodular with bounded curvature c ∈ (0, 1).Then, for the policy π (with tabular parametrization) obtained via SUBPO, it holds that J(π) ≥ (1 − c)J(π ⋆ ), where π ⋆ is an optimal non-Markovian policy.Thus, under assumptions of bounded curvature, c ∈ (0, 1), we can guarantee constant factor optimality for SUBPO (proof in Appendix E.2). Vondrak (2010) establishes a curvature-based hardness result for the simpler problem of submodular set function maximization under cardinality constraints, implying that 1 − c is a near-optimal approximation ratio.Moreover, if c = 0, i.e., F denotes modular rewards, the SUBPO algorithm reduces to standard PG, and hence recovers the guarantees and benefits of the modular PG.In particular, with tabular policy parameterization, under mild regularity assumptions, any stationary point of the modular PG cost function is a global optimum (Bhandari &amp; Russo, 2019).</p>
<p>RELATED WORK</p>
<p>Beyond Markovian RL.Several prior works in RL identify the deficiency in the modelling ability of classical Markovian rewards.This manifests itself especially when exploration is desired, e.g., when the transition dynamics are not completely known (Tarbouriech et al., 2020;Hazan et al., 2019) or when the reward is not completely known Lindner et al. (2021); Belogolovsky et al. (2021).Chatterji et al. (2021) considers binary feedback drawn from the logistic model at the end episode and explores state representation, such as additivity, to propose an algorithm capable of learning non-Markovian policies.While all these address in some aspect the shortcomings of Markovian rewards, they tend to focus on a specific aspect instead of postulating a new class of reward functions as we do in this work.</p>
<p>Convex RL.Convex RL also seeks to optimize a family of non-additive rewards.The goal is to find a policy that optimizes a convex function over the state visitation distribution (which averages over the randomness in the MDP and the policies actions).This framework has applications, e.g., in exploration and experimental design (Hazan et al., 2019;Zahavy et al., 2021;Duff, 2002;Tarbouriech et al., 2020;Mutny et al., 2023).While sharing some motivating applications, convex and submodular RL are rather different in nature.Beyond the high-level distinction that convex and submodular function classes are complimentary, our non-additive (submodular) rewards are defined over the actual sequence of states visited by the policy, not its average behaviour.Mutti et al. (2022) points out that this results in substantial differences, noting the deficiency of convex RL in modelling expected utilities of the form as in Eq. ( 2), which we address in our work.We observe that MODPO get stuck by repeatedly maximizing its modular reward, whereas SUBPO-M achieves comparable performance to SUBPO-NM while being more sample efficient.(Y-axis: normalized J(π), X-axis: epochs) Submodular Maximization.Submodular functions are widely studied in combinatorial optimization and operations research and have found many applications in machine learning (Krause &amp; Golovin, 2014;Bilmes, 2022;Tohidi et al., 2020).The seminal work of Nemhauser et al. (1978) shows that greedy algorithms enjoy a constant factor 1 − 1/e approximation for maximizing monotone submodular functions under cardinality constraints, which is information-and complexity-theoretically optimal (Feige, 1998).Beyond simpler cardinality (and matroid) constraints, more complex constraints have been considered: most relevant is the s-t-submodular orienteering problem, which aims to find an s-t-path in a graph of bounded length maximizing a submodular function of the visited nodes (Chekuri &amp; Pal, 2005), and can be viewed as a special case of SUBRL on deterministic SMDP's with deterministic starting state and hard constraint on the goal state.It has been used as an abstraction for informative path planning (Singh et al., 2009).We generalize the setup and connect it with modern policy gradient techniques.Wang et al. ( 2020) considers planning under the surrogate multi-linear extension of submodular objectives.Certain problems considered in our work satisfy a notion called adaptive submodularity, which generalizes the greedy approximation guarantee over a set of policies (Golovin &amp; Krause, 2011).While adaptive submodularity allows capturing history-dependence, it fails to address complex constraints (such as those imposed by CMP's).</p>
<p>While submodularity is typically considered for discrete domains (i.e., for functions defined on {0, 1} |V| , the concept can be generalized to continuous domains, e.g., [0, 1] |V| using notions such as DR-submodularity (Bian et al., 2017b).This notion forms a class of non-convex problems admitting provable approximation guarantees in polynomial time, which we exploit in Section 5.The problem of learning submodular functions has also been considered (Balcan &amp; Harvey, 2011).Dolhansky &amp; Bilmes (2016) introduce the class of deep submodular functions, neural network models guaranteed to yield functions that are submodular in their input.These may be relevant when learning unknown rewards using function approximation, which is an interesting direction for future work.</p>
<p>Since submodularity is a natural characterization of diminishing returns, numerous tasks involving exploration or discouraging repeated actions (Basu et al., 2019) can be captured via submodular functions.In addition to our experiments discussing experiment design, item collection and coverage objectives, Table 1 provides a summary of problems that can be addressed with SUBRL.</p>
<p>The submodular bandit problem is at the interface of learning and optimizing submodular functions (Streeter &amp; Golovin, 2008;Chen et al., 2017;Yue &amp; Guestrin, 2011).Algorithms with no-regret (relative to the 1-1/e approximation) exist, whose performance can be improved by exploiting linearity (Yue &amp; Guestrin, 2011) or smoothness (Chen et al., 2017) in the objective.Our results in Section 5 can be viewed as addressing (a generalization of) the submodular stochastic bandit problem.Exploiting further linearity or smoothness to improve sample complexity is interesting direction for future work.</p>
<p>EXPERIMENTS</p>
<p>We empirically study the performance of SUBRL on multiple environments.They are i) Informative path planning, ii) Item collection, iii) Bayesian D-experimental design, iv) Building exploration, v) Car racing and vi) Mujoco-Ant.The environments involve discrete (i-iv) and continuous (v-vi) stateaction spaces and capture a range of submodular rewards, illustrating the versatility of the framework.</p>
<p>The problem is challenging in two aspects: firstly, how to maximize submodular rewards, and secondly, how to maintain an effective state representation to enable history-dependent policies.Our experiments mainly focus on the first aspect and demonstrate that even with a simple Markovian state representation, by greedily maximizing marginal gains, one can achieve good performance similar to the ideal case of non-Markovian representation in many environments.However, we We consider two variants of SUBPO: SUBPO-M and SUBPO-NM, corresponding to Markovian and non-Markovian policies, respectively.SUBPO-NM uses a stochastic policy that conditions an action on the history.We model the policy using a neural network that maps the history to a distribution over actions, whereas SUBPO-M maps the state to a distribution over actions.Disregarding sample complexity, we expect SUBPO-NM perform the best, since it can track the complete history.In our experiments, we always compare with modular RL (MODPO), a baseline that represents standard RL, where the additive reward for any state s is F ({s}).In MODPO, we maximize the undiscounted sum of additive rewards, whereas, in contrast, SUBPO maximizes marginal gains.The rest of the process remains the same, i.e., we use the same policy gradient method.We implemented all algorithms in Pytorch and will make the code and the videos public.We deploy Pytorch's automatic differentiation package to compute an unbiased gradient estimator.Experiment details and extended empirical analysis are in Appendix F. Below we explain our observations for each environment:</p>
<p>Informative path planning.We simulate a bio-diversity monitoring task, where we aim to cover areas with a high density of gorilla nests with a quadrotor in the Kagwene Gorilla Sanctuary (Fig. 1a).</p>
<p>The quadrotor at location s covers a limited sensing region around it, D s .The quadrotor starts in a random initial state and follows deterministic dynamics.It is equipped with five discrete actions representing directions.Let ρ : V → R be the nest density obtained by fitting a smooth rate function (Mutný &amp; Krause, 2021) over Gorilla nest counts (Funwi-gabga &amp; Mateu, 2011).The objective function is given by F (τ ) = g( s∈τ D s ), where g(V ) = v∈V ρ(v).As shown in Fig. 3a, we observe that MODPO repeatedly maximizes its modular reward and gets stuck at a high-density region, whereas SUBPO achieves performance as good as SUBPO-NM while being more sample efficient.To generalize the analysis, we replace the nest density with randomly generated synthetic multimodal functions and observe a similar trend (Appendix F).</p>
<p>Item collection.As shown in Fig. 1b, the environment consists of a grid with a group of items G = {banana, apple, strawberries, watermelon} located at g i ⊆ V, i ∈ G.We consider stochastic dynamics such that with probability 0.9, the action we take is executed, and with probability 0.1, a random action is executed (up, down, left, right, stay).The agent has to find a policy that generates trajectories τ , which picks d i items from group g i , for each i.Formally, the submodular reward function can be defined as
F (τ ) = i∈G min(|τ ∩ g i |, d i ).
We performed the experiment with 10 different randomly generated environments and 20 runs in each.In this environment, the agent must keep track of items collected so far to optimize for future items.Hence as shown in Fig. 3b, SUBPO-NM based on non-Markovian policy achieves good performance, and SUBPO-M achieves a slightly lower but yet comparable performance just by maximizing marginal gains.</p>
<p>Bayesian D-experimental design.In this experiment, we seek to estimate an a-priori unknown function f .The function f is assumed to be regular enough to be modelled using Gaussian Processes.</p>
<p>Where should we sample f to estimate it as well as possible?Formally, our goal is to optimize over trajectories τ that provide maximum mutual information between f and the observations y τ = f τ +ϵ τ at the points τ .The mutual information is given by I(y τ ; f ) = H(y τ ) − H(y τ |f ), representing the reduction in uncertainty of f after knowing y τ , where H(y τ ) is entropy.We define the monotonic submodular function F (τ ) = I(y τ ; f ).The gorilla nest density f is an a-priori unknown function.</p>
<p>We generate 10 different environments by assuming random initialization and perform 20 runs on  c) show that SUBPO scales very well to high dimensional continuous domains.each to compute statistical confidence.In Fig. 3c, we observe a similar trend that MODPO gets stuck at a high uncertainty region and cannot effectively optimize the information gained by the entire trajectory, whereas SUBPO-M achieves performance as good as SUBPO-NM while being very sample efficient due to the smaller search space of the Markovian policy class.</p>
<p>Building exploration.The environment consists of two rooms connected by a corridor.The agent at s covers a nearby region D s around itself, marked as a green patch in Fig. 4a.The task is to find a trajectory τ that maximizes the submodular function F (τ ) = | ∪ s∈τ D s |.The agent starts in the corridor's middle and has deterministic dynamics.The horizon H is just enough to cover both rooms.Based on the time-augmented state space, there exists a deterministic Markovian policy that can solve the task.However, it is a challenging environment for exploration with Monte Carlo samples using Markovian policies.As shown in Fig. 5a, SUBPO achieves a sub-optimal solution of exploring primarily one side, whereas SUBPO-NM tracks the history and learns to explore the other room.</p>
<p>Car Racing is an interesting high-dimensional environment, with continuous state-action space, where a race car tries to finish the racing lap as fast as possible (Prajapat et al., 2021).The environment is accompanied by an important challenge of learning a policy to maneuver the car at the limit of handling.The track is challenging, consisting of 13 turns with different curvature (Fig. 4b).The car has a six-dimensional state space representing position and velocities.The control commands are two-dimensional, representing throttle and steering.Detailed information is in the Appendix F. The car is equipped with a camera and observes a patch around its state s as D s .The objective function is
F (τ ) = | ∪ s∈τ D s |.
We set a finite horizon of 700.The SUBPO-NM will have a large state space of 700 × 6, which makes it difficult to train.For variance reduction, we use a baseline b(s) in Eq. ( 5) that estimates the cumulative sum of marginal gains.As shown in Fig. 5b, under the coverage-based reward formulation, the agent trained with MODPO tries to explore a little bit but gets stuck with a stationary action at the beginning of the episode to get a maximum modular reward.However, the SUBPO agent tries to maximise the marginal again at each timestep and hence learns to drive on the race track (https://youtu.be/jXp0QxIQ-E).Although it is possible to use alternative reward functions to train the car using standard RL, the main objective of this study is to demonstrate SUBPO on the continuous domains and how submodular functions can provide versatility to achieve surrogate goals.</p>
<p>MuJoCo Ant.The task is a high-dimensional locomotion task, as depicted in Fig. 4c.The state space dimension is 30, containing information about the robot's pose and the internal actuator's orientation.The control input dimension is 8, consisting of torque commands for each actuator.The Ant at any location s covers locations in 2D space, D s and receives a reward based on it.The goal is to maximize
F (τ ) = | ∪ s∈τ D s |.
The results depicted in Fig. 5c demonstrate that SUBPO maximizes marginal gain and learns to explore the environment, while MODPO learns to stay stationary, maximizing modular rewards.The environment carries the core challenge of continuous control and high-dimensional observation spaces.This experiment shows that SUBPO can effectively scale to high-dimensional domains.</p>
<p>CONCLUSION</p>
<p>We introduced a novel framework, submodular RL for decision-making under submodular rewards.We prove the first-of-its-kind inapproximability result for SUBRL i.e., the problem is not just NP-Hard but intractable even to approximate up to any constant factor.We propose an algorithm, SUBPO for this problem and show that under simplified assumptions, it achieves constant-factor approximation guarantees.We show that the algorithm exhibits strong empirical performance and scales very well to high-dimensional spaces.We hope this work will expand the reach of the RL community to embrace the broad class of submodular objectives which are relevant to many practical real-world problems.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>We have included all of the code and environments used in this study in the supplementary materials.These resources will be made open-source later on.The attached code contains a README.mdfile that provides comprehensive instructions for running the experiments.Furthermore, Appendix F contains additional emperical results and the parameters to reproduce the results.Regarding the theoretical results, all the proofs of the propositions and the theorems can be found in the appendix.</p>
<p>Ruosong Wang, Hanrui Zhang, Devendra Singh Chaplot, Denis Garagić, and Ruslan Salakhutdinov.</p>
<p>Planning with submodular objective functions.arXiv preprint arXiv:2010.11863,2020.</p>
<p>Yisong Yue and Carlos Guestrin.Linear submodular bandits and their application to diversified retrieval.Part I</p>
<p>Appendix</p>
<p>A PROOF FOR PROPOSITIONS Proposition 1.For any deterministic MDP with a fixed initial state, the optimal Markovian policy achieves the same value as the optimal non-Markovian policy.</p>
<p>Proof.The proof consists of two parts.First, we establish the existence of an optimal deterministic non-Markovian policy for a deterministic MDP M with non-Markovian rewards F .Second, we demonstrate that the trajectory generated by the optimal deterministic NM policy (π NM ) in M can also be generated by a deterministic Markovian policy (π M ).Since both policies yield the same trajectory, resulting in: J(π NM ) = J(π M ).(Notably, it is sufficient to look at a value of the single induced trajectory since we consider a deterministic policy in a deterministic MDP with a fixed initial state.)</p>
<p>i) We can construct an extended MDP, denoted as M e , where the state space consists of trajectories τ 0:h for all time steps h ∈ [H].In M e , the trajectory rewards F are Markovian.Due to Markovian rewards in M e , there exists an optimal deterministic Markovian policy for M e (Puterman, 1994) that, when projected back to M, corresponds to a non-Markovian policy (π NM ).</p>
<p>ii) Without loss of generality, let τ = ((s 0 , a 0 ), (s 1 , a 1 ), . . ., s H ) be a trajectory induced by π NM ∈ Π NM .If the states are augmented with time, the same trajectory τ is induced by a deterministic Markovian policy defined as π M (a i |s i ) = 1 and π M (a j |s i ) = 0 for i ̸ = j, ∀i, j.Due to the identical induced trajectory, we have J(π M ) = J(π NM ).</p>
<p>Proposition 2. For any set function F , among the Markovian policies Π M , there exists an optimal policy that is deterministic.</p>
<p>Proof.Given any stochastic policy, it is possible to select a state and modify the action distribution at that state to a deterministic action such that the value of the new policy will be at least equal to the value of the original policy.Below we show a construction to ensure the monotonic improvement of the policy, W.l.o.g., suppose there are two actions, denoted as a k and a ′ k , available at a state s k (the argument remains applicable for any finite number of actions).Consider the objective J(π) as shown below (for simplicity, we wrote only the trajectories affected with action at horizon k at state s k ),
J(π) = τ ∈Γ µ(s 0 ) H−1 i=0 p i (s i+1 |s i , a i )π i (a i |s i ) F ({(s 0 , a 0 ), . . . (s k , a k ) . . . s H }) = τ ∈Γ:(s k ,a k )∈τ µ(s 0 )   H−1 i=0,i̸ =k p i (s i+1 |s i , a i )π i (a i |s i )   p k (s k+1 |s k , a k ) π k (a k |s k ) x F (τ ) + τ ∈Γ:(s k ,a ′ k )∈τ µ(s 0 )   H−1 i=0,i̸ =k p i (s i+1 |s i , a i )π i (a i |s i )   p k (s k+1 |s k , a ′ k ) π k (a ′ k |s k ) y F(τ )
+ constant (remaining terms, do not vary with x and y)</p>
<p>The objective is to maximize J(π) subject to simplex constraints.The policy remains fixed for all states except s k .It is important to note that J is a linear function of variables x and y.Given that J is linear within the simplex constraints, the optimal solution lies on a vertex of the simplex
(π k (a k |s k ) + π k (a ′ k |s k ) = 1
).This vertex represents a deterministic decision for state s k .We can define a new policy π ′ based on π by adjusting the action distribution at s k to the optimal deterministic action (either a ′ k or a k ).It is evident that J(π ′ ) ≥ J(π).This process can be repeated for all states.By starting with any optimal stochastic policy, we can obtain a deterministic policy that is at least as good as the original stochastic policy.This concludes the proof.</p>
<p>SOP</p>
<p>SubRL
π τ ′ β ≥ f (τ ⋆ ) E π [f (τ ) ]</p>
<p>B INAPPROXIMABILITY PROOF</p>
<p>First, we introduce a set of known hard problems that we will use to establish the hardness of SUBRL.</p>
<p>Group Steiner tree (GST).The input to the group Steiner problem is an edge-weighted graph G = (V, E, l) and k subsets of nodes g 1 , g 2 , . . ., g k called groups.Starting from a root node r, the goal in GST is to find a minimum weight tree
T ⋆ = (V (T ⋆ ), E(T ⋆ )) in G such that each group is visited at least once, i.e., V (T ⋆ ) ∩ g i ̸ = ∅, ∀i ∈ [k].
Covering Steiner problem (SCP).The input to SCP is an edge-weighted graph G = (V, E, l) and k subsets of groups g 1 , g 2 , . . ., g k , each group has a positive integer d i representing a minimum visiting requirement.Starting from a root node r, the goal in SCP is to find a minimum weight tree
T ⋆ = (V (T ⋆ ), E(T ⋆ )) in G such that the tree covers at least d i nodes in group g i , i.e., |V (T ⋆ )∩g i | ≥ d i , ∀i ∈ [k].
The SCP generalizes GST problem to an arbitrary constraint d i .</p>
<p>Submodular Orienteering Problem (SOP).In rooted SOP, we are given a root node r ∈ V (T ), and the goal is to find a walk τ of length at most B that maximizes some submodular function F defined on the nodes of the underlying graph.</p>
<p>Approximation ratio: Let x be an input instance of a maximization problem.The approximation ratio β is defined as β ≥ OPT(x) ALG(x) , β ≥ 1, where OPT is the global optimal and ALG is the value attained by the algorithm.The hardness lower bound and approximation upper bounds refer to the lower and upper bound on the approximation ratio β.</p>
<p>Theorem 1.Let OPT be the optimal value and γ &gt; 0.Even for deterministic SMDP's, the SUBRL problem is hard to approximate within a factor of Ω(log 1−γ OPT) unless NP ⊆ ZTIME(n polylog(n) ).</p>
<p>Proof.We reduce SUBRL to rooted SOP, demonstrating the inapproximability of SUBRL.Lemma 1 establishes the hardness of rooted SOP.</p>
<p>Given an instance of SOP with a graph G = (V, E) and a root node r ∈ V , the goal is to find a walk τ that maximizes a submodular function F (τ ), subject to a budget constraint |τ | ≤ B. This can be converted to a SMDP (input to SUBRL), ⟨S, A, P, ρ, F, H⟩ tuple in polynomial time as follows:
i) Set S ← H ×V , A ← E, H ← − B, ρ(0, r) = 1 and the submodular function F remains unchanged. ii) Iterate over the edges e ∈ E. Let e = (v ′ , v), set P ((h + 1, v ′ )|(h, v), a) = 1, for all h ∈ [H − 1]
where action a = e.</p>
<p>The solution to the SUBRL problem is a policy π that can be rolled out to obtain a solution for the SOP, which is also a polynomial time operation.By assuming the existence of a polynomial time algorithm for SUBRL with an approximation ratio β = o(log 1−γ OPT), we can approximate SOP with 6).However, this contradicts the fact that rooted SOP cannot be approximated better than Ω(log 1−γ OPT) (Lemma 1).Proved by contradiction.
F (τ ) &gt; F (τ ⋆ ) β (Fig.
The results shows that there is no algorithm that can guarantee J(π) ≥ OPT log 1−γ OPT for all the input instances.As OPT increases with the input size of the problem, the ratio 1 log 1−γ OPT degrades and hence no algorithm can approximate the problem up to any constant factor c &gt; 0. For the sake of completeness, we include Theorem 4.1 from Chekuri &amp; Pal (2005) and modify it for the rooted SOP.The reduction scheme is the same as Chekuri &amp; Pal (2005).</p>
<p>Lemma 1 (Theorem 4.1 from Chekuri &amp; Pal (2005)).The rooted submodular orienteering problem (SOP) in undirected graphs is hard to approximate within a factor of Ω(log 1−γ OPT) unless NP ⊆ ZTIME(n polylog(n) ).</p>
<p>Proof.The group Steiner problem (GST) is hard to approximate to within a factor of Ω(log 2−γ OPT) unless NP has quasi-polynomial time Las Vegas algorithms (Halperin &amp; Krauthgamer, 2003).We reduce the problem of rooted SOP to GST, proving the inapproximability of rooted SOP.This represents that if we have an efficient algorithm for SOP, then we can recover a solution for GST by using the same SOP algorithm.Submodular function F .Given an SCP instance, define a submodular function
F (S) = k i=1 min(d i , |S ∩ g i |)
. F is a monotone submodular set function.Consider an optimal solution of SCP as T ⋆ of cost OPT.We can take an Euler tour of the tree T ⋆ and obtain a tour from r of length at most 2OPT that covers all groups.</p>
<p>Reduction.We will reduce rooted SOP problem to the SCP (SCP generalises GST with any d i &gt;0).Let's say we have an algorithm A for SOP with Ω(log i d i ). ( i d i is optimal value for SOP).In a single iteration, A will generate a walk that covers f (V (T ⋆ )/ log f (V (T ⋆ )) of length B, which can be converted to a tour P of length at most 2B.We can remove the nodes in P and reduce the coverage requirement of the groups that are partially covered and repeat the above procedure.Using Lemma 2, all groups will be covered up to the requisite amount in O(log 2 i d i ) iterations.Combining all the tours yields a tree of length O(log 2 i d i )B that is a "feasible solution" of the SCP.B can be evaluated using binary search and is within a constant factor of OPT.When specialized to the GST, i.e., d i = 1, the approximability ratio becomes O(log 2 k).</p>
<p>Contradiction.Following the reduction above, assuming an algorithm A for SOP with an approximation ratio of log k results in O(log 2 k) approximation ratio for GST.Hence, an α = o(log k) approximation algorithm for SOP will give an approximation of O(α log k) for GST.But GST is hard to approximate to within a factor of Ω(log 2−γ OPT).Hence SOP is hard to approximate within a factor of Ω(log 1−γ OPT).</p>
<p>Lemma 2. A algorithm A for SOP with approximability ratio Ω(log k) can cover k nodes after O(log 2 k) iterations.</p>
<p>Proof.Let L n be the nodes available after the n th iteration with an algorithm having β ≥ Ω(log k).
L 0 ← − k L 1 ← − L 0 − L 0 log L 0 . . . L n+1 ← − L n − L n log L n x− x log x
In the first iteration with
x = k L 1 = x − x log x = x(1 − 1 log x ) ≤ xe −1 log x = ke −1 log k . By definition, L 1 ≥ L 2 ≥ . . . L n , hence L i ≤ ke −1/ log k ∀i ∈ [1, n].
Nodes available after n th iteration :
L 0 (1 − 1 log L0 )(1 − 1 log L1 ) . . . (1 − 1 log Ln ). L 0 (1 − 1 log L 0 )(1 − 1 log L 1 ) . . . (1 − 1 log L n ) ≤ k e −1/ log k × e −1/ log k . . . e −1/ log k n times = ke −n/ log k
for n &gt; log 2 k, nodes available: ke −n/ log k &lt; ke − log 2 k/ log k = ke − log k = 1.Hence Proved.</p>
<p>C DISCUSSION</p>
<p>Submodularity.Since the algorithm and the theoretical hardness result readily extend to general set functions beyond submodular rewards, a natural question that arises is how critical is that F is a submodular function and what can we say beyond submodular rewards?In this work, submodularity emerges in the lower bound (inapproximability hardness), implying that the problem is not just intractable but intractable even to approximate up to any constant factor.Additionally, it emerges in the upper bound of 1 − c under curvature assumption and in the upper bound of (1 − 1/e) under the simplified SMDP setting.There cannot exist an algorithm for bandit SMDP with better guarantees (Feige, 1998), and SUBPO is able to achieve the optimal ratio (1 − 1/e), thus utilising submodularity to provide intuition on why SUBPO (a REINFORCE type strategy) is a right strategy.</p>
<p>Overall, submodularity lets us characterise the spectrum of the computational complexity of the SubRL framework, while some results, e.g., our algorithm SUBPO, inapproximability hardness, naturally carry over to the general non-Markovain rewards beyond submodular F (general nonadditive reward function).</p>
<p>Policy class.The restriction to Markovian policies in the theoretical limits section is mainly for emphasizing the "hardness result" even for the simple policy class, implying the source of hardness is not the representation of non-Markovian policy (which is an exponential object itself).The overall goal is to learn a policy that achieves a higher objective value; hence, we do not, in general, restrict it to the Markovian policy class.We treat the problem of learning state representation separately, which can be done, e.g. with RNN, and is an add-on to the SUBPO, e.g.SUBPO-NM optimises in a non-Markovian policy class.</p>
<p>Expressivity of rewards.The optimal policies for submodular rewards cannot be captured by the Markovian rewards in general since the optimal policies are non-Markovian.However, when the policy search is restricted to the Markovian class, the optimal policy is deterministic (Proposition 2), and hence there exists a Markovian reward that would lead to the same optimal policy.But this does not help to solve the problem since finding such Markovian rewards has to be NP-hard to approximate Theorem 1.</p>
<p>In contrast to finding such surrogate Markovian rewards, submodularity provides a natural way to capture the task.Moreover, since we do not restrict to the Markovian policies, given a policy class with compact history representation, SUBPO can learn behaviours beyond the expressivity of the Markovian rewards (Abel et al., 2021).</p>
<p>Applications.Since submodularity is a natural characterization of diminishing returns, numerous tasks involving exploration or discouraging repeated actions can be captured via submodular functions.In addition to our experiments discussing experiment design, item collection and coverage objectives, the following Table 1 provides a summary of problems that can be addressed with SUBRL.</p>
<p>Tasks</p>
<p>Relevant works Submodular reward function F (τ )</p>
<p>State entropy exploration (Hazan et al., 2019)
F (τ ) = −1 |τ | v∈V I (v,•)∈τ log |{t:(v,t)∈τ }| |τ | D-Optimal Experimental Design (Mutny et al., 2023) F (τ ) = I(y τ ; f ), I(y; f ) = H(y τ ) − H(y τ |f ) Steiner covering (Chekuri &amp; Pal, 2005) F (τ ) = i∈G min(|τ ∩ g i |, d i ), pick d i items of group g i State coverage functions (Prajapat et al., 2022) F (τ ) = v∈V |{t ∈ [H] : (v, t) ∈ τ }|, F (τ ) = | v∈τ D v | Weighted coverage function (Karimi et al., 2017) F (τ ) = g( s∈τ D s ), g(V ) = v∈V ρ(v)∇ θ J(π θ ) = E τ ∼f (τ ;π θ )   H−1 i=0 ∇ θ log π θ (a i |s i )   H−1 j=i F (s j+1 |τ 0:j ) − b(τ 0:i )    H−1 i=0 π θ (a i |s i ))F (τ ) = τ f (τ ; π θ )g(τ ; π θ )F (τ ) = E τ ∼f (τ ;π θ ) H−1 i=0 ∇ θ log π θ (a i |s i ) F (τ )(6)
Using marginal gain F (s|τ 0:j ) = F (τ 0:j ∪{s})−F (τ 0:j ) and telescopic sum
H−1 j=0 F (s j+1 |τ 0:j ) = F (τ ) − F (s 0 ), ∇ θ J(π θ ) = E τ ∼f (τ ;π θ )   H−1 i=0 ∇ θ log π θ (a i |s i )   H−1 j=0 F (s j+1 |τ 0:j ) + F (s 0 )     = E τ ∼f (τ ;π θ )   H−1 i=0 ∇ θ log π θ (a i |s i )   H−1 j=0 F (s j+1 |τ 0:j ) + F (s 0 )    
For any function of partial trajectory up to i, b ′ (τ 0:i ), we have,
ai π θ (a i |s i )∇ θ log π θ (a i |s i )b ′ (τ 0:i ) = ai ∇ θ π θ (a i |s i )b ′ (τ 0:i ) = 0.
Thus one can subtract any history-dependent baseline without altering the gradient estimator,
= E τ ∼f (τ ;π θ )   H−1 i=0 ∇ θ log π θ (a i |s i )   H−1 j=0 F (s j+1 |τ 0:j ) + F (s 0 ) − b ′ (τ 0:i )     = E τ ∼f (τ ;π θ )   H−1 i=0 ∇ θ log π θ (a i |s i )   H−1 j=i F (s j+1 |τ 0:j )    
Finally, we can subtract a baseline again using a similar trick as above and we get the theorem statement: Proof.ϵ-Bandit MDP considers state independent transitions (only horizon dependent), i.e., v j , v k ∈ V, j ̸ = k, ∀h ∈ [H] and ∀v ′ ∈ V, P h (v j |v ′ , a j ) = 1 − ϵ h , and
= E τ ∼f (τ ;π θ )   H−1 i=0 ∇ θ log π θ (a i |s i )   H−1 j=i F (s j+1 |τ 0:j ) − b(τ 0:i )     Transition probability a 0 a 0 ϵ 1 − ϵ a 0 a 0 a 1 v 1 a 1 a 0 a 1 v 1 v 1 v 0 v 0 v 0 a 0 a 1 a 0 a 0 a 1 h=0 h=2 h=1 a 1P h (v k |v ′ , a j ) = ϵ h |V|−1 for ϵ h ∈ 0, |V| |V|+1 .
For simplicity, we consider a fixed size V, but it can also vary with the horizon.To denote explicit dependent on the horizon, we use P h and v instead of directly s.</p>
<p>Similarly, policy parameterization in Theorem 3 considers state independent policy (only horizon dependent) π h (a|v ′ ) = π h (a|v ′′ )∀h ∈ [H], ∀v ′ , v ′′ ∈ V, in short notation we denote them as π h (a).</p>
<p>Note that π h (a i |v i ) corresponds to the self-loop actions ("stay").In the proof, we reparameterize the probability of self-loop actions with that of other actions (i.e., π h (a
i |v i ) = 1 − a̸ =ai π h (a|v i )), resulting in relaxation of the simplex constraint a π h (a|v) = 1, ∀h, v → a̸ =ai π h (a|v i ) ≤ 1 .
In the following, for ease of notation, we denote v
i := (i, v), in particular, F ((v ′ i , a i ) H−1 i=0 , v H ) := F ((i, v ′ , a i ) H−1 i=0 , (H, v)). Consider the objective J(π), J(π) = τ ∈Γ µ(v 0 ) H−1 h=0 p h (v h+1 |v h , a h )π h (a h |v h )F ((v i , a i ) H−1 i=0 , v H ) = τ ∈Γ µ(v 0 ) H−1 h=0 p h (v h+1 |a h )π h (a h )F ((v i , a i ) H−1 i=0 , v H ) (state independent assumptions)
We show DR-submodularity by showing ∀π ∈ P, ∂ 2 J(π) ∂π ′ ∂π ′′ ≤ 0, ∀π ′ , π ′′ ∈ P. We first reparameterize the self-loop actions (which bring the agent back to the same state) in J(π) by substituting π h (a l h |v l h ) = 1 − a̸ =a l h π h (a|v l h ) in J(π).Here a l is a looping action for state v l at horizon h.</p>
<p>First, we prove monotonicity of J(π) by showing ∂J(π)   ∂π h (a ′ h ) ≥ 0.
∂J(π) ∂π h (a ′ h ) = v l ∈V ∂J v l (π) ∂π h (a ′ h )
, where, We prove ∂J v l (π) ∂π h (a ′ h ) ≥ 0 for any v l , which implies ∂J(π)   ∂π h (a ′ h ) ≥ 0. Note: For every trajectory in E := {τ ∈ Γ : (v l h , a ′ h ) ∈ τ } (1 st summation), we have a trajectory in L := {τ ∈ Γ : (v l h , a h ) ∈ τ } (2 nd summation) that differ only in v ′ h+1 and v l h+1 and all other states are exactly same.For every trajectory in L, there is a trajectory in E with a higher value.We define a short notation
∂J v l (π) ∂π h (a ′ h ) = τ ∈Γ:(v l h ,a ′ h )∈τf − h := µ(v 0 ) H−1 i̸ =h p i (v i+1 |a i )π i (a i ),
Note f ′ − h and f l − h are equal due to state independent transition and policy.Drop the actions (the function F depends on the states) and let R := τ \v ′ , = τ ∈Γ:(v l h ,a ′ h )∈τ
f ′ − h (1 − ϵ h − ϵ h |V| − 1 ) F (R ∪ {v ′ h+1 }) − F (R) ≥ 0 (since, ϵ h ≤ |V|−1 |V| )
The reason for (1 − ϵ h − ϵ h |V|−1 ) : (1 − ϵ h ) = p h (v ′ h+1 |a ′ h ) = p h (v l h+1 |a l h ) and ϵ h |V|−1 = p h (v ′ h+1 |a l h ) = p h (v l h+1 |a ′ h ).In Eq. ( 7), the two terms (corresponding to set L and E) are subtracted, with probability (1 − ϵ h ) the first term will be larger since F (R ∪ {v ′ h+1 }) − F (R) ≥ 0 and with probability ϵ h |V|−1 the second term will be larger since the stochastic transition (e.g., looping action a l h can jump to next state v ′ h+1 and a ′ h stays to the same state v l h+1 .This can happen with probability ϵ h |V|−1 ).In other stochastic transitions, in expectation, the two terms will sum to zero.In the above, we have proved the monotonicity of J(π) given F (•) is a monotone function.We have,
∂J(π) ∂π h (a ′ h ) = τ µ(v 0 )(1 − |V|ϵ h |V| − 1 ) H−1 h=0 p h (v h+1 |a h )   H−1 h̸ =i π h (a h )   F (R ∪ {v ′ h+1 }) − F (R) ≥ 0
To obtain the hessian terms, we can follow the same process as above at some horizon g and state a ′′ .Let R := τ (v ′ , v ′′ ), (v ′′ is the state corresponding to action a ′′ ),
∂ 2 J(π) ∂π g (a ′′ g )∂π h (a ′ h ) = τ ∈Γ f ′− g,h (1 − |V|ϵ g |V| − 1 )(1 − |V|ϵ h |V| − 1 ) F (R ∪ {v ′′ g+1 , v ′ h+1 }) − F (R ∪ {v ′ h+1 }) − (F (R ∪ {v ′′ g+1 })) − F (R)) = τ ∈Γ f ′− g,h (1 − |V|ϵ g |V| − 1 )(1 − |V|ϵ h |V| − 1 ) F (R ∪ {v ′′ g+1 } A ∪{v ′ h+1 }) − F (R ∪ {v ′′ g+1 } A ) − (F (R ∪ {v ′ h+1 }) − F (R))
≤ 0 (By submodularity of F , R ⊆ A, ∆ F (v ′ |A) ≤ ∆ F (v ′ |R)) Hence J(π) is monotone DR-submodular.</p>
<p>E.2 BOUNDED CURVATURE</p>
<p>Proposition 3. Consider a tabular SMDP, s.t. the reward function F is monotone submodular with bounded curvature c ∈ (0, 1).Then, for the policy π (with tabular parametrization) obtained via SUBPO, it holds that J(π) ≥ (1 − c)J(π ⋆ ), where π ⋆ is an optimal non-Markovian policy.</p>
<p>Proof.Consider the objectives J(π) and H(π) defined with submodular reward F (τ ) and its corresponding modular rewards F m (τ ) = s∈τ F (s) respectively, J(π) =  Similarly, since F m (τ ) ≥ F (τ ) ≥ (1 − c)F m (τ ), the following holds component-wise,
∇ π H(π)| π=π ′ ≥ ∇ π J(π)| π=π ′ ≥ (1 − c)∇ π H(π)| π=π ′ ∀π ′ .(8)
At the convergence of SUBPO, the stationary point π satisfies,
max π ′ ∈Π ⟨∇ π J(π), π ′ − π⟩ ≤ 0 =⇒ max π ′ ∈Π
⟨∇ π H(π), π ′ − π⟩ ≤ 0. (using Eq. ( 8), c ̸ = 1)</p>
<p>Hence π is also a stationary point for modular objective H(π).Under mild regularity assumptions, any stationary point of the policy gradient cost function with modular rewards is a global optimum (Bhandari &amp; Russo, 2019).Let π be the policy where SUBPO converges, then,
J(π) = τ f (τ ; π)F (τ ) ≥ (1 − c) τ f (τ ; π)F m (τ ) (9) ≥ (1 − c) τ f (τ ; π ⋆ )F m (τ )(10)≥ (1 − c) τ f (τ ; π ⋆ )F (τ ) = (1 − c)J(π ⋆ )(11)
Eq. ( 9) follows using curvature definition for any policy π.Eq. ( 10) follows since π is optimal of H(π) where as π ⋆ ∈ Π NM is optimal for J(π).Finally, Eq. ( 11) follows since F m (τ ) ≥ F (τ ).</p>
<p>Figure 3 :
3
Figure3: Comparison of SUBPO-M, SUBPO-NM and MODPO.We observe that MODPO get stuck by repeatedly maximizing its modular reward, whereas SUBPO-M achieves comparable performance to SUBPO-NM while being more sample efficient.(Y-axis: normalized J(π), X-axis: epochs)</p>
<p>Figure 4 :
4
Figure 4: Challenging tasks modelled via submodular reward functions.Primarily, the agent at location s senses a region, D s and seeks a policy to maximize submodular rewards F (τ ) = |∪ s∈τ D s |. a) The agent, starting from the middle, must learn to explore both rooms.b) The car must learn to drive &amp; finish the racing lap c) Ant must learn to walk to cover the maximum 2D space around itself.do not claim that a Markovian state representation is sufficient in general.For instance, in the building exploration and item collection environments, Markovian policies are insufficient, and a history-dependent approach is necessary for further optimization.Natural avenues are to augment the state representation to incorporate additional information, e.g., based on domain knowledge, or to use a non-Markovian parametric policy class such as RNNs.Exploring such representations is application specific, and beyond the scope of our work.</p>
<p>Figure 5: a) Coverage in building exploration, SUBPO-NM tracks history and can explore the other room b) Car trained with SUBPO-M learns to drive through the track (Y-axis: normalized [0-start &amp; 1-finish]) c) Ant trained with SUBPO-M learns to explore the domain (Y-axis: normalized with the domain area).Both b,c) show that SUBPO scales very well to high dimensional continuous domains.each to compute statistical confidence.In Fig.3c, we observe a similar trend that MODPO gets stuck at a high uncertainty region and cannot effectively optimize the information gained by the entire trajectory, whereas SUBPO-M achieves performance as good as SUBPO-NM while being very sample efficient due to the smaller search space of the Markovian policy class.</p>
<p>Figure 6 :
6
Figure 6: Reduction of SUBRL to submodular orienteering problem (SOP)</p>
<p>Figure 7 :
7
Figure7: Nomenclature: With action, a j from any state the agent jumps to state s j with probability 1 − ϵ (solid), and with probability ϵ it jumps to any other state uniformly (dashed)</p>
<p>(v 0 , a 0 ), . . .(v l h , a ′ h ), (v ′ h+1 , a h+1 ), (v h+2 , a h+2 ) (v 0 , a 0 ), . . .(v l h , a l h ), (v l h+1 , a h+1 ), (v h+2 , a h+2 ) B . . .v H )</p>
<p>τf</p>
<p>(τ ; π)F (τ ), andH(π) = τ f (τ ; π)F m (τ ).</p>
<p>ForF∇</p>
<p>any policy π, J(π) ≥ (1 − c)H(π) using curvature definition.Consider ∇ π J(π),∇ π J(π) = τ ∇ π f (τ ; π)F (τ ) = τ ∇ π f (τ ; π) (s i+1 |τ 0:i ) + F ({s 0 }) π f (τ ; π)F m (τ ) = (1 − c)∇ π H(π).</p>
<p>Advances in Neural Information Processing Systems, 24, 2011.
Tom Zahavy, Brendan O'Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex mdps. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021), 2021.</p>
<p>Table 1 :
1
A few examples that can be tackled with submodular reinforcement learning framework D SUBMODULAR POLICY OPTIMIZATION, SUBPO'S POLICY GRADIENT PROOF Theorem 2. Given an SMDP and the policy parameters θ, with any set function F ,
Discourage repeated action/(Basu et al., 2019)F (τ ) = | s∈τ D s |, e.g., s = (v, t) and(including coverage on Time)D s := {(v, t),(v, t + 1),(v, t + 2)}Log determinant objectives(Wang et al., 2020)F (τ ) = log det
s∈τ F ({s}) + λI Facility locationKrause &amp; Golovin (2014)F (τ ) = i=1 max j∈τ M i,j , M ij ≥ 0</p>
<p>denotes trajectory distribution ignoring transition at v h .(v ′ h+1 |a ′ h )F ((v 0 , a 0 ), . . .(v l h , a ′ h ), (v ′ h+1 , a h+1 ), (v h+2 , a h+2 ) . . .v H ) )F ((v 0 , a 0 ), . . .(v l h , a l h ), (v l h+1 , a h+1 ), (v h+2 , a h+2 ) . . .v H )
= τ ∈Γ:(v l h ,a ′ h )∈τ f ′ − h p h − f l − h p h (v l h+1 |a l hτ ∈Γ:(v l h ,a l h )∈τ
Without loss of generality, this can be extended to state-action based rewards
autoregressive policies (RNNs or transformers) can be used to capture history-dependence in the same algorithm
ACKNOWLEDGMENTSThis publication was made possible by an ETH AI Center doctoral fellowship to Manish Prajapat.We would like to thank Mohammad Reza Karimi, Pragnya Alatur and Riccardo De Santi for the insightful discussions.We thank Bhavya Sukhija and Alizée Pace for reviewing the manuscript.The project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program grant agreement No 815943 and the Swiss National Science Foundation under NCCR Automation grant agreement 51NF40 180545.We observe that MODPO gets stuck by repeatedly maximizing its modular reward, whereas SUBPO-M achieves comparable performance to SUBPO-NM while being more sample efficient.(Y-axis: normalized J(π), X-axis: epochs)F EXPERIMENTSIn this work, we examined state-action spaces that are both discrete (finite) and continuous.Here, we provide an overview of the hyperparameters and experiment specifics for each type.F.1 FINITE STATE ACTION SPACESWe consider a grid world environment with domain size 30 × 30, having a total of 900 states.Horizon H = 40, i.e., the agent has to find a path of length 40 that jointly maximize the objective.The action set comprised of {right, up, lef t, down, stay}.The agent's policy was parameterized by a two-layer multi-layer perceptron, consisting of 64 neurons in each layer.The non-linearity in the network was induced by employing the Rectified Linear Unit (ReLU) activation function.By employing a stochastic policy, the agent generated a categorical distribution over the action set for each state.Subsequently, this distribution was passed through a softmax probability function.We employed a batch size of B = 500 and a low entropy coefficient of α = 0 or 0.005, depending on the specific characteristics of the environment.We randomly generated ten different environments and conducted 20 runs for each environment, resulting in a total of 200 experiments.These experiments were executed concurrently on a server with 400 cores, allocating two cores for each job.It takes less than 20 minutes to complete 150 epochs and obtain a saturated policy, indicating no further improvement.All our plot shows the training curve (objective vs epochs).For instances where we utilized randomly sampled environments, such as coverage with GP samples, gorilla nest density, or item collection environment, we have included the corresponding environment files in the attached code for easy reference.To conduct additional analysis using the Informative Path Planning Environment, we made modifications to the underlying function previously based on gorilla nest density.Specifically, we introduced three variations: i) a constant function, ii) a bimodal distribution, and iii) a multi-modal distribution randomly sampled from a Gaussian Process (GP).As depicted in Figure8, we noticed a consistent trend across all three variations.The MODPO algorithm repeatedly maximized its modular reward but became trapped in high-density regions.In contrast, the SUBPO-M algorithm demonstrated performance comparable to SUBPO-NM while exhibiting greater sample efficiency.F.2 CONTINUOUS STATE-ACTION SPACESCar Racing.In the car racing environment, our objective is to achieve the fastest completion of a one-lap race.To accomplish this, we aim to learn a policy that effectively controls a car operating at its handling limits.Our simulation study closely emulates the experimental platform employed at ETH Zurich, which utilizes miniature autonomous race cars.Building uponLiniger et al. (2015), we model the dynamics of each car using a dynamic bicycle model augmented with Pacejka tire models(Bakker et al., 1987).However, we deviate from the approach presented in(Liniger et al., 2015)by formulating the dynamics in curvilinear coordinates, where the car's position and orientation are represented relative to a reference path.This coordinate transformation significantly simplifies the reward definition and facilitates policy learning.The state representation of an individual car is denoted as z = [ρ, d, µ, V x , V y , ψ] T .Here, ρ measures the progress along the reference path, d quantifies the deviation from the reference path, µ characterizes the local heading relative to the reference path, V x and V y represent the longitudinal and lateral velocities in the car's frame, respectively, and ψ represents the car's yaw rate.The car's inputs are represented as [D, δ] T , where D ∈ [−1, 1] represents the duty cycle input to the electric motor, ranging from full braking at −1 to full acceleration at 1, and δ ∈ [−1, 1] corresponds to the steering angle.The car is equipped with a camera and observes a patch around its state s as D s .The objective function isFor simplicity, we define the observation patch of some 5 m and spanning the entire width of the race track.We add additive reward penalization to avoid hitting the walls.If a narrow-width patch is used, then one can eliminate the use of reward penalization (used to avoid hitting the boundaries) as coverage near the boundaries is low and the agent learns to drive in the middle or go to the extreme if they gain due to going fast.The test track, depicted in Figure4b, consists of 13 turns with varying curvatures.We utilize an optimized X-Y path as the reference path obtained through a lap time optimization tool.It is worth noting that using a pre-optimized reference path is not obligatory, but we observed improved algorithm convergence when employing this approach.To convert the continuous-time dynamics into a discrete-time Markov Decision Process (MDP), we discretize the dynamics using an RK4 integrator with a sampling time of 0.03 seconds.For the training, we started the players on the start line (ρ = 0) and randomly assigned d ∈ {0.1, −0.1}.We limit one episode to 700 horizon.For each policy gradient step, we generated a batch of 8 game trajectories and ran the training for roughly 6000 epochs until the player consistently finished the lap.This takes roughly 1 hour of training for a single-core CPU.We use Adam optimizer with a slow learning rate of 10 −3 .For our experiments, we ran 20 different random runs and reported the mean of all the seeds.As a policy, we use a multi-layer perceptron with two hidden layers, each with 128 neurons and used ReLU activation functions and a Tanh output layer to enforce the input constraints.For variance reduction, we use a baseline b(s) in Eq. (5) that estimates the cumulative sum of marginal gains.One can think of this as a heuristic to estimate marginal gains.The same setting was also used for the competing algorithm MODPO.MuJoCo.It is a physics engine for simulating high-dimensional continuous control tasks in robotics.In this experiment, we consider the Ant environment, which is depictive of the core challenges that arise in the Mujoco environment.For detailed information about the Mujoco Ant environment, we refer the reader to https://gymnasium.farama.org/environments/mujoco/ant/.For the training, we use the default random initialization of Mujoco-Ant.We consider a bounded domain of[−20, 20]2 .The agent covers a discrete grid of 5x5 around its location in the 2D space (only for efficient reward computation, we discretize the domain into a 400 × 400 grid, dynamics is continuous) and receives a reward based on the coverage.To train the agent faster, one can also couple MuJoCo's inbuilt additive rewards (tuned for faster walking) with the submodular coverage rewards, which results in submodular rewards.We limit one episode to a horizon of 400.We use a vectorized environment that samples a batch of 15 trajectories at once.We trained for roughly 20,000 epochs until the agent consistently walks and explores the domain.This takes roughly 6 hours of training for a single-core CPU.We use Adam optimizer with a learning rate of 10 −2 .For our experiments, we ran 20 different random runs and reported the mean of all the seeds.As a policy, we use a multi-layer perceptron with two hidden layers, each with 128 neurons and used ReLU activation functions and a Tanh output layer to enforce the input constraints.Similar to the car racing environment, we use a baseline b(s) in Eq. (5) that estimates the cumulative sum of marginal gains.The same setting was also used for the competing algorithm MODPO.
On the expressivity of markov reward. David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, Satinder Singh, Advances in Neural Information Processing Systems. 342021</p>
<p>. Iii Leemon C Baird, Updating, Wright-Patterson Afb Wright, Oh, 1993</p>
<p>Tyre modelling for use in vehicle dynamics studies. Egbert Bakker, Lars Nyborg, Hans B Pacejka, 10.4271/870421SAE Technical Paper. SAE International021987</p>
<p>Learning submodular functions. Maria-Florina Balcan, J A Nicholas, Harvey, Proceedings of the forty-third annual ACM symposium on Theory of computing. the forty-third annual ACM symposium on Theory of computing2011</p>
<p>Blocking bandits. Soumya Basu, Rajat Sen, Sujay Sanghavi, Sanjay Shakkottai, Advances in Neural Information Processing Systems. 201932</p>
<p>Infinite-horizon policy-gradient estimation. Jonathan Baxter, Peter L Bartlett, journal of artificial intelligence research. 152001</p>
<p>Inverse reinforcement learning in contextual mdps. Stav Belogolovsky, Philip Korsunsky, Shie Mannor, Chen Tessler, Tom Zahavy, Machine Learning. 2021</p>
<p>Jalaj Bhandari, Daniel Russo, arXiv:1906.01786Global optimality guarantees for policy gradient methods. 2019arXiv preprint</p>
<p>Continuous dr-submodular maximization: Structure and algorithms. An Bian, Kfir Levy, Andreas Krause, Joachim M Buhmann, Advances in Neural Information Processing Systems. 2017a30</p>
<p>Guarantees for greedy maximization of non-submodular functions with applications. Joachim M Andrew An Bian, Andreas Buhmann, Sebastian Krause, Tschiatschek, International conference on machine learning. PMLR2017b</p>
<p>Guaranteed nonconvex optimization: Submodular maximization over continuous domains. Baharan Andrew An Bian, Joachim Mirzasoleiman, Andreas Buhmann, Krause, Artificial Intelligence and Statistics. PMLR2017c</p>
<p>Jeff Bilmes, arXiv:2202.00132Submodularity in machine learning and artificial intelligence. 2022arXiv preprint</p>
<p>On the theory of reinforcement learning with once-per-episode feedback. Niladri Chatterji, Aldo Pacchiano, Peter Bartlett, Michael Jordan, Advances in Neural Information Processing Systems. 202134</p>
<p>A recursive greedy algorithm for walks in directed graphs. Chandra Chekuri, M , 10.1109/SFCS.2005.946th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05). 2005</p>
<p>Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Lin Chen, Michele Conforti and Gérard CornuéjolsAndreas Krause, Michele Conforti and Gérard CornuéjolsAmin Karbasi, Michele Conforti and Gérard CornuéjolsNIPS. 2017. 19847Interactive submodular bandit</p>
<p>Deep submodular functions: Definitions and learning. Brian W Dolhansky, Jeff A Bilmes, Advances in Neural Information Processing Systems. 292016</p>
<p>Optimal learning: Computational procedures for Bayes -adaptive Markov decision processes. O'gordon Michael, Duff, 2002University of Massachusetts AmherstPhD thesis</p>
<p>A threshold of ln n for approximating set cover. Uriel Feige, 10.1145/285055.285059J. ACM. 0004-5411454jul 1998</p>
<p>Chapter 19 gradient estimation. C Michael, Fu, S0927-0507(06)13019-4Simulation. Shane G Henderson, Barry L Nelson, Elsevier200613</p>
<p>Understanding the nesting spatial behaviour of gorillas in the kagwene sanctuary, cameroon. Stochastic Environmental Research and Risk Assessment. Neba Funwi, - , Jorge Mateu, 10.1007/s00477-011-0541-1262011</p>
<p>Adaptive submodularity: Theory and applications in active learning and stochastic optimization. Daniel Golovin, Andreas Krause, Journal of Artificial Intelligence Research. 422011</p>
<p>Variance reduction techniques for gradient estimates in reinforcement learning. Evan Greensmith, Peter L Bartlett, Jonathan Baxter, Journal of Machine Learning Research. 592004</p>
<p>Orienteering problem: A survey of recent variants, solution approaches and applications. Aldy Gunawan, Hoong , Chuin Lau, Pieter Vansteenwegen, 10.1016/j.ejor.2016.04.059European Journal of Operational Research. 0377-221725522016</p>
<p>Polylogarithmic inapproximability. Eran Halperin, Robert Krauthgamer, 10.1145/780542.780628Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing, STOC '03. the Thirty-Fifth Annual ACM Symposium on Theory of Computing, STOC '03Association for Computing Machinery20031581136749</p>
<p>Gradient methods for submodular maximization. Hamed Hassani, Mahdi Soltanolkotabi, Amin Karbasi, Advances in Neural Information Processing Systems. 201730</p>
<p>Provably efficient maximum entropy exploration. Elad Hazan, Sham Kakade, Karan Singh, Abby Van Soest, International Conference on Machine Learning. PMLR2019</p>
<p>Mohammad Karimi et al. Stochastic submodular maximization: The case of coverage functions. M Sham, Kakade, Advances in Neural Information Processing Systems. 2001. 201714Advances in neural information processing systems</p>
<p>Swarm slam: Challenges and perspectives. Miquel Kegeleirs, Giorgio Grisetti, Mauro Birattari, Frontiers in Robotics and AI. 86182682021</p>
<p>Submodular function maximization. Andreas Krause, Daniel Golovin, Tractability. 32014</p>
<p>Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Andreas Krause, Ajit Singh, Carlos Guestrin, J. Mach. Learn. Res. 1532-44359jun 2008</p>
<p>Information directed reward learning for reinforcement learning. David Lindner, Matteo Turchetta, Sebastian Tschiatschek, Kamil Ciosek, Andreas Krause, Proc. Neural Information Processing Systems (NeurIPS). Neural Information essing Systems (NeurIPS)December 2021</p>
<p>Optimization-based autonomous racing of 1: 43 scale rc cars. Alexander Liniger, Alexander Domahidi, Manfred Morari, Optimal Control Applications and Methods. 3652015</p>
<p>Sensing cox processes via posterior sampling and positive bases. Mojmír Mutný, Andreas Krause, CoRR, abs/2110.111812021</p>
<p>Active exploration via experiment design in markov chains. Mojmir Mutny, Tadeusz Janik, Andreas Krause, International Conference on Artificial Intelligence and Statistics. PMLR2023</p>
<p>Challenging common assumptions in convex reinforcement learning. Mirco Mutti, Riccardo De Santi, Piersilvio De Bartolomeis, Marcello Restelli, Advances in Neural Information Processing Systems. 202235</p>
<p>An analysis of approximations for maximizing submodular set functions-i. Mathematical programming. Laurence A George L Nemhauser, Marshall L Wolsey, Fisher, 197814</p>
<p>Competitive policy optimization. Manish Prajapat, Kamyar Azizzadenesheli, Alexander Liniger, Yisong Yue, Anima Anandkumar, Uncertainty in Artificial Intelligence. PMLR2021</p>
<p>Near-optimal multiagent learning for safe coverage control. Manish Prajapat, Matteo Turchetta, Melanie Zeilinger, Andreas Krause, Advances in Neural Information Processing Systems. 202235</p>
<p>Markov Decision Processes: Discrete Stochastic Dynamic Programming. Martin L Puterman, John Wiley &amp; Sons, Inc., USA1st edition, 1994. ISBN 0471619779</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, ICML. PMLR2015</p>
<p>Highdimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael I Jordan, Pieter Abbeel, 4th International Conference on Learning Representations, ICLR. 2016</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Nonmyopic adaptive informative path planning for multiple robots. Amarjeet Singh, Andreas Krause, William J Kaiser, Proceedings of the 21st International Joint Conference on Artificial Intelligence, IJCAI'09. the 21st International Joint Conference on Artificial Intelligence, IJCAI'09San Francisco, CA, USA2009</p>
<p>An online algorithm for maximizing submodular functions. Matthew Streeter, Daniel Golovin, Advances in Neural Information Processing Systems. 200821</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Policy gradient methods for reinforcement learning with function approximation. David Richard S Sutton, Satinder Mcallester, Yishay Singh, Mansour, Advances in neural information processing systems. 121999</p>
<p>Active model estimation in markov decision processes. Jean Tarbouriech, Shubhanshu Shekhar, Matteo Pirotta, Mohammad Ghavamzadeh, Alessandro Lazaric, Conference on Uncertainty in Artificial Intelligence. PMLR2020</p>
<p>Submodularity in action: From machine learning to signal processing applications. Ehsan Tohidi, Rouhollah Amiri, Mario Coutino, David Gesbert, Geert Leus, Amin Karbasi, 10.1109/MSP.2020.3003836IEEE Signal Processing Magazine. 3752020</p>
<p>Submodularity and curvature: the optimal algorithm. Jan Vondrak, RIMS Kôkyûroku Bessatsu. 01 2010</p>            </div>
        </div>

    </div>
</body>
</html>
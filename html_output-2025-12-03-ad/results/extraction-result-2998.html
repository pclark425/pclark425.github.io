<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2998 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2998</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2998</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-266999418</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.07764v2.pdf" target="_blank">When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment</a></p>
                <p><strong>Paper Abstract:</strong> AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2998.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2998.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mobile LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mobile Large Language Model Agent (local LLaMA-7B instance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A local, resource-constrained LLM agent deployed on mobile devices (example: LLaMA-7B) responsible for real-time multimodal perception, short-term/contextual memory, local grounding, and executing text/tool/embodied actions or offloading to edge agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mobile LLM agent (LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tiny local LLM (0-10B parameters; implemented using LLaMA-7B / Video-LLaMA for perception) combined with modality encoders (ImageBind/LanguageBind) to ingest multimodal sensory inputs, produce local scene descriptions, maintain short-term memory/context, and execute local alignment and actuation modules.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term / in-context memory (convertible to long-term via external storage)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Maintains short-term memory through in-context tokens and internal states; short-term memory (scene descriptions, recent interactions) can be uploaded to edge servers to be stored in long-term vector databases for retrieval-augmented use by edge agents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Accident report generation (Car Crash dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Perceive multimodal environment (video, image, audio, IMU, depth) on-vehicle, produce local scene descriptions and follow local plans (text, API calls, embodied actions), and offload intermediate results when confronted with complex multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multimodal perception + local response / tool use</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mobile agents keep short-term contextual state for immediate responsiveness and offload to edge for long-horizon reasoning; perception fidelity and ability to convert short-term to long-term memory (via edge vector DB) determine overall success in multi-round tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited context window and parameter count constrain long-horizon reasoning; short-term memory alone insufficient for complex grounding, requiring offload to edge (bandwidth and latency trade-offs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2998.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2998.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge Large Language Model Agent (global GPT-family instance, e.g., GPT-4/GPT-3.5-turbo; 'Accident Report Assistant')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large global LLM hosted at edge servers (>10B parameters) that provides long-term memory, retrieval-augmented generation, chain-of-thought reasoning, planning, and self-reflection to ground and refine mobile agents' outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Edge LLM agent (GPT-4 / Accident Report Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A global LLM deployed on edge servers (examples: GPT-3.5-turbo, GPT-4) that accesses stored historical interactions (long-term memory in vector DBs), performs RAG, CoT/ToT/GoT reasoning and self-reflection, and issues high-level plans back to mobile agents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented long-term memory stored in external vector databases</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Historical mobile-agent interactions are embedded and stored in vector databases; during inference the edge agent retrieves relevant memory vectors (RAG) and conditions generation on retrieved documents and chain-of-thought traces; edge agents also accumulate intermediate thoughts (CoT) whose freshness/value is tracked.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Accident report generation (Car Crash dataset) / multistep reasoning and planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aggregate local perception reports from multiple mobile agents, perform multi-step chain-of-thought reasoning and planning to produce a consolidated accident report and high-level action plans for local agents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / planning / document generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Edge LLMs leveraging long-term memory and RAG improve consistency and grounding for long-horizon, multi-agent tasks; CoT and self-reflection on the edge improve reasoning quality but increase computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires storage and retrieval infrastructure (vector DBs); retrieval errors or stale memory can hurt grounding; CoT and multi-path reasoning consume substantial context tokens and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2998.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2998.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmentation mechanism where a retrieval model fetches relevant external documents or memory entries (from a vector DB) which are then used as conditioning context for the LLM's generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Edge LLM agent (using RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Edge LLMs perform retrieval from vector databases containing embedded historical interactions and environment data, and incorporate retrieved content into generation to improve factuality, consistency, and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external database (vector DB)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Historical interactions are embedded and stored; during a query the retrieval model issues nearest-neighbor lookups to fetch relevant embeddings/documents; fetched items are concatenated or otherwise fused into the LLM input (RAG) to condition generation and support long-horizon consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Grounded report generation and multi-agent coordination (accident reports)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Support generation of consistent accident reports by retrieving past responses, guidelines, and contextual data to ground current output.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>document generation / grounding / long-horizon consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG enables edge agents to leverage long-term memory for improved consistency and domain-specialized grounding in complex tasks; it is a central mechanism in the proposed split system for preserving long-horizon context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dependency on retrieval quality and vector DB freshness; additional latency and bandwidth for retrieval and transferring retrieved context; design trade-offs between retrieval size and communication cost for mobile-edge interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2998.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2998.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AoT / LAoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Age-of-Thought (AoT) metric and Least Age-of-Thought (LAoT) model caching algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AoT quantifies the freshness/value of intermediate chain-of-thought units produced during CoT reasoning; LAoT uses AoT to evict cached global models whose accumulated thoughts are least valuable, optimizing edge model caching for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Edge LLM agent with LAoT caching</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Edge agents accumulate intermediate reasoning 'thoughts' during CoT; AoT scores each thought by freshness/importance; LAoT caching algorithm uses AoT-weighted value to decide which large global models to keep loaded in edge memory to minimize inference cost and accuracy loss across services.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>model-state + thought-augmented cache (meta-memory about intermediate CoT outputs stored/accumulated per model)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>During CoT inference, intermediate thoughts are recorded and assigned AoT values (older thoughts deemed less valuable). Edge servers maintain a model cache; LAoT evicts models whose cached thoughts have the least aggregate AoT value to free memory for more valuable models, balancing edge accuracy loss, model switching cost, and inference latency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Model caching for multi-service edge inference (evaluated on 30 service types, 10 edge agents; use case: accident-reporting pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Manage which global LLM models to keep loaded on limited-memory edge servers to serve multi-service LLM workloads, where historical CoT thoughts and their freshness influence model utility.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>infrastructure optimization / model management for reasoning agents</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LAoT (AoT-driven caching) reduces total execution cost and edge inference cost and improves edge accuracy compared to baselines (cloud-only inference, FIFO, LFU) by prioritizing models with fresher/more valuable accumulated thoughts; as time increases, popular models remain cached reducing switching cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No numeric performance values reported in text; effectiveness depends on how well AoT correlates with future utility; overhead of tracking and storing intermediate thoughts and AoT scores; context-window limits can still force eviction when token budgets are exceeded.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-based language models and applications <em>(Rating: 2)</em></li>
                <li>A survey of chain of thought reasoning: Advances, frontiers and future <em>(Rating: 2)</em></li>
                <li>The rise and potential of large language model based agents: A survey. <em>(Rating: 2)</em></li>
                <li>ToolLLM: Facilitating large language models to master 16000+ real-world APIs <em>(Rating: 1)</em></li>
                <li>Large language models empowered autonomous edge ai for connected intelligence. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2998",
    "paper_id": "paper-266999418",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "Mobile LLM agent",
            "name_full": "Mobile Large Language Model Agent (local LLaMA-7B instance)",
            "brief_description": "A local, resource-constrained LLM agent deployed on mobile devices (example: LLaMA-7B) responsible for real-time multimodal perception, short-term/contextual memory, local grounding, and executing text/tool/embodied actions or offloading to edge agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Mobile LLM agent (LLaMA-7B)",
            "agent_description": "Tiny local LLM (0-10B parameters; implemented using LLaMA-7B / Video-LLaMA for perception) combined with modality encoders (ImageBind/LanguageBind) to ingest multimodal sensory inputs, produce local scene descriptions, maintain short-term memory/context, and execute local alignment and actuation modules.",
            "memory_used": true,
            "memory_type": "short-term / in-context memory (convertible to long-term via external storage)",
            "memory_mechanism_description": "Maintains short-term memory through in-context tokens and internal states; short-term memory (scene descriptions, recent interactions) can be uploaded to edge servers to be stored in long-term vector databases for retrieval-augmented use by edge agents.",
            "task_name": "Accident report generation (Car Crash dataset)",
            "task_description": "Perceive multimodal environment (video, image, audio, IMU, depth) on-vehicle, produce local scene descriptions and follow local plans (text, API calls, embodied actions), and offload intermediate results when confronted with complex multi-step reasoning tasks.",
            "task_type": "multimodal perception + local response / tool use",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Mobile agents keep short-term contextual state for immediate responsiveness and offload to edge for long-horizon reasoning; perception fidelity and ability to convert short-term to long-term memory (via edge vector DB) determine overall success in multi-round tasks.",
            "limitations_or_challenges": "Limited context window and parameter count constrain long-horizon reasoning; short-term memory alone insufficient for complex grounding, requiring offload to edge (bandwidth and latency trade-offs).",
            "uuid": "e2998.0",
            "source_info": {
                "paper_title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Edge LLM agent",
            "name_full": "Edge Large Language Model Agent (global GPT-family instance, e.g., GPT-4/GPT-3.5-turbo; 'Accident Report Assistant')",
            "brief_description": "A large global LLM hosted at edge servers (&gt;10B parameters) that provides long-term memory, retrieval-augmented generation, chain-of-thought reasoning, planning, and self-reflection to ground and refine mobile agents' outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Edge LLM agent (GPT-4 / Accident Report Assistant)",
            "agent_description": "A global LLM deployed on edge servers (examples: GPT-3.5-turbo, GPT-4) that accesses stored historical interactions (long-term memory in vector DBs), performs RAG, CoT/ToT/GoT reasoning and self-reflection, and issues high-level plans back to mobile agents.",
            "memory_used": true,
            "memory_type": "retrieval-augmented long-term memory stored in external vector databases",
            "memory_mechanism_description": "Historical mobile-agent interactions are embedded and stored in vector databases; during inference the edge agent retrieves relevant memory vectors (RAG) and conditions generation on retrieved documents and chain-of-thought traces; edge agents also accumulate intermediate thoughts (CoT) whose freshness/value is tracked.",
            "task_name": "Accident report generation (Car Crash dataset) / multistep reasoning and planning",
            "task_description": "Aggregate local perception reports from multiple mobile agents, perform multi-step chain-of-thought reasoning and planning to produce a consolidated accident report and high-level action plans for local agents.",
            "task_type": "multi-step reasoning / planning / document generation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Edge LLMs leveraging long-term memory and RAG improve consistency and grounding for long-horizon, multi-agent tasks; CoT and self-reflection on the edge improve reasoning quality but increase computational cost.",
            "limitations_or_challenges": "Requires storage and retrieval infrastructure (vector DBs); retrieval errors or stale memory can hurt grounding; CoT and multi-path reasoning consume substantial context tokens and compute.",
            "uuid": "e2998.1",
            "source_info": {
                "paper_title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A memory-augmentation mechanism where a retrieval model fetches relevant external documents or memory entries (from a vector DB) which are then used as conditioning context for the LLM's generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Edge LLM agent (using RAG)",
            "agent_description": "Edge LLMs perform retrieval from vector databases containing embedded historical interactions and environment data, and incorporate retrieved content into generation to improve factuality, consistency, and grounding.",
            "memory_used": true,
            "memory_type": "retrieval-augmented external database (vector DB)",
            "memory_mechanism_description": "Historical interactions are embedded and stored; during a query the retrieval model issues nearest-neighbor lookups to fetch relevant embeddings/documents; fetched items are concatenated or otherwise fused into the LLM input (RAG) to condition generation and support long-horizon consistency.",
            "task_name": "Grounded report generation and multi-agent coordination (accident reports)",
            "task_description": "Support generation of consistent accident reports by retrieving past responses, guidelines, and contextual data to ground current output.",
            "task_type": "document generation / grounding / long-horizon consistency",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "RAG enables edge agents to leverage long-term memory for improved consistency and domain-specialized grounding in complex tasks; it is a central mechanism in the proposed split system for preserving long-horizon context.",
            "limitations_or_challenges": "Dependency on retrieval quality and vector DB freshness; additional latency and bandwidth for retrieval and transferring retrieved context; design trade-offs between retrieval size and communication cost for mobile-edge interaction.",
            "uuid": "e2998.2",
            "source_info": {
                "paper_title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AoT / LAoT",
            "name_full": "Age-of-Thought (AoT) metric and Least Age-of-Thought (LAoT) model caching algorithm",
            "brief_description": "AoT quantifies the freshness/value of intermediate chain-of-thought units produced during CoT reasoning; LAoT uses AoT to evict cached global models whose accumulated thoughts are least valuable, optimizing edge model caching for reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Edge LLM agent with LAoT caching",
            "agent_description": "Edge agents accumulate intermediate reasoning 'thoughts' during CoT; AoT scores each thought by freshness/importance; LAoT caching algorithm uses AoT-weighted value to decide which large global models to keep loaded in edge memory to minimize inference cost and accuracy loss across services.",
            "memory_used": true,
            "memory_type": "model-state + thought-augmented cache (meta-memory about intermediate CoT outputs stored/accumulated per model)",
            "memory_mechanism_description": "During CoT inference, intermediate thoughts are recorded and assigned AoT values (older thoughts deemed less valuable). Edge servers maintain a model cache; LAoT evicts models whose cached thoughts have the least aggregate AoT value to free memory for more valuable models, balancing edge accuracy loss, model switching cost, and inference latency.",
            "task_name": "Model caching for multi-service edge inference (evaluated on 30 service types, 10 edge agents; use case: accident-reporting pipeline)",
            "task_description": "Manage which global LLM models to keep loaded on limited-memory edge servers to serve multi-service LLM workloads, where historical CoT thoughts and their freshness influence model utility.",
            "task_type": "infrastructure optimization / model management for reasoning agents",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "LAoT (AoT-driven caching) reduces total execution cost and edge inference cost and improves edge accuracy compared to baselines (cloud-only inference, FIFO, LFU) by prioritizing models with fresher/more valuable accumulated thoughts; as time increases, popular models remain cached reducing switching cost.",
            "limitations_or_challenges": "No numeric performance values reported in text; effectiveness depends on how well AoT correlates with future utility; overhead of tracking and storing intermediate thoughts and AoT scores; context-window limits can still force eviction when token budgets are exceeded.",
            "uuid": "e2998.3",
            "source_info": {
                "paper_title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-based language models and applications",
            "rating": 2,
            "sanitized_title": "retrievalbased_language_models_and_applications"
        },
        {
            "paper_title": "A survey of chain of thought reasoning: Advances, frontiers and future",
            "rating": 2,
            "sanitized_title": "a_survey_of_chain_of_thought_reasoning_advances_frontiers_and_future"
        },
        {
            "paper_title": "The rise and potential of large language model based agents: A survey.",
            "rating": 2,
            "sanitized_title": "the_rise_and_potential_of_large_language_model_based_agents_a_survey"
        },
        {
            "paper_title": "ToolLLM: Facilitating large language models to master 16000+ real-world APIs",
            "rating": 1,
            "sanitized_title": "toolllm_facilitating_large_language_models_to_master_16000_realworld_apis"
        },
        {
            "paper_title": "Large language models empowered autonomous edge ai for connected intelligence.",
            "rating": 1,
            "sanitized_title": "large_language_models_empowered_autonomous_edge_ai_for_connected_intelligence"
        }
    ],
    "cost": 0.0096125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment
16 Feb 2024</p>
<p>Minrui Xu minrui001@e.ntu.edu.sg 
Fellow, IEEEDusit Niyato dniyato@ntu.edu.sg 
Jiawen Kang 
Zehui Xiong zehuixiong@sutd.edu.sg 
Fellow, IEEEShiwen Mao smao@ieee.org 
Fellow, IEEEZhu Han zhuhan22@gmail.com 
Fellow, IEEEDong In Kim dikim@skku.ac.kr 
Fellow, IEEEKhaled B Letaief 
In Dong 
Kim </p>
<p>School of Computer Science and Engineering
Nanyang Technological University
639798SingaporeSingapore</p>
<p>School of Automation
Guangdong University of Technology
China</p>
<p>Pillar of Information Systems Technology and Design
University of Technology
Singapore</p>
<p>Department of Electrical and Computer Engineering
Auburn University
36849-5201AuburnALUSA</p>
<p>Department of Electrical and Computer Engineering
University of Houston
77004HoustonTXUSA</p>
<p>Department of Computer Science and Engineering
Kyung Hee University
446-701SeoulSouth Korea</p>
<p>Department of Electrical and Computer Engineering
Sungkyunkwan University
16419SuwonSouth Korea</p>
<p>Department of Electrical and Computer Engineering
Hong Kong University of Science and Technology (HKUST)
Hong Kong</p>
<p>When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment
16 Feb 2024A9BA9EA5F22F30FF5A354B94F137402FarXiv:2401.07764v2[cs.AI]6G networksAI agentsintegrated sensing and communicationdigital twinstask-oriented communications
AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment.Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy.Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions.In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively.In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating intermodule communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications.Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.</p>
<p>I. INTRODUCTION</p>
<p>AI agents, designed to integrate AI models into everyday services as personal assistants to humans, have become a pivotal element in advancing towards artificial general intelligence (AGI) [1], [2].AI agents powered by large language models (LLMs), i.e., LLM agents, possess the capability to follow user instructions, observe environments, make decisions, and execute actions at a human-equivalent level.Therefore, LLM agents can proactively provide users with recommendations for final decisions by understanding and remembering cross-application user intentions and behaviors.Particularly, AI agents observe surrounding environments by processing information in various modalities from sensors, leveraging the versatility of multimodal LLMs [3].In addition, LLM agents can solve complex tasks by grounding the plan of action to achieve their missions through reasoning, memory, and verification.After the alignment between LLM agents and humans, agents can attain human-like intelligence to provide recommendations to users with text, tools, and embodied actions that are consistent with human values.</p>
<p>Although the deployment of LLM agents on mobile devices in 6G networks allows democratizing access to services currently considered prohibitively expensive at cloud data centers, several issues remain in implementing the LLM agents for complex, multi-round interaction agent services [4], [5].For mobile devices with limited capacities, running AI models of agents, which is both computation-and memory-intensive, is challenging for supporting the long-term execution of LLMs.In addition, these limitations are further exacerbated by the restricted context windows of LLMs, hindering LLM agents from performing long-term and complex interactions, such as perception, reasoning, and coding, which consume considerable available context resources [6].To address these challenges, a split learning system based on collaborative endedge-cloud computing, which aims at partitioning LLM agents into mobile and edge agents, emerges as a viable solution.In this system, mobile LLM agents, operating local LLMs (0-10B parameters, e.g., LLAMA-7B) on mobile devices, can handle real-time, direct perception and alignment tasks.Meanwhile, edge LLM agents, hosting global LLMs, (&gt;10B parameters, e.g., GPT-3) on edge servers, can utilize global information and historical memory to help mobile LLM agents perform complex tasks.</p>
<p>There are several advantages to partitioning LLM agents into mobile and edge agents in 6G networks.First, flexible deployment of LLM agents can be supported by heterogeneous devices with different locations, capabilities, and contextual adaptability.Specifically, mobile LLM agents with proper local LLMs can operate effectively with their computing capabilities regardless of their locations and user scenarios.Second, long-horizon collaboration can be enabled across multiple mobile devices by bridging the integration between lowlevel operational plans of local LLMs and high-level strategic plans of global LLMs.Third, mobile LLM agents exhibit enhanced adaptability in dynamic open-ended environments.</p>
<p>For instance, mobile LLM agents can understand instructions using local LLMs and then adjust their actions based on immediate environmental feedback for real-time responsiveness and relevance during their interactions with physical environments.</p>
<p>In this article, we propose a split learning system of LLM agents consisting of mobile LLM agents and edge LLM agents in 6G networks, which is democratic, flexible, and longhorizon for running sustainable AI agents in open-ended environments.First, we introduce the basic concept of AI agents and introduce the processes of constructing LLM agents via collaborative end-edge-cloud computing.Secondly, we discuss three main issues in developing LLM agents in 6G networks, including multimodal perception, interactive grounding, and alignment with humans.Thirdly, we investigate a real-world application that leverages mobile and edge LLM agents to generate accident reports collaboratively.At an accident site, vehicles can employ mobile LLM agents to observe the surrounding scene of a car accident and generate their local environmental descriptions.By sending these descriptions to edge servers, edge LLM agents can use global observations to deduce and offer more detailed and precise plans for vehicles.Finally, the mobile LLM agents can generate text responses, functional call requests, and embodied actions based on the global plan.In addition, we propose a metric called age of thought (AoT) to assess the significance of thoughts, i.e., the intermediate steps generated by LLMs, during the reasoning and planning processes of edge LLM agents.This metric emphasizes that older thoughts hold less importance and thus can ensure the high performance of cached models.Based on this metric, we introduce the Least Age-of-Thought (LAoT) model caching algorithm, which evicts global models that have the least impactful and relevant thoughts, and thus reduces the grounding cost in terms of latency, resource consumption, and performance loss for serving edge LLM agents in 6G networks.Overall, our main contribution can be summarized as follows.</p>
<p>• We propose a split learning system for LLM agents in 6G networks, which aims to provide democratic AI assistant services via the collaboration of mobile and edge LLM agents over end-edge-cloud computing.• During the integration of 6G networks and LLM agents, we discuss several major issues, including integrated sensing and communication for multimodal perception, digital twins for grounding decisions, and task-oriented communications for the alignment of agents.• We propose a new optimization framework in the system, i.e., model caching for AI agents, which aims at maximizing the in-context learning capabilities of LLM agents while reducing the network costs of serving mobile and edge LLM agents.</p>
<p>II. COLLABORATIVE END-EDGE-CLOUD COMPUTING FOR LLM AGENTS IN 6G NETWORKS</p>
<p>As a pivotal stride towards achieving AGI, AI agents are the key computational entities that can proactively perceive user instructions, observe the environment, ground decisions, and perform human-like actions [2].In 6G networks, AI agents are developed to execute intricate tasks collaboratively, from managing networks to acting as personal assistants for humans.According to the difference in fundamental working mechanisms, there are two major categories of AI agents, i.e., reinforcement learning (RL) agents and LLM agents, which are discussed below.</p>
<p>A. Categories of AI Agents 1) RL Agents: Utilizing RL algorithms to observe states, make decisions, and take actions in an environment, RL agents learn through trial and error, by receiving feedback as rewards or penalties as a result of their actions.They aim to maximize their cumulative reward over time by learning optimal policies.For example, in communications and networking, RL agents can make decisions for dynamic network access, transmit power control, wireless caching, and data offloading locally to maximize network performance under uncertain network environments.Specifically, RL agents formulate the communication and networking environment into a Markov decision process (MDP) consisting of states, actions, transition probabilities, and rewards.However, although RL agents learn to make decisions for network access and management [2], they cannot interact with humans and other agents using texts in open-ended environments which limits their potential to offer more diverse services that require understanding and responding to human instructions.</p>
<p>2) LLM Agents: To achieve human-level intelligence, LLM agents build upon versatile and powerful LLMs that have demonstrated remarkable capabilities in few-shot and zeroshot environment perception and instruction understanding [1], [2].In addition to the decision-making capabilities of RL agents, LLM agents can interact with the environment through texts, API tools, and embodied actions continuously while gradually improving their performance during the interaction.Meanwhile, pre-training on large-scale datasets elicits emerging abilities of LLMs, allowing them to tackle various downstream tasks related to data management, question answering, route planning, and scientific inquiries.Furthermore, equipped with memory, reasoning, planning, and tool capabilities, LLM agents can not only make decisions for network environments but also leverage language understanding and employ tools such as the Internet and databases for tackling complex control tasks.Compared with the generalization of RL agents, the role-playing capability of LLM agents allows them to serve specific roles while handling different tasks.For example, LLM agents can act as experiment assistants, automating the design, planning, and execution of scientific experiments based on human-crafted instructions.However, textual instructions are usually not sufficient for LLM agents to perceive the entire environment in a realistic setting.</p>
<p>To enhance LLMs with multi-sensory capabilities, such as visual and audio understanding, multimodal LLMs [3], like GPT-4V(ision), are introduced for agents to perceive and process inputs from multiple modalities, including tactile feedback, gestures, Inertial Measurement Units (IMUs) motion sensor data, and 3D maps.For visual input, multimodal LLMs can be leveraged to generate a description for the current environment, where they can produce multimodal descriptions, such as text, audio, and images, which enable better accessibility for visually impaired individuals and improve positioning capabilities.Specifically, multimodal LLM agents can use a pre-trained encoder to convert signals from different modalities into a common textual representation, allowing for reasoning across modalities [7].</p>
<p>B. Construction of Mobile Edge-empowered Agents</p>
<p>As illustrated in Fig. 1, the construction of LLM agents in collaborative end-edge-cloud computing consists of three main processes, namely, mobile LLM agent execution, edge LLM agent execution, and inter-agent communication between mobile agents and edge agents to update information and assign tasks.</p>
<p>1) Mobile LLM Agent Execution: Initially, each user downloads tiny local LLMs (0-10B), e.g., LLAMA-7B, to its mobile device from edge servers via radio access networks (RANs) for personalized initialization.During initialization, users can configure mobile LLM agents with personal profiles such as age, gender, and career, which agents use to tailor their interactions and responses with specific roles.In addition, mobile LLM agents can leverage contextual initialization based on the current situation by processing and analyzing historical interactions.There are two major methods for LLM agents to perceive environments, i.e., human instruction and sensing.On the one hand, human instructions are given through interactive dialogues between humans and LLM agents.On the other hand, LLM agents can perceive the physical environment, which provides multimodal sensory inputs from interacting objects, including visual, auditory, and spatial data.</p>
<p>To process the received instruction and multimodal sensing data, mobile LLM agents can utilize pre-trained components, such as modality encoders, word embedding layers, and projection layers, to combine multi-sensory inputs.Each modality encoder is specific to one modality, such as CLIP for images, CLAP for audio signals, IMU2CLIP for IMU motion sensor, and Intervideo for videos [8].In mobile devices, multiple encoders process and combine the multimodal input data and then project the output into the text token embedding space of local LLMs.To process human instructions, the word embedding layer is a crucial component that maps words or tokens into a continuous vector space, capturing semantic relationships between them, and helping in understanding userspecific instructions.In mobile AI agents, due to limited capacities in mobile devices, tiny local LLMs with a limited amount of parameters can generate real-time responses based on local perception but cannot tackle complex tasks that require comprehensive consideration and generalization.</p>
<p>2) Global Agent Execution: In edge servers, edge LLM agents with huge global LLMs (&gt;10B), e.g., GPT3, can leverage long-term memory, reasoning, and planning modules to enhance the quality of responses with global information and understanding of environments.Historical interactions of mobile LLM agents can be stored as long-term memory in vector databases through memory embedding layers.Based on the long-term memory from mobile LLM agents, edge LLM agents can use retrieval-augmented generation (RAG) to output responses with better performance and consistency [9].In addition, edge LLM agents can utilize chain-of-thought (CoT) reasoning to improve the performance in complex tasks [10].When tackling complex tasks, edge LLM agents using CoT start by employing various reasoning paths to deduce potential answers, considering that each complex problem has multiple ways of thinking.This way, edge LLM agents can adapt to unfamiliar scenarios through knowledge generalization and transfer abilities inherent in global LLMs.Furthermore, edge LLM agents can leverage self-reflection to verify reasoning paths, gaining more accurate results of their actions and making better decisions for future behaviors.</p>
<p>C. Inter-agent Communication between Local and Edge Agents</p>
<p>When mobile LLM agents are incapable of accomplishing complex tasks, they can offload the intermediate results, including local perceptions and user intentions, to edge LLM agents equipped with huge global LLMs and global information for remote execution.Mobile LLM agents can transmit intermediate results, such as text or other embeddings, through inter-agent communication over RANs.Due to limited bandwidth and uncertain wireless channels, mobile LLM agents need to optimize the size of the transmitted content, i.e., intermediate inference results of local LLMs, and configure communication parameters for successful offloading, e.g., the transmit power and the chosen channel.Based on the responses and decision results generated by edge LLM agents, mobile LLM agents adapt global general plans to the local specific plans to interact with users and the environments.After understanding the locally specific plans using local tiny LLMs, mobile LLM agents generate responses, use API tools, and perform embodied actions locally using their actuation modules.</p>
<p>III. ISAC FOR WIRELESS PERCEPTION: UBIQUITOUS AND ADAPTABILITY</p>
<p>To run LLM agents efficiently in 6G networks with ubiquitous low-end devices, mobile LLM agents can perceive user instructions and sense environments for modeling and understanding the current situation.In addition, to improve adaptability and generalization, mobile LLM agents need to offload computation-intensive and intractable tasks to edge LLM agents for remote execution.Therefore, mobile LLM agents need to collect and extract information from noisy observations and communicate with edge servers to transfer information, which requires the implementation of integrated sensing and communication (ISAC) by utilizing the wireless communication infrastructure.</p>
<p>1) Environmental Perception: In multi-functional 6G networks, mobile LLM agents can autonomously perceive the surrounding environment using equipped sensors [7], which consume network resources for supporting the sensing functionality.By integrating basic perceptual abilities such as vision, text, and light sensitivity, LLM agents can develop various user-friendly perception modules [11].For example, LLM agents in mobile devices can perceive more complex user inputs, such as eye-tracking, body motion capture, and even brainwave signals in brain-computer interaction.Furthermore, LLM agents in vehicular networks can be equipped with Lidar, GPS, and IMUs, allowing them to perceive location-based data for vehicles and mobile users.</p>
<p>2) Human-language Instruction: During the interaction between users and agents, text instructions can be given to mobile LLM agents by providing them with explicit requests as well as implied values and intentions.Mobile LLM agents can understand implicit meanings within textual input based on contextual interaction with users, thanks to their short-term memory.After processing through local LLMs, mobile LLM agents can respond with answers in human language.Additionally, users can also provide instructions via audio, which contains environmental information compared to text [7].Handling audio input involves leveraging existing models, cascading paradigms, and integrating audio with other modalities to enhance agents' perception and understanding of the environment.</p>
<p>3) Inter-agent Interactions: In the proposed system, ubiquitous interaction between mobile and edge LLM agents is crucial for offloading intermediate results, receiving feedback, interactive reasoning, and self-reflection over RANs [6].During collaboration between mobile and edge agents, they need to continuously communicate with each other with messages in text or other embedded formats in a noisy environment.Therefore, this communication process usually consumes a large amount of bandwidth and network resources for longterm and multimodal interactions.</p>
<p>For the wireless perception of LLM agents in multifunctional 6G networks, ISAC is promising to improve spectral and energy efficiencies for mobile LLM agents to collect information from environments and transmit intermediate results to edge LLM agents simultaneously.For example, mobile LLM agents in vehicles need to perform radar sensing and transmit the perception results to edge LLM agents simultaneously.By utilizing network resources more efficiently for sensing and communication, ubiquitous LLM agents can be deployed in wireless environments and become more adaptable to a dynamic and open-ended world.</p>
<p>IV. DIGITAL TWINS FOR WIRELESS GROUNDING: RELIABILITY AND CONSISTENCY</p>
<p>For grounding the responses and actions, mobile LLM agents maintain digital twins (DTs) at the edge servers to interactively perform retrieval-augmented generation (RAG), reasoning and planning, and reflection with edge LLM agents in 6G networks with hyper reliable and low-latency communication.DTs of mobile LLM agents are created as digital replicas of physical entities with perceived data and can help to perform global grounding with internal memory and external knowledge.Through continuously updating external observation and internal reasoning results, DTs of mobile LLM agents can be created for real-time monitoring, analysis, and optimization of the decisions of mobile agents while performing complex tasks.</p>
<p>1) Memory and Retrieval-Augmented Generation: In the proposed system, mobile LLM agents maintain a short-term memory while global agents maintain a long-term memory for grounding agents' responses and actions.In mobile LLM agents, the short-term memory is collected through various mechanisms, such as in-context learning, maintaining internal states, utilizing scene descriptions or environment feedback, and generating task plans.Additionally, short-term memory in mobile LLM agents can be converted to long-term memory by leveraging external storage resources in edge servers, such as vector databases, that allow rapid querying and retrieval of information as needed.Based on the long-term memory, edge LLM agents can perform RAG [9] to improve consistency in generation by using a retrieval model to retrieve relevant information from a knowledge base or a set of reference documents and then incorporating this retrieved information into the generation process.In this way, by processing retrieved content using global LLMs, RAG can be leveraged in complex and long-horizon tasks using specialized knowledge, up-to-date information, and customizable definitions for better performance and consistency.Specifically, edge LLM agents can access past responses of mobile LLM agents to ensure consistent collaboration.</p>
<p>2) Reasoning and Planning: Edge LLM agents can tackle complex tasks by decomposing them into sequential steps and sub-tasks to output accurate responses.To perform intricate reasoning, CoT [10] involving a step-by-step reasoning process along a single path can improve the reliability and interpretability of LLMs decisions.Specifically, edge LLM agents have the ability to use CoT to break down complex tasks and offer step-by-step instructions for mobile LLM agents to complete individual tasks.In addition, self-consistent CoT (CoT-SC) is proposed to improve performance of reasoning tasks by aggregating multiple language model outputs and selecting the most consistent answer through a majority vote.To extend CoT, tree-of-thoughts (ToT), a proposed extension of CoT that formulates thought units into a tree structure, allows LLMs to explore coherent thought units as intermediate steps, enabling better problem-solving and planning capabilities.Moreover, graph-of-thoughts (GoT) is a static structure that specifies the graph decomposition of a given task in the CoT paradigm.It prescribes the transformations to be applied to language model thoughts, along with their order and dependencies.Although these step-by-step reasoning and planning mechanisms allow for multiple choices at each step and mimic human thinking, they might request more computing resources from edge servers to generate the intermediate results compared with outputting the results directly.</p>
<p>3) Verification and Reflection: To ensure the correctness of the reasoning process before final response generation, LLM agents can leverage verification reflection to validate the correctness of each step in the CoT process.For example, SelfCheck [2] is a zero-shot checking scheme for LLMs that aims to improve question-answering accuracy by identifying errors in the LLM's reasoning process.It works as a step-bystep checker, individually checking each step in the LLM's reasoning process based on the available context.Using confidence scores as weights, SelfCheck allows for improved question-answering accuracy by focusing on the most accurate answer.Therefore, LLM agents can independently summarize and infer more abstract, complex, and high-level information.</p>
<p>Therefore, there should be a tradeoff between the performance of grounding modules and the computing resources consumed during the grounding processes.For inter-agent grounding, mobile and edge LLM agents can leverage their own self-correction capabilities to improve the accuracy of final decisions leveraging computing resources in mobile devices and edge servers.Moreover, inter-agent grounding requires additional networking resources to transmit correction results between mobile and edge LLM agents for crossverification for grounding the actions of LLM agents.</p>
<p>V. TASK-ORIENTED COMMUNICATIONS FOR WIRELESS ALIGNMENT: TRUSTWORTHY AND GENERALIZABILITY</p>
<p>In 6G networks with limited bandwidth resources, taskoriented communications [12] refer to a communication approach where the performance is measured based on the success level of achieving a sequence of application-related tasks, rather than traditional metrics such as data rate or wireless link reliability.The alignment of LLM agents in 6G networks offering ubiquitous connectivity can be regarded as a type of task-oriented communication where LLM agents can leverage the massive resources of mobile devices and edge servers to achieve their alignment goals.For instance, mobile LLM agents are the data destinations of global general plans from edge LLM agents to generate texts, call API functions, and perform embodied actions.In addition, feedback from humans and mobile LLM agents can be collected as datasets for supervised fine-tuning, reinforcement learning from human feedback (RLHF), and direct preference optimization (DPO) to regularize global LLMs.Beyond data-oriented communications, the accomplishment of alignment between mobile and edge LLM agents and humans in view of semantics is directly linked to task-oriented communication performance and strategies that wireless users and mobile and edge LLM agents can provide real-time evaluation and feedback.</p>
<p>1) Text Responses: Since LLMs are pre-trained on largescale datasets with biased data, a mismatch or distribution shift between the training and test data can cause LLMs to generate incorrect information, known as hallucination [13].However, to ensure that mobile and edge LLM agents align with human intentions and preferences, the system needs to reduce the likelihood of generating harmful outputs and improve usability by better following human instructions.For instance, Ope-nAI (https://openai.com/blog/introducing-superalignment), the creator of ChatGPT, has announced that they are going to leverage 20% of computing resources to fine-tune strong pretrained LLMs for regularizing LLMs to faithfully follow instructions or generate safe outputs.Fortunately, wireless alignment enables massive users to contribute their efforts and computing resources in alignment activities and contribute their efforts towards unlocking the full potential of LLMs while following human value and intentions, positively impacting various domains and enriching human experiences.</p>
<p>2) Tool Usage and Generation: By instruction fine-tuning on API datasets, mobile LLM agents should become proficient in leveraging tools and APIs to accomplish intricate tasks and interact with different virtual applications effectively based on general plans from edge LLM agents [14].Therefore, depending on specific environments and agent types, mobile LLM agents can be customized using local instruction Response: In the video, we see a car driving down the road next to a tall building in the city.The road is wide, and there are many trees and tall buildings visible in the background.The car appears to be white and it's driving slowly.There are no other vehicles on the road, and there are no pedestrians or bicycles in sight.The view of the building changes as the car moves down the road ...</p>
<p>Prompt :</p>
<p>Based on [description of the environment], how to generate an accident report for this car crash?Let's think step by step.Response: As a safety professional, I understand that you want me to help you generate an accident report based on the scenario provided.</p>
<p>Here's a step-by-step guide on how to create an accident report for the hypothetical car crash:</p>
<p>Step 1: Gather Information: ...</p>
<p>Step 2: Identify Parties Involved: ...</p>
<p>Step 3: Description of Accident: ...</p>
<p>Step 4: Investigation Findings: ...</p>
<p>Step 5: Conclusion &amp; Recommendations: ...</p>
<p>User: I want to get traffic incidents news in real-time reported by department of transportation.</p>
<p>Assistant: I will call the function 'Traffic_Incident_Report' next step."function_call": {"name": "Traffic_Incident_Report", "arguments": "{country, region}", Observation: {"report": {"type": "FeatureCollection", "features": [{"properties": {"location_description": "I-95N at MM 118.2", "mrm": "118.2","route": "I-95N", "priority": "Major", "icon": "incident"}, "geometry": {"coordinates": ["-77.513872","38.126552"], "type": "Point" }, "type": "Feature", "id": "INNO7328665401-11272023"}]} Prompt: Based on the guideline and [description of the environment], how to generate an accident report for this car crash?Let's think step by step.Response: Based on the information from the video and following the guideline for generating a car accident report, we can proceed step by step.However, it's important to note that the description provided does not mention a specific car crash.We'll create a hypothetical scenario based on the video description:</p>
<p>Step 1: Details of the Accident: ...</p>
<p>Step 2: Time, Date, and Location: ...</p>
<p>Step 3: Weather and Roadway Conditions: ...</p>
<p>Step 4: Driver's and Witness Statements: ...</p>
<p>Step 5: Accident Diagram: ...</p>
<p>Step 6: Violations and Citations: ...</p>
<p>Step 7: Officer's Findings and Conclusions: ... Since the description does not include a specific accident, the report is based on assumptions and hypothetical details.In a real situation, all the specific and factual details available at the scene would need to be accurately and thoroughly documented.</p>
<p>Global reasoning &amp; planing</p>
<p>Memory: Guideline_Car_Accident_Police_Report.pdf Prompt: Based on the knowledge, what is the guideline to generate an accident report for a car crash?Response: To generate an accident report for a car crash based on the guidelines provided in the document, you should include the following key elements: Details of the Accident: ..... Time, Date, and Location: ... Statements: ... Accident Diagram: ... Violations and Citations: ... Officer's Findings: ... Remember, police reports can contain both facts (like vehicle make and model, accident location and time, weather conditions) and opinions (such as a determination of fault, which is the opinion of the police officer) ...</p>
<p>Offload local reasoning results</p>
<p>Return global reasoning and planning results</p>
<p>Perception results</p>
<p>Reasoning and planning results</p>
<p>Mobile LLM agents</p>
<p>Edge LLM agents</p>
<p>Mobile LLM agents</p>
<p>Mobile</p>
<p>LLM agents</p>
<p>Fig. 2: An example of mobile and edge LLM agents used in the generation of accident reports for car crashes.fine-tuning datasets, which encompass real-world APIs and practical scenarios, to accomplish both single-tool and multitool tasks.Furthermore, mobile LLM agents can showcase exceptional adaptability when faced with unfamiliar APIs and tool-use datasets that are outside their usual field, particularly when users are inexperienced in the given environments.</p>
<p>3) Embodied Actions: To effectively interact with humans and physical environments, mobile LLM agents can perform embodied actions, including movements, gestures, or other physical behaviors [2] to interact with the physical world directly under the high-level plans from edge LLM agents.Embodied actions of mobile LLM agents are physically performed according to their design, mechanical features, and technology.For instance, vehicles can perform basic mechanical movements, such as driving forward and backward, tuning, braking, and accelerating.By performing these embodied actions, vehicles can easily adapt to road conditions, e.g., bumpy roads, slippery surfaces, and bad weather by adjusting internal temperature and air quality.To navigate through unfamiliar environments, robots with mobile LLM agents need to gather information, carry out tasks, and interact with other agents like humans.By performing embodied actions, mobile and edge LLM agents can extend their capabilities beyond digital boundaries, and interact and manipulate their physical surroundings directly.</p>
<p>VI. CASE STUDY OF MODEL CACHING FOR COLLABORATIVE MOBILE AND EDGE LLM AGENTS</p>
<p>In the split learning system of LLM agents over collaborative end-edge-cloud computing, each mobile AI agent is composed of a perception module, a local reasoning module, and an alignment module while each edge LLM agent consists of a global reasoning and planning module.In addition to allocating traditional computing, communication, and storage resources for executing LLM agents, the LLMs running in these agents are new resources to be allocated for performing contextual tasks of AI agents.Specifically, mobile LLM agents can leverage local LLMs for zero-shot environmental perception and auction, which is more comprehensive.Meanwhile, edge LLM agents with global LLMs can perform more intricate step-by-step reasoning and planning with global information for reliable and interpretative decision-making.</p>
<p>To construct the perception module of mobile LLM agents that can collect multimodal information from the environment, we leverage the ImageBind [8] and the LanguageBind [15] for sensing the environment.The ImageBind, based on ViT-Huge, and the LanguageBind, based on ViT-Large, use image embeddings as a central anchor to align embeddings from other modalities like text, audio, depth, thermal, and IMU data.The LanguageBind employs contrastive learning to align and bind different modalities including video, infrared, depth, and audio from the environment with the language modality.In this study, we evaluate the perception module using the IN1K dataset for image data, the K400 dataset for video data, the LLVIP dataset for infrared data, the NYU-D dataset for depth data, the ESC-50 dataset for audio data, and the Ego4D dataset for IMU data.</p>
<p>In this use case, we leverage mobile and edge LLM agents to generate accident reports collaboratively for car crashes, where multiple mobile agents perceive the environment and report their local observations to edge LLM agents.Meanwhile, edge LLM agents generate a general report by leveraging global information and high-level plans for mobile agents.We validate the performance of LLM agents based on the Car Crash dataset (https://github.com/Cogito2012/CarCrashDataset).The mobile LLM agents on vehicles are implemented based on After the perception of environments, each mobile LLM agent for perception describes the local situation and reports the current situation to the edge LLM agent.Based on the collected local perception, the edge LLM agent aggregates them into a comprehensive picture for the following multistep reasoning and planning processing, e.g., CoT.Finally, the edge LLM agent provides general plans to mobile LLM agents for actuation and lets them interact with users and environments with local plans translated by their local LLMs, including text responses, APIs, and embodied actions.Due to the limited context window of mobile and edge LLM agents, we consider the inference of perception and actuation to be zero-shot and their performance is determined by perception fidelity of multimodal information and successful ratio during interaction with users and environments.Furthermore, edge LLM agents can provide suggestions for multiple local agents and their performance is affected by their historical thoughts.As the CoT is a step-by-step inference process that generates multiple intermediate thoughts during the grounding of final decisions, the thought that is closer to the final decisions should contribute more value to making the final decisions.In this regard, we propose a metric of age-of-thought (AoT) to evaluate the value of thoughts based on their freshness.</p>
<p>With the limited memory of edge servers and the massive amount of parameters of LLMs, edge servers cannot load all the models into the main memory at the same time.To provide AI services to satisfy user requirements, edge servers need to schedule the global AI models for reasoning and planning for the requested services.To minimize the cost in terms of edge accuracy loss, model switching cost, edge inference cost, edge inference latency, and cloud inference cost, effective model caching algorithms should be designed to manage loaded models for edge LLM agents.Especially, the cached models not only can be evicted proactively according to the caching policies, but also can be evicted due to the used context exceeding the context window.Therefore, we propose an LAoT model caching algorithm based on the concept of AoT where the model with the least valuable thoughts is evicted.</p>
<p>The maximum token consumption for each CoT step is set to 200 in the experiment.The context window of LLaMA is 4K tokens, the context window of GPT-3.5-turbo is 16K tokens, and the context window of GPT-4 is 32K tokens.We consider an edge server with 64 GPUs with 80 GB memory, 312 TFLOPS, and 300W max thermal design power.We consider 30 types of services and 10 edge LLM agents.The experimental results are illustrated in Fig. 3.As the number of time slots increases, the cost of edge inference of LLM agents decreases due to less switching cost and higher model performance.The reason is that the popular models are loaded into the memory of edge servers.In addition, during the inference of edge LLM agents, the thoughts are accumulated to improve the reasoning and planning results and thus the edge accuracy loss is lower.Overall, we can observe that the LC algorithm can improve the accuracy of edge LLM agents while reducing total execution costs compared to the existing baselines, including cloud-only inference, the first-in-first-out (FIFO) policy, and the least frequently used (LFU) policy.</p>
<p>VII. CONCLUSIONS AND FUTURE DIRECTIONS</p>
<p>In this article, we have proposed a split learning system for LLM agents over collaborative end-edge-cloud computing in 6G networks for multimodal perception, interactive grounding, and alignment.We have introduced the evolution of LLM agents and the construction of LLM agents over end-edgecloud computing with collaborative mobile and edge LLM agents.Furthermore, we have investigated the communication and networking issues in developing mobile edge-empowered agents including perception, grounding, and alignment.Finally, we have developed a use case for the application of mobile and edge LLM agents in vehicular networks and propose a model caching algorithm to optimize the performance of AI agent services while reducing execution costs.</p>
<p>In future research, it is important to explore further integration of 6G networks and AI agents.This could involve incorporating next-generation multiple access, metasurface, and over-the-air computation to support LLM agents in dynamic wireless environments.Additionally, it is crucial to address the model privacy concerns that may arise during collaboration between mobile and edge LLM agents.This will help prevent any potential information breach, especially in cases where malicious edge servers may attempt to access users' private information from running models.</p>
<p>Fig. 1 :
1
Fig. 1: The split learning system of mobile and edge LLM agents over collaborative end-edge-cloud computing.</p>
<p>Please generate an accident report for this car crash.</p>
<p>Prompt:</p>
<p>Based on the description of the car crash, please generate a accident diagram and a piece of audio for the accident report.Response: Candidate APIs: Vehicle Communication, Traffic Management, Telematics, Mobile Notification, Geolocation, Healthcare and Hospital, Insurance Claim, Legal and Police Report Offload perception results</p>
<p>Fig. 3 :
3
Fig. 3: The execution cost of various model caching algorithms in different time slots.</p>
<p>TABLE I :
I
Evaluation of mobile LLM agents with different perception modules for different modalities.
ImageBindLanguageBindModelViT-Huge (632 M) ViT-Large/14 (307M)Image -IN1K77.7-Video -K40050.064.0Infrared -LLVIP63.487.2Depth -NYU-D54.065.1Audio -ESC-5066.989.8IMU -Ego4D25.0-LLaMA. The perception module is developed based on Video-LLaMA, the local grounding module is developed basedon LLaMA-7B-Chat, and the alignment module is devel-oped based on ToolLLM. The edge LLM agent on theedge server is developed based on GPT4 and we implementa GPT named "Accident Report Assistant" as the globalAI agent (https://chat.openai.com/g/g-7sWJT5dSD-accident-report-assistant). The actuation module is implemented withToolLLaMA, which is a fine-tuned LLaMA-7B model usingthe instruction-solution pairs.</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023arXiv preprint</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v (ision). Z Yang, L Li, K Lin, J Wang, C.-C Lin, Z Liu, L Wang, arXiv:2309.174212023arXiv preprint</p>
<p>Large language models empowered autonomous edge ai for connected intelligence. Y Shen, J Shao, X Zhang, Z Lin, H Pan, D Li, J Zhang, K B Letaief, IEEE Communications Magazine. 2024</p>
<p>Pushing large language models to the 6G edge: Vision, challenges, and opportunities. Z Lin, G Qu, Q Chen, X Chen, Z Chen, K Huang, arXiv:2309.167392023arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Q Wu, G Bansal, J Zhang, Y Wu, S Zhang, E Zhu, B Li, L Jiang, X Zhang, C Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Anymal: An efficient and scalable any-modality augmented language model. S Moon, A Madotto, Z Lin, T Nagarajan, M Smith, S Jain, C.-F Yeh, P Murugesan, P Heidari, Y Liu, arXiv:2309.160582023arXiv preprint</p>
<p>Imagebind: One embedding space to bind them all. R Girdhar, A El-Nouby, Z Liu, M Singh, K V Alwala, A Joulin, I Misra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Retrieval-based language models and applications. A Asai, S Min, Z Zhong, D Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics20236Tutorial Abstracts)</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, arXiv:2309.154022023arXiv preprint</p>
<p>Integrated sensing and communications: Toward dual-functional wireless networks for 6G and beyond. F Liu, Y Cui, C Masouros, J Xu, T X Han, Y C Eldar, S Buzzi, IEEE Journal on Selected Areas in Communications. 4062022</p>
<p>Taskoriented communications for 6G: Vision, principles, and technologies. Y Shi, Y Zhou, D Wen, Y Wu, C Jiang, K B Letaief, IEEE Wireless Communications. 3032023</p>
<p>Trustworthy llms: a survey and guideline for evaluating large language models' alignment. Y Liu, Y Yao, J.-F Ton, X Zhang, R G H Cheng, Y Klochkov, M F Taufiq, H Li, arXiv:2308.053742023arXiv preprint</p>
<p>Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. B Zhu, B Lin, M Ning, Y Yan, J Cui, H Wang, Y Pang, W Jiang, J Zhang, Z Li, arXiv:2310.018522023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4494 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4494</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4494</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-3e52bce2f3840c43fbdbcdf7320aaa68aeb5fdde</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3e52bce2f3840c43fbdbcdf7320aaa68aeb5fdde" target="_blank">Automatically Learning Hybrid Digital Twins of Dynamical Systems</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an evolutionary algorithm that employs Large Language Models to autonomously propose, evaluate, and optimize HDTwins, and reveals that HDTwinGen produces generalizable, sample-efficient, and evolvable models, significantly advancing DTs' efficacy in real-world applications.</p>
                <p><strong>Paper Abstract:</strong> Digital Twins (DTs) are computational models that simulate the states and temporal dynamics of real-world systems, playing a crucial role in prediction, understanding, and decision-making across diverse domains. However, existing approaches to DTs often struggle to generalize to unseen conditions in data-scarce settings, a crucial requirement for such models. To address these limitations, our work begins by establishing the essential desiderata for effective DTs. Hybrid Digital Twins ($\textbf{HDTwins}$) represent a promising approach to address these requirements, modeling systems using a composition of both mechanistic and neural components. This hybrid architecture simultaneously leverages (partial) domain knowledge and neural network expressiveness to enhance generalization, with its modular design facilitating improved evolvability. While existing hybrid models rely on expert-specified architectures with only parameters optimized on data, $\textit{automatically}$ specifying and optimizing HDTwins remains intractable due to the complex search space and the need for flexible integration of domain priors. To overcome this complexity, we propose an evolutionary algorithm ($\textbf{HDTwinGen}$) that employs Large Language Models (LLMs) to autonomously propose, evaluate, and optimize HDTwins. Specifically, LLMs iteratively generate novel model specifications, while offline tools are employed to optimize emitted parameters. Correspondingly, proposed models are evaluated and evolved based on targeted feedback, enabling the discovery of increasingly effective hybrid models. Our empirical results reveal that HDTwinGen produces generalizable, sample-efficient, and evolvable models, significantly advancing DTs' efficacy in real-world applications.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4494.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4494.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HDTwinGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid Digital Twin Generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary framework that uses LLM-based agents to autonomously propose symbolic mechanistic components and neural components (hybrid models), fit parameters with offline optimizers, and iteratively refine model structure using natural-language feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HDTwinGen (LLM-driven evolutionary HDTwin synthesizer)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>HDTwinGen represents hybrid model specifications as executable code and runs an iterative evolutionary loop: a modeling agent (an LLM) generates model specifications (Python code with symbolic mechanistic components and PyTorch neural residuals), an offline optimizer fits continuous parameters (Adam SGD on training trajectories), the model is evaluated on validation data producing component-wise and overall MSE, and an evaluation agent (an LLM) produces targeted natural-language feedback to guide the next generation. Top-K models are retained as context for next LLM prompts enabling in-context learning and directed search for closed-form ODE components and hybrid architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4-1106-Preview (used as the LLM agent in implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dynamical systems / computational modeling (applications shown in pharmacology (cancer PKPD), epidemiology (COVID-19), and ecology (plankton and hare-lynx)), i.e., systems biology, epidemiology, ecology, and ML/modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>not applicable (system operates on datasets and prompts; no corpus-of-papers processing reported)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Mechanistic (closed-form) dynamical equations (ODEs) and hybrid mechanistic+neural residual models (additive decomposition); discovery/selection of symbolic functional forms for state dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Paper illustrates mechanistic forms used in datasets (e.g. logistic growth: dN/dt = r N (1 - N/K); cancer PKPD ODE: dx/dt = (rho * log(K/x(t)) - beta_c * C(t) - (alpha_r d(t) + beta_r d(t)^2)) * x(t)). HDTwinGen aims to propose similar closed-form mechanistic components and hybrid residuals in code.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM-based code generation: LLM modeling agent is prompted with a structured system context (variable descriptions, skeleton code, dataset stats, qualitative requirements) and previous top-K models; it synthesizes Python code representing symbolic mechanistic terms and neural modules. Parameters emitted as placeholders are then fit numerically to trajectory data (MSE minimization with Adam). The evaluation agent (LLM) inspects model code and validation metrics and issues natural-language recommendations which are fed back to the modeling agent. No automated parsing of external papers is described; LLM injects domain priors via prompts and context.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Empirical held-out validation and test evaluation on trajectory datasets (train/val/test splits). Component-wise and overall validation mean squared error (MSE) used as outer/selection metric. Comparisons made to baselines (SINDy, GP, DyNODE, RNN, Transformer, APHYNITY) and ablations (ZeroShot, ZeroOptim). Out-of-distribution (OOD) robustness and sample-efficiency experiments performed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean squared error (MSE) on held-out test trajectories (reported per dataset in Table 1 and Table 2); component-wise validation MSE and overall validation MSE used within the evolutionary loop. (The paper reports dataset-specific numerical MSEs in tables; HDTwinGen attains the lowest reported test MSE across evaluated benchmarks.)</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>not reported (no explicit fraction/percentage of 'correct' discovered laws provided); evaluation is in terms of predictive MSE and empirical robustness rather than binary correctness of recovered analytic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on human-provided structured prompt/context and the underlying LLM's domain knowledge; current implementation limited to continuous-time dynamical systems; potential risk of bias from black-box LLMs being reflected in generated mechanistic components; no claim of guaranteed formal discovery of true governing equations (evaluation focuses on predictive performance and robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared empirically against equation-discovery and modeling baselines: Sparse Identification of Nonlinear Dynamics (SINDy), Genetic Programming (GP), DyNODE (neural ODE control), RNN, Transformer, APHYNITY (hybrid ODE+NN regularized); HDTwinGen reports lower test MSE and better OOD robustness and sample efficiency in the presented benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatically Learning Hybrid Digital Twins of Dynamical Systems', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4494.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4494.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4-1106-Preview (LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4-1106-Preview (used as modeling and evaluation LLM agents in HDTwinGen)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The specific LLM used by the authors as the modeling and evaluation agents that generate symbolic mechanistic code and natural-language model-refinement feedback within HDTwinGen's evolutionary loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM modeling/evaluation agents (prompt-driven code synthesis and critique)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Configured as two agents: (1) modeling agent synthesizes Python code (symbolic mechanistic formulas + PyTorch neural modules) given structured prompts (system description, skeleton code, dataset stats, prior models) and top-K memory; (2) evaluation agent analyzes produced models and validation metrics and produces dense natural-language feedback to guide subsequent generations. LLMs operate by in-context learning and chain-of-thought style reasoning embodied in the iterative prompt-execute-evaluate loop. Temperature reported as 0.7 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4-1106-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used in engineering of models for dynamical systems spanning pharmacology (cancer), epidemiology (COVID-19), and ecology (plankton, hare-lynx).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>not applicable (LLM used to generate model code for datasets; paper does not report corpus-level literature extraction by this LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Synthesizes candidate symbolic mechanistic relations (closed-form ODE terms) and hybrid residual networks; not described as directly mining published papers for laws in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>No explicit novel analytic laws extracted from literature are reported as discovered by the LLM; LLM outputs are executable mechanistic components and neural residuals suitable for ODE dynamics (examples of dataset mechanistic equations are provided elsewhere in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompt engineering with a structured modeling context and code skeleton: LLM emits executable code containing symbolic equations (represented as Python expressions) which are then executed and fitted to data. The process can be seen as distilling domain priors into code rather than parsing external literature in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Generated code is executed, its parameters fit on training trajectories (Adam optimizer), and evaluated on validation/test splits; evaluation metrics (component-wise MSE and overall MSE) are returned to the LLM evaluation agent for feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported specifically for the LLM itself; overall system metrics reported are predictive MSE on benchmark datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM performance depends on quality of system context prompts and prior knowledge encoded in prompts; potential hallucination or introduction of biased/inaccurate mechanistic terms if LLM domain knowledge is insufficient; no automated provenance-tracking of factual claims from literature is demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Paper includes ablations with different LLMs and temperature settings (Appendix H.6) showing compatibility, and compares the whole HDTwinGen pipeline (which uses GPT4-1106-Preview) against non-LLM equation-discovery and neural baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatically Learning Hybrid Digital Twins of Dynamical Systems', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4494.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4494.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ref77 (LLM for dynamical systems discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-driven discovery of dynamical systems in pharmacology using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that, by title, indicates use of large language models for data-driven discovery of dynamical systems in pharmacology; included in the paper's references as related work pointing to LLMs applied to discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data-driven discovery of dynamical systems in pharmacology using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>not specified in this paper (reference only)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Paper is cited as related work; no method details are given in the body of the current paper. The citation implies LLMs have been applied to discover dynamical systems in pharmacology, but the present paper does not summarize methodology or results for that citation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Pharmacology / dynamical systems discovery</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>1 (single cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Implied: discovery of dynamical-system equations (closed-form ODEs) relevant to pharmacology</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>not described here (only cited)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>not described here (only cited)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatically Learning Hybrid Digital Twins of Dynamical Systems', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4494.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4494.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SINDy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Identification of Nonlinear Dynamics (SINDy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An equation-discovery method that constructs a library of candidate functions and uses sparse regression to identify parsimonious closed-form dynamical equations from time-series data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering governing equations from data by sparse identification of nonlinear dynamical systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SINDy (sparse regression over candidate function libraries)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SINDy builds a feature/library of candidate basis functions (polynomials, cross-terms, etc.), computes time derivatives from data (finite differences), and performs sparse regression to select a small subset that reconstructs the dynamics as closed-form ODEs. In this paper SINDy is used as a baseline equation-discovery method.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dynamical systems, system identification, physics/engineering/biology applications</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>not applicable (SINDy is a method cited and used as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Closed-form ODEs (sparse analytic expressions for time derivatives)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not specific to this paper beyond the general method; SINDy was run with a polynomial library of order two in experiments (features: {1, x0, x1, x0*x1}).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Finite-difference derivative approximation from trajectories + sparse regression over a manually chosen function library.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>In this paper SINDy is applied to benchmark datasets and evaluated by test-set MSE (same metrics as other baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test prediction MSE (reported in Table 1); SINDy numbers shown per-dataset in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>not reported as a percentage of correct laws; performance summarized by MSE on datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Noted in the paper: SINDy (and symbolic methods generally) struggles to scale to higher-dimensional settings and relies on expert choices for variable selection and the function library primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly in benchmarks to HDTwinGen, GP, DyNODE, RNN, Transformer, APHYNITY; SINDy typically performed worse than HDTwinGen on these empirical predictive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatically Learning Hybrid Digital Twins of Dynamical Systems', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4494.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4494.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP (Genetic Programming / symbolic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming / Symbolic Regression (e.g., Eureqa-style methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evolutionary symbolic-regression methods that search over mathematical expressions (trees) to find analytic relationships that fit data, often used to distill empirical laws from experimental observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Genetic programming as a means for programming computers by natural selection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic programming / symbolic regression baseline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implemented as a baseline in experiments (using existing GP/symbolic regression implementations) to search for closed-form ODEs from trajectory data; in the paper GP is compared on predictive MSE across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Symbolic regression applicable across physics, biology, ecology, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>not applicable (method cited and used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Closed-form analytic expressions (symbolic regression outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>No specific discovered equations from GP are detailed in the main text beyond dataset-level results in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Evolutionary search over expression trees (symbolic operators and terms) to minimize data-fitting loss (used here as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Evaluated on held-out trajectory test sets via MSE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test MSE reported in Table 1 for GP baseline across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>GP/symbolic methods can be computationally expensive and unstable; depend on choice of primitives and can yield overfit or uninterpretable expressions without strong regularization; noted to have high variance across runs in some reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in experiments to HDTwinGen and other baselines; HDTwinGen outperformed GP on the presented benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatically Learning Hybrid Digital Twins of Dynamical Systems', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4494.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4494.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schmidt & Lipson 2009</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distilling free-form natural laws from experimental data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundational symbolic-regression work that introduced methods to recover free-form physical laws from experimental time-series using symbolic regression techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling free-form natural laws from experimental data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eureqa-style symbolic regression (historical reference)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a canonical reference in the literature on discovering governing equations from data. The present paper references it in the context of symbolic/equation discovery methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics / experimental sciences / system identification</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>1 (canonical reference)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Free-form analytic physical laws (symbolic expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Classic examples in that literature include recovered conserved quantities or differential equations, but no specific equations from Schmidt & Lipson are reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Symbolic regression over candidate functional forms using evolutionary/search methods (historical reference; no new extraction in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described here (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Noted generally (and implied in this paper) that symbolic discovery methods face scaling challenges to higher-dimensional systems and require careful feature/library selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Referenced as background; current paper compares modern baselines (SINDy, GP) against HDTwinGen.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatically Learning Hybrid Digital Twins of Dynamical Systems', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4494.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4494.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Generative Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep generative symbolic regression (recent methods for recovering mathematical expressions from data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods (and specifically a cited paper) that employ deep learning and RL-style or generative approaches to guide symbolic regression toward recovering analytic expressions from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep generative symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep generative symbolic regression (cited approach)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of recent approaches that combine deep models and symbolic regression to recover mathematical expressions from data; not used directly in experiments but placed in related work on equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / symbolic regression / system identification</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>1 (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Symbolic mathematical expressions / analytic formulas recovered from data</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not provided in this paper (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Not specified in the present paper (reference only); these methods typically use neural sequence or generative models to propose candidate symbolic expressions and optimize them with RL or gradient-based search.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned in the paper's related-work context: symbolic methods can struggle to scale and typically require manual specification of function primitives or decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cited as related work; HDTwinGen is presented as an alternative that leverages LLMs for code-based specification and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatically Learning Hybrid Digital Twins of Dynamical Systems', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling free-form natural laws from experimental data <em>(Rating: 2)</em></li>
                <li>Discovering governing equations from data by sparse identification of nonlinear dynamical systems <em>(Rating: 2)</em></li>
                <li>D-CODE: Discovering closed-form ODEs from observed trajectories <em>(Rating: 2)</em></li>
                <li>Deep generative symbolic regression <em>(Rating: 2)</em></li>
                <li>Data-driven discovery of dynamical systems in pharmacology using large language models <em>(Rating: 2)</em></li>
                <li>Large language models to enhance bayesian optimization <em>(Rating: 1)</em></li>
                <li>Large language models as optimizers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4494",
    "paper_id": "paper-3e52bce2f3840c43fbdbcdf7320aaa68aeb5fdde",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "HDTwinGen",
            "name_full": "Hybrid Digital Twin Generator",
            "brief_description": "An evolutionary framework that uses LLM-based agents to autonomously propose symbolic mechanistic components and neural components (hybrid models), fit parameters with offline optimizers, and iteratively refine model structure using natural-language feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "HDTwinGen (LLM-driven evolutionary HDTwin synthesizer)",
            "system_description": "HDTwinGen represents hybrid model specifications as executable code and runs an iterative evolutionary loop: a modeling agent (an LLM) generates model specifications (Python code with symbolic mechanistic components and PyTorch neural residuals), an offline optimizer fits continuous parameters (Adam SGD on training trajectories), the model is evaluated on validation data producing component-wise and overall MSE, and an evaluation agent (an LLM) produces targeted natural-language feedback to guide the next generation. Top-K models are retained as context for next LLM prompts enabling in-context learning and directed search for closed-form ODE components and hybrid architectures.",
            "model_name": "GPT4-1106-Preview (used as the LLM agent in implementation)",
            "model_size": null,
            "scientific_domain": "Dynamical systems / computational modeling (applications shown in pharmacology (cancer PKPD), epidemiology (COVID-19), and ecology (plankton and hare-lynx)), i.e., systems biology, epidemiology, ecology, and ML/modeling.",
            "number_of_papers": "not applicable (system operates on datasets and prompts; no corpus-of-papers processing reported)",
            "law_type": "Mechanistic (closed-form) dynamical equations (ODEs) and hybrid mechanistic+neural residual models (additive decomposition); discovery/selection of symbolic functional forms for state dynamics.",
            "law_examples": "Paper illustrates mechanistic forms used in datasets (e.g. logistic growth: dN/dt = r N (1 - N/K); cancer PKPD ODE: dx/dt = (rho * log(K/x(t)) - beta_c * C(t) - (alpha_r d(t) + beta_r d(t)^2)) * x(t)). HDTwinGen aims to propose similar closed-form mechanistic components and hybrid residuals in code.",
            "extraction_method": "LLM-based code generation: LLM modeling agent is prompted with a structured system context (variable descriptions, skeleton code, dataset stats, qualitative requirements) and previous top-K models; it synthesizes Python code representing symbolic mechanistic terms and neural modules. Parameters emitted as placeholders are then fit numerically to trajectory data (MSE minimization with Adam). The evaluation agent (LLM) inspects model code and validation metrics and issues natural-language recommendations which are fed back to the modeling agent. No automated parsing of external papers is described; LLM injects domain priors via prompts and context.",
            "validation_approach": "Empirical held-out validation and test evaluation on trajectory datasets (train/val/test splits). Component-wise and overall validation mean squared error (MSE) used as outer/selection metric. Comparisons made to baselines (SINDy, GP, DyNODE, RNN, Transformer, APHYNITY) and ablations (ZeroShot, ZeroOptim). Out-of-distribution (OOD) robustness and sample-efficiency experiments performed.",
            "performance_metrics": "Mean squared error (MSE) on held-out test trajectories (reported per dataset in Table 1 and Table 2); component-wise validation MSE and overall validation MSE used within the evolutionary loop. (The paper reports dataset-specific numerical MSEs in tables; HDTwinGen attains the lowest reported test MSE across evaluated benchmarks.)",
            "success_rate": "not reported (no explicit fraction/percentage of 'correct' discovered laws provided); evaluation is in terms of predictive MSE and empirical robustness rather than binary correctness of recovered analytic laws.",
            "challenges_limitations": "Relies on human-provided structured prompt/context and the underlying LLM's domain knowledge; current implementation limited to continuous-time dynamical systems; potential risk of bias from black-box LLMs being reflected in generated mechanistic components; no claim of guaranteed formal discovery of true governing equations (evaluation focuses on predictive performance and robustness).",
            "comparison_baseline": "Compared empirically against equation-discovery and modeling baselines: Sparse Identification of Nonlinear Dynamics (SINDy), Genetic Programming (GP), DyNODE (neural ODE control), RNN, Transformer, APHYNITY (hybrid ODE+NN regularized); HDTwinGen reports lower test MSE and better OOD robustness and sample efficiency in the presented benchmarks.",
            "uuid": "e4494.0",
            "source_info": {
                "paper_title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT4-1106-Preview (LLM agents)",
            "name_full": "GPT4-1106-Preview (used as modeling and evaluation LLM agents in HDTwinGen)",
            "brief_description": "The specific LLM used by the authors as the modeling and evaluation agents that generate symbolic mechanistic code and natural-language model-refinement feedback within HDTwinGen's evolutionary loop.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM modeling/evaluation agents (prompt-driven code synthesis and critique)",
            "system_description": "Configured as two agents: (1) modeling agent synthesizes Python code (symbolic mechanistic formulas + PyTorch neural modules) given structured prompts (system description, skeleton code, dataset stats, prior models) and top-K memory; (2) evaluation agent analyzes produced models and validation metrics and produces dense natural-language feedback to guide subsequent generations. LLMs operate by in-context learning and chain-of-thought style reasoning embodied in the iterative prompt-execute-evaluate loop. Temperature reported as 0.7 in experiments.",
            "model_name": "GPT4-1106-Preview",
            "model_size": null,
            "scientific_domain": "Used in engineering of models for dynamical systems spanning pharmacology (cancer), epidemiology (COVID-19), and ecology (plankton, hare-lynx).",
            "number_of_papers": "not applicable (LLM used to generate model code for datasets; paper does not report corpus-level literature extraction by this LLM)",
            "law_type": "Synthesizes candidate symbolic mechanistic relations (closed-form ODE terms) and hybrid residual networks; not described as directly mining published papers for laws in this work.",
            "law_examples": "No explicit novel analytic laws extracted from literature are reported as discovered by the LLM; LLM outputs are executable mechanistic components and neural residuals suitable for ODE dynamics (examples of dataset mechanistic equations are provided elsewhere in the paper).",
            "extraction_method": "Prompt engineering with a structured modeling context and code skeleton: LLM emits executable code containing symbolic equations (represented as Python expressions) which are then executed and fitted to data. The process can be seen as distilling domain priors into code rather than parsing external literature in the reported experiments.",
            "validation_approach": "Generated code is executed, its parameters fit on training trajectories (Adam optimizer), and evaluated on validation/test splits; evaluation metrics (component-wise MSE and overall MSE) are returned to the LLM evaluation agent for feedback.",
            "performance_metrics": "Not reported specifically for the LLM itself; overall system metrics reported are predictive MSE on benchmark datasets.",
            "success_rate": "not reported",
            "challenges_limitations": "LLM performance depends on quality of system context prompts and prior knowledge encoded in prompts; potential hallucination or introduction of biased/inaccurate mechanistic terms if LLM domain knowledge is insufficient; no automated provenance-tracking of factual claims from literature is demonstrated.",
            "comparison_baseline": "Paper includes ablations with different LLMs and temperature settings (Appendix H.6) showing compatibility, and compares the whole HDTwinGen pipeline (which uses GPT4-1106-Preview) against non-LLM equation-discovery and neural baselines.",
            "uuid": "e4494.1",
            "source_info": {
                "paper_title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Ref77 (LLM for dynamical systems discovery)",
            "name_full": "Data-driven discovery of dynamical systems in pharmacology using large language models",
            "brief_description": "A cited work that, by title, indicates use of large language models for data-driven discovery of dynamical systems in pharmacology; included in the paper's references as related work pointing to LLMs applied to discovery tasks.",
            "citation_title": "Data-driven discovery of dynamical systems in pharmacology using large language models",
            "mention_or_use": "mention",
            "system_name": "not specified in this paper (reference only)",
            "system_description": "Paper is cited as related work; no method details are given in the body of the current paper. The citation implies LLMs have been applied to discover dynamical systems in pharmacology, but the present paper does not summarize methodology or results for that citation.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Pharmacology / dynamical systems discovery",
            "number_of_papers": "1 (single cited work)",
            "law_type": "Implied: discovery of dynamical-system equations (closed-form ODEs) relevant to pharmacology",
            "law_examples": "not provided in this paper",
            "extraction_method": "not described here (only cited)",
            "validation_approach": "not described here (only cited)",
            "performance_metrics": "not described here",
            "success_rate": "not described here",
            "challenges_limitations": "not described here",
            "comparison_baseline": "not described here",
            "uuid": "e4494.2",
            "source_info": {
                "paper_title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SINDy",
            "name_full": "Sparse Identification of Nonlinear Dynamics (SINDy)",
            "brief_description": "An equation-discovery method that constructs a library of candidate functions and uses sparse regression to identify parsimonious closed-form dynamical equations from time-series data.",
            "citation_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "mention_or_use": "mention",
            "system_name": "SINDy (sparse regression over candidate function libraries)",
            "system_description": "SINDy builds a feature/library of candidate basis functions (polynomials, cross-terms, etc.), computes time derivatives from data (finite differences), and performs sparse regression to select a small subset that reconstructs the dynamics as closed-form ODEs. In this paper SINDy is used as a baseline equation-discovery method.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dynamical systems, system identification, physics/engineering/biology applications",
            "number_of_papers": "not applicable (SINDy is a method cited and used as a baseline)",
            "law_type": "Closed-form ODEs (sparse analytic expressions for time derivatives)",
            "law_examples": "Not specific to this paper beyond the general method; SINDy was run with a polynomial library of order two in experiments (features: {1, x0, x1, x0*x1}).",
            "extraction_method": "Finite-difference derivative approximation from trajectories + sparse regression over a manually chosen function library.",
            "validation_approach": "In this paper SINDy is applied to benchmark datasets and evaluated by test-set MSE (same metrics as other baselines).",
            "performance_metrics": "Test prediction MSE (reported in Table 1); SINDy numbers shown per-dataset in Table 1.",
            "success_rate": "not reported as a percentage of correct laws; performance summarized by MSE on datasets.",
            "challenges_limitations": "Noted in the paper: SINDy (and symbolic methods generally) struggles to scale to higher-dimensional settings and relies on expert choices for variable selection and the function library primitives.",
            "comparison_baseline": "Compared directly in benchmarks to HDTwinGen, GP, DyNODE, RNN, Transformer, APHYNITY; SINDy typically performed worse than HDTwinGen on these empirical predictive tasks.",
            "uuid": "e4494.3",
            "source_info": {
                "paper_title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GP (Genetic Programming / symbolic regression)",
            "name_full": "Genetic Programming / Symbolic Regression (e.g., Eureqa-style methods)",
            "brief_description": "Evolutionary symbolic-regression methods that search over mathematical expressions (trees) to find analytic relationships that fit data, often used to distill empirical laws from experimental observations.",
            "citation_title": "Genetic programming as a means for programming computers by natural selection",
            "mention_or_use": "mention",
            "system_name": "Genetic programming / symbolic regression baseline",
            "system_description": "Implemented as a baseline in experiments (using existing GP/symbolic regression implementations) to search for closed-form ODEs from trajectory data; in the paper GP is compared on predictive MSE across datasets.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Symbolic regression applicable across physics, biology, ecology, etc.",
            "number_of_papers": "not applicable (method cited and used as baseline)",
            "law_type": "Closed-form analytic expressions (symbolic regression outputs)",
            "law_examples": "No specific discovered equations from GP are detailed in the main text beyond dataset-level results in Table 1.",
            "extraction_method": "Evolutionary search over expression trees (symbolic operators and terms) to minimize data-fitting loss (used here as baseline).",
            "validation_approach": "Evaluated on held-out trajectory test sets via MSE.",
            "performance_metrics": "Test MSE reported in Table 1 for GP baseline across datasets.",
            "success_rate": "not reported",
            "challenges_limitations": "GP/symbolic methods can be computationally expensive and unstable; depend on choice of primitives and can yield overfit or uninterpretable expressions without strong regularization; noted to have high variance across runs in some reported results.",
            "comparison_baseline": "Compared in experiments to HDTwinGen and other baselines; HDTwinGen outperformed GP on the presented benchmarks.",
            "uuid": "e4494.4",
            "source_info": {
                "paper_title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Schmidt & Lipson 2009",
            "name_full": "Distilling free-form natural laws from experimental data",
            "brief_description": "A foundational symbolic-regression work that introduced methods to recover free-form physical laws from experimental time-series using symbolic regression techniques.",
            "citation_title": "Distilling free-form natural laws from experimental data",
            "mention_or_use": "mention",
            "system_name": "Eureqa-style symbolic regression (historical reference)",
            "system_description": "Cited as a canonical reference in the literature on discovering governing equations from data. The present paper references it in the context of symbolic/equation discovery methods.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Physics / experimental sciences / system identification",
            "number_of_papers": "1 (canonical reference)",
            "law_type": "Free-form analytic physical laws (symbolic expressions)",
            "law_examples": "Classic examples in that literature include recovered conserved quantities or differential equations, but no specific equations from Schmidt & Lipson are reproduced in this paper.",
            "extraction_method": "Symbolic regression over candidate functional forms using evolutionary/search methods (historical reference; no new extraction in this paper).",
            "validation_approach": "Not described here (citation only).",
            "performance_metrics": "Not described here.",
            "success_rate": "Not described here.",
            "challenges_limitations": "Noted generally (and implied in this paper) that symbolic discovery methods face scaling challenges to higher-dimensional systems and require careful feature/library selection.",
            "comparison_baseline": "Referenced as background; current paper compares modern baselines (SINDy, GP) against HDTwinGen.",
            "uuid": "e4494.5",
            "source_info": {
                "paper_title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Deep Generative Symbolic Regression",
            "name_full": "Deep generative symbolic regression (recent methods for recovering mathematical expressions from data)",
            "brief_description": "A class of methods (and specifically a cited paper) that employ deep learning and RL-style or generative approaches to guide symbolic regression toward recovering analytic expressions from data.",
            "citation_title": "Deep generative symbolic regression",
            "mention_or_use": "mention",
            "system_name": "Deep generative symbolic regression (cited approach)",
            "system_description": "Cited as an example of recent approaches that combine deep models and symbolic regression to recover mathematical expressions from data; not used directly in experiments but placed in related work on equation discovery.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine learning / symbolic regression / system identification",
            "number_of_papers": "1 (cited work)",
            "law_type": "Symbolic mathematical expressions / analytic formulas recovered from data",
            "law_examples": "Not provided in this paper (citation only).",
            "extraction_method": "Not specified in the present paper (reference only); these methods typically use neural sequence or generative models to propose candidate symbolic expressions and optimize them with RL or gradient-based search.",
            "validation_approach": "Not described here.",
            "performance_metrics": "Not described here.",
            "success_rate": "Not described here.",
            "challenges_limitations": "Mentioned in the paper's related-work context: symbolic methods can struggle to scale and typically require manual specification of function primitives or decomposition.",
            "comparison_baseline": "Cited as related work; HDTwinGen is presented as an alternative that leverages LLMs for code-based specification and iterative refinement.",
            "uuid": "e4494.6",
            "source_info": {
                "paper_title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling free-form natural laws from experimental data",
            "rating": 2
        },
        {
            "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "rating": 2
        },
        {
            "paper_title": "D-CODE: Discovering closed-form ODEs from observed trajectories",
            "rating": 2
        },
        {
            "paper_title": "Deep generative symbolic regression",
            "rating": 2
        },
        {
            "paper_title": "Data-driven discovery of dynamical systems in pharmacology using large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models to enhance bayesian optimization",
            "rating": 1
        },
        {
            "paper_title": "Large language models as optimizers",
            "rating": 1
        }
    ],
    "cost": 0.019674499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automatically Learning Hybrid Digital Twins of Dynamical Systems</h1>
<p>Samuel Holt, ${ }^{<em>}$ Tennison Liu</em> \&amp; Mihaela van der Schaar<br>DAMTP, University of Cambridge<br>Cambridge, UK<br>{sih31, tl522, mv472}@cam.ac.uk</p>
<h4>Abstract</h4>
<p>Digital Twins (DTs) are computational models that simulate the states and temporal dynamics of real-world systems, playing a crucial role in prediction, understanding, and decision-making across diverse domains. However, existing approaches to DTs often struggle to generalize to unseen conditions in data-scarce settings, a crucial requirement for such models. To address these limitations, our work begins by establishing the essential desiderata for effective DTs. Hybrid Digital Twins (HDTwins) represent a promising approach to address these requirements, modeling systems using a composition of both mechanistic and neural components. This hybrid architecture simultaneously leverages (partial) domain knowledge and neural network expressiveness to enhance generalization, with its modular design facilitating improved evolvability. While existing hybrid models rely on expertspecified architectures with only parameters optimized on data, automatically specifying and optimizing HDTwins remains intractable due to the complex search space and the need for flexible integration of domain priors. To overcome this complexity, we propose an evolutionary algorithm (HDTwinGen) that employs Large Language Models (LLMs) to autonomously propose, evaluate, and optimize HDTwins. ${ }^{2}$ Specifically, LLMs iteratively generate novel model specifications, while offline tools are employed to optimize emitted parameters. Correspondingly, proposed models are evaluated and evolved based on targeted feedback, enabling the discovery of increasingly effective hybrid models. Our empirical results reveal that HDTwinGen produces generalizable, sample-efficient, and evolvable models, significantly advancing DTs' efficacy in real-world applications.</p>
<h2>1 Introduction</h2>
<p>Digital Twins (DTs) are computational models that accurately simulate the states and temporal dynamics of real-world systems [1, 2]. They are particularly useful in modeling dynamical systems, which consist of multiple interdependent components that evolve over time [3, 4]. Take, for example, the epidemiological dynamics of a contagious disease containing various components, including infection rates, recovery rates, population movement, and intervention strategies. DTs can integrate these factors to simulate future outcomes (e.g. predict disease spread), understand system changes (e.g. examining shifts in disease dynamics for varying demographics), and evaluate the impact of control measures (e.g. to curb disease transmission) [5, 6].</p>
<p>Desiderata. A notable differentiator between DTs and general machine learning (ML) models is the emphasis on generalization. DTs are designed to simulate completely unseen scenarios or interventions at inference time. Therefore, a crucial consideration is $\boldsymbol{\nabla}[\mathbf{P 1}]$ out-of-distribution</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>generalization: the ability to generalize to state-action distributions beyond those observed during training. This challenge is often compounded by the scarcity of observational data available to accurately learn dynamics, highlighting the importance of [P2] sample-efficient learning. Additionally, the model should be [P3] evolvable: capable of efficiently adapting (i.e. with minimal retraining) to changes in the underlying system dynamics. This is particularly crucial in healthcare domains, such as epidemiological modeling and treatment planning, where DTs are regularly updated to reflect fundamental changes in disease transmission patterns (caused by viral mutations, vaccination coverage) or evolving drug resistance mechanisms, often with minimal additional data of emergent dynamics [6, 7].
Existing approaches for creating DTs primarily utilize two approaches: mechanistic models or MLbased neural models. Mechanistic models, denoted as $f_{\text {mech }}$, are closed-form equations grounded in domain knowledge such as biological or physical principles. They offer high accuracy and generalization given sufficient domain understanding but are limited in their ability to model systems where scientific knowledge is incomplete [8, 9]. Of related note, techniques have been introduced to discover governing equations directly from data, but face challenges in scaling to more complex problem settings [10, 11]. Conversely, neural approaches, $f_{\text {neural }}$, leverage neural networks (NN) to learn DTs directly from data, often requiring minimal knowledge [12, 13, 14, 15]. Such models are effective given sufficient training data that provides adequate coverage of state-action distributions, but struggle in data-scarce settings and are difficult to evolve to reflect changing conditions due to their overparameterized, monolithic nature.
Key considerations. Informed by this context, Hybrid Digtal Twins (HDTwins) combine the strengths of both approaches through compositions of neural and mechanistic components, i.e. $f=f_{\text {mech }} \circ$ $f_{\text {neural }}$. Here, $f_{\text {mech }}$ symbolically incorporates domain-grounded priors, improving generalization and regularization while simplifying the complexity of patterns that have to be learned by the neural component. In other terms, $f_{\text {neural }}$ complements the mechanistic component by modeling complex temporal patterns in regions where the mechanistic model might be oversimplified or incomplete. Consequently, HDTwins can more accurately and robustly capture system dynamics, particularly in settings with (limited) empirical data and (partial) domain knowledge.
Conceptually, hybrid modeling involves two stages: model specification, determining the model structure (e.g. neural architecture, symbolic equations), and model parameterization, estimating model parameters (e.g. neural weights, coefficients). This process, with model specification in particular, has traditionally relied heavily on human expertise to craft problem-specific models [16, 17, 18, 19]. In this work, we investigate the feasibility of automatically designing hybrid models with minimal expert involvement, which would significantly enhance the efficiency and scalability of model development. This task is challenging, as it requires searching for optimal specification and corresponding parameters within a vast combinatorial model space [20, 21]. To address this, we introduce HDTwinGen, a novel evolutionary framework that autonomously and efficiently designs HDTwins. At a high level, our method represents hybrid model specifications in code and leverages large language models (LLMs) for their domain knowledge, contextual understanding, and learning capabilities to propose symbolically represented models and search the model space [22, 23, 24]. This is coupled with offline optimization tools to empirically estimate model parameters from training data. More specifically, HDTwinGen utilizes two LLM agents: the modeling agent, whose task is to generate novel model specifications, and the evaluation agent, which analyzes performance and provides targeted recommendations for improvement. Through multiple iterations, HDTwinGen efficiently evolves better performing hybrid models with informed modifications.
Contributions: (1) Conceptually, we present the first work in automated hybrid model design, jointly optimizing model specification and parameterization of hybrid digital twins. (2) Technically, we introduce HDTwinGen, a novel evolutionary framework employing LLMs and offline optimization tools to propose, evaluate, and iteratively enhance hybrid models. (3) Empirically, we demonstrate that our method learns more accurate DTs, achieving better out-of-distribution generalization, sample-efficient learning, and $\boldsymbol{\square}$ increased flexibility for modular evolvability.</p>
<h1>2 Digital Twins of Dynamical Systems</h1>
<p>A dynamical system $\mathcal{S}:=(\mathcal{X}, \mathcal{U}, \Phi)$ is a tuple of its $d_{\mathcal{X}}$-dimensional state space $\mathcal{X} \subseteq \mathbb{R}^{d_{\mathcal{X}}}$, an (optional) $d_{\mathcal{U}}$-dimensional action space $\mathcal{U} \subseteq \mathbb{R}^{d_{\mathcal{U}}}$, and a dynamics model $\Phi$. The state at time</p>
<p>$t \in \mathcal{T} \subseteq \mathbb{R}<em _omega_theta_="\omega(\theta)" _theta_="\theta,">{+}$is represented as a vector, $x(t) \in \mathcal{X}$ and similarly the action taken is represented as a vector $u(t) \in \mathcal{U}$. The continuous-time dynamics of the system can be described by $\mathrm{d} x(t) / \mathrm{d} t=$ $\Phi(x(t), u(t), t)$, where $\Phi: \mathcal{X} \times \mathcal{U} \times \mathcal{T} \rightarrow \mathcal{X}$. We optionally consider the existence of some policy $\pi: \mathcal{X} \rightarrow P(\mathcal{U})$ that acts on the system by mapping a state $x(t)$ to a distribution over actions $u(t)$.
Digital Twins. Digital twins (DTs) aim to approximate $\Phi: \mathcal{X} \times \mathcal{U} \times \mathcal{T} \rightarrow \mathcal{X}$ using a computational model $f</em>, \Theta$, and $\Omega(\theta)$ are the spaces of all possible models, specifications, and parameters, respectively. Next, we outline the key desiderata for a DT:
[P1] Generalization to unseen state-action distributions. As DTs are required to simulate varying conditions, they should extrapolate to state-action distributions not observed during training time. Formally, the generalization error $\mathbb{E}} \in \mathcal{F}$ learned from data. Here, we use $\theta \in \Theta$ to denote the specification of the model (e.g. linear) and $\omega(\theta) \in \Omega(\theta)$ to indicate the set of parameters specified by $\theta$. Additionally, $\mathcal{F<em _OOD="{OOD" _text="\text">{(x(t), u(t), y(t)) \sim p</em>$ represents the out-of-distribution scenario. [P2] Sample-efficient learning. Given the often limited availability of real-world data, DTs should learn robustly from minimal empirical data. In other words, they must have good sample complexity, achieving the desired level of generalization with a limited number of observations [25].
[P3] Evolvability. Dynamical systems are, by nature, non-stationary and evolve over time [26, 27]. From a modeling perspective, the DT should be easily evolved to reflect changing underlying dynamics, minimizing the need for additional data or expensive model re-development, i.e. $\theta$ and $\omega(\theta)$ should be easily adjustable to reflect changing system dynamics.
For the purpose of model learning, we assume access to an offline dataset containing $N \in \mathbb{N}^{+}$trajectories, where the measurements of the systems are made at discrete time points $[T]=\left[t_{1}, t_{2}, \ldots T\right]$. This dataset, $\mathcal{D}=\left{\left{\left(x^{(n)}(t), u^{(n)}(t), y^{(n)}(t)\right) \mid t \in[T]\right}\right}_{n=1}^{N}$, contains state-action trajectories sampled regularly over time, where $y^{(n)}(t)=x^{(n)}(t+\Delta t)$ represents the subsequent state.}}}[\mathcal{L}\left(f_{\theta, \omega(\theta)}(x(t), u(t)), y(t)\right)]$ should be minimized, where $\mathcal{L}$ is some loss function, and $p_{\text {OOD }</p>
<h1>3 Hybrid Digital Twins</h1>
<p>HDTwin. A Hybrid Digital Twin is a composition of mechanistic and neural components, represented as $f_{\theta, \omega(\theta)}=f_{\text {mech }} \circ f_{\text {neural }}[18,28]$. This class of hybrid models offers several advantages that align with our desiderata. The mechanistic component allows partial knowledge to be encoded through its symbolic form, which, while not sufficient alone to accurately predict underlying dynamics, is complemented by the neural components that learn from available data. This combination aids in generalization ([P1]), especially moving beyond conditions observed in training, and improves sample complexity ([P2]). Furthermore, the mechanistic component can be quickly and easily updated with new parameters due to its simpler, lower-dimensional structure, allowing the overall model to adapt efficiently to remain accurate in changing conditions ([P3]). In this work, we focus on additive compositions, $f_{\theta, \omega(\theta)}=f_{\text {mech }}+f_{\text {neural }}$, as they are more interpretable. Additionally, it enables individual contributions of mechanistic and neural components to be easily disentangled and simplifies the optimization to allow gradient-based methods [16]. Nonetheless, we encourage future works to investigate alternative composition strategies (e.g. branching composition) to develop more advanced HDTwins [29].
Learning the hybrid model can be decomposed into two steps: (1) model specification, or learning the structure, $\theta \in \Theta$, of the dynamics function that describes how the system evolves over time; and (2) model parameterization, which estimates the specific values of parameters $\omega(\theta) \in \Omega(\theta)$ for a given specification $\theta$. For instance, the logistic-growth model specifies a structure for population growth, while parameterization involves estimating the growth rate and carrying capacity. ${ }^{3}$ More generally, this learning problem can be mathematically formulated as a bilevel optimization problem:</p>
<p>$$
\theta^{<em>}=\underset{\theta \in \Theta}{\arg \min } \mathcal{L}_{\text {outer }}\left(\theta, \omega^{</em>}(\theta)\right), \quad \text { where } \quad \omega^{*}(\theta)=\underset{\omega \in \Omega(\theta)}{\arg \min } \mathcal{L}_{\text {inner }}(\theta, \omega(\theta))
$$</p>
<p>Here, the upper-level problem involves finding the optimal specification $\theta^{<em>}$ that minimizes the outer objective $\mathcal{L}_{\text {outer }}$, while the lower-level problem involves finding the optimal parameters $\omega^{</em>}(\theta)$ for a given specification $\theta$ that minimizes the inner objective function $\mathcal{L}_{\text {inner }}$. To be more concrete, the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: HDTwinGen: evolutionary framework. The process begins with user-provided modeling context $\mathcal{S}^{\text{context}}$ and $\mathcal{D}=\left{\mathcal{D}<em _text_val="\text{val">{\text{train}}, \mathcal{D}</em>$. This iterative loop repeats for $G$ iterations.
outer objective measures the generalization performance, empirically measured on the validation set $\mathcal{L}}}\right}$. 1) In iteration $g$, the modeling agent generates model specification as a Python program $f_{\theta, \omega(\theta)}$. 2) Parameters are optimized using the offline optimization tool to yield $f_{\theta, \omega^{*}(\theta)}$. 3) The HDTwin is evaluated based on model loss $v$ and component-wise loss $\delta$. Subsequently, the model pool $\mathcal{P}^{(g)}$ is updated with top- $K$ models. 4) The evaluation agent provides targeted feedback for model improvement $H^{(g)}$ by analyzing models in $\mathcal{P}^{(g)}$ using performance metrics requirements outlined in $\mathcal{S}^{\text {context }<em _text="\text" _train="{train">{\text {val }}$, while the inner objective measures the fitting error, as evaluated on the training set $\mathcal{L}</em>$.}</p>
<p>Combinatorial search space. The space of possible specifications $\Theta$ (e.g. different networks, functional forms) is discrete and combinatorially large, while $\Omega(\theta)$ represents the continuous space of parameters to be optimized. Selecting the optimal $\theta, \omega(\theta)$ thus involves searching through a vast combinatorial space. Performing this search through traditional means, such as genetic programming [21] or evolutionary algorithms [20], is computationally challenging, time-consuming, and often technically infeasible. To the best of our knowledge, our work is the first to address the problem of automatic HDTwin development, where we incorporate LLMs (combined with offline optimization tools) to automatically optimize both the specification and the parameterization of hybrid models.</p>
<h1>4 HDTwinGen: Automatic Design of HDTwins</h1>
<p>Human experts craft models by making strategic design decisions based on their domain knowledge, starting with a sensible initial model specification and performing intelligent modifications based on empirical evaluations. Our key insight is that LLMs can effectively emulate these capabilities to efficiently navigate the search space in Equation (1) and autonomously design HDTwins. More specifically, our method utilizes LLMs for three major purposes: source of domain knowledge, where LLMs inject domain-consistent knowledge into the model specification, particularly through the symbolic representation $f_{\text {mech }} \boldsymbol{\square}$ efficient search, by making intelligent modifications to the specification to converge more efficiently on the optimal hypothesis; and $\boldsymbol{\square}$ contextual understanding, enabling the algorithm to incorporate task-specific context and targeted feedback for model improvement $[22,23,24]$.
Overview. We operationalize this insight through HDTwinGen, an evolutionary algorithm that iteratively evolves a population of candidate solutions to automatically search for the best HDTwin. Our approach employs a framework comprising three key elements: (1) human experts provide an initial system description, modeling objectives, and requirements as a structured prompt; (2) a modeling agent proposes new model specifications, optimizes their parameters on a training dataset, and collects validation performance metrics; (3) an evaluation agent assesses the proposed models using both data-driven performance metrics and qualitative evaluations against expert-defined objectives and requirements. The agents communicate using natural language and a custom code format representing the HDTwin model, facilitating autonomous and iterative model enhancement. An overview of our method is presented in Figure 1, with pseudocode in Appendix E.1.
Initial prompt design. The optimization process begins with a human expert providing a structured prompt, referred to as the modeling context $\mathcal{S}^{\text {context }}$. This modeling context outlines the system description, modeling objectives $\mathcal{L}$, and requirements $\mathcal{R}$ :</p>
<ol>
<li>The system description semantically describes the system, including state and action variables, giving the algorithm the contextual understanding necessary for informed model development.</li>
<li>The modeling objective specifies quantitative performance requirements via a metric $\mathcal{L}$.</li>
<li>The modeling requirements $\mathcal{R}$ are qualitative and described in natural language, detailing aspects such as interpretability (e.g. fully mechanistic or hybrid model) and additional scientific knowledge (e.g. a log-linear relationship between variables).</li>
</ol>
<p>In practice, $\mathcal{R}$ can incorporate various requirements, allowing for the design of both purely mechanistic and hybrid models, a flexibility that we demonstrate experimentally. The model is represented in Python, where purely mechanistic specifications are represented in native Python and neural components are represented using PyTorch [31]. Moreover, $\mathcal{S}^{\text {context }}$ includes a skeleton code to guide the synthesis of executable code in a predetermined format. For illustrative purposes, an example of $\mathcal{S}^{\text {context }}$ is provided in Appendix E.4.
Evolutionary optimization overview. Given $\mathcal{S}^{\text {context }}$ as input, HDTwinGen performs $G$ iterations of optimization, where $G \in \mathbb{N}^{+}$. The population of proposed HDTwins at iteration $g$ is represented as $\mathcal{P}^{(g)}$. Each iteration creates a new candidate model based on previously created models in $\mathcal{P}^{(g)}$ and feedback. Only the top $K$ models are retained after each iteration, except when $g&lt;K$, in which case all generated models are kept, i.e. $\max _{g \in[G]}\left|\mathcal{P}^{(g)}\right|=K$. Each model in $\mathcal{P}^{(g)}$ is characterized by a tuple containing its model specification (represented symbolically through code) and validation metrics. After completing $G$ iterations, the model with the best validation performance in $\mathcal{P}^{(G)}$ is selected as the final model.</p>
<h1>4.1 Modeling Agent</h1>
<p>Proposing HDTwins. The goal of the modeling step is to propose novel HDTwins based on previously proposed models and feedback from the evaluation agent. Specifically, on the $g$-th iteration, the modeling agent takes as input $\boldsymbol{\nabla}^{(g-1)}$ : the set of top- $K$ previously generated models; $\boldsymbol{\nabla} H^{(g-1)}$ : the most recent feedback produced by the evaluation agent (where on the initial step, $g=1$, both are empty, i.e., $H^{(0)}=\varnothing, \mathcal{P}^{(0)}=\varnothing$ ); and $\boldsymbol{\nabla}^{\text {context }}$ : the modeling context. The modeling agent generates a model specification $\theta$ using a predefined code format (i.e. skeleton code). By observing multiple previously best-performing models and their performances, the modeling agent can exploit this context as a rich form of in context-learning and evolve improved specifications in subsequent generations [22]. Each generated specification emits its corresponding parameters, $\omega(\theta)$ are fitted to the training set $\mathcal{D}<em _omega_theta_="\omega(\theta)" _theta_="\theta,">{\text {train }}$. More formally, we represent this generative procedure as $f</em>} \sim \operatorname{LLM<em _mathcal_X="\mathcal{X">{\text {model }}\left(H, \mathcal{P}^{(g)}, \mathcal{S}^{\text {context }}\right)$.
Model specification. To generate model specifications, the modeling agent decomposes the system into a set of components, with each component describing the dynamics of a specific state variable. In other words, for a system with $d</em>$ specifies purely mechanistic equations, the component dynamics are entirely defined using closed-form equations. Conversely, in a hybrid model, the mechanistic equation can be augmented with a neural network (implemented in PyTorch) to model residuals (i.e. in an additive fashion). The choice between mechanistic and hybrid models is left to the user, balancing the trade-off between transparency and predictive performance. Concretely, the specification step involves 'filling in' the skeleton code with a detailed body of code, specifying the decomposition, and delineating each component's dynamics function as a separate code structure (for a generated HDTwin example, see Appendix I).
Model optimization. The generated specification emits $\omega(\theta)$, which are treated as placeholder values, and are then optimized against the training dataset. Specifically, we optimize the mean squared error for the parameters that minimize this loss as $\omega^{\star}(\theta)=\arg \min }}$ state variables, there will be $d_{\mathcal{X}}$ components. Each component is characterized by its own set of inputs and a unique dynamics function that describes the dynamics of its associated state variable over time. This modular representation enables independent analysis and optimization of individual components. In cases where $\mathcal{R<em _omega_theta_="\omega(\theta)" _theta_="\theta,">{\omega(\theta)} \mathcal{L}\left(f</em>}, \mathcal{D<em _omega_star="\omega^{\star" _theta_="\theta,">{\text {train }}\right)$. In this work, we consider $\omega(\theta)$ to be continuous variables, and as such, we optimize $\theta$ by stochastic gradient descent, using the Adam optimizer [32]. However, we note other optimization algorithms, such as black-box optimizers, could also be used (for more details, see Appendix F, Equation (5)). The parameter optimization step then yields the complete model, $f</em>$.
Quantitative evaluation. For each generated model, we evaluate them quantitatively. Specifically, we collect the validation mean squared error loss per component, which we denote as}(\theta)</p>
<p>$\delta=\left[\delta_{1}, \delta_{2}, \ldots, \delta_{d_{A}}\right]$ (Appendix F, Equation (6)). We also compute the validation loss of the overall model as well as $v=\mathcal{L}\left(f_{\theta, \omega^{<em>}(\theta)}, \mathcal{D}<em>{\text {val }}\right)$. Finally, the generated model and its validation losses are included in a tuple and added to the top- $K$ models $\mathcal{P}^{(g)} \leftarrow \mathcal{P}^{(g-1)} \oplus\left(f</em>{\theta, \omega^{</em>}(\theta)}, \delta, v\right)$, where $\mathcal{P}^{(g)}$ automatically removes the lowest performing models, and also only adds a new model to $\mathcal{P}^{(g)}$ if it is unique. We highlight that we consider the top- $K$ models only to apply selection pressure, such that only the best-performing models are considered when generating the next HDTwin [33].</p>
<h1>4.1.1 Evaluation Agent</h1>
<p>Model evaluation. The goal of the evaluation step is to reflect on the current set of top- $K$ models, $\mathcal{P}^{(g)}$ against requirements $\mathcal{R}$ and provide actionable and detailed feedback to the modeling agent for model improvement: $H^{(g)} \sim \operatorname{LLM}_{\text {eval }}\left(\mathcal{R}, \mathcal{P}^{(g)}\right)$. We note that $H^{(g)}$ is provided in natural language and can be viewed as a dense feedback signal, a notable distinction from traditional learning methods, where feedback often takes the form of simple scalar values, such as loss gradients or rewards. Leveraging natural language feedback allows the agent to (1) engage in comparative analysis, identifying effective specifications in $\mathcal{P}^{(g)}$ contributing to higher performance and discerning patterns common in less effective models, informing its suggestions for further model improvement; (2) qualitatively evaluate models against qualitative requirements $\mathcal{R}$-leveraging the LLM's capacity to reason about proposed HDTwins to reflect these requirements via model improvement feedback.</p>
<p>Enhancing search. By providing rich feedback to improve model specification, the evaluation and modeling agent collaborate to efficiently evolve high-performing models. Empirically, in Appendix J, we observe that the evaluation agent provides targeted and specific feedback, including componentspecific suggestions, proposing alternative decompositions, removing parameters, or introducing non-linear terms. It is noteworthy that the feedback $H^{(g)}$, expressed flexibly in natural language, could easily be further enriched through direct human feedback. We demonstrate this human-inthe-loop capability by including expert feedback during the optimization process through $H^{(g)}$ and observed that it was integrated into newly generated HDTwins. Though further investigation is beyond the scope of this work, this demonstration highlights promising avenues for augmenting human-machine collaboration in the autonomous design of DTs.</p>
<h2>5 Related Works</h2>
<p>For an extended related work, refer to Appendix B. Our work focuses on autonomously learning DTs from data, with several relevant research strands:</p>
<p>Neural sequence models. ML approaches commonly address learning system dynamics as a sequential modeling problem. In these settings, $f_{\theta, \omega(\theta)}$ are typically black-box models, where $\theta \in \Theta$ is the NN architecture and $\omega(\theta)$ are its weights. Early models like Hidden Markov Models [34] and Kalman filters [35] made simplifying Markovian and linearity assumptions, later extended to nonlinear settings [36, 37]. Subsequent models, including recurrent neural networks [38], along with their advanced variants [39, 40, 41], introduced the capability to model longer-term dependencies. More recent advancements include attention mechanisms [42] and Transformer models [43], significantly improving the handling of long-term dependencies in sequence data. Another line of work, Neural Ordinary Differential Equations (NODE) [14, 44, 45], interprets neural network operations as differential equations. These methods have found utility in modeling a range of complex systems [46, 47, 48, 49]. While deep sequence models are proficient at capturing complex dynamics, they are heavily reliant on training data for generalization ([P1, P2]), and their monolithic and overparameterized structures limit evolvability ([P3]).</p>
<p>Mechanistic (discovery) models. Beyond purely neural approaches, another line of work aims to discover a system's governing equations directly from data. Here $\theta \in \Theta$ are closed-form equations and $\omega(\theta)$ are their parameters. These include symbolic regression techniques [10], Eureqa [50], SINDy [11], D-CODE [51, 52], among others [52, 53] that search for $\theta$ and $\omega(\theta)$ from data. These techniques struggle to scale to higher-dimensional settings and rely on experts to perform variable selection and define the function set and primitives available to the search algorithms.</p>
<p>Hybrid models. Recent efforts have also created hybrid models by integrating physical laws with neural models. Physics-informed neural networks [15, 54], and methods including Hamiltonian Neural Networks [55], Lagrangian Neural Networks [56] integrate structural priors of physical</p>
<p>systems to improve generalization. These techniques introduce specialized mechanisms to incorporate precisely known physical principles. Additionally, [57] integrates prior ODE/PDE knowledge into a hybrid model, using specialized regularization to penalize the neural component's information content. [58, 59] consider settings where an expert equation is known, but equation variables are latent and unobserved. Correspondingly, they employ two sets of latent variables: one governed by expert equations and another linked to neural components. [60] performs data augmentation by sampling out-of-distribution trajectories from expert models. While existing approaches rely on expert models to perform the hybrid model design, HDTwinGen is an automated approach to jointly optimize hybrid model specification and its parameters.</p>
<h1>6 Experiments and Evaluation</h1>
<p>In this section, we evaluate HDTwinGen and verify that it significantly outperforms state-of-the-art methods in modeling system dynamics over time from an observed dataset and corresponding system description. ${ }^{4}$
Benchmark datasets. We evaluate against six real-world complex system datasets; where each dataset is either a real-world dataset or has been sampled from an accurate simulator designed by human experts. Three are derived from a state-of-the-art biomedical Pharmacokinetic-Pharmacodynamic (PKPD) model of lung cancer tumor growth, used to simulate the combined effects of chemotherapy and radiotherapy in lung cancer [61] (Equation (2))this has been extensively used by other works [62, 63, 64]. Here we use this bio-mathematical lung cancer model to create three variations of lung cancer under the effect of no treatments (Lung Cancer), chemotherapy only (Lung Cancer (with Chemo.)), and chemotherapy combined with radiotherapy (Lung Cancer (with Chemo. \&amp; Radio.)). We also compare against an accurate and complex COVID-19 epidemic agent-based simulator (COVID-19) [65], which is capable of modeling non-pharmaceutical interventions, such as physical distancing during a lockdown. Furthermore, we compare against an ecological model of a microcosm of algae, flagellate, and rotifer populations (Plankton Microcosm)replicating an experimental three-species prey-predator system [66]. Moreover, we also compare against a real-world dataset of hare and lynx populations (Hare-Lynx), replicating predator-prey dynamics [67]. We detail all benchmark datasets details in Appendix C.
Evaluation Metrics. We employ mean squared error (MSE) to evaluate the benchmark methods on a held-out test dataset of state-action trajectories, denoted as $\mathcal{D}<em E="E" M="M" S="S">{\text {test }}$, using the loss defined in Equation (5) and report this as $\mathcal{T}</em>$. Each metric is averaged over ten runs with different random seeds, and we present these averages along with their $95 \%$ confidence intervals, further detailed in Appendix G.
Benchmark methods. To assess whether HDTwinGen is state-of-the-art, we compare it with the most competitive and popular neural network models, which, when modeling the dynamics of a system over time, becomes a form of ODE model, that is a neural ODE [14] with action inputs (DyNODE) [68]. Moreover, we also compare against a recurrent neural network (RNN) [69] and a state-of-the-art transformer (Transformer) [64]. We also compare against mechanistic dynamical equations derived from equation discovery methods for ODEs, including Genetic Programming (GP) [10] and Sparse Identification of Nonlinear Dynamics (SINDy) [11]. Lastly, we compare against a hybrid model (APHYNITY) that integrates prior knowledge in the form of ODEs into hybrid models, while penalizing the information content from the neural component [57]. Moreover, we compare against the ablations of our method, of the zero-shot generated HDTwin (ZeroShot) and this model with subsequently optimized parameters (ZeroOptim). We provide method implementation, hyperparameter, and experimental details in Appendix D.</p>
<h2>7 Main Results</h2>
<p>We evaluated all our benchmark methods across all our datasets tabulated in Table 1. HDTwinGen models the system the most accurately, achieving the lowest test prediction mean squared error on the held-out test dataset of state-action trajectories. In the interest of space, we include additional experimental evaluations in the appendix. Specifically, we also evaluate HDTwinGen performance on a suite of synthetically and procedurally generated benchmarks (Appendix H.9); comparisons</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Benchmark method performance. Reporting the test prediction $\operatorname{MSE}\left(\mathcal{T}_{M S E}\right)$ of the produced system models on held-out test datasets across all benchmark datasets. HDTwinGen achieves the lowest test prediction error. The results are averaged over ten random seeds, with $\pm$ indicating $95\%$ confidence intervals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Lung Cancer</th>
<th style="text-align: center;">Lung Cancer (with Chemo.)</th>
<th style="text-align: center;">Lung Cancer (with Chemo. \&amp; Radio.)</th>
<th style="text-align: center;">Ham-Lynx</th>
<th style="text-align: center;">Plankton Microscopy</th>
<th style="text-align: center;">COVID-19</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">$\mathcal{T}_{M S E \downarrow}$</td>
<td style="text-align: center;">$\mathcal{T}_{M S E \downarrow}$</td>
<td style="text-align: center;">$\mathcal{T}_{M S E \downarrow}$</td>
<td style="text-align: center;">$\mathcal{T}_{M S E \downarrow}$</td>
<td style="text-align: center;">$\mathcal{T}_{M S E \downarrow}$</td>
<td style="text-align: center;">$\mathcal{T}_{M S E \downarrow}$</td>
</tr>
<tr>
<td style="text-align: left;">SINDy</td>
<td style="text-align: center;">$327 \pm 5.79$</td>
<td style="text-align: center;">$11.8 \pm 0.395$</td>
<td style="text-align: center;">$13.7 \pm 0.573$</td>
<td style="text-align: center;">$388 \pm 4.29 \mathrm{e}+14$</td>
<td style="text-align: center;">$0.80135 \pm 0$</td>
<td style="text-align: center;">$93.4 \pm 0.458$</td>
</tr>
<tr>
<td style="text-align: left;">GP</td>
<td style="text-align: center;">$158 \pm 94.1$</td>
<td style="text-align: center;">$154 \pm 505$</td>
<td style="text-align: center;">$171 \pm 8.99$</td>
<td style="text-align: center;">$514 \pm 381$</td>
<td style="text-align: center;">$0.00474 \pm 0.0564$</td>
<td style="text-align: center;">$10.1 \pm 18$</td>
</tr>
<tr>
<td style="text-align: left;">DyNODE</td>
<td style="text-align: center;">$327 \pm 5.8$</td>
<td style="text-align: center;">$52 \pm 47.1$</td>
<td style="text-align: center;">$16.3 \pm 5.58$</td>
<td style="text-align: center;">$439 \pm 0$</td>
<td style="text-align: center;">$0.00036 \pm 0.00078$</td>
<td style="text-align: center;">$74 \pm 2.56$</td>
</tr>
<tr>
<td style="text-align: left;">RNN</td>
<td style="text-align: center;">$1.17 \mathrm{e}+06 \pm 3.08 \mathrm{e}+04$</td>
<td style="text-align: center;">$708 \pm 86.1$</td>
<td style="text-align: center;">$136 \pm 5.6$</td>
<td style="text-align: center;">$3.71 \mathrm{e}+03 \pm 3.39 \mathrm{e}+03$</td>
<td style="text-align: center;">$0.0281 \pm 0.0406$</td>
<td style="text-align: center;">$1.38 \mathrm{e}+04 \pm 1.65 \mathrm{e}+03$</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">$7.48 \pm 1.06$</td>
<td style="text-align: center;">$0.348 \pm 0.0618$</td>
<td style="text-align: center;">$0.216 \pm 0.0343$</td>
<td style="text-align: center;">$716 \pm 42.5$</td>
<td style="text-align: center;">$3.69 \mathrm{e}-05 \pm 1.83 \mathrm{e}-05$</td>
<td style="text-align: center;">$\mathbf{0 . 3 0 9} \pm \mathbf{0 . 2 2 2}$</td>
</tr>
<tr>
<td style="text-align: left;">APHYNITY</td>
<td style="text-align: center;">$9.06 \pm 1.27$</td>
<td style="text-align: center;">$81.6 \pm 81.3$</td>
<td style="text-align: center;">$1.21 \mathrm{e}+03 \pm 1.69 \mathrm{e}+03$</td>
<td style="text-align: center;">$321 \pm 12.6$</td>
<td style="text-align: center;">$4.21 \mathrm{e}-05 \pm 3.45 \mathrm{e}-05$</td>
<td style="text-align: center;">$158.8 \pm 9.97$</td>
</tr>
<tr>
<td style="text-align: left;">ZeroShot</td>
<td style="text-align: center;">$5.45 \mathrm{e}+03 \pm 6.71 \mathrm{e}+07$</td>
<td style="text-align: center;">$292 \pm 40.2$</td>
<td style="text-align: center;">$5.81 \mathrm{e}+03 \pm 4.02 \mathrm{e}+03$</td>
<td style="text-align: center;">$338 \pm 0$</td>
<td style="text-align: center;">$0.325 \pm 0.242$</td>
<td style="text-align: center;">$2.31 \mathrm{e}+03 \pm 2.24 \mathrm{e}+07$</td>
</tr>
<tr>
<td style="text-align: left;">ZeroOptim</td>
<td style="text-align: center;">$216 \pm 172$</td>
<td style="text-align: center;">$21.2 \pm 45$</td>
<td style="text-align: center;">$6.08 \pm 7.9$</td>
<td style="text-align: center;">$353 \pm 0$</td>
<td style="text-align: center;">$0.0132 \pm 0.00116$</td>
<td style="text-align: center;">$7.88 \pm 0.0414$</td>
</tr>
<tr>
<td style="text-align: left;">HDTwinGen</td>
<td style="text-align: center;">$\mathbf{4 . 4 1 : 0 . 0 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 8 9 : 0 . 0 4 5 3}$</td>
<td style="text-align: center;">$\mathbf{6 . 1 3 1 : 0 . 1 9 8}$</td>
<td style="text-align: center;">$\mathbf{2 9 1 : 3 0 . 3}$</td>
<td style="text-align: center;">$\mathbf{2 . 5 1 e - 0 6 : 2 . 2 e - 0 6}$</td>
<td style="text-align: center;">$\mathbf{1 . 7 2 : 2 . 2 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Out of distribution shifts. On a variation of the Lung Cancer (with Chemo. \&amp; Radio.), HDTwinGen is more robust to OOD shifts in unseen state-action distributions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Lung Cancer <br> (with Chemo. \&amp; Radio.)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">$\operatorname{IID} \mathcal{T}_{M S E} \downarrow$</td>
<td style="text-align: center;">$\operatorname{OOD} \mathcal{T}_{M S E} \downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">DyNODE</td>
<td style="text-align: center;">$0.0115 \pm 0.0121$</td>
<td style="text-align: center;">$1.75 \pm 0.769$</td>
</tr>
<tr>
<td style="text-align: left;">SINDy</td>
<td style="text-align: center;">$0.302 \pm 0.286$</td>
<td style="text-align: center;">$5.9 \pm 2.55$</td>
</tr>
<tr>
<td style="text-align: left;">RNN</td>
<td style="text-align: center;">$1.43 \mathrm{e}+04 \pm 2.02 \mathrm{e}+03$</td>
<td style="text-align: center;">$1.84 \mathrm{e}+05 \pm 4.06 \mathrm{e}+04$</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">$0.0262 \pm 0.00514$</td>
<td style="text-align: center;">$1.19 \mathrm{e}+04 \pm 2.78 \mathrm{e}+03$</td>
</tr>
<tr>
<td style="text-align: left;">ZeroShot</td>
<td style="text-align: center;">$4.95 \mathrm{e}+03 \pm 1.43 \mathrm{e}+04$</td>
<td style="text-align: center;">$1.91 \mathrm{e}+04 \pm 6.36 \mathrm{e}+04$</td>
</tr>
<tr>
<td style="text-align: left;">ZeroOptim</td>
<td style="text-align: center;">$3.49 \pm 0.0364$</td>
<td style="text-align: center;">$4.84 \pm 5.17$</td>
</tr>
<tr>
<td style="text-align: left;">HDTwinGen</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 8 7 2 : 0 . 0 1 8 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 8 4 6 : 0 . 0 8 9 1}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Sample efficiency. Analyzing performance as a function of the number of training trajectories in the Lung Cancer (with Chemo. \&amp; Radio.) dataset. We observe that HDTwinGen achieves the lowest test prediction error, even in the very challenging low data regime. This highlights the role of priors embedded in HDTwin in sample-efficient generalization.
against domain-specific baselines (Appendix H.8) and various ablation experiments, including ablation of LLM hyperparameters, prompt design, and algorithm settings (Appendices H. 5 to H.7).</p>
<h1>7.1 Insight Experiments</h1>
<p>This section provides an in-depth analysis of HDTwinGen's effectiveness related to its benchmark counterparts. Specifically, we examine the core desiderata for an effective DT described in Section 2: [P1] out-of-distribution generalization, [P2] sample-efficient learning, and [P3] evolvability.
[P1] Can an HDTwin generalize to out-of-distribution shifts? To explore out-of-distribution shifts, we adapt the Lung Cancer (with Chemo. \&amp; Radio.) to produce a training dataset of states in a range that is outside those observed in the test set over all trajectories (Appendix H.1). We tabulate this in Table 2. Empirically, we find that HDTwinGen is more robust to out-of-distribution shifts than existing methods, benefiting from explicit decomposition and robust hybrid models. Notably, the neural network method DyNODE shows the largest relative error increase from IID to OOD by two orders of magnitude, while the mechanistic method SINDy exhibits a smaller increase by only one order of magnitude. This demonstrates the importance of hybrid models that leverage both neural and mechanistic components to enhance generalization performance under distribution shifts.
[P2] Can HDTwinGen improve sample-efficiency in model learning? To explore the low data settings, we re-ran all benchmark methods with fewer samples in their training dataset on the Lung Cancer (with Chemo. \&amp; Radio.) dataset. We plot this in Figure 2. Empirically, we observe that HDTwinGen can achieve lower performance errors, especially in lower-sample regimes.
[P3] Can HDTwinGen evolve its modular HDTwin to fit the system? We analyze this from an empirical point of view to determine if HDTwinGen can correctly evolve the generated HDTwin and reduce its prediction error over subsequent generations. We observe that HDTwinGen can indeed understand, reason, and iteratively evolve the generated code representation of the HDTwin to incorporate a better fitting HDTwin, as observed in Figure 3. In particular, the annotated results demonstrate that HDTwinGen effectively refines the hybrid model by strategically adjusting its neural and mechanistic components (in a fashion akin to human experts), leading to significant improvements</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: HDTwinGen effectively evolves HDTwin. Validation MSE of the HDTwin generated in each iteration, showing the Pareto-front of the best generated HDTwin (Top-1 HDTwin), and the generated HDTwin per generation step-additionally with a few of the HDTwins labeled with their model descriptions. HDTwinGen can efficiently understand, modify, and hence evolve the HDTwin to achieve a better-fitting model (Appendix H.4).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: COVID-19 unobserved intervention. The symbolic code-based representation of HDTwin can be easily adapted to unobserved interventions through targeted adjustments of parameters.</p>
<p>in accuracy and robustness. This iterative evolution process demonstrates HDTwinGen's ability to adapt and optimize its modular components.</p>
<p><strong>Can HDTwinGen Understand and Modify Its HDTwin?</strong> We investigate whether large language model (LLM) agents can take an optimized high-dimensional twin (HDTwin) from an existing benchmark dataset and adapt it to model an unobserved intervention that is not present in the training data. We note that this intervention emulates scenarios where the dynamics of the underlying system changes. We affirmatively answer this question by constructing a scenario where our COVID-19 simulator incorporates an unobserved intervention of a lockdown policy, which reduces physical interactions between individuals (Appendix H.2). As demonstrated in Figure 4, we observe that the code-model representation of the HDTwin can be (1) understood by the modeling agent LLM and (2) adapted in its parameters to accurately model and reflect this intervention. We find that HDTwinGen is the <em>only method capable of changing the overall functional behavior by modifying a single parameter in the model</em>; in contrast, all other existing data-driven methods require a dataset of state-action trajectories under the new dynamics introduced by this intervention.</p>
<p><strong>Ablation Studies</strong>. We conducted ablation studies on HDTwinGen and found several key insights. First, retaining the top-K models within the LLM context leads to improved model generation (Appendix H.5). Additionally, HDTwinGen is compatible with various LLMs and different temperature settings (Appendix H.6). It also benefits from including textual descriptions of the variables to be modeled as prior information (Appendix H.7). Finally, HDTwinGen can be specifically instructed to generate mechanistic white-box models if desired (Appendix H.10).</p>
<h1>8 Limitations and Discussions</h1>
<p>In summary, this work addresses the problem of learning digital twins for continuous-time dynamical systems. After establishing clear learning objectives and key requirements, we introduce Hybrid Digital Twins (HDTwins)a promising approach that combines mechanistic understanding with neural architectures. HDTwins encode domain knowledge symbolically while leveraging neural networks for enhanced expressiveness. Conventional hybrid models, however, rely heavily on expert specification with learning limited to parameter optimization, constraining their scalability and applicability. To overcome these limitations, we propose a novel approach to automatically specify and parameterize HDTwins through HDTwinGen, an evolutionary framework that leverages LLMs to iteratively search for and optimize high-performing hybrid twins. Our empirical results demonstrate that evolved HDTwins consistently outperform existing approaches across multiple criteria, exhibiting superior out-of-distribution generalization, enhanced sample efficiency, and improved modular evolvability.
Limitations. While our results are promising, several important limitations remain. HDTwinGen's efficacy depends critically on human experts providing initial system specifications and on the underlying LLM's domain knowledge and model generation capabilities. Our current implementation focuses exclusively on continuous-time systems, which, although broadly applicable, represent only a subset of real-world systems. Future work could extend our approach through human-in-the-loop feedback mechanisms, integration with external tools, and expansion to broader system classes.
Ethical implications. We acknowledge the risk of bias transmission from the black-box LLMs into the evolved models. While our hybrid approach enables greater expert scrutiny through its human-interpretable components, we strongly recommend a comprehensive evaluation of evolved models for fairness, bias, and privacy concerns before deployment in sensitive applications.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>We thank the anonymous reviewers, area and program chairs, members of the van der Schaar lab, and Andrew Rashbass for many insightful comments and suggestions. TL and SH would like to acknowledge and thank AstraZeneca for their sponsorship and support. This work was supported by Microsoft's Accelerate Foundation Models Academic Research initiative.</p>
<h2>References</h2>
<p>[1] F. Tao, J. Cheng, Q. Qi, M. Zhang, H. Zhang, and F. Sui, "Digital twin-driven product design, manufacturing and service with big data," The International Journal of Advanced Manufacturing Technology, vol. 94, pp. 3563-3576, 2018.
[2] J. Corral-Acero, F. Margara, M. Marciniak, C. Rodero, F. Loncaric, Y. Feng, A. Gilbert, J. F. Fernandes, H. A. Bukhari, A. Wajdan et al., "The 'digital twin'to enable the vision of precision cardiology," European heart journal, vol. 41, no. 48, pp. 4556-4564, 2020.
[3] H. A. Simon, The sciences of the artificial. MIT press, 1996.
[4] J. Ladyman, J. Lambert, and K. Wiesner, "What is a complex system?" European Journal for Philosophy of Science, vol. 3, pp. 33-67, 2013.
[5] Q. Qi and F. Tao, "Digital twin and big data towards smart manufacturing and industry 4.0: 360 degree comparison," Ieee Access, vol. 6, pp. 3585-3593, 2018.
[6] V. Iranzo and S. Prez-Gonzlez, "Epidemiological models and covid-19: a comparative view," History and Philosophy of the Life Sciences, vol. 43, no. 3, p. 104, 2021.
[7] I. Bozic, J. G. Reiter, B. Allen, T. Antal, K. Chatterjee, P. Shah, Y. S. Moon, A. Yaqubie, N. Kelly, D. T. Le et al., "Evolutionary dynamics of cancer in response to targeted combination therapy," elife, vol. 2, p. e00747, 2013.
[8] R. Rosen, G. Von Wichert, G. Lo, and K. D. Bettenhausen, "About the importance of autonomy and digital twins for the future of manufacturing," Ifac-papersonline, vol. 48, no. 3, pp. 567-572, 2015 .</p>
<p>[9] T. Erol, A. F. Mendi, and D. Doan, "The digital twin revolution in healthcare," in 2020 4th international symposium on multidisciplinary studies and innovative technologies (ISMSIT). IEEE, 2020, pp. 1-7.
[10] J. R. Koza, "Genetic programming as a means for programming computers by natural selection," Statistics and computing, vol. 4, pp. 87-112, 1994.
[11] S. L. Brunton, J. L. Proctor, and J. N. Kutz, "Discovering governing equations from data by sparse identification of nonlinear dynamical systems," Proceedings of the national academy of sciences, vol. 113, no. 15, pp. 3932-3937, 2016.
[12] D. Ha and J. Schmidhuber, "Recurrent world models facilitate policy evolution," Advances in neural information processing systems, vol. 31, 2018.
[13] J. Yoon, D. Jarrett, and M. Van der Schaar, "Time-series generative adversarial networks," Advances in neural information processing systems, vol. 32, 2019.
[14] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, "Neural ordinary differential equations," Advances in neural information processing systems, vol. 31, 2018.
[15] M. Raissi, P. Perdikaris, and G. E. Karniadakis, "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations," Journal of Computational physics, vol. 378, pp. 686-707, 2019.
[16] L. Faure, B. Mollet, W. Liebermeister, and J.-L. Faulon, "A neural-mechanistic hybrid approach improving the predictive power of genome-scale metabolic models," Nature Communications, vol. 14, no. 1, p. 4669, 2023.
[17] J. Pinto, J. R. Ramos, R. S. Costa, and R. Oliveira, "A general hybrid modeling framework for systems biology applications: Combining mechanistic knowledge with deep neural networks under the sbml standard," AI, vol. 4, no. 1, pp. 303-318, 2023.
[18] P. Wang, Z. Zhu, W. Liang, L. Liao, and J. Wan, "Hybrid mechanistic and neural network modeling of nuclear reactors," Energy, vol. 282, p. 128931, 2023.
[19] R. Cheng, A. Verma, G. Orosz, S. Chaudhuri, Y. Yue, and J. Burdick, "Control regularization for reduced variance reinforcement learning," in International Conference on Machine Learning. PMLR, 2019, pp. 1141-1150.
[20] E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin, "Large-scale evolution of image classifiers," in International conference on machine learning. PMLR, 2017, pp. 2902-2911.
[21] T. N. Mundhenk, M. Landajuela, R. Glatt, C. P. Santiago, D. M. Faissol, and B. K. Petersen, "Symbolic regression via neural-guided genetic programming population seeding," in Proceedings of the 35th International Conference on Neural Information Processing Systems, 2021, pp. $24912-24923$.
[22] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[23] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-ofthought prompting elicits reasoning in large language models," Advances in neural information processing systems, vol. 35, pp. 24 824-24 837, 2022.
[24] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., "Palm: Scaling language modeling with pathways," Journal of Machine Learning Research, vol. 24, no. 240, pp. 1-113, 2023.
[25] M. J. Kearns and U. Vazirani, An introduction to computational learning theory. MIT press, 1994.
[26] H. A. Simon, "The architecture of complexity," Proceedings of the American philosophical society, vol. 106, no. 6, pp. 467-482, 1962.</p>
<p>[27] T. L. Rogers, B. J. Johnson, and S. B. Munch, "Chaos is not rare in natural ecosystems," Nature Ecology \&amp; Evolution, vol. 6, no. 8, pp. 1105-1111, 2022.
[28] M. Sokolov, M. von Stosch, H. Narayanan, F. Feidl, and A. Butt, "Hybrid modeling-a key enabler towards realizing digital twins in biopharma?" Current Opinion in Chemical Engineering, vol. 34, p. 100715, 2021.
[29] S. Chaudhuri, K. Ellis, O. Polozov, R. Singh, A. Solar-Lezama, Y. Yue et al., "Neurosymbolic programming," Foundations and Trends in Programming Languages, vol. 7, no. 3, pp. 158243, 2021.
[30] A. Tsoularis and J. Wallace, "Analysis of logistic growth models," Mathematical biosciences, vol. 179, no. 1, pp. 21-55, 2002.
[31] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., "Pytorch: An imperative style, high-performance deep learning library," Advances in neural information processing systems, vol. 32, 2019.
[32] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.
[33] J. H. Holland, "Genetic algorithms," Scientific american, vol. 267, no. 1, pp. 66-73, 1992.
[34] L. R. Rabiner, "A tutorial on hidden markov models and selected applications in speech recognition," Proceedings of the IEEE, vol. 77, no. 2, pp. 257-286, 1989.
[35] R. Kalman, "A new approach to linear filtering and prediction problems," Trans. ASME, D, vol. 82, pp. 35-44, 1960.
[36] L. Li, Y. Zhao, D. Jiang, Y. Zhang, F. Wang, I. Gonzalez, E. Valentin, and H. Sahli, "Hybrid deep neural network-hidden markov model (dnn-hmm) based speech emotion recognition," in 2013 Humaine association conference on affective computing and intelligent interaction. IEEE, 2013, pp. 312-317.
[37] R. G. Krishnan, U. Shalit, and D. Sontag, "Deep kalman filters," arXiv preprint arXiv:1511.05121, 2015.
[38] J. L. Elman, "Finding structure in time," Cognitive science, vol. 14, no. 2, pp. 179-211, 1990.
[39] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997.
[40] K. Cho, B. Van Merrinboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, "Learning phrase representations using rnn encoder-decoder for statistical machine translation," arXiv preprint arXiv:1406.1078, 2014.
[41] I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to sequence learning with neural networks," Advances in neural information processing systems, vol. 27, 2014.
[42] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," arXiv preprint arXiv:1409.0473, 2014.
[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, . Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[44] E. Dupont, A. Doucet, and Y. W. Teh, "Augmented neural odes," Advances in neural information processing systems, vol. 32, 2019.
[45] S. I. Holt, Z. Qian, and M. van der Schaar, "Neural laplace: Learning diverse classes of differential equations in the laplace domain," in International Conference on Machine Learning. PMLR, 2022, pp. 8811-8832.</p>
<p>[46] M. A. Zaytar and C. El Amrani, "Sequence to sequence weather forecasting with long short-term memory recurrent neural networks," International Journal of Computer Applications, vol. 143, no. 11, pp. 7-11, 2016.
[47] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[48] L. Sehovac and K. Grolinger, "Deep learning for load forecasting: Sequence to sequence recurrent neural networks with attention," Ieee Access, vol. 8, pp. 36 411-36 426, 2020.
[49] S. Holt, A. Hyk, Z. Qian, H. Sun, and M. van der Schaar, "Neural laplace control for continuous-time delayed systems," in International Conference on Artificial Intelligence and Statistics. PMLR, 2023, pp. 1747-1778.
[50] M. Schmidt and H. Lipson, "Distilling free-form natural laws from experimental data," science, vol. 324, no. 5923, pp. 81-85, 2009.
[51] Z. Qian, K. Kacprzyk, and M. van der Schaar, "D-CODE: Discovering closed-form ODEs from observed trajectories," in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=wENMvIsxNN
[52] K. Kacprzyk, Z. Qian, and M. van der Schaar, "D-cipher: discovery of closed-form partial differential equations," Advances in Neural Information Processing Systems, vol. 36, 2024.
[53] K. Kacprzyk, T. Liu, and M. van der Schaar, "Towards transparent time series forecasting," in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=TYXtXLYHpR
[54] S. Cuomo, V. S. Di Cola, F. Giampaolo, G. Rozza, M. Raissi, and F. Piccialli, "Scientific machine learning through physics-informed neural networks: Where we are and what's next," Journal of Scientific Computing, vol. 92, no. 3, p. 88, 2022.
[55] S. Greydanus, M. Dzamba, and J. Yosinski, "Hamiltonian neural networks," Advances in neural information processing systems, vol. 32, 2019.
[56] M. Cranmer, S. Greydanus, S. Hoyer, P. Battaglia, D. Spergel, and S. Ho, "Lagrangian neural networks," arXiv preprint arXiv:2003.04630, 2020.
[57] Y. Yin, V. Le Guen, J. Dona, E. de Bzenac, I. Ayed, N. Thome, and P. Gallinari, "Augmenting physical models with deep networks for complex dynamics forecasting," Journal of Statistical Mechanics: Theory and Experiment, vol. 2021, no. 12, p. 124012, 2021.
[58] N. Takeishi and A. Kalousis, "Physics-integrated variational autoencoders for robust and interpretable generative modeling," Advances in Neural Information Processing Systems, vol. 34, pp. 14809-14821, 2021.
[59] Z. Qian, W. Zame, L. Fleuren, P. Elbers, and M. van der Schaar, "Integrating expert odes into neural odes: pharmacology and disease progression," Advances in Neural Information Processing Systems, vol. 34, pp. 11364-11383, 2021.
[60] A. Wehenkel, J. Behrmann, H. Hsu, G. Sapiro, G. Louppe, and J.-H. Jacobsen, "Robust hybrid learning with expert augmentation," Transactions on Machine Learning Research, 2023. [Online]. Available: https://openreview.net/forum?id=oe4dl4MCGY
[61] C. Geng, H. Paganetti, and C. Grassberger, "Prediction of Treatment Response for Combined Chemo- and Radiation Therapy for Non-Small Cell Lung Cancer Patients Using a Bio-Mathematical Model," Scientific Reports, vol. 7, no. 1, p. 13542, Oct. 2017.
[62] I. Bica, A. M. Alaa, J. Jordon, and M. van der Schaar, "Estimating counterfactual treatment outcomes over time through adversarially balanced representations," in International Conference on Learning Representations, 2020.
[63] N. Seedat, F. Imrie, A. Bellot, Z. Qian, and M. van der Schaar, "Continuous-time modeling of counterfactual outcomes using neural controlled differential equations," arXiv preprint arXiv:2206.08311, 2022.</p>
<p>[64] V. Melnychuk, D. Frauen, and S. Feuerriegel, "Causal transformer for estimating counterfactual outcomes," in International Conference on Machine Learning. PMLR, 2022, pp. 15 29315329 .
[65] C. C. Kerr, R. M. Stuart, D. Mistry, R. G. Abeysuriya, K. Rosenfeld, G. R. Hart, R. C. Nez, J. A. Cohen, P. Selvaraj, B. Hagedorn et al., "Covasim: an agent-based model of covid-19 dynamics and interventions," PLOS Computational Biology, vol. 17, no. 7, p. e1009149, 2021.
[66] T. Hiltunen, L. Jones, S. Ellner, and N. G. Hairston Jr, "Temporal dynamics of a simple community with intraguild predation: an experimental test," Ecology, vol. 94, no. 4, pp. 773779, 2013.
[67] E. P. Odum and G. W. Barrett, "Fundamentals of ecology," The Journal of Wildlife Management, vol. 36, no. 4, p. 1372, 1972.
[68] V. M. M. Alvarez, R. Roca, and C. G. Flcuteescu, "Dynode: Neural ordinary differential equations for dynamics modeling in continuous control," arXiv preprint arXiv:2009.04278, 2020.
[69] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning representations by backpropagating errors," nature, vol. 323, no. 6088, pp. 533-536, 1986.
[70] S. Holt, A. Hyk, and M. van der Schaar, "Active observing in continuous-time control," Advances in Neural Information Processing Systems, vol. 36, 2024.
[71] S. Holt, Z. Qian, and M. van der Schaar, "Deep generative symbolic regression," in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=o7koEEMA1bR
[72] K. Kacprzyk, S. Holt, J. Berrevoets, Z. Qian, and M. van der Schaar, "ODE discovery for longitudinal heterogeneous treatment effects inference," in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=pxI5IPeWgW
[73] T. Liu, N. Astorga, N. Seedat, and M. van der Schaar, "Large language models to enhance bayesian optimization," in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=OOxotBmGol
[74] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen, "Large language models as optimizers," in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=Bb4VGOWELI
[75] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," arXiv preprint arXiv:2107.03374, 2021.
[76] S. Holt, M. R. Luyten, and M. van der Schaar, "L2MAC: Large language model automatic computer for extensive code generation," in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=EhrzQwsV4K
[77] S. Holt, Z. Qian, T. Liu, J. Weatherall, and M. van der Schaar, "Data-driven discovery of dynamical systems in pharmacology using large language models," in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.
[78] N. Astorga, T. Liu, N. Seedat, and M. van der Schaar, "Partially observable cost-aware activelearning with large language models," in The Thirty-Eighth Annual Conference on Neural Information Processing Systems, 2024.
[79] W. Bonnaff and T. Coulson, "Fast fitting of neural ordinary differential equations by bayesian neural gradient matching to infer ecological interactions from time-series data," Methods in Ecology and Evolution, vol. 14, no. 6, pp. 1543-1563, 2023.
[80] S. K. Kumar, "On weight initialization in deep neural networks," arXiv preprint arXiv:1704.08863, 2017.</p>
<p>[81] A. Graves, S. Fernndez, and J. Schmidhuber, "Multi-dimensional recurrent neural networks," in International conference on artificial neural networks. Springer, 2007, pp. 549-558.
[82] B. K. Petersen, M. L. Larma, T. N. Mundhenk, C. P. Santiago, S. K. Kim, and J. T. Kim, "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients," in International Conference on Learning Representations, 2020.
[83] S. Hsiang, D. Allen, S. Annan-Phan, K. Bell, I. Bolliger, T. Chong, H. Druckenmiller, L. Y. Huang, A. Hultgren, E. Krasovich et al., "The effect of large-scale anti-contagion policies on the covid-19 pandemic," Nature, vol. 584, no. 7820, pp. 262-267, 2020.
[84] O. N. Bjrnstad, K. Shea, M. Krzywinski, and N. Altman, "The seirs model for infectious disease dynamics." Nature methods, vol. 17, no. 6, pp. 557-559, 2020.
[85] F. Brauer, C. Castillo-Chavez, and C. Castillo-Chavez, Mathematical models in population biology and epidemiology. Springer, 2012, vol. 2.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>A HDTwinGen Overview ..... 17
B Extended Related Work ..... 17
C Benchmark Dataset Environment Details ..... 18
C. 1 Cancer PKPD ..... 18
C. 2 COVID-19 ..... 19
C. 3 Plankton Microcosm ..... 19
C. 4 Hare-Lynx ..... 20
D Benchmark Method Implementation Details ..... 20
E HDTwinGen Implementation Details ..... 21
E. 1 HDTwin pseudocode ..... 21
E. 2 Training HDTwins ..... 21
E. 3 HDTwinGen Prompt Templates ..... 22
E. 4 HDTwinGen System Requirements Prompts ..... 23
F Model Optimization Losses ..... 27
G Evaluation Metrics ..... 27
H Additional Results ..... 28
H. 1 Out-of-distribution Experiment and Setup ..... 28
H. 2 COVID-19 Unobserved Intervention Experiment and Setup ..... 28
H. 3 HDTwinGen top-1 decreases over time ..... 28
H. 4 HDTwinGen Evolution ..... 29
H. 5 HDTwinGen Ablation No Memory ..... 32
H. 6 Evaluating Different LLMs ..... 32
H. 7 Prompt Ablations with Varying Amounts of Prior Information ..... 32
H. 8 Domain-Specific Baselines ..... 33
H. 9 Procedurally Generated Synthetic Model Benchmark ..... 34
H. 10 Interpretability Scale, Performance of only White-Box Models ..... 35
H. 11 HDTwinGen Flexibly Integrates Expert-in-the-loop Feedback ..... 35
H. 12 HDTwinGen Accelerates Model Development and Enhances Performance ..... 36
I Hybrid Model Output Examples ..... 38
J HDTwinGen can reason about HDTwins ..... 40</p>
<h1>A HDTwinGen Overview</h1>
<p>We provide an illustrative example of HDTwinGen working in practice in Figure 5.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: HDTwinGen Illustrative Example in Operation. HDTwinGen can generate and further evolve HDTwins for a particular system based on user-given system requirements and a dataset $\mathcal{D}=\left{\mathcal{D}<em _test="{test" _text="\text">{\text {train }}, \mathcal{D}</em>}}\right}$ of state-action trajectories. First, the system requirements-which include dataset statistics are incorporated into a prompt and fed into the modeling agent that returns the code for the HDTwin. This HDTwin is then trained on the training dataset $\mathcal{D<em _text="\text" _val="{val">{\text {train }}$, and a validation loss is computed with $\mathcal{D}</em>$. In subsequent generations, the evaluation agent is given the existing generated top- $K$ HDTwins, their corresponding validation losses, and validation losses per component, and is asked to reflect on how to improve the HDTwin. This provides detailed, actionable feedback, leveraged from its inherent understanding, and provides this as detailed verbal feedback as $H$, whereby this feedback is next used with the modeling agent to generate the next HDTwin [P3]. This process iterates several generation times, and the best-performing HDTwin (w.r.t. validation performance) is returned. Overall, this produces an HDTwin that fulfills [P1-P3].}</p>
<h2>B Extended Related Work</h2>
<p>Sequence models. ML approaches frequently address system dynamics as a sequential modeling problem. Initial models like Hidden Markov Models [34] and Kalman filters [35] made simplifying Markovian and linearity assumptions, later extended to nonlinear settings [36, 37]. Subsequent models, including recurrent neural networks [38], along with their advanced variants [39, 40, 41], introduced the capability to model longer-term dependencies. More recent advancements include attention mechanisms [42] and transformer models [43], significantly improving the handling of long-term dependencies in sequence data. Another line of work, Neural Ordinary Differential Equations (NODE) [14, 44, 45], interprets neural network operations as integrations of differential equations to model continuous-time processes. Despite being initially driven by natural language processing applications [47], these methods have found utility in modeling complex systems like weather forecasting [46] and energy systems [48]. Furthermore, sequence models can be used in model-based RL [70].
Physics-inspired models. Beyond purely data-centric approaches, recent efforts have focused on integrating physical laws into neural system models. Physics-informed neural networks [15, 54] embed physical laws, often as partial differential equations, directly into the learning process. Other notable methods include Hamiltonian Neural Networks [55] and Lagrangian Neural Networks [56], which respect the structural principles of physical systems. These methods are primarily concerned with modeling physics-related phenomenon and require relatively precise knowledge about the system being modeled (e.g. specific differential equations or energy conservation principles) and specialized mechanisms to incorporate them. Regardless, they have demonstrated that the integration of known principles can significantly improve extrapolation abilities beyond the range of training data. We are similarly inspired to incorporate prior knowledge. In contrast, our work aims to integrate more general or partial knowledge flexibly into a hybrid model using LLMs within a evolutionary multi-agent framework, while introducing more generalized mechanisms to incorporate loose-form prior knowledge.</p>
<p>Discovering closed-form models. Closely aligned with our research are techniques aimed at discovering closed-form mathematical expressions from data. Symbolic regression [10, 71, 72] and methods like Eureqa [50], SINDy [11], and D-CODE [51, 52] have showcased their prowess in discovering physical laws from experimental observations. However, these techniques can struggle in higher-dimensional settings and rely on experts to perform the system decomposition to identify the most relevant variables before feeding this information to the algorithm. Additionally, they also rely on experts to specify the function set and mathematical operations that the algorithm uses to search for symbolic expressions. In contrast, our method autonomously learns both the system decomposition and the functional forms of component dynamics, potentially enhancing scalability and efficiency. Moreover, the incorporation of LLMs facilitates the flexible integration of prior knowledge at various stages of the search process [73, 74]. Furthermore, using LLMs to generate code, prior work has shown LLM multi-agent frameworks' ability to excel at large code-generation tasks [75, 76], which we could expect to apply here to scale up the size of the generated models in future works. Such future work could also explore acquiring features as well [77, 78].</p>
<h1>C Benchmark Dataset Environment Details</h1>
<p>In the following, we outline the six real-world system dynamics datasets, where each dataset is either a real-world dataset or has been sampled from an accurate simulator designed by human experts.</p>
<h2>C. 1 Cancer PKPD</h2>
<p>Three of our environments that we sample a dataset from are derived from a state-of-the-art biomedical Pharmacokinetic-Pharmacodynamic (PKPD) model of lung cancer tumor growth, used to simulate the combined effects of chemotherapy and radiotherapy in lung cancer [61]-this has been extensively used by other works [62, 63, 64]. Here we use this bio-mathematical lung cancer model to create three variations of lung cancer under the effect of no treatments (Lung Cancer), chemotherapy only (Lung Cancer (with Chemo.)), and chemotherapy combined with radiotherapy (Lung Cancer (with Chemo. \&amp; Radio.)); for each model we sample a respective dataset. First, let us detail the general case of Lung Cancer (with Chemo. \&amp; Radio.), which comes from the general model (Cancer PKPD Model), and then detail the variations.</p>
<p>Cancer PKPD Model. This is a state-of-the-art biomedical Pharmacokinetic-Pharmacodynamic (PKPD) model of tumor growth, that simulates the combined effects of chemotherapy and radiotherapy in lung cancer [61] (Equation (2))-this has been extensively used by other works [62, 63, 64]. Specifically, this models the volume of the tumor $x(t)$ for days $t$ after the cancer diagnosis-where the outcome is one-dimensional. The model has two binary treatments: (1) radiotherapy $u_{t}^{r}$ and (2) chemotherapy $u_{t}^{c}$.</p>
<p>$$
\frac{d x(t)}{d t}=\left(\underbrace{\rho \log \left(\frac{K}{x(t)}\right)}<em c="c">{\text {Tumorgrowth }}-\underbrace{\beta</em>} C(t)<em r="r">{\text {Chemotherapy }}-\underbrace{\left(\alpha</em>\right) x(t)
$$} d(t)+\beta_{r} d(t)^{2}\right)}_{\text {Radiotherapy }</p>
<p>Where the parameters $K, \rho, \beta_{c}, \alpha_{r}, \beta_{r}$ for each simulated patient are detailed in [61], which are also described in Table 3. Additionally, the chemotherapy drug concentration $c(t)$ follows an exponential</p>
<p>Table 3: Cancer PKPD parameter values.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Variable</th>
<th style="text-align: center;">Parameter</th>
<th style="text-align: center;">Parameter Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Tumor growth</td>
<td style="text-align: center;">Growth parameter</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$7.00 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Carrying capacity</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">Radiotherapy</td>
<td style="text-align: center;">Radio cell kill $(\alpha)$</td>
<td style="text-align: center;">$\alpha_{r}$</td>
<td style="text-align: center;">0.0398</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Radio cell kill $(\beta)$</td>
<td style="text-align: center;">$\beta_{r}$</td>
<td style="text-align: center;">Set s.t. $\alpha / \beta=10$</td>
</tr>
<tr>
<td style="text-align: center;">Chemotherapy</td>
<td style="text-align: center;">Chemo cell kill</td>
<td style="text-align: center;">$\beta_{c}$</td>
<td style="text-align: center;">0.028</td>
</tr>
</tbody>
</table>
<p>decay relationship with a half-life of one day:</p>
<p>$$
\frac{d c(t)}{d t}=-0.5 c(t)
$$</p>
<p>where the chemotherapy binary action represents increasing the $c(t)$ concentration by $5.0 \mathrm{mg} / \mathrm{m}^{3}$ of Vinblastine given at time $t$. Whereas the radiotherapy concentration $d(t)$ represents $2.0 G y$ fractions of radiotherapy given at timestep $t$, where Gy is the Gray ionizing radiation dose.</p>
<p>Time-dependent confounding. We introduce time-varying confounding into the data generation process. This is accomplished by characterizing the allocation of chemotherapy and radiotherapy as Bernoulli random variables. The associated probabilities, $p_{c}$ and $p_{r}$, are determined by the tumor diameter as follows:</p>
<p>$$
p_{c}(t)=\sigma\left(\frac{\gamma_{c}}{D_{\max }}\left(\bar{D}(t)-\delta_{c}\right)\right) \quad p_{r}(t)=\sigma\left(\frac{\gamma_{r}}{D_{\max }}\left(\bar{D}(t)-\delta_{r}\right)\right)
$$</p>
<p>where $D_{\max }=13 \mathrm{~cm}$ represents the largest tumor diameter, $\theta_{c}=\theta_{r}=D_{\max } / 2$ and $\bar{D}(t)$ signifies the mean tumor diameter. The parameters $\gamma_{c}$ and $\gamma_{r}$ manage the extent of time-varying confounding. We use $\gamma_{c}=\gamma_{r}=2$.
Sampling datasets. Using the above Cancer PKPD model, we sample $N=1,000$ trajectories, which equates to $N=10,000$ patients, where we sample their initial tumor volumes from a uniform distribution $x(0) \sim \mathcal{U}(0,1149)$, and use the Cancer PKPD Equation (2) along with the action policy of Equation (4) to forward simulate patient trajectories for 60 days, using a Euler stepwise solver. This forms one dataset sample. We repeat this process with independent random seeds to generate $\mathcal{D}<em _text="\text" _val="{val">{\text {train }}, \mathcal{D}</em>$. Specifically for each benchmark method run for random seed, we re-sample the datasets. For each variation described above, we either include the chemotherapy dosing action, chemotherapy and radiotherapy dosing action or neither. We further outline this dataset's system description and variable descriptions with the following prompt template as given in Appendix E.4.}}, \mathcal{D}_{\text {test }</p>
<h1>C. 2 COVID-19</h1>
<p>We use the accurate and complex epidemic agent-based simulator of COVASIM [65] to simulate COVID-19 epidemics. This is an advanced simulator that is capable of simulating non-pharmaceutical interventions (such as lockdowns through social distancing, and school closures) and pharmaceutical interventions (such as vaccinations). As this is an agent-based simulator, each agent is an individual in a population, and they can be in one of the following states minimally, of being susceptible to COVID-19, exposed, infectious or recovered (which includes deaths). We use the simulator with the default parameters set by the open source implementation of the simulator ${ }^{5}$. COVASIM runs a simulation for a population of individuals. To ensure an accurate simulation, we simulate 24 countries collecting trajectories for each, wherein each simulation we use a population size of $1,000,000$ individuals, and simulate each individual separately (disabling simulation rescaling) and start with a random number of individuals who are infected with COVID-19, $I(0)=\mathcal{U}(10,000,100,000)$, and forward simulate the simulation for 60 days. We repeat this process with independent random seeds to generate $\mathcal{D}<em _text="\text" _val="{val">{\text {train }}, \mathcal{D}</em>$. Specifically for each benchmark method run for random seed, we re-sample the datasets. We further outline this dataset's system description and variable descriptions with the following prompt template as given in Appendix E. 4 .}}, \mathcal{D}_{\text {test }</p>
<h2>C. 3 Plankton Microcosm</h2>
<p>This describes an ecological model of a microcosm of algae, flagellate, and rotifer populations, thus replicating an experimental three-species prey-predator system [66]. We use the dataset made available by [79] ${ }^{6}$. The dataset consists of a single trajectory of 102 time steps, and we use a train, val, test split of $70 \%, 15 \%$ and $15 \%$, ensuring the splits are along the time dimension to maintain the integrity of temporal causality, following their chronological order. We further outline this dataset's system description and variable descriptions with the following prompt template as given in Appendix E. 4 .</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C. 4 Hare-Lynx</h1>
<p>This describes a real-world dataset of hare and lynx populations, replicating predator-prey dynamics [67]. We use the dataset made available by [79]. The dataset consists of a single trajectory of 92 time steps, and we use a train, val, test split of $70 \%, 15 \%$ and $15 \%$, ensuring the splits are along the time dimension to maintain the integrity of temporal causality, following their chronological order. We further outline this dataset's system description and variable descriptions with the following prompt template as given in Appendix E. 4 .</p>
<h2>D Benchmark Method Implementation Details</h2>
<p>To be competitive we compare against popular black-box models, which, when modeling the dynamics of a system over time, becomes a form of ODE model, that is a neural ODE [14] with action inputs (DyNODE) [68]. We also compare against transparent dynamical equations derived from equation discovery methods for ODEs such as Sparse Identification of Nonlinear Dynamics (SINDy) [11]. Moreover, we compare against the ablations of our method, of the zero-shot generated HDTwin (ZeroShot) and this model with subsequently optimized parameters (ZeroOptim).</p>
<h2>DyNODE</h2>
<p>DyNODE is a black-box neural network-based dynamics model [68], that models the underlying dynamics of a system by incorporating control into the standard neural ordinary differential equation framework [14]. We use a DyNODE model with 3-layer Multilayer Perceptron (MLP), with a hidden dimension of 128 units, with tanh activation functions, and make it competitive by using Xavier weight initialization [80]. To be competitive we use the same objective, optimizer and same hyperparameters for the optimizer that we use in HDTwinGen. That of an Adam optimizer [32], with a learning rate of 0.01 , with a batch size of 1,000 and early stopping with a patience of 20 , and train it for 2,000 epochs to ensure it converges.</p>
<h2>Causal Transformer</h2>
<p>Causal Transformer is a state-of-the-art transformer model for estimating counterfactual outcomes [64]. Due to the complexity of the Causal Transformer, incorporating three separate transformer networks, each one for processing covariates, past treatments, and past outcomes, respectivelywhich is unique to estimating counterfactual outcomes in treatment effect settings; we implemented only a single transformer to model the past outcomes, which is applicable to our datasets and task domains. Specifically, this consists of a standard transformer encoder, where the input dataset is normalized to the training dataset. We encode input observed dimension of the state-action into an embedding vector dimension of size 250 through a linear layer, followed by the addition of a standard positional encoder [64]; this is then fed into a transformer encoder layer, with a head size of 10, dropout 0.1 , and the output of this is then fed into a linear layer to reconstruct the next step ahead state, of size of the state dimension. We train this model using the AdamW [32] optimizer with a learning rate of 0.00005 and a step learning rate scheduler of step size 1.0 and gamma 0.95 ; we also implement gradient clipping to 0.7 , with a batch size of 1,000 and early stopping with a patience of 20 , and train it for 2,000 epochs to ensure it converges.</p>
<h2>RNN</h2>
<p>Recurrent Neural Network [81] is a standard baseline that is widely used in autoregressive time series next step ahead prediction. We implement this where the input dataset is normalized to the training dataset. It consists of a gated recurrent unit RNN taking the state-action dimension in mapping it to a hidden dimension of size 250, with two layers. The output is then fed to a linear layer to convert the hidden dimension back to the state dimension to predict the next step ahead. To be competitive we use the same objective, optimizer and same hyperparameters for the optimizer that we use in HDTwinGen. That of an Adam optimizer [32], with a learning rate of 0.01 , with a batch size of 1,000 and early stopping with a patience of 20 , and train it for 2,000 epochs to ensure it converges.</p>
<h2>SINDy</h2>
<p>Sparse Identification of Nonlinear Dynamics (SINDy) [11], is a data-driven framework that aims to discover the governing dynamical system equations directly from time-series data, discovering a white-box closed-form mathematical model. The algorithm works by iteratively performing sparse</p>
<p>regression on a library of candidate functions to identify the sparsest yet most accurate representation of the dynamical system.</p>
<p>In our implementation, we use a polynomial library of order two, which is a feature library of $\mathcal{L}=\left{1, x_{0}, x_{1}, x_{0} x_{1}\right}$. Finite difference approximations are used to compute time derivatives from the input time-series data, of order one. Here the alpha parameter is kept constant at 0.5 across all experiments, and the sparsity threshold is set to 0.02 for all experiments, apart from the COVID-19 dataset where it is set to $1 \times 10^{-5}$.</p>
<h1>APHYNITY</h1>
<p>APHYNITY [60] is implemented using domain-specific expert models as defined in Appendix H. 8 combined with a 3-layer MLP, with the same hyper-parameters as in [60].</p>
<h2>GP</h2>
<p>Genetic programming (GP) is implemented using the implementation and hyper-parameters from the baseline in [82].</p>
<h2>HDTwinGen</h2>
<p>See the section Appendix E for the implementation details. Specifically, ZeroShot and ZeroOptim are ablations of our method using the exact same setup, hyperparameters and prompts. Here ZeroShot generates one HDTwin, and does not fit its parameters, thus evaluating the loss of the model output directly from the LLM. Whereas ZeroOptim, repeats ZeroShot with the additional step of optimizing the parameters of the HDTwin that was generated-again using the same training as detailed in Appendix E.2.</p>
<h2>E HDTwinGen Implementation Details</h2>
<p>Our proposed method follows the framework as described in Section 4. We present pseudocode in Appendix E.1, how the code-generated HDTwins are trained in Appendix E.2, prompt templates in Appendix E.3, system requirements prompts in Appendix E. 4 for each dataset, and we provide examples of training runs in Appendix 3. Specifically, we find a top-K, where $K=16$ is sufficient. Additionally, we use the LLM of GPT4-1106-Preview, with a temperature of 0.7 .</p>
<h2>E. 1 HDTwin pseudocode</h2>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Pseudocode for Hybrid Digital Twin Generator Framework
    Input: modeling context \(\mathcal{S}^{\text {context }}\), training dataset \(\mathcal{D}_{\text {train }}\), validation dataset \(\mathcal{D}_{\text {val }}\), maximum
        generations \(G\), top \(K\) programs to consider, \(\mathcal{R}\)
    Output: Best fitting hybrid model \(f_{\theta, \omega(\theta)^{*}}\).
    \(\mathcal{P} \leftarrow \emptyset, H \leftarrow \emptyset\)
    for \(g=1\) to \(G\) do
        \(f_{\theta, \omega(\theta)} \sim L L M_{\text {model }}\left(H, \mathcal{P}^{(g)}, \mathcal{S}^{\text {context }}\right)\{\text { Generate HDTwin from modeling agent }\}\)
        \(\omega(\theta)^{*}=\arg \min <span class="ge">_{\omega(\theta) \in \Omega(\theta)} \mathcal{L}\left(f_</span>{\theta, \omega(\theta)}, \mathcal{D}_{\text {train }}\right)\{\text { Fit the model }\}\)
        Compute validation loss per component and overall \(\delta, v\)
        \(\mathcal{P}^{(g+1)} \leftarrow \mathcal{P}^{(g)} \oplus\left(f_{\theta, \omega(\theta)^{*}}, \delta, v\right)\{\text { Add HDTwin to the set of top-K HDTwins }\}\)
        \(H \sim L L M_{\text {eval }}\left(\mathcal{R}, \mathcal{P}^{(g)}\right)\{\text { Generate self-reflection on how to improve the HDTwin. }\}\)
    end for
    Return: \(f_{\theta, \omega(\theta)^{*}}\{\\) The best fitting model that scored the lowest validation loss \(\}\)
</code></pre></div>

<h2>E. 2 Training HDTwins</h2>
<p>Once the modeling agent has generated an HDTwin $f_{\theta, \omega(\theta)}$, it generates it as code. Specifically, it outputs code for a PyTorch [31] neural network module, where this code string is executed, and the module is then trained on the training dataset. The agent importantly observes a code skeleton within its system requirements context $\mathcal{S}^{\text {context }}$, of which examples of such a skeleton are given in Appendix E.4. However we stipulate that the skeleton must be a "torch.nn.Module", be called</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ COVASIM is an opensource simulator, from which we access it here https://github.com/ InstituteforDiseaseModeling/covasim.
${ }^{6}$ The Plankton Microcosm and Hare-Lynx datasets are both open source and available from https:// github.com/WillemBonnaffe/NODEBNGM.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
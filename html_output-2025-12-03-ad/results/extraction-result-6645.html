<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6645 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6645</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6645</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-2208e506f72518a16ea86dfa604995c12fa8e4ca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2208e506f72518a16ea86dfa604995c12fa8e4ca" target="_blank">Premise Order Matters in Reasoning with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> It is observed that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps, and permuting the premise order can cause a performance drop of over 30%.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6645.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6645.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's high-capability autoregressive large language model evaluated in this paper on multi-step grade-school math problems (R-GSM) and shown to be the strongest of the evaluated models on these arithmetic tasks, but still sensitive to sentence ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>R-GSM (derived from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (grade-school mathematics / arithmetic + reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problem (problem description only; no extra instructions on R-GSM)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-style grade-school problems (2–8 reasoning steps; 5–8 sentences in R-GSM variants)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot prompting; greedy decoding (temperature=0); for R-GSM the input contained only the problem description (no chain-of-thought or few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-answer correctness of final solution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>R-GSM overall: Init Acc 94.1%, Reorder Acc 85.0% (Table 2a). On subset where original was solved by the model: Init 100% -> Reorder 89.9% (Table 2b).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probing (attention/activation/logit-lens) was performed for arithmetic; analysis is behavioral: models tend to process sentences left-to-right and often bind intermediate numeric values to the order they appear rather than to the facts they represent.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Common failures include: (1) overlooking temporal or causal order when sentences are reordered ('temporal' errors), (2) using numbers from previous sequential steps incorrectly (blind sequential use of numbers), (3) encountering unknown intermediate variables when the required numeric information appears later in the reordered text, and (4) other arithmetic and reasoning mistakes when sentence order does not align with solution steps.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Qualitative: GPT-4-turbo (most capable in this study) shows highest accuracy and is less impacted by reordering than weaker models; paper does not present systematic model-size scaling curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Premise Order Matters in Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6645.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6645.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM 2-L evaluated on R-GSM problems; strong arithmetic/word-problem performance but sensitive to sentence reordering, with performance drops on longer/more-step problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>R-GSM (derived from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (grade-school mathematics / arithmetic + reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problem (problem description only)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-style (grade-school; 2–8 reasoning steps; 5–8 sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot prompting; greedy decoding (temperature=0); input only problem description for R-GSM</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>R-GSM overall: Init Acc 86.4%, Reorder Acc 79.5% (Table 2a). On subset where original was solved by the model: Init 100% -> Reorder 87.9% (Table 2b).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic probes reported; behavioral analysis indicates PaLM 2-L is less likely than some models to commit certain sequential-number errors but still suffers when sentence order breaks the natural calculation order.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Temporal-order errors (overlooking that a needed numeric quantity appears later), introducing unknown variables when processing sequentially, and other arithmetic reasoning mistakes when problem sentence order mismatches the step order.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Qualitative: PaLM 2-L performs comparably to GPT-4-turbo on many problems but shows different sensitivity patterns (e.g., worse with backward ordering versus forward); no explicit parameter-scale trend reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Premise Order Matters in Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6645.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6645.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 1.0 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.0 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini 1.0 Pro evaluated on R-GSM; exhibits substantial accuracy drop under sentence reordering and relatively high rates of error compared to top models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.0 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>R-GSM (derived from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (grade-school math)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problem</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-style grade-school (2–8 steps; 5–8 sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot prompting; greedy decoding (temperature=0); input only problem description</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>R-GSM overall: Init Acc 80.5%, Reorder Acc 69.1% (Table 2a). On subset where original was solved by the model: Init 100% -> Reorder 74.6% (Table 2b).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic interpretability analyses; behavioral error breakdown indicates a high proportion of 'Unknown' and 'Others' categories alongside temporal-order mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High rate of fact/number misuse when sentences reordered; temporal-order errors; unknown-variable issues; less robust to reorderings than the strongest models.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Qualitative: Gemini shows larger absolute degradation on reorder variants compared to stronger models; no parametric scaling analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Premise Order Matters in Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6645.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6645.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5-turbo evaluated on R-GSM; weakest arithmetic performance among evaluated models and most sensitive to sentence reordering, with large drops in accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>R-GSM (derived from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (grade-school math)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problem</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-style (grade-school problems; 2–8 steps; 5–8 sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot prompting; greedy decoding (temperature=0); input only problem description</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>R-GSM overall: Init Acc 67.3%, Reorder Acc 51.8% (Table 2a). On subset where original was solved by the model: Init 100% -> Reorder 64.9% (Table 2b). Performance degradation on reordered problems can exceed 35% relative to initial solved set.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No attention/activation probing; paper's behavioral analysis shows GPT-3.5 often 'uses' numbers in the literal sequential order they appear and thus substitutes wrong quantities when reorder disrupts the calculation order.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Blind sequential use of numbers from earlier steps, temporal-order mistakes, substituting wrong previously computed number for a different quantity when sentence order changes, leading to arithmetic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Qualitative: GPT-3.5-turbo performs worst of the evaluated models and shows the largest relative drop when problems are reordered; paper does not provide systematic scaling with model size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Premise Order Matters in Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6645.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6645.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R-GSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reordered GSM8K (R-GSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new benchmark introduced in this paper: 220 pairs of GSM8K-based grade-school math problems where one variant is the original and the paired variant is a manually rewritten sentence-order-preserving variant that keeps the same ground-truth answer but changes sentence order to test sensitivity to premise ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>R-GSM (constructed from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic + reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems (5–8 sentences; last sentence kept fixed while others may be permuted and lightly edited for grammar)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-grade-school level; problems selected with ≥5 sentences and 2–8 reasoning steps</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot evaluation using model's default interface; model input contains only the problem description (no explicit chain-of-thought or additional instructions in R-GSM experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact final-answer correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dataset-level observed impacts: all evaluated LLMs show reduced accuracy on reordered variants; e.g., GPT-4-turbo 94.1% -> 85.0% (Table 2a).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral error analysis provided: errors categorized as 'Temporal' (overlooking temporal order), 'Unknown' (intermediate unknown variables when reading sequentially), and 'Others'. No internal mechanistic probes on representations of numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Primary failure modes induced by reordering: temporal-order confusions, treating numbers as positional/sequence-bound rather than semantically bound, and cascading arithmetic mistakes when necessary numbers appear later.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Paper reports that more capable models degrade less on reordering (GPT-4-turbo and PaLM 2-L degrade less than GPT-3.5), but no systematic parametric scaling analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Premise Order Matters in Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6645.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6645.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM8K (Grade School Math 8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used benchmark of grade-school math word problems used as the base for R-GSM; models are evaluated for multi-step arithmetic reasoning on natural-language problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (grade-school arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school level; multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>commonly zero-shot/few-shot and chain-of-thought in literature, but in this paper GSM8K was used only as the source for R-GSM construction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>This paper uses GSM8K as the source dataset for constructing reordered variants; no new internal analysis of GSM8K itself reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Premise Order Matters in Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6645.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6645.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long-digit-add reversal (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input reversal for long-digit addition (prior works: Lee et al., Zhou et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work cited by the paper showing that reversing digits (presenting least-significant digit first) simplifies learning of long-digit addition for Transformers and improves length generalization, illustrating that input order can change how models compute arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-digit addition (synthetic arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>symbolic digit sequences (reversed vs. normal ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>long sequence addition / length generalization</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>training-time input ordering (not natural-language prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>length generalization / addition accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral/algorithmic analysis from prior work: reversal enables a local per-digit computation with carry, reducing need to attend across distant positions; cited in this paper as conceptual support for order effects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without reversal, Transformers often need to attend across many digits and fail to generalize to longer lengths; reversal alleviates this.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Prior works show improved length generalization when reversal applied; not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Premise Order Matters in Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems. <em>(Rating: 2)</em></li>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>What algorithms can transformers learn? a study in length generalization <em>(Rating: 2)</em></li>
                <li>Transformers can achieve length generalization but not robustly <em>(Rating: 2)</em></li>
                <li>Unnatural error correction: Gpt-4 can almost perfectly handle unnatural scrambled text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6645",
    "paper_id": "paper-2208e506f72518a16ea86dfa604995c12fa8e4ca",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GPT-4-turbo",
            "name_full": "GPT-4-turbo",
            "brief_description": "OpenAI's high-capability autoregressive large language model evaluated in this paper on multi-step grade-school math problems (R-GSM) and shown to be the strongest of the evaluated models on these arithmetic tasks, but still sensitive to sentence ordering.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-turbo",
            "model_family": "autoregressive decoder-only Transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "R-GSM (derived from GSM8K)",
            "task_type": "multi-step word problems (grade-school mathematics / arithmetic + reasoning)",
            "problem_format": "natural-language word problem (problem description only; no extra instructions on R-GSM)",
            "difficulty_level": "GSM8K-style grade-school problems (2–8 reasoning steps; 5–8 sentences in R-GSM variants)",
            "prompting_method": "zero-shot prompting; greedy decoding (temperature=0); for R-GSM the input contained only the problem description (no chain-of-thought or few-shot).",
            "performance_metric": "accuracy (exact-answer correctness of final solution)",
            "performance_value": "R-GSM overall: Init Acc 94.1%, Reorder Acc 85.0% (Table 2a). On subset where original was solved by the model: Init 100% -&gt; Reorder 89.9% (Table 2b).",
            "internal_analysis": "No mechanistic probing (attention/activation/logit-lens) was performed for arithmetic; analysis is behavioral: models tend to process sentences left-to-right and often bind intermediate numeric values to the order they appear rather than to the facts they represent.",
            "failure_modes": "Common failures include: (1) overlooking temporal or causal order when sentences are reordered ('temporal' errors), (2) using numbers from previous sequential steps incorrectly (blind sequential use of numbers), (3) encountering unknown intermediate variables when the required numeric information appears later in the reordered text, and (4) other arithmetic and reasoning mistakes when sentence order does not align with solution steps.",
            "scaling_trend": "Qualitative: GPT-4-turbo (most capable in this study) shows highest accuracy and is less impacted by reordering than weaker models; paper does not present systematic model-size scaling curves.",
            "uuid": "e6645.0",
            "source_info": {
                "paper_title": "Premise Order Matters in Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PaLM 2-L",
            "name_full": "PaLM 2-L",
            "brief_description": "Google's PaLM 2-L evaluated on R-GSM problems; strong arithmetic/word-problem performance but sensitive to sentence reordering, with performance drops on longer/more-step problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L",
            "model_family": "autoregressive decoder-only Transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "R-GSM (derived from GSM8K)",
            "task_type": "multi-step word problems (grade-school mathematics / arithmetic + reasoning)",
            "problem_format": "natural-language word problem (problem description only)",
            "difficulty_level": "GSM8K-style (grade-school; 2–8 reasoning steps; 5–8 sentences)",
            "prompting_method": "zero-shot prompting; greedy decoding (temperature=0); input only problem description for R-GSM",
            "performance_metric": "accuracy",
            "performance_value": "R-GSM overall: Init Acc 86.4%, Reorder Acc 79.5% (Table 2a). On subset where original was solved by the model: Init 100% -&gt; Reorder 87.9% (Table 2b).",
            "internal_analysis": "No internal mechanistic probes reported; behavioral analysis indicates PaLM 2-L is less likely than some models to commit certain sequential-number errors but still suffers when sentence order breaks the natural calculation order.",
            "failure_modes": "Temporal-order errors (overlooking that a needed numeric quantity appears later), introducing unknown variables when processing sequentially, and other arithmetic reasoning mistakes when problem sentence order mismatches the step order.",
            "scaling_trend": "Qualitative: PaLM 2-L performs comparably to GPT-4-turbo on many problems but shows different sensitivity patterns (e.g., worse with backward ordering versus forward); no explicit parameter-scale trend reported.",
            "uuid": "e6645.1",
            "source_info": {
                "paper_title": "Premise Order Matters in Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini 1.0 Pro",
            "name_full": "Gemini 1.0 Pro",
            "brief_description": "Gemini 1.0 Pro evaluated on R-GSM; exhibits substantial accuracy drop under sentence reordering and relatively high rates of error compared to top models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini 1.0 Pro",
            "model_family": "autoregressive decoder-only Transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "R-GSM (derived from GSM8K)",
            "task_type": "multi-step word problems (grade-school math)",
            "problem_format": "natural-language word problem",
            "difficulty_level": "GSM8K-style grade-school (2–8 steps; 5–8 sentences)",
            "prompting_method": "zero-shot prompting; greedy decoding (temperature=0); input only problem description",
            "performance_metric": "accuracy",
            "performance_value": "R-GSM overall: Init Acc 80.5%, Reorder Acc 69.1% (Table 2a). On subset where original was solved by the model: Init 100% -&gt; Reorder 74.6% (Table 2b).",
            "internal_analysis": "No mechanistic interpretability analyses; behavioral error breakdown indicates a high proportion of 'Unknown' and 'Others' categories alongside temporal-order mistakes.",
            "failure_modes": "High rate of fact/number misuse when sentences reordered; temporal-order errors; unknown-variable issues; less robust to reorderings than the strongest models.",
            "scaling_trend": "Qualitative: Gemini shows larger absolute degradation on reorder variants compared to stronger models; no parametric scaling analysis provided.",
            "uuid": "e6645.2",
            "source_info": {
                "paper_title": "Premise Order Matters in Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "OpenAI's GPT-3.5-turbo evaluated on R-GSM; weakest arithmetic performance among evaluated models and most sensitive to sentence reordering, with large drops in accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_family": "autoregressive decoder-only Transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "R-GSM (derived from GSM8K)",
            "task_type": "multi-step word problems (grade-school math)",
            "problem_format": "natural-language word problem",
            "difficulty_level": "GSM8K-style (grade-school problems; 2–8 steps; 5–8 sentences)",
            "prompting_method": "zero-shot prompting; greedy decoding (temperature=0); input only problem description",
            "performance_metric": "accuracy",
            "performance_value": "R-GSM overall: Init Acc 67.3%, Reorder Acc 51.8% (Table 2a). On subset where original was solved by the model: Init 100% -&gt; Reorder 64.9% (Table 2b). Performance degradation on reordered problems can exceed 35% relative to initial solved set.",
            "internal_analysis": "No attention/activation probing; paper's behavioral analysis shows GPT-3.5 often 'uses' numbers in the literal sequential order they appear and thus substitutes wrong quantities when reorder disrupts the calculation order.",
            "failure_modes": "Blind sequential use of numbers from earlier steps, temporal-order mistakes, substituting wrong previously computed number for a different quantity when sentence order changes, leading to arithmetic errors.",
            "scaling_trend": "Qualitative: GPT-3.5-turbo performs worst of the evaluated models and shows the largest relative drop when problems are reordered; paper does not provide systematic scaling with model size.",
            "uuid": "e6645.3",
            "source_info": {
                "paper_title": "Premise Order Matters in Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "R-GSM",
            "name_full": "Reordered GSM8K (R-GSM)",
            "brief_description": "A new benchmark introduced in this paper: 220 pairs of GSM8K-based grade-school math problems where one variant is the original and the paired variant is a manually rewritten sentence-order-preserving variant that keeps the same ground-truth answer but changes sentence order to test sensitivity to premise ordering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "R-GSM (constructed from GSM8K)",
            "task_type": "multi-step word problems (arithmetic + reasoning)",
            "problem_format": "natural-language word problems (5–8 sentences; last sentence kept fixed while others may be permuted and lightly edited for grammar)",
            "difficulty_level": "GSM8K-grade-school level; problems selected with ≥5 sentences and 2–8 reasoning steps",
            "prompting_method": "zero-shot evaluation using model's default interface; model input contains only the problem description (no explicit chain-of-thought or additional instructions in R-GSM experiments)",
            "performance_metric": "accuracy (exact final-answer correctness)",
            "performance_value": "Dataset-level observed impacts: all evaluated LLMs show reduced accuracy on reordered variants; e.g., GPT-4-turbo 94.1% -&gt; 85.0% (Table 2a).",
            "internal_analysis": "Behavioral error analysis provided: errors categorized as 'Temporal' (overlooking temporal order), 'Unknown' (intermediate unknown variables when reading sequentially), and 'Others'. No internal mechanistic probes on representations of numbers.",
            "failure_modes": "Primary failure modes induced by reordering: temporal-order confusions, treating numbers as positional/sequence-bound rather than semantically bound, and cascading arithmetic mistakes when necessary numbers appear later.",
            "scaling_trend": "Paper reports that more capable models degrade less on reordering (GPT-4-turbo and PaLM 2-L degrade less than GPT-3.5), but no systematic parametric scaling analysis is provided.",
            "uuid": "e6645.4",
            "source_info": {
                "paper_title": "Premise Order Matters in Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GSM8K",
            "name_full": "GSM8K (Grade School Math 8K)",
            "brief_description": "A widely-used benchmark of grade-school math word problems used as the base for R-GSM; models are evaluated for multi-step arithmetic reasoning on natural-language problems.",
            "citation_title": "Training verifiers to solve math word problems.",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM8K",
            "task_type": "multi-step word problems (grade-school arithmetic)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school level; multi-step arithmetic",
            "prompting_method": "commonly zero-shot/few-shot and chain-of-thought in literature, but in this paper GSM8K was used only as the source for R-GSM construction",
            "performance_metric": "accuracy",
            "performance_value": null,
            "internal_analysis": "This paper uses GSM8K as the source dataset for constructing reordered variants; no new internal analysis of GSM8K itself reported here.",
            "failure_modes": null,
            "scaling_trend": null,
            "uuid": "e6645.5",
            "source_info": {
                "paper_title": "Premise Order Matters in Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Long-digit-add reversal (prior work)",
            "name_full": "Input reversal for long-digit addition (prior works: Lee et al., Zhou et al.)",
            "brief_description": "Prior work cited by the paper showing that reversing digits (presenting least-significant digit first) simplifies learning of long-digit addition for Transformers and improves length generalization, illustrating that input order can change how models compute arithmetic.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": null,
            "task_type": "long-digit addition (synthetic arithmetic)",
            "problem_format": "symbolic digit sequences (reversed vs. normal ordering)",
            "difficulty_level": "long sequence addition / length generalization",
            "prompting_method": "training-time input ordering (not natural-language prompting)",
            "performance_metric": "length generalization / addition accuracy",
            "performance_value": null,
            "internal_analysis": "Behavioral/algorithmic analysis from prior work: reversal enables a local per-digit computation with carry, reducing need to attend across distant positions; cited in this paper as conceptual support for order effects.",
            "failure_modes": "Without reversal, Transformers often need to attend across many digits and fail to generalize to longer lengths; reversal alleviates this.",
            "scaling_trend": "Prior works show improved length generalization when reversal applied; not analyzed in this paper.",
            "uuid": "e6645.6",
            "source_info": {
                "paper_title": "Premise Order Matters in Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 2
        },
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2
        },
        {
            "paper_title": "What algorithms can transformers learn? a study in length generalization",
            "rating": 2
        },
        {
            "paper_title": "Transformers can achieve length generalization but not robustly",
            "rating": 2
        },
        {
            "paper_title": "Unnatural error correction: Gpt-4 can almost perfectly handle unnatural scrambled text",
            "rating": 1
        }
    ],
    "cost": 0.014117749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Premise Order Matters in Reasoning with Large Language Models</h1>
<p>Xinyun Chen ${ }^{1 <em>}$, Ryan A. Chi ${ }^{12 </em>}$, Xuezhi Wang ${ }^{1}$ and Denny Zhou ${ }^{1}$<br>*Equal contribution, ${ }^{1}$ Google DeepMind, ${ }^{2}$ Stanford University<br>{xinyunchen, xuezhiw, dennyzhou}@google.com, ryanchi@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over $\mathbf{3 0 \%}$. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.</p>
<h2>Sorted $\rightarrow$ correct</h2>
<p>Rules: If duve, then dox. If kere and riff, then teos. If feg and tess, then rabe. If wozz, then riff. If rabe, then thoxe. If ag and riff and kere, then hepe. If thoxe, then vepe. If rife and hume and ag, then dox. If feg and thoxe, then ire. If rebe and ene and heff, then kone. If ire and dox, then wope. If tess and vepe, then nup. If rabe and vide and nife, then ag. If nup and ag and vide, then duve. If zeck and hepe and dox, then riff. If nup, then hepe. If vene, then hume. If hepe and tess and ag, then kere. If fum, then vide. If kere and wope, then fum. If jog and kone, then thoxe. If fum, then wozz.</p>
<p>Facts: Alice is cote. Alice is kone. Alice is duve. Alice is ag. Alice is jog. Alice is teos. Alice is riff. Alice is feg. Alice is vide.</p>
<p>Query: Is Alice wozz? In each step, indicate whether you are using a fact, or a rule.
... since Alice is fum, by the rule "If fum, then wozz," Alice is wozz. Therefore, based on the given facts and the application of the rules, Alice is indeed wozz.</p>
<h2>X Shuffled $\rightarrow$ incorrect</h2>
<p>Rules: If nup, then hepe. If kere and riff, then teos. If feg and teos, then rabe. If wozz, then riff. If tess and vepe, then nup.If ag and riff and kere, then hepe. If feg and thoxe, then ire. If rife and hume and ag, then dox. If ire and dox, then wope. If rebe and ene and heff, then kone. If hepe and tess and ag, then kere. If rabe, then thoxe. If rabe and vide and nife, then ag. If fum, then wozz. If zeck and hepe and dox, then riff. If kere and wope, then fum. If vene, then hume. If thoxe, then vepe. If fum, then vide. If duve, then dox. If jog and kone, then thoxe. If nup and ag and vide, then duve.</p>
<p>Facts: Alice is cote. Alice is kone. Alice is duve. Alice is ag. Alice is jog. Alice is teos. Alice is riff. Alice is feg. Alice is vide.</p>
<p>Query: Is Alice wozz? In each step, indicate whether you are using a fact, or a rule.
... we haven't established a clear path to "fum" based on the given facts and rules... based on the provided facts and the logical steps followed, we cannot conclude that Alice is wozz.</p>
<p>Figure 1 | Premise order affects the reasoning performance: a failure case for logical reasoning. Left: rules are sorted in the same order as the ground truth proof (forward order with $\tau=1$ as defined in Section 2.1). Right: the wrong prediction with GPT-4-turbo after shuffling the rule set $(\tau=0)$. Distracting rules are in bold and light blue.</p>
<h1>1. Introduction</h1>
<p>Large language models (LLMs) have demonstrated impressive performance across a variety of reasoning tasks (Austin et al., 2021; Chen et al., 2021; Cobbe et al., 2021; Hendrycks et al., 2021; Wei et al., 2022). In particular, recent state-of-the-art LLMs have reached or even surpassed human performance on multiple reasoning benchmarks, including STEM problem-solving and code generation (Bubeck et al., 2023; Gemini, 2023; Li et al., 2022). However, recent works show that LLMs exhibit failure modes that align with human-like cognitive bias (Berglund et al., 2023; Hagendorff et al., 2023; Jones and Steinhardt, 2022; McCoy et al., 2023; Shi et al., 2023). For example, Berglund et al. (2023) revealed the Reversal Curse; i.e., LLMs trained on "A is B" tend to fail to infer that "B is A." Distractibility is another failure mode (Jones and Steinhardt, 2022; Shi et al., 2023), where the LLM performance drastically decreases when irrelevant context is included in the task description.</p>
<p>In this work, we investigate the effect that premise order has on LLM reasoning. Specifically, in deductive reasoning, changing the order of premises alone does not change the conclusion. Consider the following illustrative example:</p>
<ol>
<li>If $A$ then $B$.</li>
<li>If $B$ then $C$.</li>
<li>$A$ is True.</li>
</ol>
<p>We can derive that $C$ is True regardless of the order of these 3 premises. While some studies show that humans have a preference on the premise order to facilitate their reasoning (Dekeyser et al., 2000; Girotto et al., 1997), the premise order does not drastically affect human performance, especially for problems that only involve modus ponens (if P then Q; P; therefore Q), which are relatively straightforward for humans.</p>
<p>In contrast to humans, we observe that for LLMs, the premise order has a significant impact on reasoning performance. In particular, LLMs reach the best performance when the premises are arranged in the same order as they appear in the ground-truth proof. Taking the illustrative problem above as an example, we observe two phenomena:</p>
<ol>
<li>Presenting "If A then B" before "If B then C" in the prompt generally achieves a higher accuracy compared to the reversed order.</li>
<li>The performance gap is more significant when the number of premises increases.</li>
</ol>
<p>Intuitively, such a preference on the premise order aligns with human preference (Dekeyser et al., 2000) because in the preferred order, each derivation step can be done on-the-fly while looking at premises one by one, without needing to look back and forth across all premises at each step.
We conduct a systematic study on the premise order effect using a variety of SoTA LLMs, including GPT-4-turbo, GPT-3.5-turbo (OpenAI, 2023), PaLM 2-L (Google, 2023), and Gemini 1.0 Pro (Gemini, 2023). Our primary focus is deductive reasoning, and we benchmark all LLMs on problems that only involve modus ponens (if P then Q; P; therefore Q), where all LLMs in our evaluation at least achieve decent performance with a small number of premises. We show that the accuracy decrease caused by different ordering can be more than $30 \%$. The ordering effect is further amplified when irrelevant premises (i.e., premises that are not needed to derive a conclusion) are presented in the prompt. Figure 1 illustrates a failure case, where all LLMs fail to generate the proof after changing the order of relevant rules. Interestingly, while all LLMs perform best when the premise order follows the ground truth proof, they reveal different preferences on other alternative orderings. Specifically, compared to randomly ordering the premises, GPT-4-turbo and GPT-3.5-turbo generally achieve better</p>
<p>performance when the premise order is exactly the reverse of the ground truth proof, which enables LLMs to perform derivation via backward chaining. On the other hand, PaLM 2-L generally achieves the worst performance with such a reversed order.</p>
<p>Besides logical reasoning, we construct R-GSM to further investigate the ordering effect on mathematical reasoning. Specifically, we build R-GSM on top of a subset of GSM8K experiments, where we change the order of sentences in the problem description and manually verify that the ground truth answer remains the same. Our experiments again show that the performance of all LLMs notably drop, especially on longer problems that require more reasoning steps.</p>
<p>Our evaluation highlights that even in reasoning domains where the premise order does not matter, premise order does matter in LLM reasoning. Specifically, the premise ordering effect indicates that LLMs are more comfortable reasoning via reading left-to-right instead of back-and-forth, which can be attributed to the auto-regressive model design or the reasoning bias learned from the training corpus. We leave proposing new training and modeling techniques to mitigate the premise order effect as future work.</p>
<h1>2. Benchmarks</h1>
<h3>2.1. Logical Reasoning</h3>
<p>Prior work has revealed the weaknesses of LLMs in logical reasoning (Han et al., 2022; Saparov and He, 2022; Saparov et al., 2023; Wan et al., 2024; Xu et al., 2023; Yan et al., 2023), especially when the proof is long and requires the knowledge of multiple deduction theorems. To isolate the effect of premise orders, we focus on a confined problem space adapted from SimpleLogic (Zhang et al., 2022), which only includes propositional logic problems with definite clauses. Specifically, each problem includes: (1) a set of facts $A_{1}, \ldots, A_{n}$ that hold true; (2) a set of rules of the form "If $X$, then $Y$ ", "If $X_{0}$ and $X_{1}$, then $Y$ ", or "If $X_{0}$ and $X_{1}$ and $X_{2}$, then $Y$ "; and (3) a conclusion " $C$ is True" to be proved. As opposed to SimpleLogic - which formulates the problem as a binary classification task (i.e., indicate whether the conclusion is True or False) - in our benchmark, every problem has a ground-truth label of True, and we consider the prediction to be correct only when the generated proof is completely valid. With these strict criteria, the LLM is required to produce the step-by-step deduction that leads to the conclusion, and any hallucination of non-existent facts and rules is considered erroneous.</p>
<p>The key characteristic of our benchmark is that for each logical reasoning problem, we synthetically generate variants with different premise orders. Specifically, we denote the order that conforms to the ground truth proof with forward chaining as the forward order, where the rule applied in each derivation step is sequentially presented in the problem description. Intuitively, presenting premises in the forward order simplifies the problem for humans, as this allows us to write the proof on-the-fly while reading the premises. Conversely, a premise ordering that is more random increases the task difficulty, since carrying out the derivation requires us to repetitively look for premises for each reasoning step. Motivated by this intuition, we categorize different premise orders based on their Kendall tau distance $\tau$ (Cicirello, 2019; Sen, 1968) to the forward order, normalized into the range $[-1,1]$. Specifically, $\tau=1$ is the forward order, and we denote the order with $\tau=-1$ as the backward order, which is the reverse of the forward order and aligns with the proof via backward chaining. $\tau \approx 0$ suggests that there is no strong correlation between the premise order in the problem description and the proof. To thoroughly investigate the LLM preference on different premise orders, we evaluate the model performance on $\tau=0.5,0$ and -0.5 , in addition to the forward $(\tau=1)$ and backward $(\tau=-1)$ orders. We present examples with $\tau=1$ and 0 in Figure 1, and defer examples with other $\tau$ values to Figure 11 in Appendix B.</p>
<p>We measure the premise order effect by varying the following two factors:</p>
<ul>
<li>Number of rules required in the proof. It is expected that the premise order effect is more significant with more rules. For our benchmark, we generate problems whose numbers of rules range from 4 to 12 .</li>
<li>Number of distracting rules (i.e., rules that are not useful for the proof) presented in the problem. The presence of distracting rules also complicates the problem, as premise selection itself is challenging (Ferreira and Freitas, 2020; Irving et al., 2016; Wang et al., 2017), and LLMs are shown to be easily distracted by irrelevant context (Shi et al., 2023). We include problem variants with 0,5 and 10 distracting rules.</li>
</ul>
<p>We generate 200 problems for each number of required rules. Considering different premise orders and numbers of distracting rules, each problem includes 15 variants, resulting in a total of 27 K problems in our benchmark.</p>
<h1>2.2. R-GSM for Mathematical Reasoning</h1>
<h2>$\checkmark$ Original $\rightarrow$ correct</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h2>$\times$ Reordered $\rightarrow$ incorrect</h2>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | R-GSM example where the original problem can be correctly solved by all LLMs in our evaluation, but all of them failed on the reordered one. Different calculation steps and their corresponding problem statements are annotated in light blue. Specifically, the reasoning steps of the original problem follows the ordering of problem statements, while the reordered problem does not.</p>
<p>To further assess the effect of premise orders beyond logical reasoning, we construct the R-GSM dataset based on GSM8K (Cobbe et al., 2021), which is a popular benchmark of grade school math word problems. Specifically, we first select GSM8K test problems with at least 5 sentences in the problem description, then filter out those problems where there is no alternative ordering that does not change the ground truth answer, e.g., problem statements that follow the causal order of an event series. For each of the remaining problem, we keep the last sentence untouched and rewrite the problem description with a different ordering of other sentences. Minor editing on words is allowed to ensure the grammatical correctness of the problem description. To facilitate the annotation</p>
<p>process, for each problem, we write a simple function to enumerate all alternative orderings of problem statements until an ordering that causes the LLM prediction failure is discovered, which can be used for our manual rewriting if the alternative ordering found in the enumeration process happens to preserve the ground truth answer. In total, our R-GSM benchmark contains 220 pairs of problems, including both the original GSM8K problem description and the manually rewritten one with a different ordering of problem statements. Despite that over $60 \%$ of problems in R-GSM only have 5 sentences, and all problems have at most 8 sentences, our evaluation shows that all LLMs still perform considerably worse on rewritten problems. Figure 2 presents an example in R-GSM where all LLMs correctly solve the original problem but not the rewritten one. Specifically, the reasoning steps for the original problem follows the ordering of problem statements, while for the rewritten problem, the second calculation step in the correct solution should refer to the second-to-last sentence instead of the second sentence in the problem description. We provide a more detailed case study in Section 3.3, and present the full dataset statistics in Appendix A.</p>
<h1>3. Experiments</h1>
<h3>3.1. Experimental Setup</h3>
<p>We evaluate the premise ordering effect on GPT-4-turbo, GPT-3.5-turbo, PaLM 2-L and Gemini 1.0 Pro. We perform the greedy decoding with the temperature 0 , and apply the zero-shot prompting in all experiments. On R-GSM, the model input only contains the problem description without additional instructions. For logical reasoning, as shown in Figure 1, we add an instruction in the prompt to ask for a derivation that specifies which premise is used in each step.</p>
<h3>3.2. Logical Reasoning</h3>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Logical reasoning without distracting rules. See Table 6 in Appendix E for accuracy numbers.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 | Logical reasoning with distracting rules. See Tables 7 and 8 for accuracy numbers.
Figure 3 presents the results with different numbers of relevant rules included in ground truth proofs, where the problem does not contain distracting rules, and the shuffled accuracy is the aggregation of</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 | Results on different $\tau$ without distracting rules. See Table 9 for accuracy numbers.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 | Results on different $\tau$ with distracting rules. See Tables 10 and 11 for accuracy numbers.
results with $\tau=0.5,0$ and -0.5 . Across different LLMs, the forward order consistently achieves the best performance, which aligns with the human preference. The performance drop caused by alternative orderings becomes more significant when the number of rules increases. Meanwhile, models with weaker reasoning capabilities are also more sensitive to different premise orders. Specifically, while the accuracy decrease of GPT-4-turbo and PaLM 2-L is up to $20-30 \%$, with Gemini 1.0 Pro and GPT-3.5-turbo, changing the premise order from the forward order can degrade the accuracy from over $65 \%$ to below $25 \%$, with an accuracy decrease of more than $40 \%$.</p>
<p>Breakdown on different premise orders. We present the results of fine-grained breakdown on premise ordering in Figure 5, where the orders are categorized based on Kendall tau distance $\tau$ as described in Section 2.1. Interestingly, while the top preference of all LLMs is the forward order, their preferences on other orders are not alike. Specifically, GPT-4-turbo generally prefers the backward order over other orders, and the overall performance decreases with a smaller absolute value of $\tau$. This observation is also consistent with the human reasoning pattern, as backward chaining is another well-established inference method. On the other hand, PaLM 2-L generally performs the worst with the backward order. With the decrease of $\tau$ (i.e., the premise order deviates more from the forward order), the accuracy drops. The preferences of Gemini 1.0 Pro and GPT-3.5-turbo are less consistent, still they prefer the backward order more often than other non-forward premise orders.</p>
<p>Effect of distracting rules. We assess the effect of distracting rules of GPT-4-turbo and PaLM 2-L, which reach a decent performance without the presence of distracting rules. Figures 4 and 6 show that adding distracting rules further decreases the reasoning performance and magnifies the effect of different premise orders. Still, the overall preferences of both LLMs remain the same as the scenario without distracting rules. Specifically, both LLMs again achieve the best performance with the forward order, and GPT-4-turbo prefers the backward order over other non-forward orders, while PaLM 2-L performance decreases with a smaller $\tau$.</p>
<p>Error analysis. In Table 1, we present the breakdown on prediction errors with different premise orders. We consider the following error categories:</p>
<ol>
<li>wrong refutation: the LLM wrongly claims that the conclusion can not be proved;</li>
<li>rule hallucination: the LLM generates rules that do not exist in the problem;</li>
<li>fact hallucination: the LLM generates facts that do not exist in the problem and are unproven.</li>
</ol>
<p>We observe that for all LLMs, fact hallucination is typically the most common error pattern, and this error type escalates dramatically with the decrease of $\tau$. The main reason is that LLMs are inclined to use the rules in the sequential order as they present in the problem, so when the next rule in the problem is not yet applicable, LLMs might still hallucinate facts to complete the proof step. Simultaneously, we observe that the percentage of wrong refutation is generally lower for $\tau=-1$ than for $|\tau|&lt;1$. We present an example of wrong refutation in Figure 1, and we include more examples of rule and fact hallucination in Figure 10 of Appendix B.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\tau$</th>
<th style="text-align: center;">Correct</th>
<th style="text-align: center;">Wrong <br> Refutation</th>
<th style="text-align: center;">Hallucination</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule</td>
<td style="text-align: center;">Fact</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-turbo</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">96.5\%</td>
<td style="text-align: center;">0.5\%</td>
<td style="text-align: center;">$1.5 \%$</td>
<td style="text-align: center;">$1.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">76.0\%</td>
<td style="text-align: center;">10.5\%</td>
<td style="text-align: center;">$2.0 \%$</td>
<td style="text-align: center;">11.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">82.0\%</td>
<td style="text-align: center;">4.5\%</td>
<td style="text-align: center;">3.5\%</td>
<td style="text-align: center;">10.0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-0.5$</td>
<td style="text-align: center;">84.5\%</td>
<td style="text-align: center;">1.0\%</td>
<td style="text-align: center;">4.5\%</td>
<td style="text-align: center;">10.0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">84.0\%</td>
<td style="text-align: center;">0.0\%</td>
<td style="text-align: center;">3.5\%</td>
<td style="text-align: center;">12.5\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">30.0\%</td>
<td style="text-align: center;">24.5\%</td>
<td style="text-align: center;">9.5\%</td>
<td style="text-align: center;">35.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.0\%</td>
<td style="text-align: center;">54.5\%</td>
<td style="text-align: center;">9.5\%</td>
<td style="text-align: center;">33.0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5\%</td>
<td style="text-align: center;">55.0\%</td>
<td style="text-align: center;">7.5\%</td>
<td style="text-align: center;">34.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-0.5$</td>
<td style="text-align: center;">2.0\%</td>
<td style="text-align: center;">50.0\%</td>
<td style="text-align: center;">8.5\%</td>
<td style="text-align: center;">37.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">1.5\%</td>
<td style="text-align: center;">34.5\%</td>
<td style="text-align: center;">14.5\%</td>
<td style="text-align: center;">47.0\%</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">88.0\%</td>
<td style="text-align: center;">0.5\%</td>
<td style="text-align: center;">3.0\%</td>
<td style="text-align: center;">8.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">74.5\%</td>
<td style="text-align: center;">1.5\%</td>
<td style="text-align: center;">9.5\%</td>
<td style="text-align: center;">14.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">65.5\%</td>
<td style="text-align: center;">2.0\%</td>
<td style="text-align: center;">11.0\%</td>
<td style="text-align: center;">21.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-0.5$</td>
<td style="text-align: center;">59.5\%</td>
<td style="text-align: center;">1.5\%</td>
<td style="text-align: center;">10.0\%</td>
<td style="text-align: center;">29.0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">57.5\%</td>
<td style="text-align: center;">1.0\%</td>
<td style="text-align: center;">11.5\%</td>
<td style="text-align: center;">30.0\%</td>
</tr>
<tr>
<td style="text-align: center;">Gemini 1.0 Pro</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16.5\%</td>
<td style="text-align: center;">28.0\%</td>
<td style="text-align: center;">5.0\%</td>
<td style="text-align: center;">50.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.0\%</td>
<td style="text-align: center;">59.0\%</td>
<td style="text-align: center;">3.5\%</td>
<td style="text-align: center;">37.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.0\%</td>
<td style="text-align: center;">34.0\%</td>
<td style="text-align: center;">9.0\%</td>
<td style="text-align: center;">57.0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-0.5$</td>
<td style="text-align: center;">0.5\%</td>
<td style="text-align: center;">24.5\%</td>
<td style="text-align: center;">9.5\%</td>
<td style="text-align: center;">65.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0.5\%</td>
<td style="text-align: center;">27.5\%</td>
<td style="text-align: center;">11.5\%</td>
<td style="text-align: center;">60.5\%</td>
</tr>
</tbody>
</table>
<p>Table 1 | Error analysis for logical reasoning with 12 relevant rules and no distracting rules.</p>
<h1>3.3. R-GSM for Mathematical Reasoning</h1>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Init Acc</th>
<th style="text-align: center;">Reorder Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4-turbo</td>
<td style="text-align: center;">$94.1 \%$</td>
<td style="text-align: center;">$85.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2-L</td>
<td style="text-align: center;">$86.4 \%$</td>
<td style="text-align: center;">$79.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Gemini 1.0 Pro</td>
<td style="text-align: center;">$80.5 \%$</td>
<td style="text-align: center;">$69.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo</td>
<td style="text-align: center;">$67.3 \%$</td>
<td style="text-align: center;">$51.8 \%$</td>
</tr>
</tbody>
</table>
<p>(a)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Init Acc</th>
<th style="text-align: center;">Reorder Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4-turbo</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$89.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2-L</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$87.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Gemini 1.0 Pro</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$74.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$64.9 \%$</td>
</tr>
</tbody>
</table>
<p>(b)</p>
<p>Table 2 | Results on the R-GSM dataset: (a) accuracies on the full dataset; (b) for each model, the accuracies on the R-GSM subset where the original problems are correctly solved, thus the initial accuracy is $100 \%$ for all models.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 | R-GSM results with different numbers of reasoning steps in the ground truth. See Table 12 in Appendix F for accuracy numbers.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8 | R-GSM results with different problem lengths. See Table 13 for accuracy numbers.</p>
<p>Table 2a demonstrates the overall results on R-GSM. Again, all LLMs achieve a lower performance on R-GSM. Note that the original GSM8K problems are not necessarily written in the most preferable way, and thus sometimes the manual rewriting facilitates the reasoning and allows the model to correctly solve the reordered version of a problem that it fails on the original one. Therefore, in Table 2b, for each LLM, we also present the accuracy on those problems with their original descriptions solved by the model. We show that all LLMs fail on at least $10 \%$ of reordered problems that they are initially able to solve, and this performance degradation is more than $35 \%$ with GPT-3.5-turbo.</p>
<p>Breakdown of problem complexity. Figures 7 and 8 present the breakdown results on different number of reasoning steps and different number of problem sentences, respectively. Unsurprisingly, across all LLMs, the proof accuracy suffers on problems that require more reasoning steps and contain a greater number of sentences. Overall, the gap between the accuracies on initial and rewritten problems is more significant with more reasoning steps and longer problems for both GPT-4-turbo and Gemini 1.0 Pro, while the gap remains similar across different numbers of reasoning steps and problem lengths for PaLM 2-L and GPT-3.5-turbo.</p>
<p>Error analysis. To further understand the failure modes, for each LLM, we analyze those error cases where the original problems can be correctly solved but not the reordered ones, and we categorize the common error types in Table 3. Similar to our observation in logical reasoning experiments, the prediction errors in R-GSM are primarily due to the LLMs blindly using numbers in the sequential order of their appearances in the problem. Specifically, the most common error case for all LLMs is their tendency to overlook temporal order. Figure 2 presents such an example, where the prediction failure is because some earlier events are described in the later part of the problem. Another category of errors occurs when some quantities are not specified while processing the problem in the sequential order, which introduces unknown variables for calculation. Take, for example, the problem in Figure 9. In the original problem, the number of each animal can be directly calculated based on its preceding sentence. However, in the reordered problem, the number of gerbils cannot directly be computed</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Temporal</th>
<th style="text-align: center;">Unknown</th>
<th style="text-align: center;">Others</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4-turbo</td>
<td style="text-align: center;">$45.0 \%$</td>
<td style="text-align: center;">$15.0 \%$</td>
<td style="text-align: center;">$40.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo</td>
<td style="text-align: center;">$21.6 \%$</td>
<td style="text-align: center;">$19.6 \%$</td>
<td style="text-align: center;">$58.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2-L</td>
<td style="text-align: center;">$34.8 \%$</td>
<td style="text-align: center;">$4.3 \%$</td>
<td style="text-align: center;">$60.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Gemini 1.0 Pro</td>
<td style="text-align: center;">$29.5 \%$</td>
<td style="text-align: center;">$18.2 \%$</td>
<td style="text-align: center;">$52.3 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3 | Error analysis on R-GSM. "Temporal" refers to the temporal order, and "Unknown" refers to the unknown variables.</p>
<h1>? Original $\rightarrow$ correct</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<h2>$\times$ Reordered $\rightarrow$ incorrect</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9 | R-GSM example where the original problem can be correctly solved by all LLMs, but GPT-3.5-Turbo fails on the reordered version while all the other LLMs still solve it correctly.
based on the preceding sentences, since the number of fish remains unknown up to that point, and the LLM must read the remaining sentences and calculate the number of fish first. However, the prediction from GPT-3.5-turbo instead uses the number calculated in the previous step (i.e., the number of rabbits) to calculate the number of gerbils, resulting in an error. Such a failure mode is less common with PaLM 2-L, but still constitutes a non-negligible proportion of prediction errors for the other LLMs. We present more examples of model predictions in Appendix C.</p>
<h2>4. Related Work</h2>
<p>Failure modes of LLMs. The premise order effect in this work is connected to several failure modes of LLMs in the literature, including the reversal curse (Berglund et al., 2023), distractibility (Shi et al., 2023), position bias (Liu et al., 2024; Wang et al., 2023), and limited capability of logical reasoning (Han et al., 2022; Saparov and He, 2022; Saparov et al., 2023; Wan et al., 2024; Xu et al., 2023; Yan et al., 2023; Zhu et al., 2023). Specifically, Shi et al. (2023) show that including irrelevant context in the problem statement leads to a considerable performance drop on GSM8K and other reasoning benchmarks, revealing that LLMs are distractible. This finding is in-line with our</p>
<p>evaluation on logical reasoning, where we observe that adding irrelevant rules not only degrades the overall logical reasoning performance, but also escalates the premise order effect. The Reversal Curse (Berglund et al., 2023) unveils another perspective of the order effect, where they show that an LLM that recognizes "A is B" does not necessarily learn that "B is A." While their work studies the order effect between two entities within a single factual statement, our work focuses on reasoning problems with multiple premises, without restrictions on the number of (or relationship between) entities. In particular, for logical reasoning, we demonstrate that random permutations of premises often result in worse accuracy than the purely backward order. Liu et al. (2024) discover the lost-in-the-middle phenomenon in the long-context scenario: the LLM performance is the best when the relevant information to solve the task is placed at the beginning or the end of the input context, while the performance is the worst when the LLM needs to utilize input context in the middle. In Appendix D, we show that lost-in-the-middle phenomenon does not affect the performance on our tasks, since the length of input problems does not exceed 300 tokens in our benchmark, which is relatively small compared to the context length limit of LLMs in our evaluation. Yan et al. (2023) present an approach called Concise and Organized Perception for deductive reasoning, which first generates directed graphs by connecting facts and rules in the problem, then prune and reorder the context accordingly before calling the LLM to solve the problem. The improvement achieved by this approach again demonstrates the effect of premise ordering and irrelevant premises on logical reasoning. While such input preprocessing methods can mitigate the ordering effect on certain reasoning tasks, they require task-specific design and do not generalize across domains. We consider developing generic end-to-end reasoning techniques for LLMs to address the premise order effect as future work.</p>
<p>Order effect for human logical reasoning. Although the premise order does not matter in deductive reasoning, several studies show that the premise order can impact the human reasoning performance (Dekeyser et al., 2000; Girotto et al., 1997). Dekeyser et al. (2000) described co-reference as a human preference of premise order; i.e., humans prefer the premises to be presented in an order where they can draw immediate conclusions after seeing each one. In this work, we show that LLMs also have such a preference, and they achieve the best performance when the ordering of rules follows the ground truth proof. Girotto et al. (1997) studied how the premise order affects logical reasoning for humans, and found that the premise order has a significant effect in solving modus tollens problems (i.e., if P, then Q; not Q; therefore, not P), but not modus ponens problems (i.e., if P, then Q; P; therefore, Q). However, differing from our work, they studied the influence of different ordering between rules and facts, e.g., their experiments on modus tollens problems show that presenting negation statements (not Q) before rules (if P, then Q) improves the performance over the reverse order. On the other hand, our work focuses on modus ponens problems that are easier for both humans and LLMs, and we show that the LLM performance is still quite sensitive to the ordering of the premises.</p>
<p>Order effect of language models. Some prior works show that language models are able to understand permuted texts to some extent, i.e., after a random permutation of words, models usually preserve a reasonable performance (Abdou et al., 2022; Sinha et al., 2020). Moreover, Cao et al. (2023) show that even when a large fraction of words are scrambled, GPT-4 still achieves decent performance on several reasoning benchmarks. In contrast to permuted texts in these works that are typically unnatural and nonsensical, our premise order permutations do not alter the semantic meaning and remain syntactically valid (we manually verify this). Nevertheless, we demonstrate that LLM reasoning performance is highly brittle to the ordering of the premises. For long-digit addition, prior works demonstrate that reversing the input numbers is a key to achieve better length generalization performance (Lee et al., 2023; Zhou et al., 2023, 2024). Specifically, by reversing the input numbers so that the least significant digit is presented first, the Transformer learns a simpler way</p>
<p>of performing addition, where the model only needs to perform computation with the corresponding digits of operands and the carry-on digit at each step, without the need of looking at other digits. This approach enables the Transformer to better perform addition when trained from scratch, which also aligns with our finding: after reversing the input numbers, the premise order (i.e., orders of digits) follows the right ordering of performing long-digit addition, thus enables Transformers to better learn the task.</p>
<h1>5. Conclusion</h1>
<p>In this work, we show that the premise order significantly affects LLMs' performance on reasoning tasks, even when the premise order does not change the underlying task itself. Our comprehensive evaluation demonstrates that LLM tendencies resemble human preference w.r.t. premise order, i.e., LLMs achieve the best performance when the premise order follows the intermediate reasoning steps to solve the problem. Conversely, LLMs face difficulties when the reasoning problem requires the model to read the problem description back-and-forth, resulting in a performance drop of over $30 \%$. We further extend the study to mathematical reasoning and present the R-GSM benchmark, and again experimentally confirm the ordering effect.
While humans also have a preference of premise orders for reasoning problems, LLMs are much more susceptible to such ordering effects. We can attempt to ascribe the premise order effect to several candidate factors, such as the auto-regressive model design, training objectives, and training data mixture. However, we leave proposing theoretical explanations of this limitation and developing new techniques towards addressing the premise order effect as future work.</p>
<h2>Acknowledgment</h2>
<p>We would like to thank Chen Liang and Dale Schuurmans for helpful discussion and feedback.</p>
<h2>References</h2>
<p>M. Abdou, V. Ravishankar, A. Kulmizev, and A. Søgaard. Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6907-6919, 2022.
J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
L. Berglund, M. Tong, M. Kaufmann, M. Balesni, A. C. Stickland, T. Korbak, and O. Evans. The reversal curse: Llms trained on" a is b" fail to learn" b is a". arXiv preprint arXiv:2309.12288, 2023.
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
Q. Cao, T. Kojima, Y. Matsuo, and Y. Iwasawa. Unnatural error correction: Gpt-4 can almost perfectly handle unnatural scrambled text. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8898-8913, 2023.
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>V. A. Cicirello. Kendall tau sequence distance: Extending kendall tau from ranks to sequences. arXiv preprint arXiv:1905.02752, 2019.
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
M. Dekeyser, W. Schroyens, W. Schaeken, O. Spitaels, and G. d'Ydewalle. Preferred premise order in propositional reasoning: Semantic informativeness and co-reference. Deductive reasoning and strategies, pages 73-95, 2000.
D. Ferreira and A. Freitas. Premise selection in natural language mathematical texts. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7365-7374, 2020.</p>
<p>Gemini. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
V. Girotto, A. Mazzocco, and A. Tasso. The effect of premise order in conditional reasoning: A test of the mental model theory. Cognition, 63(1):1-28, 1997.</p>
<p>Google. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
T. Hagendorff, S. Fabi, and M. Kosinski. Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Nature Computational Science, 3(10):833-838, 2023.
S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, L. Benson, L. Sun, E. Zubova, Y. Qiao, M. Burtell, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022.
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
G. Irving, C. Szegedy, A. A. Alemi, N. Eén, F. Chollet, and J. Urban. Deepmath-deep sequence models for premise selection. Advances in neural information processing systems, 29, 2016.
E. Jones and J. Steinhardt. Capturing failures of large language models via human cognitive biases. Advances in Neural Information Processing Systems, 35:11785-11799, 2022.
N. Lee, K. Sreenivasan, J. D. Lee, K. Lee, and D. Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.
Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022.
N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, $12: 157-173,2024$.
R. T. McCoy, S. Yao, D. Friedman, M. Hardy, and T. L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638, 2023.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>A. Saparov and H. He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.
A. Saparov, R. Y. Pang, V. Padmakumar, N. Joshi, S. M. Kazemi, N. Kim, and H. He. Testing the general deductive reasoning capacity of large language models using ood examples. arXiv preprint arXiv:2305.15269, 2023.
P. K. Sen. Estimates of the regression coefficient based on kendall's tau. Journal of the American statistical association, 63(324):1379-1389, 1968.
F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Schärli, and D. Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 31210-31227. PMLR, 2023.
K. Sinha, P. Parthasarathi, J. Pineau, and A. Williams. Unnatural language inference. arXiv preprint arXiv:2101.00010, 2020.
Y. Wan, W. Wang, Y. Yang, Y. Yuan, J.-t. Huang, P. He, W. Jiao, and M. R. Lyu. A \&amp; b= b \&amp; a: Triggering logical reasoning failures in large language models. arXiv preprint arXiv:2401.00757, 2024.
M. Wang, Y. Tang, J. Wang, and J. Deng. Premise selection for theorem proving by deep graph embedding. Advances in neural information processing systems, 30, 2017.
P. Wang, L. Li, L. Chen, Z. Cai, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.
F. Xu, Q. Lin, J. Han, T. Zhao, J. Liu, and E. Cambria. Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. arXiv preprint arXiv:2306.09841, 2023.
S. Yan, C. Shen, J. Liu, and J. Ye. Concise and organized perception facilitates large language models for deductive reasoning. arXiv preprint arXiv:2310.03309, 2023.
H. Zhang, L. H. Li, T. Meng, K.-W. Chang, and G. V. d. Broeck. On the paradox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022.
H. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. Susskind, S. Bengio, and P. Nakkiran. What algorithms can transformers learn? a study in length generalization. arXiv preprint arXiv:2310.16028, 2023.
Y. Zhou, U. Alon, X. Chen, X. Wang, R. Agarwal, and D. Zhou. Transformers can achieve length generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024.
Z. Zhu, Y. Xue, X. Chen, D. Zhou, J. Tang, D. Schuurmans, and H. Dai. Large language models can learn rules. arXiv preprint arXiv:2310.07064, 2023.</p>
<h1>A. R-GSM Dataset Statistics</h1>
<p>Table 4 presents the statistics of our R-GSM benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># Steps</th>
<th style="text-align: right;"># Problems</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: right;">20</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: right;">43</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: right;">65</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: right;">43</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: right;">23</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: center;">(a)</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: center;"># Sentences</td>
<td style="text-align: right;"># Problems</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: right;">133</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: right;">65</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: right;">19</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: right;">3</td>
</tr>
</tbody>
</table>
<p>Table $4 \mid$ Statistics of the R-GSM dataset, with 220 problems in total: (a) breakdown on the number of reasoning steps; (b) breakdown on the number of sentences in the questions.</p>
<h2>B. Logical Reasoning Examples</h2>
<p>Figure 10 presents common classes of errors - hallucinated rules and facts - by LLMs while solving our logical reasoning benchmark.</p>
<p>Figure 11 presents a sample logical reasoning problem with premise orders of different $\tau$ values. We can see that the rules become less ordered when the absolute value of $\tau$ decreases.</p>
<h2>C. R-GSM Examples</h2>
<p>In this section, we present more examples of LLM predictions on R-GSM problems.
Figure 12 presents a failure case of a probability problem, which falls into the "Others" category in the error analysis (Table 3). Specifically, in the reordered problem, after the LLM reads the sentence about the scenario with a normal teacher coming in, the LLM immediately attempts to compute the probability that Marcus has to turn in his homework, ignoring that the LLM needs to compute the probability that a normal teacher will come in using the next sentence.</p>
<p>Figures 13 shows another wrong prediction of GPT-4 Turbo, where the error pattern is analogous to rule hallucination in logical reasoning evaluation. Interestingly, when moving the sentence about yellow cars preceding to the sentence about quantities of blue and green cars, GPT-4 Turbo starts to hallucinate the relationship between the number of yellow cars and the number of blue cars, resulting in insufficient information to correctly solve the problem.</p>
<p>Figures 14 and 15 present examples where both the original and reordered problems are correctly solved by LLMs in our evaluation. In both original problems, the succeeding sentences do not strongly</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 10 | Examples of hallucinated rules (left) and facts (right) produced by GPT-3.5-Turbo while solving our logical reasoning benchmark.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 11 | An example logical reasoning problem with different premise orders. The number emojis are for ease of viewing. The ampersands were originally "and"s in the original prompt. The facts and query have been excluded for brevity.
depend on the preceding sentences.</p>
<h1>D. Does Logical Reasoning Suffer from the Lost-in-the-middle Issue?</h1>
<p>Liu et al. (2024) demonstrate that when the input context becomes long, LLMs might suffer from the lost-in-the-middle issue: the model performance significantly degrades when relevant information to solve the task is in the middle of the input, instead of at the beginning or the end. Therefore, when given distracting rules for logical reasoning, another potential factor that might affect the model performance is the position of relevant rules in the model input.
To examine the effect of such position bias, we conduct ablations on PaLM 2-L with 10 distracting rules, and we compare the performance with relevant rules added in the beginning, middle or the end of the problem description. Table 5 shows that with the same order and number of rules, the variation in performance is very small, whereas changing the order significantly affects the results. Note that the longest inputs in our logical reasoning benchmark, i.e., problems with 12 relevant rules and 10 distracting rules, only contain no more than 300 tokens, which is relatively short compared to the context length limit of LLMs in our evaluation. These results confirm that on our tasks where the input problems (and thus input context) are short, lost-in-the-middle phenomenon is not the primary cause of the performance difference. In our primary experiments, for all logical reasoning problems,</p>
<p>we interleave distracting rules with relevant rules in the input context.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># rules, position \order</th>
<th style="text-align: left;">Forward</th>
<th style="text-align: left;">Backward</th>
<th style="text-align: left;">Shuffled</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">8, beginning</td>
<td style="text-align: left;">$68.0 \%$</td>
<td style="text-align: left;">$40.0 \%$</td>
<td style="text-align: left;">$45.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">8, middle</td>
<td style="text-align: left;">$67.0 \%$</td>
<td style="text-align: left;">$39.0 \%$</td>
<td style="text-align: left;">$44.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">8, end</td>
<td style="text-align: left;">$67.0 \%$</td>
<td style="text-align: left;">$40.0 \%$</td>
<td style="text-align: left;">$45.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">12, beginning</td>
<td style="text-align: left;">$36.5 \%$</td>
<td style="text-align: left;">$17.0 \%$</td>
<td style="text-align: left;">$16.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">12, middle</td>
<td style="text-align: left;">$36.5 \%$</td>
<td style="text-align: left;">$17.0 \%$</td>
<td style="text-align: left;">$18.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">12, end</td>
<td style="text-align: left;">$35.0 \%$</td>
<td style="text-align: left;">$16.0 \%$</td>
<td style="text-align: left;">$19.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5 | Logical reasoning results performance of PaLM 2-L, with relevant rules at different positions of the input context.</p>
<h1>E. Full Results for Logical Reasoning</h1>
<p>Tables 6 and 9 present the accuracy numbers for Figures 3 and 5, which are results on different numbers of relevant rules without distracting rules.</p>
<p>Tables 7 and 10 present the accuracy numbers for Figures 4 and 6 with 5 distracting rules.
Tables 8 and 11 present the accuracy numbers for Figures 4 and 6 with 10 distracting rules.</p>
<h2>F. Full Results on R-GSM</h2>
<p>Tables 12 and 13 present the accuracy numbers for Figures 7 and 8, which are breakdown results on R-GSM problems with different numbers of reasoning steps and different numbers of sentences in the problem description respectively.</p>
<h1>? Original $\rightarrow$ correct</h1>
<p>Marcus is trying to decide whether he really needs to do his homework. There's a $50 \%$ chance that tomorrow he'll have a substitute teacher who won't collect the homework. Even if the normal teacher comes in, there's a $40 \%$ chance she'll give everyone an extension. Even if the whole class doesn't get an extension, there's a $20 \%$ chance Marcus can convince the teacher his dog ate his assignment and get a personal extension. What is the percentage chance that Marcus will actually have to turn in his homework tomorrow?</p>
<h2>$X$ Reordered $\rightarrow$ incorrect</h2>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Lets break it down:</p>
<ol>
<li>The normal teacher comes in and does not give an extension ( $60 \%$ chance of this happening since there's a $40 \%$ chance she will give an extension).
-In this case, Marcus has a 20\% chance of getting a personal extension if he claims his dog ate his homework. This means there's an $80 \%$ chance he will not get the personal extension and will have to turn in his homework.
The probability of this scenario (normal teacher, no class extension, no personal extension) is $60 \% * 80 \%=48 \%$.</li>
<li>The normal teacher comes in and gives an extension ( $40 \%$ chance of this happening).</li>
<li>If the teacher gives an extension, Marcus does not have to turn in his homework tomorrow, so this scenario does not contribute to the probability of Marcus having to turn in his homework.
Now, we add up the probabilities of the scenarios where Marcus has to turn in his homework: Probability of having to turn in homework = Probability of scenario $1=48 \%$ So, there is a $\mathbf{4 8 \%}$ chance that Marcus will have to turn in his homework tomorrow.</li>
</ol>
<p>Figure 12 | R-GSM example where the original problem can be correctly solved by GPT-4 Turbo, but the model fails on the reordered one.</p>
<h1>Original $\rightarrow$ correct</h1>
<p>A bumper car rink has 12 red cars. They have 2 fewer green cars than they have red cars. They have 3 times the number of blue cars as they have green cars. The rink also has yellow cars. If the rink has 75 cars in total how many yellow cars do they have?</p>
<p>To find out how many yellow cars there are, let's first calculate the number of cars of each other color.</p>
<ul>
<li>Red cars: 12</li>
<li>Green cars: $12-2=10$</li>
<li>Blue cars: 3 times the number of green cars $=3 * 10=30$</li>
</ul>
<p>Now, let's add up the red, green, and blue cars to see how many cars we have accounted for:</p>
<ul>
<li>Total (Red + Green + Blue) $=12+10+30=$ 52</li>
</ul>
<p>The rink has 75 cars in total, so to find out how many yellow cars there are, we subtract the total of red, green, and blue cars from the overall total:</p>
<ul>
<li>Yellow cars =</li>
</ul>
<p>Total cars - (Red + Green + Blue cars)</p>
<ul>
<li>Yellow cars
$=75-52=23$
Therefore, the rink has 23 yellow cars.</li>
</ul>
<p>A bumper car rink has 12 red cars. They have 2 fewer green cars than they have red cars. The rink also has yellow cars. They have 3 times the number of blue cars as they have green cars. If the rink has 75 cars in total how many yellow cars do they have?</p>
<p>To find out how many yellow cars there are, let's break down the information given:</p>
<ol>
<li>There are 12 red cars.</li>
<li>There are 2 fewer green cars than red cars, so there are $12-2=10$ green cars.</li>
<li>The number of yellow cars is 3 times the number of blue cars, but we don't know the number of blue cars directly. Instead, we know the relationship between the number of green cars and the number of yellow cars, which seems to have been misstated. Let's correct that understanding: If there's a relationship involving the number of yellow cars directly given, it's not clearly stated in terms of blue or green cars. So, we'll proceed with the information we have.</li>
<li>The total number of cars is 75 .</li>
</ol>
<p>First, let's calculate the total number of cars that are not yellow, since we know the numbers of red and green cars:</p>
<p>Total non-yellow cars=Number of red cars+Number of green cars
Total non-yellow cars $=12+10=22$
Given that the total number of cars is 75 , we can find the number of yellow cars by subtracting the total number of non-yellow cars from the total:</p>
<p>Number of yellow cars $=$ Total number of cars-Total non-yellow cars
Number of yellow cars $=75-22=53$
Therefore, the rink has 53 yellow cars.</p>
<p>Figure 13 | R-GSM example where the original problem can be correctly solved by all models, but GPT-4 Turbo and Gemini Pro failed on the reordered one.</p>
<h1>$\checkmark$ Original $\rightarrow$ correct</h1>
<p>Sheila charged $\$ 85.00$ worth of merchandise on her credit card. She ended up returning one item that cost $\$ 15.00$. After she returned the item, she bought a frying pan that was on sale for $20 \%$ off $\$ 20.00$ and a set of towels that was $10 \%$ off $\$ 30.00$. She put both of these purchases on her credit card. What is the new balance on her credit card?</p>
<p>Sheila charged $\$ 85.00$ worth of merchandise on her credit card. After she returned an item, she bought a frying pan that was on sale for $20 \%$ off $\$ 20.00$ and a set of towels that was $10 \%$ off $\$ 30.00$. She put both of these purchases on her credit card. The returned item cost $\$ 15.00$. What is the new balance on her credit card?</p>
<p>The frying pan was on sale for $20 \%$ off $\$ 20.00$ so that's $20^{\star} .01=\$&lt;&lt;20^{\star} .01=0.20 \gg&gt;0.20$ off
The frying pan was $\$ 20.00$ and it's $20 \%$ off so that's $20^{\star} .20=\$&lt;&lt;20^{\star} .20=4.00 \gg&gt;4.00$ off
The frying pan is $\$ 20.00$ and it's $\$ 4.00$ off so it now costs $20-4=\$&lt;&lt;20-4=16.00 \gg&gt;16.00$
The towels were $10 \%$ off $\$ 30.00$ so that's $10^{\star} .01=\$&lt;&lt;10^{\star} .01=0.10 \gg&gt;0.10$ off
The towels were $\$ 30.00$ and they're $10 \%$ off so that's $30^{\star} .10=\$&lt;&lt;30^{\star} .10=3.00 \gg&gt;3.00$ off
The towels are $\$ 30.00$ and they're $\$ 3.00$ off so they now cost $30-3=\$&lt;&lt;30-3=27.00 \gg&gt;27.00$
The frying pan is $\$ 16.00$ and the towels are $\$ 27.00$ so together they cost $16+27=\$&lt;&lt;16+27=43.00 \gg&gt;43.00$
Sheila charged $\$ 85.00$ worth of merchandise and returned an item for $\$ 15.00$ so her balance was $85-15=$ $\$&lt;&lt;85-15=70.00 \gg&gt;70.00$</p>
<p>Sheila charged $\$ 43.00$ worth of merchandise on her credit card so her new balance is $70+43=$ $\$&lt;&lt;70+43=113.00 \gg&gt;113.00$. Answer: 113.</p>
<p>Figure $14 \mid$ R-GSM example where both the original and the reordered problems were correctly solved by all LLMs in our evaluation.</p>
<h1>? Original $\rightarrow$ correct</h1>
<p>Tony is painting a room with four walls. The north and south walls are $10 \times 8$ feet. The east and west walls are $5 \times 8$ feet. A gallon of paint can cover 20 square feet and cost $\$ 12$. How much will it cost to paint the room?</p>
<h2>? Reordered $\rightarrow$ correct</h2>
<p>A gallon of paint can cover 20 square feet and cost $\$ 12$. Tony is painting a room with four walls. The east and west walls are $5 \times 8$ feet. The north and south walls are $10 \times 8$ feet. How much will it cost to paint the room?</p>
<p>First, let's calculate the total area of the walls that Tony is going to paint. The east and west walls are both 5 feet wide and 8 feet tall. So for each of these walls: Area $=$ Width $\times$ Height $=5$ feet $\times 8$ feet $=40$ square feet</p>
<p>Since there are two of these walls, we need to double that: Total area for east and west walls $=2 \times 40$ square feet $=$ 80 square feet</p>
<p>The north and south walls are both 10 feet wide and 8 feet tall. So for each of these walls: Area $=$ Width $\times$ Height $=$ 10 feet $\times 8$ feet $=80$ square feet</p>
<p>Again, since there are two of these walls, we need to double that:
Total area for north and south walls $=2 \times 80$ square feet $=160$ square feet
Now, let's add the total areas of all four walls to get the total area to be painted:
Total area $=$ Area of east and west walls + Area of north and south walls
Total area $=80$ square feet +160 square feet $=240$ square feet
Now we know that one gallon of paint covers 20 square feet and costs $\$ 12$. To find out how many gallons Tony needs, we divide the total area by the coverage of one gallon:
Gallons needed $=$ Total area $/$ Coverage per gallon
Gallons needed $=240$ square feet $/ 20$ square feet per gallon
Gallons needed $=12$ gallons
Since paint is typically sold by the gallon, Tony will need to buy 12 gallons of paint to cover the 240 square feet of wallspace.
Now, let's calculate the total cost for the paint:
Total cost $=$ Number of gallons $\times$ Cost per gallon
Total cost $=12$ gallons $\times \$ 12$ per gallon
Total cost $=\$ 144$
Therefore, it will cost Tony $\mathbf{\$ 1 4 4}$ to buy the paint needed to cover the walls of the room.</p>
<p>Figure $15 \mid$ R-GSM example where both the original and the reordered problems were correctly solved by all LLMs in our evaluation.</p>            </div>
        </div>

    </div>
</body>
</html>
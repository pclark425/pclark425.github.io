<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5277 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5277</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5277</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-ac94dcc2a3449791720fe478d8213c3830f6821c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ac94dcc2a3449791720fe478d8213c3830f6821c" target="_blank">C5T5: Controllable Generation of Organic Molecules with Transformers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> C5T5 is proposed, a novel self-supervised pretraining method that enables transformers to make zero-shot select-and-replace edits, altering organic substances towards desired property values and provides a powerful interface to domain experts.</p>
                <p><strong>Paper Abstract:</strong> Methods for designing organic materials with desired properties have high potential impact across fields such as medicine, renewable energy, petrochemical engineering, and agriculture. However, using generative modeling to design substances with desired properties is difficult because candidate compounds must satisfy multiple constraints, including synthetic accessibility and other metrics that are intuitive to domain experts but challenging to quantify. We propose C5T5, a novel self-supervised pretraining method that enables transformers to make zero-shot select-and-replace edits, altering organic substances towards desired property values. C5T5 operates on IUPAC names -- a standardized molecular representation that intuitively encodes rich structural information for organic chemists but that has been largely ignored by the ML community. Our technique requires no edited molecule pairs to train and only a rough estimate of molecular properties, and it has the potential to model long-range dependencies and symmetric molecular structures more easily than graph-based methods. C5T5 also provides a powerful interface to domain experts: it grants users fine-grained control over the generative process by selecting and replacing IUPAC name fragments, which enables experts to leverage their intuitions about structure-activity relationships. We demonstrate C5T5's effectiveness on four physical properties relevant for drug discovery, showing that it learns successful and chemically intuitive strategies for altering molecules towards desired property values.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5277.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5277.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C5T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controllable Characteristic-Conditioned Chemical Changer with T5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based conditional infilling method that operates on IUPAC names to make targeted, zero-shot select-and-replace edits to organic molecules to push computed physicochemical properties (logP, logD, PSA, refractivity) toward desired discretized buckets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (t5-large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer (conditional infilling / seq-to-seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>t5-large (~700M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>IUPAC names and computed descriptors from PubChem (~109M compounds; training split ~90M). logP values taken from PubChem (XLogP3); logD, refractivity, and PSA computed using ChemAxon calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / molecular design (modifying physicochemical properties relevant to bioavailability: octanol-water partition/distribution coefficients (logP/logD), polar surface area (PSA), molar refractivity).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Self-supervised conditional infilling: random spans of tokenized IUPAC names are masked with sentinel tokens during training; sequences are prepended with discretized property tokens (<low>, <med>, <high>). At inference, the fragment to edit is masked, the desired property token is substituted, and the model samples autoregressively (greedy decoding used in experiments) to produce replacement IUPAC token spans (zero-shot select-and-replace).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>IUPAC name tokens (human-interpretable token vocabulary derived from OPSIN keywords and locants/stereochemistry markers). Generated molecules are parsed back to structures using ChemAxon/OPSIN.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (parsable by ChemAxon), novelty (not present in PubChem), computed property values (logP/logD/PSA/refractivity distributions before vs after edit), min/max generated property values per source, comparison to best-eligible-in-dataset baseline, qualitative chemical intuition of added tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>PubChem (Preferred IUPAC Names and XLogP3), ChemAxon property calculators (used to compute/validate logD, refractivity, PSA), XLogP3 from PubChem/OpenEye.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>C5T5 can shift molecular properties toward requested buckets in a zero-shot fashion; it more reliably raises properties (e.g., increasing logP) than lowering them for some properties. Generated sets show high novelty (typically >90%) and substantial validity (varying across sources, many >80%, some lower ~72%). For logP optimization, generated molecules sometimes reach property values outside the range of the best-in-dataset eligible compounds. The model rediscovers chemically intuitive edits (e.g., adding long hydrocarbon chains for higher logP, adding polar groups for lower logP).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to SMILES- or graph-based generative methods, C5T5 emphasizes interpretability and fine-grained user control (selective masking of IUPAC tokens) and does not require paired scaffold/decorator datasets (trainable in unsupervised/self-supervised fashion). It can model symmetric and long-range semantic edits easily due to token-level IUPAC representation. Tradeoffs include potentially harder syntax learning (IUPAC) versus SMILES, and edit-interface differences (select-and-replace vs scaffold-append approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Coarse-grained control (properties discretized into three buckets) rather than continuous optimization; success depends on masking location chosen by user (requires domain expertise or enumeration); IUPAC tokenization may not cleanly represent arbitrary subgraph edits; occasional generation of chemically unstable but syntactically valid molecules (e.g., adjacent NH2 groups that would rearrange/react); inability in some cases to lower properties if mask doesn't cover removable polar groups; computational cost (trained on 8 A100s; total AWS cost ~USD 12,000 across experiments) and associated carbon footprint (~320 kg CO2e reported); potential dual-use concerns (could be used to optimize harmful molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C5T5: Controllable Generation of Organic Molecules with Transformers', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Liggpt: Molecular generation using a transformer-decoder model <em>(Rating: 2)</em></li>
                <li>Transformer neural network for structure constrained molecular optimization <em>(Rating: 2)</em></li>
                <li>Attention-based generative models for de novo molecular design <em>(Rating: 2)</em></li>
                <li>Molecular optimization by capturing chemist's intuition using deep neural networks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5277",
    "paper_id": "paper-ac94dcc2a3449791720fe478d8213c3830f6821c",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "C5T5",
            "name_full": "Controllable Characteristic-Conditioned Chemical Changer with T5",
            "brief_description": "A transformer-based conditional infilling method that operates on IUPAC names to make targeted, zero-shot select-and-replace edits to organic molecules to push computed physicochemical properties (logP, logD, PSA, refractivity) toward desired discretized buckets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (t5-large)",
            "model_type": "encoder-decoder transformer (conditional infilling / seq-to-seq)",
            "model_size": "t5-large (~700M parameters)",
            "training_data": "IUPAC names and computed descriptors from PubChem (~109M compounds; training split ~90M). logP values taken from PubChem (XLogP3); logD, refractivity, and PSA computed using ChemAxon calculators.",
            "application_domain": "drug discovery / molecular design (modifying physicochemical properties relevant to bioavailability: octanol-water partition/distribution coefficients (logP/logD), polar surface area (PSA), molar refractivity).",
            "generation_method": "Self-supervised conditional infilling: random spans of tokenized IUPAC names are masked with sentinel tokens during training; sequences are prepended with discretized property tokens (&lt;low&gt;, &lt;med&gt;, &lt;high&gt;). At inference, the fragment to edit is masked, the desired property token is substituted, and the model samples autoregressively (greedy decoding used in experiments) to produce replacement IUPAC token spans (zero-shot select-and-replace).",
            "output_representation": "IUPAC name tokens (human-interpretable token vocabulary derived from OPSIN keywords and locants/stereochemistry markers). Generated molecules are parsed back to structures using ChemAxon/OPSIN.",
            "evaluation_metrics": "Validity (parsable by ChemAxon), novelty (not present in PubChem), computed property values (logP/logD/PSA/refractivity distributions before vs after edit), min/max generated property values per source, comparison to best-eligible-in-dataset baseline, qualitative chemical intuition of added tokens.",
            "benchmarks_or_datasets": "PubChem (Preferred IUPAC Names and XLogP3), ChemAxon property calculators (used to compute/validate logD, refractivity, PSA), XLogP3 from PubChem/OpenEye.",
            "results_summary": "C5T5 can shift molecular properties toward requested buckets in a zero-shot fashion; it more reliably raises properties (e.g., increasing logP) than lowering them for some properties. Generated sets show high novelty (typically &gt;90%) and substantial validity (varying across sources, many &gt;80%, some lower ~72%). For logP optimization, generated molecules sometimes reach property values outside the range of the best-in-dataset eligible compounds. The model rediscovers chemically intuitive edits (e.g., adding long hydrocarbon chains for higher logP, adding polar groups for lower logP).",
            "comparison_to_other_methods": "Compared to SMILES- or graph-based generative methods, C5T5 emphasizes interpretability and fine-grained user control (selective masking of IUPAC tokens) and does not require paired scaffold/decorator datasets (trainable in unsupervised/self-supervised fashion). It can model symmetric and long-range semantic edits easily due to token-level IUPAC representation. Tradeoffs include potentially harder syntax learning (IUPAC) versus SMILES, and edit-interface differences (select-and-replace vs scaffold-append approaches).",
            "limitations_or_challenges": "Coarse-grained control (properties discretized into three buckets) rather than continuous optimization; success depends on masking location chosen by user (requires domain expertise or enumeration); IUPAC tokenization may not cleanly represent arbitrary subgraph edits; occasional generation of chemically unstable but syntactically valid molecules (e.g., adjacent NH2 groups that would rearrange/react); inability in some cases to lower properties if mask doesn't cover removable polar groups; computational cost (trained on 8 A100s; total AWS cost ~USD 12,000 across experiments) and associated carbon footprint (~320 kg CO2e reported); potential dual-use concerns (could be used to optimize harmful molecules).",
            "uuid": "e5277.0",
            "source_info": {
                "paper_title": "C5T5: Controllable Generation of Organic Molecules with Transformers",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Liggpt: Molecular generation using a transformer-decoder model",
            "rating": 2
        },
        {
            "paper_title": "Transformer neural network for structure constrained molecular optimization",
            "rating": 2
        },
        {
            "paper_title": "Attention-based generative models for de novo molecular design",
            "rating": 2
        },
        {
            "paper_title": "Molecular optimization by capturing chemist's intuition using deep neural networks",
            "rating": 2
        }
    ],
    "cost": 0.00862075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>C5T5: Controllable Generation of Organic Molecules with Transformers</h1>
<p>Daniel Rothchild<br>EECS<br>UC Berkeley<br>drothchild@berkeley.edu<br>Alex Tamkin<br>Computer Science<br>Stanford University<br>atamkin@stanford.edu<br>Julie Yu<br>Data Science \&amp; Chemistry<br>UC Berkeley<br>julieyu@berkeley.edu<br>Ujval Misra<br>EECS<br>UC Berkeley<br>ujval@berkeley.edu<br>Joseph Gonzalez<br>EECS<br>UC Berkeley<br>jegonzal@berkeley.edu</p>
<h4>Abstract</h4>
<p>Methods for designing organic materials with desired properties have high potential impact across fields such as medicine, renewable energy, petrochemical engineering, and agriculture. However, using generative modeling to design substances with desired properties is difficult because candidate compounds must satisfy multiple constraints, including synthetic accessibility and other metrics that are intuitive to domain experts but challenging to quantify. We propose C5T5, a novel selfsupervised pretraining method that enables transformers to make zero-shot select-and-replace edits, altering organic substances towards desired property values. C5T5 operates on IUPAC names-a standardized molecular representation that intuitively encodes rich structural information for organic chemists but that has been largely ignored by the ML community. Our technique requires no edited molecule pairs to train and only a rough estimate of molecular properties, and it has the potential to model long-range dependencies and symmetric molecular structures more easily than graph-based methods. C5T5 also provides a powerful interface to domain experts: it grants users fine-grained control over the generative process by selecting and replacing IUPAC name fragments, which enables experts to leverage their intuitions about structure-activity relationships. We demonstrate C5T5's effectiveness on four physical properties relevant for drug discovery, showing that it learns successful and chemically intuitive strategies for altering molecules towards desired property values.</p>
<h2>1 Introduction</h2>
<p>Organic molecules are used in countless applications across human society: as medicines, industrial chemicals, fuels, pesticides, plastics, television screens, solar cells, and many others. Traditionally, new molecules are designed for particular tasks by hand, but the space of all possible molecules is so vast (e.g. the total number of drug-like molecules may be as high as $10^{60}$ ) that most useful materials are probably still undiscovered [1]. To automate materials discovery, domain experts have turned to high-throughput screening, in which a large library of potentially useful molecules is generated heuristically, and the most promising molecules are chosen for further study using computational models that estimate how effective each substance will be for the target application [2]. Unfortunately, high-throughput screening faces the fundamental limitation that the total number of molecules that can be screened is still only a tiny fraction of all possible molecules.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Increasing a molecule's octanol-water partition coefficient with C5T5. (1) A molecular fragment (acetyloxy) is identified in a molecule of interest. (2) The molecular fragment is replaced with a sentinel token (<s1>) and the property value is set to the desired bucket (<high>). (3) Sampling from C5T5 produces a new fragment (decyl). Substituting in the fragment yields a new molecule. The long chain of carbons added to the molecule increases its solubility in octanol while decreasing its solubility in water.</p>
<p>Generating molecules directly using machine learning addresses this limitation, but de novo molecular design using machine learning can be of limited use in domains like drug discovery, where experts' intuitions about structure-activity relationships and external factors like patentability are important to consider in the design process. These constraints can often be expressed by providing known portions of the molecular structure; for example a domain expert may be interested in a particular scaffold because it has favorable intellectual property attributes, or certain parts of a drug may be needed for the desired biological activity, while other parts can be modified to increase bioavailability.
To address this real-world setting, we consider the problem of learning to make localized modifications to a molecule that change its physical properties in a desired way. We propose C5T5: Controllable Characteristic-Conditioned Chemical Changer with T5 [3], a novel method for generative modeling of organic molecules that gives domain experts fine-grained control over the molecular optimization process while also providing more understandable predictions than prior methods (Figure 1). Our two key contributions are 1) recasting molecular modeling as language modeling on the semantically rich IUPAC name base representation, and 2) the development of a novel conditional language modeling strategy using transformers that supports targeted modifications to existing molecules.</p>
<p>IUPAC Names. The IUPAC naming system is a systematic way of naming organic molecules based on functional groups and moieties, which are commonly occurring clusters of connected atoms that have known chemical behaviors. Organic chemists have discovered countless chemical reactions that operate on functional groups, and they use these reactions to develop synthesis routes for novel molecules. Despite this, existing generative methods for organic molecules have ignored IUPAC names as a representation, instead opting for atom-based representations like SMILES [4] and molecular graphs [5]. We argue that these representations are less suitable for molecular optimization because adding or removing arbitrary atoms has no intuitive meaning to chemists and is unlikely to allow for easy synthesis; see Figure 2 for a comparison of IUPAC names and SMILES. To the best of our knowledge we are the first to use IUPAC names as a base representation for molecular modeling.</p>
<p>Self-Supervised Objective for Zero-Shot Editing. To enable targeted modifications of molecules without predefined edit pairs, we train transformers with a conditional variant of a self-supervised infilling task by masking out certain tokens in the IUPAC name and training the model to replace these missing tokens. Crucially, we prepend the IUPAC names with discretized property values during training, enabling the model to learn the conditional relationships between the property value and molecular structure. During inference, we replace the true property values with desired property values, mask out the portion of the molecule we want replaced, and ask the model to infill the masked tokens as usual. To the best of our knowledge, this sort of self-supervision to enable guided select-and-replace editing has not been explored previously; we anticipate this method could be broadly applied in other controlled generation contexts, such as modeling various aspects of text like affect, politeness, or topic [6-9].
As we show in Section 4, C5T5 is able to make interpretable targeted modifications to molecules that lead to desired changes across several physical properties important in drug design.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Visual representations of IUPAC names and SMILES. Tokens in IUPAC names correspond to well-known functional groups and moieties. In contrast, tokens in SMILES correspond to individual atoms and bonds.</p>
<h1>2 Related Work</h1>
<p>Modeling. A number of machine learning methods have been developed for the task of designing organic molecules, but most do not allow a user to make targeted modifications to a molecule. Some methods, like generative adversarial networks and unconditional sequence models, provide no control over a generated molecule's structure [10-14], and are therefore more useful for generating candidate libraries than optimizing a particular molecule. Other methods, like variational autoencoders or sequence models that are conditioned on a base molecule, allow specifying that generated molecules should be similar to a starting molecule in some learned space, but there is no way to specifically target a certain part of the molecule to modify [15-29]. Recognizing the importance of leveraging domain experts' intuition about structure-activity relationships, several methods, published mostly in chemistry venues, have explored constraining generated molecules to contain a scaffold, or a subgraph of the full molecular graph [30-32]. However, these methods append to scaffolds arbitrarily instead of allowing domain experts to specify which part of the molecule they would like to modify or append to, limiting their utility for human-in-the-loop molecular optimization.
A few methods have explored allowing targeted modifications, where a domain expert can mask out a portion of a starting molecule and ask the model to replace the mask with a novel side chain [33-35]. These methods are limited because they only support masking parts of the molecule that can be truncated by cutting a single bond, and because they require a dataset of paired molecules (scaffolds \&amp; decorators) that must be constructed using hand-crafted rules. In contrast, C5T5 learns in an entirely unsupervised fashion and therefore requires no paired data; the only limit to what can be masked is what can be represented using IUPAC tokens.</p>
<p>Representation. Existing methods all use SMILES (or a derivative representation) or graphs to represent molecules. There are a number of drawbacks to using the SMILES representation: a small change in a molecule can lead to a large change in the SMILES string [24]; flattening the graph into a list of atoms artificially creates variable- and long-range dependencies between bonded atoms; and it is difficult to reason about common substructures, because the same structure can be represented in many different ways depending on how the graph was flattened. And although graphs seem like a natural representation for molecules, graphs do a poor job encoding symmetry, long-range interactions between atoms that are many bonds apart but nearby in 3D space, and long-range interactions that arise from conjugated systems [5]. C5T5 operates instead of IUPAC names, which we argue in Section 3.1 is a more suitable representation for molecular optimization because tokens have much more semantic meaning. See Appendix A for more details on how C5T5 relates to prior work.</p>
<p>Transformers for Molecular Modeling Outside of molecular optimization, transformers have found a number of applications in molecular modeling tasks, including property prediction [36, 37], chemical reaction prediction [38], retrosynthesis [39] and generating proteins [40, 41]. A few works have explored using transformers for generative modeling of organic molecules [15, 17, 22]. Some works have also proposed using transformers for scaffold-conditioned generative modeling [27, 35]. This work extends these efforts by proposing a simple yet effective training and zero-shot adaptation method, and by using IUPAC names instead of SMILES strings.</p>
<p>IUPAC Names Although we are unaware of prior work using IUPAC names as a base representation for molecular modeling, several works have explored using machine learning to convert between IUPAC names and other molecular representations [42-44].</p>
<h1>3 Method</h1>
<p>Molecular optimization is a difficult problem because it requires modifying a molecule that already satisfies a number of requirements. Modifications need to improve a particular aspect of the molecule without degrading its performance on other metrics, and without making it too difficult to synthesize. We argue that by using IUPAC names (Section 3.1) and by allowing users to target particular parts of a molecule to modify (Section 3.2), C5T5 has the potential to support human-in-the-loop molecular editing that complements domain experts' intuitions about structure-activity relationships and synthetic accessibility.</p>
<h3>3.1 IUPAC Naming</h3>
<p>The International Union of Pure and Applied Chemistry (IUPAC) publishes a set of rules that allow systematic conversion between a chemical structure and a human-readable name [45]. For example, 2-chloropentane refers unambiguously to five carbons ("pent") connected by single bonds ("ane") with a chlorine atom ("chloro") bonded to the second carbon from one end ("2-"). IUPAC names are used ubiquitously in scholarly articles, patents, and educational materials. In contrast to other linear molecular representations like SMILES and its derivatives, where single tokens mostly refer to individual atoms and bonds, tokens in IUPAC names generally have a rich semantic meaning. For example, the token "ic acid" denotes a carboxylic acid, which is a common functional group that has well-known physical and chemical properties; there are many known chemical reactions that either start with or produce carboxylic acids. Other tokens denote additional functional groups (e.g. "imide," "imine," "al," "one"), locants (e.g. " 1 ," " 2 ," " N "), which indicate connectivity, alkanes (e.g. "meth," "eth," "prop"), which denote the lengths of carbon chains, polycyclic rings (e.g. "naphthalene," "anthracene"), stereochemistry markers ("R," "S"), and multipliers (e.g. "di," "tri"), which concisely represent duplicated and symmetric structures. Figure 2 shows the relationships between IUPAC names, graph representations, and SMILES.
For molecular optimization, C5T5 supports qualitatively different molecular edits compared to graphand SMILES-based methods by virtue of its use of IUPAC names. In particular, editing a locant token corresponds to moving a functional group along a carbon backbone or changing the connectivity of a fused ring system. And editing a multiplier token corresponds to creating or eliminating duplicated and symmetric structures. For example, changing "ethylbenzene" to "hexaethylbenzene" replicates the ethyl structure around the entire benzene ring with a single token edit. These sorts of modifications require much more extensive editing for SMILES- and graph-based methods. ${ }^{1}$
We argue that IUPAC names are especially attractive for molecular optimization, since the process requires interaction between the algorithm and a domain expert, so interpretability is paramount. Compared to graph- or SMILES-based models, C5T5 makes predictions that can be traced back to moieties and functional groups that domain experts are more likely to understand, trust, and know how to synthesize than arbitrary collections of atoms and bonds.
In addition to improved interpretability, we argue that using IUPAC names has advantages purely from the standpoint of modeling data, since moving from SMILES to IUPAC names is akin to moving from a character-based to a word-based sequence model. Modeling at this higher level of abstraction enables the network to direct more of its capacity to structure at the relevant semantic level, instead of relearning lower-level details like the specific atomic composition of functional groups. In this vein, we demonstrate the potential of IUPAC names by learning word2vec representations of IUPAC name tokens [46], drawn from a list of over 100 million names in the PubChem repository [47] and tokenized using a list of tokens in OPSIN-an open-source IUPAC Name parser library (MIT License) [48]. For example, as shown in Figure 2, the chemical "2-acetyloxybenzoic acid" gets tokenized to ["2", "-", "acet", "yl", "oxy", "benzo", "ic acid"]. As with natural language modeling, we find that the embedding space learned by word2vec encodes the semantic meaning of the tokens, as shown in Figure 3. Different classes of tokens tend to be clustered together, and similar tokens within clusters are located nearby. For example, aromatic compounds with two rings are clearly separated from those with three, locants are ordered roughly correctly from 1 to 100, and multiplier</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>tokens are also roughly in order (zoom not shown). Following Mikolov et al. [46], we also find that simple arithmetic operations in the embedding vector space correspond to semantic analogies between tokens. For example, the nearest neighbor of "phosphonous acid" - "nitrous acid" + "nitroso" is the embedding for "phosphoroso." ${ }^{2}$ The nearest neighbor of "diphosphate" - "disulfate" + "sulfate" is "phosphate." Likewise for "selenate" - "tellurate" + "tellurite" being closest to "selenite."</p>
<h1>3.2 Conditional Language Modeling with Transformers</h1>
<p>We now present the C5T5 objective, which trains a model to alter the properties of a molecule through localized edits. Importantly, this behavior does not require training on human-defined edit pairs, a strategy limited by either a fixed set of hand-specified chemical alterations or an expensive experimentally-derived training set. Instead, this editing behavior emerges as a zero-shot side-effect of our conditional language modeling objective, requiring only a simple forward pass using our pretrained model without additional gradient updates.
Given a property value $P$, a fragment of a molecule $F$, and the rest of the molecule $C$ (the context), we wish to learn the conditional distribution $P(F \mid C, P)$. Then, for a new molecule, one could alter the molecule towards a desired property value by redacting the original $F$, changing $P$ to $P^{\prime}$ and sampling a new $F^{\prime} \sim P\left(F \mid C, P^{\prime}\right)$. Intuitively, this asks our model what kinds of molecular fragments the model would expect given the context and the new property value.
To learn this conditional distribution, we propose a conditional generalization of the infilling objective used to train T5 [3] and ILM [49]. This process consists of several steps, also illustrated in Figure 1:</p>
<ol>
<li>Replacing random spans of the tokenized IUPAC name with sentinel tokens.</li>
<li>Prepending the resulting sequence with a token indicating the original molecule's computed property value. To obtain these property value tokens, we discretize the distribution of property values into three buckets, specified in Table 1.</li>
<li>Training the model as in T5 to produce the sequence of redacted tokens, prepended by their corresponding sentinel tokens.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Property</th>
<th style="text-align: center;">$&lt;$ low $&gt;$</th>
<th style="text-align: center;">$&lt;$ med $&gt;$</th>
<th style="text-align: center;">$&lt;$ high $&gt;$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Octanol-water partition coeff. (logP)</td>
<td style="text-align: center;">$(-\infty,-0.4)$</td>
<td style="text-align: center;">$(-0.4,5.6)$</td>
<td style="text-align: center;">$(5.6, \infty)$</td>
</tr>
<tr>
<td style="text-align: left;">Octanol-water distribution coeff. (logD)</td>
<td style="text-align: center;">$(-\infty,-0.4)$</td>
<td style="text-align: center;">$(-0.4,5.6)$</td>
<td style="text-align: center;">$(5.6, \infty)$</td>
</tr>
<tr>
<td style="text-align: left;">Polar surface area (PSA)</td>
<td style="text-align: center;">$(0,90)$</td>
<td style="text-align: center;">$(90,140)$</td>
<td style="text-align: center;">$(140, \infty)$</td>
</tr>
<tr>
<td style="text-align: left;">Refractivity</td>
<td style="text-align: center;">$(0,40)$</td>
<td style="text-align: center;">$(40,130)$</td>
<td style="text-align: center;">$(130, \infty)$</td>
</tr>
</tbody>
</table>
<p>Table 1: Numerical ranges across properties for each property value token. Cutoffs for $\log \mathrm{P}$, PSA, and refractivity were chosen as common thresholds for druglikeness screening following [50-52]. We use the same cutoff for $\log \mathrm{D}$ as $\log \mathrm{P}$.</p>
<p>This conditional infilling objective incentivizes the model to learn the relationship between the computed property value and the missing tokens. To make a localized edit to a molecule, we then replace the desired fragments with sentinel tokens, change the property value token, and sample autoregressively from the predictive distribution. Thus, our approach hybridizes the flexible editing capabilities of ILM [49] with the controllability of CTRL [9]. See Appendix C for experimental details. Code is available at https://github.com/dhroth/c5t5.</p>
<h2>4 Results</h2>
<p>To demonstrate the promise of combining IUPAC names with conditional modeling using T5, we explore several molecular optimization tasks relevant to drug discovery. Specifically, we train C5T5 to make localized changes that affect the octanol-water partition and distribution coefficients (logP, logD), polar surface area (PSA), and refractivity-four properties commonly used to estimate bioavailability of a candidate drug [50, 51, 53]. $\log \mathrm{P}$ and $\log \mathrm{D}$ measure the ratio of a compound's</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: T-SNE visualization of the word2vec embedding space. "Charge": tokens that indicate formal charge. "Group": functional groups and moieties. "Count/Mult": multipliers. "Ring loc." fused-ring locants. "Stereo": stereochemistry markers. "Locants": simple locants. "Elements": single-atom tokens. As shown, the 2D location of tokens carries high semantic meaning; for example, locants are not only collocated, but are approximately in order.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Calculated property values of optimized molecules vs. original. Values computed for 30 randomly chosen starting molecules. Top left: octanol-water partition coefficient. Top right: octanol-water distribution coefficient at pH of 7. Bottom left: molar refractivity. Bottom right: polar surface area. Blue violins show the distribution of generated molecule properties when the model was asked to complete molecules to achieve a <high> property value, and red for <low>. Cutoffs between <high> and <med> and between <med> and <low> are shown as dashed blue and red lines, respectively. The black dashed line is $\mathrm{y}=\mathrm{x}$.
solubility in octanol, a lipid-like structure, and water; drugs need to be somewhat soluble in both to be orally absorbed. PSA and refractivity both relate to charge separation within the molecule. As shown in Appendix B, C5T5 generates mostly valid and novel molecules, with values of logP that lie outside of the range of a "best in dataset" baseline for targeted modifications.</p>
<h1>4.1 C5T5 Successfully Modifies Properties</h1>
<p>First, we demonstrate that the localized changes proposed by C5T5 do in fact control the property value as desired. C5T5 allows domain experts to choose where to make changes to a molecule based on their intuition and particular application: the user masks out tokens in the IUPAC name that can be modified, and the model fills in the masked spans in a way that changes the property value as directed. There is no canonical way to choose particular tokens to mask for evaluation purposes, so we simply choose a number of starting molecules randomly from PubChem [47], and then we iteratively mask all length-one to length-five spans (to match the training distribution) and run inference. In some instances, there is only one way to fill in the mask that yields a valid IUPAC name, regardless of the desired property value (e.g. a single-token mask that removes a parenthesis, a comma between two locants, or a hyphen after a locant). This would not be an issue in real usage, so in our evaluation we simply ignore cases when the model generates the starting molecule. We also experiment with masking multiple spans per molecule during inference, as is done during training. This is useful in practice when there are multiple areas of the molecule that can be changed in tandem to achieve the desired property value, but in our evaluation we observe qualitatively similar results when masking only a single span, so for computational efficiency we limit ourselves to single spans. We expect multi-span masks to be much more important when optimizing for more complex properties.</p>
<p>Figure 4 shows that C5T5 successfully generates molecules with higher property values when passed <high>, and with lower property values when passed <low>. The model is much more successful at raising property values than lowering them, especially for refractivity and polar surface area. For both of these properties, increasing the property value is straightforward: just add polar groups to replace whatever tokens were masked. In contrast, lowering these properties is only possible when the mask coincides with a polar group, in which case the model must find a non-polar substitute while still maintaining the molecule's validity. Even if unsuccessful at lowering these two property values on average, C5T5 can still be used in this case to suggest a number of candidate edits, and the one with the lowest property value can be selected using a property prediction model. This is an improvement over high-throughput screening and untargeted machine-learning methods for molecular optimization, since it isn't restricted to a predefined library of candidate molecules, and it still allows the user to choose particular parts of the molecule to modify.</p>
<p>Table 2: Tokens most preferentially added when C5T5 is asked to make modifications resulting high vs. low $\log \mathbf{P}$ values. Multipliers compare the actual rate of adding tokens compared to the expected number if the model drew randomly from the data distribution independent of property value. Blue tokens are hydrocarbons (i.e. lipophilic groups). Red tokens contain hydrogen bonding donors or acceptors (i.e. hydrophilic groups)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Target: <high></th>
<th style="text-align: center;"></th>
<th style="text-align: left;">Target: <low></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">trityl</td>
<td style="text-align: center;">77.0 x</td>
<td style="text-align: left;">phospho</td>
<td style="text-align: center;">48.1 x</td>
</tr>
<tr>
<td style="text-align: left;">pentadecyl</td>
<td style="text-align: center;">20.2 x</td>
<td style="text-align: left;">phosphonato</td>
<td style="text-align: center;">44.7 x</td>
</tr>
<tr>
<td style="text-align: left;">Z</td>
<td style="text-align: center;">17.7 x</td>
<td style="text-align: left;">sulfinam</td>
<td style="text-align: center;">41.2 x</td>
</tr>
<tr>
<td style="text-align: left;">perylen</td>
<td style="text-align: center;">11.8 x</td>
<td style="text-align: left;">hydrazon</td>
<td style="text-align: center;">34.3 x</td>
</tr>
<tr>
<td style="text-align: left;">undecyl</td>
<td style="text-align: center;">8.1 x</td>
<td style="text-align: left;">sulfinato</td>
<td style="text-align: center;">26.3 x</td>
</tr>
<tr>
<td style="text-align: left;">heptadecyl</td>
<td style="text-align: center;">7.9 x</td>
<td style="text-align: left;">Z</td>
<td style="text-align: center;">17.9 x</td>
</tr>
<tr>
<td style="text-align: left;">ylium</td>
<td style="text-align: center;">7.6 x</td>
<td style="text-align: left;">oxonium</td>
<td style="text-align: center;">10.3 x</td>
</tr>
<tr>
<td style="text-align: left;">isoindolo</td>
<td style="text-align: center;">5.9 x</td>
<td style="text-align: left;">amoyl</td>
<td style="text-align: center;">9.3 x</td>
</tr>
<tr>
<td style="text-align: left;">bH</td>
<td style="text-align: center;">5.9 x</td>
<td style="text-align: left;">carbamic acid</td>
<td style="text-align: center;">8.6 x</td>
</tr>
<tr>
<td style="text-align: left;">iod</td>
<td style="text-align: center;">5.8 x</td>
<td style="text-align: left;">sulfin</td>
<td style="text-align: center;">6.9 x</td>
</tr>
</tbody>
</table>
<h1>4.2 Modified Tokens are Chemically Intuitive</h1>
<p>One main advantage of C5T5 is that the changes it suggests are in the intuitive language of IUPAC names, rather than suggesting arbitrary combinations of atoms. Table 2 shows the tokens that the model most preferentially adds to a molecule when asked to produce low vs. high logP. Unsurprisingly, the most common tokens added when increasing $\log \mathrm{P}$ are generally long carbon chains (pentadecyl, undecyl, heptadecyl) and other hydrocarbons (trityl, perylen). Conversely, when the model is asked to produce low-logP modifications, hydrophilic groups are added. LogP is a simple metric, and by proposing molecular edits for $\log \mathrm{P}$ that are obvious and easily understandable to domain experts, we expect users to gain confidence that C5T5 will suggest reasonable edits for more complex properties.
To further investigate the types of optimizations C5T5 suggests, Figure 5 visualizes two of the starting molecules from Figure 4 with $\log \mathrm{P}$ values of -2.4 and 5.8. For each molecule, we mask spans as usual and generate after prepending with <low> (molecules on the left) and <high> (molecules on the right). The IUPAC name of the top starting molecule is "3,3-bis(aminomethyl)pentane-1,5-diol," where "bis" signifies that the CNH2 group should be duplicated, and "diol" means duplicate OH groups at the ends. By virtue of the IUPAC name encoding symmetry, C5T5 is easily able to generate similarly symmetric molecules. For example, the molecule in the top left is "3,3-bis(aminomethyl)pentane-1,5-disulfinic acid," where the "ol" has been replaced with "sulfinic acid." Although symmetry is often highly desired, C5T5 is not limited to generating symmetric molecules. For example, the middle molecule on the right is "3,3-bis(aminomethyl)-1-heptadecylpentane-1,5-diol"-C5T5 added an additional carbon chain non-symmetrically at the end of the pentane. ${ }^{3}$
Sometimes C5T5 generates valid but unstable compounds. For example, the neighboring $\mathrm{NH}_{2}$ groups in the bottom left molecule in the top half of Figure 5 are unstable, and would turn into aldehydes in an</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Visualizations of base and logP-optimized molecules. Two molecules from the logP plot in Figure 4, with three molecules after optimizing with C5T5 for each of <low> and <high> logP.
aqueous solution. All machine learning methods are susceptible to this sort of mistake, underscoring the importance of the type of human-in-the-loop optimization that C5T5 enables.</p>
<h1>5 Discussion and Conclusion</h1>
<p>We propose C5T5, a simple and effective zero-shot method for targeted control of molecular properties with transformers. Unlike prior approaches that make user-targeted modifications, our method requires no database of paired edits; instead, it simply trains in a self-supervised fashion on a large dataset of molecules and coarse estimates of their molecular property values. Core to our method is the use of IUPAC names as a data representation, which captures molecular structure at an appropriate level of abstraction and enables an intuitive editing interface for domain experts. C5T5 successfully rediscovers chemically-intuitive strategies for altering four drug-related properties in molecules, a notable feat given the absence of any human demonstration of these editing strategies.
Our work also has several limitations. The select-and-replace interface provided by the infilling objective may not always match the needs or preferred design process of biochemists who might wish to use it. The interface also only suggests how to fill in missing parts of a molecule, and it relies on domain expertise or enumeration to decide which parts of the molecule should be changed to begin with. In addition, we explore only a coarse-grained bucketing of property values, leaving a more fine-grained treatment for future work. IUPAC names might also be too limiting in cases where a user wants to edit a subgraph of a molecule that does not correspond neatly to a small number of IUPAC tokens. Finally, although there are many potential positive impacts to better drug design, including safer and more effective medicines to prolong the length and quality of people's lives, the generality of this method means that bad actors could optimize molecules towards harmful properties as well.
Future work will investigate using C5T5 with more molecular properties, such as the power conversion efficiency of solar cells. We also leave to future work extending C5T5 to jointly model multiple properties, and adding a more flexible editing interface.</p>
<h1>6 Acknowledgements</h1>
<p>ChemAxon's calculator was used to compute IUPAC names, chemical descriptors, and molecular properties (version 20.17.0, https://www.chemaxon.com).
We thank ACD/Labs for computing IUPAC names for several datasets.
This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE 1752814. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
AT is supported by an OpenPhil AI fellowship.
In addition to NSF CISE Expeditions Award CCF-1730628, this research is supported by gifts from Amazon Web Services, Ant Group, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia, Scotiabank, Splunk and VMware.</p>
<h1>References</h1>
<p>[1] Jean-Louis Reymond, Lars Ruddigkeit, Lorenz Blum, and Ruud van Deursen. The enumeration of chemical space. Wiley Interdisciplinary Reviews: Computational Molecular Science, 2(5): $717-733,2012$.
[2] James P Hughes, Stephen Rees, S Barrett Kalindjian, and Karen L Philpott. Principles of early drug discovery. British journal of pharmacology, 162(6):1239-1249, 2011.
[3] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.
[4] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28 (1):31-36, 1988.
[5] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gmez-Bombarelli, Timothy Hirzel, Aln Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS'15, page 2224-2232, Cambridge, MA, USA, 2015. MIT Press.
[6] Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, and Stefan Scherer. Affect-lm: A neural language model for customizable affective text generation. arXiv preprint arXiv:1704.06851, 2017.
[7] Jessica Ficler and Y. Goldberg. Controlling linguistic style aspects in neural language generation. ArXiv, abs/1707.02633, 2017.
[8] Tong Niu and Mohit Bansal. Polite dialogue generation without parallel data. Transactions of the Association for Computational Linguistics, 6:373-389, 2018.
[9] N. Keskar, Bryan McCann, L. Varshney, Caiming Xiong, and R. Socher. Ctrl: A conditional transformer language model for controllable generation. ArXiv, abs/1909.05858, 2019.
[10] Francesca Grisoni, Michael Moret, Robin Lingwood, and Gisbert Schneider. Bidirectional molecule generation with recurrent neural networks. Journal of chemical information and modeling, 60(3):1175-1183, 2020.
[11] Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha Farias, and Aln Aspuru-Guzik. Objective-reinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint arXiv:1705.10843, 2017.
[12] Benjamin Sanchez-Lengeling, Carlos Outeiral, Gabriel L Guimaraes, and Alan Aspuru-Guzik. Optimizing distributions over molecular space. an objective-reinforced generative adversarial network for inverse-design chemistry (organic).
[13] Nicola De Cao and Thomas Kipf. MolGAN: An implicit generative model for small molecular graphs. ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models, 2018.
[14] Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In International Conference on Machine Learning, pages 3183-3191. PMLR, 2019.
[15] Jiazhen He, Huifang You, Emil Sandstrm, Eva Nittinger, Esben Jannik Bjerrum, Christian Tyrchan, Werngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemist's intuition using deep neural networks. Journal of cheminformatics, 13(1):1-17, 2021.
[16] Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-graph translation for molecule optimization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1xJAsA5F7.</p>
<p>[17] Bonggun Shin, Sungsoo Park, JinYeong Bak, and Joyce C Ho. Controlled molecule generator for optimizing multiple chemical properties. In Proceedings of the Conference on Health, Inference, and Learning, pages 146-153, 2021.
[18] Kevin Yang, Wengong Jin, Kyle Swanson, Regina Barzilay, and Tommi Jaakkola. Improving molecular design by stochastic iterative target augmentation. In International Conference on Machine Learning, pages 10716-10726. PMLR, 2020.
[19] Panagiotis-Christos Kotsias, Josep Ars-Pous, Hongming Chen, Ola Engkvist, Christian Tyrchan, and Esben Jannik Bjerrum. Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks. Nature Machine Intelligence, 2(5):254-265, 2020.
[20] Rafael Gmez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jos Miguel Hernndez-Lobato, Benjamn Snchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Aln Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 4(2):268-276, 2018. doi: 10. 1021/acscentsci.7b00572. URL https://doi.org/10.1021/acscentsci.7b00572. PMID: 29532027.
[21] Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative model based on conditional variational autoencoder for de novo molecular design. Journal of cheminformatics, 10(1):1-9, 2018.
[22] Orion Dollar, Nisarg Joshi, David Beck, and Jim Pfaendtner. Attention-based generative models for de novo molecular design. Chemical Science, 2021.
[23] Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Constrained graph variational autoencoders for molecule design. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, page 7806-7815, Red Hook, NY, USA, 2018. Curran Associates Inc.
[24] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International Conference on Machine Learning, pages 2323-2332. PMLR, 2018.
[25] ukasz Maziarka, Agnieszka Pocha, Jan Kaczmarczyk, Krzysztof Rataj, Tomasz Danel, and Micha Warcho. Mol-cyclegan: a generative model for molecular optimization. Journal of Cheminformatics, 12(1):1-18, 2020.
[26] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of cheminformatics, 9(1):1-14, 2017.
[27] Viraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. Liggpt: Molecular generation using a transformer-decoder model. 2021.
[28] Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, page 6412-6422, Red Hook, NY, USA, 2018. Curran Associates Inc.
[29] Chence Shi<em>, Minkai Xu</em>, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=S1esMikHYPr.
[30] Yibo Li, Jianxing Hu, Yanxing Wang, Jielong Zhou, Liangren Zhang, and Zhenming Liu. Deepscaffold: A comprehensive tool for scaffold-based de novo drug discovery using deep learning. Journal of chemical information and modeling, 60(1):77-91, 2019.
[31] Jaechang Lim, Sang-Yeon Hwang, Seokhyun Moon, Seungsu Kim, and Woo Youn Kim. Scaffold-based molecular design with a graph generative model. Chemical Science, 11(4): $1153-1164,2020$.</p>
<p>[32] Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stiefl, and Marc Brockschmidt. Learning to extend molecular scaffolds with structural motifs. arXiv preprint arXiv:2103.03864, 2021.
[33] Josep Ars-Pous, Atanas Patronov, Esben Jannik Bjerrum, Christian Tyrchan, Jean-Louis Reymond, Hongming Chen, and Ola Engkvist. Smiles-based deep generative scaffold decorator for de-novo drug design. Journal of cheminformatics, 12:1-18, 2020.
[34] Maxime Langevin, Herv Minoux, Maximilien Levesque, and Marc Bianciotto. Scaffoldconstrained molecular generation. Journal of Chemical Information and Modeling, 2020.
[35] Jiazhen He, Felix Mattsson, Marcus Forsberg, Esben Jannik Bjerrum, Ola Engkvist, Christian Tyrchan, Werngard Czechtizky, et al. Transformer neural network for structure constrained molecular optimization. 2021.
[36] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, pages 429-436, 2019.
[37] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems, 33, 2020.
[38] Philippe Schwaller, Teodoro Laino, Thophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.
[39] Pavel Karpov, Guillaume Godin, and Igor V Tetko. A transformer model for retrosynthesis. In International Conference on Artificial Neural Networks, pages 817-830. Springer, 2019.
[40] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Towards cracking the language of life's code through self-supervised deep learning and high performance computing. bioRxiv, 2020. doi: 10.1101/2020.07.12.199554. URL https://www.biorxiv.org/content/early/2020/07/12/2020.07.12.199554.
[41] Daria Grechishnikova. Transformer neural network for protein-specific de novo drug generation as a machine translation problem. Scientific reports, 11(1):1-13, 2021.
[42] Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Stout: Smiles to iupac names using neural machine translation. Journal of Cheminformatics, 13(1):1-14, 2021.
[43] Jennifer Handsel, Brian Matthews, Nicola Knight, and Simon Coles. Translating the molecules: adapting neural machine translation to predict iupac names from a chemical identifier. 2021.
[44] Lev Krasnov, Ivan Khokhlov, Maxim Fedorov, and Sergey Sosnin. Struct2iupac-transformerbased artificial neural network for the conversion between chemical notations. 2021.
[45] Henri A Favre and Warren H Powell. Nomenclature of organic chemistry: IUPAC recommendations and preferred names 2013. Royal Society of Chemistry, 2013.
[46] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
[47] Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han, Jane He, Siqian He, Benjamin A Shoemaker, et al. Pubchem substance and compound databases. Nucleic acids research, 44(D1):D1202-D1213, 2016.
[48] Daniel M Lowe, Peter T Corbett, Peter Murray-Rust, and Robert C Glen. Chemical name to structure: Opsin, an open source solution, 2011.
[49] Chris Donahue, Mina Lee, and Percy Liang. Enabling language models to fill in the blanks. In $A C L, 2020$.</p>
<p>[50] Arup K Ghose, Vellarkad N Viswanadhan, and John J Wendoloski. A knowledge-based approach in designing combinatorial or medicinal chemistry libraries for drug discovery. 1. a qualitative and quantitative characterization of known drug databases. Journal of combinatorial chemistry, $1(1): 55-68,1999$.
[51] Daniel F Veber, Stephen R Johnson, Hung-Yuan Cheng, Brian R Smith, Keith W Ward, and Kenneth D Kopple. Molecular properties that influence the oral bioavailability of drug candidates. Journal of medicinal chemistry, 45(12):2615-2623, 2002.
[52] Stephen A Hitchcock and Lewis D Pennington. Structure- brain exposure relationships. Journal of medicinal chemistry, 49(26):7559-7583, 2006.
[53] Sanjivanjit K Bhal, Karim Kassam, Ian G Peirson, and Greg M Pearl. The rule of five revisited: applying $\log \mathrm{d}$ in place of $\log \mathrm{p}$ in drug-likeness filters. Molecular pharmaceutics, 4(4):556-560, 2007.
[54] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP (Demonstration), 2018.
[55] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.
[56] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.
[57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
[58] Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Base Repr.</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">T? $^{a}$</th>
<th style="text-align: center;">UD? $^{b}$</th>
<th style="text-align: center;">CO? $^{c}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">C5T5</td>
<td style="text-align: center;">IUPAC</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[15]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">Seq2Seq/Transformer</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[34]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[33]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[16]</td>
<td style="text-align: center;">Graph/Motifs</td>
<td style="text-align: center;">JT-VAE+GAN</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[30]</td>
<td style="text-align: center;">Graph/Atoms</td>
<td style="text-align: center;">GNN+VAE</td>
<td style="text-align: center;">$\mathcal{X}^{d}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[31]</td>
<td style="text-align: center;">Graph/Atoms</td>
<td style="text-align: center;">VAE+GNN</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[32]</td>
<td style="text-align: center;">Graph/Motifs</td>
<td style="text-align: center;">VAE+GNN</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[27]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">GPT</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[17]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">Transformer+LSTM</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[35]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[19]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">cRNN</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[20]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">VAE+ConvNet+GRU</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[21]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">cVAE</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[22]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">VAE+Transformer</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[23]</td>
<td style="text-align: center;">Graph/Atoms</td>
<td style="text-align: center;">VAE+GNN</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[28]</td>
<td style="text-align: center;">Graph/Atoms</td>
<td style="text-align: center;">GCN+GAN+RL</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[24]</td>
<td style="text-align: center;">Graph/Motifs</td>
<td style="text-align: center;">JT-VAE</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[25]</td>
<td style="text-align: center;">Graph/Motifs</td>
<td style="text-align: center;">CycleGAN+JT-VAE</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">[29]</td>
<td style="text-align: center;">Graph/Atoms</td>
<td style="text-align: center;">Flow+RL</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">[26]</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">RNN+RL</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of C5T5 to prior methods</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>A Comparison To Prior Methods</h1>
<p>Table 3 shows how C5T5 and prior methods for molecular optimization differ along several axes.</p>
<h2>B Additional Experiments</h2>
<p>Section 4.1 shows that C5T5 successfully modifies property values. Here, for molecules generated to optimize $\log \mathrm{P}$, we show the novelty and validity of the generated molecules, along with a comparison to a baseline of the best eligible compound in PubChem. To match how the model was trained and how new molecules were generated, eligible compounds are those that could be generated by masking any consecutive span of at most 5 IUPAC name tokens and replacing the masked tokens with any number of replacement tokens. ${ }^{4}$ Results are shown in Tables 4 and 5. Percent validity is the fraction of generated molecules that T5 generated with valid sentinel tokens that were considered chemically valid by the ChemAxon $\log \mathrm{P}$ calculator. Percent novelty is the fraction of distinct generated molecules that do not appear in PubChem (excluding when C5T5 re-generated the source molecule). As shown, despite the comparative difficulty of learning IUPAC name syntax compared to SMILES syntax, C5T5 consistently finds novel and valid molecules that significantly outperform the best-in-dataset baseline.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table 4: For each source molecule, we show the percent of generated molecules that are novel (not in PubChem, including invalid names) and valid (can be parsed by ChemAxon), the number of generated molecules, the min/max logP of any generated molecule, the number of eligible compounds in PubChem and the min/max logP of all molecules in PubChem that could be generated by masking up to 5 consecutive tokens. IUPAC names of source molecules are listed in Table 5.</p>
<table>
<thead>
<tr>
<th>Src.</th>
<th># gen.</th>
<th>\% novel</th>
<th>\% valid</th>
<th>max gen.</th>
<th>min gen.</th>
<th># elig.</th>
<th>max PC</th>
<th>min PC</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>82</td>
<td>$95.1 \%$</td>
<td>$81.7 \%$</td>
<td>14.22</td>
<td>-5.04</td>
<td>26</td>
<td>10.81</td>
<td>-2.7</td>
</tr>
<tr>
<td>2</td>
<td>133</td>
<td>$93.2 \%$</td>
<td>$91.7 \%$</td>
<td>9.41</td>
<td>-3.82</td>
<td>28</td>
<td>1.92</td>
<td>-1.46</td>
</tr>
<tr>
<td>3</td>
<td>217</td>
<td>$98.6 \%$</td>
<td>$91.7 \%$</td>
<td>9.15</td>
<td>-2.17</td>
<td>4</td>
<td>3.01</td>
<td>1.66</td>
</tr>
<tr>
<td>4</td>
<td>140</td>
<td>$100.0 \%$</td>
<td>$94.3 \%$</td>
<td>10.21</td>
<td>1.15</td>
<td>3</td>
<td>8.18</td>
<td>3.78</td>
</tr>
<tr>
<td>5</td>
<td>128</td>
<td>$92.2 \%$</td>
<td>$88.3 \%$</td>
<td>6.91</td>
<td>0.86</td>
<td>19</td>
<td>4.24</td>
<td>2.35</td>
</tr>
<tr>
<td>6</td>
<td>160</td>
<td>$100.0 \%$</td>
<td>$75.6 \%$</td>
<td>8.16</td>
<td>0.47</td>
<td>4</td>
<td>2.56</td>
<td>1.6</td>
</tr>
<tr>
<td>7</td>
<td>159</td>
<td>$99.4 \%$</td>
<td>$86.2 \%$</td>
<td>9.05</td>
<td>-2.22</td>
<td>8</td>
<td>3.91</td>
<td>-1.34</td>
</tr>
<tr>
<td>8</td>
<td>137</td>
<td>$99.3 \%$</td>
<td>$84.7 \%$</td>
<td>14.74</td>
<td>-2.97</td>
<td>1</td>
<td>1.08</td>
<td>1.08</td>
</tr>
<tr>
<td>9</td>
<td>122</td>
<td>$95.1 \%$</td>
<td>$81.1 \%$</td>
<td>8.57</td>
<td>-4.89</td>
<td>34</td>
<td>4.75</td>
<td>-0.07</td>
</tr>
<tr>
<td>10</td>
<td>112</td>
<td>$92.9 \%$</td>
<td>$88.4 \%$</td>
<td>8.44</td>
<td>-2.96</td>
<td>112</td>
<td>5.03</td>
<td>-2.3</td>
</tr>
<tr>
<td>11</td>
<td>127</td>
<td>$97.6 \%$</td>
<td>$81.1 \%$</td>
<td>9.52</td>
<td>0.11</td>
<td>9</td>
<td>5.13</td>
<td>2.28</td>
</tr>
<tr>
<td>12</td>
<td>114</td>
<td>$90.4 \%$</td>
<td>$85.1 \%$</td>
<td>7.71</td>
<td>-3.32</td>
<td>515</td>
<td>6.13</td>
<td>-2.95</td>
</tr>
<tr>
<td>13</td>
<td>115</td>
<td>$94.8 \%$</td>
<td>$90.4 \%$</td>
<td>8.12</td>
<td>-1.91</td>
<td>36</td>
<td>4.05</td>
<td>0.3</td>
</tr>
<tr>
<td>14</td>
<td>149</td>
<td>$97.3 \%$</td>
<td>$85.2 \%$</td>
<td>14.09</td>
<td>-4.3</td>
<td>7</td>
<td>2.1</td>
<td>0.44</td>
</tr>
<tr>
<td>15</td>
<td>135</td>
<td>$100.0 \%$</td>
<td>$83.0 \%$</td>
<td>7.52</td>
<td>-0.7</td>
<td>2</td>
<td>5.08</td>
<td>4.12</td>
</tr>
<tr>
<td>16</td>
<td>214</td>
<td>$100.0 \%$</td>
<td>$97.7 \%$</td>
<td>6.86</td>
<td>-1.83</td>
<td>3</td>
<td>1.91</td>
<td>-0.05</td>
</tr>
<tr>
<td>17</td>
<td>156</td>
<td>$99.4 \%$</td>
<td>$92.3 \%$</td>
<td>10.58</td>
<td>-2.88</td>
<td>2</td>
<td>0.76</td>
<td>0.76</td>
</tr>
<tr>
<td>18</td>
<td>231</td>
<td>$98.7 \%$</td>
<td>$83.1 \%$</td>
<td>9.79</td>
<td>-1.35</td>
<td>6</td>
<td>5.6</td>
<td>3.46</td>
</tr>
<tr>
<td>19</td>
<td>148</td>
<td>$93.2 \%$</td>
<td>$82.4 \%$</td>
<td>9.98</td>
<td>-0.19</td>
<td>63</td>
<td>7.01</td>
<td>0.76</td>
</tr>
<tr>
<td>20</td>
<td>143</td>
<td>$96.5 \%$</td>
<td>$72.7 \%$</td>
<td>14.12</td>
<td>0.33</td>
<td>14</td>
<td>4.61</td>
<td>1.31</td>
</tr>
<tr>
<td>21</td>
<td>232</td>
<td>$99.1 \%$</td>
<td>$85.3 \%$</td>
<td>9.9</td>
<td>-1.6</td>
<td>6</td>
<td>3.15</td>
<td>1.14</td>
</tr>
<tr>
<td>22</td>
<td>150</td>
<td>$96.0 \%$</td>
<td>$92.7 \%$</td>
<td>7.96</td>
<td>-0.74</td>
<td>22</td>
<td>4.95</td>
<td>2.71</td>
</tr>
<tr>
<td>23</td>
<td>127</td>
<td>$94.5 \%$</td>
<td>$87.4 \%$</td>
<td>7.54</td>
<td>-3.1</td>
<td>18</td>
<td>4.67</td>
<td>0.37</td>
</tr>
<tr>
<td>24</td>
<td>160</td>
<td>$99.4 \%$</td>
<td>$82.5 \%$</td>
<td>7.54</td>
<td>-1.53</td>
<td>2</td>
<td>1.28</td>
<td>1.28</td>
</tr>
<tr>
<td>25</td>
<td>274</td>
<td>$100.0 \%$</td>
<td>$95.6 \%$</td>
<td>10.82</td>
<td>1.12</td>
<td>2</td>
<td>6.12</td>
<td>5.82</td>
</tr>
</tbody>
</table>
<h1>C Experimental Details</h1>
<p>Dataset Preparation We download PubChem from ftp.ncbi.nlm.nih.gov/pubchem/ Compound/CURRENT-Full/XML/ and extracted each molecule's Preferred IUPAC Name and computed octanol-water partition coefficient (logP). There are 109M total compounds in the version of PubChem we downloaded in January 2021. For experiments using logP, we used the XLogP3 values from PubChem, which were computed using OpenEye's software. For $\log \mathrm{D}(\mathrm{pH}=7$ ), refractivity, and polar surface area, we computed values using ChemAxon's calculator with default parameters. Separately for each target property, we excluded chemicals that had no logP value in PubChem or that were not parseable by ChemAxon's calculator. Of the remaining molecules, we randomly split into a training set with 90 M compounds and a validation set with $\sim 10 \mathrm{M}-19 \mathrm{M}$ compounds.</p>
<p>Tokenization We use HuggingFace's T5Tokenizer, which is based on the SentencePiece algorithm [54]. Because our goal is to have tokens that domain experts are familiar with, we do not train SentencePiece on the IUPAC names, since doing so learns both combinations of and substrings of moiety names. Instead, we manually specify the tokens to be all the keywords in the Opsin IUPAC name parsing library [48]. To these keywords, we add locants 1-100, stereochemistry markers (R, S, $\mathrm{E}, \mathrm{Z}, \ldots$ ), and a few miscellaneous tokens to e.g. handle spiro centers. This leads to a total of 1274 tokens. After tokenization, we truncate all names to at most 128 tokens.</p>
<p>Training We train a t5-large model ( $\sim 700 \mathrm{M}$ params) available from HuggingFace that was pretrained on English text. We keep the first 1274 embeddings from the pretrained embedding table, along with the pretrained embeddings for the 100 sentinel tokens. When training, we mask $15 \%$ of the tokens in each input in spans of mean length 3 tokens, with a minimum span length of 1 .</p>
<p>Table 5: IUPAC Name lookup table for Table 4</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Source Molecule</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3,3-bis(aminomethyl)pentane-1,5-diol</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1-(3-hydroxypropyl)-N-(1-methoxybutan-2-yl)pyrazole-4-sulfonamide</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4-chloro-N-[2-[[2-(4-fluorophenyl)acetyl]amino]ethyl]-1,3-thiazole-5-carboxamide</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1-[(1S)-1-(3-fluorophenyl)propyl]-3-iodoindole</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4-(4-fluorophenyl)-N-[(1R,2R)-2-methylcyclohexyl]piperazine-1-carbothioamide</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N'-(3-ethyl-4-oxophthalazine-1-carbonyl)-4-methyl-2-phenyl-1,3-thiazole-5-carbohydrazide</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">(E)-2-methoxy-3-methylhex-4-en-1-ol</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">N-methyl-1-[2-(4-methylthiadiazol-5-yl)-1,3-thiazol-4-yl]methanamine</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2-[(7-methyl-[1,2,4]triazolo[1,5-a]pyridin-2-yl)amino]ethylurea</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">4-[[2-(2-oxopyridin-1-yl)acetyl]amino]benzoic acid</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">[6-prop-2-enoxy-4-(trifluoromethyl)pyridin-2-yl]hydrazine</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">4-(2-methylphenyl)sulfonylpiperidin-3-amine</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">3-[ethyl(2-methylpropyl)amino]propane-1-thiol</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">6-methoxy-4-N-methyl-4-N-[(2-methylfuran-3-yl)methyl]pyrimidine-4,5-diamine</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">3-phenylmethoxy-5-(trifluoromethoxy)quinoline-2-carboxylic acid</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">3-[4-[acetamido-[3-methoxy-4-[(2-methylphenyl)carbamoylamino]phenyl]methyl]piperidin-1-yl]-</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">3-phenylpropanoic acid (3R)-3-[[(2S)-2-[benzyl(methyl)amino]butanoyl]amino]pyrrolidine-1-carboxamide 6-cyclobutyl-2-N-[3-(1-ethylsulfinylethyl)phenyl]-5-(trifluoromethyl)pyrimidine-2,4-diamine 6-fluoro-2-(4-phenylpyridin-2-yl)-1H-benzimidazole 4-chloro-3-(2-oxo-1,3-dihydroindol-5-yl)benzonitrile 1-(6-tert-butylpyridazin-3-yl)-N-methyl-N-[(2-methyl-1,3-oxazol-4-yl)methyl]azetidin-3-amine 2-[(4aR,8aS)-3,4,4a,5,6,7,8,8a-octahydro-1H-isoquinolin-2-yl]-N-(2,4-dimethoxyphenyl)acetamide 2-[2-(2,4-dichlorophenoxy)ethoxy]-4-methoxybenzoic acid (2Z)-2-[(1,7-dimethylquinolin-1-ium-2-yl)methylidene]-1-ethyl-7-methylquinoline N'-[(3S)-1-[[3-(2,4-dichlorophenyl)phenyl]methyl]-2-oxoazepan-3-yl]-3-(2-methylpropyl)-2-prop-2-enylbutanediamide</td>
</tr>
</tbody>
</table>
<p>We use a linear warmup of the learning rate for 10,000 steps followed by a $1 / T$ decay. All models were trained using the AdamW optimizer. We train the logP model for 2.5 M iterations using a max learning rate of $10^{-3}$. We train the refractivity/logD/Polar SA models using a maximum learning rate of $2 \times 10^{-4} / 10^{-4} / 2 \times 10^{-4}$ starting from the logP model after $1 \mathrm{M} / 2.5 \mathrm{M} / 2.5 \mathrm{M}$ iterations. (We trained using the latest model available when we started a run.) All models were trained on 8 NVIDIA A100s with a batch size of 16 per GPU.</p>
<p>Generation We generate novel molecules greedily from the output of T5's decoder. We discard any generations where the sentinel tokens do not line up, and we further discard any molecules that cannot be parsed by ChemAxon's calculators.</p>
<p>Cloud Computing Cost We train T5-Large models [3] on PubChem for each investigated property value using AWS p4d.24xlarge instances in the us-east-1 region. The logP model was trained for 2.5 M iterations over 8 days; the $\log \mathrm{D}$ and refractivity models for 350 k iterations over 1 day each (initialized from the logP model after 2.5 M iterations); and the Polar SA model for 1.6 M iterations over 5 days (initialized from the logP model after 1 M iterations). Total on-demand AWS cost for the models presented here is $\sim \$ 12,000$, and total carbon dioxide-equivalent emissions is $\sim 320 \mathrm{~kg}[55] .^{5}$</p>
<p>Software We use HuggingFace 4.2.2 (Apache 2.0 license) [56], PyTorch 1.8.0 (BSD) [57], ChemAxon 20.17.0 (Academic License), scikit-learn 0.24.2 (New BSD) [58] and python 3.9 (PSFL).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The emissions calculator does not yet support NVIDIA A100 GPUs, so we calculated with V100-32GB instead.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ For computational efficiency, we filter out molecules that differ in length by more than 15 tokens, or that have more than 15 non-overlapping tokens in their bag of tokens, before checking whether they could indeed be generated by masking some length- 5 sequence of tokens.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
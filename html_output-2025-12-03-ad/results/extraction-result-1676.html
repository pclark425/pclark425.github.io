<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1676 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1676</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1676</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-221971078</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2009.13303v2.pdf" target="_blank">Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey</a></p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-to-real gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1676.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1676.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Matas2018</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real reinforcement learning for deformable object manipulation (Matas et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulation-trained DRL policies for deformable-object manipulation using domain randomization; study highlights limits of simulators for object deformability and shows that excessive visual randomization can hurt real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real reinforcement learning for deformable object manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Kinova Mico (7DOF) with gripper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DOF robotic arm used for dexterous manipulation tasks involving deformable objects; intended for closed-loop vision-based grasping and manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>dexterous robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet (sim)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics-based simulator used to render robot, objects and interactions; simulates rigid-body dynamics and simplified contact for manipulation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics with randomized visual/dynamics parameters</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body dynamics, approximate contact dynamics, visual appearance (textures/colors)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>deformable-object physics (degree of deformability poorly modeled), exact contact/stiffness behavior of deformable objects, potentially simplified sensor noise and actuator dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical lab setup with real Kinova Mico arm manipulating objects of varying deformability and appearance under real lighting and sensing conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>grasping and manipulation of deformable objects</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>deep reinforcement learning with domain randomization (and stochastic grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>qualitative success in real-world grasps; specific numerical metrics not reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>visual randomization (colors, textures) and stochasticization of grasping/dynamics parameters; paper notes randomization of object properties overall</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>insufficient modeling of object deformability/stiffness, visual differences, over-randomization of visual appearance (too many colors) degrading transfer</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>domain randomization to cover variability; careful selection of randomization magnitude (avoid excessive visual variability); acknowledgment that simulator limitations in modeling deformability limit transfer</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>accurate modeling of deformability/contact dynamics is important; excessive visual randomization can be detrimental—implies some fidelity/realism in appearance and physical behavior is necessary</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain randomization enabled successful sim-to-real for deformable-object manipulation, but simulators struggled to capture real object deformability; excessive visual randomization harmed real performance, indicating an optimal level of randomization and the need for better physics fidelity for deformable objects.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1676.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1676.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kaspar2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim2real transfer for reinforcement learning without dynamics randomization (Kaspar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrates sim-to-real transfer for peg-in-hole tasks using accurate system identification and a high-quality robot model rather than dynamics randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim2real transfer for reinforcement learning without dynamics randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>KUKA LBR iiwa + WSG50 gripper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A collaborative 7-DOF industrial manipulator equipped with a parallel gripper used for precision assembly (peg-in-hole) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>precision robotic manipulation / peg-in-hole assembly</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet (custom high-quality model)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Customized simulator with a high-fidelity robot model used to closely match real robot kinematics and dynamics for one-shot/zero-shot transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity robot modeling (system-identified), approximate environment/contact physics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>robot kinematics and dynamics carefully system-identified; actuation/joint behavior modeled with high accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>some contact dynamics, micro-variations in friction, and environmental variability likely simplified</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real KUKA LBR iiwa robot performing peg-in-hole manipulation under lab conditions</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>peg-in-hole insertion task</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (SAC referenced in table) with system identification used to build simulator</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success in real robot trials (peg insertion success), specific quantitative rates not reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>mismatch between simulated and real dynamics if system identification is imperfect; contact/friction modeling limitations</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>accurate system identification and high-quality robot model enabled direct/zero-shot transfer without dynamics randomization</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>high-fidelity modeling of robot dynamics/actuation is critical for tasks requiring precision (e.g., peg-in-hole); careful calibration is necessary</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-quality system identification and precise robot modeling can enable sim-to-real transfer for precise manipulation tasks without requiring dynamics randomization, indicating fidelity of actuation/dynamics modeling is a key enabler.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1676.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1676.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traore2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer (Traoré et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrates continual multi-task RL with policy distillation and sim-to-real transfer, applied to a small mobile platform for navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Small mobile platform (unspecified model in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A small differential-drive mobile robot used for navigation tasks and multi-task continual learning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic navigation / continual multi-task robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics simulator modeling mobile robot dynamics, sensors and navigation environments for training multiple tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics suitable for navigation and multi-task RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>basic robot dynamics, environment geometry and sensor observations (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>detailed sensor noise characteristics, complex wheel-ground interactions, and environment-specific variabilities likely simplified</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real mobile robot platform navigating real-world environments to validate continual learning and distilled policies</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>mapless navigation and multi-task behaviors via policy distillation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning with policy distillation (multi-task distillation), continual RL pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>real-world task execution success (navigation), quantitative metrics not reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>differences in sensor behavior, environmental variability, and imperfect simulation of real-world dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>policy distillation into a single student network for multi-task deployment and continual learning pipeline; use of simulation to aggregate diverse experiences</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>no explicit numeric fidelity requirements; emphasizes combining distillation and continual learning to improve real deployment</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Policy distillation and continual RL pipelines can be transferred from simulation to real mobile platforms to deploy multi-task behaviors; sim-to-real benefits from consolidating knowledge into compact student policies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1676.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1676.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI_Dexterous</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning dexterous in-hand manipulation (OpenAI et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale DRL for dexterous in-hand manipulation where dynamics randomization over physical parameters was used to produce policies that transfer to real-world robotic hands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dexterous in-hand manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Five-fingered robotic hand (physical platform not fully specified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A multi-fingered robotic hand system designed for in-hand manipulation tasks requiring complex contact-rich control.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>dexterous robotic manipulation / in-hand manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (common in these works)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics engine simulating multi-body dynamics, contacts and actuation for a robotic hand interacting with objects.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-dimensional approximate physics with extensive parameter randomization</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body contact dynamics, joint actuation, object geometry, physical parameters randomized (masses, friction, damping, actuator gains)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>limitations in true contact fidelity for complex multi-contact deformable interactions, sensors/perception noise approximated</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical multi-fingered robotic hand performing in-hand manipulation tasks on real objects under lab conditions</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>dexterous in-hand object manipulation and reorientation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>deep reinforcement learning with extensive dynamics randomization</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>real-world task success (manipulation), paper reports successful transfer qualitatively in survey; numeric rates not provided here</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>randomized object dimensions, object and robot link masses, surface friction coefficients, joint damping coefficients, actuator force gains and other physical parameters</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>incomplete modeling of contact and contact-rich dynamics, sensor/actuator discrepancies, unmodeled wear-and-tear and environmental variability</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>extensive dynamics randomization across many physical parameters to produce robust policies that generalize to real-world variations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>accurate contact dynamics and diversity in randomized physical parameters are critical; no quantitative thresholds provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Randomizing a wide set of physical parameters enables robust sim-to-real transfer for complex in-hand manipulation; diversity in dynamics during simulation is a powerful enabler but relies on covering relevant real-world variability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1676.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1676.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Balaji_DeepRacer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepRacer: an educational autonomous racing platform (Balaji et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An educational sim-to-real platform for autonomous racing where users train policies in simulation (Gazebo/RoboMaker) with random colors and parallel domain randomization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepRacer: an educational autonomous racing platform.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>AWS DeepRacer (4WD 1:18 scale car)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A small-scale autonomous racing car platform used for education and research in RL, capable of on-board perception and control for autonomous racing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous racing / mobile robot navigation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo / RoboMaker</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulation of vehicle dynamics, sensors and track environments integrated with ROS and AWS RoboMaker for training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>moderate-fidelity vehicle dynamics with visual randomization</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>basic vehicle dynamics, camera images, track geometry, simulated sensors</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>detailed tire-ground contact physics, full sensor noise characteristics, high-fidelity rendering limitations</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical 1:18 DeepRacer car running on racing tracks under real lighting and surface conditions</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>autonomous racing control policies</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (PPO referenced) with visual randomization in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>ability to race autonomously; survey lists it as sim-only platform in table (used for experimentation), specific real-world success metrics not provided</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>random colors and parallel domain randomization of visual aspects</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>visual appearance mismatch, simplified vehicle dynamics, sensor modeling differences</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>visual domain randomization to improve robustness to real-world appearance variation; integration with ROS and RoboMaker for pipeline consistency</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>survey does not specify numeric thresholds; moderate fidelity in vehicle dynamics and consistent perception pipeline considered useful</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Educational platforms like DeepRacer use visual domain randomization to help policies generalize; consistent simulation pipeline and integration with robotics middleware aid transfer experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1676.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1676.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Witman2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet (Witman et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applies sim-to-real RL to control thermal effects of a plasma jet, using a customized simulator (MuJoCo + Ogre3D) and demonstrating transfer to a Mitsubishi Melfa RV-6SL robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Mitsubishi Melfa RV-6SL (robot arm)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Industrial robot arm used to manipulate a plasma jet actuator to control thermal effects on targets; requires precise control of tool trajectory and thermal dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic control of plasma jet / specialized manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo + Ogre3D (custom simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulates robot kinematics/dynamics and visual/thermal effects for the plasma jet; primarily physics-based simulation with rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>custom physics and rendering for task-specific thermal dynamics (moderate-to-high fidelity for thermal control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>robot dynamics, task-specific thermal/visual effects approximated in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>detailed plasma physics and exact thermal transfer in all real conditions likely approximated; sensor noise models unclear</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real industrial robot controlling a plasma jet in lab settings with sensors measuring thermal effects</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>control policies for thermal effect regulation via plasma jet manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (A3C sim + off-policy real referenced in table)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>control of thermal effect on target, survey references successful sim-to-real transfer qualitatively; numeric metrics not reported</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>complex plasma and thermal dynamics that are hard to model precisely; sensor and actuator differences between sim and real</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>task-specific simulator customization and robust policy training to handle modeling mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>requires simulator that captures the task-relevant thermal dynamics sufficiently well; exact quantitative requirements not provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Task-specific simulators with reasonable modeling of thermal and robot dynamics can support sim-to-real RL for specialized control tasks like plasma jet thermal regulation, but accurate modeling of physics relevant to the task is important.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1676.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1676.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qin2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning (Qin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses curriculum learning and DRL to train six-legged robot controllers in V-Rep and transfers to real hardware using curriculum and IK priors to aid sim-to-real.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Six-legged robot (platform unspecified beyond six-legged)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A hexapod robot used for navigation and obstacle avoidance, requiring stable locomotion policies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>legged robot navigation / locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>V-Rep</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator modeling legged robot kinematics, dynamics and environment for locomotion experiments; supports inverse kinematics for curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>moderate-fidelity dynamics suitable for locomotion prototyping</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>robot kinematics, contact approximations for legs, environment obstacles</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>detailed foot-ground contact friction variability, sensor noise, and micro-variations in terrain</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical hexapod robot performing navigation and obstacle avoidance in real environments</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>stable locomotion, navigation and obstacle avoidance</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (PPO) with curriculum learning and inverse kinematics priors</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>successful transfer to real robot for navigation/obstacle avoidance; numerical metrics not provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>mismatch in contact/friction modeling, terrain variability and sensor/actuator differences</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>curriculum learning to gradually increase environment complexity and use of IK priors to guide learning; incremental training aids transfer</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>progressively increasing environment complexity in sim helps bridge the gap; precise contact modeling remains important but not fully quantified</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curriculum learning and incremental environment complexity facilitate sim-to-real transfer for legged locomotion by guiding policies through progressively realistic training stages.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1676.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1676.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>James2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks (James et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposes randomized-to-canonical (sim-to-sim) image translation to map highly randomized simulated images to a canonical simulation domain, enabling more data-efficient sim-to-real grasping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Robotic grasping platform (general robotic arm with gripper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A manipulation platform used for vision-based closed-loop grasping where perception is a critical component.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic grasping / vision-based manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>(sim unspecified here; work uses randomized simulated images and canonical sim domain)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulators generating highly randomized visual data and a canonical simulated image domain to which randomized images are mapped; physics simulation for grasping included</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>visually randomized simulation with canonical domain for adaptation; physics approximate for grasping</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>visual appearance (textures, lighting, camera pose) extensively randomized, grasp-relevant dynamics approximated</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>exact contact physics and complex tactile sensing; some physics approximations retained</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real robotic arm performing vision-based grasping under real lighting and object variation</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>vision-based closed-loop grasping policies</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning for grasping plus sim-to-sim (image translation) domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>real-world grasp success rate improvements via randomized-to-canonical adaptation; exact numbers not provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>strong visual randomization of textures, lighting and camera positions in simulation; translated to canonical images before policy input</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>visual domain gap between randomized sim images and real images; imperfect modeling of contact/tactile cues</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>using a sim-to-sim adaptation (randomized->canonical) to reduce visual gap before applying policies to real robots</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>visual variability should cover target real distribution; adaptation to a canonical domain can reduce need for photorealism</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Translating randomized simulated images to a canonical simulated domain (sim-to-sim) before training policies improves data efficiency and enables more robust sim-to-real grasping without requiring photorealistic rendering.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real reinforcement learning for deformable object manipulation <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks <em>(Rating: 2)</em></li>
                <li>Sim2real transfer for reinforcement learning without dynamics randomization <em>(Rating: 2)</em></li>
                <li>Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1676",
    "paper_id": "paper-221971078",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Matas2018",
            "name_full": "Sim-to-real reinforcement learning for deformable object manipulation (Matas et al.)",
            "brief_description": "Simulation-trained DRL policies for deformable-object manipulation using domain randomization; study highlights limits of simulators for object deformability and shows that excessive visual randomization can hurt real-world performance.",
            "citation_title": "Sim-to-real reinforcement learning for deformable object manipulation.",
            "mention_or_use": "mention",
            "agent_system_name": "Kinova Mico (7DOF) with gripper",
            "agent_system_description": "A 7-DOF robotic arm used for dexterous manipulation tasks involving deformable objects; intended for closed-loop vision-based grasping and manipulation.",
            "domain": "dexterous robotic manipulation",
            "virtual_environment_name": "PyBullet (sim)",
            "virtual_environment_description": "Physics-based simulator used to render robot, objects and interactions; simulates rigid-body dynamics and simplified contact for manipulation experiments.",
            "simulation_fidelity_level": "approximate physics with randomized visual/dynamics parameters",
            "fidelity_aspects_modeled": "rigid-body dynamics, approximate contact dynamics, visual appearance (textures/colors)",
            "fidelity_aspects_simplified": "deformable-object physics (degree of deformability poorly modeled), exact contact/stiffness behavior of deformable objects, potentially simplified sensor noise and actuator dynamics",
            "real_environment_description": "Physical lab setup with real Kinova Mico arm manipulating objects of varying deformability and appearance under real lighting and sensing conditions.",
            "task_or_skill_transferred": "grasping and manipulation of deformable objects",
            "training_method": "deep reinforcement learning with domain randomization (and stochastic grasping)",
            "transfer_success_metric": "qualitative success in real-world grasps; specific numerical metrics not reported in survey",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "visual randomization (colors, textures) and stochasticization of grasping/dynamics parameters; paper notes randomization of object properties overall",
            "sim_to_real_gap_factors": "insufficient modeling of object deformability/stiffness, visual differences, over-randomization of visual appearance (too many colors) degrading transfer",
            "transfer_enabling_conditions": "domain randomization to cover variability; careful selection of randomization magnitude (avoid excessive visual variability); acknowledgment that simulator limitations in modeling deformability limit transfer",
            "fidelity_requirements_identified": "accurate modeling of deformability/contact dynamics is important; excessive visual randomization can be detrimental—implies some fidelity/realism in appearance and physical behavior is necessary",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Domain randomization enabled successful sim-to-real for deformable-object manipulation, but simulators struggled to capture real object deformability; excessive visual randomization harmed real performance, indicating an optimal level of randomization and the need for better physics fidelity for deformable objects.",
            "uuid": "e1676.0"
        },
        {
            "name_short": "Kaspar2020",
            "name_full": "Sim2real transfer for reinforcement learning without dynamics randomization (Kaspar et al.)",
            "brief_description": "Demonstrates sim-to-real transfer for peg-in-hole tasks using accurate system identification and a high-quality robot model rather than dynamics randomization.",
            "citation_title": "Sim2real transfer for reinforcement learning without dynamics randomization.",
            "mention_or_use": "mention",
            "agent_system_name": "KUKA LBR iiwa + WSG50 gripper",
            "agent_system_description": "A collaborative 7-DOF industrial manipulator equipped with a parallel gripper used for precision assembly (peg-in-hole) tasks.",
            "domain": "precision robotic manipulation / peg-in-hole assembly",
            "virtual_environment_name": "PyBullet (custom high-quality model)",
            "virtual_environment_description": "Customized simulator with a high-fidelity robot model used to closely match real robot kinematics and dynamics for one-shot/zero-shot transfer experiments.",
            "simulation_fidelity_level": "high-fidelity robot modeling (system-identified), approximate environment/contact physics",
            "fidelity_aspects_modeled": "robot kinematics and dynamics carefully system-identified; actuation/joint behavior modeled with high accuracy",
            "fidelity_aspects_simplified": "some contact dynamics, micro-variations in friction, and environmental variability likely simplified",
            "real_environment_description": "Real KUKA LBR iiwa robot performing peg-in-hole manipulation under lab conditions",
            "task_or_skill_transferred": "peg-in-hole insertion task",
            "training_method": "reinforcement learning (SAC referenced in table) with system identification used to build simulator",
            "transfer_success_metric": "task success in real robot trials (peg insertion success), specific quantitative rates not reported in survey",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "mismatch between simulated and real dynamics if system identification is imperfect; contact/friction modeling limitations",
            "transfer_enabling_conditions": "accurate system identification and high-quality robot model enabled direct/zero-shot transfer without dynamics randomization",
            "fidelity_requirements_identified": "high-fidelity modeling of robot dynamics/actuation is critical for tasks requiring precision (e.g., peg-in-hole); careful calibration is necessary",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "High-quality system identification and precise robot modeling can enable sim-to-real transfer for precise manipulation tasks without requiring dynamics randomization, indicating fidelity of actuation/dynamics modeling is a key enabler.",
            "uuid": "e1676.1"
        },
        {
            "name_short": "Traore2019",
            "name_full": "Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer (Traoré et al.)",
            "brief_description": "Demonstrates continual multi-task RL with policy distillation and sim-to-real transfer, applied to a small mobile platform for navigation tasks.",
            "citation_title": "Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer.",
            "mention_or_use": "mention",
            "agent_system_name": "Small mobile platform (unspecified model in survey)",
            "agent_system_description": "A small differential-drive mobile robot used for navigation tasks and multi-task continual learning experiments.",
            "domain": "robotic navigation / continual multi-task robotics",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "Physics simulator modeling mobile robot dynamics, sensors and navigation environments for training multiple tasks.",
            "simulation_fidelity_level": "approximate physics suitable for navigation and multi-task RL",
            "fidelity_aspects_modeled": "basic robot dynamics, environment geometry and sensor observations (simulated)",
            "fidelity_aspects_simplified": "detailed sensor noise characteristics, complex wheel-ground interactions, and environment-specific variabilities likely simplified",
            "real_environment_description": "Real mobile robot platform navigating real-world environments to validate continual learning and distilled policies",
            "task_or_skill_transferred": "mapless navigation and multi-task behaviors via policy distillation",
            "training_method": "reinforcement learning with policy distillation (multi-task distillation), continual RL pipeline",
            "transfer_success_metric": "real-world task execution success (navigation), quantitative metrics not reported in survey",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "differences in sensor behavior, environmental variability, and imperfect simulation of real-world dynamics",
            "transfer_enabling_conditions": "policy distillation into a single student network for multi-task deployment and continual learning pipeline; use of simulation to aggregate diverse experiences",
            "fidelity_requirements_identified": "no explicit numeric fidelity requirements; emphasizes combining distillation and continual learning to improve real deployment",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Policy distillation and continual RL pipelines can be transferred from simulation to real mobile platforms to deploy multi-task behaviors; sim-to-real benefits from consolidating knowledge into compact student policies.",
            "uuid": "e1676.2"
        },
        {
            "name_short": "OpenAI_Dexterous",
            "name_full": "Learning dexterous in-hand manipulation (OpenAI et al.)",
            "brief_description": "Large-scale DRL for dexterous in-hand manipulation where dynamics randomization over physical parameters was used to produce policies that transfer to real-world robotic hands.",
            "citation_title": "Learning dexterous in-hand manipulation.",
            "mention_or_use": "mention",
            "agent_system_name": "Five-fingered robotic hand (physical platform not fully specified in survey)",
            "agent_system_description": "A multi-fingered robotic hand system designed for in-hand manipulation tasks requiring complex contact-rich control.",
            "domain": "dexterous robotic manipulation / in-hand manipulation",
            "virtual_environment_name": "MuJoCo (common in these works)",
            "virtual_environment_description": "Physics engine simulating multi-body dynamics, contacts and actuation for a robotic hand interacting with objects.",
            "simulation_fidelity_level": "high-dimensional approximate physics with extensive parameter randomization",
            "fidelity_aspects_modeled": "rigid-body contact dynamics, joint actuation, object geometry, physical parameters randomized (masses, friction, damping, actuator gains)",
            "fidelity_aspects_simplified": "limitations in true contact fidelity for complex multi-contact deformable interactions, sensors/perception noise approximated",
            "real_environment_description": "Physical multi-fingered robotic hand performing in-hand manipulation tasks on real objects under lab conditions",
            "task_or_skill_transferred": "dexterous in-hand object manipulation and reorientation",
            "training_method": "deep reinforcement learning with extensive dynamics randomization",
            "transfer_success_metric": "real-world task success (manipulation), paper reports successful transfer qualitatively in survey; numeric rates not provided here",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "randomized object dimensions, object and robot link masses, surface friction coefficients, joint damping coefficients, actuator force gains and other physical parameters",
            "sim_to_real_gap_factors": "incomplete modeling of contact and contact-rich dynamics, sensor/actuator discrepancies, unmodeled wear-and-tear and environmental variability",
            "transfer_enabling_conditions": "extensive dynamics randomization across many physical parameters to produce robust policies that generalize to real-world variations",
            "fidelity_requirements_identified": "accurate contact dynamics and diversity in randomized physical parameters are critical; no quantitative thresholds provided",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Randomizing a wide set of physical parameters enables robust sim-to-real transfer for complex in-hand manipulation; diversity in dynamics during simulation is a powerful enabler but relies on covering relevant real-world variability.",
            "uuid": "e1676.3"
        },
        {
            "name_short": "Balaji_DeepRacer",
            "name_full": "DeepRacer: an educational autonomous racing platform (Balaji et al.)",
            "brief_description": "An educational sim-to-real platform for autonomous racing where users train policies in simulation (Gazebo/RoboMaker) with random colors and parallel domain randomization strategies.",
            "citation_title": "DeepRacer: an educational autonomous racing platform.",
            "mention_or_use": "mention",
            "agent_system_name": "AWS DeepRacer (4WD 1:18 scale car)",
            "agent_system_description": "A small-scale autonomous racing car platform used for education and research in RL, capable of on-board perception and control for autonomous racing tasks.",
            "domain": "autonomous racing / mobile robot navigation",
            "virtual_environment_name": "Gazebo / RoboMaker",
            "virtual_environment_description": "Simulation of vehicle dynamics, sensors and track environments integrated with ROS and AWS RoboMaker for training and evaluation.",
            "simulation_fidelity_level": "moderate-fidelity vehicle dynamics with visual randomization",
            "fidelity_aspects_modeled": "basic vehicle dynamics, camera images, track geometry, simulated sensors",
            "fidelity_aspects_simplified": "detailed tire-ground contact physics, full sensor noise characteristics, high-fidelity rendering limitations",
            "real_environment_description": "Physical 1:18 DeepRacer car running on racing tracks under real lighting and surface conditions",
            "task_or_skill_transferred": "autonomous racing control policies",
            "training_method": "reinforcement learning (PPO referenced) with visual randomization in simulation",
            "transfer_success_metric": "ability to race autonomously; survey lists it as sim-only platform in table (used for experimentation), specific real-world success metrics not provided",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "random colors and parallel domain randomization of visual aspects",
            "sim_to_real_gap_factors": "visual appearance mismatch, simplified vehicle dynamics, sensor modeling differences",
            "transfer_enabling_conditions": "visual domain randomization to improve robustness to real-world appearance variation; integration with ROS and RoboMaker for pipeline consistency",
            "fidelity_requirements_identified": "survey does not specify numeric thresholds; moderate fidelity in vehicle dynamics and consistent perception pipeline considered useful",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Educational platforms like DeepRacer use visual domain randomization to help policies generalize; consistent simulation pipeline and integration with robotics middleware aid transfer experiments.",
            "uuid": "e1676.4"
        },
        {
            "name_short": "Witman2019",
            "name_full": "Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet (Witman et al.)",
            "brief_description": "Applies sim-to-real RL to control thermal effects of a plasma jet, using a customized simulator (MuJoCo + Ogre3D) and demonstrating transfer to a Mitsubishi Melfa RV-6SL robot.",
            "citation_title": "Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet.",
            "mention_or_use": "mention",
            "agent_system_name": "Mitsubishi Melfa RV-6SL (robot arm)",
            "agent_system_description": "Industrial robot arm used to manipulate a plasma jet actuator to control thermal effects on targets; requires precise control of tool trajectory and thermal dynamics.",
            "domain": "robotic control of plasma jet / specialized manipulation",
            "virtual_environment_name": "MuJoCo + Ogre3D (custom simulation)",
            "virtual_environment_description": "Simulates robot kinematics/dynamics and visual/thermal effects for the plasma jet; primarily physics-based simulation with rendering.",
            "simulation_fidelity_level": "custom physics and rendering for task-specific thermal dynamics (moderate-to-high fidelity for thermal control)",
            "fidelity_aspects_modeled": "robot dynamics, task-specific thermal/visual effects approximated in simulation",
            "fidelity_aspects_simplified": "detailed plasma physics and exact thermal transfer in all real conditions likely approximated; sensor noise models unclear",
            "real_environment_description": "Real industrial robot controlling a plasma jet in lab settings with sensors measuring thermal effects",
            "task_or_skill_transferred": "control policies for thermal effect regulation via plasma jet manipulation",
            "training_method": "reinforcement learning (A3C sim + off-policy real referenced in table)",
            "transfer_success_metric": "control of thermal effect on target, survey references successful sim-to-real transfer qualitatively; numeric metrics not reported",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "complex plasma and thermal dynamics that are hard to model precisely; sensor and actuator differences between sim and real",
            "transfer_enabling_conditions": "task-specific simulator customization and robust policy training to handle modeling mismatch",
            "fidelity_requirements_identified": "requires simulator that captures the task-relevant thermal dynamics sufficiently well; exact quantitative requirements not provided",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Task-specific simulators with reasonable modeling of thermal and robot dynamics can support sim-to-real RL for specialized control tasks like plasma jet thermal regulation, but accurate modeling of physics relevant to the task is important.",
            "uuid": "e1676.5"
        },
        {
            "name_short": "Qin2019",
            "name_full": "Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning (Qin et al.)",
            "brief_description": "Uses curriculum learning and DRL to train six-legged robot controllers in V-Rep and transfers to real hardware using curriculum and IK priors to aid sim-to-real.",
            "citation_title": "Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning.",
            "mention_or_use": "mention",
            "agent_system_name": "Six-legged robot (platform unspecified beyond six-legged)",
            "agent_system_description": "A hexapod robot used for navigation and obstacle avoidance, requiring stable locomotion policies.",
            "domain": "legged robot navigation / locomotion",
            "virtual_environment_name": "V-Rep",
            "virtual_environment_description": "Simulator modeling legged robot kinematics, dynamics and environment for locomotion experiments; supports inverse kinematics for curriculum",
            "simulation_fidelity_level": "moderate-fidelity dynamics suitable for locomotion prototyping",
            "fidelity_aspects_modeled": "robot kinematics, contact approximations for legs, environment obstacles",
            "fidelity_aspects_simplified": "detailed foot-ground contact friction variability, sensor noise, and micro-variations in terrain",
            "real_environment_description": "Physical hexapod robot performing navigation and obstacle avoidance in real environments",
            "task_or_skill_transferred": "stable locomotion, navigation and obstacle avoidance",
            "training_method": "reinforcement learning (PPO) with curriculum learning and inverse kinematics priors",
            "transfer_success_metric": "successful transfer to real robot for navigation/obstacle avoidance; numerical metrics not provided in survey",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "mismatch in contact/friction modeling, terrain variability and sensor/actuator differences",
            "transfer_enabling_conditions": "curriculum learning to gradually increase environment complexity and use of IK priors to guide learning; incremental training aids transfer",
            "fidelity_requirements_identified": "progressively increasing environment complexity in sim helps bridge the gap; precise contact modeling remains important but not fully quantified",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Curriculum learning and incremental environment complexity facilitate sim-to-real transfer for legged locomotion by guiding policies through progressively realistic training stages.",
            "uuid": "e1676.6"
        },
        {
            "name_short": "James2019",
            "name_full": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks (James et al.)",
            "brief_description": "Proposes randomized-to-canonical (sim-to-sim) image translation to map highly randomized simulated images to a canonical simulation domain, enabling more data-efficient sim-to-real grasping.",
            "citation_title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks.",
            "mention_or_use": "mention",
            "agent_system_name": "Robotic grasping platform (general robotic arm with gripper)",
            "agent_system_description": "A manipulation platform used for vision-based closed-loop grasping where perception is a critical component.",
            "domain": "robotic grasping / vision-based manipulation",
            "virtual_environment_name": "(sim unspecified here; work uses randomized simulated images and canonical sim domain)",
            "virtual_environment_description": "Simulators generating highly randomized visual data and a canonical simulated image domain to which randomized images are mapped; physics simulation for grasping included",
            "simulation_fidelity_level": "visually randomized simulation with canonical domain for adaptation; physics approximate for grasping",
            "fidelity_aspects_modeled": "visual appearance (textures, lighting, camera pose) extensively randomized, grasp-relevant dynamics approximated",
            "fidelity_aspects_simplified": "exact contact physics and complex tactile sensing; some physics approximations retained",
            "real_environment_description": "Real robotic arm performing vision-based grasping under real lighting and object variation",
            "task_or_skill_transferred": "vision-based closed-loop grasping policies",
            "training_method": "reinforcement learning for grasping plus sim-to-sim (image translation) domain adaptation",
            "transfer_success_metric": "real-world grasp success rate improvements via randomized-to-canonical adaptation; exact numbers not provided in survey",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "strong visual randomization of textures, lighting and camera positions in simulation; translated to canonical images before policy input",
            "sim_to_real_gap_factors": "visual domain gap between randomized sim images and real images; imperfect modeling of contact/tactile cues",
            "transfer_enabling_conditions": "using a sim-to-sim adaptation (randomized-&gt;canonical) to reduce visual gap before applying policies to real robots",
            "fidelity_requirements_identified": "visual variability should cover target real distribution; adaptation to a canonical domain can reduce need for photorealism",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Translating randomized simulated images to a canonical simulated domain (sim-to-sim) before training policies improves data efficiency and enables more robust sim-to-real grasping without requiring photorealistic rendering.",
            "uuid": "e1676.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real reinforcement learning for deformable object manipulation",
            "rating": 2,
            "sanitized_title": "simtoreal_reinforcement_learning_for_deformable_object_manipulation"
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2,
            "sanitized_title": "learning_dexterous_inhand_manipulation"
        },
        {
            "paper_title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks",
            "rating": 2,
            "sanitized_title": "simtoreal_via_simtosim_dataefficient_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        },
        {
            "paper_title": "Sim2real transfer for reinforcement learning without dynamics randomization",
            "rating": 2,
            "sanitized_title": "sim2real_transfer_for_reinforcement_learning_without_dynamics_randomization"
        },
        {
            "paper_title": "Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer",
            "rating": 2,
            "sanitized_title": "continual_reinforcement_learning_deployed_in_reallife_using_policy_distillation_and_sim2real_transfer"
        }
    ],
    "cost": 0.018761749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey</p>
<p>Wenshuai Zhao 
Turku Intelligent Embedded and Robotic Systems Lab
University of Turku
Finland</p>
<p>Jorge Peña Queralta 
Turku Intelligent Embedded and Robotic Systems Lab
University of Turku
Finland</p>
<p>Tomi Westerlund 
Turku Intelligent Embedded and Robotic Systems Lab
University of Turku
Finland</p>
<p>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey
Index Terms-Deep Reinforcement LearningRoboticsSim- to-RealTransfer LearningMeta LearningDomain Random- izationKnowledge DistillationImitation Learning
Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-toreal gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.</p>
<p>I. INTRODUCTION</p>
<p>Reinforcement learning (RL) algorithms have been increasingly adopted by the robotics community over the past years to control complex robots or multi-robot systems [1], [2], or provide end-to-end policies from perception to control [3]. Inspired by the way we learn through trial-and-error processes, RL algorithms base their knowledge acquisition in the rewards that agents obtain when they act in certain manners given different experiences. This naturally requires a large number of episodes, and therefore the learning limitations in terms of time and experience variability in real-world scenarios is evident. Moreover, learning with real robots requires the consideration of potentially dangerous or unexpected behaviors in safety-critical applications [4]. Deep reinforcement learning (DRL) algorithms have been successfully deployed in various types of simulation environments, yet their success beyond simulated worlds has been limited. An exception to this is, however, robotic tasks involving object manipulation [5], [6]. In this survey, we review the most relevant works that try to answer a key research question in this direction: how to exploit One of the most common methods is domain randomization, through which different parameters of the simulator (e..g, colors, textures, dynamics) are randomized to produce more robust policies.</p>
<p>simulation-based training in real-world settings by transferring the knowledge and adapting the policies accordingly ( Fig. 1). Simulation-based training provides data at low-cost, but involves inherent mismatches with real-world settings. Bridging the gap between simulation and reality requires, first of all, methods that are able to account for mismatches in both sensing and actuation. The former aspect has been widely studied in recent years within the deep learning field, for instance with adversarial attacks on computer vision algorithms [7]. The latter risk can be minimized through more realistic simulation. In both of these cases, some of the current approaches include works that introduce perturbances in the environment [8] or focus on domain randomization [9]. Another key aspect to take into account is that an agent deployed in the real world will potentially be exposed to novel experiences that were not present in the simulations [10], as well as the potential need to adapt their policies to encompass wider sets of tasks. Some of the approaches to bridge the gap in this direction rely on meta learning [11] or continual learning [12], among others.</p>
<p>The methods described above focus on extracting knowledge from simulation-trained agents in order to deploy them in real-life scenarios. However, other approaches exist to the same end. In recent years, simulators have been progressing towards more realistic scenarios and physics engines: Airsim [13], CARLA [14], RotorS [15], [16], and others [17]. With some of these simulators, part of the aim is to be able to deploy the robotic agents directly into the real world by providing training data and experiences with minimal mismatches between real and simulated settings. Other research efforts have been directed towards increasing safety during training in real-settings. Safety is one of the main challenges towards achieving online training of complex agents in the real-world, from robot arms to self-driving cars [4]. In this direction, recent works have shown promising results towards safe DRL that is able to ensure convergence even while reducing the exploration space [3]. In this survey, we do not cover specific simulators or techniques for direct learning in real-world settings, but instead focus on describing the main methods for transferring knowledge learned in simulation towards their deployment in real robotic platforms. This is, to the best of our knowledge, the first survey that describes the different methods being utilized towards closing the simulation-to-reality gap in DRL for robotics. We also concentrate on describing the main application fields of current research efforts. We discuss recent works from a wider point of view by including related research directions in the areas of transfer learning and domain adaptation, knowledge distillation, and meta reinforcement learning. While other surveys have focused on transfer learning techniques [18] or safe reinforcement learning [4], we provide a different point of view with an emphasis on DRL policy transfer in the robotics domain. Finally, there is also a significant amount of publications deploying DRL policies on real robots. In this survey, nonetheless, we focus on those works that specifically tackle issues in sim-to-real transfer. The focus is mostly in end-to-end approaches, but we also describe relevant research where sim-to-real transfer techniques are applied to the sensing aspects of robotic operation, primarily the transfer of DL vision algorithms to real robots.</p>
<p>The rest of this paper is organized as follows. In Section II, we briefly introduce the main approaches to DRL, together with related research directions in knowledge distillation, transfer, adaptation and meta learning. Section III then delves into the different approaches being taken towards closing the simulation-to-reality gap, with Section IV focusing on the most relevant application areas. Then, we discuss open challenges and promising research directions in Section V. Finally, Section VI concludes this survey.</p>
<p>II. BACKGROUND</p>
<p>Sim-to-real is a very comprehensive concept and applied in many fields including robotics and classic machine vision tasks. Thereby quite a few methods and concepts intersect with this aim including transfer learning, robust RL, and meta learning. In this section, we briefly introduce the concepts of  deep reinforcement learning, knowledge distillation, transfer learning and domain adaption, before going into more details about sim-to-real transfer methods for DRL. The relationship between there concepts is illustrated in Fig. 2.</p>
<p>A. Deep Reinforcement Learning</p>
<p>A standard reinforcement learning (RL) task can be regarded as a sequential decision making setup which consists of an agent interacting with an environment in discrete steps. The agent takes an action a t at each timestep t, causing the environment to change its state from s t to s t+1 with a transition probability p(s t+1 |s t , a t ). This setup can be regarded as a Markov decision process (MDP) with a set of states s ∈ S, actions a ∈ A, transitions p ∈ P and rewards r ∈ R. Therefore we can define this MDP as a tuple (1).
D ≡ (S, A, P, R)(1)
The objective of reinforcement learning is to maximize the expected reward by choosing an optimal policy which will be represented via a deep neural network in DRL. Accelerated by modern computation capacity, DRL has shown significant success on various applications [1], [19], but particular in the simulated environment [20]. Therefore, how to transfer this success from simulation to reality is drawing more and more attention, which is also the motivation of this paper.</p>
<p>B. Sim-to-Real Transfer</p>
<p>Transferring DRL policies form simulation environments to reality is a necessary step towards more complex robotic systems that have DL-defined controllers. This, however, is not a problem specific to DRL algorithms, but ML in general. While most DRL algorithms provide end-to-end policies, i.e., control mechanisms that take raw sensor data as inputs and produce direct actuation commands as outputs, these two dimensions of robotics can be separated. Closing the gap between simulation and reality gap in terms of actuation requires simulators to be more accurate, and to account for variability in agent dynamics. On the sensing part, however, the problem can be considered wider, as it also involves the more general ML problem of facing situations in the real world that have not appeared in simulation [10]. In this paper, we focus mostly on end-to-end models, and overview both research directed towards system modeling and dynamics randomization, as well as research introducing randomization from the sensing point of view.</p>
<p>C. Transfer Learning and Domain Adaptation</p>
<p>Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains [18]. In this way, transfer learning can reduce the dependence of target domain data when constructing target learners.</p>
<p>Domain adaptation is a subset of transfer learning methods. It specifies the situation when we have sufficient source domain labeled data and the same single task as the target task, but without or very few target domain data. In sim-to-real robotics, researchers tend to employ a simulator to train the RL model and then deploy it in the realistic environment, where we should take advantage of the domain adaptation techniques in order to transfer the simulation based model well.</p>
<p>D. Knowledge Distillation</p>
<p>Large networks are typical in DRL with high-dimensional input data (e.g, complex visual tasks). Policy distillation is the process of extracting knowledge to train a new network that is able to maintain a similarly expert level while being significantly smaller and more efficient [21]. In these set-ups, the two networks are typically called teacher and student. The student is trained in a supervised manner with data generated by the teacher network. In [12], the authors presented Dis-CoRL, a modular, effective and scalable pipeline for continual DRL. DisCoRL has been succesfully applied to multiple tasks learned by different teachers, with their knowledge being distilled to a single student network.</p>
<p>E. Meta Reinforcement Learning</p>
<p>Meta Learning, namely learning to learn, aims to learn the adaptation ability to unseen test tasks from multiple training tasks. A good meta learning model should be trained across a variety of learning tasks and optimized for the best performance over a distribution of tasks, including potentially unseen tasks when tested. This spirit can be applied on both supervised learning and reinforcement learning, and in the latter case it is called meta reinforcement learning (MetaRL) [22].</p>
<p>The overall configuration of MetaRL is similar to an ordinary RL algorithm, except that MetaRL usually implements an LSTM policy and incorporates the last reward r t−1 and last action a t−1 into the current policy observation. In this case, the LSTM's hidden states serve as a memory for tracking characteristics of the trajectories. Therefore, MetaRL draws knowledge from past training.</p>
<p>F. Robust RL and Imitation Learning</p>
<p>Robust RL [23] was proposed quite early as a new RL paradigm that explicitly takes into account input disturbances as well as modeling errors. It considers a bad, or even adversarial model and tries to maximize the reward as a optimization problem [24], [25].</p>
<p>Imitation learning proposes to employ expert demonstration or trajectories instead of manually constructing a fixed reward function to train RL agents. The methods of imitation learning can be broadly classified into two key areas: behaviour cloning where an agent learns a mapping from observations to actions given demonstrations [26], [27] and inverse reinforcement learning where an agent attempts to estimate a reward function that describes the given demonstrations [28]. Because it aims to give a robust reward for RL agents, sometimes imitation learning can be utilized to obtain robust RL or sim-to-real transfer [29].</p>
<p>III. METHODOLOGIES FOR SIM-TO-REAL TRANSFER</p>
<p>Research in sim-to-real transfer has resulted in an increase of several orders of magnitude in the number of publications over the past few years. Multiple research directions have been followed, and we summarize in this section the most representative methods for sim-to-real transfer. Table I lists some of the most relevant and recent works in this field. The most widely used method for learning transfer is domain randomization, with other relevant examples including policy distillation, system identification, or meta-RL. The variability in terms of learning algorithms is higher, with DRL using proximal policy optimization (PPO) [45], trust region policy optimization (TRPO) [46], maximum aposteriori policy optimization (MPO) [47], asynchronous actor critic (A3C) methods [48], soft actor critic (SAC) [49], or deep deterministic policy gradient (DDPG) [50], among others.</p>
<p>A. Zero-shot Transfer</p>
<p>The most straightforward way of transferring knowledge from simulation to reality is to build a realistic simulator, or to have enough simulated experience, so that the model can be directly applied in real-world settings. This strategy is commonly referred to as zero-shot or direct transfer. System identification to build precise models of the real world and domain randomization are techniques that can be seen as one-shot transfer. We discuss both of these separately in Sections III-B and III-C.</p>
<p>B. System Identification</p>
<p>It is of note that simulators are not faithful representation of the real world. System identification [51] is exactly to build a precise mathematical model for a physical system and to make the simulator more realistic careful calibration is necessary. Nonetheless, challenges for obtaining a realistic enough simulator are still existing. For example, it is hard to build high-quality rendered image to simulate the real vision. Furthermore, many physical parameters of the same robot might vary significantly due to temperature, humidity, positioning or its wear-and-tear in time, which brings more difficulty for system identification.</p>
<p>C. Domain Randomization Methods</p>
<p>Domain randomization is the idea that [52], instead of carefully modeling all the parameters of the real world, we Traore et al. [12] Continual RL with policy distillation and sim-to-real transfer.</p>
<p>Continual learning with policy distillation.</p>
<p>PyBullet</p>
<p>Multi-task Distillation</p>
<p>PPO2</p>
<p>Small mobile platform</p>
<p>Robotic navigation</p>
<p>Kaspar et al. [31] Sim-to-real transfer for RL without Dynamics Randomization.</p>
<p>System identification and a highquality robot model.</p>
<p>PyBullet</p>
<p>SAC KUKA LBR iiwa +WSG50 gripper</p>
<p>Peg-in-Hole manipulation</p>
<p>Matas et al. [6] Sim-to-real RL for deformable object manipulation.</p>
<p>Stochastic grasping and domain radomization.</p>
<p>(sim)</p>
<p>PyBullet</p>
<p>DDPGfD 7DOF Kinova</p>
<p>Mico Arm</p>
<p>Dexterous manipulation</p>
<p>Witman et al. [32] Sim-to-real RL for thermal effects of an atmospheric pressure plasma jet. Van Baar et al. [35] Sim-to-real transfer with robustified policies for robot tasks.</p>
<p>Variation of appearance and/ or physics parameters.</p>
<p>(sim)</p>
<p>MuJoCo +Ogre 3D</p>
<p>A3C (sim) +Off-policy</p>
<p>Mitsubishi</p>
<p>Melfa RV-6SL</p>
<p>Marble Maze Manipulation</p>
<p>Bassani et al. [36] Sim2Real RL for robotic soccer competitions.</p>
<p>Domain adaptation and custom simulator for transfer.</p>
<p>VSSS-RL</p>
<p>DDPG /DQN</p>
<p>VSSS Robot</p>
<p>Robotic Navigation</p>
<p>Qin et al. [37] Sim2Real for six-legged robots with DRL and curriculum learning.</p>
<p>Curriculum learning with inverse kinematics.</p>
<p>V-Rep</p>
<p>PPO Six-legged robot Navigation and obstacle avoid.</p>
<p>Vacaro et al. [38]</p>
<p>Sim-to-real in reinforcement learning for everyone Domain randomization (light + color + textures).</p>
<p>(sim) could highly randomize the simulation in order to cover the real distribution of the real-world data despite the bias between the model and real world. Fig. 3a shows the paradigm of domain randomization. According to the components of the simulator randomized, we divide the methods of domain randomization into two kinds: visual randomization and dynamics randomization. In robotic vision tasks including object localization [53], object detection [54], pose estimation [55], and semantic segmentation [56], the training data from simulator always have different textures, lighting, and camera positions from the realistic environments. Therefore, visual domain randomization aims to provide enough simulated variability of the visual parameters at training time such that at test time the model is able to generalize to real-world data. In addition to adding randomization to the visual input, dynamics randomization could also help acquire a robust policy particularly where the controlling policy is needed. To learn dexterous in-hand manipulation policies for a physical five-fingered hand, [57] randomizes various physical parameters in the simulator, such as object dimensions, objects and robot link masses, surface friction coefficients, robot joint damping coefficients and actuator force gains. Their successful sim-to-real transfer experiments show the powerful effect of domain randomization.</p>
<p>Besides usually making the simulated data randomized to cover the real-world data distribution, [58] provides another interesting angle to apply domain randomization. They proposes to translate the randomized simulated image and realworld into the canonical sim images and demonstrate the effectiveness of this sim-to-real approach by training a visionbased closed-loop grasping RL agent in simulation.</p>
<p>D. Domain Adaptation Methods</p>
<p>Domain adaptation methods use data from source domain to improve the performance of a learned model on a different target domain where data is always less available. Since usually there are different feature spaces between the source domain and target domain, in order to better transfer the knowledge from source data, we should attempt to make these two feature space unified. This is the main spirit of domain adaptation, and can be described by the diagram in Fig. 3b.</p>
<p>The research of domain adaptation is broadly conducted recently in vision-based tasks, such as image classification and semantic segmentation [59], [60]. However, in this paper we focus on the tasks related with reinforcement learning and the ones applied to robotics. In these scenarios, the pure vision related tasks employing domain adaptation play as priors to the succeeding building reinforcement learning agents or other controlling tasks [58], [61], [29]. There is also some image-topolicy work using domain adaptation to generalize the policy learned by synthetic data or speed up the learning on realworld robots [61]. Sometimes domain adaptation is used to directly transfer the policy between agents [62].</p>
<p>Specifically, we now formalize the domain adaptation scenarios in a reinforcement learning setting [63]. Based on the definition of MDP in equation (1), we denote the source domain as D S ≡ (S S , A S , P S , R S ) and target domain as D T ≡ (S T , A T , P T , R T ), respectively. In reinforcement learning scenarios, the states S of the source and target domain can be quite different (S S = S T ) due to the perceptual-reality gap [64], while both domains share the action spaces and the transitions P (A S ≈ A T , P S ≈ P T )and their reward functions R have structural similarity (R S ≈ R T ).</p>
<p>From the literature, we summarize three common methods for domain adaptation regardless of their tasks. They are discrepancy-based, adversarial-based, and reconstructionbased methods, which can be also used crossly. Discrepancybased methods measure the feature distance between source and target domain by calculating pre-defined statistical metrics, in order to align their feature spaces [65], [66], [67]. Adversarial-based methods build a domain classifier to distinguish whether the features come from source domain or target domain. After being trained, the extractor could produce invariant feature from both source domain and target domain [68], [69], [70]. Reconstruction-based methods also aim to find the invariant or shared features between domains. However, they realize this goal by constructing one auxiliary reconstruction task and employ the shared feature to recover the original input [71]. In this way, the shared feature should be invariant and independent with the domains. These three methods provide different angles to make the features from different domains unified, and can be utilized in both vision tasks and RL-based control tasks.</p>
<p>E. Learning with Disturbances</p>
<p>Domain randomization and dynamics randomization methods focus on introducing perturbations in the simulation environments with the aim of making the agents less susceptible to the mismatches between simulation and reality [30], [38], [40]. The same conceptual idea has been extended in other works, where perturbances has been introduced to obtain more robust agents. For example, in [72], the authors consider noisy rewards. While not directly related to sim-to-real transfer, noisy rewards can better emulate real-world training of agents. Also, in some of our recent works [8], [73], we have considered environmental perturbations that affect differently different agents that are learning in parallel. This is an aspect that needs to be considered when multiple real agents are to be deployed or trained with a common policy.</p>
<p>F. Simulation Environments</p>
<p>A key aspect in sim-to-real transfer is the choice of simulation. Independently of the techniques utilized for efficiently transferring knowledge to real robots, the more realistic a simulation is the better results that can be expected. The most widely used simulators in the literature are Gazebo [74], Unity3D, and PyBullet [75] or MuJoCo [17]. Gazebo has the advantage of being widely integrated with the Robot Operating System (ROS) middleware, and therefore can be used together with part of the robotics stack that is present in real robots. PyBullet and MuJoCo, on the other hand, present wider integration with DL and RL libraries and gym environments. In  general, Gazebo suits more complex scenarios while PyBullet and MuJoCo provide faster training.</p>
<p>In those cases where system identification for one-shot transfer is the objective, researchers have often built or customized specific simulations that meet problem-specific requirements and constraints [32], [36], [41].</p>
<p>IV. APPLICATION SCENARIOS</p>
<p>Some of the most common applications for DRL in robotics are navigation and dexterous manipulation [1], [76]. Owing to the limited operational space in which most robotic arms operate, simulation environments for dexterous manipulation are relatively easier to generate than those for more complex robotic systems. For instance, the Open AI Gym [77], one of the most widely used frameworks for reinforcement learning, provides multiple environments for dexterous manipulation.</p>
<p>A. Dexterous Robotic Manipulation</p>
<p>Robotic manipulation tasks that have been possible with DRL ranging from learning peg-in-hole tasks [40] to deformable object manipulation [6], and including more dexterous manipulation with multi-fingered hands [5], or learning force control policies [78]. The latter is particularly relevant for sim-to-real: applying excessive force to real objects might cause damage, while grasping can fail with a lack of force.</p>
<p>In [6], Matas et al. utilize domain randomization for learning manipulation of deformable objects. The authors identify as one of the main drawbacks of the simulation environment the inability to properly simulate the degree of deformability of the objects, with the real robot being unable to grasp stiffer objects. Moreover, a relevant conclusion from this work is that excessive domain randomization can be detrimental. Specifically, when the number of different colors that were being used for each texture was too large, the performance of the real robot was significantly worse.</p>
<p>B. Robotic Navigation</p>
<p>While learning navigation with reinforcement learning has been a topic of increasing research interest over the past years [79], [80], the literature focusing on sim-to-real transfer methods is sparse. The first difference with respect to moreestablished research in learning manipulation is perhaps the lack of standard simulation environments. Owing to the more specific environment and sensor suites that are required for different navigation tasks, custom simulators have often been used [36], [37], or simulation worlds have been created using Unity, Unreal Engine, or Gazebo [39], [42].</p>
<p>Sim-to-real transfer for DRL policies can be applied to complex navigation tasks: from six-legged robots [37] to depth-based mapless navigation [39], including robots for soccer competitions [36]. In order to achieve a successful transfer to the real world, different methods have been applied in the literature. Of particular interest due to their potential and novelty are the following methods: curriculum learning [37], incremental environment complexity [39], and continual learning and policy distillation for multiple tasks [12].</p>
<p>C. Other Applications</p>
<p>Some other applications of DRL and sim-to-real transfer in robotics that have emerged over the past years are the control of a plasma jet [32], tactile sensing [43], or multiagent manipulation [44].</p>
<p>V. MAIN CHALLENGES AND FUTURE DIRECTIONS</p>
<p>Albeit the progress presented in the papers we reviewed, sim-to-real remains challenging based on existing methods. For domain randomization, researchers tend to study empirically examining which randomization to add, but it is hard to explain formally how and why it works, which thereby brings the difficulty of designing efficiently simulations and randomization distributions. For domain adaptation, most existing algorithms focus on homogeneous deep domain adaptation, which assumes that the feature spaces between the source and target domains are the same. However, this assumption may not be true in many applications. Thus we expect more exploration to transfer knowledge without this limitation.</p>
<p>Two of the most promising research directions are: (i) integration of different existing methods for more efficient transfer (e.g., domain randomization and domain adaptation); and (ii) incremental complexity learning, continual learning, and reward shaping for complex or multi-step tasks.</p>
<p>VI. CONCLUSION</p>
<p>Reinforcement learning algorithms often rely on simulated data to meet their need for vast amounts of labeled experiences. The mismatch between the simulation environments and real-world scenarios, however, requires further attention to be put to methods for sim-to-real transfer of the knowledge acquired in simulation. This is, to the best of our knowledge, the first survey that focuses on the different approaches being taken for sim-to-real transfer in DRL for robotics.</p>
<p>Domain randomization has been identified as the most widely adopted method for increasing the realism of simulation and better prepare for the real world. However, we have discussed alternative research directions showing promising results. For instance, policy distillation is enabling multi-task learning and more efficient and smaller networks, while metalearning methods allow for wider variability of tasks.</p>
<p>Multiple challenges remain in this field. While practical implementations show the efficiency of the different methods, wider theoretical and empirical studies are required to better understand the effect of these techniques in the learning process. Moreover, generalization of existing results with a more comprehensive analysis is also lacking in the literature.</p>
<p>Fig. 1 :
1Conceptual view of a simulation-to-reality transfer process.</p>
<p>Fig. 2 :
2Illustration of the different methods related to sim-to-real transfer in deep reinforcement learning and their relationships.</p>
<p>behind the domain adaptation paradigm.</p>
<p>Fig. 3 :
3Illustration of two of the most widely used methods for sim-to-real transfer in DRL. Domain randomization and domain adaptation are often applied as separate techniques, but they can also be applied together.</p>
<p>TABLE I :
IClassification of the most relevant publications in Sim2Real Transfer.Description </p>
<p>Sim-to-real transfer </p>
<p>Multi-agent </p>
<p>Simulator </p>
<p>Knowledge </p>
<p>Learning </p>
<p>Real </p>
<p>Application </p>
<p>and learning details </p>
<p>learning 
/ Engine </p>
<p>Transfer </p>
<p>Algorithm </p>
<p>Robot/Platform </p>
<p>Balaji et al. [30] </p>
<p>DeepRacer: </p>
<p>an </p>
<p>educational </p>
<p>autonomous racing platform. </p>
<p>Random colors and parallel do-
main randomization </p>
<p>(sim only) 
Distr. rollout </p>
<p>Gazebo 
RoboMaker </p>
<p>PPO </p>
<p>DeepRacer </p>
<p>4WD 1:18 Car </p>
<p>Autonomous 
racing </p>
<p>ACKNOWLEDGEMENTSThis work was supported by the Academy of Finland's AutoSOS project with grant number 328755.
Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath, arXiv:1708.05866A brief survey of deep reinforcement learning. Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil An- thony Bharath. A brief survey of deep reinforcement learning. arXiv:1708.05866, 2017.</p>
<p>Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications. Thanh Thi Nguyen, Ngoc Duy Nguyen, Saeid Nahavandi, IEEE transactions on cybernetics. Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications. IEEE transactions on cybernetics, 2020.</p>
<p>End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. Richard Cheng, Gábor Orosz, M Richard, Joel W Murray, Burdick, AAAI Artificial Intelligence. 33Richard Cheng, Gábor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In AAAI Artificial Intelligence, volume 33, 2019.</p>
<p>A comprehensive survey on safe reinforcement learning. Javier Garcıa, Fernando Fernández, Journal of Machine Learning Research. 161Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1), 2015.</p>
<p>Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, Sergey Levine, arXiv:1709.10087Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv:1709.10087, 2017.</p>
<p>Sim-to-real reinforcement learning for deformable object manipulation. Jan Matas, Stephen James, Andrew J Davison, arXiv:1806.07851Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforce- ment learning for deformable object manipulation. arXiv:1806.07851, 2018.</p>
<p>Threat of adversarial attacks on deep learning in computer vision: A survey. Naveed Akhtar, Ajmal Mian, IEEE Access. 6Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. IEEE Access, 6, 2018.</p>
<p>Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning. Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing, Tomi Westerlund, In 5th ICRAEWenshuai Zhao, Jorge Peña Queralta, Li Qingqing, and Tomi Wester- lund. Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning. In 5th ICRAE, 2020.</p>
<p>Bayesian domain randomization for sim-to-real transfer. Fabio Muratore, Christian Eilers, Michael Gienger, Jan Peters, arXiv:2003.02471Fabio Muratore, Christian Eilers, Michael Gienger, and Jan Pe- ters. Bayesian domain randomization for sim-to-real transfer. arXiv:2003.02471, 2020.</p>
<p>Blind spot detection for safe sim-to-real transfer. Ramya Ramakrishnan, Ece Kamar, Debadeepta Dey, Eric Horvitz, Julie Shah, Journal of Artificial Intelligence Research. 67Ramya Ramakrishnan, Ece Kamar, Debadeepta Dey, Eric Horvitz, and Julie Shah. Blind spot detection for safe sim-to-real transfer. Journal of Artificial Intelligence Research, 67, 2020.</p>
<p>Meta reinforcement learning for sim-to-real domain adaptation. Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, Ville Kyrki, arXiv:1909.12906Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. Meta reinforcement learning for sim-to-real domain adaptation. arXiv:1909.12906, 2019.</p>
<p>Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer. René Traoré, Hugo Caselles-Dupré, Timothée Lesort, Te Sun, Natalia Díaz-Rodríguez, David Filliat, arXiv:1906.04452René Traoré, Hugo Caselles-Dupré, Timothée Lesort, Te Sun, Natalia Díaz-Rodríguez, and David Filliat. Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer. arXiv:1906.04452, 2019.</p>
<p>Airsim: High-fidelity visual and physical simulation for autonomous vehicles. Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor, Field and service robotics. Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and service robotics, 2018.</p>
<p>Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Carla, arXiv:1711.03938An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. arXiv:1711.03938, 2017.</p>
<p>Rotors-a modular gazebo mav simulator framework. Fadri Furrer, Michael Burri, Markus Achtelik, Roland Siegwart, Robot Operating System (ROS). Fadri Furrer, Michael Burri, Markus Achtelik, and Roland Siegwart. Ro- tors-a modular gazebo mav simulator framework. In Robot Operating System (ROS). 2016.</p>
<p>Distributed progressive formation control for multi-agent systems: 2d and 3d deployment of uavs in ros/gazebo with rotors. Cassandra Mccord, Jorge Peña Queralta, Tuan Nguyen Gia, Tomi Westerlund, ECMR. Cassandra McCord, Jorge Peña Queralta, Tuan Nguyen Gia, and Tomi Westerlund. Distributed progressive formation control for multi-agent systems: 2d and 3d deployment of uavs in ros/gazebo with rotors. In ECMR, 2019.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, IROS. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IROS, 2012.</p>
<p>A comprehensive survey on transfer learning. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He, Proceedings of the IEEE. the IEEEFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 2020.</p>
<p>Collaborative multi-robot systems for search and rescue: Coordination and perception. Jorge Peña Queralta, Jussi Taipalmaa, Bilge Can Pullinen, Victor Kathan, Tuan Sarker, Hannu Nguyen Gia, Moncef Tenhunen, Jenni Gabbouj, Tomi Raitoharju, Westerlund, arXiv:2008.12610arXiv preprintJorge Peña Queralta, Jussi Taipalmaa, Bilge Can Pullinen, Victor Kathan Sarker, Tuan Nguyen Gia, Hannu Tenhunen, Moncef Gabbouj, Jenni Raitoharju, and Tomi Westerlund. Collaborative multi-robot systems for search and rescue: Coordination and perception. arXiv preprint arXiv:2008.12610, 2020.</p>
<p>. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, D Przemysław, Christy Ebiak, David Dennison, Quirin Farhi, Shariq Fischer, Chris Hashme, Hesse, arXiv:1912.06680et al. Dota 2 with large scale deep reinforcement learningChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv:1912.06680, 2019.</p>
<p>Sergio Gomez Andrei A Rusu, Caglar Colmenarejo, Guillaume Gulcehre, James Desjardins, Razvan Kirkpatrick, Pascanu, arXiv:1511.06295Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guil- laume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv:1511.06295, 2015.</p>
<p>X Jane, Zeb Wang, Dhruva Kurth-Nelson, Hubert Tirumala, Joel Z Soyer, Remi Leibo, Munos, arXiv:1611.05763Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv:1611.05763, 2016.</p>
<p>Robust reinforcement learning. Jun Morimoto, Kenji Doya, Neural computation. 172Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation, 17(2), 2005.</p>
<p>Action robust reinforcement learning and applications in continuous control. Chen Tessler, Yonathan Efroni, Shie Mannor, arXiv:1901.09184Chen Tessler, Yonathan Efroni, and Shie Mannor. Action ro- bust reinforcement learning and applications in continuous control. arXiv:1901.09184, 2019.</p>
<p>Robust reinforcement learning for continuous control with model misspecification. J Daniel, Nir Mankowitz, Rae Levine, Yuanyuan Jeong, Jackie Shi, Abbas Kay, Jost Tobias Abdolmaleki, Timothy Springenberg, Todd Mann, Martin Hester, Riedmiller, arXiv:1906.07516Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement learning for continuous control with model misspecification. arXiv:1906.07516, 2019.</p>
<p>Alvinn: An autonomous land vehicle in a neural network. A Dean, Pomerleau, Advances in neural information processing systems. Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural information processing systems, 1989.</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, AISTATS. Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011.</p>
<p>Algorithms for inverse reinforcement learning. Y Andrew, Stuart J Ng, Russell, Icml. 1Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforce- ment learning. In Icml, volume 1, 2000.</p>
<p>Sim-toreal transfer of accurate grasping with eye-in-hand observations and continuous control. Mengyuan Yan, Iuri Frosio, Stephen Tyree, Jan Kautz, arXiv:1712.03303Mengyuan Yan, Iuri Frosio, Stephen Tyree, and Jan Kautz. Sim-to- real transfer of accurate grasping with eye-in-hand observations and continuous control. arXiv:1712.03303, 2017.</p>
<p>Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac, Vineet Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend, arXiv:1911.01562Educational autonomous racing platform for experimentation with sim2real reinforcement learning. Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac, Vineet Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend, et al. Deepracer: Educational autonomous racing platform for experimentation with sim2real reinforcement learning. arXiv:1911.01562, 2019.</p>
<p>Manuel Kaspar, Juan David Munoz Osorio, Jürgen Bock, arXiv:2002.11635Sim2real transfer for reinforcement learning without dynamics randomization. Manuel Kaspar, Juan David Munoz Osorio, and Jürgen Bock. Sim2real transfer for reinforcement learning without dynamics randomization. arXiv:2002.11635, 2020.</p>
<p>Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet. Matthew Witman, Dogan Gidon, B David, Berend Graves, Ali Smit, Mesbah, Plasma Sources Science and Technology. 289Matthew Witman, Dogan Gidon, David B Graves, Berend Smit, and Ali Mesbah. Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet. Plasma Sources Science and Technology, 28(9), 2019.</p>
<p>Modelling generalized forces with reinforcement learning for simto-real transfer. Rae Jeong, Jackie Kay, Francesco Romano, Thomas Lampe, Tom Rothorl, Abbas Abdolmaleki, Tom Erez, Yuval Tassa, Francesco Nori, arXiv:1910.09471Rae Jeong, Jackie Kay, Francesco Romano, Thomas Lampe, Tom Rothorl, Abbas Abdolmaleki, Tom Erez, Yuval Tassa, and Francesco Nori. Modelling generalized forces with reinforcement learning for sim- to-real transfer. arXiv:1910.09471, 2019.</p>
<p>Flexible robotic grasping with sim-to-real transfer based reinforcement learning. Michel Breyer, Fadri Furrer, Tonci Novkovic, Roland Siegwart, Juan Nieto, ArXiv e-printsMichel Breyer, Fadri Furrer, Tonci Novkovic, Roland Siegwart, and Juan Nieto. Flexible robotic grasping with sim-to-real transfer based reinforcement learning. ArXiv e-prints, 2018.</p>
<p>Simulation to real transfer learning with robustified policies for robot tasks. R Van Baar, Corcodel, Sullivan, Jha, D Romeres, Nikovski, J van Baar, R Corcodel, A Sullivan, D Jha, D Romeres, and D Nikovski. Simulation to real transfer learning with robustified policies for robot tasks. 2018.</p>
<p>Learning to play soccer by reinforcement and applying sim-to-real to compete in the real world. F Hansenclever, Renie A Bassani, Jose Delgado, Nilton De O Lima Junior, R Heitor, Medeiros, H M Pedro, Alain Braga, Tapp, arXiv:2003.11102Hansenclever F Bassani, Renie A Delgado, Jose Nilton de O Lima Junior, Heitor R Medeiros, Pedro HM Braga, and Alain Tapp. Learning to play soccer by reinforcement and applying sim-to-real to compete in the real world. arXiv:2003.11102, 2020.</p>
<p>Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning. Yue Bangyu Qin, Yi Gao, Bai, ICRAE. Bangyu Qin, Yue Gao, and Yi Bai. Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning. In ICRAE, 2019.</p>
<p>Sim-to-real in reinforcement learning for everyone. Juliano Vacaro, Guilherme Marques, Bruna Oliveira, Gabriel Paz, Thomas Paula, Wagston Staehler, David Murphy, LARS-SBR-WREJuliano Vacaro, Guilherme Marques, Bruna Oliveira, Gabriel Paz, Thomas Paula, Wagston Staehler, and David Murphy. Sim-to-real in reinforcement learning for everyone. In LARS-SBR-WRE, 2019.</p>
<p>Sim-to-real transfer with incremental environment complexity for reinforcement learning of depth-based robot navigation. Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, Julien Marzat, arXiv:2004.14684Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, and Julien Marzat. Sim-to-real transfer with incremental environment com- plexity for reinforcement learning of depth-based robot navigation. arXiv:2004.14684, 2020.</p>
<p>Reinforcement learning with cartesian commands and sim to real transfer for peg in hole tasks. Manuel Kaspar, Jürgen Bock, Manuel Kaspar and Jürgen Bock. Reinforcement learning with cartesian commands and sim to real transfer for peg in hole tasks.</p>
<p>Andrew Hundt, Benjamin Killeen, Heeyeon Kwon, Chris Paxton, Gregory D Hager, arXiv:1909.11730Efficient reinforcement learning for multi-step visual tasks via reward shaping. Andrew Hundt, Benjamin Killeen, Heeyeon Kwon, Chris Paxton, and Gregory D Hager. " good robot!": Efficient reinforcement learning for multi-step visual tasks via reward shaping. arXiv:1909.11730, 2019.</p>
<p>Sim-to-real transfer of robotic gripper pose estimation-using deep reinforcement learning, generative adversarial networks, and visual servoing. Ole-Magnus Pedersen, NTNUMaster's thesisOle-Magnus Pedersen. Sim-to-real transfer of robotic gripper pose estimation-using deep reinforcement learning, generative adversarial networks, and visual servoing. Master's thesis, NTNU, 2019.</p>
<p>Sim-to-real transfer for optical tactile sensing. Zihan Ding, F Nathan, Edward Lepora, Johns, arXiv:2004.00136Zihan Ding, Nathan F Lepora, and Edward Johns. Sim-to-real transfer for optical tactile sensing. arXiv:2004.00136, 2020.</p>
<p>Multi-agent manipulation via locomotion using hierarchical sim2real. Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, Vikash Kumar, arXiv:1908.05224Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. Multi-agent manipulation via locomotion using hierarchical sim2real. arXiv:1908.05224, 2019.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017.</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, ICML. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015.</p>
<p>Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, arXiv:1806.06920Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv:1806.06920, 2018.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, ICML. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, 2016.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, arXiv:1801.01290Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv:1801.01290, 2018.</p>
<p>P Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv:1509.02971, 2015.</p>
<p>System identification and control using genetic algorithms. Kristinn Kristinsson, Guy Albert Dumont, IEEE Transactions on Systems, Man, and Cybernetics. 225Kristinn Kristinsson and Guy Albert Dumont. System identification and control using genetic algorithms. IEEE Transactions on Systems, Man, and Cybernetics, 22(5), 1992.</p>
<p>Real-World Robotic Perception and Control Using Synthetic Data. P Joshua, Tobin, UC BerkeleyPhD thesisJoshua P Tobin. Real-World Robotic Perception and Control Using Synthetic Data. PhD thesis, UC Berkeley, 2019.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, IROS. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IROS, 2017.</p>
<p>Training deep networks with synthetic data: Bridging the reality gap by domain randomization. Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, Stan Birchfield, CVPR Workshops. Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic data: Bridging the reality gap by domain randomization. In CVPR Workshops, 2018.</p>
<p>Implicit 3d orientation learning for 6d object detection from rgb images. Martin Sundermeyer, Maximilian Zoltan-Csaba Marton, Manuel Durner, Rudolph Brucker, Triebel, ECCV. Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection from rgb images. In ECCV, 2018.</p>
<p>Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, Boqing Gong, ICCV. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni- Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real generalization without ac- cessing target domain data. In ICCV, 2019.</p>
<p>Learning dexterous in-hand manipulation. Bowen Openai: Marcin Andrychowicz, Maciek Baker, Rafal Chociej, Bob Jozefowicz, Jakub Mcgrew, Arthur Pachocki, Matthias Petron, Glenn Plappert, Alex Powell, Ray, The International Journal of Robotics Research. 3912020OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1), 2020.</p>
<p>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, Konstantinos Bousmalis, CVPR. Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalash- nikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. In CVPR, 2019.</p>
<p>Deep visual domain adaptation: A survey. Mei Wang, Weihong Deng, Neurocomputing. 312Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312, 2018.</p>
<p>Cycada: Cycle-consistent adversarial domain adaptation. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, Trevor Darrell, ICML. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, ICRA. Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. In ICRA, 2018.</p>
<p>Learning invariant feature spaces to transfer skills with reinforcement learning. Abhishek Gupta, Coline Devin, Yuxuan Liu, Pieter Abbeel, Sergey Levine, arXiv:1703.02949Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant feature spaces to transfer skills with rein- forcement learning. arXiv:1703.02949, 2017.</p>
<p>Darla: Improving zero-shot transfer in reinforcement learning. Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, P Christopher, Alexander Burgess, Matthew Pritzel, Charles Botvinick, Alexander Blundell, Lerchner, arXiv:1707.08475Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforce- ment learning. arXiv:1707.08475, 2017.</p>
<p>Sim-to-real robot learning from pixels with progressive nets. Matej Andrei A Rusu, Thomas Večerík, Nicolas Rothörl, Razvan Heess, Raia Pascanu, Hadsell, Conference on Robot Learning. Andrei A Rusu, Matej Večerík, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. In Conference on Robot Learning, 2017.</p>
<p>Deep domain confusion: Maximizing for domain invariance. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell, arXiv:1412.3474Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv:1412.3474, 2014.</p>
<p>Learning transferable features with deep adaptation networks. Mingsheng Long, Yue Cao, Jianmin Wang, Michael Jordan, ICML. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learn- ing transferable features with deep adaptation networks. In ICML, 2015.</p>
<p>Return of frustratingly easy domain adaptation. Baochen Sun, Jiashi Feng, Kate Saenko, arXiv:1511.05547Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. arXiv:1511.05547, 2015.</p>
<p>Domain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor Lempitsky, The Journal of Machine Learning Research. 171Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1), 2016.</p>
<p>Simultaneous deep transfer across domains and tasks. Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko, ICCV. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simulta- neous deep transfer across domains and tasks. In ICCV, 2015.</p>
<p>Unsupervised pixel-level domain adaptation with generative adversarial networks. Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan, CVPR. Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR, 2017.</p>
<p>Domain separation networks. Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan, Advances in neural information processing systems. Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain separation networks. In Advances in neural information processing systems, 2016.</p>
<p>Reinforcement learning with perturbed rewards. Jingkang Wang, Yang Liu, Bo Li, AAAI. Jingkang Wang, Yang Liu, and Bo Li. Reinforcement learning with perturbed rewards. In AAAI, 2020.</p>
<p>Ubiquitous distributed deep reinforcement learning at the edge: Analyzing byzantine agents in discrete action spaces. Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing, Tomi Westerlund, The 11th International Conference on Emerging Ubiquitous Systems and Pervasive Networks. 2020EUSPN 2020Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing, and Tomi West- erlund. Ubiquitous distributed deep reinforcement learning at the edge: Analyzing byzantine agents in discrete action spaces. In The 11th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2020), 2020.</p>
<p>Design and use paradigms for gazebo, an open-source multi-robot simulator. Nathan Koenig, Andrew Howard, IROS. 3Nathan Koenig and Andrew Howard. Design and use paradigms for gazebo, an open-source multi-robot simulator. In IROS, volume 3, 2004.</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. Erwin Coumans, Yunfei Bai, Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 2016.</p>
<p>Reinforcement learning in robotics: A survey. J Kober, The International Journal of Robotics Research. 3211J. Kober et al. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11), 2013.</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, arXiv:1606.01540Jie Tang, and Wojciech Zaremba. Openai gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv:1606.01540, 2016.</p>
<p>Learning force control policies for compliant manipulation. M Kalakrishnan, IROS. M. Kalakrishnan et al. Learning force control policies for compliant manipulation. In IROS, 2011.</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, J Joseph, Abhinav Lim, Li Gupta, Ali Fei-Fei, Farhadi, ICRA. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017.</p>
<p>A survey on visual navigation for artificial agents with deep reinforcement learning. Fanyu Zeng, Chen Wang, Shuzhi Sam Ge, IEEE Access. 8Fanyu Zeng, Chen Wang, and Shuzhi Sam Ge. A survey on visual navigation for artificial agents with deep reinforcement learning. IEEE Access, 8, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8325 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8325</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8325</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276116990</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.03214v2.pdf" target="_blank">iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language Models (VLMs) are known to struggle with spatial reasoning and visual alignment. To help overcome these limitations, we introduce iVISPAR, an interactive multimodal benchmark designed to evaluate the spatial reasoning capabilities of VLMs acting as agents. iVISPAR is based on a variant of the sliding tile puzzle—a classic problem that demands logical planning, spatial awareness, and multi-step reasoning. The benchmark supports visual 3D, 2D, and text-based input modalities, enabling comprehensive assessments of VLMs’ planning and reasoning skills. We evaluate a broad suite of state-of-the-art open-source and closed-source VLMs, comparing their performance while also providing optimal path solutions and a human baseline to assess the task’s complexity and feasibility for humans. Results indicate that while VLMs perform better on 2D tasks compared to 3D or text-based settings, they struggle with complex spatial configurations and consistently fall short of human performance, illustrating the persistent challenge of visual alignment. This underscores critical gaps in current VLM capabilities, highlighting their limitations in achieving human-level cognition. Project website: https://microcosm.ai/ivispar .</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8325.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8325.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sonnet-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sonnet-3.5 (Claude Team)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source vision-language model (VLM) evaluated as an agent in the interactive Sliding Geom Puzzle; supports multi-image inputs and longish context windows and was run via official API in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sonnet-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal VLM from the Claude team, supports multi-image inputs and a minimum context length ≥800 tokens; used via official API in the experiments (no architecture/training details provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sliding Geom Puzzle (SGP)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based sequential sliding-tile variant on a 4×4 board requiring multi-step planning and spatial localization (objects are geometric shapes with colors), evaluated in 3 modalities: 3D rendered view, 2D top-down view, and text representation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Interactive action-perception loop: at each step the model receives current and goal states (in one modality) plus up to two past steps, and outputs natural-language move commands of the form "move <color> <shape> <direction>". Uniform minimal instruction template for all models; models run in context-aware zero-shot mode, CoT prompting allowed; episodes capped at 20 actions; auxiliary board-state inference task also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-Thought (CoT) style reasoning in zero-shot mode with a constrained context window of two past steps; models generate explicit action sequences via natural language commands. Evaluation used A* to compute optimal solutions for comparison; no special prompt-engineering per model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 2 (per paper): Completed episodes - Avg 54.56%, 3D 28.67%, 2D 89.67%, Text 45.33%. Mean optimal-path deviation - Avg 3.05, 3D 4.10, 2D 1.44, Text 3.60. Board-state inference accuracy - Avg 60.00%, 3D 35.38%, 2D 84.62%. (900 episodes per model across modalities; per-episode action cap = 20.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Produces goal-directed action sequences and often follows near-optimal paths in 2D; Chain-of-Thought outputs (examples in appendix) show explicit reasoning about object positions and blocking relationships; high 2D board-state inference accuracy indicates capacity to identify objects and coordinates in 2D, but much lower 3D localization (drop from 84.6% to 35.4%) indicates limits in 3D visual alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms other evaluated models (highest success rate among tested models, particularly strong in 2D). Compared against an optimal A* agent (upper bound), a random agent (lower bound), and a 30-person human baseline (humans solved tasks perfectly with near-optimal paths). Sonnet-3.5 > Gemini-2.0-flash > GPT-4o > open-source models in aggregate; closed-source models > open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Substantially worse in 3D modality due to coordinate prediction errors and occlusions; more ineffective moves and higher mean deviation from optimal in 3D. Makes out-of-bounds and occupied-destination attempts especially when 3D localization fails. Context-window restriction (past two steps) and per-step prompt repetition limit episodic memory. No model-specific training or fine-tuning on the puzzle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8325.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8325.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.0-flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.0-flash (Gemini Team)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal VLM evaluated in the iVISPAR Sliding Geom Puzzle benchmark via API, using the same interactive language-command interface and CoT-style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0-flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal vision-language model (Gemini family); supports multi-image inputs and required context length; used via official API for the experiments (no architecture/training details provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sliding Geom Puzzle (SGP)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based sequential sliding-tile variant (4×4) requiring multi-step spatial planning and object localization across 3D/2D/text modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same interactive setup as other VLMs: receives start and goal states in chosen modality, constrained context of two previous steps, outputs natural-language move commands ('move <color> <shape> <direction>'). Zero-shot with CoT; standardized minimal instruction prompt used for every model.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-Thought zero-shot reasoning with a two-step context window; action proposals parsed by the simulator and applied if legal. Auxiliary board-state inference evaluated via text prediction and set-matching (Hungarian algorithm).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: Completed episodes - Avg 27.11%, 3D 12.67%, 2D 47.33%, Text 21.33%. Mean optimal-path deviation - Avg 4.87, 3D 5.25, 2D 4.09, Text 5.26. Board-state inference accuracy - Avg 54.08%, 3D 28.67%, 2D 79.49%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>CoT examples show spatial planning steps and blocking analysis. Produces some effective, goal-directed moves (better than random baseline), particularly in 2D; board-state inference shows drop from 2D to 3D consistent with localization difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs below Sonnet-3.5 and above most open-source models; closed-source cluster generally outperforms open-source ones. Humans and the A* agent remain much stronger; random baseline much weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High coordinate errors and missed detections in 3D; frequent ineffective moves and collisions in 3D; text modality shows out-of-bounds mistakes indicating difficulty representing board dimensions when given only text. Suffers on longer/purer-planning (long shortest-path lengths, many geoms).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8325.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8325.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal LLM (GPT-4 family) used via API to play the Sliding Geom Puzzle in iVISPAR, producing natural-language move commands and CoT reasoning in a constrained zero-shot interactive loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal large model from OpenAI (GPT-4 family); supports vision inputs and multi-turn text-based interactions; used via official API in experiments (no internal architecture details provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sliding Geom Puzzle (SGP)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>4×4 sequential sliding-tile puzzle variant requiring spatial planning and object localization in 3D/2D/text modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Interactive stepwise action-perception loop: current and goal states provided (3D/2D image or text); model returns actions in prescribed format; two-step context window; zero-shot CoT allowed; episodes capped at 20 steps; auxiliary board-state inference evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-Thought reasoning examples provided in paper; models compared against optimal A* trajectories and random policy. Models interpret visual scenes and plan sequential moves; no fine-tuning on the task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: Completed episodes - Avg 17.56%, 3D 9.33%, 2D 37.33%, Text 36.00%. Mean optimal-path deviation - Avg 5.30, 3D 5.45, 2D 4.15, Text 6.30. Board-state inference accuracy - Avg 41.67%, 3D 19.49%, 2D 63.85%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>CoT outputs show explicit reasoning about blocking objects and multi-step plans. In many episodes GPT-4o produces goal-directed behavior (sometimes better than random) but is substantially below human and optimal agent performance; board inference shows large 2D→3D drop indicating limited 3D alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Per paper, GPT-4o is behind Sonnet-3.5 and Gemini-2.0-flash in success rates but outperforms the open-source models on average. Humans perform near-perfectly; optimal agent solves tasks by construction; random baseline is much worse.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Significant coordinate and localization errors in 3D vision leading to out-of-bounds and occupied-destination move attempts; higher regret (deviation from optimal) especially in text modality and in complex configurations. Also experienced a small number of format-parsing issues (1 formatting error listed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8325.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8325.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL2.5-78B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL 2.5 (78B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-language model (reported as 78B) evaluated on the SGP; performed near the random baseline on visual modalities and better on text-only tasks compared to visual ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL2.5-78B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal VLM reported in the paper as InternVL2.5 with ~78B parameters; evaluated from publicly available checkpoints with sampling settings (temp=1.0, top-p=0.95, top-k=50).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>78B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sliding Geom Puzzle (SGP)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>4×4 sliding-tile style puzzle (grid-based) requiring sequential planning and object localization in 3D/2D/text modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same interactive API loop, minimal shared prompt template, two-step context window, CoT allowed; evaluated across 900 episodes per model (300 per modality). Auxiliary board-state inference measured via setmatching.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot CoT reasoning and generation of natural-language move commands. No specialized symbolic planner; behavior emerges from pre-trained model capabilities. A* used for optimal comparison and Hungarian algorithm for auxiliary matching.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: Completed episodes - Avg 10.16%, 3D 1.67%, 2D 9.42%, Text 19.33%. Mean optimal-path deviation - Avg 5.98, 3D 6.39, 2D 5.86, Text 5.69. Board-state inference accuracy - Avg 34.95%, 3D 16.51%, 2D 53.38%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows very limited success in 3D/2D vision; slightly better in text modality, suggesting reliance on textual pattern reasoning rather than grounded visual spatial reasoning. Auxiliary task shows low correct-object counts and substantial coordinate errors in 3D.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs near-random on visual tasks and worse than closed-source models; stronger on text than vision, opposite of closed-source trend. Humans and A* vastly outperform InternVL2.5-78B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High coordinate error rates (see Table 5) and many format errors reported (format errors count high for some open-source runs). Very low completed-episode rates in 3D, frequent ineffective and invalid actions; often fails to parse or produce valid geom triplets in auxiliary task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8325.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8325.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-OneVision-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-OneVision-72B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal VLM (reported ~72B) evaluated on SGP; exhibited near-random performance on visual inputs with somewhat better behavior in text-only modality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-OneVision-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source vision-language model (LLaVA OneVision family) reported in paper as a ~72B-parameter model; used from public checkpoints with standardized sampling hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sliding Geom Puzzle (SGP)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>4×4 grid-based sliding tile puzzle variant requiring planning and spatial localization across 3D/2D/text modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Interactive loop with natural-language move outputs, two-step context window, minimal universal prompt, CoT allowed; episodes capped at 20 moves; auxiliary board-state inference evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot Chain-of-Thought style reasoning; no specialized symbolic planner. Evaluated against A* optimal and random baselines; Hungarian matching for auxiliary task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: Completed episodes - Avg 8.22%, 3D 0.67%, 2D 1.33%, Text 22.67%. Mean optimal-path deviation - Avg 6.35, 3D 6.75, 2D 6.81, Text 5.50. Board-state inference accuracy - Avg 26.36%, 3D 14.72%, 2D 38.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Produces some Chain-of-Thought outputs but overall low correct-object inference and high coordinate errors in 3D indicate limited 3D spatial grounding; better relative performance in text suggests reliance on linguistic heuristics rather than visual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Per paper, among the lowest-performing models on visual input; closed-source models substantially outperform LLaVA-OneVision-72B. Humans and optimal algorithm remain much stronger.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very low visual success rates (especially in 3D), large coordinate errors and hallucinations in some cases; format errors and parsing issues present; struggles with object occlusions and precise localization in 3D scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8325.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8325.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2-72B (Qwen Family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal VLM (reported ~72B) evaluated on SGP; performed near-random on visual tasks with relatively better text-only performance and moderate board-state inference errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source vision-language model in the Qwen family reported in the paper as Qwen2-72B; used from public checkpoints with sampling settings (temp=1.0, top-p=0.95, top-k=50).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sliding Geom Puzzle (SGP)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>4×4 sequential sliding-tile variant requiring planning and object localization under occlusion in 3D, fully visible in 2D, or described textually.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Interactive natural-language action loop, two-step context window, zero-shot with CoT permitted; same minimal instruction template across all models; auxiliary board-inference evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>CoT-style reasoning in zero-shot; models attempt to map visual input to textual board states and propose actions; A* provides optimal baseline and Hungarian algorithm used for auxiliary matching.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: Completed episodes - Avg 5.89%, 3D 0.67%, 2D 1.67%, Text 15.33%. Mean optimal-path deviation - Avg 6.37, 3D 6.66, 2D 6.54, Text 5.90. Board-state inference accuracy - Avg 41.54%, 3D 18.77%, 2D 64.31%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows occasional goal-directed moves but overall struggles on visual modalities; auxiliary inference shows moderate 2D performance but large drop in 3D coordinate accuracy, indicating weak 3D spatial grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>One of the weakest performers among evaluated models on visual tasks; closed-source models achieve much higher success rates. Performs better on text relative to its visual scores, similar to other open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High coordinate errors and frequent ineffective moves in 3D; often fails to reach goal within 20-step cap; shows format and parsing errors in auxiliary task outputs for some runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. <em>(Rating: 2)</em></li>
                <li>Exploring and benchmarking the planning capabilities of large language models. <em>(Rating: 2)</em></li>
                <li>Evaluating spatial understanding of large language models. <em>(Rating: 2)</em></li>
                <li>What's "up" with vision-language models? investigating their struggle with spatial reasoning. <em>(Rating: 2)</em></li>
                <li>SpaRC and SpaRP: Spatial reasoning characterization and path generation for understanding spatial reasoning capability of large language models. <em>(Rating: 2)</em></li>
                <li>Sliding puzzles gym: A scalable benchmark for state representation in visual reinforcement learning. <em>(Rating: 1)</em></li>
                <li>PUZ-ZLES: A benchmark for neural algorithmic reasoning. <em>(Rating: 1)</em></li>
                <li>Look before you leap: Unveiling the power of GPT-4v in robotic vision-language planning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8325",
    "paper_id": "paper-276116990",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Sonnet-3.5",
            "name_full": "Sonnet-3.5 (Claude Team)",
            "brief_description": "A closed-source vision-language model (VLM) evaluated as an agent in the interactive Sliding Geom Puzzle; supports multi-image inputs and longish context windows and was run via official API in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Sonnet-3.5",
            "model_description": "Closed-source multimodal VLM from the Claude team, supports multi-image inputs and a minimum context length ≥800 tokens; used via official API in the experiments (no architecture/training details provided in the paper).",
            "model_size": null,
            "puzzle_name": "Sliding Geom Puzzle (SGP)",
            "puzzle_type": "Grid-based sequential sliding-tile variant on a 4×4 board requiring multi-step planning and spatial localization (objects are geometric shapes with colors), evaluated in 3 modalities: 3D rendered view, 2D top-down view, and text representation.",
            "task_setup": "Interactive action-perception loop: at each step the model receives current and goal states (in one modality) plus up to two past steps, and outputs natural-language move commands of the form \"move &lt;color&gt; &lt;shape&gt; &lt;direction&gt;\". Uniform minimal instruction template for all models; models run in context-aware zero-shot mode, CoT prompting allowed; episodes capped at 20 actions; auxiliary board-state inference task also evaluated.",
            "mechanisms_or_strategies": "Chain-of-Thought (CoT) style reasoning in zero-shot mode with a constrained context window of two past steps; models generate explicit action sequences via natural language commands. Evaluation used A* to compute optimal solutions for comparison; no special prompt-engineering per model.",
            "performance_metrics": "From Table 2 (per paper): Completed episodes - Avg 54.56%, 3D 28.67%, 2D 89.67%, Text 45.33%. Mean optimal-path deviation - Avg 3.05, 3D 4.10, 2D 1.44, Text 3.60. Board-state inference accuracy - Avg 60.00%, 3D 35.38%, 2D 84.62%. (900 episodes per model across modalities; per-episode action cap = 20.)",
            "evidence_of_spatial_reasoning": "Produces goal-directed action sequences and often follows near-optimal paths in 2D; Chain-of-Thought outputs (examples in appendix) show explicit reasoning about object positions and blocking relationships; high 2D board-state inference accuracy indicates capacity to identify objects and coordinates in 2D, but much lower 3D localization (drop from 84.6% to 35.4%) indicates limits in 3D visual alignment.",
            "comparisons": "Outperforms other evaluated models (highest success rate among tested models, particularly strong in 2D). Compared against an optimal A* agent (upper bound), a random agent (lower bound), and a 30-person human baseline (humans solved tasks perfectly with near-optimal paths). Sonnet-3.5 &gt; Gemini-2.0-flash &gt; GPT-4o &gt; open-source models in aggregate; closed-source models &gt; open-source models.",
            "limitations_or_failure_cases": "Substantially worse in 3D modality due to coordinate prediction errors and occlusions; more ineffective moves and higher mean deviation from optimal in 3D. Makes out-of-bounds and occupied-destination attempts especially when 3D localization fails. Context-window restriction (past two steps) and per-step prompt repetition limit episodic memory. No model-specific training or fine-tuning on the puzzle.",
            "uuid": "e8325.0",
            "source_info": {
                "paper_title": "iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Gemini-2.0-flash",
            "name_full": "Gemini-2.0-flash (Gemini Team)",
            "brief_description": "A closed-source multimodal VLM evaluated in the iVISPAR Sliding Geom Puzzle benchmark via API, using the same interactive language-command interface and CoT-style reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0-flash",
            "model_description": "Closed-source multimodal vision-language model (Gemini family); supports multi-image inputs and required context length; used via official API for the experiments (no architecture/training details provided in the paper).",
            "model_size": null,
            "puzzle_name": "Sliding Geom Puzzle (SGP)",
            "puzzle_type": "Grid-based sequential sliding-tile variant (4×4) requiring multi-step spatial planning and object localization across 3D/2D/text modalities.",
            "task_setup": "Same interactive setup as other VLMs: receives start and goal states in chosen modality, constrained context of two previous steps, outputs natural-language move commands ('move &lt;color&gt; &lt;shape&gt; &lt;direction&gt;'). Zero-shot with CoT; standardized minimal instruction prompt used for every model.",
            "mechanisms_or_strategies": "Chain-of-Thought zero-shot reasoning with a two-step context window; action proposals parsed by the simulator and applied if legal. Auxiliary board-state inference evaluated via text prediction and set-matching (Hungarian algorithm).",
            "performance_metrics": "Table 2: Completed episodes - Avg 27.11%, 3D 12.67%, 2D 47.33%, Text 21.33%. Mean optimal-path deviation - Avg 4.87, 3D 5.25, 2D 4.09, Text 5.26. Board-state inference accuracy - Avg 54.08%, 3D 28.67%, 2D 79.49%.",
            "evidence_of_spatial_reasoning": "CoT examples show spatial planning steps and blocking analysis. Produces some effective, goal-directed moves (better than random baseline), particularly in 2D; board-state inference shows drop from 2D to 3D consistent with localization difficulties.",
            "comparisons": "Performs below Sonnet-3.5 and above most open-source models; closed-source cluster generally outperforms open-source ones. Humans and the A* agent remain much stronger; random baseline much weaker.",
            "limitations_or_failure_cases": "High coordinate errors and missed detections in 3D; frequent ineffective moves and collisions in 3D; text modality shows out-of-bounds mistakes indicating difficulty representing board dimensions when given only text. Suffers on longer/purer-planning (long shortest-path lengths, many geoms).",
            "uuid": "e8325.1",
            "source_info": {
                "paper_title": "iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A closed-source multimodal LLM (GPT-4 family) used via API to play the Sliding Geom Puzzle in iVISPAR, producing natural-language move commands and CoT reasoning in a constrained zero-shot interactive loop.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source multimodal large model from OpenAI (GPT-4 family); supports vision inputs and multi-turn text-based interactions; used via official API in experiments (no internal architecture details provided in the paper).",
            "model_size": null,
            "puzzle_name": "Sliding Geom Puzzle (SGP)",
            "puzzle_type": "4×4 sequential sliding-tile puzzle variant requiring spatial planning and object localization in 3D/2D/text modalities.",
            "task_setup": "Interactive stepwise action-perception loop: current and goal states provided (3D/2D image or text); model returns actions in prescribed format; two-step context window; zero-shot CoT allowed; episodes capped at 20 steps; auxiliary board-state inference evaluated.",
            "mechanisms_or_strategies": "Chain-of-Thought reasoning examples provided in paper; models compared against optimal A* trajectories and random policy. Models interpret visual scenes and plan sequential moves; no fine-tuning on the task.",
            "performance_metrics": "Table 2: Completed episodes - Avg 17.56%, 3D 9.33%, 2D 37.33%, Text 36.00%. Mean optimal-path deviation - Avg 5.30, 3D 5.45, 2D 4.15, Text 6.30. Board-state inference accuracy - Avg 41.67%, 3D 19.49%, 2D 63.85%.",
            "evidence_of_spatial_reasoning": "CoT outputs show explicit reasoning about blocking objects and multi-step plans. In many episodes GPT-4o produces goal-directed behavior (sometimes better than random) but is substantially below human and optimal agent performance; board inference shows large 2D→3D drop indicating limited 3D alignment.",
            "comparisons": "Per paper, GPT-4o is behind Sonnet-3.5 and Gemini-2.0-flash in success rates but outperforms the open-source models on average. Humans perform near-perfectly; optimal agent solves tasks by construction; random baseline is much worse.",
            "limitations_or_failure_cases": "Significant coordinate and localization errors in 3D vision leading to out-of-bounds and occupied-destination move attempts; higher regret (deviation from optimal) especially in text modality and in complex configurations. Also experienced a small number of format-parsing issues (1 formatting error listed).",
            "uuid": "e8325.2",
            "source_info": {
                "paper_title": "iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "InternVL2.5-78B",
            "name_full": "InternVL 2.5 (78B)",
            "brief_description": "An open-source vision-language model (reported as 78B) evaluated on the SGP; performed near the random baseline on visual modalities and better on text-only tasks compared to visual ones.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternVL2.5-78B",
            "model_description": "Open-source multimodal VLM reported in the paper as InternVL2.5 with ~78B parameters; evaluated from publicly available checkpoints with sampling settings (temp=1.0, top-p=0.95, top-k=50).",
            "model_size": "78B",
            "puzzle_name": "Sliding Geom Puzzle (SGP)",
            "puzzle_type": "4×4 sliding-tile style puzzle (grid-based) requiring sequential planning and object localization in 3D/2D/text modalities.",
            "task_setup": "Same interactive API loop, minimal shared prompt template, two-step context window, CoT allowed; evaluated across 900 episodes per model (300 per modality). Auxiliary board-state inference measured via setmatching.",
            "mechanisms_or_strategies": "Zero-shot CoT reasoning and generation of natural-language move commands. No specialized symbolic planner; behavior emerges from pre-trained model capabilities. A* used for optimal comparison and Hungarian algorithm for auxiliary matching.",
            "performance_metrics": "Table 2: Completed episodes - Avg 10.16%, 3D 1.67%, 2D 9.42%, Text 19.33%. Mean optimal-path deviation - Avg 5.98, 3D 6.39, 2D 5.86, Text 5.69. Board-state inference accuracy - Avg 34.95%, 3D 16.51%, 2D 53.38%.",
            "evidence_of_spatial_reasoning": "Shows very limited success in 3D/2D vision; slightly better in text modality, suggesting reliance on textual pattern reasoning rather than grounded visual spatial reasoning. Auxiliary task shows low correct-object counts and substantial coordinate errors in 3D.",
            "comparisons": "Performs near-random on visual tasks and worse than closed-source models; stronger on text than vision, opposite of closed-source trend. Humans and A* vastly outperform InternVL2.5-78B.",
            "limitations_or_failure_cases": "High coordinate error rates (see Table 5) and many format errors reported (format errors count high for some open-source runs). Very low completed-episode rates in 3D, frequent ineffective and invalid actions; often fails to parse or produce valid geom triplets in auxiliary task.",
            "uuid": "e8325.3",
            "source_info": {
                "paper_title": "iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LLaVA-OneVision-72B",
            "name_full": "LLaVA-OneVision-72B",
            "brief_description": "An open-source multimodal VLM (reported ~72B) evaluated on SGP; exhibited near-random performance on visual inputs with somewhat better behavior in text-only modality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-OneVision-72B",
            "model_description": "Open-source vision-language model (LLaVA OneVision family) reported in paper as a ~72B-parameter model; used from public checkpoints with standardized sampling hyperparameters.",
            "model_size": "72B",
            "puzzle_name": "Sliding Geom Puzzle (SGP)",
            "puzzle_type": "4×4 grid-based sliding tile puzzle variant requiring planning and spatial localization across 3D/2D/text modalities.",
            "task_setup": "Interactive loop with natural-language move outputs, two-step context window, minimal universal prompt, CoT allowed; episodes capped at 20 moves; auxiliary board-state inference evaluated.",
            "mechanisms_or_strategies": "Zero-shot Chain-of-Thought style reasoning; no specialized symbolic planner. Evaluated against A* optimal and random baselines; Hungarian matching for auxiliary task.",
            "performance_metrics": "Table 2: Completed episodes - Avg 8.22%, 3D 0.67%, 2D 1.33%, Text 22.67%. Mean optimal-path deviation - Avg 6.35, 3D 6.75, 2D 6.81, Text 5.50. Board-state inference accuracy - Avg 26.36%, 3D 14.72%, 2D 38.00%.",
            "evidence_of_spatial_reasoning": "Produces some Chain-of-Thought outputs but overall low correct-object inference and high coordinate errors in 3D indicate limited 3D spatial grounding; better relative performance in text suggests reliance on linguistic heuristics rather than visual grounding.",
            "comparisons": "Per paper, among the lowest-performing models on visual input; closed-source models substantially outperform LLaVA-OneVision-72B. Humans and optimal algorithm remain much stronger.",
            "limitations_or_failure_cases": "Very low visual success rates (especially in 3D), large coordinate errors and hallucinations in some cases; format errors and parsing issues present; struggles with object occlusions and precise localization in 3D scenes.",
            "uuid": "e8325.4",
            "source_info": {
                "paper_title": "iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Qwen2-72B",
            "name_full": "Qwen2-72B (Qwen Family)",
            "brief_description": "An open-source multimodal VLM (reported ~72B) evaluated on SGP; performed near-random on visual tasks with relatively better text-only performance and moderate board-state inference errors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2-72B",
            "model_description": "Open-source vision-language model in the Qwen family reported in the paper as Qwen2-72B; used from public checkpoints with sampling settings (temp=1.0, top-p=0.95, top-k=50).",
            "model_size": "72B",
            "puzzle_name": "Sliding Geom Puzzle (SGP)",
            "puzzle_type": "4×4 sequential sliding-tile variant requiring planning and object localization under occlusion in 3D, fully visible in 2D, or described textually.",
            "task_setup": "Interactive natural-language action loop, two-step context window, zero-shot with CoT permitted; same minimal instruction template across all models; auxiliary board-inference evaluation.",
            "mechanisms_or_strategies": "CoT-style reasoning in zero-shot; models attempt to map visual input to textual board states and propose actions; A* provides optimal baseline and Hungarian algorithm used for auxiliary matching.",
            "performance_metrics": "Table 2: Completed episodes - Avg 5.89%, 3D 0.67%, 2D 1.67%, Text 15.33%. Mean optimal-path deviation - Avg 6.37, 3D 6.66, 2D 6.54, Text 5.90. Board-state inference accuracy - Avg 41.54%, 3D 18.77%, 2D 64.31%.",
            "evidence_of_spatial_reasoning": "Shows occasional goal-directed moves but overall struggles on visual modalities; auxiliary inference shows moderate 2D performance but large drop in 3D coordinate accuracy, indicating weak 3D spatial grounding.",
            "comparisons": "One of the weakest performers among evaluated models on visual tasks; closed-source models achieve much higher success rates. Performs better on text relative to its visual scores, similar to other open-source models.",
            "limitations_or_failure_cases": "High coordinate errors and frequent ineffective moves in 3D; often fails to reach goal within 20-step cap; shows format and parsing errors in auxiliary task outputs for some runs.",
            "uuid": "e8325.5",
            "source_info": {
                "paper_title": "iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_good_path_planners_a_benchmark_and_investigation_on_spatialtemporal_reasoning"
        },
        {
            "paper_title": "Exploring and benchmarking the planning capabilities of large language models.",
            "rating": 2,
            "sanitized_title": "exploring_and_benchmarking_the_planning_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Evaluating spatial understanding of large language models.",
            "rating": 2,
            "sanitized_title": "evaluating_spatial_understanding_of_large_language_models"
        },
        {
            "paper_title": "What's \"up\" with vision-language models? investigating their struggle with spatial reasoning.",
            "rating": 2,
            "sanitized_title": "whats_up_with_visionlanguage_models_investigating_their_struggle_with_spatial_reasoning"
        },
        {
            "paper_title": "SpaRC and SpaRP: Spatial reasoning characterization and path generation for understanding spatial reasoning capability of large language models.",
            "rating": 2,
            "sanitized_title": "sparc_and_sparp_spatial_reasoning_characterization_and_path_generation_for_understanding_spatial_reasoning_capability_of_large_language_models"
        },
        {
            "paper_title": "Sliding puzzles gym: A scalable benchmark for state representation in visual reinforcement learning.",
            "rating": 1,
            "sanitized_title": "sliding_puzzles_gym_a_scalable_benchmark_for_state_representation_in_visual_reinforcement_learning"
        },
        {
            "paper_title": "PUZ-ZLES: A benchmark for neural algorithmic reasoning.",
            "rating": 1,
            "sanitized_title": "puzzles_a_benchmark_for_neural_algorithmic_reasoning"
        },
        {
            "paper_title": "Look before you leap: Unveiling the power of GPT-4v in robotic vision-language planning.",
            "rating": 1,
            "sanitized_title": "look_before_you_leap_unveiling_the_power_of_gpt4v_in_robotic_visionlanguage_planning"
        }
    ],
    "cost": 0.01494875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>iVISPAR -An Interactive Visual-Spatial Reasoning Benchmark for VLMs
30 Sep 2025</p>
<p>Julius Mayer 
Institute of Cognitive Science
Osnabrück University
OsnabrückGermany</p>
<p>Mohamad Ballout 
Institute of Cognitive Science
Osnabrück University
OsnabrückGermany</p>
<p>Serwan Jassim 
Institute of Cognitive Science
Osnabrück University
OsnabrückGermany</p>
<p>† Farbod 
Institute of Cognitive Science
Osnabrück University
OsnabrückGermany</p>
<p>Nosrat Nezami 
Institute of Cognitive Science
Osnabrück University
OsnabrückGermany</p>
<p>Elia Bruni 
Institute of Cognitive Science
Osnabrück University
OsnabrückGermany</p>
<p>iVISPAR -An Interactive Visual-Spatial Reasoning Benchmark for VLMs
30 Sep 20252739DA3627D1293B0906F9AAD33DC413arXiv:2502.03214v2[cs.CL]
Vision-Language Models (VLMs) are known to struggle with spatial reasoning and visual alignment.To help overcome these limitations, we introduce iVISPAR, an interactive multimodal benchmark designed to evaluate the spatial reasoning capabilities of VLMs acting as agents.iVISPAR is based on a variant of the sliding tile puzzle-a classic problem that demands logical planning, spatial awareness, and multi-step reasoning.The benchmark supports visual 3D, 2D, and text-based input modalities, enabling comprehensive assessments of VLMs' planning and reasoning skills.We evaluate a broad suite of state-of-theart open-source and closed-source VLMs, comparing their performance while also providing optimal path solutions and a human baseline to assess the task's complexity and feasibility for humans.Results indicate that while VLMs perform better on 2D tasks compared to 3D or text-based settings, they struggle with complex spatial configurations and consistently fall short of human performance, illustrating the persistent challenge of visual alignment.This underscores critical gaps in current VLM capabilities, highlighting their limitations in achieving human-level cognition.Project website: https://microcosm.ai/ivispar.</p>
<p>Introduction</p>
<p>The rapid advancement of Vision-Language Models (VLMs) has spurred significant debate regarding their capacity to achieve human-level cognition.These models are increasingly deployed as general reasoning systems capable of addressing complex problems across diverse domains, with applications extending into dynamic, real-world scenarios such as physical agent-based tasks and planning (Wang et al., 2024a;Xi et al., 2023;Zeng Figure 1: VLMs' success rates of completed games over 900 episodes across vision 3D, vision 2D, and text.et al., 2023).However, critical gaps persist in their spatial reasoning and visual alignment capabilities, areas essential for understanding, interpreting, and manipulating objects and their spatial relationships (Kamath et al., 2023a;Bordes et al., 2024;Campbell et al., 2024).</p>
<p>Spatial reasoning, a foundational aspect of problem-solving, navigation, and interaction with the physical world, requires models to bridge vision and cognition by interpreting visual information to understand spatial arrangements.Tasks such as mentally rotating shapes, predicting object movement, and recognizing patterns exemplify the importance of visual-spatial reasoning.Despite these critical requirements, progress in VLMs has been hampered by evaluation benchmarks that fail to capture the dynamic and multi-step complexity of real-world spatial reasoning.Existing benchmarks predominantly rely on static, text-or imagebased setups that often oversimplify spatial contexts, focusing on 2D environments without interactivity or dynamic problem-solving capabilities.This limitation perpetuates a lack of meaningful progress in visual-spatial reasoning within more realistic 3D environments.</p>
<p>Contributions.</p>
<p>To bridge this gap, we introduce iVISPAR (Interactive Visual-Spatial Reasoning), a novel benchmark designed to systematically evaluate VLMs as agents in dynamic 3D environments.iVISPAR is built around the sliding tile puzzle, a well-established problem in developmental psychology that demands logical planning, spatial awareness, and multi-step problemsolving.As part of our contributions, we introduce the Sliding Geom Puzzle, a variant that replaces traditional numbered tiles with geometric objects distinguished by their color and shape, adding an additional layer of visual reasoning.</p>
<p>Notably, iVISPAR is grounded in a well-studied, formalized problem with access to optimal solutions, ensuring a robust framework for evaluation. 1he benchmark supports scalable task complexity by adjusting factors such as board size, the number of tiles, and solution paths, ranging from simple configurations to NP-complete challenges that surpass baseline human performance.</p>
<p>Leveraging a prompt-based API, iVISPAR enables VLMs to interact with a simulated environment through an iterative action-perception loop.Experimentation results demonstrate that while state-of-the-art VLMs can handle basic spatial reasoning tasks, they face significant difficulties with more complex scenarios, especially in 3D environments.Evaluating models in such 3D settings is essential, as they more closely mirror the spatial complexity of real-world environments.By contrasting their performance against optimal solutions and human baselines, we highlight the persistent gap between current VLM capabilities and human-level spatial reasoning.</p>
<p>Our contributions are threefold: (i) a novel interactive benchmark that systematically evaluates visual-spatial reasoning in VLMs; (ii) a scalable task design rooted in a formalized problem with optimal solutions; and (iii) empirical insights into the strengths and limitations of VLMs across varying task complexities and modalities.iVISPAR lays the foundation for advancing VLM research toward overcoming critical gaps in reasoning and alignment capabilities.</p>
<p>Related work</p>
<p>Spatial Reasoning Benchmarks</p>
<p>Physical understanding in interactive agents has long been studied through simulation-based benchmarks (Li et al., 2024b;Mecattaf et al., 2024;Jassim et al., 2024;Wang et al., 2025;Hu et al., 2023;Zhao et al., 2025;Guruprasad et al., 2024;Su et al., 2024;Feng et al., 2025), although many of these frameworks are not directly suited for VLM evaluation due to limited language interfaces, low task fidelity, or demanding simulation requirements.Several datasets targeting visual reasoning have been applied to deep learning models (Johnson et al., 2016;Li et al., 2023), but they do not support interactive planning or action execution by language agents.Other works have explored similar setups using geometric object games, primarily in the context of language game learning with deep learning agents (Wang et al., 2016;Kuhnle and Copestake, 2017); related efforts such as Sliding Puzzles Gym and PUZZLES (Oliveira et al., 2024;Estermann et al., 2024) have been proposed as RL benchmarks, but lack the language interface and fine-grained 3D problem generation introduced in our setting.</p>
<p>Spatial Reasoning in LLMs</p>
<p>Even though Large Language Models (LLMs) are primarily trained via next-token prediction on textual corpora, their capacity for spatial reasoning have attracted recent attention (Abdou et al., 2021;Patel and Pavlick, 2021).LLMs have also been explored as agents for spatial planning (Bohnet et al., 2024), path planning (Aghzal et al., 2024), and spatial path generation (Rizvi et al., 2024) in purely textual or symbolic environments.Several recent studies have examined whether LLMs implicitly encode spatial structures and geometric reasoning, ranging from digital twin generation via symbolic rules (Wang et al., 2024c), to textual spatial question answering in diverse settings (Mirzaee et al., 2021), and evaluations across grid, ring, and tree topologies (Yamada et al., 2024).</p>
<p>Spatial Reasoning in VLMs</p>
<p>Visual reasoning has emerged as a key focus in evaluating VLMs, with growing interest in their capacity to interpret spatial relationships and object configurations (Zhang et al., 2024;Rajabi and Kosecka, 2024b;Roberts and Roberts, 2024;Campbell et al., 2025); concurrently, several studies have examined the degree to which these mod- els align visual inputs with linguistic representations (Merullo et al., 2023;Ilharco et al., 2021).Recent advancements in VLMs have prompted a surge in evaluations, yet most studies primarily rely on visual question-answering tests (Liu et al., 2023;Rajabi and Kosecka, 2024a;Wang et al., 2024b;Cheng et al., 2024;Tang et al., 2024;Duan et al., 2025;Wang et al., 2023;Kamath et al., 2023b).Beyond static evaluations, a growing body of work explores the use of VLMs and foundation models as interactive agents within simulated environments, where they are tasked with manipulating objects, navigating spaces, or executing spatial instructions in grounded contexts (Wu et al., 2024;Li et al., 2024b;Mecattaf et al., 2024;Jassim et al., 2024;Wang et al., 2025;Su et al., 2024).This includes applications in embodied AI and robotics, where VLMs are increasingly integrated into control loops to support visuomotor reasoning and spatial decision-making (Hu et al., 2023;Zhao et al., 2025;Guruprasad et al., 2024;Feng et al., 2025).</p>
<p>In this context, we present iVISPAR, an interactive multimodal benchmark designed to evaluate the spatial reasoning capabilities of VLMs acting as agents.</p>
<p>3 The iVISPAR Benchmark iVISPAR2 is an interactive, multimodal puzzle simulator that presents agents with a board state in one of three input modalities: a 3D rendered image, a 2D top-down view, or a text-based representation (see Figure 2).By rendering scenes in 3D space, iVISPAR offers a more realistic depiction of spatial environments compared to traditional 2D grid visualizations and enables systematic comparisons across modalities.Agents interact with the board by issuing natural language commands through a text-based API to apply actions to the board (see Figure 3).iVISPAR supports procedural generation of puzzle instances with finely controlled parameters, allowing for a scalable dataset of tasks with adjustable complexity across many spatial properties, and benchmarking performance with multiple baseline models.</p>
<p>Sliding Geom Puzzle</p>
<p>A central environment in iVISPAR is the Sliding Geom Puzzle (SGP), a reimagining of the classic sliding tile puzzle (see Appendix A.3). Instead of numbered tiles, SGP uses geometric objects (geoms) uniquely defined by combinations of color and shape, increasing visual-spatial complexity and enhancing task scalability.This design shift requires models to interpret object features rather than follow numerical sequences, mirroring real-world spatial reasoning where items are distinguished by appearance, size, or structure.The task draws inspiration from physical scenarios such as organizing items, assembling structures, or packing, promoting a more authentic evaluation of realworld spatial capabilities.</p>
<p>Game dynamics</p>
<p>The objective is to rearrange the pieces on the board by moving them over free spaces to match a given goal configuration.In each episode, agents receive observations of the start and goal states (see Figure 2), accompanied by task instructions (see Appendix A.1.1).Agents apply move actions to geoms by referencing their unique color and shape combination and specifying the direction of intended movement.Geoms can be moved in cardinal directions (LEFT, RIGHT, UP, DOWN), with actions formatted as "move <color> <shape> <direction>": "move blue sphere right" Actions are validated and applied if legal, with agents receiving updated board states regardless of the action's success after each move command.Effective and ineffective actions both result in valid new board states but, respectively, decrease or increase the path length to the goal state.Invalid moves, such as occupied destination and out-ofbounds actions, fail to alter the board state, as do illegal commands, which violate the instructed action format.This action-perception loop repeats until the goal state is achieved or a step limit is reached.Due to limited context windows, VLM agents receive task instructions at each time step.A sample agent-environment interaction is provided in Appendix A.1.3.</p>
<p>Observation Spaces</p>
<p>Agents observe a combination of the current board state and the goal state.Additionally, they can receive a sequence of past state-action pairs, determined by the size of the configured context window.Images for 3D observations are presented from an angled top-down perspective and may include partially occluded objects, whereas 2D observations follow a graph-like layout with fully visible elements.Both may optionally include embedded, text-based chess-style coordinate labels as spatial cues along the outer edge of the grid board as well as on free tiles.In 2D observations, shapes are mapped consistently from their 3D counterparts to preserve object identity across modalities.Images can also be marked with an embedded text label and a colored background to differentiate between past (grey), current (blue), and goal state (green).This modality does not rely on images.</p>
<p>Complexity Scalability</p>
<p>The GSTP is a well-known NP-hard problem due to the need for multi-step planning across a constrained grid (Gozon and Yu, 2024).SGP inherits this complexity but introduces greater flexibility in scaling difficulty without altering the game's core mechanics.This flexibility provides more degrees of freedom, making the task more tractable for VLM agents.Key scaling factors include board size, number of objects, object variability, length of the shortest path solution, and the geom interference factor (see Appendix A.1.2).The shortest path solution for all episode configurations is calculated using the A* algorithm (Hart et al., 1968), as detailed in Appendix A.7.1.The interference factor denotes the extent to which objects obstruct one another's optimal paths, increasing the global solution length beyond the cumulative Manhattan distances of individual paths.This interference can create configurations with short optimal paths but increased planning requirements, significantly raising the problem's difficulty.Available geometric shapes include ["cube," "pyramid," "sphere," "cylinder," "cone," "prism"], with colors freely selectable by referencing RGB values.Agents must navigate combinatorial complexity by matching shapes and colors, promoting spatial strategies over the sequential patterns seen in numerical tile puzzles.Episode configurations are generated procedurally, requiring models to generalize across puzzle instances.Human and algorithmic benchmarks for these experiments are detailed in Section 4.2.</p>
<p>Experiments</p>
<p>Performance of VLMs is tested for the SGP to assess their capabilities in scene understanding, problem-solving, and multi-step planning within constrained environments.</p>
<p>Dataset Generation</p>
<p>Experiments were conducted on a dataset of SGPs on a fixed board size to 4 × 4: smaller grids (e.g., 3 × 3) collapse many spatial-relation cases, while larger ones (≥ 5 × 5) dilute object visibility without yielding further complexity benefits.Performance is assessed by varying complexity across two parameters: the number of objects (2-11) and the shortest path length (2-11).Configurations maintain a geom interference factor of 0, ensuring the shortest path equals the cumulative Manhattan distance.Initial experiments indicated that VLM agents faced significant challenges at higher task complexities.Three episodes are sampled for each complexity level, producing a dataset of 300 diverse board configurations.The set of geom properties consists of four shapes, sphere, pyramid, cube, and cylinder, and four colors, red, green, blue, and yellow, resulting in 16 unique combinations.VLM agents are tested on the same dataset for each modality, resulting in 900 episodes for each model.</p>
<p>Baselines</p>
<p>To contextualize agent performance and provide upper and lower bounds, we establish four baselines encompassing human and AI agents.</p>
<p>Human performance was evaluated with 30 participants using a web app GUI of the SGP, where participants interacted by prompting text commands over a command line, mirroring the interaction method of VLM agents.Baselines were provided for the 3D vision modality on the same dataset as the VLM agents.</p>
<p>AI baselines were introduced for two agents: an optimal agent executing shortest path solutions computed by A* (Hart et al., 1968), and a random agent performing uninformed but valid actions uniformly sampled from those leading to new board states.Algorithms for the AI agents are detailed in Appendix A.7.</p>
<p>Models</p>
<p>We evaluate a selection of open-and closed-source VLMs that scored high on OpenCompass3 and which support multi-image inputs and a minimum context length of 800 tokens.Selected models are: Sonnet-3.5 (Claude Team, 2024), Gemini-2.0-flash(Gemini Team, 2024), GPT-4o (OpenAI et al., 2024), InternVL2.5-78B(Chen et al., 2024), LLaVA-OneVision-72B (Li et al., 2024a), Qwen2-72B (Wang et al., 2024d).For closed-source models, we rely on the official APIs and for opensource models, on the publicly available checkpoints.We use a temperature of 1.0, top-p of 0.95, and top-k of 50 for all open-source models.An overview of all models and their details can be found in the Appendix A.2.</p>
<p>Context-Aware Zero-Shot Reasoning</p>
<p>The models employ Chain-of-Thought (CoT) reasoning (Wei et al., 2022) to break down complex problems into smaller sub-tasks, enhancing accuracy and interpretability (Appendix A.1.3).We constrain VLMs' context windows to the past two steps, incorporating state representations alongside the model's action responses.This approach prioritizes extracting maximum value from limited experience to preserve the models' sequential coherence and minimize computational overhead.Operating within this context-aware zero-shot reasoning framework, the models interpret task requirements without examples, drawing exclusively from pretrained knowledge, task instructions, and limited past interactions.</p>
<p>Instruction Prompts</p>
<p>We avoided prompt engineering for any single model; the chosen template is the same for all systems and contains only the minimal information needed.Fixing one validated template provides a consistent basis for comparison and makes the benchmark easily reproducible.The visual and text prompts are isomorphic: the image placeholder is the only difference, so no modality receives extra hints.Our human-baseline study likewise found the final wording easy to follow.This supports our aim of testing spatial-reasoning ability itself, without relying on prompt engineering, so we use one clear, uniform template for all models.</p>
<p>Evaluation</p>
<p>Agent performance is evaluated through two primary metrics: the fraction of solved environments and mean step-deviation from the optimal path Mean step-deviation from optimal path measures the deviation from optimal behavior during problem-solving.At each step t, the shortest path solution from the current board state to the goal, computed by A*, is used to assess efficiency.Formally,
R(t) = d(s t , s * ) − d(s 0 , s * ) − t .
where d(s, s * ) denotes the shortest path length from state s to the goal s * .This metric quantifies how much further the agent is from the goal compared to an optimal agent after the same number of steps.A regret value of zero indicates that the agent follows an optimal trajectory, while positive regret reflects inefficiencies or unnecessary detours.By capturing performance even in unsolved environments, this approach provides insights into agent behavior under varying complexities.</p>
<p>To gain deeper insights, we analyze the most common error patterns exhibited by agents.This allows us to identify model weaknesses, recurring failure cases, and patterns of suboptimal decisionmaking.</p>
<p>Auxiliary Task</p>
<p>Additionally, we evaluate the models' ability to infer and represent board states from visual input across all 300 episodes.Given an image and accompanying instructions, each model is tasked with predicting the corresponding board configuration in text form, using the same format as the textual representation shown in Figure 2.This auxiliary task further enriches our understanding of the models' behavior and their capacity to interpret spatial information from visual inputs.</p>
<p>To analyze this task, we frame the comparison between the true and predicted board states as a set matching problem, solved using the Hungarian algorithm.A match is defined as any pair of geoms sharing at least either color or shape.Geoms that share neither are considered missed (if only present in the true state) or hallucinated (if only present in the prediction).Matched geoms may still contain mismatches in coordinates, color, or shape.Predicted elements that cannot be parsed into valid geom triplets are counted as format errors.</p>
<p>Results</p>
<p>We evaluated the spatial reasoning capabilities of VLMs in our SGP environment on 3D vision and compared it to 2D vision and text-based modalities across 300 episodes each (see Figure 4).To standardize gameplay, the number of actions per episode was capped at 20.</p>
<p>Success rates: The percentage of episodes completed and the mean deviations of steps from the optimal path were measured for each modality and compared to human performance as well as random actions (Figure 4).</p>
<p>Action classification: We classified actions based on their effects on the board and calculated their average occurrence per episode to provide insights into the challenges VLMs face in efficiently completing episodes (see Figure 5 top).Effective and ineffective actions both result in valid new board states but, respectively, decrease or increase the path length to the goal state.Invalid moves, such as occupied destination and out-of-bounds actions, while illegal commands break the instructed action format, all of which leave the board state unchanged.</p>
<p>Auxiliary Task: For the board state inference task, we evaluate the number of geoms that were correctly inferred, missed, hallucinated, or contained a mismatch in coordinates, color, or shape.Format errors denote cases where the output failed to follow the expected structure (Figure 5, bottom).</p>
<p>Complexity scales: We evaluated the cumulative performance of VLMs across the three modal-ities using two complexity scales, the shortest path length required to solve an episode and the number of geoms on the board.Longer shortest paths demand a broader global planning horizon and consistent goal-directed progress, while higher geom counts require efficient local planning to optimize rearrangement order and manage free spaces.Figure 7 illustrates the performance of VLMs in 100 combinations of complexity, highlighting the average minimal distance to the goal state in 20 steps.</p>
<p>Discussion</p>
<p>Model Performance</p>
<p>All models show basic task understanding and spatial reasoning, progressing toward the goal state (see Figure 4).Performance, however, varies widely.Closed-source models outperform opensource ones: Sonnet-3.5 achieves the highest success rate at 89.7% in the 2D visual modality, followed by Gemini-2.0-Flashand GPT-4o.In contrast, open-source models such as InternVL2.5-78B,LLaVA-OneVision-72B, and Qwen2-72B perform near the random baseline.Human participants solve the tasks perfectly with near-optimal paths, setting a high benchmark.</p>
<p>Notably, even models solving fewer than 1% of tasks often produce more efficient paths than a random baseline (see Figure 4, bottom), indicating traces of goal-directed behavior despite overall failure.These task performances are also consistent with the further analysis of the models' error types and their accuracy in the board state inference task, which we discuss in Section 6.2.</p>
<p>Error Patterns</p>
<p>We analyzed the types of mistakes models make during interaction with the simulator and evaluated their ability to infer board states from visual input.Overall, models rarely issue illegal commands or exhibit format errors (see Figure 5, top and bottom), suggesting that most VLMs understand how to follow instructions and interact with the environment appropriately.</p>
<p>However, board state inference accuracy reveals a sharp performance drop from 2D to 3D inputs: while models correctly identify an average of 4.2 objects in 2D, this number falls to 1.4 in the 3D setting (see Figure 6, right).This is primarily due to substantial increases in coordinate prediction errors, alongside moderate rises in color, shape mismatches, and missed detections.In contrast, hallucinations and format-related issues remain largely stable across both modalities.</p>
<p>These findings offer a clear explanation for the weaker performance in the 3D vision condition: precise localization of objects remains a critical challenge.As illustrated in Figure 5, this results in more ineffective moves, including frequent attempts to place objects out-of-bounds or onto already occupied cells.</p>
<p>Modality Impact</p>
<p>Despite being evaluated on identical tasks, model performance varied substantially across input modalities (see Figure 4).All closed-source models (Sonnet-3.5,Gemini-2.0-flash,GPT-4o) performed best on 2D vision, followed by text, and worst on 3D vision.This suggests that these models may have undergone more training on 2D visual inputs, which are more common in spatial benchmarks.Interestingly, text input, despite posing significant challenges for humans, ranked second, indicating some robustness in linguistic reasoning.In contrast, open-source models (In-ternVL2.5,LLaVA-OneVision, Qwen2) performed poorly across the board, with near-random scores on visual inputs.Their relatively stronger performance on text tasks may reflect a reliance on superficial pattern recognition rather than grounded spatial understanding.As shown in Figure 6 (left), error patterns for ineffective moves and collisions align with the overall performance ranking across modalities.Out-of-bounds errors are most frequent in the text condition, nearly twice as common as in 2D vision, indicating that understanding board dimensions was a primary challenge in the textual setting.Additional results from our board state inference task further support this view, showing that models, predict more correct objects on the board in Vision 2D compared to Vision 3D (Figure 6, right).</p>
<p>Complexity Scaling</p>
<p>We analyzed the correlation matrix between the number of objects on the board and the shortest path solution length to assess how different types of complexity affect model performance (see Figure 7,top).While performance consistently drops with increasing complexity in both dimensions, the heatmaps reveal modality-specific trends.Performance declines more steeply with increasing geom count (particularly in 3D), suggesting that sequential planning under visual conditions poses a major challenge.In contrast, in the text-only setting, the number of geoms seems to have little effect, with errors mostly determined by the length of the shortest path solution.This highlights limitations in spatial reference from language alone.Figure 7: Cumulative graphs aggregated across agents.Top: Correlation matrix of remaining shortest-path lengths to the goal for tasks with optimal paths between 2-11 steps.Each run is capped at 20 actions, and the metric is computed at the agent's final state, either upon reaching the goal or, if unsolved, after the 20th action.Bottom: Error types in the board state inference auxiliary task over increasing number of geoms on the board.</p>
<p>Data from the auxiliary task of board state inference show that, while errors to predict the coordinates of geoms on the board increase with the number of geoms on the board, other error types remain relatively stable even for a higher number of geoms on the board (see Figure 7).Format errors and the number of hallucinated geoms is overall low, mismatches with colors and shapes increasing only slightly, and surprisingly the number of missed objects stays relatively stable as well.</p>
<p>Conclusion</p>
<p>We have introduced iVISPAR, a novel interactive multimodal benchmark designed to evaluate the spatial reasoning capabilities in 3D vision of VLMs acting as agents.The benchmark, centered on the Sliding Geom Puzzle, evaluates VLMs' abilities in logical planning, spatial awareness, and multistep problem-solving, aiming to reflect real-world spatial reasoning.Our evaluation tested a suite of state-of-the-art open-source and closed-source VLMs on a dataset of board configurations, scaled across two levels of complexity.We compared them to baselines for human capabilities, optimal and random agents, providing insight into their performance under varying conditions.</p>
<p>Our findings demonstrate that VLMs struggle with spatial reasoning in 3D vision and that there are significant performance differences between the tested VLMs.While they understand the instructions and outperform random agents in simple spatial tasks, they struggle with more complex configurations and intricate problem properties.Interestingly, VLMs show stronger performance in 2D vision compared to 3D or text-based tasks.Our auxiliary board state inference task revealed that VLMs frequently miss geoms, misplace them on the board, or mismatch their colors or shapes, errors that occur more often with 3D vision input than with 2D.This suggests that visual alignment for 3D spatial reasoning continues to pose a significant challenge, underscoring persistent gaps in VLM capabilities and highlighting barriers to achieving human-level cognitive performance.</p>
<p>Future Work Looking ahead, we plan to expand the benchmark to incorporate additional tasks focused on scene understanding, as well as rotation and transformation challenges.</p>
<p>Resources For the most up-to-date results on state-of-the-art models and access to the leaderboard, please visit: https://microcosm.ai/ivispar.</p>
<p>Limitations</p>
<p>We restricted the context window, limiting the number of images VLMs can process.Extended image inputs often disrupt VLMs' understanding of sequential coherence and increase computational demands and API costs.This contrasts with human participants, who recall each step of an episode and draw from past experiences.</p>
<p>Additionally, while some models are optimized for long-context reasoning or "deep thinking," their architecture and usage patterns are ill-suited for step-wise, interactive simulations.Their per-frame API costs are disproportionately higher, making them impractical for the interaction format used in our benchmark.This also limits direct comparisons to human participants, who recall previous steps and integrate episodic knowledge more efficiently.</p>
<p>Impact Statement</p>
<p>This paper contributes to advancements in visionlanguage models.While our work has potential applications in broader AI research, it does not introduce immediate ethical or societal risks beyond those already associated with the field.As our work is largely theoretical and not at a scale that could pose significant concerns, it does not raise specific risks of misuse or unintended consequences.</p>
<p>Output Example</p>
<p>To reach the goal state I need to compare the current active state with the goal state.</p>
<p>Current active state • In the current active state the green sphere is on top blocking movements of objects below it.</p>
<p>• The yellow sphere is on the bottom row B1.</p>
<p>Goal state</p>
<p>• In the goal state the green sphere is two steps down in column B.</p>
<p>• The red cylinder is in the bottom left, two steps down.</p>
<p>• The yellow sphere is in position B2, two steps up.</p>
<p>Analyzing the images I see that the yellow spheres path is blocked by the green cylinder I'll first move another object to clear the way.Moving the blue cube rightward will help clear the blocked pathway for the green cylinder.A.5 Supplementary Graphs</p>
<p>Action: Move the blue cube right</p>
<p>A.1.4 Episode Progression Example</p>
<p>A.4 Detailed Results</p>
<p>A.4.1 Performance Results</p>
<p>Model</p>
<p>Figure 2 :
2
Figure 2: Example of VLMs' observations for a state (blue) and the goal (green) at each step during an episode of the Sliding Geom Puzzle environment, on a 4×4 board with 10 geoms and an optimal path length of 2. Left to right, each tested modality: vision 3D, vision 2D, and text-based representation.For more examples, see Appendix A.1.2</p>
<p>Figure 3 :
3
Figure 3: Depiction of the interaction flow between VLM agents and the iVISPAR simulator with a progression through an episode with the shortest path solution of 4 steps being solved by prompted actions from a VLM agent.For a full example of an episode progression, see Appendix A.1.4.</p>
<p>Figure 2 shows 3D vision (left) and 2D vision (middle) for the active state (top) and the goal state (bottom).The text-based representation encodes past, active, and goal states directly in the prompt string supplied to the agent.Agents receive the list of geoms in the order of board coordinates.A visualization of the text-based active (top) and goal states (bottom) is shown in Figure 2 (right).</p>
<p>Figure 4 :
4
Figure4: VLM evaluation on 900 episodes per model across all three modalities, with 95% confidence intervals.Baseline comparisons for human performance and random moves are shown.Top: VLMs' success rates of episodes completed with higher values denoting better performance.Bottom: VLMs' mean step deviation from the optimal path with lower values denoting better performance.Full numerical results are provided in Appendix A.4</p>
<p>Figure 5 :
5
Figure 5: Error patterns showing average action counts per episode during SGP interaction (top) and average geoms per episode for the board state inference auxiliary task (bottom), both averaged across modalities (see Sections 5 and 4.7), each aggregated across modalities.Full numerical results are provided in Appendix A.4.</p>
<p>Figure 6 :
6
Figure 6: Error patterns showing average action counts per episode during SGP interaction (left; see Section 5) and average geoms per episode for the board state inference auxiliary task (right; see Section 4.7), shown per modality and aggregated across agents.Full numerical results are provided in Appendix A.4.</p>
<p>Figure 8 :
8
Figure 8: Examples of VLMs' observations for a state (blue) and the goal (green) at each step during an episode of the Sliding Geom Puzzle environment, on a 4×4 board with 2, 5, 8 and 11 geoms and an optimal path length of 8, 4, 2, and 6 respectively.Left to right, each tested modality: vision 3D, vision 2D, and text-based representation.</p>
<p>Figure 9 :
9
Figure 9: Example of an episode progression for an environment in vision 3D (other modalities progress analogously) with an optimal path length of 9, showing steps 1 to 12 in order, including 3 mistakes (red action text).</p>
<p>Figure 11 :
11
Figure 11: VLMs' average action counts per episode by category for each modality.Number of actions per episode is capped at 20. Effective / ineffective actions respectively decrease / increase the path length to the goal state.Occupied destination and out-of-bounds are invalid moves, while illegal commands break the instructed action format, all of which leave the board state unchanged.</p>
<p>Figure 12 :
12
Figure 12: VLMs' average shortest path to the goal state across all modalities.Number of actions per episode is capped at 20.</p>
<p>Table 2 :
2
Evaluation of models across three modalities.Each row shows average episode completion rate (%), mean deviation from the optimal path (see Section 4.6), and board state inference accuracy (%).
MetricAvg3D2DTextClosed Source ModelsCompleted episodes54.5628.6789.6745.33Sonnet-3.5Optimal path deviation3.054.101.443.60Board state inference60.0035.3884.62-Completed episodes27.1112.6747.3321.33Gemini-2.0-flashOptimal path deviation4.875.254.095.26Board state inference54.0828.6779.49-Completed episodes17.569.3337.336.00GPT-4oOptimal path deviation5.305.454.156.30Board state inference41.6719.4963.85-Open Source ModelsCompleted episodes10.161.679.4219.33InternVL2.5-78BOptimal path deviation5.986.395.865.69Board state inference34.9516.5153.38-Completed episodes8.220.671.3322.67LLaVA-OneVision-72BOptimal path deviation6.356.756.815.50Board state inference26.3614.7238.00-Completed episodes5.890.671.6715.33Qwen2-72BOptimal path deviation6.376.666.545.90Board state inference41.5418.7764.31-Aggregate AveragesCompleted episodes20.597.0426.6821.83AverageOptimal path deviation5.325.764.415.32Board state inference43.1022.2663.94-</p>
<p>Table 3 :
3
Evaluation of models across three modalities.Each row shows average steps per episode that were effective moves (EM), ineffective moves (IM), occupied destination moves (OD), out of bounds moves (OB) and illegal commands (IC).
A.4.3 Error Counts for the Auxiliary TaskModelMetricAvg3D2DTextClosed Source ModelsCorrect3.902.305.50-Missed1.421.841.00-Hallucinated0.000.000.00-Sonnet-3.5Coord Errors1.082.160.00-Color Errors0.380.760.00-Shape Errors0.370.740.00-Format Errors0.000.000.00-Correct3.521.865.17-Missed0.911.020.80-Hallucinated0.140.130.14-Gemini-2.0-flashCoord Errors1.983.480.48-Color Errors0.661.140.18-Shape Errors0.651.140.16-Format Errors0.050.000.09-Correct2.711.274.15-Missed1.311.670.95-Hallucinated0.030.010.04-GPT-4oCoord Errors2.343.331.35-Color Errors0.771.180.35-Shape Errors0.751.180.32-Format Errors0.020.040.00-Aggregate AveragesCorrect3.371.814.94-Missed1.211.510.92-Hallucinated0.060.050.06-AverageCoord Errors1.802.990.61-Color Errors0.601.030.18-Shape Errors0.591.020.16-Format Errors0.020.010.03-</p>
<p>Table 4 :
4
Error analysis for the auxiliary position inference task across vision modalities (closed source models).
ModelMetricAvg3D2DTextOpen Source ModelsCorrect2.271.073.47-Missed0.891.000.77-Hallucinated0.030.040.01-InternVL2.5-78BCoord Errors1.622.920.32-Color Errors0.591.110.08-Shape Errors0.581.080.08-Format Errors1.631.301.97-Correct1.710.962.47-Missed1.021.180.86-Hallucinated0.340.310.37-LLaVA-OneVision-72BCoord Errors3.303.952.65-Color Errors1.281.580.97-Shape Errors1.231.570.90-Format Errors0.370.090.65-Correct2.701.224.18-Missed0.971.080.85-Hallucinated0.580.810.36-Qwen2-72BCoord Errors2.523.801.24-Color Errors0.931.420.43-Shape Errors1.121.670.58-Format Errors0.220.060.38-Aggregate AveragesCorrect2.231.083.37-Missed0.961.090.83-Hallucinated0.320.390.25-AverageCoord Errors2.483.561.40-Color Errors0.931.370.49-Shape Errors0.981.440.52-Format Errors0.740.481.00-</p>
<p>Table 5 :
5
Error analysis for the auxiliary position inference task across vision modalities (open source models).</p>
<p>The formalization is achieved through the adaptation of the sequential generalized sliding-tile puzzle, as described in the Appendix A.3. Optimal solutions are computed using the A* algorithm, detailed in Section 4.2.
All source code is available under MIT license at: https://github.com/SharkyBamboozle/iVISPAR
OpenCompass Official Rankings: https://rank. opencompass.org.cn/leaderboard-multimodal
AcknowledgmentsThis work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -456666331, 321892712.A AppendixA.1 Episode Details A.1.1 System Prompt Instructions Interactive Sliding Geom Puzzle Game You are a highly intelligent AI solving a shape puzzle on a 4x4 grid.The board has two states: the current active state and the goal state.Your task is to generate valid actions that transform the current state into the goal state along the shortest path.Steps:(1) Analyze current state.(2) Compare to goal.(3) Check past actions.(4) Propose next move.Movement Rules: Each object occupies one tile.Objects cannot leave the grid or overlap.the position of all objects on the board.3. Respond with a list of the chess-style coordinates and their objects.## Board OverviewThe board has labeled columns, rows, and fields Closed Source Models Sonnet-3.5 (Claude Team, 2024) ---Gemini-2.0-flash(Gemini Team, 2024) ---GPT-4o(OpenAI et al., 2024)---Open Source Models InternVL 2.5(Chen et al., 2024)Qwen 2.5(Yang et al., 2024b)InternViT(Chen et al., 2024)78.4B LLaVA OneVision(Li et al., 2024a)Qwen 2(Yang et al., 2024a)SigLIP(Zhai et al., 2023)73.2B Qwen 2 VL(Wang et al., 2024d)Qwen 2(Yang et al., 2024a)ViT(Dosovitskiy et al., 2020)73.4BTable1: Overview of evaluated models.-indicates unavailable information.A configuration of tiles is represented as an injective mapping from the set {1, . . ., n} to positionsA.3 Sliding Tile PuzzleEach tile must be repositioned from an arbitrary initial configuration S = {s 1 , . . ., s n } to a specified goal configuration G = {g 1 , . . ., g n }, such as an ordered row-major layout.Let the movement path of tile i, where 1 ≤ i ≤ n, be expressed as p i : N 0 → V .The puzzle seeks a set of feasible paths P = {p 1 , . . ., p n } that satisfy the following conditions for all 1 ≤ i, j ≤ n with i ̸ = j, and for all time steps t ≥ 0: Incremental Movement: p i (t + 1) = p i (t) or (p i (t + 1), p i (t)) ∈ E. Tiles move to adjacent, unoccupied positions or stay still.Goal Achievement: p i (0) = s i and p i (T ) = g i for some T ≥ 0. Each tile must start at s i and reach g i .Exclusive Occupancy: p i (t) ̸ = p j (t) for all i ̸ = j.Two tiles cannot occupy the same position at the same time.In this sequential version, tiles move one at a time.Therefore, the head-on collision and corner-following constraints found in the generalized sliding-tile puzzle are omitted, as simultaneous tile movements are not permitted.A.6 Additional Agent Interaction DataA.6.1 Systematic Formatting ErrorsUnless noted otherwise, the numeral in parentheses after a model name is the count of formatting errors for that category.Notably, Sonnet-3.5 is not listed since it did not make any format errors, explaining its high benchmarking score.(E8) Solution not found (N = 147) Prompts that did not begin their solution with "Solution: " are not picked up by the solution parser.GPT-4o (1), InternVL-2.5-78B (145), Qwen2-72B (1).A.6.2 Chain of ThoughtsSonnet-3.5 CoT ExampleLooking at the current state and goal state: 1.The blue cube is correctly positioned in A2 in both states.2. The yellow pyramid needs to move from D1 to C1. 3. The red cylinder is in D2 and needs to be in D3.The yellow pyramid needs to be moved to make space for other movements.Looking at the past states we tried moving the yellow pyramid right and the red cylinder up.Now we need to move the yellow pyramid to its goal position.I can see that the yellow pyramid should move left to reach position C1 as shown in the goal state.Qwen2-72B CoT ExampleAfter analyzing the images, it is clear that moving the red sphere down was an invalid action because it resulted in no change to the board state.The red sphere is still in the same position, and the green cylinder has not moved.To progress, I need to find a valid action that will move one of the objects to match the goal state.Upon further inspection, I notice that the red sphere needs to be moved to the top-right corner of the board to match the goal state.However, the current state shows the red sphere in the bottom-right corner, which means I need to move it up to get it closer to the goal position.GPT-4o CoT ExampleTo reach the goal state I need to compare the current active state with the goal state.In the current active state the green sphere is on top blocking movements of objects below it.The yellow sphere is on the bottom row B1.In the goal state the green sphere is two steps down in column B. The red cylinder is in the bottom left, two steps down.The yellow sphere is in position B2, two steps up.Analyzing the images I see that the yellow spheres path is blocked by the green cylinder I'll first move another object to clear the way.Moving the blue cube rightward will help clear the blocked pathway for the green cylinder.InternVL-2.5-78B CoT Exampleto reach the goal state, let's analyze the current board and the objects that need to be moved: current state (active)• the red cylinder is on tile b4.• the yellow pyramid is on tile d4.• the blue cylinder is on tile c3.• the yellow cube is on tile d2.goal state (goal)• the red cylinder needs to be on tile a4.• the blue cylinder needs to be on tile b4.• the yellow cube needs to be on tile d3.• the yellow pyramid needs to be on tile c4.step-by-step plan 1. move the red cylinder to a4: -action: move the red cylinder left.2. move the yellow pyramid to c4: -action: move the yellow pyramid down.move the yellow cube to d3:-action: move the yellow cube down.move the blue cylinder to b4:-action: move the blue cylinder up.first action the first step is to move the red cylinder from tile b4 to tile a4.Gemini-2.0-flash CoT ExampleThe green sphere needs to move from B2 to C3.It is currently at B2, moving up would move the green sphere to B3 where it is blocked by the red pyramid.I need to move the green sphere to the right, then up.Its already been moved right once, so my next action is to move the green sphere up.LLaVA-OneVision-72B CoT ExampleThe blue cube on D2 needs to move to the position of the red cylinder on D1.Since the red cylinder cannot be displaced, the blue cube must be moved elsewhere.Moving the blue cube left to C2 does not change the board state because another blue cube already occupies that square.Moving it up to D1 would push the red cylinder out of bounds, which is not allowed.Therefore, the blue cube on D2 should move left to D3. Append current_state to path 7: end for return path
Can language models encode perceptual structure without grounding?. Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, Anders Søgaard, 2021a case study in color</p>
<p>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. Mohamed Aghzal, Erion Plaku, Ziyu Yao, 10.48550/arXiv.2310.03249arxiv:2310.032492024Preprint</p>
<p>Exploring and benchmarking the planning capabilities of large language models. Bernd Bohnet, Azade Nova, Aaron T Parisi, Kevin Swersky, Katayoon Goshvadi, Hanjun Dai, Dale Schuurmans, Noah Fiedel, Hanie Sedghi, 10.48550/arXiv.2406.13094arxiv:2406.130942024Preprint</p>
<p>Haider Al-Tahan, and 22 others. 2024. An introduction to vision-language modeling. Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C Li, Adrien Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, 10.48550/ARXIV.2405.17247CoRR, abs/2405.17247</p>
<p>Understanding the limits of vision language models through the lens of the binding problem. Declan Campbell, Sunayana Rane, Tyler Giallanza, Nicolò De Sabbata, Kia Ghods, Amogh Joshi, Alexander Ku, Steven M Frankland, Thomas L Griffiths, Jonathan D Cohen, Taylor W Webb, 10.48550/ARXIV.2411.00238CoRR, abs/2411.002382024</p>
<p>Understanding the limits of vision language models through the lens of the binding problem. Declan Campbell, Sunayana Rane, Tyler Giallanza, Nicolò De Sabbata, Kia Ghods, Amogh Joshi, Alexander Ku, Steven M Frankland, Thomas L Griffiths, Jonathan D Cohen, Taylor W Webb, 10.48550/arXiv.2411.00238arxiv:2411.002382025Preprint</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, arXiv:2412.05271Zhaoyang Liu, and 1 others. 2024arXiv preprint</p>
<p>SpatialRGPT: Grounded spatial reasoning in vision language models. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, Sifei Liu, 10.48550/arXiv.2406.01584arxiv:2406.01584 [cs]. Version: 32024Preprint</p>
<p>Introducing the next generation of claude. Claude Team, 2024</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, ArXiv, abs/2010.119292020</p>
<p>Advancing the understanding and evaluation of ARgenerated scenes: When vision-language models shine and stumble. Lin Duan, Yanming Xiu, Maria Gorlatova, 10.48550/arXiv.2501.13964arxiv:2501.139642025Preprint</p>
<p>PUZ-ZLES: A benchmark for neural algorithmic reasoning. Benjamin Estermann, Luca A Lanzendörfer, Yannick Niedermayr, Roger Wattenhofer, 10.48550/arXiv.2407.00401arxiv:2407.004012024Preprint</p>
<p>Reflective planning: Vision-language models for multistage long-horizon robotic manipulation. Yunhai Feng, Jiaming Han, Zhuoran Yang, Xiangyu Yue, Sergey Levine, Jianlan Luo, 10.48550/arXiv.2502.16707arxiv:2502.167072025Preprint</p>
<p>On computing makespan-optimal solutions for generalized slidingtile puzzles. Marcus Gozon, Jingjin Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Benchmarking vision, language, &amp; action models on robotic learning tasks. Pranav Guruprasad, Harshvardhan Sikka, Jaewoo Song, Yangyue Wang, Paul Pu Liang, 10.48550/arXiv.2411.05821arxiv:2411.058212024Preprint</p>
<p>A formal basis for the heuristic determination of minimum cost paths. Nils J Peter E Hart, Bertram Nilsson, Raphael, IEEE transactions on Systems Science and Cybernetics. 19684</p>
<p>Look before you leap: Unveiling the power of GPT-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, 10.48550/arXiv.2311.17842arxiv:2311.178422023Preprint</p>
<p>Probing contextual language models for common ground with visual representations. Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi, 10.48550/arXiv.2005.00619arxiv:2005.006192021Preprint</p>
<p>GRASP: A novel benchmark for evaluating language GRounding and situated physics understanding in multimodal language models. Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, Elia Bruni, 10.48550/arXiv.2311.09048arxiv:2311.090482024Preprint</p>
<p>CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, Ross Girshick, 10.48550/arXiv.1612.06890arxiv:1612.068902016Preprint</p>
<p>What's "up" with vision-language models? investigating their struggle with spatial reasoning. Amita Kamath, Jack Hessel, Kai-Wei Chang, 10.18653/V1/2023.EMNLP-MAIN.568Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023a. December 6-10, 2023</p>
<p>What's "up" with vision-language models? investigating their struggle with spatial reasoning. Amita Kamath, Jack Hessel, Kai-Wei Chang, 10.48550/arXiv.2310.19785arxiv:2310.197852023bPreprint</p>
<p>Shape-World -a new test methodology for multimodal language understanding. Alexander Kuhnle, Ann Copestake, 10.48550/arXiv.1704.04517arxiv:1704.045172017Preprint</p>
<p>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li, arXiv:2408.03326Llavaonevision: Easy visual task transfer. 2024aarXiv preprint</p>
<p>MuEP: A multimodal benchmark for embodied planning with foundation models. Kanxue Li, Baosheng Yu, Qi Zheng, Yibing Zhan, Yuhui Zhang, Tianle Zhang, Yijun Yang, Yue Chen, Lei Sun, Qiong Cao, Li Shen, Lusong Li, 10.24963/ijcai.2024/15Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence. the Thirty-ThirdInternational Joint Conference on Artificial Intelligence2024bDapeng Tao, and Xiaodong He. International Joint Conferences on Artificial Intelligence Organization</p>
<p>Super-CLEVR: A virtual benchmark to diagnose domain robustness in visual reasoning. Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, Alan Yuille, 10.48550/arXiv.2212.00259arxiv:2212.002592023Preprint</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, 10.48550/arXiv.2205.00363arxiv:2205.003632023Preprint</p>
<p>A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3d embodied environment. Matteo G Mecattaf, Ben Slater, Marko Tešić, Jonathan Prunty, Konstantinos Voudouris, Lucy G Cheke, arxiv:2410.232422024Preprint</p>
<p>Linearly mapping from image to text space. Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick, 10.48550/arXiv.2209.15162arxiv:2209.151622023Preprint</p>
<p>SpartQA: : A textual question answering benchmark for spatial reasoning. Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjmashidi, 10.48550/arXiv.2104.05832arxiv:2104.058322021Preprint</p>
<p>Sliding puzzles gym: A scalable benchmark for state representation in visual reinforcement learning. Bryan Lincoln Marques De Oliveira, Bruno Brandão, Murilo Lopes Da Luz, Luana Guedes, Barros Martins, Telma Woerle De Lima, Luckeciano Soares, Melo Carvalho, NeurIPS 2024 Workshop on Open-World Agents. 2024</p>
<p>Aaron Openai, Adam Hurst, Adam P Lerer, Adam Goucher, Aditya Perelman, Aidan Ramesh, Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, Alex Aleksander M Ądry, Baker-Whitcomb, arXiv:2410.21276Alex Nichol, and 400 others. 2024. Gpt-4o system card. Preprint. Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International Conference on Learning Representations. 2021</p>
<p>GSR-BENCH: A benchmark for grounded spatial reasoning evaluation via multimodal LLMs. Navid Rajabi, Jana Kosecka, 10.48550/arXiv.2406.13246arxiv:2406.13246 [cs]. Version: 22024aPreprint</p>
<p>Towards grounded visual spatial reasoning in multi-modal vision language models. Navid Rajabi, Jana Kosecka, 10.48550/arXiv.2308.09778arxiv:2308.097782024bPreprint</p>
<p>SpaRC and SpaRP: Spatial reasoning characterization and path generation for understanding spatial reasoning capability of large language models. Md Imbesat, Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych, 10.48550/arXiv.2406.04566arxiv:2406.045662024Preprint</p>
<p>Smart visionlanguage reasoners. Denisa Roberts, Lucas Roberts, 10.48550/arXiv.2407.04212arxiv:2407.042122024Preprint</p>
<p>ActPlan-1k: Benchmarking the procedural planning ability of visual language models in household activities. Ying Su, Zhan Ling, Haochen Shi, Jiayang Cheng, Yauwai Yim, Yangqiu Song, 10.48550/arXiv.2410.03907arxiv:2410.039072024Preprint</p>
<p>Sparkle: Mastering basic spatial capabilities in vision language models elicits generalization to composite spatial reasoning. Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, Jinhua Zhao, 10.48550/arXiv.2410.16162arxiv:2410.161622024Preprint</p>
<p>Is A picture worth A thousand words? delving into spatial reasoning for vision language models. Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Neel Joshi, 10.48550/ARXIV.2406.14852CoRR, abs/2406.148522024a</p>
<p>Is a picture worth a thousand words? delving into spatial reasoning for vision language models. Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Yixuan Li, Neel Joshi, 10.48550/arXiv.2406.14852arxiv:2406.148522024bPreprint</p>
<p>SimBench: A rule-based multi-turn interaction benchmark for evaluating an LLM's ability to generate digital twins. Jingquan Wang, Harry Zhang, Huzaifa Mustafa Unjhawala, Peter Negrut, Shu Wang, Khailanii Slaton, Radu Serban, Jin-Long Wu, Dan Negrut, arxiv:2408.119872024cPreprint</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin, arXiv:2409.121912024darXiv preprint</p>
<p>Learning language games through interaction. I Sida, Percy Wang, Christopher D Liang, Manning, arxiv:1606.024472016Preprint</p>
<p>Xingrui Wang, Wufei Ma, Zhuowan Li, Adam Kortylewski, Alan Yuille, 10.48550/arXiv.2310.179143d-aware visual question answering about parts, poses and occlusions. 2023arXiv preprint</p>
<p>Are large vision language models good game players?. Xinyu Wang, Bohan Zhuang, Qi Wu, 10.48550/arXiv.2503.02358arxiv:2503.023582025Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 1 others. 202235</p>
<p>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs. Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, Shiyu Chang, 10.48550/arXiv.2407.01863arxiv:2407.018632024Preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, arXiv:2309.07864Senjie Jin, Enyu Zhou, and 1 others. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint</p>
<p>Evaluating spatial understanding of large language models. Yutaro Yamada, Yihan Bao, Andrew K Lampinen, Jungo Kasai, Ilker Yildirim, 10.48550/arXiv.2310.14540arxiv:2310.145402024Preprint</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, arXiv:2407.10671Jianxin Ma, and 40 others. 2024a. Qwen2 technical report. arXiv preprint</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, arXiv:2412.15115others. 2024b. Qwen2.5 technical report. Junyang Lin, Kai Dang, 22arXiv preprint</p>
<p>Large language models for robotics: A survey. Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S Yu, arXiv:2311.072262023arXiv preprint</p>
<p>Sigmoid loss for language image pre-training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, arXiv:2303.153432023Preprint</p>
<p>Multimodal self-instruct: Synthetic abstract image and visual reasoning instruction using language model. Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, Yueting Zhuang, 10.48550/arXiv.2407.07053arxiv:2407.070532024Preprint</p>
<p>Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning. Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, Wenwu Zhu, 10.48550/arXiv.2504.12680arxiv:2504.126802025Preprint</p>            </div>
        </div>

    </div>
</body>
</html>